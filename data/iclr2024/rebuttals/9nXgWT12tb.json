[
    {
        "title": "Correlated Attention in Transformers for Multivariate Time Series"
    },
    {
        "review": {
            "id": "ELqdith5vk",
            "forum": "9nXgWT12tb",
            "replyto": "9nXgWT12tb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3225/Reviewer_rWhs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3225/Reviewer_rWhs"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a novel \"correlated attention mechanism\" specifically designed to address the challenges presented by cross-correlations in Multivariate Time Series (MTS). Recognizing a gap in existing Transformer-based models which do not adequately capture these cross-correlations, this research seeks to bridge this gap by offering an advanced mechanism that not only grasps instantaneous cross-correlations but also encompasses lagged cross-correlations and auto-correlation.\n\nThe correlated attention mechanism is adeptly crafted to compute cross-covariance matrices between different lag values for queries and keys. A significant feature of this mechanism is its ability to be seamlessly integrated into popular Transformer models, thereby enhancing their efficiency.\n\nIn practical applications, such as production planning, the mechanism demonstrates its utility by effectively addressing the lagged interval between variations like demand and production rates. The research further strengthens its case by adapting the original multi-head attention to accommodate both temporal attentions from existing models and the newly proposed correlated attentions. This design ensures that the base Transformer's embedded layer is directly enhanced with cross-correlation information during representation learning."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "**Strengths**\n\nThe authors meticulously focus on harnessing transformer-based architectures for addressing the forecasting problems associated with multivariate time series (MTS). After a thorough investigation of the prevailing methods in the industry, they present a pivotal question:\n\n*How can we seamlessly elevate the broad class of existing and future Transformer-based architectures to also capture feature-wise dependencies? Can modelling feature-wise dependencies improve Transformers\u2019 performance on non-predictive tasks?*\n\nTo address this, the authors:\n\n1. Delve deep into the mechanisms of Self-attention and De-stationary Attention. They argue that the Transformer models, as currently conceived, cannot explicitly utilize information at the feature dimension. While there have been efforts to tackle these concerns, the extant methodologies are either too specialized or do not adequately account for the intricacies inherent to MTS data.\n\n2. Introduce the Correlated Attention Block (CAB) as a remedy to the aforementioned challenges. They employ normalization to stabilize the time series and leverage lagged cross-correlation filtering to manage lag-related issues. Furthermore, score aggregation is utilized to consolidate scores from different lagged time points, culminating in the final output.\n\n3. Propose rapid computation techniques for CAB, alongside strategies for its integration into multi-head attention mechanisms.\n\n4. The paper excels in its mathematical exposition \u2013 the formulas are presented in a standardized manner, making them easy to follow. Additionally, the experiments are comprehensive and well-executed.\n\nIn terms of originality, quality, clarity, and significance, this work shines by offering both a novel perspective and tangible solutions to the MTS forecasting problems using transformer-based architectures. Combining existing ideas with innovative approaches, the paper removes limitations observed in previous results, making it a notable contribution to the domain."
                },
                "weaknesses": {
                    "value": "While the paper has several strengths, there are also areas where it could be improved:\n\n1. Lack of evaluation on prediction tasks: For prediction tasks, such as the MLTSF dataset, the paper does not provide an evaluation of the impact of the Correlated Attention Block (CAB) or compare it with other models that utilize inter-variable correlations. Including such evaluations and comparisons would provide a more comprehensive understanding of the effectiveness of CAB in prediction tasks.\n\n2. Insufficient description of hyperparameter settings: The paper lacks detailed explanations of the hyperparameter settings. For example, in Equation (5), how the initial value of lambda (\\lambda) is chosen and how the values of k and c are determined are not clearly stated. Providing more guidance on these hyperparameters would help readers understand the choices made and improve reproducibility.\n\n3. Non-compliance with ICLR submission requirements: The paper does not follow the submission requirements of ICLR by placing the appendix together with the main text. It would be better to separate the appendix from the main text, following the formatting guidelines specified by the conference.\n\nAddressing these areas of improvement would enhance the clarity, reproducibility, and comprehensiveness of the paper, providing readers with a better understanding of the proposed method and its performance in prediction tasks."
                },
                "questions": {
                    "value": "1. How were the hyperparameters determined in the experiments? Specifically, can you provide more details on the selection of hyperparameters such as the initial value of lambda (\\lambda) and the values of k and c? Understanding the rationale behind these choices would help in reproducing the results and provide insights into the sensitivity of the proposed method to hyperparameter settings.\n\n2. Can you provide a more detailed evaluation of the Correlated Attention Block (CAB) on prediction tasks, such as the MLTSF dataset? It would be interesting to see how CAB performs compared to other models that utilize inter-variable correlations in prediction tasks. This analysis would shed light on the effectiveness of CAB in different scenarios and provide a better understanding of its potential advantages.\n\n3. In Table 2 and Table 3, it is observed that in some cases, the performance of CAB+Transformer is not as good as Nonstationary, and in some cases, Nonstationary+CAB even leads to worse results. Can you provide an explanation for these observations? What factors contribute to the varying performance of the proposed method in different settings? Understanding the limitations and potential trade-offs of the proposed method would provide valuable insights for future improvements."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3225/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697959306047,
            "cdate": 1697959306047,
            "tmdate": 1699636270856,
            "mdate": 1699636270856,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WpupxPTo1x",
                "forum": "9nXgWT12tb",
                "replyto": "ELqdith5vk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3225/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3225/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply 1.1 to Reviewer rWhs"
                    },
                    "comment": {
                        "value": "We are really appreciative of your acknowledgement of our work's contribution. Per your request, we have also included additional experiments in Appendix C in the revised manuscript on long-term forecasting for three common datasets spanning various domains. The results demonstrate the benefit of our CAB even in forecasting. Given this and if the following concerns of yours are addressed, we hope you may consider increasing the score for our work.\n \n $\\textbf{Q1:}$ \"Can you provide a more detailed evaluation of the Correlated Attention Block (CAB) on prediction tasks, such as the MLTSF dataset? It would be interesting to see how CAB performs compared to other models that utilize inter-variable correlations in prediction tasks. This analysis would shed light on the effectiveness of CAB in different scenarios and provide a better understanding of its potential advantages.\"\n \n \n\n$\\textbf{Response:}$ As mentioned above, we have included additional experiments for Multivariate Long-term Time Series Forecasting in Appendix C in the revised manuscript. As shown therein, when paired with a strong baseline for long-term forecasting, CAB results in  competitive performance. Specifically, in the dataset like Exchange in our test, CAB gives significant performance boost for longer series sequence. With the flexibility of CAB, we look forward to its deployment on top of many existing Transformers for efficiency improvement. \n\n\n\n$\\textbf{Q2:}$ \"In Table 2 and Table 3, it is observed that in some cases, the performance of CAB+Transformer is not as good as Nonstationary, and in some cases, Nonstationary+CAB even leads to worse results. Can you provide an explanation for these observations? What factors contribute to the varying performance of the proposed method in different settings? Understanding the limitations and potential trade-offs of the proposed method would provide valuable insights for future improvements.\"\n\n\n$\\textbf{Response:}$ For the anomaly detection task in Table 3, we note that the main metrics within interest is usually the F1 score, which is the harmonic mean of precision (P) and recall (R), thereby giving balance between the two. In terms of F1, CAB actually provides consistent improvement when plugged into both Transformer and Non-stationary Transformer, where decrease in F1 score is only in one dataset for each base model. For the imputation task in Table 2, this task differs from other tasks in that it involves missing data. This can make the representation learning in CAB harder, as the lagged cross-correlation filtering step is sensitive to time step data. For example, if a time lag with high correlation is yet missing, CAB would not have data to detect such potentiall lagged cross-correlation. \n\n\n$\\textbf{Q3:}$ How were the hyperparameters determined in the experiments? Specifically, can you provide more details on the selection of hyperparameters such as the initial value of lambda (\\lambda) and the values of k and c? Understanding the rationale behind these choices would help in reproducing the results and provide insights into the sensitivity of the proposed method to hyperparameter settings.\n \n \n\n$\\textbf{Response:}$ We kindly refer the reviewer to our Appendix A in the paper for details on implementation and hyperparameter setting, which would provide information to all of your question. \n\n\n$\\textbf{Q4:}$ \"Non-compliance with ICLR submission requirements: The paper does not follow the submission requirements of ICLR by placing the appendix together with the main text. It would be better to separate the appendix from the main text, following the formatting guidelines specified by the conference.\"\n\n\n$\\textbf{Response:}$ We thank you for that, and will separate the main text from the supplementaries in later versions including our revised manuscript now. \n\n------ \n\nWe thank you again for your meaningful comments. If there is any concern left, we would be pleased to answer in the remaining time of the discussion period. If our positive results on forecasting as well as asnwers could have properly addressed your main concerns, we hope that you could consider raising the score for our work."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3225/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700097740907,
                "cdate": 1700097740907,
                "tmdate": 1700097740907,
                "mdate": 1700097740907,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "tlNhAzCYyZ",
            "forum": "9nXgWT12tb",
            "replyto": "9nXgWT12tb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3225/Reviewer_KQRi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3225/Reviewer_KQRi"
            ],
            "content": {
                "summary": {
                    "value": "The paper focuses on how to learn the feature-wise correlation when applying a transformer in the multivariate time series for various tasks.  The proposed correlated attention operates across feature dimensions to compute a cross-variance matrix between keys and queries. They introduce a lag value in the process so that it can learn not only instantaneous but also aged cross-correlations. The proposed method shows improved performance on tasks such as classification and anomoly detection."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper's main focus is to address the learning of feature-wise correlation in the transformer attention setup. They explore if the learning feature-wise correlation actually helps in tasks other than forecasting such as anomaly detection, imputation, and classification.\n\nThe proposed correlated attention can capture not only conventional cross-correlation but also capture auto-correlation, and lagged cross-correlation.  The idea that allows one to learn lagged correlation and be able to integrate the most relevant multiple lagged correlation sounds interesting."
                },
                "weaknesses": {
                    "value": "1, Some parts of the paper presentation could be improved,  such as the explanation of the methods, for more details check the question sections. \n2. The experiment section does not look very convincing due to the comparison setup (if it is fair or not, please refer to the question section) and results. Given the huge computational cost of integrating the cross-correlation, the experiment results do not look that significant."
                },
                "questions": {
                    "value": "1. Some parts are a bit confusing, for instance,  \n\u201cCrossFormer deploys a convoluted architecture, which is isolated from other prevalent Transformers with their own established merits in temporal modeling and specifically designed for only MTS forecasting, thereby lacking flexibility\u201d I am a bit confused, could you explain more in detail this?\n\nThe section to explain equation 5 needs to be improved. Especially the explanation for operator argTopK()  reads a bit confusing.\n\n3. It is confusing how the value of k in equation 6 is defined, do you get the value of c first and then calculate k with the topK operation?  It is not very clear to me why not directly take top k,  for instance, top 5,  lagging value, and use it instead of getting a value k by using the topK operator?  Any motivation behind?\n\n4. I am not sure how to go from equation 7 to the result they got in the section below, maye some proof?\n5. It seems even with FFT, the computational complexity is still quite high for a time series with a large feature dimension.\n\n6. When evaluating a specific task, it is crucial to compare its performance with a model that has been explicitly designed and optimized for that particular task. For instance, both non-stationary transformers, dlinear and FEDformer are designed for forecasting tasks. The reviewer are not 100% sure that whether it is a fair comparison when applying those to classification, and anomaly detection tasks.\n\n7. I think the most fair comparison is transformer vs transformer +CBA where the transformer has the same number of heads as the transformer +CBA (when we count both temporal attention and correlated attention heads). Does the transformer in Table 2 has exact same head as the transformer +CBA? The results of the transformer in that table do not show much improvement when compared to transformer + CBA. \n\n8. In anomaly detection and classification, it shows that transformer +CBA has significant improvement compared to transformer, this was not observed in the imputation task, any insights into that?\n\n9. It would be interesting to see the performance on the forecasting task as well I think since there is nothing in the design that specifically restricts it to the non-predictive task.\n\n10 . I think the paper main motivation was addressing the feature-wise correlation specifically for non-predictive tasks, but I am missing the discussion what is the difference when you learning the feature-wise correlation for forecasting or for non-predictive task and what is in this model that makes it more fit for the non-predictive task"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3225/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698772227433,
            "cdate": 1698772227433,
            "tmdate": 1699636270776,
            "mdate": 1699636270776,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fFyvgrxZT2",
                "forum": "9nXgWT12tb",
                "replyto": "tlNhAzCYyZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3225/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3225/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply 1.1 to Reviewer KQRi"
                    },
                    "comment": {
                        "value": "We thank you the reviewer for all the comments and suggestions, and would like to hereby address your questions. If we could answer all of your concerns, we hop taht you can consider increasing the score for our work. \n\n\n$\\textbf{Q1:}$ \" \u201cCrossFormer deploys a convoluted architecture, which is isolated from other prevalent Transformers with their own established merits in temporal modeling and specifically designed for only MTS forecasting, thereby lacking flexibility\u201d I am a bit confused, could you explain more in detail this?\"\n\n$\\textbf{Response:}$ We just want to discuss that CrossFormer's architecture is very unconventional from other prevalent Transformer with simple multi-head attention that yet has been  shown to be very good in temporal modeling. By deviating from the very base architecture of vanilla Transformer, Crossformer may not utilize such advantages; for your information, Crossformer deploys a Hierarchical Encoder-Decoder architecture with Dimension-Segment-Wise embedding, which devides time into segmants,  Two-Stage Attention and  segment merging, which merges back the time. We thank you for the suggestion, and will re-write it for more clarity in the updated versions in the future. \n\n\n\n$\\textbf{Q2:}$ \"It is confusing how the value of k in equation 6 is defined, do you get the value of c first and then calculate k with the topK operation? It is not very clear to me why not directly take top k, for instance, top 5, lagging value, and use it instead of getting a value k by using the topK operator? Any motivation behind?\"\n\n$\\textbf{Response:}$ We would like to clarify that such $k = c \\log(T)$ is indeed the input $k$ to TopK operation, instead of getting a value $k$ out of topK operator. So yes, you would choose $c$ first, which is a hyperparameter, and then use $k = c \\log(T)$ as the input to TopK. Also, motivation for doing $k = c \\log(T)$ is that even if one uses FFT  of $O(T \\log(T))$ for fast computation, your final computational cost will still depend on $O(d^2 T \\cdot k  )$ for summing up $k$ terms in Eq. (6), so setting $k = c \\log(T) = O(\\log(T))$ would allow you to maintain such $O(T \\log(T))$ complexity. \n\n\n$\\textbf{Q3:}$ \"I am not sure how to go from equation 7 to the result they got in the section below, maye some proof?\"\n\n$\\textbf{Response:}$ We would like to explain it in more details here. First, please note that Eq (7) \n$S_{X, Y}(f) = F(X_t) F^*(Y_t)$ should be interpreted as $S_{X, Y}(f) = F(X_t)(f) \\cdot F^*(Y_t)(f)$. Note that in the two equations in Eq Eq (7), the variable $f$ corresponds to frequency domain and $l$ corresponds to time domain. While we write $S_{X, Y}(f)$  and $(X \\star Y)(l)$ in point-wise form as in the paper, which demonstrates the Cross-correlation Theorem and how you can compute  $(X \\star Y)(l)=  \\sum_{t=1}^{T} X_{t-l} Y_t$ via FFT and inverse FFT, the actual implementation using Pytorch's FFT library would let you input, for example in case of $d=1$ feature, the vector $X_t$  of size $T$ in time domain (i.e. a vector of all $l$ stacked together), and perform FFT in $O(T \\log(T))$ to return you the vector $F(X_t)$ of size $T/2+1$ in frequency domain (i.e. a vector of all $f$ stacked together). From there, you can compute $S_{X, Y}(f) = F(X_t) F^*(Y_t)$, which is again a vector  of all $f$ stacked together.  From the second equation in Eq (7), you can now  feed such vector into the inverse FFT to get the vector  $(X \\star Y)(l)= F^{-1}( S_{X, Y}(f) )$ of all $l = 0\\to T-1$ stacked together. We hope that this can clarify your concern. \n\n\n$\\textbf{Q4:}$ \"It seems even with FFT, the computational complexity is still quite high for a time series with a large feature dimension.\"\n\n$\\textbf{Response:}$ We kindly refer the reviewer to our new experiments on run-time of the baselines in Appendix D of the manuscript. It shows that the overhead incurred by our CAB to the base models is quite minimal, so the resulting models still maintain around the same order of magnitude in computation as other SOTA Transformers. We do agree with you CAB will have another multiplicative factor of $d$ in its complexity, yet CAB has advantage in $T$, which has been usually the real computational bottleneck to be addressed in the literature of Transformers' computational complexities. Further acceleration in $d$ would be interesting future direction."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3225/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700097591659,
                "cdate": 1700097591659,
                "tmdate": 1700097591659,
                "mdate": 1700097591659,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eD5azcinqZ",
                "forum": "9nXgWT12tb",
                "replyto": "42Vbo3w5ru",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3225/Reviewer_KQRi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3225/Reviewer_KQRi"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer comment"
                    },
                    "comment": {
                        "value": "Thank you for the author's detailed answer, clarification, and extra experimental results. After going through the revision, and the other reviewers' comments and answers, I decided to keep my score. My reasons are that compared to the computational complexities, the introduced method seems to have very minor improvement, I am skeptical of how much of that contribution is really coming due to the introduction of the feature-wise correlation or if it is due to other factors such as optimizations, randomness and some differences on the architects of the network."
                    }
                },
                "number": 29,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3225/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700742433365,
                "cdate": 1700742433365,
                "tmdate": 1700742433365,
                "mdate": 1700742433365,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cEp14DIJ15",
            "forum": "9nXgWT12tb",
            "replyto": "9nXgWT12tb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3225/Reviewer_mjBo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3225/Reviewer_mjBo"
            ],
            "content": {
                "summary": {
                    "value": "The author extend autoformer to cross-correlation and propose a correlated attention mechanism to capture feature-wise dependencies."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The authors propose a correlated attention mechanism to capture lagged cross-covariance between variates, which can combined with existing encoder-only transformer structure.\n2. The experiments show correlated attention mechanism enhances base Transformer models."
                },
                "weaknesses": {
                    "value": "1. The novelty of the paper is limited.  The proposed correlated attention is basically a extension for Autoformer, which only captures auto-correlation. However, this method neither proposes a good method to reduce the computational complexity caused by calculating cross-correlation, which is almost unacceptable in actual scenarios, nor does the author conduct a comparative experiment with Autoformer to prove that the introduction of corss-correlation can bring to achieve practical improvements. \n2. The integration of correlated attention to existing transformer structure is conducted with a mixture-of-head attention structure, which is a concatenation of CAB and transformer outputs. CAB acts as a rather independent component and does not truly integrated into existing transformer structures."
                },
                "questions": {
                    "value": "1. Judging from the design of CAB, it can predict independently without requiring additional transformer deconstruction. Why didn't you test the independent CAB? \n2. The design of CAB is based on autoformer. Why is there no comparison between the effects of CAB and autoformer?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3225/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3225/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3225/Reviewer_mjBo"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3225/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698777236875,
            "cdate": 1698777236875,
            "tmdate": 1700664114862,
            "mdate": 1700664114862,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BOradmL5ah",
                "forum": "9nXgWT12tb",
                "replyto": "cEp14DIJ15",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3225/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3225/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply 1.1 to Reviewer mjBo"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the comments, and believe that there can be some misunderstanding on our work contribution that we would be pleased to hereby clarify. \nWe really hope that if your concerns can be properly addressed in the rebuttal and the position of our contribution is clarified, you can positively re-evaluate our work. Before giving our detailed answers, we kindly summarize the following points:\n- Our architecture has many differences from Autoformer, which itself was very specifically designed to capture auto-correlation in long-term forecasting, in order to capture lagged cross-correlation. To this end, we have provided a paragraph (highlighted in blue) at the end of Section 3.2.2 in the revised manuscript to emphasize the differences.\n\n- We have included the performance of Autoformer in all of our experiments in the revised manuscript. The results confirm that Autoformer is not as competitive as other baselines and our models; this is quite expected as Autoformer was designed for forecasting, and also a reason why we did not include it in our original experiments focusing more on encoder-only tasks. \n- For completeness, in Appendix C of the revised manuscript, we also provide new experiments on long-term forecasting, which Autoformer excels in, on three datasets spanning different domains. In these tests, the our Non-stationary+CAB model achieves the best performance among Transformer on two out of three datasets. While Autoformer still has very competitive results, it generally lags behind Fedformer, which is one of the latest and SOTA Transformers for long-term forecasting.\n\n$\\textbf{Q1:}$ \"The novelty of the paper is limited. The proposed correlated attention is basically a extension for Autoformer, which only captures auto-correlation. \";\n\"The design of CAB is based on autoformer. Why is there no comparison between the effects of CAB and autoformer?\"\n\n\n$\\textbf{Response:}$ We believe that our proposed CAB (correlated attention block) has many differences from the Autoformer, and have provided a paragraph (highlighted in blue) at the end of Section 3.2.2 in the revised manuscript to emphasize the differences. We would like to next re-iterate our discussion in the paper here in our response for your convenience. \n\nSince the CAB aims to capture the lagged cross-correlation,which is relevant to yet more generalized than the auto-correlation module in Autoformer, we believe it is crucial to emphasize the main differences. First, Autoformer overall is a decomposed encoder-decoder architecture proposed for long-term forecasting, so its auto-correlation module is specifically designed to work with series seasonalities extracted from various series decomposition steps of Autoformer. On the other hand, CAB ensures flexibility with any input series representation by deploying normalization step and learnable temperature coefficient $\\tau$ reweighting the correlation matrices. Second, while Autoformer computes purely auto-correlation scores and aggregates their exact values for TopK , CAB computes cross-correlation matrices and aggregates the absolute values of such entries for TopK in Equation 5 (as correlation can stem from either positive or negative correlation). Finally, to facilitate robustness to different input series representation, CAB adopts learnable weights $\\lambda$ in TopK operation, which balances between auto-correlation and cross-correlation, and $\\beta$ in sub-series aggregation, which balances between instantaneous and lagged cross-correlation."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3225/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700097419492,
                "cdate": 1700097419492,
                "tmdate": 1700097419492,
                "mdate": 1700097419492,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wH23nivaTD",
                "forum": "9nXgWT12tb",
                "replyto": "FpzuO1fA9v",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3225/Reviewer_mjBo"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3225/Reviewer_mjBo"
                ],
                "content": {
                    "comment": {
                        "value": "Given your experiment on long-term forecasting and the validation of utilizing cross-correlation to achieve further improvements on top of the Autoformer, I would consider boosting my scores. However, I still have some concerns regarding your updated experimental results:\n\n1. Currently, there is a limited number of datasets being tested for forecasting tasks, with only three datasets available. Furthermore, in one of these datasets, CAB did not surpass Autoformer. I believe such results make it challenging to demonstrate significant overall improvements brought about by the CAB method with cross-correlation.\n\n2. Your runtime analysis was conducted on the ETTh1 dataset, which contains only seven variables. On this dataset, the squared complexity of CAB may not pose a significant issue. However, if applied to datasets with a larger number of variables, such as traffic data, complexity could become a problem."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3225/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700448308923,
                "cdate": 1700448308923,
                "tmdate": 1700448308923,
                "mdate": 1700448308923,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "x1xtFQCnob",
            "forum": "9nXgWT12tb",
            "replyto": "9nXgWT12tb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3225/Reviewer_M5Eq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3225/Reviewer_M5Eq"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel concept called the Correlated Attention Block (CAB), designed to efficiently capture cross-correlations between multivariate time series (MTS) data within Transformer-based models. The CAB is a versatile component that seamlessly integrates into existing models. Its key innovation lies in the correlated attention mechanism, which operates across feature channels, enabling the computation of cross-covariance matrices between queries and keys at various lag values. This selective aggregation of representations at the sub-series level opens the door to automated discovery and representation learning of both instantaneous and lagged cross-correlations while inherently encompassing time series auto-correlation.\n\nThe authors conducted an extensive series of experiments, focusing on Imputation, Anomaly Detection and Classification tasks. Their results demonstrate remarkable performance, underscoring the potential of the CAB to enhance the analysis and modeling of MTS data."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper is well-structured, presenting a thorough background introduction and a step-by-step introduction of the novel concept, the Correlated Attention Block (CAB). A notable feature of this work is the seamless integration of CAB into encoder-only architectures of Transformers, making it a potentially good-to-have addition to the field.\n\nFurthermore, the authors conducted an extensive set of experiments across three different tasks, utilizing a variety of common datasets. The results consistently show impressive performance, often outperforming previous state-of-the-art methods. This robust evaluation underscores the potential of the proposed design in improving representation learning for multivariate time series, making it a valuable contribution to the field."
                },
                "weaknesses": {
                    "value": "1. Page 9, Line 1 of **Conclusion And Future Work**: There's a minor typo that needs correction - \"bloc\" should be changed to \"block.\"\n2. Citation Style: The reference list shows some inconsistency in the citation style. To enhance clarity and uniformity, consider standardizing the format across all references. For example, you could list all NeurIPS papers with consistent formatting, and for papers from other conferences or sources, ensure that their respective publication details are included appropriately. For instance:\n    -   NeurIPS papers should consistently include the conference name and URL. For example, \"Vaswani et al. (2017, NeurIPS, URL) and Shen et al. (2020, NeurIPS, URL).\"\n    -   Papers from other conferences or sources should similarly follow a consistent format, such as including the conference name and URL as needed. For example, \"Cao et al. (2020, Conference Name, URL)\" and \"Li et al. (2019, Conference Name, URL).\"\n3.  Motivation for Correlated Attention Block: While the paper mentions that CAB efficiently learns feature-wise dependencies, it would be beneficial to provide more clarity on the specific aspects of CAB's design that contribute to this efficiency. Clearly articulating which components within the CAB block are the key drivers of this efficiency could help readers better understand the innovation.\n4.  Analysis of FFT Efficiency: While Section 3.2.2 discusses the time efficiency of FFT, it would be valuable to include a clear analysis that quantifies how much the use of FFT improves the performance of CAB compared to vanilla CAB or previous baselines. Providing concrete numbers or performance metrics would strengthen the paper's findings in this regard. \n5. Limitation of Encoder-Only Models: It's important to acknowledge that the design of CAB is limited to encoder-only models and does not support time series forecasting. While this limitation is briefly mentioned, expanding on the reasons behind this constraint and discussing potential avenues for future work or extensions to address this limitation would add depth to the paper."
                },
                "questions": {
                    "value": "1.  In Table 2, it is evident that for the first three datasets, the TimesNet baseline consistently outperforms CAB when the mask ratio exceeds 25%. Could you provide insights into this performance discrepancy?\n2.  Could you elaborate on the primary challenges or obstacles preventing the integration of CAB into encoder-decoder models for conducting multivariate time series forecasting?\n3.  Could you provide a detailed breakdown of the distinct contributions of each component within CAB, both in terms of performance enhancement and efficiency gains?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3225/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3225/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3225/Reviewer_M5Eq"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3225/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698810158305,
            "cdate": 1698810158305,
            "tmdate": 1700680799422,
            "mdate": 1700680799422,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aK0cCZKHXL",
                "forum": "9nXgWT12tb",
                "replyto": "x1xtFQCnob",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3225/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3225/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply 1.1 to Reviewer M5Eq"
                    },
                    "comment": {
                        "value": "We thank the reviewer very much for the insightful comments as well as the acknowledgement of our proposed method's benefits. Please find our revised manuscript taking into account your suggestions and providing additional experiments in the appendix to address the reviewer's concerns on computational complexity of CAB and CAB's performance on predictive tasks. \nGiven the new  positive experimental results and if we can properly address all of your concerns, we really hope that you can increase the score for our work.Now, we hereby would like to address the main concerns of the reviewer.\n\n\n\n$\\textbf{Q1:}$ \"It would be beneficial to provide more clarity on the specific aspects of CAB's design that contribute to this efficiency.\";\n\"Could you provide a detailed breakdown of the distinct contributions of each component within CAB, both in terms of performance enhancement and efficiency gains?\"\n\n$\\textbf{Response:}$ We kindly refer the reviewer to the ablation studies in Appendix B for performance analysis of specific aspects of CAB. In the revised manuscript, we added blue text further highligting the most crucial components of the CAB. For a summary, from the empirical findings, the lagged cross-correlation filtering component (even when we hard-fix the learnable parameters as constants) resulted in highest increase in the accuracy of 1.39\\% from the basic CAB model without lagged cross-correlation. Then, given that the lagged-cross correlation filtering is used, letting the parameter $\\beta$, which is in Eq. (6) of our paper and balances the effect of instantaneous and lagged cross-correlation, be learnable would result in the highest increase in accuracy of 1.81\\%.  \n\n\n\n$\\textbf{Q2:}$  \"Could you elaborate on the primary challenges or obstacles preventing the integration of CAB into encoder-decoder models for conducting multivariate time series forecasting?\";\n\"It's important to acknowledge that the design of CAB is limited to encoder-only models and does not support time series forecasting. While this limitation is briefly mentioned, expanding on the reasons behind this constraint and discussing potential avenues for future work or extensions to address this limitation would add depth to the paper.\"\n\n$\\textbf{Response:}$ Before providing detailed explanations, we would like to note that:\n-  We provide new experiments in Appendix C of the revised manuscript demonstrating that the CAB, even when naively plugged into the decoder architecture of base architectures, indeed consistently improves the performance of such Transformer-models over three long-term forecasting datasets spanning different domains!   \n- We believe that the integration of CAB into the decoder can be done even better in the future with proper design of masking mechanism, as discussed in details below. \n\nFirst, we recall the main difference between the decoder and the encoder is that the decoder has two multi-head attention (MHA) blocks: one with masking and one without masking (i.e. conventional MHA). The masking is meant to prevent information to preserve the auto-regressive property, so that the forecasting of the next time step should depend only on the time steps strictly before it. In typical self-attention mechanism, this is done by  masking out (setting to $-\\infty$) all values in the lower triangular of the scoring matrix $Q K^T\\in \\mathbb{R}^{T \\times T}$ that is input to softmax. As you can see, $Q K^T$ can be thought of as the attention/correlation matrix in time (with dimension $T\\times T$), so masking out the triangular part corresponds to \"blinding\" the history in making predictions; one the other hand, in CAB, the $Roll(K, l)^T Q \\in \\mathbb{R}^{d_k \\times d_k}$, which is input to softmax, is correlation matrix across feature with dimension $d_k \\times d_k$ instead of time, so the same masking strategy, even if naively applicable, would not have intuitive or physical phenomena as in vanilla self-attention, thereby being more of a heuristic approach. In the experiments in Appendix C, we only replace the non-masked MHA with our mixture-of-head attetions comprised of temporal attentions and our CAB, while keeping the masked MHA of the base Transformer. As you can see from the results, the CAB consistently improves performance of Non-stationary Transformer across all the tested datasets and resulting in the model that outperforms the previous SOTA Transformers for long-term forecasting."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3225/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700097191931,
                "cdate": 1700097191931,
                "tmdate": 1700097191931,
                "mdate": 1700097191931,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PQMpYPUU8h",
                "forum": "9nXgWT12tb",
                "replyto": "x1xtFQCnob",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3225/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3225/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Discussion Period"
                    },
                    "comment": {
                        "value": "If you have further concerns regarding our answers, we are more than happy to respond to them during this discussion period. Otherwise, we would really appreciate your acknowledgement that you concerns indeed have been cleared. \n\nAgain, we thank you very much for your time and reviews!"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3225/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700427780334,
                "cdate": 1700427780334,
                "tmdate": 1700427799090,
                "mdate": 1700427799090,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ov19sIUKll",
                "forum": "9nXgWT12tb",
                "replyto": "x1xtFQCnob",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3225/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3225/Authors"
                ],
                "content": {
                    "title": {
                        "value": "End of Discussion Period"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nWe kindly remind that the discussion period is coming to the end. Thus, please provide us with any concern you may still have. Otherwise, we would really appreciate your acknowledgement that you concerns indeed have been cleared and hope that you consider raising the score for our work, especially given the additional experiments having been added supporting our method.\n\nAgain, we thank you very much for your time and and hard work reviewing our paper!\n\nAuthors"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3225/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700645146515,
                "cdate": 1700645146515,
                "tmdate": 1700645155551,
                "mdate": 1700645155551,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "25UfVZODlz",
                "forum": "9nXgWT12tb",
                "replyto": "jZslCR0uVj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3225/Reviewer_M5Eq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3225/Reviewer_M5Eq"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "Thanks for the response, and I'm willing to raise my score to 6 given most of my concerns are resolved."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3225/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700680885766,
                "cdate": 1700680885766,
                "tmdate": 1700680923678,
                "mdate": 1700680923678,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]