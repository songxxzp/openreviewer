[
    {
        "title": "The Joint Effect of Task Similarity and Overparameterization on Catastrophic Forgetting - An Analytical Model"
    },
    {
        "review": {
            "id": "2Dwbu9l2VN",
            "forum": "u3dHl287oB",
            "replyto": "u3dHl287oB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2816/Reviewer_SXpA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2816/Reviewer_SXpA"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies catastrophic forgetting in an analytical way. \nSpecifically, they focus on two-task continual linear regression and uncover a pattern in overparameterized models \nwhere intermediate task similarity leads to the most forgetting."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The results under overparameterized models are interesting."
                },
                "weaknesses": {
                    "value": "1. The writing needs improvement as it is difficult to grasp the central logical structure.\n2. Format in assumption 1 seems not correct since there is a long blank.\n3. The proof is difficult to follow, I can tell why the three terms in Lemma 6 can be replaced by the long formulas.\n4. I think the paper 'Analysis of catastrophic forgetting for random orthogonal transformation tasks in the overparameterized regime' discusses a similar task. It is difficult to tell what the difference and improvement compared to that."
                },
                "questions": {
                    "value": "1. I have tried to read all proof, but it is difficult to follow and check all the long expressions. \nI think it will be better to make the proof clearer to understand."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2816/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698563245420,
            "cdate": 1698563245420,
            "tmdate": 1699636225250,
            "mdate": 1699636225250,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tQs328a2XP",
                "forum": "u3dHl287oB",
                "replyto": "2Dwbu9l2VN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2816/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2816/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their comments and we are glad that the reviewer generally found our results interesting.\n\nHowever, there seems to be a misunderstanding regarding the scope of the paper. The reviewer wrote in their summary that we \n*\u201cuncover a pattern in overparameterized models where intermediate task similarity leads to the most forgetting\u201d*. This is true but it is only a part of our paper.\n\nGenerally, our paper is the first to reveal a nuanced analytical interplay between overparameterization and task similarity, and show that overparameterization alone is not enough to understand catastrophic forgetting (in contrast to a current line of continual learning empirical papers which only focuses on the benefits of overparameterization). Our Theorem 3 and Figure 3 clearly demonstrate this.\n\nMoreover, in Weakness 4 the reviewer wrote that they believe that Goldfarb and Hand (2023) already addressed a similar research question.  \nWe have to respectfully disagree, since there are important differences between our work and theirs, which we have already thoroughly discussed in multiple places in our paper (e.g., in the Introduction, Learning Dynamics of Section 2.2, and in the Simulations of Section 2.4). Specifically, Goldfarb and Hand (2023) only study the effect of overparameterization without considering any notion of task similarity. They analyze the regime of *low* task similarity, i.e., applying completely random transformations to the first data matrix ($\\alpha=1$), while our main result analyzes the *full* spectrum of task similarity from $\\alpha=0$ to $1$.  \nAdditionally, that paper uses restrictive data assumptions and provides an upper bound on performance, while our work does not assume any particular data model and our main result is an *exact* expression for expected forgetting.\n\n---\n\nAs to the more minor comments,\n\n- **Weakness 3 & Question 1:**\n> The proof is difficult to follow.\n\n  **Response:** Deriving exact expressions for our analysis was indeed difficult and required 60+ pages of complicated proofs. We originally tried to make our proof\u2019s idea accessible using the Proof Sketch that appears right after Theorem 3.  \n  Following the reviewer\u2019s comment, we will further revise the manuscript and add an entire page presenting a Proof Outline in a new appendix for the final version (this requires a lot of time and we want the reviewers-authors discussion to start in the meanwhile).\n\n---\n\n- **Weakness 2:**\n> Format in Assumption 1 seems incorrect since there is a long blank.\n\n  **Response:** The long blank was not a mistake but a styling choice (using \\hfill). We now removed this blank to avoid confusion.\n\n---\nIf we have adequately addressed the reviewer's concerns, we kindly ask the reviewer to consider raising their score. If there are any remaining concerns, please let us know."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2816/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700238847044,
                "cdate": 1700238847044,
                "tmdate": 1700238847044,
                "mdate": 1700238847044,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "sDs474knH3",
            "forum": "u3dHl287oB",
            "replyto": "u3dHl287oB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2816/Reviewer_6udf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2816/Reviewer_6udf"
            ],
            "content": {
                "summary": {
                    "value": "In order to explore the common impact of task similarity and overparameterization on forgotten, the paper obtains a conclusion through formula derivation. The conclusion shows a non-monotonic behavior in task similarity when the model is suitably overparameterized, and a monotonic behavior when it is critically overparameterized. In addition, the paper verified the conclusion through a large number of experiments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper obtained a precise and effective conclusion through a large number of formula derivations and experiments. This conclusion can help us better understand the forgetting problems of the model."
                },
                "weaknesses": {
                    "value": "The experimental setting of the paper is very limited, and the scope of application of the conclusions is questionable."
                },
                "questions": {
                    "value": "1.\tIs the conclusion proved to be universal? If in a non-linear complex neural network, is the model forgetting the same as the conclusion?\n2.\tWe can only mess up pixels as a standard of dissimilarity, which does not match the real scene. So it's hard to verify the conclusions on other data.\n3.\tIs overparameterization equivalent to the learning capacity of the model? Do the conclusions of the paper apply to other data and models?\n4.\tThe formulas derived in this paper are rather complicated. Can you please explain Pseudoinverse Properties and Operator Norm Properties in detail?\n5.\tThis paper only conducted experiments on the mnist dataset. Can the same conclusion be reached for other complex datasets?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2816/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2816/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2816/Reviewer_6udf"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2816/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698646752027,
            "cdate": 1698646752027,
            "tmdate": 1699636225176,
            "mdate": 1699636225176,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CKu2FqEFNT",
                "forum": "u3dHl287oB",
                "replyto": "sDs474knH3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2816/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2816/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate the reviewer's comments and are pleased that they recognize the value and contribution of our findings to the continual learning literature.\n\nBelow we address their questions:\n\n- **Question 1:** \n> Is the conclusion proved to be universal (for non-linear complex neural networks)?  \n\n  **Response:** Our rigorous analysis directly applies to linear regression models. Empirically, we verified our analysis\u2019 validity on linear models and showed evidence that our findings can explain some behaviors observed in MLPs trained on continual permutation benchmarks.  \n  Extending our analysis to similar continual settings in more complex networks should be very interesting, but will probably require more intricate analytical tools.  \n  Existing theoretical results in CL mostly deal with simple models, and there are still very few analytical results on more complex models. We believe that in order to derive results for more complex models, the community must first thoroughly understand the simpler ones (e.g., linear models). Now that we\u2019ve established an interplay between task similarity and overparameterization in linear models, the community can concentrate on extending it to more complex models (e.g., MLPs or CNNs).  \n  We now added this as a future direction in the \u201cFuture work\u201d section of the revised manuscript.\n\n- **Questions 2&5:** \n> Pixel shuffling is the only current knob for dissimilarity in the experiments, which does not match real world scenarios. Can the same experimental conclusions be reached for other complex datasets?   \n\n  **Response:** We hypothesize that the true effect of task similarity comes from similarity in the NTK feature regime as presented in Appendix A. Pixel shuffling is just one proxy for which to generate the desired effect in the NTK feature regime but the relationship between experimental setup and the NTK features is a complex one. We leave the full uncovering of this effect for future work (we mention this in the \u201cFuture work\u201d section of the revised manuscript).\n\n-  **Question 3:** \n> Is overparameterization equivalent to learning capacity?\n\n   **Response:** Overparameterization is related to learning capacity but is not equivalent. Any model that surpasses the complexity of the interpolation threshold has an equal capacity to fully fit to the training set. However, more overparameterized models enjoy the beneficial properties of smoother optimization landscapes and sometimes more favorable generalization properties.\n\n- **Question 4:** \n> Can we explain Pseudoinverse Properties and Operator Norm Properties in detail?\n\n  **Response:** The pseudoinverse property that we used in Equation (4) is that for any matrix $\\mathbf{X}$ and an orthogonal matrix $\\mathbf{O}$, it holds that $\\mathbf{X}=\\mathbf{X}\\mathbf{X}^{+}\\mathbf{X}$ and $(\\mathbf{X}\\mathbf{O})^{+}=\\mathbf{O}^{+}\\mathbf{X}^{+}=\\mathbf{O}^{\\top}\\mathbf{X}^{+}$. The operator norm property refers to $\\Vert\\mathbf{X}\\mathbf{v}\\Vert\\le\\Vert\\mathbf{X}\\Vert\\Vert\\mathbf{v}\\Vert$.  \n   To improve readability, we now added these explicitly within Equation (4) and underneath it.\n\n---\n\nWe again thank the reviewer for the feedback.  \nIf we have adequately addressed the reviewer's concerns, we kindly ask the reviewer to continue supporting our submission.  \nIf there are any remaining concerns, please let us know."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2816/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700235603050,
                "cdate": 1700235603050,
                "tmdate": 1700235689171,
                "mdate": 1700235689171,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "whswrCxR5N",
            "forum": "u3dHl287oB",
            "replyto": "u3dHl287oB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2816/Reviewer_ckB1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2816/Reviewer_ckB1"
            ],
            "content": {
                "summary": {
                    "value": "The paper considers the continual learning problem in the case of two linear regression tasks. The first task is arbitrary, while the second is assumed to be a random orthogonal transformation of the first one.\n\nThe main result of the paper is a precise expression of the normalized, expected forgetting (Theorem 3). The expression is related to the level of task similarity and overparameterization.\n\nThe paper proceeds to specialize the main theorem to the highly overparameterized regime and at the interpolation threshold, visualizing the amount of forgetting on synthetic data or the MNIST dataset."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper has several strengths:\n- First, while the proof idea is straightforward (e.g., as shown in the proof sketch), it involves lengthy algebraic manipulations that the paper manages to excel in; this is commendable.\n- Related to the first strength, even though the proof can be lengthy, the main paper is clearly written and easy to understand.\n- The experiments numerically verify the correctness of the theorem and provide interesting insights into catastrophic forgetting. While some of the observations have been made in prior works, the paper suitably discussed that, and moreover provided different results in the highly overparameterized regime."
                },
                "weaknesses": {
                    "value": "While I tend to vote for acceptance, I do have several questions or comments:\n\n- At first glance it seems a little bit weird to have the same $y$ for two different tasks. I think it might be motivated by the case of permuted MNIST where permutations do not change the label. It would be great if the authors could comment on that in their revision.\n- Certainly there is some gap between the theory and MNIST experiments. The authors are encouraged to discuss that gap. For example, the training loss in the theory vs test error in the experiments. The least-squares loss in the theory vs classification loss in the experiments (I suppose).\n- Is there any deeper connection between the bound in the highly overparameterized regime, $\\alpha^2(1-\\alpha)^2$, and the corresponding bound in Lemma 9 of Evron et al. (2022) for the case $k=2$?\n- While the main theorem is accomplished by algebraic calculations, I wonder whether there is a geometric proof that would achieve the same. For example, the two steps can be thought of as two projections (onto certain subspaces), and the forgetting can be bounded deterministically by some quantity related to the principal angles. With a random orthogonal transformation, the principal angles between the two subspaces would be in some sense random. Would it be easier if one analyzes the randomness of the principal angles, and could the algebraic proofs be replaced by geometric reasoning?"
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2816/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2816/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2816/Reviewer_ckB1"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2816/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698869570355,
            "cdate": 1698869570355,
            "tmdate": 1700572240991,
            "mdate": 1700572240991,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BwWpbC4jjA",
                "forum": "u3dHl287oB",
                "replyto": "whswrCxR5N",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2816/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2816/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their valuable feedback, and we are glad that they appreciate our work.  \nBelow we address the reviewer's concerns\n\n---\n- **Question/Weakness 1:**\n> It seems weird to have the same y for two different tasks.\n\n  **Response:** This is due to our analysis being analogous to the data permutation task benchmarks of continual learning. These tasks are especially amenable to analysis as they are each equally difficult for an MLP to solve. Thus we can fairly compare their accuracies and forgetting between tasks.  \n  More generally, many practical continual learning scenarios are subject to domain shifts (e.g., sees another \u201cregion\u201d in $\\mathcal{X}$), without changing the output space $\\mathcal{Y}$.\n\n---\n- **Question/Weakness 2:**\n> There is some gap between the theory and MNIST experiments (training loss vs. test error; regression vs. classification).\n\n  **Response:** We view the present work as an initial result in understanding the relationship between task similarity and forgetting. Nonetheless there are still clear analogies between the analysis and experiments of our work. The permutation task setting is a variant of the DOTS model where random permutation matrices are used instead of random orthogonal matrices. In continual learning practice, training error and test error are strongly correlated: when forgetting is observed in the test error then it is also observed in the training error. Additionally, studying training error allows us to loosen the data assumptions of prior work (Goldfarb, Lin) to give better insight into the problem\u2019s worst-case performance. Regression and classification problems are also closely related. One can turn the MNIST experiments into a one-hot regression problem and expect to observe the same effects as in the classification problem.\n\n---\n- **Question 3:** Is there any deeper connection between the bound in the highly overparameterized regime, $\\alpha^2 (1-\\alpha)^2$, and the corresponding bound in Lemma 9 of Evron et al. (2022) for the case ?\n\n  **Response:** There is indeed a deeper connection! Given no task-repetitions ($k=2$), the result from Evron et al. (2022) yields a forgetting upper bound of $\\frac{1}{2} \\max_{i} \\{ \\cos^2 (\\theta_i) (1-\\cos^2 (\\theta_i)) \\}$, where $\\theta_i$ are the nonzero principal angles between the solution subspaces of the two tasks (here, they are equivalent to the principal angles between the data itself; you can see Claim 19 in their paper).  \n  This expression is of course very similar to our overparameterized results, i.e., $\\alpha^{2}(1-\\alpha)^{2}=\\left(\\frac{m}{p}\\right)^{2}\\left(1-\\frac{m}{p}\\right)^{2}$.  \n\n  Trying to explain this very briefly and intuitively: When $m=0$, our random transformation $\\textbf{O}$ (applied to $\\textbf{X}_1$ to create $\\textbf{X}_2$) is a deterministic identity operator. This, of course, yields principal angles of $0$ between tasks, incurring no forgetting.  \n  When $m=p$, the random transformation becomes completely random, and since we assume high overparameterization, it means that the two subspaces are going to be almost orthogonal w.h.p. (i.e., have angles of $90^{\\circ}$, again incurring no forgetting.  \n  To understand $m=\\frac{p}{2}$, it is perhaps easy to imagine two random lines in $2$ dimensions. In this case it is known that the expected angle between them should be $\\approx 45^{\\circ}$, which corresponds to the highest forgetting in Evron et al. (2022).\n\n  Following this review, we have now incorporated a similar discussion (on a slightly different aspect) in our \u201cGeometric interpretation and Comparison to Evron et al. (2022)\u201d paragraph in the discussion of the revised manuscript. We believe that it helps see deeper connections like the one that the reviewer noticed.\n\n---\n- **Question 4:** \n> Is there a geometric proof that achieves the same result as our algebraic calculations (randomness of principle angles)?\n\n  **Response:** This is a good question. Following the discussion here and with reviewer [TGNt](https://openreview.net/forum?id=u3dHl287oB&noteId=TRtKnkPHKr), and the discussion we\u2019ve added to the manuscript, it is apparent that geometric reasoning is indeed partially possible here. Specifically, analyzing worst-case expected forgetting should be related to the largest principal angle between 2 random subspaces (see, e.g., \u201cOn the largest principal angle between random subspaces\u201d, 2006).\n  However, there should be important differences, requiring different proof techniques. Specifically, our analysis allows for the entire range of $m$ (number of directions/pixels that we rotate), while the aforementioned other paper requires the two subspaces to be completely random. Thus, it seems like they can only help analyze the $m=p$ case.\n\n---\nWe again thank the reviewer for the feedback.  \nIf we have adequately addressed the reviewer's concerns, we kindly ask the reviewer to continue supporting our submission.  \nIf there are any remaining concerns, please let us know."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2816/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700237463797,
                "cdate": 1700237463797,
                "tmdate": 1700237463797,
                "mdate": 1700237463797,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mPxQyJPCbQ",
                "forum": "u3dHl287oB",
                "replyto": "BwWpbC4jjA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2816/Reviewer_ckB1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2816/Reviewer_ckB1"
                ],
                "content": {
                    "title": {
                        "value": "Reply"
                    },
                    "comment": {
                        "value": "Dear Authors, thank you for the rebuttal and your patience in the discussion phase. I apologize for the late reply.\n\nI read your rebuttal and the corresponding modifications in the revised paper. I think my comments have been addressed.\n\nI think the paper makes interesting, non-trivial, and to my knowledge, novel theoretical contributions to continual learning (linear regression case), and I'd like to see the paper get accepted. I increased my confidence from 3 to 5. I intended to increase my score from 6 to 7, while there is no such choice.\n\nAt the same time, I read comments from other reviews and the corresponding rebuttals from the authors.\n\nI do agree with Reviewer SXpA that the proofs can be hard to follow (I didn't read them). For me, this at most precludes higher scores (e.g., 8), but it does not form a ground for rejection.\n\nI understood the paper received divergent scores. I don't entirely agree with some of them. I, therefore, increased my score from 6 (or one could understand it as 7) to 8 to counteract them."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2816/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700573188370,
                "cdate": 1700573188370,
                "tmdate": 1700573188370,
                "mdate": 1700573188370,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FXjfnXUu6f",
            "forum": "u3dHl287oB",
            "replyto": "u3dHl287oB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2816/Reviewer_TGNt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2816/Reviewer_TGNt"
            ],
            "content": {
                "summary": {
                    "value": "This paper provides a mathematical analysis of how task similarity and overparameterization jointly affect forgetting in continual learning. By proposing a pair of orthogonally transformed datasets, authors define their similarity measurement DOTS $\\alpha$ and the level of overparameterization $\\beta$. Theorem 3 provides the worst-case expected forgetting w.r.t $\\alpha$ and $\\beta$. Furthermore, synthetic and empirical experiments provide more support for the proposed analysis."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Writing is clear and easy to follow.\n2. Their novel result provides a new perspective for forgetting analysis in continual learning.\n3. The theoretical result is solid and consistent with empirical results."
                },
                "weaknesses": {
                    "value": "1. The dataset assumption is strong, which only focuses on 2 datasets, and can be converted by orthogonal transformation. \n2. It is unclear how DOTS can compare with the notion of similarity in related works."
                },
                "questions": {
                    "value": "1. Is the theory able to provide worst-case forgetting analysis with more general dataset assumption, or T>2 datasets?\n2. It is still unclear that how DOTS relates to other notions of similarity, e.g. the principle angles in Evron et al. (2022). Is that possible to plot a figure that shows the relation between DOTS and $\\theta$?\n3. In Evron et al.(2022), Figure 3 shows the worst-case forgetting on T=2 tasks, where p=d-1(rank(X1)=rank(X2)=d-1), which in this paper, should corresponds to $\\beta=1-\\frac{d-1}{d}\\approx 0$, which should fall into the very less overparameterization regime. However, there results still show the descent with a large angle. Can authors provide some explanation? (And this is also one of the reasons I hope to know how DOTS relates to $\\theta$)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2816/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2816/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2816/Reviewer_TGNt"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2816/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699001610790,
            "cdate": 1699001610790,
            "tmdate": 1699636224635,
            "mdate": 1699636224635,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TRtKnkPHKr",
                "forum": "u3dHl287oB",
                "replyto": "FXjfnXUu6f",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2816/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2816/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their valuable feedback, and we are glad that they found our paper novel and theoretically sound.\n\nBelow we address some of the reviewer's concerns,\n\n- **Weakness:** \n> It is unclear how DOTS can compare with the notion of similarity in related works.\n\n  **Response:** In our linear regression setting, the existing similarity notion most comparable to our DOTS should be the principal angles between the solution subspaces of the two data matrices ($\\mathbf{X}_1,\\mathbf{X}_2$)}, which were used to quantify task similarity in Doan et al. (2021) and Evron et al. (2022). We elaborate on this further here below, and in the discussion in our revised manuscript.\n\n---\n\n- **Question 1:** \n> Is the theory able to provide worst-case forgetting analysis with more general dataset assumption, or T>2 datasets?\n\n  **Response:** These are important questions and we added the following discussion to a new section on \u201cLimitations and Future work\u201d in the revised manuscript.\n  Our analysis has primarily examined settings with $T=2$ tasks. Extending these analytical results to $T\\ge 3$ tasks poses an immediate challenge. The complexity of our analysis, which already required intricate techniques and proofs, suggests that tackling this extension may be considerably difficult. Moreover, the convergence analysis presented in a previous paper (Evron et al. 2022) for learning $T\\ge 3$ tasks cyclically has proven to be notably more challenging than that for $T=2$ tasks, and was further improved in a follow-up paper (Swartworth et al. 2023).\n---\n\n- **Questions 2&3:** \n> How does DOTS relate to other notions of similarity in prior work? Is there a contradiction to Figure 3 in Evron et al. (2022)?\n\n  **Response:** We thank the reviewer for this interesting question!  \n  Evron et al. (2022) showed analytically that intermediate task similarity (a principle angle of $45^{\\circ}$) is most difficult in two-task linear regression.  \n  Their analysis applies to *any* two arbitrary tasks, and thus seemingly contradicts the behavior observed, e.g., in our Figure 1(b), where maximal dissimilarity is most difficult.  \n  The key to settling this apparent disagreement is the *randomness* of our transformations.  \n  Their analysis focuses on any two *deterministic* tasks, while our second task is given by a *random* transformation of the first, as done in many popular continual learning benchmarks (e.g., permutation and rotation tasks).  \n  \n  For simplicity, instead of the case that the reviewer mentioned, where $d=p-1$, let us focus on $d=1$ (in two tasks these are almost equivalent; see Claim 19 in Evron et al. (2022)).\n  \n  To gain a geometric intuition, consider two tasks of rank $d=1$ ($\\mathbf{x}_1, \\mathbf{x}_2$). Consider also a *maximal* DOTS proxy for task dissimilarity ($\\alpha=\\frac{m}{p}=1$), i.e., $\\mathbf{x}_2 = \\mathbf{O}\\mathbf{x}_1$ is simply a random rotation of $\\mathbf{x}_1$ in $p$ dimensions. It is known that $\\mathbb{E} \\big| \\big\\langle \\frac{\\mathbf{x}_1}{\\Vert{\\mathbf{x}_1}\\Vert}, \\frac{\\mathbf{x}_2}{\\Vert{\\mathbf{x}_2}\\Vert} \\big\\rangle \\big| \\approx \\frac{1}{\\sqrt{p}}$. \n  Near the interpolation threshold, \\eg when $p= 2$ (recall that $d=1$), we get that ${\\mathbb{E} \\big| \\big\\langle \\frac{\\mathbf{x}_1}{\\Vert{\\mathbf{x}_1}\\Vert}, \\frac{\\mathbf{x}_2}{\\Vert{\\mathbf{x}_2}\\Vert} \\big\\rangle \\big| \\approx \\frac{1}{\\sqrt{2}}} \\Longrightarrow {\\mathbb{E}\\angle(\\mathbf{x}_1,\\mathbf{x}_2) \\approx 45^{\\circ}}$, \n  corresponding to the *intermediate* task dissimilarity in Evron et al. (2022), where forgetting is *maximal*.\n  On the other hand, given high overparameterization levels ($p\\to\\infty$), we get that ${\\mathbb{E} \\big| \\big\\langle \\frac{\\mathbf{x}_1}{\\Vert{\\mathbf{x}_1}\\Vert}, \\frac{\\mathbf{x}_2}{\\Vert{\\mathbf{x}_2}\\Vert} \\big\\rangle \\big| \\approx \\frac{1}{\\sqrt{p}} \\to 0} \\Longrightarrow {\\mathbb{E} \\angle(\\mathbf{x}_1,\\mathbf{x}_2)\\to 90^{\\circ}}$, corresponding to the *maximal* task dissimilarity in Evron et al. (2022), where forgetting is *minimal*.\n\n\n  We originally compared our results to Evron et al. (2022) in the discussion section. Following the reviewer\u2019s question, we included the discussion above in the revised manuscript to further elaborate on these aspects.\n\n\n---\nWe again thank the reviewer for the feedback.  \nIf we have adequately addressed the reviewer's concerns, we kindly ask the reviewer to continue supporting our submission.  \nIf there are any remaining concerns, please let us know."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2816/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700236288261,
                "cdate": 1700236288261,
                "tmdate": 1700236441454,
                "mdate": 1700236441454,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xWgqHVH2Up",
            "forum": "u3dHl287oB",
            "replyto": "u3dHl287oB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2816/Reviewer_yV41"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2816/Reviewer_yV41"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to reveal the connection between the parametrization regime and forgetting. The authors start by analyzing the forgetting in a linear regression model and define their definition of their specific distribution shift for the second task. Moreover, the upper bound of the forgetting (derived as the loss of task 1 after learning task 2), is calculated and later used for different parameterization scenarios. The authors conclude that in an overparametrized regime, forgetting decreases as the task dissimilarity increases, however, in an underparametrized scenario, the trend is reversed (more forgetting for more dissimilar tasks). Finally, they evaluated the soundness of their derivations in a custom version of the permuted-NIST tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "I think the presentation in the paper is concise and to the point. The mathematical derivations in the given context are sound and clear. Maybe it would be better to show some of the derivations in eq 4 in the appendix but overall it is fine. \n\nThe results within the linear regression and simple nist-type experiments look interesting and worth pursuing in future works."
                },
                "weaknesses": {
                    "value": "**Limited scope:** The main issue that I see in the paper is the limited scope of the provided analysis. I am fine with simple experiments in a theoretical paper but the upper bound of forgetting is very specific to the linear regression task. \n\n**Overparametrization proxy:** The second issue is tightly related to the above, in theorem 3,  the proxy for overparametrization is defined as $1 - \\frac{d}{p}$, where $d$ is the rank of the input data and $p$ is the data dimensionality. It makes sense in linear regression since the number of parameters is the same as the data dimension, i.e., $p$. This is true to some extent in the MLP layers (at least they are correlated). However, none of the derivations hold in the case of the convolutional layers where there are shared parameters and $p$ cannot be a surrogate for the number of parameters. I encourage the authors to evaluate their upper bound in CNNs. \n\n**Task similarity definition:** Also, the definition of task similarity is very limited to the rotation of a subset of the data dimensions. All of the derivations are based on this initial assumption. I believe a good theoretical paper on this subject should expand this definition so that more real-world cases can be included in the analysis. My comment is the same for the permuted-NIST scenario. A simple permutation of a subset of pixels is not enough to back the main message in this more challenging scenario."
                },
                "questions": {
                    "value": "My question goes back to the previous comments:\n\n**Q1:** Is the overparametrization proxy enough to provide a similar analysis in the CNN case? i.e., can we just substitute the $p$ with the number of parameters in a CNN layer?\n\n**Q2:** Have the authors tried to evaluate their upper bound in more challenging scenarios?\n\n**Q3:** Have the authors tried to test their upper bounds when the definition of the task similarity is different? i.e., more subtle semantic distribution shifts in the data. e.g. CIFAR-100 Superclass."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2816/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2816/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2816/Reviewer_yV41"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2816/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699208074517,
            "cdate": 1699208074517,
            "tmdate": 1699636224555,
            "mdate": 1699636224555,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SIlPhxSdm2",
                "forum": "u3dHl287oB",
                "replyto": "xWgqHVH2Up",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2816/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2816/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their comments on our paper, and we are glad that they found our results interesting.\n\nUnfortunately, there seem to be several misunderstandings that perhaps interfered with the reviewer\u2019s assessment of our paper. For instance, the reviewer wrote that *\u201cin an overparametrized regime, forgetting decreases as the task dissimilarity increases, however, in an underparametrized scenario, the trend is reversed\u201d*. However,\n\n- We only analyzed overparameterized models (from the interpolation threshold to a highly overparameterized regime), as seen from Theorem 3 (the rank $d$ is upper bounded by the dimensionality $p$). Underparameterized regimes are practically less common today, and cannot be very interesting under our realizability assumption (since there would only be a unique solution and there would be no place to discuss the bias incurred by continual learning).\n\n- Even in the overparameterized regime, we did not conclude that forgetting decreases as task dissimilarity increases, but rather showed a nuanced non-monotonic behavior (kindly see Section 2.3.1 on the Extremal Cases and Figures 2 and 3).\n\n\nImportantly, we believe that our paper serves as an analytical counterexample to the findings of several previous empirical papers, which concluded that overparameterization always mitigates forgetting.\nSpecifically, we use our model to demonstrate that, and even if we take overparameterization to the extreme, we can still have a considerable amount of forgetting (and it does not necessarily decay to zero), e.g. when $\\alpha=1-\\frac{m}{p} = 0.5$. We believe that it is more convincing that we were able to find this counterexample even in the simplest model of linear regression and in a simple-yet-standard data model.  This suggests that more complex models probably also have a nuanced behavior with regard to overparameterization, though not necessarily exactly the same behavior. \n\n\nTo enhance clarity, we revised the manuscript and incorporated a brief discussion in Section 2.3.2, just before Figure 3. Additionally, we explicitly highlighted this aspect in the contribution list within the introduction.\n\nThe reviewer also asked (Q2,Q3) whether we tried to evaluate our upper bound *\u201cin more challenging scenarios\u201d* (e.g., CNNs) or with different notions of task similarity. The answer is that we did not, but neither did we claim that our analysis simply extends to such cases. For example, permutation benchmarks like the one we aim to characterize, are not very suitable for CNNs, since the permutations significantly interfere with the spatial assumptions convolutional models implicitly make (shift-equivariance of representations and shift-invariance of the classification).  \nAgain, our aim is that our analysis can be used as a counterexample to the practical common belief that overparameterization always mitigates forgetting and as a demonstration of a more nuanced interplay between overparameterization and task similarity. More generally, when doing theoretical research, we believe that one must start from the simplest models as we did, before addressing more complex scenarios as the reviewer rightly suggested.\n\nFollowing this discussion, we now include these suggestions in the \"Future work\" section at the end of our revised manuscript.\n\n---\n\nIf we have adequately addressed the reviewer's concerns, we kindly ask the reviewer to consider raising their score. If there are any remaining concerns, please let us know."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2816/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700239134747,
                "cdate": 1700239134747,
                "tmdate": 1700239134747,
                "mdate": 1700239134747,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gJdjrO0bxZ",
                "forum": "u3dHl287oB",
                "replyto": "SIlPhxSdm2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2816/Reviewer_yV41"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2816/Reviewer_yV41"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors"
                    },
                    "comment": {
                        "value": "I have read the authors' responses and appreciate their efforts in addressing my concerns and clarifying the scope and claims of their papers.\n\nWhile I find the findings interesting, they hinge on a particularly narrow definition of task similarity. This definition isn't widely accepted or realistic. I acknowledge that the authors present a counterexample to previous findings, but it's a specific instance with limited relevance in real-world scenarios. Previous research does not suggest that overparametrization always mitigates forgetting; rather, it appears to be a common occurrence in more realistic situations. The paper could gain significantly if it included some theoretical insights related to CNNs or alternative definitions of task similarity. However, as it stands, the paper\u2019s focus is too narrow for an ICLR audience.\n\nFor these reasons, I will maintain my initial evaluation score."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2816/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700684034127,
                "cdate": 1700684034127,
                "tmdate": 1700684034127,
                "mdate": 1700684034127,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vSLgwjYBTP",
            "forum": "u3dHl287oB",
            "replyto": "u3dHl287oB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2816/Reviewer_vLwn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2816/Reviewer_vLwn"
            ],
            "content": {
                "summary": {
                    "value": "The authors present an analytical upper bound for the expected amount of catastrophic forgetting during linear regression with gradient descent as a function of overparameterisation and overlap between tasks. In particular, they show that there exist two phenomenologically different learning regimes: In the overparameterised regime (i.e. $rank({\\bf X}) / rank({\\bf w})$ small) intermediate similarity between tasks leads to largest amount of forgetting (inverted U-shape) and monotonic behaviour is observed near the interpolation threshold ($rank({\\bf X}) / rank({\\bf w}) \\approx 1$). To this end, the authors make the following assumptions: (1) The target values for both tasks $({\\bf y})$ are identical, (2) both tasks are realisable (i.e. a zero-loss solution exists) (3) the second task is an orthogonal projection of the first task (c.f. permutation) and (4) the first task has successfully converged to the global, minimum-norm solution of task one before training on task two. By changing the subspace which the orthogonal operator transforms, the overlap between the tasks can be exactly controlled and its effect analytically studied.\n\n\n**--------------------------------------**\n\n**Unfortunately, I can not add official comments (anymore), thus I will add my comments to the author's comments here**\n\n**Reply 1**\nWeaknesses 1&2 and Questions 1,2,3: Thank you for the clarifications. As it is not obvious from section 2.1 why equation (3) is the closest point on the solution manifold when starting to train from the convergence point of task 1, I would like to suggest to include this derivation in the appendix.\n\nWeakness 3: Thank you for the clarification.\n\nWeakness 6: As there is no direct mapping from permuting MNIST to DOTS, I have mild concerns that the observed phenomena could be explained away by the fact that most information is in the center of the image and thus increasing the permuted area from the center to the outside may result in a non-linear transformation of the dimensional of the data-manifold. If the effect is non-linear, comparisons to phenomena observed when manipulating DOTS would be erroneous. However, I appreciate that the authors try to test their hypotheses on more complex data.\n\nWeakness 4: The minimum-norm solutions studied by the authors are part of the \"rich\" learning regime. The lazy regime does exist in (multi-layer) linear networks. For example, in the regression model, weight values that lay in the null-space of the training data of the first task could be unequal zero. For example, when initialising the weight matrix from large random values, GD would still converge to a gloabl optimum, however, the convergence points would be unequal to equation (2) and (3). Thus, I would argue that the presented analytical results only apply to the rich learning regime.\n\nFurther, in an over-parametrized linear two-layer network, the first layer can be initialised, large and arbitrarily (and in theory even kept fixed). GD would still converge to a global optimum, however, as a consequence, the learned representations in the hidden layer would be \"lazy\". Again, I don't think that the analytical result apply to this regime and thus simulations that compare these two regimes (i.e. training from (very) small initial weights and training from large initial weights) would be useful. \n\nSee also question: For Figures 6 & 7, how are the neural networks initialized? Do they operate in the rich or lazy regime?\n\n\n**Reply 2**\nI would like to thank the authors for addressing my questions.\n\nWeakness 5: Thank you for adding these additional simulations.\n\nQuestion: Can you give an intuitive explanation for why the expected forgetting is non-asymptotic for overparameterized regime and near interpolation threshold\n\nSorry, that question was not very well formulated. What I tried to ask is: Do you have an intuitive explanation for why in the overparametrized regime, the expected forgetting for increasing task dissimilarity if falling off, whereas near the interpolation threshold, it keeps increasing?\n\n**The following points remain unaddressed:**\n\n- **Weakness 7: Simulation details and / or code are not provided which makes interpretation of simulation results difficult and reproduction impossible.**\n- **Weakness 10: The introduction to continual learning / catastrophic forgetting is, in my opinion, not referenced well enough**\n\n\nAs a result of the changes made by the authors I have increased my rating to a 6.\n\n**--------------------------------------**"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors tackle an important research question: What are the underlying mechanisms that govern catastrophic forgetting during the continuos acquisition of knowledge using gradient descent. Their analytical result fully describes phenomenologically distinct operating regimes during the continuous optimisation of a linear regression model using gradient descent. The authors validate their analytical result using simulation studies and test whether their results generalise to more complicated two-layer networks. Notation is consistent throughout the paper and explanations for variables, equations and derivations are provided. The paper and plots are generally well structured and accessible."
                },
                "weaknesses": {
                    "value": "1. The model (i.e. single linear projection) and algorithm (gradient descent, full-batch gradient descent?!) are not stated in the introduction. Information about the regime to which the analytical result applies is scattered throughout the paper, which makes it difficult to contextualise the results.\n2. It would be really helpful if all assumptions of the analytical result would be stated explicitly and collected within one section of the paper. For example, the ${\\bf 0}$ initialisation and resulting convergence to the global, minimum-norm solution of the first task is hidden in Scheme 1 but crucial to understand the extent of the analytical result.\n3. I think the paper could benefit from making it more explicit that the analysis focuses on the dimensionality and overlap with respect to the manifold of the data distribution, which is often called overparametereised, but should not be confused with the overparameterisation deep networks (i.e. wide hidden layers).\n(4. Resulting from 3., the model can not be used to make predictions about the underparameterised regime, e.g. bottlenecked networks.)\n5. I think the paper should state explicitly that the analysis is limited to the rich learning regime. It does not provide any insights into the lazy learning regime. Comparisons and references from the paper\u2019s \u201coverparameterised regime\u201d to e.g. the NTK regime are confusing and incorrect.\n6. Simulation results in Figure 2. are limited to $d = 2$. There are no simulation results to validate the analytical result for edge-cases like p = d and large(r) d etc.\n7. I am not entirely sure if the author\u2019s version of permuted MNIST is suitable to study changes in task similarity that are comparable to DOTS, as vanilla MNIST data is on a very low dimensional data manifold and large parts of the information is centred on the middle of the picture. I think I would prefer a simulation study that uses artificial data as in Figure 4., with well controlled DOTS in two layer networks.\n8. Simulation details (initialisation scheme, learning rates etc.) and / or code are not provided.\n\nMinor:\n9. The notation in equations (2) and (3) is confusing. There are too many equal signs\n10. The introduction to continual learning / catastrophic forgetting is, in my opinion, not referenced well enough\n11. Using X_1 and X interchangeably is confusing and seems unnecessary"
                },
                "questions": {
                    "value": "- To what optimisation algorithm does the analytical result exactly apply?\n- Could you please hint me at the Theorem in Gunasekar et al. (2018) from which equation (3) is derived?\n- What assumptions are made on the size of batches and the size of the learning rate?\n- Why does Theorem 3 have the assumption of p >= 5?\n- Why can you assume that a randomly sampled data matrix X is identical to the worst case in Figure 2?\n- The smallest expected forgetting in the interpolation regime (except for values close to 0 DOTS) is larger than the largest expected forgetting in the overparametereised regime. Why is that the case and how is it relevant to the analysis?\n- Is there a benefit of using an informal illustration instead of slices from figure 3 in figure 1? The inverted U-shape seems to be slightly exaggerated as expected forgetting for large DOTS don\u2019t fall off nearly as strongly as suggested by the illustration.\n- Can you give an intuitive explanation of why the expected forgetting are (non-)asymptotic for overparameterised regime and near the interpolation threshold?\n- Do the authors think that their results do generalise to learning more than two tasks? I think that would clearly contradict some of the assumptions and thus maybe should be listed as a limitation of the work?\n- Is it maybe possible to use a log-scaled axis in Figure 3 instead of using subplots to represent the results?\n- Is it possible that the x-axis of Figure 3, bottom right is labelled incorrectly? Values from 0 to 0.1 seem to be missing\n- For figures 6. and 7., how are neural networks initialised? Do they operate in the rich or lazy regime?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2816/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2816/Reviewer_vLwn",
                        "ICLR.cc/2024/Conference/Submission2816/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 6,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2816/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699562432511,
            "cdate": 1699562432511,
            "tmdate": 1700752111629,
            "mdate": 1700752111629,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "eJpcz7hgrM",
                "forum": "u3dHl287oB",
                "replyto": "vSLgwjYBTP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2816/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2816/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part 1 of the Author Response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the thorough and helpful review. \nBelow we address the weaknesses the reviewer has pointed out and also answer all the questions.\nWe believe that the changes made following the discussion below, helped to improve our paper significantly.\n\n\n\n- **Weaknesses 1&2 and Questions 1,2,3:**\n> What is the exact optimization scheme to which our results apply (algorithm, batch size, learning rate)? The model and algorithm are not stated in the introduction. Which theorem from Gunasekar et al. is used.\n\n  **Response:** We apologize about the lack of clarity on the aspects mentioned by the reviewer. Below we address these questions and specify how the revised manuscript clarifies them.  \n\n  We study the simplest continual learning procedure - starting from a zero initialization $\\textbf{0}_p$ and iteratively minimizing the current task\u2019s loss using (S)GD initialized with the previous task\u2019s solution. In each iteration, we assume that learning \u201cconverges\u201d to the limit solution, obtained by (S)GD with *any* batch size (as long as the learning rate is small enough; see Section 2.1 in Gunasekar et al. (2018)). This is a similar procedure to the one studied in several analytical papers (e.g., Evron et al. (2022); Goldfarb and Hand (2023)).\n\n  As can be seen in our revised manuscript, we now (1) specifically mention that we study an (S)GD scheme in the introduction; (2) changed the title of Section 2.2 to \u201cThe Analyzed Learning Scheme and its Learning Dynamics\u201d; (3) specifically write before Scheme 1 that this is the scheme that we analyze; (4) added a footnote to the analytical iterates, explaining that any batch size can be use as long as the learning rate is small enough; (5) highlight right after the scheme that we do not actually compute pseudoinverses; (6) and finally, specifically refer to Scheme 1 in our main Theorem 3.\n\n\n---\n\n- **Weakness 3:** The analysis focuses on the dimensionality and overlap with respect to the manifold of the data distribution, which should not be confused with the overparameterization of deep networks.\n\n  **Response:** We agree with the reviewer and we added this to the \u201cLimitations and Future work\u201d section in our revised manuscript.\n\n---\n\n- **Weakness 6:** \n> Unsure if permuted MNIST is suitable to study changes in task similarity that are comparable to DOTS. Vanilla MNIST is on a very low dimensional manifold. I would prefer a simulation study that uses artificial data.\n\n  **Response:** The permuted MNIST experiments are intended to illustrate the ideas of the analysis in a more realistic image classification scenario. Note that the permuted image setting is an analogous version of the DOTS simulation where random permutation matrices are used instead of random orthogonal matrices. We recognize that MNIST images lie on a low dimensional manifold and this notion is what motivated our decision in the analysis to have data of low rank d; in this case, low effective dimensionality is a similarity between MNIST and the simulated data.\n\n---\n\n- **Weakness 4:** The paper should state explicitly that the analysis is limited to the rich learning scheme. It does not provide any insights into the lazy learning regime.\n\n  **Response:** We see two possible interpretations of this statement, and we are slightly confused by both.  \n  In case the reviewer was referring to our theoretical analysis, we note that we work in the setting of linear regression, in which there is no rich/lazy regime distinction. As far as we are aware, the lazy vs rich distinction applies only to non-linear parameterizations, where the initialization scale *qualitatively* affects the final solution (e.g., minimizing the L2 norm in \"lazy\", vs L1 norm in \"rich\", in the squared linear regression model of Woodworth et al. (COLT 2020)). In contrast, as can be deduced from our Eq. 3, in linear regression, changing the initialization just shifts the final solution location (along a direction orthogonal to the data manifold) \u2014 which is not a qualitative change.   \n  In case the reviewer meant to the application of our linear regression results to neural networks, then, as far as we are aware, the only formal way to prove that linear regression results directly apply to neural networks is to be in a regime where the neural network is approximately linear, such as the NTK regime. Therefore, we only claim our results can be directly applied to neural networks in this case. Perhaps some of our conclusions also apply to the rich regime, but we do not see how this could be theoretically proven."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2816/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700238004069,
                "cdate": 1700238004069,
                "tmdate": 1700238004069,
                "mdate": 1700238004069,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]