[
    {
        "title": "L2MAC: Large Language Model Automatic Computer for Unbounded Code Generation"
    },
    {
        "review": {
            "id": "mNALhmROkR",
            "forum": "EhrzQwsV4K",
            "replyto": "EhrzQwsV4K",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8334/Reviewer_Sz28"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8334/Reviewer_Sz28"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents L2MAC, a framework for using large language models (LLMs) as automatic computers for long and consistent code generation. The framework consists of an LLM, an external memory (which stores both instructions and data),  and a control unit (which manages the interaction between the LLM and the external memory). The CU enables the LLM to execute a prompt program that contains a list of instructions to solve a user-given task. The CU also provides the LLM with precise read and write operations to access and update the memory, as well as error checking and correction mechanisms to ensure the quality and coherence of the generated code. The paper demonstrates the effectiveness of L2MAC by implementing Code-L2MAC, a practical instantiation of the framework for code generation tasks. Code-L2MAC can generate large code bases for system design tasks that require multiple components and features, outperforming existing SOTA methods such as GPT4 and AutoGPT."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "+ The paper presents a novel approach with the introduction of the L2MAC framework, which augments LLMs by integrating memory and control mechanisms. This innovation stands as the first practical LLM-based stored-program computer.\n+ The authors have instantiated the LLM SPC framework as Code-L2MAC, specifically tailored for intricate tasks like long code generation. The proposed method exhibits superior performance compared to state-of-the-art techniques.\n+ The introduced benchmark and evaluation metrics for long code generation tasks are valuable for further research in this field."
                },
                "weaknesses": {
                    "value": "+ In the manuscript, there appears to be an inconsistency in the usage of the terms \"L2MAC\" and \"Code-L2MAC.\" The authors aim to differentiate between the overarching framework, denoted as L2MAC (which stands for LLM-based SPC), and its specific instantiation, referred to as Code-L2MAC, designed for long code generation tasks. Notably, the title of the paper erroneously employs \"L2MAC\" when \"Code-L2MAC\" would be more appropriate for the context of long code generation. Similar discrepancies are observed in the abstract, the first summarized contribution, Figure 1, and its accompanying text, among other sections. It is recommended that the authors undertake a meticulous revision to clearly delineate between these two terms. Additionally, while the L2MAC framework's primary focus is on the long code generation task, its potential applicability to a wider array of experiments should not be overlooked. To underscore the framework's versatility and generalizability, it would be advantageous for the authors to incorporate additional tasks.\n+ Regarding the comparative analysis, the manuscript omits some pivotal baselines, notably the Reflecting LLMs. While the related works section enumerates three methodologies\u2014Single LLMs, Reflecting LLMs, and Autonomous Agent LLMs with memory\u2014the results presented in Table 2 only encompass those of GPT-4 and AutoGPT. This omission should be addressed to provide a more comprehensive evaluation."
                },
                "questions": {
                    "value": "1. In Section 3.3.1, the authors mention that if none of the introduced errors are found, CU asks the LLM to summarize the generated output in the current context window. The degree of summarization determines whether contextual information will be lost, which is crucial for long context handling. Can you provide a detailed and clearer explanation?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8334/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698681597672,
            "cdate": 1698681597672,
            "tmdate": 1699637036934,
            "mdate": 1699637036934,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZGo2Rp71Ub",
                "forum": "EhrzQwsV4K",
                "replyto": "mNALhmROkR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8334/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8334/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Sz28 [Part 1/3]"
                    },
                    "comment": {
                        "value": "We are delighted you appreciated the novelty of the L2MAC framework and the value of our benchmark. We structure our response as follows:\n\n* (A) Precise usage of L2MAC and Code-L2MAC\n* (B) L2MAC on Further Applications\n* (C) Reflecting LLM Baselines\n* (D) Summarization Between Instructions\n\n---\n\n### **(A) Precise Usage of L2MAC and Code-L2MAC**\n\nWe agree with your comment on the precise use of L2MAC and Code-L2MAC and have meticulously gone through the main text to resolve these and clearly delineate between these two terms.\n\n**UPDATE:** We have meticulously gone through the paper and resolved these terms. We will change the title and the mentions within the abstract to Code-L2MAC where appropriate. At the rebuttal stage, we cannot change the title and the abstract, but we will update them for the camera-ready revision.\n\n---\n\n### **(B) L2MAC on Further Applications**\n\nWe thank you for highlighting the _potential_ of L2MAC in _other applications_! Indeed, the general-purpose property of stored-program computers is fascinating, and the motivation of L2MAC in such a framework suggests it could inherit this property. We _restricted our claims_ to L2MAC's applicability to coding-related tasks to ensure they _stayed within_ what we could _verify empirically_. Still, we agree that it would be valuable to discuss the areas of applications where this framework could be explored in future work. We include these considerations in appendix I.1.\n\nFirst, _coding_ tasks cover a _broad_ set of impactful _applications_, which, combined with our empirical results, suggests that Code-L2MAC (and, by extension, thus, L2MAC) encompasses wide applicability. To strengthen this point, we increased the variety of our benchmark with new tasks, as discussed in the global response **(R2)**.\n\nApart from that, we can consider other domains for this framework. As discussed in section 3.1, a crucial consideration for L2MAC is the acknowledgment that LLMs are imperfect. Thus, the _evaluation tools_ for instantiating L2MAC play an _important role_ in its effectiveness (note that this role could become less and less crucial as LLMs progress). Consequently, additional applications that would render most immediate for the usage of this framework are the ones for which we have effective evaluation tools or where there is a less stringent requirement on the output structure. This set might include _AutoML_, generating _mathematical proofs_ (Li et al., 2020), or writing _long textual documents_; the first two have natural or existing verification tools (Bayer et al., 2022); the latter imposes less strict requirements, where an LLM could also be used as an evaluator.\n\nMaterializing experiments on these tasks is out of the scope of this paper, although each of them constitutes an exciting direction for future work.\n\n**UPDATE:** We have included the above discussion on the potential additional applications of L2MAC in appendix I.1. We have doubled the number of coding tasks in our benchmark to increase the variety of settings; see global response **(R2)**.\n\n---"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8334/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700179265007,
                "cdate": 1700179265007,
                "tmdate": 1700179644411,
                "mdate": 1700179644411,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iiyoJ3Td0A",
                "forum": "EhrzQwsV4K",
                "replyto": "mNALhmROkR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8334/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8334/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Sz28 [Part 2/3]"
                    },
                    "comment": {
                        "value": "### **(C) Reflecting LLM Baselines**\n\nWe agree that comparing Code-L2MAC to reflecting LLM approaches would be valuable to provide a more comprehensive evaluation and to make our experiments consistent with the related work section. Consequently, we have included in all our experiments _three new baselines_ of more recent state-of-the-art reflecting code generation methods: **CodeT** (Chen et al., 2022), **Self-Refine** (Madaan et al., 2023), and **Reflexion** (Shinn et al., 2023). Code-L2MAC significantly outperforms these reflecting baselines throughout all tasks in the benchmark.\n\nAs described in the global response **(R1)**, these three new reflecting baselines are now included in the updated main experimental results table 2, also shown below. Code-L2MAC still fully implements the highest percentage of user-specified feature requirements across all tasks, where its code has minimal syntactical errors and a high number of self-generated unit tests. Therefore, _Code-L2MAC is state-of-the-art_ for these large code generation benchmark tasks.\n\n**Table 2.** (Main experimental results averaged over ten random seeds)\n\n**URL Shortener App**\n| Method | Features % | # Errors | LOC | Tests Passed |\n| ------------ | -------------------- | ------------- | -------------- | ----------------- |\n| | \u2191 | \u2193 | | \u2191 |\n| GPT4 | 53.6\u00b110.5 | 0\u00b10 | 119\u00b121.1 | 2.56\u00b10.95 |\n| CodeT | 52.9\u00b16.74 | 0.05\u00b10.105 | 110\u00b111.8 | 3.6\u00b10.513 |\n| Self-Refine | 47.9\u00b18.53 | 0.05\u00b10.105 | 124\u00b115.7 | 3.65\u00b11.15 |\n| Reflexion | 38.8\u00b16.02 | 0.1\u00b10.209 | 96.2\u00b19.11 | 2.35\u00b10.631 |\n| AutoGPT | 25.3\u00b119.6 | 0\u00b10 | 136\u00b141.9 | 3.3\u00b11.91 |\n| Code-L2MAC | **91.6\u00b18.22** | **0\u00b10** | **330\u00b147.6** | **14\u00b16.71** |\n\n**Online Social Media App**\n| Method | Features % | # Errors | LOC | Tests Passed |\n| ------------ | -------------------- | ------------- | -------------- | ----------------- |\n| | \u2191 | \u2193 | | \u2191 |\n| GPT4 | 19.5\u00b18.28 | 4.09\u00b13.32 | 116\u00b131.5 | 0.818\u00b10.785 |\n| CodeT | 19.5\u00b15.19 | 0.4\u00b10.603 | 106\u00b117.7 | 2.6\u00b11.76 |\n| Self-Refine | 16.4\u00b12.62 | 0.938\u00b10.714 | 110\u00b119.6 | 1.81\u00b10.938 |\n| Reflexion | 15.2\u00b18.05 | 2.53\u00b11.69 | 122\u00b124 | 1.33\u00b12.44 |\n| AutoGPT | 33.3\u00b118 | 0.6\u00b10.369 | 148\u00b135.5 | 3\u00b12.86 |\n| Code-L2MAC | **82.4\u00b114.6** | **0\u00b10** | **395\u00b152.9** | **18.3\u00b16.8** |\n\n**Online Chat App**\n| Method | Features % | # Errors | LOC | Tests Passed |\n| ------------ | -------------------- | ------------- | -------------- | ----------------- |\n| | \u2191 | \u2193 | | \u2191 |\n| GPT4 | 11\u00b12.26 | 0.3\u00b10.346 | 127\u00b124.1 | 1.2\u00b11 |\n| CodeT | 10.5\u00b14.61 | 0\u00b10 | 91.6\u00b125.9 | 3.32\u00b11.57 |\n| Self-Refine | 14.2\u00b14.19 | 0.211\u00b10.304 | 111\u00b113.8 | 1.42\u00b10.927 |\n| Reflexion | 10.2\u00b13.08 | 0\u00b10 | 76\u00b16.88 | 2.85\u00b10.822 |\n| AutoGPT | 23.1\u00b111.8 | 1.85\u00b12.47 | 220\u00b165.8 | 3.08\u00b13.34 |\n| Code-L2MAC | **59.4\u00b125.9** | **0\u00b10** | **374\u00b1123** | **18.8\u00b19.11** |\n\n**UPDATE:** We share the reviewer's opinion about the importance of how Code-L2MAC compares to existing reflecting methods and have included _three new_ reflecting _baselines_ in our experiments in table 2.\n\n---"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8334/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700179410194,
                "cdate": 1700179410194,
                "tmdate": 1700179657735,
                "mdate": 1700179657735,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oIAOZcY2Io",
                "forum": "EhrzQwsV4K",
                "replyto": "mNALhmROkR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8334/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8334/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Sz28 [Part 3/3]"
                    },
                    "comment": {
                        "value": "### **(D) Summarization Between Instructions**\n\nWe agree that the degree of summarization affects whether contextual information will be preserved in the output, and this is, in fact, a shortcoming of the methods that rely on such summarization, as we suggest in the Related Work comparison with Autonomous Agents LLMs: _\"(2) they compress the previous action history, thus usually losing critical information in the process. In code generation tasks, (2) indicates forgetting which files exist and their content\"._\n\nYou rightly point out that in section 3.1.1. we describe how when the LLM signals the current instruction has been completed, the CU calls the evaluator on the current state of the file store, and if _\"none [errors] are found, it [the CU] asks the LLM to summarize the generated output in the current context window.\"_ Where this summary is provided to the LLM along with the next instruction $\\mathcal{I}^{(k+1)}$ to tackle.\n\nHowever, our method does not depend upon this summarization message $M\\_{rs}$ step and can tackle each instruction from scratch. Indeed, all the essential information regarding previous instructions outputs is contained in the file store/external memory and can be accessed on-demand. We empirically verify this by performing an _ablation_ of our method that _removes_ this _summarization message step without affecting the quality of the output code by much_, as shown below in table 8.\n\n**Table 8.**\n| Method | Features % | # Errors | LOC | Tests Passed |\n| ---------------------------------------------------------- | ----------------- | --------- | -------------- | ---------------- |\n| Code-L2MAC (Ablation, without instruction output summarization) | 89.4\u00b19.88 | **0\u00b10** | 274\u00b143.3 | 8.2\u00b13.12 |\n| Code-L2MAC | **91.6\u00b18.22** | **0\u00b10** | **330\u00b147.6** | **14\u00b16.71** |\n\nWe included this summarization message step to steer the LLM to find the correct files faster since we expect more similarity and interdependence between contiguous instructions than between more distant ones.\n\n**UPDATE**: We evaluated an ablation of Code-L2MAC that omits the summarization message step when having completed an instruction and loading a new instruction. This ablation shows it does not significantly affect the code's quality. We incorporated these results in a **(new) appendix N**.\n\n---\n\n\n**References**\n\n- Wenda Li, Lei Yu, Yuhuai Wu, and Lawrence C. Paulson. Isarstep: a benchmark for high-level mathematical reasoning, 2021.\n- Jonas Bayer, Christoph Benzm\u00fcller, Kevin Buzzard, Marco David, Leslie Lamport, Yuri Matiyasevich, Lawrence Paulson, Dierk Schleicher, Benedikt Stock, and Efim Zelmanov. Mathematical proof between generations, 2022.\n- Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu Chen. Codet: Code generation with generated tests. arXiv preprint arXiv:2207.10397, 2022.\n- Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023.\n- Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik R Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8334/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700179530545,
                "cdate": 1700179530545,
                "tmdate": 1700179544386,
                "mdate": 1700179544386,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YTMmJeRGgo",
            "forum": "EhrzQwsV4K",
            "replyto": "EhrzQwsV4K",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8334/Reviewer_4yCT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8334/Reviewer_4yCT"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes L2MAC, a practical LLM-based stored-program automatic computer for long and consistent code generation. The experimental results show L2MAC outperforms GPT-4 and AutoGPT for a variety of tasks, including URL shortening, online microblogging, and online chat applications."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper introduces a practical LLM-based stored-program automatic computer framework-- L2MAC, for long code generation tasks. L2MAC can generate code for a variety of tasks, including URL shortening, online microblogging, and online chat applications, and outperform SOTA works."
                },
                "weaknesses": {
                    "value": "1.\tThe performance of L2MAC is evaluated on URL shortening, online microblogging, and online chat applications. Can this method be used for other applications, such as programming?\n2.\tIs there a limit on the length of the generated code files? \n3.\tHow to ensure the efficiency of generated code files?"
                },
                "questions": {
                    "value": "Please refer to weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8334/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8334/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8334/Reviewer_4yCT"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8334/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698725756927,
            "cdate": 1698725756927,
            "tmdate": 1700388331831,
            "mdate": 1700388331831,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3spslGNS1V",
                "forum": "EhrzQwsV4K",
                "replyto": "YTMmJeRGgo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8334/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8334/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 4yCT [Part 1/2]"
                    },
                    "comment": {
                        "value": "Thank you for your comments and suggestions, which have driven improvements in the paper! We organize our response as follows:\n\n* (A) Additional Tasks\n* (B) Practical Output Limits of Code-L2MAC\n* (C) How do we Ensure the Efficiency of Generated Code Files?\n\n---\n\n### **(A) Additional Tasks**\n\nWe agree with your suggestion; as explained in the global response **(R2)**, we included three _new programming tasks_: a **recipe application**, an **event planner application,** and a **financial tracking application**. The new results are tabulated in table 4 (new appendix J) and also shown below.\n\nCode-L2MAC continues to achieve state-of-the-art results on these large code generation benchmark tasks.\n\n**Table 4.** (Three new task experimental results averaged over ten random seeds)\n\n**Recipe App**\n| Method | Features % | # Errors | LOC | Tests Passed |\n| ------------ | -------------------- | ------------- | -------------- | ----------------- |\n| | \u2191 | \u2193 | | \u2191 |\n| GPT4 | 21.6\u00b12.12 | 0\u00b10 | 107\u00b16.62 | 3.15\u00b10.38 |\n| CodeT | 20.5\u00b14.86 | 0\u00b10 | 96.5\u00b113.8 | 3.05\u00b10.879 |\n| Self-Refine | 26\u00b13.45 | 0.1\u00b10.209 | 149\u00b127.4 | 2\u00b11.97 |\n| Reflexion | 19\u00b13.36 | 0.25\u00b10.299 | 95.9\u00b114.5 | 2.95\u00b10.852 |\n| AutoGPT | 39.2\u00b114.9 | 1.85\u00b11.45 | 106\u00b119.1 | 1.3\u00b12.02 |\n| Code-L2MAC | **82\u00b17.1** | **0\u00b10** | **497\u00b140.7** | **24.6\u00b12.7** |\n\n**Event Planner App**\n| Method | Features % | # Errors | LOC | Tests Passed |\n| ------------ | -------------------- | ------------- | -------------- | ----------------- |\n| | \u2191 | \u2193 | | \u2191 |\n| GPT4 | 9.2\u00b10.853 | 0.025\u00b10.0506 | 74.6\u00b14.12 | 1.75\u00b10.395 |\n| CodeT | 11.2\u00b11.12 | 0.05\u00b10.105 | 75.2\u00b18.77 | 2.45\u00b10.704 |\n| Self-Refine | 14.5\u00b12.94 | 0.15\u00b10.171 | 118\u00b120.1 | 3.9\u00b11.9 |\n| Reflexion | 10\u00b11.5 | 0\u00b10 | 82\u00b110.5 | 3\u00b10.774 |\n| AutoGPT | 35.7\u00b132.9 | 0\u00b10 | 23.9\u00b120.7 | 0\u00b10 |\n| Code-L2MAC | **83\u00b12.96** | **0\u00b10** | **473\u00b139.3** | **25.6\u00b13.04** |\n\n**Financial Tracking App**\n| Method | Features % | # Errors | LOC | Tests Passed |\n| ------------ | -------------------- | ------------- | -------------- | ----------------- |\n| | \u2191 | \u2193 | | \u2191 |\n| GPT4 | 26.2\u00b14.67 | 0.0513\u00b10.104 | 80.5\u00b18.52 | 2.13\u00b10.422 |\n| CodeT | 21.4\u00b13.25 | 0\u00b10 | 65.9\u00b16.93 | 2.25\u00b10.368 |\n| Self-Refine | 23.6\u00b12.45 | 0.25\u00b10.299 | 87.2\u00b18.24 | 0.55\u00b10.514 |\n| Reflexion | 22.5\u00b13.12 | 0.2\u00b10.419 | 86.8\u00b113.7 | 2.7\u00b10.745 |\n| AutoGPT | 32.9\u00b144.2 | 0\u00b10 | 25\u00b115.8 | 0\u00b10 |\n| Code-L2MAC | **62\u00b113.1** | **0\u00b10** | **307\u00b184.5** | **12\u00b14.19** |\n\n**Update:** We now include these new benchmark tasks for all the baselines in a **(new) appendix J** entitled \"Additional Diverse Programming Code Generation Tasks\".\n\n---\n\n### **(B) Practical Output Limits of Code-L2MAC**\n\nThere are two inherent limitations to the length of the generated code files in the current implementation of Code-L2MAC. Both are discussed within the paper (section 4, footnote 7, and section 7), and both can be readily addressed, providing fertile ground for future work. These are:\n\n- _Code files should be smaller than the context window constraint $c$._ Reading a file involves outputting its whole content into the LLM's context, and writing a file implies rewriting it. This means that to avoid falling out of context, the maximum length for a file is the LLM's context window size, but in fact, it is undesirable to have a file of length more than half the LLM's context window size since this will imply that the LLM cannot modify it without falling out of context. This limitation can be overcome by endowing the LLM with the capacity to selectively read and write parts of the file (e.g., function names and headers) in the same spirit as diffs are conducted in git. We discuss this and its implications for the method's efficiency in Future Work (app. I).\n- _All the code file paths are listed in the context window $C^t$._ Therefore, the maximum number of file paths (e.g. ['app.py,' 'test\\_app.py,' \u2026]) listed Code-L2MAC can have in memory is strictly less than the context length. This can be readily solved by allowing the LLM only to list the file paths inside a folder and the possibility to navigate inside a sub-folder. In such a case, the constraint would happen only regarding the degree in the file tree, but we could theoretically have infinite depth in the file store.\n\n**UPDATE:** We incorporate a version of the above discussion into the existing future work appendix I.\n\n---"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8334/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700178901483,
                "cdate": 1700178901483,
                "tmdate": 1700179049185,
                "mdate": 1700179049185,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vKcTxqofvQ",
                "forum": "EhrzQwsV4K",
                "replyto": "YTMmJeRGgo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8334/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8334/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 4yCT [Part 2/2]"
                    },
                    "comment": {
                        "value": "### **(C) How do we Ensure the Efficiency of Generated Code Files?**\n\nThank you for this exciting question. Indeed, the efficiency of the generated code is crucial in many real-world applications. Although this dimension is not currently considered in our current implementation of Code-L2MAC, there is a straightforward way in which this could be incorporated since it is easy to quantify efficiency. The _runtime_ computational, and memory _complexity_ of a test could be provided as _feedback_ to the LLM, which could use this measure to reflect upon the efficiency of the current implementation and _optimize_ it if necessary.\n\nNonetheless, this does not constitute a crucial component of our implementation, intended to act as the first instantiation of our framework. Other potential refinements are discussed in our Future Work (app. I), and we encourage (and consider) pursuing an optimized implementation of Code-L2MAC in future work.\n\n**UPDATE:** We included the above discussion in our Future Work section in appendix I as a new item."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8334/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700179039058,
                "cdate": 1700179039058,
                "tmdate": 1700179039058,
                "mdate": 1700179039058,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1hraNTm4we",
                "forum": "EhrzQwsV4K",
                "replyto": "vKcTxqofvQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8334/Reviewer_4yCT"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8334/Reviewer_4yCT"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the authors\u2019 responses. These have addressed my questions. I have changed my score to accept."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8334/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700388303917,
                "cdate": 1700388303917,
                "tmdate": 1700388303917,
                "mdate": 1700388303917,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MQqLfxzuO5",
            "forum": "EhrzQwsV4K",
            "replyto": "EhrzQwsV4K",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8334/Reviewer_4zmV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8334/Reviewer_4zmV"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a LLM-based computer that uses LLM in its core as the `computation` engine and enables the model to interact with two different memories, `instruction registry` as a storage for prompts and user-defined instructions and `file store` as a means to store intermediate outputs. The authors create an interesting synergy with how conventional computers (e.g. Von Neumann architecture) operate and aims to replace the core compute and control engine with LLMs. As one of the applications, the author explored how such LLM-enabled computing platform can productively be used for the task of code generation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "$\\mathtt{+}$ I found the synergy between conventional von Neumann architecture and L2MAC interesting and how the authors created a 1to1 mapping between different components in conventional computing platforms and their proposed design.\n\n$\\mathtt{+}$ The results for code generation tasks is promising."
                },
                "weaknesses": {
                    "value": "$\\mathtt{-}$ While I think the paper proposes an interesting idea, but I found the writing very challenging and difficult to understand and follow.\n\n$\\mathtt{-}$ While the general-purpose computers can excel work in a variety of task, L2MAC focuses on one particular task and it is not clear how such model can generalized to different application and programs.\n\n$\\mathtt{-}$ While the core idea is still new, most of the explored idea like self-refinement, using external memory, etc. have been explored before in the literature."
                },
                "questions": {
                    "value": "This is an interesting and timely idea. While the authors only explore one application for such general purpose computer, it would be interesting to see what other applications this computer can enable. To be honest, I found the writing of the paper/formulating the idea challenging to follow and that makes contributions less clear. \n\n(Q1) I appreciate the authors providing a comparison with the related work in Table1. I am wondering if it would make sense to have a comparison with those in the scope of code generation. I am curious to see how your approach compares with reflecting LLM techniques. \n\n(Q2) I understand the choice of target application for your model, but how do you think L2MAC can be extended to cover other applications and domains?\n\n(Q3) Can you clarify how do you generate test programs for each applications? Do you have another verifier to ensure the correctness of the unit test? What would happen if the unit test are limited coverage in testing the target application?\n\n(Q4) I also find the metric of `feature %` to be confusing. As an end-user, I would like my program to be fully functional and correct. Why do you need to design a new metric for the evaluation? How do you relate your metric with correctness of the program? Is there any correlation here?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8334/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699325776520,
            "cdate": 1699325776520,
            "tmdate": 1699637036665,
            "mdate": 1699637036665,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "o718aQHI26",
                "forum": "EhrzQwsV4K",
                "replyto": "MQqLfxzuO5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8334/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8334/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 4zmV [Part 1/5]"
                    },
                    "comment": {
                        "value": "Thanks for your thoughtful comments and suggestions! We are glad you enjoyed the motivation of our framework through the von Neumann architecture. Please find our answers below and the corresponding updates to the revised submission. We structure our response as follows:\n\n* (A) Clarified Writing\n* (B) Generalizing to Other Tasks\n* (C) Comparison with Reflecting LLMs\n* (D) New Ground-truth Metrics and Discussion on Previous Ones\n  * (D.1) Ground-truth Human Expert Validation Justifies the Use of Feature %\n  * (D.2) Generated Tests' Coverage\n  * (D.3) Challenges of Using an Existing Evaluation Metrics\n\n----\n\n### **(A) Clarified Writing**\n\nThank you for flagging this.\n\n**UPDATE**: We have carefully reviewed the manuscript and have _identified and fixed typos_ that could lead to confusion.\n\n---\n\n### **(B) Generalizing to Other Tasks**\n\nWe thank you for highlighting the _potential_ of L2MAC in _other applications_! Indeed, the general-purpose property of stored-program computers is fascinating, and the motivation of L2MAC in such a framework suggests it could inherit this property. We _restricted our claims_ to L2MAC's applicability to coding-related tasks to ensure they _stayed within_ what we could _verify empirically_. Still, we agree that it would be valuable to discuss the areas of applications where this framework could be explored in future work. We include these considerations in appendix I.1.\n\nFirst, _coding_ tasks cover a _broad_ set of impactful _applications_, which, combined with our empirical results, suggests that Code-L2MAC (and, by extension, thus, L2MAC) encompasses wide applicability. To strengthen this point, we increased the variety of our benchmark tasks with new tasks, as discussed in the global response **(R2)**.\n\nApart from that, we can consider other domains for this framework. As discussed in section 3.1, a crucial consideration for L2MAC is the acknowledgment that LLMs are imperfect. Thus, the _evaluation tools_ for instantiating L2MAC play an _important role_ in its effectiveness (note that this role could become less and less crucial as LLMs progress). Consequently, additional applications that would render most immediate for the usage of this framework are the ones for which we have effective evaluation tools or where there is a less stringent requirement on the output structure. This set might include _AutoML_, generating _mathematical proofs_ (Li et al., 2020), or writing _long textual documents_; the first two have natural or existing verification tools (Bayer et al., 2022); the latter imposes less strict requirements, where an LLM could also be used as an evaluator. Materializing experiments on these tasks is out of the scope of this paper, although each of them constitutes an exciting direction for future work.\n\n**UPDATE:** We have included the above discussion on the potential additional applications of L2MAC in appendix I.1. We have doubled the number of coding tasks in our benchmark to increase the variety of settings; see global response (R2).\n\n---"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8334/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700177372597,
                "cdate": 1700177372597,
                "tmdate": 1700362100881,
                "mdate": 1700362100881,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iCboPxIxRP",
                "forum": "EhrzQwsV4K",
                "replyto": "MQqLfxzuO5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8334/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8334/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 4zmV [Part 2/5]"
                    },
                    "comment": {
                        "value": "### **(C) Comparison with Reflecting LLMs**\n\nWe agree that comparing Code-L2MAC to reflecting LLM techniques would be helpful. We thus included _three new baselines_ that represent state-of-the-art reflecting code generation methods: **CodeT** (Chen et al., 2022), **Self-Refine** (Madaan et al., 2023), and **Reflexion** (Shinn et al., 2023).\n\nWe performed a complete re-run of these and the previous methods across all tasks, including three new ones; see the general response **(R2)**. We made the comparison fairer by providing these baselines with the same tools that Code-L2MAC uses; we provide the implementation details and hyperparameters in a newly expanded appendix F.\n\nAs you rightly mentioned, our _central contribution_ is introducing the first practical _LLM-based stored-program computer_. In addition to that, as you point out, our implementation incorporates concepts of refinement and external memory as discussed in previous work, which is further discussed in the _related work_, section 5, and the extended related work (appendix D), which we do not claim as contributions. Specifically, we find the following differences between Code-L2MAC and reflecting LLM methods, such as Self-Refine and Reflexion to be:\n\n- Self-Refine refines the _most recent_ output from the LLM and Reflexion the _recent outputs_ that are only within the LLM's current context window $C^t$; whereas Code-L2MAC can refine the entire file store, encompassing all previous outputs from the LLM\u2014allowing it to fix or improve earlier outputs from multiple instructions back, outside its context window, which are now erroring due to a new code implementation change. This enables Code-L2MAC to generate long, consistent, and interrelated code structures.\n- Self-Refine and Reflexion are constrained to operating on an output within the context window constraint $c$, whereas Code-L2MAC can manage a total output greater than the context window constraint through the L2MAC framework.\n\n**UPDATE:** We share the reviewer's curiosity about how Code-L2MAC compares to existing reflecting methods and have included _three new_ reflecting _baselines_ in our experiments (see R2 in the global response). We also agree that the novelty of memory that stems from the stored-program computer framework to existing memory augmentation literature might not be apparent, so we _updated_ our _extended related work_ to include a version of the above discussion.\n\n---"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8334/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700178104249,
                "cdate": 1700178104249,
                "tmdate": 1700178932656,
                "mdate": 1700178932656,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "z35iUpEBYF",
                "forum": "EhrzQwsV4K",
                "replyto": "MQqLfxzuO5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8334/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8334/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 4zmV [Part 3/5]"
                    },
                    "comment": {
                        "value": "### **(D) New Ground-truth Metrics and Discussion on Previous Ones**\n\n(D.1) We start by addressing your questions regarding how Feature % correlates with the correctness of the program, and (D.2) we discuss your great point about code coverage. Finally, (D.3), we discuss why the existing code evaluation metrics through human-written tests are not appropriate for our benchmark.\n\n**(D.1) Ground-truth Human Expert Validation Justifies the Use of Feature %**\n\nTo elucidate the motivation of **Feature %**, we note that the metric _\"Is it fully functional?\"_ is very ***sparse*** and does not reflect the degrees of correctness that an implementation can have. As an extreme case, it is tough to prove that a given code base contains no bugs, and in practice, most code bases contain bugs. This does not imply that they are not functional, although each time a bug is correctly fixed, we can agree that the code base has gotten more functional. Consequently, asking, _\"To what degree is it functional?\"_ is more reasonable. A way to quantify this is to ask _\"Of the_ ***functions/features*** _that I expect this code base to fulfill correctly,_ ***how many are actually fulfilled*** _?\"_. This is what feature % quantifies. As a proxy, and in line with previous literature (Chiang et al., 2023), we quantify this by having GPT4 assess what features specified by the user were implemented fully as code (see appendix E for example prompts).\n\nThis being established, we agree with the added value of a _ground truth_ human evaluation of the resulting code that unambiguously validates the result is fully functional and fulfills the requirements in the task description.\n\nTo that end, we _hired two professional software engineers as human experts_, separate from the authors of this work, to perform code reviews of the generated code bases for each method against the user-requested task feature checklist, counting only features that they verified were correctly and fully implemented. We regard the resulting metric, labeled **Human Expert Features %**, as the ground truth.\n\nWe tabulate (in table 5 below) this metric across three random seed runs. We highlight two conclusions.\n\n- Code-L2MAC significantly outperforms other baselines based on **Human Expert Features %.**\n- The human and LLM counterparts, **Human Expert Features %** and **Features %,** strongly correlate ($\\rho=0.976$), thereby establishing **Features %** as a good proxy for the ground truth. This validates our usage of **Features %** as a scalable and cost-effective way to evaluate the amount of features implemented in code bases from new method-task pairs. This conclusion aligns with existing literature on using LLMs as a proxy for human evaluators (Chiang et al., 2023).\n\n**Table 5.**\n| | URL Shortener App | Online Social Media App | Online Chat App |\n|-------|---------------------------------------|---------------------------------------|---------------------------------------|\n| Method | Human Expert Features % | Features % | Human Expert Features % | Features % | Human Expert Features % | Features % |\n| | \u2191 | \u2191 | \u2191 | \u2191 | \u2191 | \u2191 |\n| GPT4 | 31.4 | 53.6 | 11.1 | 19.5 | 10 | 11 |\n| AutoGPT | 15.7 | 25.3 | 6.35 | 33.3 | 15 | 23.1 |\n| Code-L2MAC | **78.4** | **91.6** | **61.9** | **82.4** | **60** | **59.4** |\n\n**UPDATE:** We now include the human expert validation results and an adaptation of the above discussion in an additional **(new) appendix K** labeled \"Human Expert Validation of Features Implemented Percentage Metric.\" We also included the above justification for the Feature % metric in a new subsection of appendix G (evaluation metrics).\n\n---"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8334/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700178321719,
                "cdate": 1700178321719,
                "tmdate": 1700362645830,
                "mdate": 1700362645830,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "T3r35nS6id",
                "forum": "EhrzQwsV4K",
                "replyto": "MQqLfxzuO5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8334/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8334/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 4zmV [Part 4/5]"
                    },
                    "comment": {
                        "value": "**(D.2) Generated Tests' Coverage**\n\nRegarding generated tests, we request that the LLM generate tests that verify the validity of the implemented code, which are also used for reflection. We agree that such tests are a proxy for such validity since they could come in the form of mock tests that do not reflect the validity of the code, but this does not seem to be the case, as shown by the fact that _Code-L2MAC can fix tests that start failing throughout the execution_ (mock tests would never fail), while for example, _AutoGPT accumulates failing tests_, as shown in section 6.2.\n\nWe thank you for pointing out the notion of coverage for the tests, which opens the door to help understand more precisely the implications of the passed and failed tests. Correspondingly, we incorporated a new metric reflecting the _coverage percentage_ of the tested code. Using the standard test suite quality metric of code coverage percentage of the unit tests (Miller & Maloney, 1963), we observe that Code-L2MAC has a high code coverage percentage despite its substantially larger amount of generated lines of code. As shown in table 4, in the global response **(R2)**, Code-L2MAC achieves an average coverage percentage of 93.93% across three tasks.\n\n**UPDATE:** We now include a new coverage evaluation metric for all the new tasks and baselines in a **(new) appendix J** and include this metric in our evaluation metrics in appendix G.\n\n---\n\n**(D.3) Challenges of Using an Existing Evaluation Metric of Human-written Test Cases to Validate the Code**\n\nWe also explored using a priori hand-written test cases that are consistent across the different methods, which is the existing way to evaluate small code snippet generation tasks. We implemented this metric and present the results in table 6 below, which shows it is correlated to our main evaluation metric of **Features %.**\n\nIt is relevant to start with some challenges this metric presents:\n\n1. **Handwritten test cases assume and impose a known interface or a stringent implementation structure**. In small code, snippet tasks, such as those in HumanEval (Chen et al., 2021) or MBPP (Austin et al., 2021), an explicitly defined function interface is explicitly presented to the LLM, and the LLM only responds with the code for that function body. In this situation, handwritten test cases can assume the pre-defined implementation structure. However, in code base generation tasks defined through feature requirements, there is freedom about the segmentation into components, modules, or classes, which must be appropriately determined by the code generation method. For example, allowing users to register an account can be achieved with many code implementations. By specifying _tests, we filter this ambiguity into a given implementation approach,_ and we _cannot_ account for _all other possible code implementation_ approaches to implement a functional feature correctly.\n2. **Requires expert-crafted task-specific test cases a priori.** This hinders the scalability of this approach.\n\nWe added these handwritten test cases into each method's context window $C^t$ throughout all generation stages. Once the method generated a code base, we then randomly changed the hand-written test case parameters to still be the same test, just with different test parameters, to avoid the method memorizing the test examples, e.g., changing the initial user_ids to a random value. Since all methods often generated a code base that did not precisely match the implementation of the test cases while still having a code base that would conceptually pass the test purpose, we used GPT4 to port the test cases to each specific code base implementation. We term the proportion of such tests that pass human-written tests as **HT %**, the percentage of human tests that pass for a given code base. Table 6, which is computed over five random seed runs, shows that this metric correlates to our **Feature %** evaluation metric ($\\rho=0.695$), which further provides empirical evidence for such a metric.\n\n**Table 6.**\n| Method | Features % | HT % | # Errors | LOC | Tests Passed | Cov % |\n|------------|------------------|------------------|------------------|---------------|--------------------|-------------------|\n| | \u2191 | \u2191 | \u2193 | | \u2191 | \u2191 |\n| GPT4 | 25\u00b179.6 | 0\u00b10 | 3.75\u00b110.9 | 134\u00b119.9 | 6.75\u00b17.16 | 80.5\u00b114.5 |\n| CodeT | 13.2\u00b142.1 | 11.1\u00b135.4 | 0\u00b10 | 126\u00b114.4 | 7.75\u00b13.98 | 86.8\u00b14.57 |\n| Self-Refine| 30.6\u00b130.7 | 33.3\u00b141.4 | 0.2\u00b10.555 | 140\u00b19.83 | 9\u00b10 | 74.6\u00b18.85 |\n| Reflexion | 30.9\u00b120.8 | 33.3\u00b114.4 | 0\u00b10 | 84.5\u00b133.9 | 3.5\u00b10.919 | 96.5\u00b15.88 |\n| Code-L2MAC | **76.5\u00b133.3** | **41.7\u00b154.7** | **0\u00b10** | **286\u00b1172** | **10\u00b19.09** | **83\u00b18.72** |\n\n**UPDATE:** We have now included an extended version of this discussion in a **(new) appendix L** entitled \"Challenges and the Evaluation of Human-written Test-cases.\"\n\n---"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8334/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700178556755,
                "cdate": 1700178556755,
                "tmdate": 1700178949584,
                "mdate": 1700178949584,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fNvFcIT3TS",
                "forum": "EhrzQwsV4K",
                "replyto": "MQqLfxzuO5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8334/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8334/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 4zmV [Part 5/5]"
                    },
                    "comment": {
                        "value": "**References:**\n\n- Wenda Li, Lei Yu, Yuhuai Wu, and Lawrence C. Paulson. Isarstep: a benchmark for high-level mathematical reasoning, 2021.\n- Jonas Bayer, Christoph Benzm\u00fcller, Kevin Buzzard, Marco David, Leslie Lamport, Yuri Matiyasevich, Lawrence Paulson, Dierk Schleicher, Benedikt Stock, and Efim Zelmanov. Mathematical proof between generations, 2022.\n- Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu Chen. Codet: Code generation with generated tests. arXiv preprint arXiv:2207.10397, 2022.\n- Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023.\n- Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik R Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.\n- Chiang, Cheng-Han, and Hung-yi Lee. \"Can Large Language Models Be an Alternative to Human Evaluations?.\" _arXiv preprint arXiv:2305.01937_ (2023).\n- Joan C Miller and Clifford J Maloney. Systematic mistake analysis of digital computer programs. Communications of the ACM, 6(2):58\u201363, 1963.\n- Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.\n- Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8334/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700178586969,
                "cdate": 1700178586969,
                "tmdate": 1700178960742,
                "mdate": 1700178960742,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ROc6A5DNvy",
            "forum": "EhrzQwsV4K",
            "replyto": "EhrzQwsV4K",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8334/Reviewer_v4fH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8334/Reviewer_v4fH"
            ],
            "content": {
                "summary": {
                    "value": "The paper investigates the challenges and capabilities of Large-Language Models (LLMs) in the context of code generation from natural language instructions. A central issue identified is the difficulty LLMs face when generating extensive programs due to the limitations imposed by context window length. To tackle this problem, the authors introduce a system, L2MAC, which augments LLMs with an instruction registry and a file store. Through the orchestration of a control unit, L2MAC breaks down code generation into smaller, more manageable steps, and permits the modification of previously generated code via the file store. Evaluations presented in the paper indicate that L2MAC delivers superior performance in three system design tasks compared to two established benchmarks, GPT-4 and AutoGPT."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. **Relatively Novel Approach**: The paper presents a novel idea of employing a control unit, instruction registry, and a file store to enhance LLMs. Although the individual components have been introduced in prior work (planning, test case generation, using external tools, refining with code execution feedback), the application of these to a stored-program computer in this context seems to be a fresh approach.\n\n2. **Detailed Descriptions**: The paper provides a thorough description of all the crucial modules. The inclusion of example prompts and turn-by-turn actions in the appendix is an added advantage, as it supports better comprehension and reproducibility.\n\n3. **Addressing Key Challenges**: The paper tackles an important and complex problem - the generation of comprehensive and cohesive programs. It makes significant observations, such as emphasizing the need to enable LLMs to revise previously-generated code, which contributes to the understanding of the problem."
                },
                "weaknesses": {
                    "value": "1. **Questionable Evaluation Metrics**: The paper employs several evaluation metrics that are based on LLMs rather than ground truths like human-written test cases. This approach raises concerns about the representation of these metrics in terms of code quality. For instance, 'Features %' is determined by a GPT-4 call and not by running and testing the code. Similarly, 'Tests Passed' is based on the LLM-generated test cases, which may not accurately reflect test coverage or code quality. The paper could improve its evaluation by incorporating well-established metrics, such as human-written test cases, and applying a consistent set of tests across different methods. [partially addressed in the rebuttal].\n\n2. **Overclaiming**: Although the paper introduces an intriguing concept, it also overstates several claims. For example, it suggests that L2MAC could enable LLMs to generate virtually unbounded code, but in practice, it is still limited by the context window when breaking down tasks and reading/writing code files. Moreover, while the paper claims that L2MAC can generate large codebases, the evaluation only presents a modest code length (300-400 LOCs).\n\n3. **Weak Baselines**: The paper fails to incorporate more state-of-the-art code generation baselines like CodeT (Chen et al., 2022), Self-refine (Madaan et al., 2023), and Reflexion (Shinn et al., 2023). The use of GPT-4 alone as a baseline appears to be a strawman argument, while AutoGPT is not renowned for superior code generation capability in any popular benchmark, such as HumanEval or MBPP [addressed in the rebuttal]."
                },
                "questions": {
                    "value": "1. Have you considered incorporating ground truths, such as human-written test cases, into your evaluation metrics? Were there any challenges in doing so? Additionally, have you conducted any evaluation using more established metrics, and if so, could you share the results?\n\n2. Might the inclusion of more sophisticated code generation models such as CodeT, Self-refine, and Reflexion have provided a broader comparison of L2MAC's performance? If this is the case, could you possibly incorporate some of these comparative results in the paper?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8334/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8334/Reviewer_v4fH",
                        "ICLR.cc/2024/Conference/Submission8334/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8334/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699427960449,
            "cdate": 1699427960449,
            "tmdate": 1700517526740,
            "mdate": 1700517526740,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1xvbpjaVhv",
                "forum": "EhrzQwsV4K",
                "replyto": "ROc6A5DNvy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8334/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8334/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer v4fH [Part 1/4]"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful comments and suggestions! We are glad you enjoyed the relevance of the problem of program cohesiveness that L2MAC tackles, the novelty of our approach through the stored-program computer, and the detailed descriptions. Please find our answers below and the corresponding updates to the revised submission.\n\n* (A) Additional Evaluation Metrics\n  * (A.1) Ground-truth Human Expert Validation Justifies the Use of Feature %\n  * (A.2) Extending Tests with Code Coverage\n  * (A.3) Human-written Test Cases to Measure the Number of Features Implemented\n* (B) Refining Claims\n  * (B.1) \"extensive\" Now Replaces \"unbounded\"\n  * (B.2) Additional Experiment Task Demonstrating Code-L2MAC Can Generate 1,000+ Lines of Code\n* (C) Three New State-of-the-art Code Generation Baselines\n\n---\n\n### **(A) Additional Evaluation Metrics**\n\nWe agree with the reviewer that the choice of evaluation metrics is vital to provide a fair comparison with the baselines. Although there is precedent for using LLMs as a proxy for human evaluation (Chiang et al., 2023), we agree that it is desirable to get a fuller picture of what they represent and ideally validate such metrics by comparing how they perform against ground truth validation methods.\n\n**(A.1) Ground-truth Human Expert Validation Justifies the Use of Feature %**\n\nTo that end, we **hired two professional software engineers** as _human experts_, separate from the authors of this work, to perform code reviews of the generated code bases for each method against the user-requested task feature checklist, counting only features that they verified were correctly and fully implemented. We regard the resulting metric, labeled **Human Expert Features %**, as the ground truth.\n\nWe tabulate (in table 5 below) this metric across three random seed runs. We highlight two conclusions.\n\n- Code-L2MAC significantly outperforms other baselines based on **Human Expert Features %.**\n- The human and LLM counterparts, **Human Expert Features %** and **Features %,** strongly correlate ($\\rho=0.976$), thereby establishing **Features %** as a good proxy for the ground truth. This validates our usage of **Features %** as a scalable and cost-effective way to evaluate the amount of features implemented in code bases from new method-task pairs. This conclusion aligns with existing literature on using LLMs as a proxy for human evaluators (Chiang et al., 2023).\n\n\n\n**Table 5.**\n| | URL Shortener App | Online Social Media App | Online Chat App |\n|-------|---------------------------------------|---------------------------------------|---------------------------------------|\n| Method | Human Expert Features % | Features % | Human Expert Features % | Features % | Human Expert Features % | Features % |\n| | \u2191 | \u2191 | \u2191 | \u2191 | \u2191 | \u2191 |\n| GPT4 | 31.4 | 53.6 | 11.1 | 19.5 | 10 | 11 |\n| AutoGPT | 15.7 | 25.3 | 6.35 | 33.3 | 15 | 23.1 |\n| Code-L2MAC | **78.4** | **91.6** | **61.9** | **82.4** | **60** | **59.4** |\n\n**UPDATE:** We now include the human expert validation results and an adaptation of the above discussion in an additional **(new) appendix K** labeled \"Human Expert Validation of Features Implemented Percentage Metric.\"\n\n---\n\n**(A.2) Extending Tests with Code Coverage**\n\nTo get more insight into the implications of the self-generated \"Tests passed\" metric, we use the standard test suite quality metric of code coverage percentage of the unit tests (Miller & Maloney, 1963), and similarly observe Code-L2MAC has a high code coverage percentage despite its substantially larger amount of generated lines of code. As shown in table 4, in the global response **R2**, Code-L2MAC achieves an average coverage percentage of 93.93% across three tasks.\n\n**UPDATE:** We now include this new evaluation metric for all the new tasks and baselines in a **(new) appendix J** and include this metric in our evaluation metrics in appendix G.\n\n---"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8334/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700176612422,
                "cdate": 1700176612422,
                "tmdate": 1700177121673,
                "mdate": 1700177121673,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Icxqt0GPpZ",
                "forum": "EhrzQwsV4K",
                "replyto": "ROc6A5DNvy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8334/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8334/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer v4fH [Part 2/4]"
                    },
                    "comment": {
                        "value": "**(A.3) Human-written Test Cases to Measure the Number of Features Implemented**\n\nWe also explored using a priori hand-written test cases that are consistent across the different methods. We implemented this metric and present the results in table 6 below, which shows it is correlated to our proposed main evaluation metric of **Features %.**\n\nIt is relevant to discuss some challenges this metric presents:\n\n1. **Handwritten test cases assume and impose a known interface or a stringent implementation structure**. In small code snippet tasks, such as those in HumanEval (Chen et al., 2021) or MBPP (Austin et al., 2021), an explicitly defined function interface is explicitly presented to the LLM, and the LLM only responds with the code for that function body. In this situation, handwritten test cases can assume the pre-defined implementation structure. However, in code base generation tasks defined through feature requirements, there is a freedom about the segmentation into components, modules, or classes, which must be appropriately determined by the code generation method. For example, allowing users to register an account can be achieved with many code implementations. By specifying _tests, we filter this ambiguity into a given implementation approach,_ and we _cannot_ account for _all other possible code implementation_ approaches to implement a functional feature correctly.\n2. **Requires expert-crafted task-specific test cases a priori.** This hinders the scalability of this approach.\n\nWe added these handwritten test cases into each method's context window $C^t$ throughout all generation stages. Once the method generated a code base, we then randomly changed the hand-written test case parameters to still be the same test, just with different test parameters, to avoid the method memorizing the test examples, e.g., changing the initial user_ids to a random value. Since all methods often generated a code base that did not precisely match the implementation of the test cases while still having a code base that would conceptually pass the test purpose, we used GPT4 to port the test cases to each specific code base implementation. We term the proportion of such tests that pass human-written tests as **HT %**, the percentage of human tests that pass for a given code base. Table 6, which is computed over five random seed runs, shows that this metric correlates to our **Feature %** evaluation metric ($\\rho=0.695$), which further provides empirical evidence for such a metric.\n\n**Table 6.**\n| Method | Features % | HT % | # Errors | LOC | Tests Passed | Cov % |\n|------------|------------------|------------------|------------------|---------------|--------------------|-------------------|\n| | \u2191 | \u2191 | \u2193 | | \u2191 | \u2191 |\n| GPT4 | 25\u00b179.6 | 0\u00b10 | 3.75\u00b110.9 | 134\u00b119.9 | 6.75\u00b17.16 | 80.5\u00b114.5 |\n| CodeT | 13.2\u00b142.1 | 11.1\u00b135.4 | 0\u00b10 | 126\u00b114.4 | 7.75\u00b13.98 | 86.8\u00b14.57 |\n| Self-Refine| 30.6\u00b130.7 | 33.3\u00b141.4 | 0.2\u00b10.555 | 140\u00b19.83 | 9\u00b10 | 74.6\u00b18.85 |\n| Reflexion | 30.9\u00b120.8 | 33.3\u00b114.4 | 0\u00b10 | 84.5\u00b133.9 | 3.5\u00b10.919 | 96.5\u00b15.88 |\n| Code-L2MAC | **76.5\u00b133.3** | **41.7\u00b154.7** | **0\u00b10** | **286\u00b1172** | **10\u00b19.09** | **83\u00b18.72** |\n\n**UPDATE:** We have now included an extended version of this discussion in a **(new) appendix L** entitled \"Challenges and the Evaluation of Human-written Test-cases.\"\n\n---\n\n### **(B) Refining Claims**\n\nBased on your comments, we (1) refined our claims of adapting any mention of Code-L2MAC can generate _unbounded_ output to Code-L2MAC can generate _extensive_ output, and (2) provide an additional experiment task demonstrating Code-L2MAC can generate 1,000+ lines of code.\n\n**(B.1) \"extensive\" Now Replaces \"unbounded\"**\n\nWe thank you for spotting the source of confusion that our use of \"virtually unbounded\" represents. We agree that this can be misleading and revised the use of this expression to remove any trace of such a claim.\n\n**UPDATE:** We will change the title and abstract to change the word \"unbounded\" to \"extensive\" and have done this throughout the paper. We kindly note that at the rebuttal stage, we cannot change the title and the abstract. However, we will make this change during the camera-ready revision.\n\n---"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8334/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700176858287,
                "cdate": 1700176858287,
                "tmdate": 1700177112470,
                "mdate": 1700177112470,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2Dx49JoyjT",
                "forum": "EhrzQwsV4K",
                "replyto": "ROc6A5DNvy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8334/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8334/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer v4fH [Part 3/4]"
                    },
                    "comment": {
                        "value": "**(B.2) Additional Experiment Task Demonstrating Code-L2MAC Can Generate 1,000+ Lines of Code**\n\nBy removing the restrictions we imposed upon Code-L2MAC to economize API-calls, such as limiting the number of instructions to 10, we get a variation we term Code-L2MAC-Large that we tested on the Online Chat application task where it reached reached over 1,000+ LOCs (5x the LOC of AutoGPT, the next highest) as shown in table 7 below.\n\n**Table 7.**\n| Method | Features % | # Errors | LOC | Tests Passed |\n|-------------------|-----------------|-----------------|--------------------|------------------|\n| | \u2191 | \u2193 | | \u2191 |\n| Code-L2MAC-Large | **53.3\u00b119** | **0.333\u00b11.43** | **1,030\u00b140.8** | **5.67\u00b113.7** |\n\n**UPDATE** : We now discuss Code-L2MAC-Large and this experiment as an additional **(new) appendix M**, entitled \"Generating 1,000+ Lines of Code with Code-L2MAC\".\n\n---\n\n### **(C) Three New State-of-the-art Code Generation Baselines**\n\nWe agree with the importance of considering these three methods and thus included them as **three new baselines** that represent state-of-the-art reflecting code generation methods: **CodeT** (Chen et al., 2022), **Self-Refine** (Madaan et al., 2023), and **Reflexion** (Shinn et al., 2023).\n\nWe performed a complete re-run of these and the previous methods across all tasks, including three new ones; see the general response **(R2)**. We made the comparison fairer by providing these baselines with the same tools that Code-L2MAC uses; we provide the implementation details and hyperparameters in a newly expanded appendix F.\n\nThese three new reflecting baselines are now included in the updated main experimental results table 2, also shown below. Code-L2MAC still fully implements the highest percentage of user-specified feature requirements across all tasks, where its code has minimal syntactical errors and a high number of self-generated unit tests. Therefore, Code-L2MAC is state-of-the-art for completing these system design large code generation benchmark tasks.\n\n**Table 2.** (Main experimental results averaged over ten random seeds)\n\n**URL Shortener App**\n| Method | Features % | # Errors | LOC | Tests Passed |\n| ------------ | -------------------- | ------------- | -------------- | ----------------- |\n| | \u2191 | \u2193 | | \u2191 |\n| GPT4 | 53.6\u00b110.5 | 0\u00b10 | 119\u00b121.1 | 2.56\u00b10.95 |\n| CodeT | 52.9\u00b16.74 | 0.05\u00b10.105 | 110\u00b111.8 | 3.6\u00b10.513 |\n| Self-Refine | 47.9\u00b18.53 | 0.05\u00b10.105 | 124\u00b115.7 | 3.65\u00b11.15 |\n| Reflexion | 38.8\u00b16.02 | 0.1\u00b10.209 | 96.2\u00b19.11 | 2.35\u00b10.631 |\n| AutoGPT | 25.3\u00b119.6 | 0\u00b10 | 136\u00b141.9 | 3.3\u00b11.91 |\n| Code-L2MAC | **91.6\u00b18.22** | **0\u00b10** | **330\u00b147.6** | **14\u00b16.71** |\n\n**Online Social Media App**\n| Method | Features % | # Errors | LOC | Tests Passed |\n| ------------ | -------------------- | ------------- | -------------- | ----------------- |\n| | \u2191 | \u2193 | | \u2191 |\n| GPT4 | 19.5\u00b18.28 | 4.09\u00b13.32 | 116\u00b131.5 | 0.818\u00b10.785 |\n| CodeT | 19.5\u00b15.19 | 0.4\u00b10.603 | 106\u00b117.7 | 2.6\u00b11.76 |\n| Self-Refine | 16.4\u00b12.62 | 0.938\u00b10.714 | 110\u00b119.6 | 1.81\u00b10.938 |\n| Reflexion | 15.2\u00b18.05 | 2.53\u00b11.69 | 122\u00b124 | 1.33\u00b12.44 |\n| AutoGPT | 33.3\u00b118 | 0.6\u00b10.369 | 148\u00b135.5 | 3\u00b12.86 |\n| Code-L2MAC | **82.4\u00b114.6** | **0\u00b10** | **395\u00b152.9** | **18.3\u00b16.8** |\n\n**Online Chat App**\n| Method | Features % | # Errors | LOC | Tests Passed |\n| ------------ | -------------------- | ------------- | -------------- | ----------------- |\n| | \u2191 | \u2193 | | \u2191 |\n| GPT4 | 11\u00b12.26 | 0.3\u00b10.346 | 127\u00b124.1 | 1.2\u00b11 |\n| CodeT | 10.5\u00b14.61 | 0\u00b10 | 91.6\u00b125.9 | 3.32\u00b11.57 |\n| Self-Refine | 14.2\u00b14.19 | 0.211\u00b10.304 | 111\u00b113.8 | 1.42\u00b10.927 |\n| Reflexion | 10.2\u00b13.08 | 0\u00b10 | 76\u00b16.88 | 2.85\u00b10.822 |\n| AutoGPT | 23.1\u00b111.8 | 1.85\u00b12.47 | 220\u00b165.8 | 3.08\u00b13.34 |\n| Code-L2MAC | **59.4\u00b125.9** | **0\u00b10** | **374\u00b1123** | **18.8\u00b19.11** |\n\n**Update:** These three reflecting LLM baselines have been included in the main experimental results in the paper in table 2.\n\n---"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8334/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700177050208,
                "cdate": 1700177050208,
                "tmdate": 1700177103455,
                "mdate": 1700177103455,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9hDsQgXYK9",
                "forum": "EhrzQwsV4K",
                "replyto": "ROc6A5DNvy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8334/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8334/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer v4fH [Part 4/4]"
                    },
                    "comment": {
                        "value": "**References:**\n\n- Chiang, Cheng-Han, and Hung-yi Lee. \"Can Large Language Models Be an Alternative to Human Evaluations?.\" _arXiv preprint arXiv:2305.01937_ (2023).\n- Joan C Miller and Clifford J Maloney. Systematic mistake analysis of digital computer programs. Communications of the ACM, 6(2):58\u201363, 1963.\n- Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.\n- Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.\n- Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu Chen. Codet: Code generation with generated tests. arXiv preprint arXiv:2207.10397, 2022.\n- Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023.\n- Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik R Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8334/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700177091460,
                "cdate": 1700177091460,
                "tmdate": 1700177091460,
                "mdate": 1700177091460,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JMZv1GUO2Z",
                "forum": "EhrzQwsV4K",
                "replyto": "ROc6A5DNvy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8334/Reviewer_v4fH"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8334/Reviewer_v4fH"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your detailed answer. The added results, including an expert's review, ground-truth test cases, and extra uses, greatly improve the evaluation and tackle most of my worries. As a result, I've raised my score by one level.\n\nHowever, I still have some worries about the paper's claims. I would like to see more careful claims and a discussion of limits. For example, even the extra tests with 1000 lines of code (LOC) may not be enough to count as a large code base. Also, the extra results with real-world test cases are now part of the input for creating code, which is not the usual way to test code quality.\n\nWhile I get that the proposed solution might not work well without seeing the test cases, I think the paper should make this method's limits clear. It would be even better if the authors could show results without giving the ground-truth test cases to the Language Models (LLMs)."
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8334/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700518854355,
                "cdate": 1700518854355,
                "tmdate": 1700519195537,
                "mdate": 1700519195537,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tNpgRWoyKg",
                "forum": "EhrzQwsV4K",
                "replyto": "ROc6A5DNvy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8334/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8334/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer v4fH Comment [Part 1/2]"
                    },
                    "comment": {
                        "value": "We are glad you appreciated our improvements to the paper. Thank you again for your valuable input throughout the review process, and we are grateful for your increased score by one level. \nOnce again, the issues you raised drove us toward further experiments that have proven insightful and beneficial for the paper. We organize our response as follows:\n\n* (A) Code-L2MAC Lines of Code Limits\n* (B) Discussion on Human-written Test Cases and Results Without Test Cases Given to the Methods\n  * (B.1) Discussion on Human-written Test Cases\n  * (B.2) Results Without Human-written Test Cases Given to the Methods\n\n---\n\n### **(A) Code-L2MAC Lines of Code Limits**\n\nWe agree that there are limits to the number of lines of code (LOC) that Code-L2MAC can write effectively in the current implementation. Both are discussed within the paper (section 4, footnote 7, and section 7), and both can be readily addressed, providing fertile ground for future work. These are:\n\n* _Code files should be smaller than the context window constraint $c$_. Reading a file involves outputting its whole content into the LLM\u2019s context, and writing a file implies rewriting it. This means that to avoid falling out of context, the maximum length for a file is the LLM\u2019s context window size, but in fact, it is undesirable to have a file of length more than half the LLM\u2019s context window size since this will imply that the LLM cannot modify it without falling out of context. This limitation can be overcome by endowing the LLM with the capacity to selectively read and write parts of the file (e.g., function names and headers) in the same spirit as diffs are conducted in git. We discuss this and its implications for the method\u2019s efficiency in Future Work (app. I).\n* _All the code file paths are listed in the context window $C^t$_. Therefore, the maximum number of file paths (e.g. [\u2018app.py,\u2019 \u2018test_app.py,\u2019 \u2026]) listed Code-L2MAC can have in memory is strictly less than the context length. This can be readily solved by allowing the LLM only to list the file paths inside a folder and the possibility to navigate inside a sub-folder. In such a case, the constraint would happen only regarding the degree in the file tree, but we could theoretically have infinite depth in the file store.\n\nAlthough there are no definitive reasons why, with these improvements, Code-L2MAC would have an inherent LOC-wise upper bound, we agree that having a more precise notion of what \u201clarge\u201d implies would be desirable. Indeed, the inherent limitations on the LLM\u2019s capabilities could implicitly impose an LOC-wise upper limit to Code-L2MAC in practice.\n\nTo that end, and to address your concern, we follow an experimental version of the \u201cProof by Induction\u201d reasoning. Our original experiments show that Code-L2MAC can handle the base case: \u201cStarting a code base from scratch.\u201d We now run three new experiments/tasks corresponding to the inductive step: \u201cGiven an extensive code base, can Code-L2MAC correctly enlarge it?\u201d\n\nWe take three existing code bases between 87,000 and 165,000 LOCs and request that Code-L2MAC extend its functionalities with a new feature. Experiments show that Code-L2MAC is also capable of handling this task, as the following table 10 shows:\n\n**Table 10**\n| Method     | Features % (Dynamic Dashboard App) | LOC (Dynamic Dashboard App) | Features % (Community Forum App) | LOC (Community Forum App) | Features % (Data Exploration App) | LOC (Data Exploration App) |\n|------------|------------------------------------|-----------------------------|-----------------------------------|---------------------------|------------------------------------|----------------------------|\n| Code-L2MAC | 80\u00b130.2                             | 165,811.6\u00b143.8              | 70\u00b134.6                            | 88,878.4\u00b19.68             | 100\u00b10                               | 87,044.5\u00b118.5              |\n\n**UPDATE**: We incorporate a version of the discussion on Code-L2MAC limits into the existing future work appendix I. We also added the new experiments in a **(new) appendix O**, entitled ***\u201cAdditional Tasks on Implementing a New Feature in an Existing Large Code Base with Code-L2MAC of up to 165,000 LOCs\u201d***, which includes their experimental setup details.\n\n---"
                    }
                },
                "number": 26,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8334/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700595391608,
                "cdate": 1700595391608,
                "tmdate": 1700595391608,
                "mdate": 1700595391608,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oLx6ZVrCA6",
                "forum": "EhrzQwsV4K",
                "replyto": "ROc6A5DNvy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8334/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8334/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer v4fH Comment [Part 2/2]"
                    },
                    "comment": {
                        "value": "### **(B) Discussion on Human-written Test Cases and Results Without Test Cases Given to the Methods**\n\nWe kindly note that this evaluation method is only used in the set of experiments described in appendix L, labeled \u201cChallenges and the Evaluation of Human-written Test-cases\u201d (\u201cA.3. Human-written Test Cases to Measure the Number of Features Implemented\u201d in our first response). In contrast, the ground-truth human evaluation, coverage, and the rest of the experiments do not provide the LLM with the tests that will be used for the evaluation.\n\nNonetheless, your remarks are relevant, and we divide them into two parts.\n\n---\n\n**(B.1) Discussion on Human-written Test Cases**\n\nIndeed, as you correctly point out and as we tried to convey in our discussion on the challenges of using \u201ca priori human-written test cases,\u201d providing the test cases to the LLMs is not ideal since it hints at how to design the implementation.\n\n**UPDATE**: We thank you for highlighting the need for a more explicit discussion of the limitations of this evaluation method (which was included during the rebuttal in a new appendix L). We updated appendix L correspondingly to clearly state the limitations mentioned above on the results obtained when including the test cases as information for the method.\n\n---\n\n**(B.2) Results Without Human-written Test Cases Given to the Methods**\n\nWe thus agree that incorporating a re-run of the experiments in appendix L without allowing the method to see the test-cases would be insightful.\n\nExpanding on the original response of (A.3) and (B.1) above, and following your suggestion, we performed a complete re-run for all the baselines for this setting, where the methods did **not** have the human-written test-cases as part of their input (i.e., excluding the test-cases from the context window $C^t$), throughout all stages of generation. We present these new results in table 11. We observe that the **HT \\%** metric correlates to our **Feature \\%** evaluation metric ($\\rho=0.928$), which further provides empirical evidence for such a metric.\n\n**Table 11**\n| Method      | Features % | HT % | # Errors | LOC | Tests Passed | Cov % |\n|-------------|------------|------|----------|-----|--------------|-------|\n| | \u2191 | \u2191 | \u2193 | | \u2191 | \u2191 |\n| GPT4        | 37.6\u00b113.3  | 20\u00b129.9 | 0\u00b10    | 94.2\u00b115.4 | 2\u00b11.76       | 85.6\u00b118.1 |\n| CodeT       | 47.1\u00b110.3  | 42.2\u00b131.5 | 0\u00b10  | 98\u00b119     | 4.8\u00b13.45     | 91.8\u00b17.15 |\n| Self-Refine | 50.6\u00b116.8  | 46.7\u00b149.2 | 0\u00b10  | 109\u00b112.9  | 3.4\u00b12.26     | 92\u00b11.96   |\n| Reflexion   | 55.3\u00b118.3  | 37.8\u00b133.2 | 0.6\u00b11.67 | 124\u00b131.3 | 3.4\u00b12.99   | 87.6\u00b112.8 |\n| Code-L2MAC  | 89.4\u00b112    | 71.1\u00b150.3 | 0\u00b10  | 283\u00b1100   | 8.6\u00b19.52     | 77.2\u00b153.7 |\n\n\n**UPDATE**: As you suggested, we have re-run these same experiments without providing the test cases to the LLM and incorporated these new results in an **(newly updated) appendix L**.\n\n---\n\nShould any uncertainties linger, we invite you to share them with us before the author discussion period concludes. Your continued engagement is deeply appreciated, and we are at your disposal for any further clarifications. Thank you!"
                    }
                },
                "number": 27,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8334/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700595622575,
                "cdate": 1700595622575,
                "tmdate": 1700595701611,
                "mdate": 1700595701611,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Cta1Uxf4Ub",
            "forum": "EhrzQwsV4K",
            "replyto": "EhrzQwsV4K",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8334/Reviewer_mgbF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8334/Reviewer_mgbF"
            ],
            "content": {
                "summary": {
                    "value": "Authors propose L2MAC tool that implements automatic generation of large code bases using LLMs. Authors implement context management to keep relevant context and summarize/compress previous context to keep it within context size bounds. They implement approach to read and write file data across all created files. They also implement generated output checker that runs static analysis and unit tests and processes error messages. Authors evaluate their tool on a benchmark set and demonstrate improved results compared to state-of-the-art baselines."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Structured framework for LLM-based computation that can deal with limited context, file input/output and output evaluation and testing.\n- Context handling that preserves information needed for the tasks and limits context to the context size\n- Read and write implementation for files generated during subtasks. Demonstrated capabilities to write, then read and update files.\n- Strongly improved results on benchmark tasks compared to strong baseline models/tools."
                },
                "weaknesses": {
                    "value": "- File read/write implementation details are not clear. Please explain how your system decides what files to write, read, and update and how this is different from previous systems that did not have this functionality.\n- Benchmark set is not described. It is not clear if the benchmarks are representative of large code base creation tasks. Evaluation is done on only 3 tasks. The number of tasks should be increased to show the versatility and that the results are not outliers.\n\nMinor comments:\n- Stored-program computer subsection does not seem to contribute much to the paper. Probably shorten or remove.\n- Code-LLMatic is used instead of CodeL2MAC in couple places. Did not update old name?\n- Footnotes on page 4 mostly do not add to the narrative. Remove?\n- Figure 3 is placed after Figure 4 for some reason."
                },
                "questions": {
                    "value": "- Figure 4 (c) - could you explain why for Code-L2MAC the figure shows the number of tests passed, while for other two tools it shows the number of tests failed? Are these numbers comparable? If so, why and how?\n- Does the \"checking of generated output\" part contain novel contributions? It seems to me that other tools and approaches also have feedback loops where code is regenerated on errors. It would be good if this was clarified. (This is possibly a minor weakness).\n- If I understand correctly, unlike autonomous tools that can add additional subtasks dynamically L2MAC creates a subtask list once at the beginning and does not subdivide or add additional tasks later. It seems that authors consider this to be a strength, because L2MAC will not go into hallucination subtask loop. However, this could also be a drawback since L2MAC may create subtask that needs to be subdivided later and it will not be able to do so. This should be evaluated. This also connects to the weakness of benchmark set that only has 3 benchmarks: perhaps other benchmarks would show the strength or weakness of this approach."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8334/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699567329402,
            "cdate": 1699567329402,
            "tmdate": 1699637036418,
            "mdate": 1699637036418,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YxQFgO8jBF",
                "forum": "EhrzQwsV4K",
                "replyto": "Cta1Uxf4Ub",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8334/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8334/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer mgbF [Part 1/3]"
                    },
                    "comment": {
                        "value": "Thanks for your thoughtful comments and suggestions! Please find our answers as follows, along with corresponding updates to the revised submission:\n* (A) Read/Write Implementation Details\n* (B) Extending and Describing the Benchmark\n* (C) Typos\n* (D) Questions\n  * (D.1) Figure 4 (c) Interpretation\n  * (D.2) Reflection in Code-L2MAC\n  * (D.3) Replanning\n\n---\n### **(A) Read / Write Implementation Details**\n\nWe agree that a detailed description individually covering the read/write implementation would be valuable. The description in Section 4, appendix C, and appendix F.1 is entangled with the description of other components, which can induce confusion.\n\nThe LLM interfaces with the control unit (CU) through calling functions, and thus, the read and write implementation can be fully described through a discussion of the functions that are provided to Code-L2MAC to that end, which are `read_files` and `write_files.`\n\nIt is worth noting that the control unit always exposes the LLM to the _list of file paths_ that are already part of the code base (e.g., \"file paths: ['app.py', 'tests/test\\_app.py']\"). The name of directories and files (i.e., file path) provides a semantical _prior_ for the _order_ in which the LLM will read each file, depending on the functionality it is looking for (among other elements, including always the current instruction $\\mathcal{I}\\^t$, and possibly previous dialog turn outputs and error responses).\n\nNote that if the LLM initially reads a file that does not contain what it was looking for or the desired functionality is spread across multiple files, the LLM can continue reading other files. Recall that when the context window is full, the CU prompts the LLM to summarize the relevant parts of the context window for the current instruction $\\mathcal{I}\\^t$. In this case, it could summarize the relevant files it needs to read if the functionality is spread across multiple files.\n\nAssume the LLM has already determined the name of the file `file_path` that it wants to read. Let us zoom into the functions (tools) provided to the LLM to signal intentions to the CU.\n\n- **`read_files(file_path)`** requests the CU to output into the context window $C^t$ the contents of `file_path`.\n- **`write_files(file_path, content)`** requests the CU to create or overwrite `file_path` and populate it with `content`.\n\nAs the empirical results demonstrate, these functions perform well as components of Code-L2MAC. The future work appendix (app. I) discusses how these functions could be optimized.\n\nTo address the remainder of your question, Code-L2MAC's reading and writing functionality is different from previous memory-augmented LLMs (we refer to the extended related work for more details, app. D) as these can be categorized as:\n\n1. **Append only memory**: These methods explicitly extend the implicit knowledge of LLMs through an external corpus (Zhong et al. 2022b) to facilitate long conversational or document summarization tasks (Liang et al., 2023).\n2. **Key-value memory**: These methods interface the LLM with a dictionary or a database (Hu et al., 2023), which does not apply to our automatic coding tasks.\n\n**UPDATE**: In full agreement with your comment about a lack of a central and thorough description of the read/write components, we provide a **(new) appendix C.1** in line with the above description to this end, labeled _\"Read / Write Implementation Details\"_.\n\n\n---\n### **(B) Extending and Describing the Benchmark**\n\nWe agree that having a larger suite of experiments would be desirable to benchmark our method. To that end, as described in (**R2)** in the global response, we _extended_ the _benchmark_ with _three new_ programming tasks: coding a recipe platform, a financial tracker, and an event planner application. The results obtained on these tasks align with those in the original tasks; Code-L2MAC consistently outperforms other methods. See **(R2)** in the global response for the precise results.\n\nOur choice of tasks aims at diversity, and we rerun each method-task pair on ten seeds to provide meaningful confidence intervals in our results. The low variance of inter-task performance between tasks for each method and the consistent SOTA performance of Code-L2MAC suggests that these results are solid and representative of an extensive suite of tasks. We provide a full description of the benchmarks in appendices E and F.\n\n**UPDATE**: Per the reviewer's comments, we **duplicated** the number of **tasks.** We included the **results in a (new) appendix J** entitled \"Additional Diverse Programming Code Generation Tasks\" and extended the benchmark description accordingly in appendix E.\n\n---\n### **(C) Typos**\n\nThank you for spotting the typos and the misordering of the figures.\n\n**UPDATE**: These issues have been solved now."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8334/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700174567565,
                "cdate": 1700174567565,
                "tmdate": 1700188497690,
                "mdate": 1700188497690,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yv4GxbyBzQ",
                "forum": "EhrzQwsV4K",
                "replyto": "Cta1Uxf4Ub",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8334/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8334/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer mgbF [Part 2/3]"
                    },
                    "comment": {
                        "value": "### **(D) Questions**\n---\n**(D.1) Figure 4 (c) Interpretation**\n\nIn Figure 4 (c), we show the tests that failed in red on top of the ones that passed in green for all methods. As you point out, Code-L2MAC accumulates very few errors. This is because when a new test fails or a previous one breaks, Code-L2MAC addresses the failures to _recover consistency_ in the code. In contrast, previous methods leave failures largely unattended, meaning failures accumulate as the code base grows and the number of passed tests stays low.\n\n**UPDATE**: We updated the caption to clarify this source of confusion by specifying they are **\"stacked histograms\"**. The above explanation of this result can be found in the last paragraph of section 6.\n\n---\n\n**(D.2) Reflection in Code-L2MAC**\n\nA unique characteristic of our checks is that they are not only aimed at validating the immediate output (e.g., existing reflecting LLM methods) but rather at imposing _consistency_ on the file store/memory as a _whole_. This implies that while in previous settings, a failing test would require a change in the LLM's output, in Code-L2MAC, it can motivate revisiting and refactoring a pre-existing component in the file store to enhance its functionality and accommodate new use cases.\n\nSpecifically, we find the following differences between Code-L2MAC and reflecting LLM methods, such as Self-Refine and Reflexion to be:\n\n- Self-Refine refines the _most recent_ output from the LLM and _Reflexion_ the _recent outputs_ that are only within the LLM's current context window $C^t$; whereas Code-L2MAC can refine the entire file store, encompassing all previous outputs from the LLM\u2014allowing it to fix or improve earlier outputs from multiple instructions back, outside its context window, which are now erroring due to a new code implementation change. This enables Code-L2MAC to generate long, consistent, and interrelated code structures.\n- Self-Refine and Reflexion are constrained to operating on an output within the context window constraint $c$, whereas Code-L2MAC can manage a total output greater than the context window constraint through the L2MAC framework.\n\n**UPDATE**: Allow us to kindly reiterate that our central contribution is introducing the first practical LLM-based stored-program computer. As part of that, as you point out, our implementation incorporates concepts of refinement that should be properly contrasted with previous work. We previously did so in the _related work_ (section 5) and the extended related work (appendix D). As a consequence of this question, we have improved this comparison by including existing reflecting LLM methods as baselines against Code-L2MAC (see R2 in the global response), and we also improved the extended related work by including a version of this discussion in the (app. D).\n\n---"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8334/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700174608019,
                "cdate": 1700174608019,
                "tmdate": 1700188930731,
                "mdate": 1700188930731,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uTJHil6SvZ",
                "forum": "EhrzQwsV4K",
                "replyto": "Cta1Uxf4Ub",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8334/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8334/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer mgbF [Part 3/3]"
                    },
                    "comment": {
                        "value": "**(D.3) Replanning**\n\nWe appreciate you bringing this up since we agree that this item needs clarification. First, we agree that allowing for (1) replanning if the original instructions prove to be an ineffective plan and (2) subdividing an instruction if needed would increase the flexibility of L2MAC. Indeed, the last sentence in Section 7 (Conclusion & Future Work) and the first item in appendix I (Future work) defer this extension as exciting future work.\n\n- **Section Conclusion & Future Work** : \"... other aspects such as complex instruction flows, re-programming the instructions, \u2026 pose exciting open directions for future work.\"\n- **Appendix I: Future work** : \"... another possible way to avoid out-of-context errors is to dynamically further break a sub-task down into smaller sub-tasks and modify the existing stored prompt-program instructions to include these \u2026\"\n\nGiven the already successful empirical results, we deem these improvements not central to this paper; however, both provide fertile ground for future work.\n\nNonetheless, we traced the concern of your question back to the following sentence in the Related Work (when discussing Autonomous Agent LLMs) _\"When coding, these agents reprogram and reformulate their step plan at runtime, resulting in frequent deviations from the task and stalls in infinite loops (Wang et al., 2023; Sato et al., 2023)\"_ which might be interpreted to suggest this is an inherent flaw in any method that admits reprogramming. However, the message we want to convey is not that the problem lies in the possibility of replanning or subdividing steps but instead in trusting the requirements of the task to be stored in context (as opposed to managed by both the Memory and a CU) while allowing for replanning. This combination can lead, as we show in the experiments (in particular, Figure 4.a.), to the original requirements being altered by the LLM, leading to a misaligned or potentially altogether different task. As mentioned, we observe this happening throughout both the original and the new tasks.\n\n**UPDATE**: We have updated the problematic sentence in the Related Work to \"When coding, these agents reprogram and reformulate their step plan at runtime without safeguarding the original intentions, resulting in frequent deviations from the task.\". Additionally, we have expanded our discussion in Future Work (app. I) regarding the exciting future work that might spark from exploring the possibility of replanning and recursively breaking down sub-tasks.\n\n---\n\n**References:**\n\n- Zexuan Zhong, Tao Lei, and Danqi Chen. Training language models with memory augmentation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 5657\u20135673, 2022. 13\n- Xinnian Liang, Bing Wang, Hui Huang, Shuangzhi Wu, Peihao Wu, Lu Lu, Zejun Ma, and Zhoujun Li. Unleashing infinite-length input capacity for large-scale language models with self-controlled memory system. arXiv preprint arXiv:2304.13343, 2023.\n- Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, and Hang Zhao. Chatdb: Augmenting llms with databases as their symbolic memory. arXiv preprint arXiv:2306.03901, 2023.\n- Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023.\n- Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik R Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8334/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700176063595,
                "cdate": 1700176063595,
                "tmdate": 1700189141345,
                "mdate": 1700189141345,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "38gtG2v65M",
                "forum": "EhrzQwsV4K",
                "replyto": "uTJHil6SvZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8334/Reviewer_mgbF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8334/Reviewer_mgbF"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for author comments and changes"
                    },
                    "comment": {
                        "value": "Thanks to the authors for extensive comments and changes to the paper. These have addressed my questions."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8334/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700346543229,
                "cdate": 1700346543229,
                "tmdate": 1700346543229,
                "mdate": 1700346543229,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]