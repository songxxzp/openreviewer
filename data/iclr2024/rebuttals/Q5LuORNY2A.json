[
    {
        "title": "Implicit NNs are Almost Equivalent to Not-so-deep Explicit NNs for High-dimensional Gaussian Mixtures"
    },
    {
        "review": {
            "id": "fsspYHXvTK",
            "forum": "Q5LuORNY2A",
            "replyto": "Q5LuORNY2A",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission275/Reviewer_yQj9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission275/Reviewer_yQj9"
            ],
            "content": {
                "summary": {
                    "value": "The authors investigate a connection between explicit and implicit neural networks. They provide an asymptotic behavior of the implicit CK and NTK matrices. They show that given an implicit NN, there exists a shallow explicit NN with quadratic polynomial activatin functions that is equivalent to the implicit NN."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The topic is relevent to the comunity. The paper is well-written."
                },
                "weaknesses": {
                    "value": "Providing more explanations about related work can be helpful for readers. For example, briefly summarizing the result of Gu et al. may be helpful."
                },
                "questions": {
                    "value": "Queations:\nHow does the assymptotic behavior stated in Theorems 1 and 2 depend on K and p?\n\nMinor comments:\n- Eq. (1): There are two \"+\" symbols.\n- In the beginning of Section 3, \"Theorem\" should be \"Theorems\"."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission275/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission275/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission275/Reviewer_yQj9"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission275/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698441302048,
            "cdate": 1698441302048,
            "tmdate": 1699635952984,
            "mdate": 1699635952984,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zU7DkfQb64",
                "forum": "Q5LuORNY2A",
                "replyto": "fsspYHXvTK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission275/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission275/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for his/her feedback and positive assessment of our work.\n\n1. **more explanation about related work**: \nWe thank the reviewer for the valuable suggestion. \nWe have added more discussions on the CK and NTK in the related work section, and in (the current) Remark 1 to better motivate the GMM in Eq. (7) and Assumption 3\nA summary of the results in (Gu et al., 2022) is also included. \nDue to space limitation, the summary is placed in **Section D** of the revised supplementary material. \nMoreover, we have fixed the typos pointed out in your minor comments.\n\n2. **dependence on K and p**. In this paper, we consider $K$ to be fixed and of order $O(1)$ for $n,p$ both large. \nSo in particular, the data dimension $p$ is not significantly small than the sample size $n$.\nThe assumption of $n,p \\to \\infty$ with $p/n \\to c \\in (0,\\infty)$ in Assumption 3 is imposed so that the present analysis is performed in a (mathematically rigorously) high-dimensional asymptotic setting. \nThe conducted analysis here does **not** demand that $n,p$ be growing variables but merely that they be *both* large (for the large $n,p$ asymptotics to be accurate, say).\nAs such, one may freely assume $p$ large but fixed and $n$ growing, without altering the conclusions of our present study (the approximation error for our results would then be of the order $O(p^{-1/2})$ instead of $O(n^{-1/2})$ in Theorem 1 and 2). \nThis is in complete adequacy with modern machine learning practice where the data dimensions are generally in hundreds or even thousands. \nThe GMM under Assumption 3 has been extensively studied in the literature of high-dimensional ML for a wide class of models ranging from kNN, LDA, SVM, and shallow neural network models, see [D1], Section 2 of [D2], and Section 4.1 of [D3]. \nWe have added the current **Remark 1** in the revised version for further discussion on this point.\n\n\n[D1] Couillet, R., Liao, Z., & Mai, X. (2018, September). Classification asymptotics in the random matrix regime. In 2018 26th European Signal Processing Conference (EUSIPCO) (pp. 1875-1879). IEEE.\n\n[D2] Blum, A., Hopcroft, J., & Kannan, R. (2020). Foundations of data science. Cambridge University Press.\n\n[D3] Couillet, R., & Liao, Z. (2022). Random matrix methods for machine learning. Cambridge University Press."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission275/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700112600477,
                "cdate": 1700112600477,
                "tmdate": 1700112600477,
                "mdate": 1700112600477,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "j5XitIfOZT",
            "forum": "Q5LuORNY2A",
            "replyto": "Q5LuORNY2A",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission275/Reviewer_9wKK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission275/Reviewer_9wKK"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to study implicit Neural Networks (INNs) through the lens of Conjugate Kernels (CKs) and Neural Tangent Kernels (NTKs).\nWith technical assumptions, among which the fact that the data is sampled from a Gaussian Mixture Model, it is shown that the CK and NTK matrices of INNs are asymptotically determined by a handful of parameters that can be derived from the activation function, the variance of the model weights, the input data and its statistics. \nIt is also shown that a 2-layer deep explicit neural network can be constructively designed to have CK and NTK matrices that match with that of a given INN.\nFinally, experiments on real and synthetic data show the eigenspectral behaviours of the estimated and approximate CK and NTK matrices, as well as the performance of the designed explicit neural networks compared to the original INNs."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- the problem tackled is important: trying to understand INNs in a theoretical way is critical to using them at their fullest capability\n- experiments on real and synthetic data are important\n- assumptions are clearly listed before each theoretical results"
                },
                "weaknesses": {
                    "value": "- **Code**: code is not provided to reproduce the experiments. To me it's a huge problem because it is a hinderance to reproducibility and I don't think there are reasons that would prevent the code to be shared.\n- **Interest of CK and NTK**: I am mainly familiar with INNs, not so much with CK and NTK. This is why it's not apparent to me simply reading the paper why characterizing CK and NTK is a good proxy to understand the overall behaviour of INNs. I think the sentence trying to explain this is \"which serve as powerful analytical tools for assessing the convergence and generalization properties of suf\ufb01cienlt wide NNs\" but it reamins unclear to me. To be perfectly clear, I do think that they could be important, I just don't understand why in this context. The related work part about NTK is also unclear to me.\n- **Bias in INNs**: in eq (1), there is no bias as opposed to Feng and Kolter 2020. I think it would be important to comment on why this parameter is gone from the parametrization and how having it would affect the proof.\n- **Assumption 3.**: assumption 3 is not commented at all. I think it's super important to comment all of these aspects especially those related to the asymptotic part. For example, the dimensionality of the data increases with the number of data points: this is very unusual and typically breaks the PAC-learning framework. Similarly, asymptotically, the mean of the data is 0 (if I understand correctly), since there is no bias in the formulation of the INNs, this does not cover all cases by a simple translation, so I think it's important to comment on that as well.\n- **Explicit NNs**: one of the conclusions I draw from section 3.2 is that it is possible to design explicit NNs that are virtually equivalent to INNs. It is also said that these would be much more efficient computationally than the corresponding INNs: if the widths were roughly equivalent this would be true, but I don't see any mention of this. If they are not, I think it needs to be proved theoretically. In any case, I think some experiments could show this easily by just computing the time of the forward pass.\n- **Relevance of the experiments**: several aspects are to me questionnable:\n   1. It seems to me that the experiments do not illustrate the result of Theorem 1 or 2 which are asymptotic. Here a single snapshot for a given pair $(p, n)$ is given rather than a plot showing $\\|G^\\star - \\bar{G}\\|$ as a function of $n$ even for simulated data. Of course this would be harder for real data since $p$ has to grow with $n$, but it also shows that this assumption is questionnable. Moreover, this would give some context for the values of $\\|G^\\star - \\bar{G}\\|$ rather than have absolute values.\n   2. I don't understand why the eigenvalues become an important part of the experimental section even though they were not mentioned in the theoretical part. Why can't we just focus on $\\|G^\\star - \\bar{G}\\|$? Moreover, the remarks on the comparison of eigenspectral behavior (page 8, second paragraph) are not quantitatively grounded. For example, I could argue that the eigenvalues distributions shown in the right panels of Figure 1 are not similar looking : the second bar in the red plot is much higher than the second bar in the left plot.\n   3. It is mentioned that the gap between ReLU-ENN and L-ReLU-ENN is noticeable but it needs a zoom to be seen and no error bars are given to assess this more quantitatively. Moreover, the performance on CIFAR-10 is extremely low on the order of 60%, very far from even basic networks on this problem: this is a very worrying sign that maybe some procedures are not conducted properly (parametrization, optimization, evaluation, ...). At the very least this should be commented, but it shows that the experiments are very far from a practical setting and it bears the question of whether these results would hold on SoTA INNs. I want to clarify that I understand it's not the matter of this paper to beat SoTA in any shape or form, but the numbers on CIFAR-10 are so low that it's an orange flag to me.\n\nMinor:\n- there are a lot of typos -> grammarly or Ltex workshop can help reduce these (or even the use of LLMs)\n- Since the computational efficiency of INNs is a matter here, I think it would be interesting to cite relevant literature that tries to accelerate the training and inference of such networks (Neural DEQ solver [E], SHINE [D], Jacobian-free backprop [B], warm start [A, C])\n- It is said \"Following (Feng and Kolter, 2020), we denote the corresponding Implicit-CK as\": here I think it would be important to explain that this is not a definition per-se but rather a property of the CK of INNs derived by Feng and Kolter 2020, and also give the initial definition.\n- There are a lot of notations, which is very confusing for any reader IMO, I think a lot could be improved by moving the NTK results in the appendix and only mention them in the core text, and move a lot more of the intermediate computations in the appendix.\n- In the paragraph \"The existence and the uniqueness of Implicit-CKs and Implicit-NTKs\", $z$ are not introduced before (I think they are only in the appendix), and it's confusing because another $z$ is used to define INNs in eq (1) and (2).\n- the label vector $j$ could be called one-hot-encoded label vector this would help readers understand what it is more than the dirac notation I think.\n- remark 1 is proved in the appendix, so it should be mentioned.\n\n\nRefs:\n[A] Micaelli, Paul, et al. \"Recurrence without Recurrence: Stable Video Landmark Detection with Deep Equilibrium Models.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n[B] Fung, Samy Wu, et al. \"Jfb: Jacobian-free backpropagation for implicit networks.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 36. No. 6. 2022.\n[C] Bai, Shaojie, et al. \"Deep equilibrium optical flow estimation.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\n[D] Ramzi, Zaccharie, et al. \"SHINE: SHaring the INverse Estimate from the forward pass for bi-level optimization and implicit models.\" arXiv preprint arXiv:2106.00553 (2021).\n[E] Bai, Shaojie, Vladlen Koltun, and J. Zico Kolter. \"Neural deep equilibrium solvers.\" International Conference on Learning Representations. 2021."
                },
                "questions": {
                    "value": "- why are some assumptions named condition?\n- why is it important to study CKs to show equivalence between INNs and ENNs?\n- is it possible to have a bias in the parametrization of INNs in this work?\n- how relevant is assumption 3?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Research integrity issues (e.g., plagiarism, dual submission)"
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "details_of_ethics_concerns": {
                    "value": "This is somewhat minor in the sense that it's the only part I checked, but I noticed that the first paragraph of the introduction as well the first part of the second paragraph of the introduction are copy-pasted with a few language changes from \"Global Convergence of Over-parameterized Deep Equilibrium Models\" Ling et al. 2023 (I happen to be reading it for another review).\nOf course it's only the introduction and it's not like it's a major part of it, but it raises concerns as to other parts of the submission, especially since this work is not even cited..."
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission275/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698606672695,
            "cdate": 1698606672695,
            "tmdate": 1699635952815,
            "mdate": 1699635952815,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lki1Inkbzy",
                "forum": "Q5LuORNY2A",
                "replyto": "j5XitIfOZT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission275/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission275/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for his/her careful reading and valuable feedback.\nIn the following, we provide a step-by-step response to all comments raised by the reviewer.\nWe hope in light of our responses the reviewer will consider raising the score accordingly.\n\n1. **Code**: We have uploaded the code; please see the `INN\\_Equiv\\_ENN` folder in the supplementary material.\n\n2.  **Interest of CK and NTK**: \nThe CK and NTK have been studied extensively in the literature to assess the convergence and generalization properties of deep neural networks models. \nIn short, NTK is a specific kernel defined in the context of (deep) neural networks. \nDuring (gradient descent) training, the network parameters change and the NTK also evolves over time.\nIt has been shown by (Jacot et al. 2018) and follow-up work that for sufficiently wide neural networks trained on gradient descent with small learning rate, (i) the NTK is approximately constant after initialization and (ii) running gradient descent to update the network parameters is **equivalent** to kernel gradient descent with the NTK.\nThis duality allows one to assess the **training dynamics, generalization, and predictions** of wide neural networks as closed-form expressions involving eigenvalues and eigenvectors of the NTK.\nAs already mentioned in Section 1.1, the NTK has been studied for different types of DNN ranging from convolutional, graph, to recurrent and implicit NN, see (Feng and Kolter, 2020).\nWe have also expanded the related work discussion on NTK in Section 1.1 on how it applies to assess DNN models.\n\n3. **Bias in INNs**: \nWe thank the reviewer for pointing this out. This is indeed a limitation of the present analysis in INN.\nFor the moment the proposed theoretical framework is *not* able to cover deterministic and/or random bias.\nTo the best of our knowledge, the only work on precise high-dimensional asymptotics of DNN models that has taken the bias into account is [C1], but only on a single-hidden-layer explicit neural network model.\nIt would be of future interest to extend the proposed analysis framework to cover deterministic or random bias, which may lead to further improvement on the network performance.\n\n4. **Assumption 3**: \nThe assumption of $n,p \\to \\infty$ with $p/n \\to c \\in (0,\\infty)$ is imposed so that the present analysis is performed in a (mathematically rigorously) high-dimensional asymptotic setting. \nThe conducted analysis here does **not** demand that $n,p$ be growing variables but merely that they be *both* large (for the large $n,p$ asymptotics to be accurate, say).\nPrecisely, our derivations and results use this technically convenient means to approximately quantify the INN performance achieved for all large $n,p$ pair. \nAs such, one may freely assume $p$ large but fixed and $n$ growing, without altering the conclusions of our present study (the approximation error for our results would then be of the order $O(p^{-1/2})$ instead of $O(n^{-1/2})$ in Theorem 1 and 2). \nThis is in complete adequacy with modern machine learning practice where the data dimensions are generally in hundreds or even thousands. \nThe GMM scaling (by $\\sqrt p$) and the growth rate in Assumption 3 are commonly used in the literature of high-dimensional ML for a wide class of models ranging from kNN, LDA, SVM, and shallow neural network models, see [C2], Section 2 of [C3], and Section 4.1 of [C4]. \nWe have added the current **Remark 1** in the revised version for further discussion on this point.\n\n5. **Explicit NNs**: \nIn Section 4, we have compared in Figure 3 the performance of INNs, L-ReLU-ENNs and ReLU ENNs with the **same** width $m$.\nWe have added the sentence \"Implicit and explicit NNs share the *same* network width $m \\in { 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192 }$\" to further clarify this setup.\nMoreover, as suggested by the reviewer, we compared the time costs of the inference and training of INNs and ENNs. \nThe inference time cost of an INN is about $25\\times$ that of an ENN with the same dimension.\nThis is due to the fact that it takes around 25 iterations for an INN to reach the iteration error threshold (1e-3). \nAdditionally, we observe that ENNs have remarkable advantage over INNs in terms of the training speed. \nWe report a brief result in the following table (each NN is trained for $150$ epochs).\n\n|        | $m$         | 256     | 1024    | 8192 |\n|--------|-------------|---------|---------|------|\n|CIFAR-10| ENN         | 0.096h    | 0.097h    | 0.111h |\n|        | INN         | 1.667h | 2.21h | 11h  |\n|MNIST   | ENN         | 0.089h    | 0.095h   | 0.097h |\n|        | INN         | 1.333h | 2h      | 10h  |\n|F-MNIST | ENN      |  0.089h    | 0.095h   | 0.097h |\n|        | INN         | 1.5h | 2h      | 10h  |\n|        |          |  |      |   |"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission275/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700112124770,
                "cdate": 1700112124770,
                "tmdate": 1700112237397,
                "mdate": 1700112237397,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "o3Gej70QuY",
                "forum": "Q5LuORNY2A",
                "replyto": "j5XitIfOZT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission275/Reviewer_9wKK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission275/Reviewer_9wKK"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to thank the authors for engaging in the rebuttal process.\nI will answer point by point subsequently:\n\n- **Code**: the code has indeed been provided now, but it's not runnable as is (with the default instructions, for mnist it runs): I get the following error `FileNotFoundError: [Errno 2] No such file or directory: './pretrained/cifar10_vgg.pth'`. It seems that the pretrained directory has not been included, and probably shouldn't be because of its size. It would be better to give a way to obtain the weights from the internet (anonymously). Moreover, I would suggest removing all potential sources of de-anonymization like comments in the original language.\nFurthermore, I think it's important to follow the NeurIPS guidelines established [here](https://nips.cc/Conferences/2020/PaperInformation/CodeSubmissionPolicy), in particular point 7: for now we don't know what the dependencies are, and we don't know how to reproduce the figures because the code is not present.\n- **Questions about the code**: 1) It seems that the 2 classes `Explicit` (which is used for the leaky relu experiment) and `Explicit_relu` (used for the relu experiment) are the same. I therefore think that the difference between the 2 that is noticed in the paper might only be due to a different seed, indeed no seeding mechanism is used in `w_matrix = torch.randn(input_dimension, dim).to(args.device)`. It is very important to address this point because it is one of the main arguments of the paper that the ReLU network is less performant. 2) The experiments are not run over multiple seeds. It is therefore difficult to know if the observed gap is significant or not. 3) I am not sure I understand the implementation of the explicit NNs: the inner weight matrix is fixed and not learned. Why this choice? How does it correspond to the paper? \n- **Construction of the explicit NN**: It became apparent to me after reading the code that actually this part of how to exactly construct the explicit NN was not clearly laid out in the paper. How are the weights set? The paper just mentions the activations part.\n- **Bias**: I apologize: apparently Feng and Kolter also do not use a bias. Still I think it is important to comment on why this is problematic in the proof. I also think it's important to mention it clearly in the paper.\n- **Assumption 3**: \n> As such, one may freely assume $p$ large but fixed and $n$ growing, without altering the conclusions of our present study (the approximation error for our results would then be of the order $O(p^{-\\frac12})$ instead of $O(n^{-\\frac12})$ in Theorem 1 and 2). This is in complete adequacy with modern machine learning practice where the data dimensions are generally in hundreds or even thousands\n\nI think this is a misleading proposition. $O(p^{-\\frac12})$ only makes sense when $p \\to \\infty$, otherwise it's just a constant and does not give the intended result which is that the approximation error should be small when the number of data points increases. In modern machine learning practice the data dimension is generally fixed (although it may be huge).\nI think however, that as other works (for example Mei and Montanari's), it would be useful to comment on the rate of the 2 limits.\nIt would be interesting also to comment on why it is needed intuitively to use high dimensional asymptotics.\n- **Experiments**: I think it's nice to see this result: how can I reproduce it with the code? Further as I mentioned in my original review, could it be possible to do the same with real data? To me this result should replace Figure 1. which does not show anything quantitative and does not illustrate Theorem 1.\n- **Text duplication**: I do not think this paper is a severe case of plagiarism, as indeed I only saw the introduction being copy-pasted. However, I would say that even in the current revised version, it's problematic that the introduction is so similar to Ling et al. : it's now not copy-pasted but slightly reworked."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission275/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700321214388,
                "cdate": 1700321214388,
                "tmdate": 1700321234018,
                "mdate": 1700321234018,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FIL43MvGbn",
                "forum": "Q5LuORNY2A",
                "replyto": "j5XitIfOZT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission275/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission275/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for his/her reply. \nWe are glad that some of the reviewer's concerns have been successfully addressed. \nWe would like to further emphasize that our major contribution is:\n1. to build a mathematically rigorous and **explicit** connection between explicit and implicit network model by focusing their CK and NTKs (that depends on the network design, activation, and data statistics, all in a rather explicit fashion); and\n2. to provide empirical evidences to support the theoretical insight obtained from the aforementioned theory.\nThis explicit theory comes (of course!) at the cost of some technical assumptions, for example the high-dimensional GMM setting, the NTK (that is computed from random weights), etc.\nGiven that said, we  believe it is of interest and important to present this result to the INN community, which, to the best of knowledge, is the first **explicit** result on the equivalence between explicit and implicit NN to the community.\n\nThe remaining comments mainly involved the definition and implementation of CK and NTK, the high-dimensional asymptotic setting considered in this paper, as well as some details to reproduce the figures.\nThey are addressed respectively as follows.\n\n**Clarifications on CK, NTK and their implementations**:\nWe would like to further clarify that, per their definition in Eqn. (4)-(6), the CK and NTK matrices are defined and computed for network having random weights, for both implicit and explicit networks, see Assumption 1 and the sentence \"As in Assumption 1, we assume that weights matrices Ws have i.i.d. entries ...\" after Eqn. (20).\nThis is in perfect accordance with the line of works on NTK, see (Jacot et al., 2018) and (Feng and Kolter, 2020), in which the NTK has been shown to be approximately constant (over time) after random initialization for wide networks trained on gradient descent with small learning rate, see also the related work discussion in Section 1.1 of the revised version.\n\nIn our experiments, we compare the eigenspectral behaviors of CK/NTK matrices with their approximations and/or with their explicit counterparts.  \nBy definition (and as stated **explicitly** in the caption), these matrices are computed for network having random weights (with expectation numerically estimated from sample means).\nFor Figure 3, we have added a clarification sentence that \"only the final readout layer of both implicit and explicit networks are trained, with all intermediate layer weights fixed at random, as in line with the NTK literature.\"\nWe are running additional experiments that train *all* weights (instead of only the last readout layer) of the network. \nNote that such additional experiments is of (empirical) interest only to networks of small width $m$ since, for wide networks, the NTK convergence results established in (Jacot et al., 2018) and (Feng and Kolter, 2020) ensure the closeness of the performance between networks with random or trained inner weights matrices.\n\n\n**Assumption 3**: We thank the reviewer of his/her further comment. We apology if our previous reply has introduced further confusion to the reviewer, below are some further clarifications:\n1. the present work is in the same vein as (Mei and Montanari, 2021) in the sense that here, in the high-dimensional asymptotics setting $n,p \\to \\infty$, the two dimensions retain a constant ratio $p/n \\to c \\in (0,\\infty)$ as in (Mei and Montanari, 2021);\n2. for the reviewer to have a better grasp of the implications of our theoretical result, we would like to argue that the number of data points **cannot** go to infinity either. \nIn fact, in ML practice and for a given problem, the number of data points itself **cannot** increase (or at least, cannot increase to go beyond a certain prefixed number).\nAnd the approximation errors given in this paper (as well as in, e.g., Mei and Montanari, 2021) should be understood a (concentration-type) bounds saying that, with high probability, the error is bounded by some expression that is a **decreasing** function of both $n$ and $p$.\nA detailed description on how such bound explicitly depends on $n,p$ is beyond the scope of this paper.\nHere, we only proved that in the high-dimensional limit as $n,p \\to \\infty$ with $p/n \\to c \\in (0,\\infty)$ as in (Mei and Montanari, 2021), the approximation error vanishes at a rate of $O(p^{-1/2})$ or $O(n^{-1/2})$."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission275/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700387756722,
                "cdate": 1700387756722,
                "tmdate": 1700387937278,
                "mdate": 1700387937278,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "XNO0uipdjY",
            "forum": "Q5LuORNY2A",
            "replyto": "Q5LuORNY2A",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission275/Reviewer_9F3o"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission275/Reviewer_9F3o"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the connection between implicit and explicit neural networks. The authors show that, with well-designed activation functions and weight variances, a relatively shallow neural network is equivalent to the implicit neural network in terms of eigenspectra of the neural tangent kernel and conjugate kernels. Additionally, the authors complement their theoretical results with numerical experiments."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper studies an important question, that is, whether implicit neural networks have advantages over explicit neural networks. The authors is well written and easy to follow and without so many errors. The steup and results are clear."
                },
                "weaknesses": {
                    "value": "1. A primary concern is the paper's ambitious claim, that is, **any** implicit neural networks is equivalent to relatively shallow networks in terms of the eigenspectra of NTK and CK. While the paper establishes this equivalence in Theorems 1 and 2, it does so under highly specific conditions. These conditions involve setting the variance hyperparameters to exceptionally small values, leading to forward propagation acting as a contraction mapping and converging at an exponential rate. This configuration may restrict the expressive capacity of implicit neural networks, effectively making them resemble relatively shallow networks. However, this setup is common. Consequently, it raises doubts about the validity of drawing broad conclusions based on this specific scenario. For instance, Neural ODEs, which are also categorized as implicit neural networks, do not adhere to Condition 1.\n\n2. Another concern arises from the authors' requirement that the input data $x$ follow Gaussian mixtures. This approach might be restrictive since, for large p, the mean $\\mu/\\sqrt{p}$ and covariance $C/p$ tend to zero, resulting in inputs that are all close to zero. Under these conditions, Theorems 1 and 2 hold as the authors also assume both n and p approach infinity at the same rate. In such cases, the network may struggle to distinguish inputs $x$, making it less relevant to assess whether a network is shallow or deep. I'm not an expert in Gaussian mixture models, so please feel free to correct me if my understanding is incorrect. Additionally, I plan to review comments from other experts in this area if available.\n3. The paper defines intermediate transitions in equations 1 and 2 but doesn't provide a clear definition of the neural network itself. This omission is notable because it's essential to have a precise understanding of the network's structure. Additionally, it's worth noting that $z^*$ cannot be considered the network's output, as its dimension varies depending on the network's design.\n4. There is a typo in Equation (10) concerning the definition of T.\n5. While the paper extensively studies NTK and CKs, it would be beneficial to complement the theoretical analysis with experiments that demonstrate the behavior of $|G^*-\\Sigma^2|$ for *finite-width* networks using simulations. This would provide more practical insights into the implications of the findings."
                },
                "questions": {
                    "value": "See the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission275/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission275/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission275/Reviewer_9F3o"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission275/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698713098548,
            "cdate": 1698713098548,
            "tmdate": 1700594410926,
            "mdate": 1700594410926,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yB6nQtDyd9",
                "forum": "Q5LuORNY2A",
                "replyto": "XNO0uipdjY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission275/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission275/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for his/her careful reading and valuable feedback that helps us improve the paper.\nIn the following, we provide a step-by-step response to all comments raised by the reviewer.\nWe hope in light of our responses the reviewer will consider raising his/her score accordingly.\n\n1. **ambitious claim**: We definitely agree with the reviewer that our proposed analysis is specific to implicit neural networks built upon contractive mapping, e.g., DEQs, and the term \"any implicit neural network\" can indeed be misleading.\nIn the revised version, we have added the sentence \"by focusing on a typical implicit NN, the deep equilibrium model (DEQ)\" in the introduction to clarify. And we have replaced \"any implicit neural network\" with **a given DEQ model** in the paper to make our statements more precise. \nWe would also like to point out, as consolidated by the reviewer, that our Condition 1 on the variance parameter is commonly used and is consistent with previous studies on over-parameterized DEQs, see [B1-B3].\n\n2. **requirement on the input data following Gaussian mixtures**:\nWe would like to kindly point out that the Gaussian mixture model (GMM) in Equation (7) is nothing but standard multivariate Gaussian distribution $\\mathcal{N}(\\mu,C)$ in $\\mathbb{R}^p$ normalized by $\\sqrt{p}$.\nThis is to ensure that the data vectors are of bounded Euclidean norm (with high probability), and is in line with previous studies on over-parameterized explicit or implicit NNs, see [B1-B2] and [B4-B5].\nIndeed, the GMM in Equation (7) and Assumption 3 have been studied in the literature of high-dimensional statistics and have been shown solvable for a wide class of machine learning (ML) models ranging from kNN, LDA, SVM, and shallow neural network models, see, e.g., [B6], Section 2 of [B7], and Section 4.1 of [B8].\nIn particular, it is known (see [B6] and Section 2 of [B7]) that different ML methods with different hyper-parameters can lead to different classification performance under Assumption 3. \nSuch theoretical results, as also in the case of our paper, can be exploited to the design of ML models and/or hyper-parameter tuning.\nWe have added the current **Remark 1** to provide further discussion on this point.\n\n\n3. **transitions in Eqs. (1) and (2) and network structure**:\nWe thank the reviewer for this valuable suggestion. \nIn the revised paper, we have clarified, after Eq. (2), that the network's output is given by $f(\\mathbf{x})=\\mathbf{a}^\\top\\mathbf{z^*}$ with readout vector $\\mathbf{a}\\in \\mathbb{R}^m$. \nThis, together with Eq.(1) and (2), provides a complete definition of a fully-connected DEQ model.\n\n4. We have fixed typos in the revised paper.\n\n5. **simulations on finite-width networks**:\nIn Section D of the supplementary material of the revised paper, we provide in Table 1 additional experiments on the spectral norm difference $\\|G^*-\\Sigma^{(2)}\\|$ for finite-width INNs.\nThe experiment is conducted on GMM data under the same setting as Figure 4 and is repeated follows.\n\n|                        |       |       |      |      |    |      |      |  |    |\n|------------------------|-------|-------|------|------|------|------|------|------|------|\n| $m$                    | 32    | 64    | 128  | 256  | 512  | 1024 | 2048 | 4096 | 8192 |\n| $\\|G^*-\\Sigma^{(2)}\\|$ | 17.32 | 10.24 | 6.53 | 3.71 | 1.60 | 0.93 | 0.81 | 0.12 | 0.12 |\n|                        |       |       |      |      |    |      |      |  |    |\n\nWe particularly observe that (i) the difference $\\|G^*-\\Sigma^{(2)}\\|$ for finite-width ploy-ENN decreases with the increase of the network width $m$, and (ii) the approximation saturates at a low level ($\\sim 0.12$) and this is due to the finite $n, p$ as opposed to our asymptotic theoretical results in Theorem 1 and 2."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission275/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700111523087,
                "cdate": 1700111523087,
                "tmdate": 1700111523087,
                "mdate": 1700111523087,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "elHe36acsr",
                "forum": "Q5LuORNY2A",
                "replyto": "cvb8bTZWnp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission275/Reviewer_9F3o"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission275/Reviewer_9F3o"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the authors' response to my concerns; however, they remain unresolved. After reviewing other reviewers' comments, I've encountered further concerns.\n\nSpecifically, reviewer 9wKK raises questions about CK and NTK, where CK refers to the NNGP kernel for neural networks under the establishment of NNGP correspondence. In this context, it pertains to the NNGP kernel for DEQ. Notably, the neural network DEQ is **not initially defined** in the authors' first submission but is now clarified after equation (2) in their revision. This NNGP kernel or CK aims to characterize the limiting covariance of $f(x_i)$ and $f(x_j)$ as width $m \\rightarrow \\infty$. Essentially, it represents the limit of $\\langle z_i^*, z_j^* \\rangle$ as $m \\rightarrow \\infty$. However, it's important to note that this limit is **not** $G_{ij}^*$ as introduced in [1], where DEQs are not officially defined either; rather, it should be $G_{ij}^* - \\sigma_b^2 \\langle x_i, x_j \\rangle$, a precise derivation presented in [2]. Unfortunately, the subsequent analyses and results heavily rely on this foundational result, and a notable **flaw** has been identified. Given this, I've chosen to maintain my score at this moment.\n\n[1] Feng, Zhili, and J. Zico Kolter. \"On the neural tangent kernel of equilibrium models.\" arXiv preprint arXiv:2310.14062 (2023).\n\n[2] Tianxiang Gao, Xiaokai Huo, Hailiang Liu, Hongyang Gao. \"Wide Neural Networks as Gaussian Processes: Lessons from Deep Equilibrium Models.\" NeurIPS 2023."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission275/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700594378800,
                "cdate": 1700594378800,
                "tmdate": 1700594378800,
                "mdate": 1700594378800,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7t7P90txpo",
            "forum": "Q5LuORNY2A",
            "replyto": "Q5LuORNY2A",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission275/Reviewer_aCT6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission275/Reviewer_aCT6"
            ],
            "content": {
                "summary": {
                    "value": "The authors show that the conjugate kernel (CK) and the neural tangent kernel (NTK) of implicit NNs can be approximated by surrogate kernels, based on an operator norm, when the data are distributed according to a particular Gaussian mixture model. In addition, they show how to construct shallow explicit NNs for which the associated CK and NTK are close to the surrogate kernels of the implicit NNs. The claim is that any implicit NN can be approximated by a shallow explicit NN. In the experiments, they demonstrate the performance of the proposed explicit NNs compared to the original implicit NNs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The idea of constructing efficient explicit NNs to approximate implicit NNs is very interesting.\n- The technical part of the paper seems solid and sensible, however, I have not verified the theoretical results.\n- In the experiments, the proposed approximate explicit NNs seem to work as expected similar to the original implicit NNs."
                },
                "weaknesses": {
                    "value": "- The paper is fairly well written for a mainly theoretical work. However, I think that the text can be improved to become more accessible to non-experts in the implicit NNs field. For example, the proof of Corollary 1 can be moved to appendix to save space.\n- The theoretical results rely on many assumptions, for example, the distribution of the data to follow this particular Gaussian mixture model. When $p$ is high, it seems that the associated GMM becomes degenerate."
                },
                "questions": {
                    "value": "The surrogate kernels $\\overline{G}$ and $\\overline{K}$ are supposed to be random matrices, but in which sense, as they seem to be expectations."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission275/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission275/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission275/Reviewer_aCT6"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission275/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699036262120,
            "cdate": 1699036262120,
            "tmdate": 1700644227892,
            "mdate": 1700644227892,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "K6KzLiCaJo",
                "forum": "Q5LuORNY2A",
                "replyto": "7t7P90txpo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission275/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission275/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for his/her careful reading and valuable feedback.\nIn the following, we provide a step-by-step response to all comments raised by the reviewer.\n\n1. **text can be improved to become more accessible**: \nWe thank the reviewer for this helpful suggestion and we have moved the proof of Corollary 1 to Section D in the supplementary material and provide only a proof sketch in the main text of the revised paper.\n\n2. **about the assumption on GMM data**: \nWe would like to kindly point out that the Gaussian mixture model under study in Equation (7) and Assumption 3 is *not* degenerate for $p$ large. \nInstead, it is nothing but standard multivariate Gaussian distribution in $\\mathbb{R}^p$ normalized by $\\sqrt{p}$.\nThis normalization is in line with the previous studies on over-parameterized explicit or implicit NNs, see [A1-A2] and [A3-A5], and more generally with the literature of high-dimensional statistics and random matrix theory. \nThis is necessary to ensure that (with high probability) the Euclidean norm $\\| \\|x_i\\|\\|$ of data vectors remains bounded in the high-dimensional limit as $n,p \\to \\infty$ (also for large but finite $p$, see Section 2 of [A6] and Section 4.1 of [A7]). \nUnder the normalization in Equation (7),  the growth rate in Assumption 3 is necessary and can in fact be shown to be optimal in a Neyman\u2013Pearson sense (e.g., Gaussian mixture with closer means is indistinguishable for large $p$),  see [A8]. \nWe have added the current **Remark 1** to further elaborate on this point.\n\n3. **randomness in $\\bar{G}$ and $\\bar{K}$**: \nThe expectations in the definition of CK $G$ and NTK $K$ are taken with respect to the random weights, and the randomness from the input GMM data still exists. \nConsequently, $G$ and $K$, as well as their approximations $\\bar{G}$ and $\\bar{K}$,  are still random matrices for random input GMM data. \nWe have added the current **Footnote 1** to clarify.\n\n\n[A1] Mei, S., & Montanari, A. (2022). The generalization error of random features regression: Precise asymptotics and the double descent curve. Communications on Pure and Applied Mathematics, 75(4), 667-766.\n\n[A2] Bubeck, S., & Sellke, M. (2021). A universal law of robustness via isoperimetry. Advances in Neural Information Processing Systems, 34, 28811-28822.\n\n[A3] Ling, Z., Xie, X., Wang, Q., Zhang, Z., & Lin, Z. (2023, April). Global convergence of over-parameterized deep equilibrium models. In International Conference on Artificial Intelligence and Statistics (pp. 767-787). PMLR.\n\n[A4] Truong, L. V. (2023). Global Convergence Rate of Deep Equilibrium Models with General Activations. arXiv preprint arXiv:2302.05797.\n\n[A5] Feng, Z., & Kolter, J. Z. (2023). On the neural tangent kernel of equilibrium models. arXiv preprint arXiv:2310.14062.\n\n[A6] Blum, A., Hopcroft, J., & Kannan, R. (2020). Foundations of data science. Cambridge University Press.\n\n[A7] Couillet, R., & Liao, Z. (2022). Random matrix methods for machine learning. Cambridge University Press.\n\n[A8] Couillet, R., Liao, Z., & Mai, X. (2018, September). Classification asymptotics in the random matrix regime. In 2018 26th European Signal Processing Conference (EUSIPCO) (pp. 1875-1879). IEEE."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission275/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700110988297,
                "cdate": 1700110988297,
                "tmdate": 1700110988297,
                "mdate": 1700110988297,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0qX2HZCVUK",
                "forum": "Q5LuORNY2A",
                "replyto": "K6KzLiCaJo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission275/Reviewer_aCT6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission275/Reviewer_aCT6"
                ],
                "content": {
                    "title": {
                        "value": "Post-rebuttal"
                    },
                    "comment": {
                        "value": "I would like to thank the authors for answering my questions and improving the manuscript based on the comments of the reviewers. However, after reading the rest of the reviews, there seem to be some technical flaws that cannot be addressed with a few updates in the current manuscript. Therefore, I will reduce my score to 5, as I still like the general idea but the paper is not ready for publication."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission275/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700644195411,
                "cdate": 1700644195411,
                "tmdate": 1700644195411,
                "mdate": 1700644195411,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]