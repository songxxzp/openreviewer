[
    {
        "title": "Variational Learning of  Gaussian Process Latent Variable Models  through  Stochastic Gradient Annealed Importance Sampling"
    },
    {
        "review": {
            "id": "NSLdf1TtBP",
            "forum": "fCQe7ei2f5",
            "replyto": "fCQe7ei2f5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission333/Reviewer_G6Yr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission333/Reviewer_G6Yr"
            ],
            "content": {
                "summary": {
                    "value": "The traditional algorithm for learning Bayesian GPLVMs is limited to simple data structure and faces challenges for high-dimensional data. Compared with variational inference (VI), importance-weighted sampling (IS) provides a more directed way of estimating and maximizing the marginal log-likelihood. To increase the effectiveness of this estimator, this paper proposes a sequence of bridge proposal distributions for sampling the latent variable $H$, and develops the corresponding stochastic gradient annealed algorithm. Results on the toy datasets show outstanding performance of AIS compared to other baseline methods. Besides, the new model and algorithm is able to learn the model with missing data and get a better prediction for the unseen data."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The introduction of a sequence of bridge distribution for the proposal distribution is interesting and effective for increasing the performance of the model\n* Maths are introduced step-by-step, which is clear and intuitive.\n* Algorithms are presented in a clear way.\n* Tables in these paper are good, showing the better performance of the newly propsosed ULA-AIS method compared with other two baselines."
                },
                "weaknesses": {
                    "value": "* The notation $\\tilde p(X, H)$ is a bit confusing since this is actually an estimator of the marginal $p(X)$, but it looks like a joint distribution.\n* The claim in the introduction \"the dimension of the additional latent variable is limited to one\" from the reference paper is only for deep GP. The latent of GPLVM has no such strong drawback.\n* Typo: undajusted.\n* No definition of the abbreviation: LV-GP or LVGP, MH, HMC.\n* Overall, the experiments are okay and the results are good. However, the experiments are not that adequate to fully convince me the validatiy of the new methods with the other two baseline methods. For example, is it possible to compare them on a synthetic dataset that the data is really from GPLVM. By this, we will fully understand the new method improves the learning performance of GPLVM than other traditional solvers, rather than some other reasons.\n* In summary, a detailed analysis on these seems to be necessary, since introducing a sequence of bridge distribution for the proposal distribution is delicate, complicated and needs to be thouroughly explained and clarified for readers.\n* The quality of the experiment results can be improved, see questions."
                },
                "questions": {
                    "value": "* Below Eq. 7, what is $q_k(H_{k-1})$? Should that be $q_{k-1}(H_{k-1})$?\n* Is the inverse length-scale plot in Fig. 1 shows the result learned by ULA-AIS? Is it possible to also visualize the class label in the left plot, so that it can show us the latent recovery accuracy.\n* For Fig. 2 and Fig. 3, what about the reconstruction results from the other two baseline methods? And what about the latent space recovered by the other two baseline methods?\n* For Fig. 3 left, the bottom is the true data and the top row is the predicted? Figures in this paper are not explained clearly in their corresponding captions. Color usage, image order, method order, and etc sometimes make me confused."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission333/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission333/Reviewer_G6Yr",
                        "ICLR.cc/2024/Conference/Submission333/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission333/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698518227806,
            "cdate": 1698518227806,
            "tmdate": 1700690651071,
            "mdate": 1700690651071,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JjApoPUiGw",
                "forum": "fCQe7ei2f5",
                "replyto": "NSLdf1TtBP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission333/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission333/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' response"
                    },
                    "comment": {
                        "value": "Weaknesses:\nThe notation \n \nR:Thank you for your comment. We appreciate your careful review of our work. You are correct in pointing out this. After rechecking the symbol, we have corrected the notation $\\hat{p}(H,K)$ to avoid confusion. \n\nThe claim in the introduction \"the dimension of the additional latent variable is limited to one\" from the reference paper is only for deep GP. The latent of GPLVM has no such strong drawback.\n\nR: We apologize for the misunderstanding. In the revised version, we have removed the statement claiming that the dimension of the additional latent variable is limited to one and provided a new explanation.\n\nTypo: undajusted.\nNo definition of the abbreviation: LV-GP or LVGP, MH, HMC.\nR: We have already corrected these typos. Please refer to the rebuttal revision for the updated version.\n\nOverall, the experiments are okay and the results are good. However, the experiments are not that adequate to fully convince me the validatiy of the new methods with the other two baseline methods. For example, is it possible to compare them on a synthetic dataset that the data is really from GPLVM. By this, we will fully understand the new method improves the learning performance of GPLVM than other traditional solvers, rather than some other reasons.\n\nR: Thank you for your suggestion. Our experimental dataset is primarily constructed following previous GPLVM verification methods. Through this approach, we ensure that our experimental dataset exhibits similar characteristics and distributions to traditional methods, enabling fair and accurate comparisons. \n\nWe acknowledge and agree with the suggestion to use synthetic datasets generated by GPLVM as an excellent way to compare the performance of our new method against other benchmark methods. In line with this, we plan to synthetically generate additional simpler datasets for further validation, which will be included in the final version of our work. \n\nQuestions:\n\n1.Below Eq. 7...\n\nR1:  We have reviewed the equation and found no errors. $q_k(H_{k-1})$ represents the probability density of a specific bridging density on $H_{k-1}$.\n\n2.Is the inverse length-scale plot in Fig. 1 shows the result learned by ULA-AIS? Is it possible to also visualize the class label in the left plot, so that it can show us the latent recovery accuracy.\n\nR2: The inverse length-scale plot shown in Fig. 1 does indeed represent the results learned by ULA-AIS. We appreciate the reviewer's suggestion to visualize the class label in the left plot. However, due to space limitations, we have included the results in the appendix. Please refer to the rebuttal revision for further details.\n\n3.For Fig. 2 and Fig. 3, what about the reconstruction results from the other two baseline methods? And what about the latent space recovered by the other two baseline methods?\n\nR3: Thank you for the reviewer's inquiry regarding Fig. 2 and Fig. 3. As per your request, we have included the reconstruction results from the other two baseline methods and the recovered latent space visualizations in the rebuttal revision. Please refer to the appendix section in our response for more detailed information\n\n4.For Fig. 3 left, the bottom is the true data and the top row is the predicted? Figures in this paper are not explained clearly in their corresponding captions. Color usage, image order, method order, and etc sometimes make me confused.\n\nR4: Thank you for the reviewer's question regarding Fig. 3. We apologize for the lack of clarity in the captions, which caused confusion. Indeed, in Fig. 3 left, the bottom row represents the true data, and the top row depicts the predicted data by our AIS method on the MINIst dataset. To address this issue, we will provide clearer explanations for all figures in the final version, ensuring that readers can accurately understand the color usage, image order, and method arrangement\n\nWe would like to inform the reviewer that we have made the necessary modifications as per your request. We have addressed concerns related to the clarity of figure captions and have taken steps to improve them for better understanding. Additionally, we have incorporated the requested visualizations for the reconstruction results from other baseline methods and the recovered latent space. We kindly request the reviewer to reconsider our paper in light of these revisions. Thank you for your time and valuable feedback."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission333/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700067808267,
                "cdate": 1700067808267,
                "tmdate": 1700067808267,
                "mdate": 1700067808267,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wOSzU1zXhP",
                "forum": "fCQe7ei2f5",
                "replyto": "JjApoPUiGw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission333/Reviewer_G6Yr"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission333/Reviewer_G6Yr"
                ],
                "content": {
                    "title": {
                        "value": "Reply to author's response"
                    },
                    "comment": {
                        "value": "Thank you for your detailed response. I still have the following questions.\n\n* Q2:\n    * Fig. 1 left still lacks legend (i.e., color-class label). From Fig. 1 right, readers would assume red, green, and blue are different methods. If the different colors in Fig. 1 left represent different class labels, please clarify it explicitly.\n    * Although ULA-AIS's neg ELBO is the best after 3000 iterations, the latent visualization in Fig. 1 is not visually better than that in Fig. 7 and Fig. 8. In that case, is the neg ELBO correctly reflects the significant superiority of ULA-AIS?\n* Q3:\n    * Same as Q2, is there any significant difference between the three methods, in terms of reconstruction? If so, please tell out explicitly. This is a dimensionality reduction task, but it seems like the dimensionality reduction results from the three methods look similar.\n* References and Appendices are mixed up right now.\n* Q4\uff1a\n    * Same as Q2, is there any significant difference between the three methods, in terms of reconstruction?\n\nDiscussions on these further questions are highly appreciated. Thanks."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission333/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700374172768,
                "cdate": 1700374172768,
                "tmdate": 1700374172768,
                "mdate": 1700374172768,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eHdnTjEtib",
                "forum": "fCQe7ei2f5",
                "replyto": "NSLdf1TtBP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission333/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission333/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' additional response"
                    },
                    "comment": {
                        "value": "Fig. 1 left still lacks legend (i.e., color-class label). From Fig. 1 right, readers would assume red, green, and blue are different methods. If the different colors in Fig. 1 left represent different class labels, please clarify it explicitly.\n\nR: Thank you for your input. We appreciate your feedback on the need for a legend in Figure 1. We have revised the figure to include a legend, and the updated version has been included in the rebuttal revision.\n\nAlthough ULA-AIS's neg ELBO is the best after 3000 iterations, the latent visualization in Fig. 1 is not visually better than that in Fig. 7 and Fig. 8. In that case, is the neg ELBO correctly reflects the significant superiority of ULA-AIS?\n\nR: Thank you very much for your review comments on our paper. We understand your concerns regarding the difficulty in visually demonstrating the superiority of our method through the visualization of the latent space. Additionally, you have also raised questions about the effectiveness of negative ELBO as an evaluation metric. Let me explain further.\n\nFirstly, the visualization of the latent space offers only one intuitive way to visually represent the clustering and distribution of data points in the latent space. However, it does not provide a comprehensive evaluation metric like negative ELBO. Negative ELBO reflects the degree of match between the model and the true data distribution, which complements the results of latent space visualization. We emphasize the importance of negative ELBO to ensure a comprehensive evaluation of the model's fit.\n\nSecondly, our ULA-AIS method not only performs well on negative ELBO but also has other advantages, such as smaller MSE and lower Negative Expected Log Likelihood . These aspects are demonstrated in the experimental results shown in Table 1.\n\nThirdly, we do acknowledge that in our previous experiments, some improper adjustments of certain hyperparameters, due to our oversight, led to less than ideal visualization results. We apologize for the confusion caused. Given this, we have taken your advice and retested the model to achieve better visualization results. We have updated the images of the latent space structure in the rebuttal revision to address this. A comparison of the updated Figures 1, 6, and 7 reveals that while all three methods achieve data dimensionality reduction and reconstruction, our method shows better clustering results. For example, the boundaries between different categories appear clearer, and there are fewer misclassified samples.\n\nI hope this explanation clarifies our position. If you have any further concerns or questions, please let us know.\n\nSame as Q2, is there any significant difference between the three methods, in terms of reconstruction? If so, please tell out explicitly. This is a dimensionality reduction task, but it seems like the dimensionality reduction results from the three methods look similar.\n\nR: If you are referring to the dimensionality reduction task, we have already addressed that in Q2.\n\nReferences and Appendices are mixed up right now.\n\nR:Thank you for bringing this to our attention. We have separated the References and Appendices in the latest version of the paper to ensure clarity and organization. We appreciate your feedback on this matter.\n\nQ4\uff1aSame as Q2, is there any significant difference between the three methods, in terms of reconstruction?\n\nR: It seems like you asked the same question in Q3, I assume this might be a typo.\n\nWe kindly remind you that the discussion period is coming to an end. If you have any further questions, please feel free to discuss them with us promptly. We are more than happy to answer them for you."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission333/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700402679275,
                "cdate": 1700402679275,
                "tmdate": 1700402711510,
                "mdate": 1700402711510,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hEGFBeuFI5",
                "forum": "fCQe7ei2f5",
                "replyto": "eHdnTjEtib",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission333/Reviewer_G6Yr"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission333/Reviewer_G6Yr"
                ],
                "content": {
                    "comment": {
                        "value": "Sorry for the confusion.\n\nMy Q3 means the third of my original questions.\n> For Fig. 2 and Fig. 3, what about the reconstruction results from the other two baseline methods? And what about the latent space recovered by the other two baseline methods?\n\nSo, is there any significant difference between the three methods, in terms of reconstruction? If so, please tell out explicitly. This is a dimensionality reduction task, but it seems like the dimensionality reduction results from the three methods look similar.\n\nMy Q4 means the fourth of my original questions.\n\n> For Fig. 3 left, the bottom is the true data and the top row is the predicted? Figures in this paper are not explained clearly in their corresponding captions. Color usage, image order, method order, and etc sometimes make me confused.\n\nSo, is there any significant difference between the three methods, in terms of reconstruction?\n\nMany thanks!"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission333/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700682990709,
                "cdate": 1700682990709,
                "tmdate": 1700682990709,
                "mdate": 1700682990709,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "llkn6COjd5",
                "forum": "fCQe7ei2f5",
                "replyto": "NSLdf1TtBP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission333/Reviewer_G6Yr"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission333/Reviewer_G6Yr"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for these answers. I have no further questions. I have raised my score to 6. But I do think there are still space to improve the overall quality of the paper. For example, the error bars in the negative ELBO loss plot; figures layout for better comparison between different methods; etc. Btw, currently the main content has slightly exceeded the 9-page limit. Probably the presentation quality needs to be improved further."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission333/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700690634028,
                "cdate": 1700690634028,
                "tmdate": 1700694987759,
                "mdate": 1700694987759,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Sme0dohn4A",
            "forum": "fCQe7ei2f5",
            "replyto": "fCQe7ei2f5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission333/Reviewer_NbSz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission333/Reviewer_NbSz"
            ],
            "content": {
                "summary": {
                    "value": "The manuscript proposes an annealed importance sampling scheme to perform scalable variational inference in Gaussian process latent variable models. A set of experiments on small datasets and two image datasets compares two variational inference algorithms."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The GPLVM is well-established and a widely used tool.\n * The proposed techniques to achieve efficient sampling is also established.\n * The manuscript comes with code and can hence be easily reproduced."
                },
                "weaknesses": {
                    "value": "* Motivation\n   - The paper does not convincingly motivate potential shortcomings of existing methods that should be overcome by the described approach.\n * Experiments\n   - Proper comparison to other models needs to be improved: e.g. comparison to a standard GPLVM or some other simple density model (KDE) is missing.\n   - Proper analysis of computational effort missing. Runtime analysis.\n * Typos\n   - Abstract: \"tighter\" rather than \"lower\"?, VI is undefined\n   - Intro: \"in high-dimensional spaces\"\n   - Algorithm 1: \"stepsizes\" rather than \"stepsides\"\n   - Section 3.1 last paragraph: \"It is obvious that\", \"Therefore, the first three terms\"\n   - References: Capitalisation needs to be reviewed, e.g. Langevin, Eyring-Kramers, Gaussian, Bayes, Monte Carlo\n   - Citations: Please properly distinguish between textual citation and parenthetical citations. In the manuscript, you only use textual citation."
                },
                "questions": {
                    "value": "* How do you come to the conclusion that your method shows \"more robust convergence\" as claimed in the contributions at the end of the introduction section? Which experiment backs this claim? Figure 4 shows a strong peak shortly before 600 iterations and a strange decay at 350.\n * Is it fair to compare MSE and NELL after a fixed number of iterations? To me, one should either fix the computational budget or compare after convergence. What happens if you evaluate at 5000 iterations?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission333/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699027685214,
            "cdate": 1699027685214,
            "tmdate": 1699635960318,
            "mdate": 1699635960318,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YaiubW2vQ8",
                "forum": "fCQe7ei2f5",
                "replyto": "Sme0dohn4A",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission333/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission333/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' response"
                    },
                    "comment": {
                        "value": "Weaknesses:\n\nMotivation.\n\nR: We highly appreciate your concern about the motivation behind our proposed method and understand your skepticism. In this response, we will provide a detailed explanation of the motivation behind our proposed method and how it effectively overcomes the limitations of existing approaches. For more detailed information, please refer to the introduction section of this paper.\n\nOur research motivation stems from the potential applications of GPLVMs in function estimation and unsupervised learning. Previous studies have mainly focused on training GPLVMs using variational inference (VI) methods and replacing the true probability terms with approximate posterior probability distributions to reduce model complexity.\n\nHowever, we have found that in VI methods, there are still challenges in dealing with high-dimensional spaces and the increase in relative variance as the latent variable dimension increases, due to the limitations of standard importance sampling. These limitations make it difficult to construct a well-performing proposal distribution in high-dimensional spaces, thereby limiting the performance of existing methods.\n\nTo overcome these issues, we propose a new method called SG-AIS, based on the well-established theory of AIS [1]. The advantages of this method are primarily manifested in the following aspects:\n- By introducing SG-AIS, we effectively address the challenges in high-dimensional spaces and reduce the increase in relative variance.\n- The concept and design of SG-AIS are based on the AIS method, but with the introduction of stochastic gradient techniques and annealing strategies, we achieve higher efficiency and better convergence performance. Additionally, we approximate the posterior distribution using Langevin stochastic flow, which leads to significant improvements.\n- We also design an efficient algorithm to reparameterize all variables in the evidence lower bound (ELBO) and propose a stochastic variant algorithm that uses data subsets to estimate gradients. This improves the speed and scalability of the algorithm.\n\nWe demonstrated the superiority of our approach through experiments on multiple benchmark datasets of GPLVMs. Our method not only exhibited tighter variational bounds and higher log-likelihood, but also demonstrated more robust convergence. These experimental results validate the effectiveness and feasibility of our proposed method.\n\nExperiments\n\n1.Proper comparison to other models needs to be improved: \n\nR1\uff1aWe completely understand your expectation for a more comprehensive comparison.\n\nWe chose to compare against Classical Sparse VI based on mean-field (MF) approximation and Importance-weighted (IW) VI methods because these approaches are closely related to our research motivation, which is to better address the posterior inference problem in the GPLVM model. Furthermore, previous work on variational inference for GPLVMs has primarily focused on these two aspects. Hence, our experimental tasks align well with these methods, such as tasks like making predictions in unseen data.\n\nThe standard GPLVM you mentioned may be referring to the original GPLVM work. However, due to the cubic computational complexity of Gaussian processes, it may require more computation time and memory resources. This is why we introduce the sparse inducing point method. We aim to improve the GPLVM model by using these methods and achieve better efficiency in terms of computational resources.\uff08please refer to the text below due to  limited space....\uff09"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission333/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700009238688,
                "cdate": 1700009238688,
                "tmdate": 1700010120347,
                "mdate": 1700010120347,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SA8ttcQWMr",
                "forum": "fCQe7ei2f5",
                "replyto": "Sme0dohn4A",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission333/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission333/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Continuing from the previous text"
                    },
                    "comment": {
                        "value": "2. Runtime analysis.\n\nR2\uff1aWe have updated the time consumption and other relevant information of the proposed algorithm in the \"rebuttal revision\" appendix. In addition, we now present the main results of the experiments in the following table for your convenience:\n\nDataset         |   Method   |    time             |       \n\nFrey Faces    |   MF          |    0.32s           |\n                     \n                      |    IW          |   1.46s (K=5)  | 2.85s\uff08K=10\uff09| 4.06s(K=15) |  5.45s(K=20) | 7.03s(K=25)|\n                      \n                      |  AIS(ours) |   1.53s (K=5) |  2.65s  (K=10)  | 3.79s(K=15)  |  4.80s(K=20) | 5.93s (K=25)\n\n\nIn our experiments, we observed that the time complexity of Importance-weighted (IW) VI and Annealed Importance Sampling (AIS) almost linearly increases with K as K increases.\n\nIn the IW algorithm, the time complexity mainly stems from the K repeated samplings of latent variables to data, which is determined by the time complexity of the GPLVM model itself, O(NM^2). As a result, as we increase the number of samples K, the frequency of repeated samplings increases, leading to a linear increase in time complexity.\n\nIn the AIS algorithm, only one sampling of latent variables to data is required, while the intermediate variable sampling is allocated to the annealing procedure, specifically the computation of Langevin stochastic flow. This sampling process is relatively less complex compared to the time complexity of the GPLVM model itself. Therefore, on this dataset, compared to IW, the time complexity of AIS becomes lower as K reaches a certain threshold.\nTypos.\n\nR: Thank you for identifying the typos in the paper. We appreciate your keen eye for detail, and we will make the necessary corrections. We apologize for any confusion caused by these errors\n\nQuestions:\n\n1. How do you come to the conclusion that your method shows \"more robust convergence\" as claimed in the contributions at the end of the introduction section? Which experiment backs this claim? Figure 4 shows a strong peak shortly before 600 iterations and a strange decay at 350.\n\nR1:The claim of \"more robust convergence\" in our contributions is based on the comparison of our proposed method with the baseline methods in terms of convergence behavior. The experiment that supports this claim is presented in Figure 4.\n\nIn Figure 4, we show the convergence curves for our method and the baseline methods over iterations. The strong peak shortly before 600 iterations that you mentioned is an expected behavior in the optimization process. It indicates that the model is actively exploring the solution space to find the optimal solution. The strange decay at 350 iterations may be due to fluctuations in the optimization process, but it does not affect the overall convergence behavior and the final performance of our method. As we described in the main text\uff1a''This can be attributed to the fact\nthat, by adding Langevin transitions, the algorithm\u2019s variational distribution gradually moves from\nthe current distribution towards the true posterior distribution, resulting in sudden drops in the loss\nfunction when reaching the target distribution. Thus, such phenomena can be regarded as a common\nfeature of annealed importance sampling and it becomes even more obvious in high-dimensional\ndatasets.''\n\n2. Is it fair to compare MSE and NELL after a fixed number of iterations? To me, one should either fix the computational budget or compare after convergence. What happens if you evaluate at 5000 iterations?\n\nR2:As mentioned before, we have already analyzed the time complexity of AIS compared to IW on the Frey Faces dataset. Our proposed method does not show an increase in time complexity compared to the baseline method IW (and sometimes even lower). Therefore, even though we used a fixed number of iterations, we can ensure the fairness of the experiments. Regarding your mention of the results after 5000 iterations, I have already obtained preliminary results, and our method still outperforms the previous method. In response to your feedback, we will include these results, even the results after convergence, in the revised article to address your concerns.\n\nThank you very much for providing us with valuable feedback and suggestions on our research work. We kindly request you to reconsider our paper and recognize the significant improvement in the performance of GPLVM modeling through our proposed method. If you have any further questions or concerns, we would be more than willing to provide additional explanations and experimental evidence to further support our research.\n\nReferences:\n\u30101\u3011Radford M Neal. Annealed importance sampling. Statistics and computing, 11(2):125\u2013139, 2001"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission333/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700009268544,
                "cdate": 1700009268544,
                "tmdate": 1700009725592,
                "mdate": 1700009725592,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "75AqdxJqjp",
                "forum": "fCQe7ei2f5",
                "replyto": "SA8ttcQWMr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission333/Reviewer_NbSz"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission333/Reviewer_NbSz"
                ],
                "content": {
                    "title": {
                        "value": "Acknowledgement of the Rebuttal"
                    },
                    "comment": {
                        "value": "Many thanks for your detailed reply.\n\nThe cubic computational effort should not be an issue for datasets of sizes $n=1000, 1599, 1965, 2163$  so the comparison to \"original GPLVM\" should be feasible. Without this comparison (potentially on a subset of the data), it is hard for me judge the \"superiority\" of your approach."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission333/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700644238173,
                "cdate": 1700644238173,
                "tmdate": 1700644238173,
                "mdate": 1700644238173,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "L1tv32scxz",
                "forum": "fCQe7ei2f5",
                "replyto": "Sme0dohn4A",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission333/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission333/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author's additional response"
                    },
                    "comment": {
                        "value": "Dear Reviewer NbSz,\n\nWe appreciate your feedback and have made significant efforts to address your concerns within the given time frame. Specifically, we have conducted additional experiments comparing our proposed approach to the Standard GPLVM [1]. We performed  experiments 8 times and averaged the results, analyzing the performance (in terms of MSE and NLL) on four different datasets. Due to limited computational resources, we were only able to run the Standard GPLVM on a subset of the image datasets. For the image reconstruction task, we randomly selected 300 images as the training set and used consistent hyperparameters for the other experiments, as stated in our main paper.\n\n\ndataset                                   |       method                |       MSE  |   NLL |\n\nOilflow (1000,12)           |      Standard  GPLVM  |  2.45\uff080.05\uff09  | -12.42\uff080.07\uff09 |\n\n **************                                    |       AIS (Ours)                  | **1.71\uff080.04\uff09** | **-15.81\uff080.04\uff09** |\n*********************************************************************************\nWine Quality (1599,11) |     Standard  GPLVM    |   **30.53\uff080.03\uff09**   |        2.82 (0.02) |\n\n *******************                                    |       AIS (Ours)            |  30.79 (0.04)  | **2.42\uff080.03\uff09**     | \n*********************************************************************************\n\nFrey Faces (300,560) |      Standard  GPLVM    | 130 (7)  | 2632 (6)  |\n\n ******************                                    |       AIS (Ours)            |  **115** \uff086\uff09| **2417\uff085\uff09**|\n*********************************************************************************\n\nMNIST (300,784) |     Standard  GPLVM    |  0.36\uff080.01\uff09   |  -484\uff083\uff09|\n\n ******************                                    |       AIS (Ours)            |**0.31\uff080.01\uff09**|**-496\uff082\uff09**|\n*********************************************************************************\n\n\nIn addition, we would like to emphasize that one of our baselines for comparison is the MF method [2]. As discussed in its experimental section, even the authors of MF approach also suggests that the performance of the Standard GPLVM may not match the Bayesian GPLVM on the Oilflow and Frey Faces datasets. This highlights the advantages of Bayesian methods, which offer greater flexibility, uncertainty modeling, and generalization capabilities compared to the maximum likelihood estimation of the Standard GPLVM. Furthermore, the focus of our paper is to explore a way to combine variational inference, AIS and Bayesian methodology to better estimate the posterior distribution. We hope that this response addresses your concerns and improves your perception of our paper.\n\n\u30101\u3011Lawrence, N. (2003). Gaussian process latent variable models for visualisation of high dimensional data. Advances in neural information processing systems, 16.\n\u30102\u3011Titsias, M., & Lawrence, N. D. (2010, March). Bayesian Gaussian process latent variable model. In Proceedings of the thirteenth international conference on artificial intelligence and statistics (pp. 844-851). JMLR Workshop and Conference Proceedings."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission333/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700666706430,
                "cdate": 1700666706430,
                "tmdate": 1700699739805,
                "mdate": 1700699739805,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "hGQk07U631",
            "forum": "fCQe7ei2f5",
            "replyto": "fCQe7ei2f5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission333/Reviewer_uro2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission333/Reviewer_uro2"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a novel approach to enhance the learning process of Gaussian Process Latent Variable Models (GPLVMs) through the integration of Annealed Importance Sampling within the framework of variational inference. This newly introduced inference methodology is anticipated to provide a more accurate and effective variational approximation, particularly in scenarios involving high-dimensional data. The experimental analysis conducted to evaluate the effectiveness of the proposed approach demonstrates notable improvements over conventional techniques, such as vanilla variational inference and importance-weighted VI.\n\nSpecifically, the results indicate that the proposed method achieves a significant reduction in the negative Evidence Lower Bound (ELBO), a crucial measure of the model's performance. Moreover, the approach yields a notable enhancement in the expected log likelihood and the Mean Squared Error (MSE) of reconstruction, indicating a substantial advancement in the model's ability to accurately represent and reconstruct complex datasets. The evaluation of these results was carried out on diverse datasets, including both digit and face datasets, further highlighting the applicability of the proposed method across various domains."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper's well-organized flow introduces GPLVM models, followed by a comprehensive exploration of variational inference methods, including vanilla and importance-weighted schemes. Its main contribution is the introduction of AIS variational inference using Langevin diffusion. This novel approach effectively addresses dimensionality reduction and prediction tasks in high-dimensional data spaces, showcasing its potential and effectiveness in advancing the field."
                },
                "weaknesses": {
                    "value": "The paper could benefit from a more comprehensive empirical validation, particularly in terms of rigorous experimentation and benchmarking against diverse data sets. Including a broader range of experiments and datasets would provide a more holistic understanding of the proposed AIS-based variational inference methods and their applicability across various contexts.\n\nA more explicit discussion on the assumptions and constraints underlying the AIS variational inference approach would provide a clearer understanding of its potential constraints and practical applicability. This would help in contextualizing the scope and generalizability of the proposed method.\n\nThere are several typos in the paper (but not limited to):\n(p4) the fisrt three term ==> the first three terms\n(p4) jointly optimizes ==> jointly optimize\n\nAddressing these potential weaknesses would not only strengthen the overall credibility of the paper but also provide a more comprehensive and balanced perspective for readers and researchers in the field."
                },
                "questions": {
                    "value": "For experiments with Oilflow dataset, with only three runs, do you think the standard deviation reliable? \nHow many runs for the data reported in Table 2?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission333/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699271003727,
            "cdate": 1699271003727,
            "tmdate": 1699635960203,
            "mdate": 1699635960203,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OEPmR5MfFN",
                "forum": "fCQe7ei2f5",
                "replyto": "hGQk07U631",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission333/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission333/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' response"
                    },
                    "comment": {
                        "value": "Thank you for your valuable feedback and suggestions. We appreciate your emphasis on the need for a more comprehensive empirical validation and benchmarking in our paper. We agree that including a broader range of experiments and datasets would enhance the understanding of our proposed AIS-based variational inference methods and their applicability across different contexts. In the revised version, we will thoroughly scrutinize various experimental settings, including the number of runs conducted, to ensure accuracy and reliability. Thank you for reminding us to pay careful attention to these details.\n\nRegarding the assumptions and constraints underlying the AIS variational inference approach, we apologize for not explicitly discussing them in the paper. We would like to emphasize that the main focus of our paper  is to present our own method, including theory, formulas, experiments, etc., due to the limited space. Additionally, we have appropriately referenced the background of our method and we have included some related work in the supplementary material. In order to address your concerns, in the revised version, we will include a more explicit discussion on the assumptions and constraints, providing a better understanding of their practical applicability. \n\nThank you for identifying the typos in the paper. We appreciate your keen eye for detail, and we will make the necessary corrections, including changing \"the fisrt three term\" to \"the first three terms\" and \"jointly optimizes\" to \"jointly optimize\". We apologize for any confusion caused by these errors.\n\nIn response to your question about the reliability of the standard deviation in the experiments with the Oilflow dataset, we acknowledge that with only three runs, the accuracy and reliability of the standard deviation might be limited. We understand the concerns raised by the small sample size. As discussed earlier, we plan to increase the number of runs in the final version to enhance the estimation of the standard deviation and improve the credibility of our results. In Table 2, the reported data is also based on three runs. We appreciate your concern, and we assure you that we will increase the number of runs in our experiments to address this limitation. Additionally, in the revised version, we will provide additional information regarding the number of runs conducted for the experiments presented in Table 2. Thank you for bringing this to our attention.\n\nThank you very much for providing us with valuable feedback and suggestions on our research work. We kindly request you to reconsider our paper and recognize the significant improvement in the performance of GPLVM modeling through our proposed method. If you have any further questions or concerns, we would be more than willing to provide additional explanations and experimental evidence to further support our research."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission333/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699980100565,
                "cdate": 1699980100565,
                "tmdate": 1700009313535,
                "mdate": 1700009313535,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "XgmYNWBdcp",
            "forum": "fCQe7ei2f5",
            "replyto": "fCQe7ei2f5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission333/Reviewer_e7un"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission333/Reviewer_e7un"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes an Annealed Importance Sampling (AIS) approach for Variational Learning of Gaussian Process Latent Variable Models. This approach addresses the limitations of existing methods in analyzing complex data structures and high-dimensional spaces. The authors introduce a transition density and use a Langevin diffusion to approximate the posterior density. They also propose the usage of the reparameterization trick to simplify gradient computation and suggest employing stochastic gradient descent for sampling. Experimental results demonstrate that their method outperforms state-of-the-art approaches in terms of variational bounds, log-likelihoods, and convergence robustness."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The authors present their contribution alongside a very throrough theoretical discussion. The amount of details provided in the derivation of the proposed method is impressive and it shows the amount of work put forth by the authors.\n\n2. The article is quite well written, and although at some points can be quite dense and difficult to follow, this stems from the amount of information that is trying to be condensed in just a few pages. While this is a double-edge sword and certainly improvements could be made, I think the effort made by the authors is quite clear.\n\n3. The AIS approach allows for the analysis of complex data structures and high-dimensional spaces, which were challenging with previous methods. This increases the applicability and usefulness of Gaussian Process Latent Variable Models (GPLVMs) in various domains by combining Sequential Monte Carlo samplers and Variational Inference (VI). This enables a wider range of posterior distribution exploration, leading to better understanding and modelling capabilities.\n\n4. The authors propose an efficient algorithm by reparameterizing all variables in the ELBO, which leads to a simpler gradient computation and therefore an easier process to optimize model parameters during training.\n\n5. In the experiments performed, the authors show that the proposed method outperforms previous models in different aspects related to its tighter variational bounds, higher log-likelihoods, and more robust convergence. This indicates that their proposed approach is effective at capturing underlying patterns in data."
                },
                "weaknesses": {
                    "value": "1. The usage of annealing techniques as well as MC samplers can mean strong computational demands, especially in high dimensional settings and in complex models. , particularly when dealing with high-dimensional data or complex models. Several experiments are conducted in related settings, and scalability results are only reported in terms of iterations while no information on the hardware used is provided. I expect the authors to provide this information in the final version of the draft.\n\n2. On the same line as the previous point, I expect the authors to release some version of the code used to produce these results. This is not mentioned in the text and I deem reproducibility to be considered an important factor.\n\n3. The selection of hyperparameter values appears to play a crucial role in the performance of the presented method. Finding optimal values for parameters such as step size ($\\eta$) or the selected number of bridging densities ($K$) may prove to be expensive and quite important in the final results. Moreover, even though it appears to be more restricted due to their usage, the choice for the evolution of $\\beta_k$ coefficients, while contained in $\\[0,1\\]$ needs to be properly crafted as well.\n\n4. The proposed approach introduces additional complexity compared to traditional methods for Gaussian Process Latent Variable Models. This may make it more challenging to implement and understand, especially for researchers or practitioners with limited experience in this area.\n\n### Minor:\n\n* Considering the information conveyed in the text, I think the article is quite well written. However, at some points, I think it could be managed so that the discussion flows better. I suggest the authors summarize further the introduction in favour of section 2 so that further details can be provided about the required background since currently it is only touched on lightly and some ideas are pivotal here. On this same line, I suggest the authors include further references in the part of the text surrounding Eqs. 2,3 and 4, especially when mentioning something deemed to be \"the classical MF-ELBO\" or a \"typical approximation\". These are small points, but I deem them relevant nonetheless.\n  \n* There seem to be some small typos throughout the text such as \"fisrt\" on page 4 or using \"d\" instead of \"D\" for dimensions on page 7 (e.g. \"2d projections\" instead of \"2D\"). I do not consider these important corrections at all since the text is very clean, I only suggest doing a final pass to fix these tiny mishaps."
                },
                "questions": {
                    "value": "1. How does the current method scale with the amount of data present in terms of running time? What are the computational requirements to run experiments such as the ones presented in the article? \n   \n2. How sensitive is the method to different choices of reparameterizations?\n\n3. Can you provide more insights into how the proposed annealing process affects the exploration of posterior distributions? What are some potential trade-offs or considerations when choosing an appropriate annealing schedule?\n   \n4. The paper mentions that experimental results demonstrate improved performance on toy datasets and image datasets, but what about other types of data or real-world applications? Are there any known limitations or challenges when applying this approach to different domains? How about running experiments for bigger datasets such as Imagenet?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission333/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699461305235,
            "cdate": 1699461305235,
            "tmdate": 1699635960121,
            "mdate": 1699635960121,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "eskyAjLYTe",
                "forum": "fCQe7ei2f5",
                "replyto": "XgmYNWBdcp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission333/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission333/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' response"
                    },
                    "comment": {
                        "value": "Weaknesses:\n1. The hardware.\nWe have updated our experiments and conducted them on a Tesla A100 GPU in the revised \"rebuttal revision\".\n2. The code.\nIn fact, we plan to publicly release the code and experimental setup related to our research paper upon publication. We will ensure that the code is user-friendly and comprehensible so that other researchers can validate and reproduce our experimental results.\n\n3. The selection of hyperparameter values.\nIn Supplementary Material Appendix D of our paper, we have provided the settings for the number of bridging densities K, step size ($\\eta$), and coefficients ($\\beta_k$). However, we acknowledge that finding optimal values for these parameters still poses challenges, and we look forward to future work addressing your concerns.\n\n4.Time complexity \nWe have updated the time consumption and other relevant information of the proposed algorithm in the \"rebuttal revision\" appendix. Please refer to it for more details.\n\nMinor:  Fluency.\n\nFor the fluency of the article, we have added more references in Section 2 in the \"rebuttal revision.\" Furthermore, we plan to include additional background knowledge in Section 2 in the final version to provide readers with a better understanding of our method.\n\nTypos.\nWe have corrected these typos and will perform a final check.\n\nQuestions:\n\n1. Running time and computational requirements to run experiments.\n\nR1\uff1aThe computational requirements to run experiments, as presented in the article, depend on factors such as the size of the dataset, the number of inducing points, and the desired number of bridging densities (K). The time complexity of the proposed method is $O(nm^2)$, where n is the dataset size and m is the number of inducing points. Additionally, the complexity linearly depends on the number of bridging densities (K) due to the introduction of annealed importance sampling. Hence, the computational requirements will vary depending on these factors. It is important to select these values appropriately to balance algorithmic time complexity and practical effectiveness.\n\n2.How sensitive is the method to different choices of reparameterizations?\n\nR2\uff1a While we have not conducted additional experiments specifically addressing this sensitivity issue, we have carefully considered and compared several reparameterization choices in our original study. We believe that the findings presented in our paper can provide insights into the impact of these choices on the method's performance. As you previously mentioned, the reparameterization method, such as Euler-Maruyama discretization, ''leads to a simpler gradient computation and therefore an easier process to optimize model parameters during training'' . However, we acknowledge that further investigations and experiments can certainly enhance our understanding of the influence of different reparameterizations. We will take this suggestion into account for future studies.\n\n3. More insights into how the proposed annealing process affects the exploration of posterior distributions?  Some potential trade-offs or considerations when choosing an appropriate annealing schedule?\n\nR3: The proposed annealing process in SG-AIS plays a crucial role in exploring the posterior distributions.\n\nIn the context of this paper, the posterior distribution refers to the distribution of the latent variables given the observed data. This distribution is often intractable and challenging to sample from directly. SG-AIS aims to approximate this posterior distribution by transforming it into a sequence of intermediate distributions, which can be more tractable and easier to sample from.\n\nThe annealing process gradually transforms the posterior distribution by introducing a temperature parameter $\\beta$. By annealing from $\\beta = 0$ to $\\beta = 1$, we move from an initial distribution, where the posterior is approximated by a simpler distribution to the target posterior distribution itself. The key idea behind annealing is it allows for a smoother exploration of the posterior space. At each intermediate distribution, we can use importance sampling to estimate the evidence by sampling from the proposal distribution and reweighting the samples using the ratios of the target and proposal distributions.\n\nAs the annealing process progresses, the samples from the proposal distribution gradually become more representative of the target distribution. This means that the exploration of the posterior space is not limited to a specific region but covers a wider range of possible configurations of the latent variables. (Due to limited space, please refer to the text below ...)"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission333/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699975475658,
                "cdate": 1699975475658,
                "tmdate": 1700403054954,
                "mdate": 1700403054954,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NoY0lKiXjq",
                "forum": "fCQe7ei2f5",
                "replyto": "XgmYNWBdcp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission333/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission333/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "The benefit of this exploration is that it allows for a more accurate estimation of the evidence, which corresponds to a tighter lower bound in the variational learning framework. By gradually annealing the temperature and exploring different distributions, SG-AIS can capture more complex structures in the posterior distribution, leading to better variational approximations in complex data and high-dimensional spaces. \n\nWhen choosing an appropriate annealing schedule for Stochastic Gradient Annealed Importance Sampling (SG-AIS), there are several trade-offs and considerations to keep in mind:\n\n1. **Computational Efficiency**: The annealing schedule should be carefully designed to balance the computational resources required for estimating the evidence. Too many bridging densities can lead to excessive computational burden, while too few densities may result in less accurate estimates.\n\n2. **Exploration vs Exploitation**: The annealing schedule should strike a balance between exploration and exploitation of the posterior distribution. An aggressive schedule that moves quickly from the base distribution to the posterior may lead to exploration limitations, while a slow schedule may lead to insufficient exploration and inefficiency.\n\n3. **Smoothness of Transition**: The annealing schedule should ensure a smooth transition between bridging densities. Abrupt changes in the densities can result in high-variance importance weights, which may lead to inaccurate estimates. Smooth transitions can be achieved by gradually adjusting the temperature or using appropriate interpolation functions.\n\n4. **Base Distribution**: The choice of a suitable base distribution is critical for effective sampling in AIS. It should be computationally efficient to sample from and cover the entire support of the target distribution. The base distribution should strike a balance between simplicity and representativeness.\n\nQueation4: The paper mentions that experimental results demonstrate improved performance on toy datasets and image datasets, but what about other types of data or real-world applications? Are there any known limitations or challenges when applying this approach to different domains? How about running experiments for bigger datasets such as Imagenet?\n\nR4:The paper indeed focused on demonstrating the improved performance of SG-AIS on toy datasets and image datasets. However, when applying this approach to other types of data or real-world applications, there may be some limitations or challenges to consider.\n\nOne potential limitation could be the scalability of the method. As the size of the dataset increases, the computational resources required for estimating the evidence using SG-AIS may become more demanding. This is particularly true for large-scale datasets such as ImageNet, which contain millions of images. Running experiments on such massive datasets might pose challenges in terms of computational efficiency and memory requirements. \n\n\nWe would like to emphasize that GPLVM indeed did not utilize ImageNet as a precedent dataset. Given that ImageNet involves higher-dimensional data, it may be more appropriate to combine GPLVM with other deep learning tools, such as convolutional neural networks (CNNs) and transformers. We are currently exploring broader application scenarios to incorporate these tools effectively and leave room for future work. Thank you for bringing up this point, as it is important to consider the specific requirements and complexities of high-dimensional datasets like ImageNet when exploring the applicability of SG-AIS.\n\nAdditionally, the annealing schedule plays a crucial role in the exploration of the posterior distribution. Designing an appropriate annealing schedule may require domain knowledge or trial and error experimentation. It might be necessary to tune the schedule to ensure a balance between exploration and exploitation, as well as a smooth transition between bridging densities. \n\nRegarding the applicability of SG-AIS in real-world applications, its performance may depend on the specific characteristics and requirements of the domain. Different datasets and applications may exhibit unique challenges, such as data sparsity, high dimensionality, or non-linear relationships, which could affect the effectiveness of SG-AIS. Evaluating the performance of SG-AIS in different domains and addressing these challenges would require further experimentation and investigation.\n\nIn summary, further research and experimentation are needed to assess its performance and address these challenges in real-world applications."
                    },
                    "title": {
                        "value": "Continuing from the previous text"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission333/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699977042218,
                "cdate": 1699977042218,
                "tmdate": 1699977114252,
                "mdate": 1699977114252,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VWhBSH6ds3",
                "forum": "fCQe7ei2f5",
                "replyto": "e0vOHMB45N",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission333/Reviewer_e7un"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission333/Reviewer_e7un"
                ],
                "content": {
                    "title": {
                        "value": "Brief response to the authors"
                    },
                    "comment": {
                        "value": "I want to express my gratitude to the authors for addressing my questions and concerns about the paper in such detail. I value the effort put into responding not just to my points but also to those raised by other reviewers. While I believe the contribution is good, there's room for enhancement in the experimental section. Specifically, detailing the limitations of the proposed approach in the regimes of large and/or wide datasets and challenging synthetic experiments could be beneficial. Despite these considerations, based on the information provided, I am inclined to uphold my initial positive score."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission333/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700676325504,
                "cdate": 1700676325504,
                "tmdate": 1700676325504,
                "mdate": 1700676325504,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]