[
    {
        "title": "Understanding Length Generalization by Thinking Like Transformers"
    },
    {
        "review": {
            "id": "Z2BMAvkHTU",
            "forum": "AssIuHnmHX",
            "replyto": "AssIuHnmHX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6895/Reviewer_LyN1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6895/Reviewer_LyN1"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on the standard decoder-only transformers model for algorithmic tasks. Prior work has shown that standard transformers exhibit little length generalization on certain arithmetic tasks. Accordingly, the author first propose the RASP Conjecture to describe the kind of problems on which the transformers can perform length generalizations. The author also present empirical evidence to support the RASP Conjecture and length generalization. Finally, the author analyzes the hard tasks for transformers, such as parity and decimal addition, and proposes particular scratchpad formats to enhance the length generalization on these tasks.\n\n**Post rebuttal update**\n\nHaving reviewed the author's response, I am impressed by the meticulous effort the authors have done in answering questions and refining the manuscript. The work has effectively addressed my initial concerns. As a result, I have raised my point."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The paper is well-written: discussions and presentations are clear. \n\n* The RASP Conjecture is well-motivated.\n\n* The case study clearly explains why certain tasks are solvable (i.e., length generalizable) and why others aren't. For each solvable task, there is a corresponding RASP format.\n\n* The author designs particular scratchpad formats for parity and decimal addition and experimentally verifies the effectiveness of these designs."
                },
                "weaknesses": {
                    "value": "* The scratchpad designs for parity and decimal addition, though effective, are a bit different from the RASP format. For example, it seems that the proposed scratchpad for addition is motivated by correcting the failure modes discussed in section 4.3. It would be nice if the author could clarify the connection between the scratchpad designs and the RASP. \n\n* This paper classifies the algorithmic tasks into easy and hard ones. The easy ones are solvable (i.e., length generalizable) by the standard transformers. As a general question, can the author recommend the possible directions to solve the hard ones? The proposed scratchpad solution requires a detailed understanding of the failure modes for each hard task, which doesn't seem quite scalable.\n\n* Missing citations. Transformers for algorithmic tasks is an interesting topic in the recent literature. It would be nice if the authors could discuss a few more papers. From the modeling perspective, [1] introduces certain twists on the transformer architecture to enable the length generalization on algorithmic/regular language tasks such as Even Pairs, Modular Arithmetic, Parity Check, and Cycle Navigation (i.e., the tasks in [2]). From the task perspective, [3] evaluates the model performance over randomly generated finite-state transduction tasks. The idea of evaluating over randomly generated tasks is complementary to the usual conduct of evaluating on particular tasks.\n\n\n[1] Chi, T.C., Fan, T.H., Rudnicky, A.I. and Ramadge, P.J., 2023. Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation. arXiv preprint arXiv:2305.03796.\n\n[2] Del\u00e9tang, G., Ruoss, A., Grau-Moya, J., Genewein, T., Wenliang, L.K., Catt, E., Cundy, C., Hutter, M., Legg, S., Veness, J. and Ortega, P.A., 2022. Neural networks and the chomsky hierarchy. arXiv preprint arXiv:2207.02098.\n\n[3] Valvoda, J., Saphra, N., Rawski, J., Williams, A. and Cotterell, R., 2022, October. Benchmarking compositionality with formal languages. In Proceedings of the 29th International Conference on Computational Linguistics (pp. 6007-6018)."
                },
                "questions": {
                    "value": "See the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6895/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6895/Reviewer_LyN1",
                        "ICLR.cc/2024/Conference/Submission6895/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6895/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698011343007,
            "cdate": 1698011343007,
            "tmdate": 1700633194083,
            "mdate": 1700633194083,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7brbZwYlAF",
                "forum": "AssIuHnmHX",
                "replyto": "Z2BMAvkHTU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6895/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6895/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response"
                    },
                    "comment": {
                        "value": "Thank you for your time and effort in reviewing our paper! We are delighted that you found our conjecture well-motivated and experimental results compelling. We are grateful that you support the acceptance of this work. We would like to address the comments and questions you\u2019ve raised, and we\u2019d be very open to further discussion.\n\n>The scratchpad designs for parity and decimal addition, though effective, are a bit different from the RASP format. For example, it seems that the proposed scratchpad for addition is motivated by correcting the failure modes discussed in section 4.3. It would be nice if the author could clarify the connection between the scratchpad designs and the RASP.\n\nThank you for pointing this out. We have restructured Section 5 to better explain the connection with RASP conjecture. To summarize, one consequence of the RASP conjecture is that by converting the task into one that has a simple RASP-L solution, we can improve length generalization performance. We can do so by changing the task format, adding a scratchpad, or both. For addition, the standard format does not have a RASP-L solution because it requires index arithmetic, which is forbidden in RASP-L (see Appendix F.2). However, if we add index hints, we can then replace the operations that require index arithmetic (forbidden) with ones that use induction heads (easy to write in RASP-L) to perform indexing. Thus, the new task format can now admit RASP-L solutions. Similarly, this gives us a perspective on why scratchpad has often been found helpful for reasoning problems. In the case of parity, we cannot represent the parity algorithm because RASP-L does not allow arbitrary loops for constant depth Transformers. However, the autoregressive nature of Transformers gives us a way to simulate a simple loop by using a scratchpad. With the scratchpad format, there now exists a RASP-L program that produces the next-scratchpad-token, without requiring any loops. We hope that this clarifies the connection between the proposed designs and the RASP conjecture.\n\n\n>As a general question, can the author recommend the possible directions to solve the hard ones? The proposed scratchpad solution requires a detailed understanding of the failure modes for each hard task, which doesn't seem quite scalable.\n\nThis is indeed a challenging aspect of the framework. Currently predicting or improving length generalization entails finding a RASP-L program that solves the task. We provided a few examples that can be generally more applicable, such as adding index hints for tasks that require complex indexing operations, and scratchpad for unrolling loops in the algorithm. Nonetheless, one promising future direction is to identify \u201cunnatural operations\u201d as suggested by RASP-L, but which are common subroutines for a large class of algorithms, and increase the training data coverage for those operations. This may help the model to perform these operations through memorization, and leverage this to learn other algorithms in a more generalizing way.\n\n>Missing citations\n\nThank you for the suggestions, we\u2019ve added a more thorough discussion in Appendix A.\n\n**Conclusion:**\n\nThank you for taking the time to read our rebuttal. If our response addresses your concerns, we kindly ask that you consider raising your score. Otherwise, please let us know about your remaining concerns/questions so we can make further improvements."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6895/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700093208762,
                "cdate": 1700093208762,
                "tmdate": 1700093208762,
                "mdate": 1700093208762,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "634UeSHVkO",
            "forum": "AssIuHnmHX",
            "replyto": "AssIuHnmHX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6895/Reviewer_q9T5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6895/Reviewer_q9T5"
            ],
            "content": {
                "summary": {
                    "value": "This paper discusses an important problem: in which algorithmic task transformers are more likely to length generalize in which format. \n\nThey propose a conjecture: transformers are likely to length generalize for a task if there exists a \"simple\" \"RASP-L\" program to solve that task. (The conjecture includes other conditions to ensure almost perfect in-distribution performances.) (The \"RASP\" programs are easier to represent and learn by transformers in the sense that their program parameterizations encourage parallel operators and discourage sequential, for-loop, and branch operations. The \"RASP-L\" programs further rule out numerically unstable operations such as they forbid float values and complex token indices processing.)\n\nThe authors also provide some experimental results that empirically align with the conjecture."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper discusses an important topic: the length generalization of transformers. It is easy to understand with many illustrative visualizations. They test the conjecture in six simple tasks whose positive/negative results all align with the conjecture. They also show improved/degraded length generalization performances as guided by the conjecture after reformulating some tasks. \n\nRegarding the concrete conjecture, at least some parts of it align with the reviewer's intuition and experiences. For example,\n* If human beings can implement a simple RASP-L program to solve a task, we do expect better length generalization of learned transformers.\n* Transformers do suffer from numerical issues in practice, either for counting tokens or modeling complex indexing mechanisms. The constraints on \"learnable\" in addition to \"RASP\" are interesting. \n\nThe reviewer also appreciates the simple trick on position embeddings for better length generalization: they concatenate all samples in a long sequence with random shifting while training transformers. This is different from the common choice, which only puts one sample in one sequence, for synthetic tasks. They find that \"packing and shifting the context allows all positional embeddings to be trained, and encourages the transformer to treat all positions symmetrically,\" therefore, towards better length generalization of transformers."
                },
                "weaknesses": {
                    "value": "The main weakness is lacking the support and validation of the conjecture. The definition of RASP-L programs is also vague and unclear.\n\nRegarding the support and validation of the conjecture: \n* There is no proof or reasoning for the conjecture. There is little evidence saying that transformers actually learned or have any relationships with the \"RASP-L\" program in general. According to the experimental results where \"transformers mostly generalize to length at most 50 when trained with length<=40\", it is also hard to believe that the true \"RASP-L\" programs have been learned. \n* The experimental results and comparison may not be solid with confusing metrics:\n   - The metrics should characterize the differences between OOD and IID, instead of the absolute performances on OOD samples. As shown in Figure 1(a), generalization is about the difference in performances between in-distribution and out-of-the-distribution samples. This is important for, e.g., the addition task, because addition with 40 digits may already be out of the representation power of a 6-layer transformer in the naive forward-order. The IID performances could be as bad as the OOD ones. \n  - There is also a lack of constraints on achieving almost perfect in-distribution performances before evaluating their length generalization. \"Almost perfect IID performance\" is assumed in the conjecture. It is also strange to evaluate the OOD performances of random runs with imperfect IID performances. If this constraint is not enforced, the current analysis may confuse the optimization difficulties with the OOD generalization performances. \n   - Similarly, the metric as \"median among 20 runs\" is not enough to fully characterize the transformers' performance, given their non-convex optimization nature. Other metrics, such as \"maximum\" or \"ratio of almost perfect,\" could be more informative and practical. \n  - The tasks are simple: from count to addition/parity. \n* Ablation studies on \"without the index hints\" are needed. \n\nRegarding the format of the conjecture as \"RASP-L\" programs,\n* The learnable constraints are interesting, but there are no reasoning/proof/experiments to support their choices, such as which operators are \"learnable\" in which sense, how to find and choose them, and if the current set of operators can represent all \"learnable\" operations.\n* Similarly, there is no characterization of the function space represented by the \"RASP-L\" programs. \n* The optimization efficiency conjecture in the appendix is also poorly supported. I would suggest the authors remove it if there are no supporting results from more tasks."
                },
                "questions": {
                    "value": "* How did you choose the \"learnable\" operators? Will the representation power of \"RASP\" programs be affected by this additional constraint? Can all \"generalizable\" tasks' solutions be represented by the \"RASP-L\" programs? \n* Are there results on the difference between the IID and OOD performances for most of the tasks?\n* Are there constraints as almost perfect IID performances before evaluating their length generalization performance? Are there results with metrics such as maximum among all runs?\n* Did you achieve almost perfect IID performances for the addition task with 40 digits with the naive forward order? \n* Can you explain why the length generalization performance of transformers is still limited (e.g., from length<=40 to length=50) even when the conjecture conditions are fulfilled?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6895/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6895/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6895/Reviewer_q9T5"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6895/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698615273049,
            "cdate": 1698615273049,
            "tmdate": 1700707354872,
            "mdate": 1700707354872,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rLPGDd5Wwy",
                "forum": "AssIuHnmHX",
                "replyto": "634UeSHVkO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6895/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6895/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response (part 1)"
                    },
                    "comment": {
                        "value": "Thank you for your time and effort in reviewing our paper! We are delighted by your appreciation for the importance of this line of work and for our generalization improvements on addition and parity. We would like to address the comments and questions you\u2019ve raised, and we\u2019d be very open to further discussion.\n\n>The definition of RASP-L programs is also vague and unclear.\n\nWe want to highlight our formal definition of RASP-L programs in Appendix F (Section F.1 defines RASP, and then Section F.2 defines RASP-L based on RASP). There we specify the allowed types and the allowed program syntax for the RASP-L language. We include an informal presentation of RASP-L in the main body for brevity and accessibility, but appreciate that this may require more clarification. We would welcome any specific pointers regarding unclear aspects of the definition as we work to improve the presentation.\n\n**Regarding OOD vs IID performance:** \n\nIn all of our experiments except for parity, we only evaluate models that achieve ~100% accuracy on in-distribution test samples. This includes addition on all training lengths. Parity without scratchpad is a special case as it is well known that Transformers cannot even fit the training set for long parity sequences (e.g. Chiang & Cholak and Bhattamishra et al). All parity with scratchpad experiments are trained to convergence. We hope this fully addresses your concern.\n\n>There is no proof or reasoning for the conjecture. There is little evidence saying that transformers actually learned or have any relationships with the \"RASP-L\" program in general. According to the experimental results where \"transformers mostly generalize to length at most 50 when trained with length<=40\", it is also hard to believe that the true \"RASP-L\" programs have been learned.\n>Can you explain why the length generalization performance of transformers is still limited (e.g., from length<=40 to length=50) even when the conjecture conditions are fulfilled?\n\nWe agree that our work has no theoretical justification for the conjecture; it is currently a purely empirical claim. Nonetheless, we view such empirical work as an important first step towards understanding new phenomena -- a step upon which theory can be built. We hope that our conjectures can eventually be formalized and proven, but we leave this (significant) endeavor for future work.\n\n(As an aside, the reviewer may be interested in our new Appendix H, where we added a theoretical example from boolean function analysis that illustrates our conjecture in a simple case).\n\nWe wish to clarify the scope of our main claims. Our Main Conjecture (Section 2) only characterizes which tasks Transformers are likely to length-generalize on, and not why or how they do so. Specifically, our Main Conjecture only claims that RASP-L is a useful predictive tool: empirically, if a task can be solved by a simple RASP-L program, then it is likely to exhibit strong length generalization, and vice versa. This is a phenomenological claim, as opposed to a mechanistic one. Nonetheless, we believe that the toy model in our introduction suggests a plausible mechanism for Transformer learning, and could guide new lines of mechanistic investigations in future work.\n\n**Re. degradation at longer lengths:**\n\nThere are a number of factors that could influence how robustly the correct solution is learned, if indeed it is. We do not expect (and indeed do not observe) perfect length generalization over arbitrary lengths. Some level of degradation is likely fundamental due to issues trying to approximate a discrete program with a continuous space (e.g. noisy optimization, finite precision, etc). \n\nOne other possible explanation, compatible with the RASP conjecture, is that there exists other \u201cshortcut\u201d solutions which do not generalize, but are simple enough to compete with the true RASP-L program during training. Since it is likely intractable to determine the minimum RASP-L program that fits a given training set, we cannot predict a priori what forms of \u201cdata diversity'' are required to rule out such \u201dshortcuts\u201c, even if our conjecture holds true. Nonetheless, our study focuses on the relevant characteristics of tasks that influence generalization performance, thus even good albeit imperfect generalization lends strong signal to the usefulness of this perspective. We have added several new paragraphs in the revised pdf (in Section 2, and Appendix G), which highlights these limitations for the reader."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6895/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700077504919,
                "cdate": 1700077504919,
                "tmdate": 1700096877665,
                "mdate": 1700096877665,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hgNQA76i2Z",
                "forum": "AssIuHnmHX",
                "replyto": "634UeSHVkO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6895/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6895/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response (part 2)"
                    },
                    "comment": {
                        "value": ">the metric as \"median among 20 runs\" is not enough to fully characterize the transformers' performance, given their non-convex optimization nature. Other metrics, such as \"maximum\" or \"ratio of almost perfect,\" could be more informative and practical.\n\nIn Fig 3, 7b, and 8, we show the individual test performance of all 20 runs (plotted as points). For brevity we default to reporting the median, but most experiments follow the same pattern as those in Fig 3. We hope these plots can give you a fuller picture of Transformer\u2019s performance. If there is a specific additional metric you would like to see, we are happy to compute it on the 20 runs shown.\n\n>The tasks are simple: from count to addition/parity.\n\nOn the positive side (length-generalizing tasks), we agree that it would be interesting to demonstrate more complex tasks which length-generalize. We focused our paper on simple tasks for exposition purposes, and to better connect with the existing literature on length generalization (where such simple tasks are standard). However, our RASP-L framework is more general, and can indeed express more complex tasks than we presented here due to space constraints (such as sorting and finite state automata). We can consider adding such tasks into the full version of this paper.\n\nOn the negative side, we could have included a large number of complex tasks, because most tasks will not length-generalize and will also not have a RASP-L program. A number of more complex tasks have been tested in the literature, where they were shown to fail at length-generalization. These tasks include: dynamic programming, multiplication, and certain kinds of logic puzzles (as tested in Dziri et al. 2023). We thought about these tasks, and could not devise RASP-L programs for them (even for e.g. multiplication), suggesting that they are unlikely to have simple RASP-L solutions. In general, Transformers only seem to length-generalize on very particular types of tasks, and so we focused on identifying positive instances of length generalization, since negative instances abound.\n\n>Ablation studies on \"without the index hints\" are needed.\n\nWe mention briefly in Section 4.3 that no generalization is observed on standard addition or parity (no index hints) under our experimental setting. We evaluate different lengths in increments of 5, and all 20 runs for all length showed a test EM of ~0% on these tasks. For brevity we do not include a plot, but we have added this detail to the updated pdf for better clarity. \n\n>The learnable constraints are interesting, but there are no reasoning/proof/experiments to support their choices, such as which operators are \"learnable\" in which sense, how to find and choose them, and if the current set of operators can represent all \"learnable\" operations.\n\nWe agree that our work does not provide a theoretical justification for the definition of RASP-L. However, we do consider the experiments in the paper as evidence that our RASP-L constraints are meaningful (though not conclusive of course).\nSpecifically, we gave RASP-L programs for all the \u201csymbolic tasks\u201d which were known to length-generalize in the literature (to the best of our knowledge) \u2014 this shows our definition is expressive enough to capture positive instances. Conversely, for tasks which do not length-generalize, we were unable to devise RASP-L programs to solve them: we view this as heuristic evidence that RASP-L is not \u201ctoo expressive.\u201d We\u2019ve added a table in Figure 1 which summarizes all our experiments, to show the RASP-L evidence more clearly.\n\nWe acknowledge that we do not have proofs that tasks (such as Parity) cannot be written in RASP-L, but we note that similar statements about computational complexity classes are notoriously difficult to prove (they amount to circuit-complexity lower-bounds). However, our claims are at least falsifiable: one simply has to give a RASP-L program that solves e.g. Parity, if such a program exists.\n\nRegarding whether the current set of operators can produce all \u201clearnable\u201d operations: Thank you for bringing up this point. We have added a more thorough discussion of this in Appendix G, paragraph \u201cLimitations of RASP-L.\u201d Briefly, we believe RASP-L captures most \u201csymbolic\u201d learnable programs, but it certainly does not capture all learnable programs. A large class of learnable but non-RASPable tasks are numerical algorithms, such as linear regression, which involve high-dimensional matrix and floating-point operations. One could imagine extending RASP-L to encompass numerical algorithms as well \u2014 we consider this an interesting direction for future work.\n\n\n>There is no characterization of the function space represented by the \"RASP-L\" programs.\n\nWe agree that it is an important open question, so we have added this discussion explicitly in Appendix G, paragraph \u201cOn Formal Language Characterizations.\u201d This is also an open question even for the original RASP language of Weiss et al. (2021)."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6895/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700077671104,
                "cdate": 1700077671104,
                "tmdate": 1700093025892,
                "mdate": 1700093025892,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Z4Jht3edXD",
                "forum": "AssIuHnmHX",
                "replyto": "634UeSHVkO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6895/Reviewer_q9T5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6895/Reviewer_q9T5"
                ],
                "content": {
                    "title": {
                        "value": "Need more time to read all"
                    },
                    "comment": {
                        "value": "I appreciate the authors' efforts in the thorough and long rebuttals. Unfortunately, I haven't been able to find time to read all the rebuttals and the appendix referred to in the rebuttal. Here, I will try to frame my current thoughts in case they are useful for the authors' further rebuttal. I will try to read all the rebuttals and the referred appendix before the deadline. \n\nFor concerns regarding experiments, one wield thing for me to believe is that the authors claim to achieve \"perfect\" or \"~100%\" performances for addition with 20/40-digit numbers using a six-layer transformer. The task may even be out of the representation power of the models in the reviewer's understanding. It would be great if the authors could share codes to reproduce these results for double-checking.\n* Also, my main concerns regarding the experiments are whether or not they are strong enough to support the authors' claims without more theoretical analysis. Unfortunately, the concerns still stand for e.g., the only +10 length generalization, the tasks are still simple, and some confusing points as stated above. For the metrics, just for what it's worth, I would recommend plotting figures like Fig. 3 for all experiments, or using more metrics like percentage of \"perfect generalization\" or maximum to show how likely the models can successfully generalize instead of the average/median performances. In some sense, we are characterizing how many local minimums are \"good\" instead of their averaged performances. \n\nFor the definition and formulation of RASP-L, I agree it is difficult to formalize \"learnable\" for neural networks, but this is an important concern from my perspective, given the claims of this paper. I have not checked Appendix G yet. Hopefully, it can help alleviate some of these concerns. \n\nOverall, I appreciate the authors' efforts in studying this important and challenging problem. On the other hand, I would also like to make sure the claims and results are precise and solid. Hopefully, my concerns could be alleviated by reading more of the rebuttals and appendix."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6895/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700514693322,
                "cdate": 1700514693322,
                "tmdate": 1700514693322,
                "mdate": 1700514693322,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Zrg5sZpjW3",
                "forum": "AssIuHnmHX",
                "replyto": "wRUi1xRn53",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6895/Reviewer_q9T5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6895/Reviewer_q9T5"
                ],
                "content": {
                    "title": {
                        "value": "A quick follow-up"
                    },
                    "comment": {
                        "value": "I would guess the authors' responses are about the inverse-order addition task, e.g., those regarding \"we know that a RASP-L program exists to solve this for any length\"? I was more concerned with the naive forward-order addition, in which case the model needs to predict the most significant digit directly as the first token by implicitly reasoning about the carries transferred through all 20/40 digits of both arguments. With causal attention, shared weights and representations of the models, and the large training data space, it is at least non-trivial, from my perspective, to obtain a near-perfect performance for this task with such a small model. It would be great if the authors could share more info or codes (just for this task) to help clarify."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6895/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700535613637,
                "cdate": 1700535613637,
                "tmdate": 1700535613637,
                "mdate": 1700535613637,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "D0BwW1y2TO",
                "forum": "AssIuHnmHX",
                "replyto": "634UeSHVkO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6895/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6895/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Initial response"
                    },
                    "comment": {
                        "value": "Thanks for the clarification question! We actually do mean there is also a RASP-L program for _forward-order_ addition. We've expanded a discussion of this in the updated Section 5.1, which we think you will find interesting. Forward addition *can* in fact generalize when we try harder, e.g. be more clever with the data diversity of the training distribution. The RASP-L programs for reverse and forward addition can be found in Listings 7 and 8. It is unintuitive that forward addition has such a solution, since the natural algorithm is done in reverse order to do the carry dependencies. However, it turns out that we can represent a similar algorithm that computes carry chains through parallel and constant step computations. We think that this is a great illustration of the usefulness and flexibility of the RASP-L perspective. The RASP-L code is hard to parse, so we'll follow up with some more intuition to help guide you through it, but hopefully this answers your question in the meantime.\n\nWe also attach a code sample for the example generation part of the addition task. We removed some unnecessary components that had to do with other ablations but still apologize for the code being more complex than it needs to be. \n\n    def generate_instance(self):\n\n        if self.hard_carry:\n            # hard carry generates questions with carry chains e.g. 9999 + 0003 = 10002\n            num1_len = np.random.randint(self.min_range, self.max_range+1, size=1)[0]\n            num2_len = num1_len\n        else:\n            num1_len = np.random.randint(self.min_range, self.max_range+1, size=1)[0]\n            num2_len = np.random.randint(self.min_range, self.max_range+1, size=1)[0]\n\n        num1 = [np.random.randint(self.min_num, 10, size=1)[0]]\n        if num1_len > 1:\n            num1 = num1 + list(np.random.randint(self.min_num, 10, size=num1_len-1))                \n            \n        if self.hard_carry:\n            num2 = [9-x for x in num1]\n            num2[-1] = np.random.randint(num2[-1]+1, 10, 1)[0]\n        else:\n            num2 = [np.random.randint(self.min_num, 10, size=1)[0]]\n            if num2_len > 1:\n                num2 = num2 + list(np.random.randint(self.min_num, 10, size=num2_len-1))      \n\n        num1 = ''.join(map(str, num1))\n        num2 = ''.join(map(str, num2))\n\n        ndigits = max(len(num1), len(num2)) + 1 # add one to the max (for the carry to be easier)\n        s1, s2 = list(map(lambda q: q.rjust(ndigits, '0'), (num1, num2)))  # padding\n\n        start_id = np.random.randint(0, len(list(self.tokenizer.count_id.values()))-ndigits+1)\n        ids = list(self.tokenizer.count_id.values())[start_id : start_id + ndigits] # contiguous sequence of ids\n\n\n        if self.index_hints:\n            s1_sequence = ' '.join([f'{a} {b}' for a, b in zip(s1, ids)])            \n            s2_sequence = ' '.join([f'{a} {b}' for a, b in zip(s2, ids)])\n        else:\n            s1_sequence = ' '.join(list(str(s1)))            \n            s2_sequence = ' '.join(list(str(s2)))    \n\n        prompt_sequence = f'{s1_sequence} {self.tokenizer.ADD_TOKEN} {s2_sequence}'\n\n        carry = 0\n        sumpairs = []\n        i = 0\n        for d1, d2 in zip(s1[::-1], s2[::-1]): # for all digits (LSB first)\n            n1, n2 = int(d1), int(d2)\n            ans = (carry + n1 + n2) \n            dAns = ans % 10\n            newCarry = 1 if ans >= 10 else 0\n\n            id = ids[ndigits-i-1]\n            sumpairs.append((id, str(dAns)))\n            i += 1\n        \n        if not self.reversed_ans:\n            sumpairs = sumpairs[::-1]\n\n        if not self.answer_index_hints:\n            sumstr = [c for (id, c) in sumpairs]\n        else:\n            sumstr = [c for pair in sumpairs for c in pair] # flatten id, digit\n        answer_sequence = ' '.join(sumstr)\n\n        final_sequence = f'{prompt_sequence} {self.tokenizer.SEP_TOKEN} {answer_sequence}'            \n        return final_sequence"
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6895/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700541105385,
                "cdate": 1700541105385,
                "tmdate": 1700541870625,
                "mdate": 1700541870625,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "k6QEvyStlY",
                "forum": "AssIuHnmHX",
                "replyto": "0nrKGPJKHs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6895/Reviewer_q9T5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6895/Reviewer_q9T5"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for all the thorough replies, the additional results, appendix, and clarifications. I do cherish this paper more with them. I also appreciate the authors' discussions of the limitations of the conjecture. \n\nOverall, I find that many of the discussions and small findings in this paper are interesting and inspiring. I would like to acknowledge them. Even though I am still worried that the main conjecture and the overall framing are that well supported, with the additional limitation discussion in Section 2 and Appendix G, I will increase my rating to 6: marginally above the acceptance threshold. \n\nI would suggest the authors discuss more of the limitations of the conjecture in the main body, as discussed in Appendix G (e.g., limited expressiveness and insufficient complexity measure). They will not diminish the contributions but will give the readers a better understanding of the problem. I also hope the authors will release the codes as stated in the rebuttal."
                    }
                },
                "number": 26,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6895/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700707325998,
                "cdate": 1700707325998,
                "tmdate": 1700707325998,
                "mdate": 1700707325998,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cJxaLFHVsq",
            "forum": "AssIuHnmHX",
            "replyto": "AssIuHnmHX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6895/Reviewer_tXQ4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6895/Reviewer_tXQ4"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a conjecture about when a decoder-only autoregressive Transformer is likely to have strong length generalization ability on symbolic algorithmic tasks. There are three conditions: 1) the true next token function for the task is realizable with a *causal Transformer*; 2) the next token function can be represented by RASP-L (a subset of the RASP language) programs; 3) the training data is sufficiently diverse, which makes sure that the shortest RASP-L program can length-generalize. The conjecture states that Transformer is likely to generalize when the three conditions are satisfied. It is shown that the conjecture is supported by some case studies. On 3 tasks which have simple RASP-L programs, Transformer empirically generalize well with a moderate max train length (e.g., >30 on the *copy with unique tokens* task). On 3 tasks that \"do not admit\" simple RASP-L solutions, Transformers struggle to generalize. The conjecture is then applied to interpret why scratchpads can improve generalization. Specifically, on a case where the scratchpad increases the complexity of the RASP-L program, Transformers show decreased generalization performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- A novel approach for understanding the reasoning and out-of-distribution generalization ability of Transformers on the symbolic tasks.\n- The analysis in the case studies is well supported by empirical results.\n- Experiment details are well introduced."
                },
                "weaknesses": {
                    "value": "- Some concepts are not clearly defined, making their meanings obscure and the statements less rigorous. For example, there is no reference or definition for the \"causal Transformer\" and \"causal masking\"; how to identify whether a RASP-L program is \"simple\" or \"difficult\".\n- The correlation between RASP-L representable and the generalization of Transformer need further discussions. Are the selected cases representative enough? Is there any task which can be represented by \nRASP-L but Transformers hard to generalize, or vise-versa? These answers are needed to make a stronger statement on the correlation between the two.\n- No discussions on the limitations of this work."
                },
                "questions": {
                    "value": "- Please refer to the weakness part.\n- Minor question: Empirical results show that the max train length has a great impact on the generalization ability of Transformers. Is the proposed method helpful in analyzing this phenomenon?\n- Minor suggestion: Some important definitions, e.g., that of RASP-L, would better to be included in the main paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6895/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6895/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6895/Reviewer_tXQ4"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6895/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698773297168,
            "cdate": 1698773297168,
            "tmdate": 1700712960037,
            "mdate": 1700712960037,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "daOw9YAnnD",
                "forum": "AssIuHnmHX",
                "replyto": "cJxaLFHVsq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6895/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6895/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response"
                    },
                    "comment": {
                        "value": "Thank you for your time and effort in reviewing our paper! We are delighted that you find our approach to understanding Transformers novel and well-supported by our experiments. We are grateful that you support the acceptance of this work. We would like to address the comments and questions you\u2019ve raised, and we\u2019d be very open to further discussion.\n\n> Some concepts are not clearly defined, making their meanings obscure and the statements less rigorous. For example, there is no reference or definition for the \"causal Transformer\" and \"causal masking\"; how to identify whether a RASP-L program is \"simple\" or \"difficult\".\n\nThanks for point this out. \u201cCausal Transformers\u201d are also often known as \u201cdecoder-only Transformers\u201d in the literature. We have added a clarification in the updated manuscript. Essentially, with causal masking, self attention modules can only attend to tokens that came before the current token, similar to other unidirectional models. \n\nA proxy for RASP-L program simplicity is the lines of code or lines of operations in the program. We illustrate this concretely with an expanded comparison of forward vs reverse addition in Section 5.1, where we compared the lines of code in the RASP-L program for each, and showed that this corresponds to generalization performance. \n\n>The correlation between RASP-L representable and the generalization of Transformer need further discussions. Are the selected cases representative enough? Is there any task which can be represented by RASP-L but Transformers hard to generalize, or vise-versa? These answers are needed to make a stronger statement on the correlation between the two.\n\nThis is a fair question. Although we cannot prove conclusively that RASP-L covers all cases of generalization success and failures, and in fact there are likely exceptions due to the complex nature of learning and generalization, we have considered most empirical analyses of Transformer length generalization on algorithmic tasks in the literature to the best of our knowledge. We have not observed exceptions in these cases. See Figure 1a for a clearer summary.\n\nGiven that most studies in the literature focus on highlighting length generalization failures, we introduced a number of new tasks to showcase that Transformers do in fact have the ability to show strong length generalization. We have identified more of these positive examples than we had room for in the manuscript, such as sorting and finite state automata. We can consider adding such tasks into the full version of this paper. On the negative side, it is easy to find tasks which do not admit RASP-L programs. Dziri et al. 2023 studied a number of tasks (e.g. dynamic programming, multiplication, and certain kinds of logic puzzles), and showed failures in length generalization. We could not devise RASP-L programs for these algorithms, suggesting that they are unlikely to have simple RASP-L solutions. Nonetheless, RASP-L only covers a subset of possible programs that could be represented by a Transformer. In particular, RASP-L focuses on symbolic tasks. A large class of learnable but non-RASPable tasks are numerical algorithms, such as linear regression, which involve high-dimensional matrix and floating-point operations. We have added a discussion of the limitations in Appendix G.\n\n\n>No discussions on the limitations of this work.\n\nThank you for pointing this out. We have added substantial discussions on limitations of this work. Please see end of Section 2 and Appendix G.\n\n>Empirical results show that the max train length has a great impact on the generalization ability of Transformers. Is the proposed method helpful in analyzing this phenomenon?\n\nIn this work, we used max train length as a measure of training data diversity. As proposed by our Main Conjecture, one condition on length generalization is diversity--- the training data should be sufficiently diverse, such that there does not exist any shorter RASP-L program which agrees with the task in-distribution but not out-of-distribution. Intuitively, a dataset containing only 5 different lengths is more likely to have shortcut solutions than a dataset that contain 50 possible lengths. We have expanded Section 5 to include a deep dive on addition, which includes a section on how increasing data diversity by changing the sampling procedure also leads to improved performance. The Main Conjecture can be used to understand both phenomena (max train length and improved sampling procedures).\n\n\n**Conclusion:**\n\nThank you for taking the time to read our rebuttal. If our response addresses your concerns, we kindly ask that you consider raising your score. Otherwise, please let us know about your remaining concerns/questions so we can make further improvements.\n\nDziri et al: Faith and Fate: Limits of Transformers on Compositionality"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6895/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700087206285,
                "cdate": 1700087206285,
                "tmdate": 1700092977808,
                "mdate": 1700092977808,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gqcruGmA7A",
                "forum": "AssIuHnmHX",
                "replyto": "daOw9YAnnD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6895/Reviewer_tXQ4"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6895/Reviewer_tXQ4"
                ],
                "content": {
                    "title": {
                        "value": "Post rebuttal response"
                    },
                    "comment": {
                        "value": "I acknowledge the authors' efforts in addressing my questions. The subsequent paper revisions and their interactive discussions with other reviewers have alleviated my initial concerns, leading to an improved score. The paper works on an important topic and proposes a viable solution, which, in my opinion, makes it suitable for being presented at the conference."
                    }
                },
                "number": 27,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6895/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700723296655,
                "cdate": 1700723296655,
                "tmdate": 1700723296655,
                "mdate": 1700723296655,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LajyCQnoQG",
            "forum": "AssIuHnmHX",
            "replyto": "AssIuHnmHX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6895/Reviewer_Ho88"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6895/Reviewer_Ho88"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to predict the feasibility of length generalization by checking whether the task can be implemented by RASP-L (L for Learnable).\n\n- RASP-L is a constrained version of the RASP language, where all variables are constrained to int8. This excludes certain operations such as index arithmetic (but still allow order comparison, predecessor and successor).\n- The paper shows that the difficulty in length generalization is correlated with the difficulty in writing the RASP-L solution.\n    - Tasks that have a simple RASP-L solution: count, mode, copy.\n    - Tasks that do not have a simple RASP-L solution: addition, parity, copy with repeated tokens.\n- The paper then shows that whether scratchpad is helpful can be explained by RASP-L as well: a properly chosen scratchpad can help with performance (e.g. by making the task solvable with induction heads), but improperly chosen scratchpad, which increases the difficulty of writing in RASP-L, can hurt the performance (e.g. on the mode task).\n\n_Post rebuttal update_: I've increased my score based on the discussions and changes in the revised paper."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- The paper analyses different factors affecting length generalization.\n- The paper provides a variety of empirical evidence."
                },
                "weaknesses": {
                    "value": "- The paper studies length generalization, though it's unclear what is considered as successful length generalization in the paper. [_Update_: the author has added a clarification.]\n    - Fig 1(b) is considered as demonstrating successful length generalization, even though the performance from all curves are dropping, some even to 0.\n    - In Fig 2(a) (i.e. the mode task), the paper seems to consider a test accuracy of 50% as reasonable generalization performance, per the comment in Sec 4.3.\n- The paper proposes to use the difficulty of RASP-L programs as an indicator for length generalization performance. However, I'm not sure what this view can teach us, either theoretically or empirically. [_Update_: the authors modified Sec 5.1 significantly on implications of the RASP-L formulation.]\n    - It's unclear how to empirically verify or act upon this conjecture. It's also not clear how to convert a task into RASP-L except for applying the definition directly, i.e. checking for whether certain criteria are satisfied.\n        - For example, two criteria provided in the paper are 1) whether the operation is non-causal and 2) whether precise index arithmetic is required. Hence one could also say that these criteria are indicative of length generalization performance, without resorting to the RASP-L language.\n    - I'm not sure whether these criteria provide new insights beyond findings in the existing literature.\n        - It is well known that auto-regressive generation cannot uncover non-causal information. [_Update_: previously I misunderstood what \"non-causal\" mean; the authors have clarified this in the rebuttal.]\n        - The concern about index arithmetic is essentially a problem with approximation (e.g. precision and weight norms), which has been raised in prior work. In particular, Hahn 2020, Chiang & Cholak 2022 and Liu et al. 2023(b) discussed limitations in the attention module, and Liu et al. 2023(a) discussed limitations in MLP.\n- Some experiments have been covered in prior work.\n    - The shifted position experiments are also included in Liu et al. 2023(a) (Fig 6 & 7).\n    - The scratchpad experiments are similar to those in Liu et al. 2023(a). This paper uses a different form of scratchpad, but the generalization performance is not very good (e.g. generalizing from length 25 to length 50 gets around 0.3 accuracy), which is related to my earlier comment about what is considered as successful length generalization.\n\n*References*\n\nHahn 2020: Theoretical Limitations of Self-Attention in Neural Sequence Models\n\nChiang & Cholak 2022: Overcoming a Theoretical Limitation of Self-Attention\n\nLiu et al. 2023(a): Transformers Learn Shortcuts to Automata\n\nLiu et al. 2023(b): Exposing Attention Glitches with Flip-Flop Language Modeling\n\n_Note: the paper has cited Hahn 2020, Chiang & Cholak 2022 and Liu et al. 2023(a)._"
                },
                "questions": {
                    "value": "- Page 2, \"Possible Mechanisms\": it's not true that omitting positional encodings may lead to \"arbitrary length\" generalization, due to failures in both MLP and attention (please see the 4 citations mentioned above).\n- It would be better to include discussions on prior work that relate algorithmic reasoning tasks to formal languages.\n  - For example, for the discussion towards the end Section 2, please consider relating to e.g. Merrill and Sabharwal 2022 and the notion of uniformity in computational complexity (which footnote 2 has mentioned).\n- Fig 3: How many test sequences are there for each trial?\n- Fig 6(b): this is not really about length generalization, but more a failure of optimization?\n\n\nMerrill and Sabharwal 2022: The Parallelism Tradeoff: Limitations of Log-Precision Transformers"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6895/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6895/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6895/Reviewer_Ho88"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6895/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698861711831,
            "cdate": 1698861711831,
            "tmdate": 1700669703474,
            "mdate": 1700669703474,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rHUi3Ly1t0",
                "forum": "AssIuHnmHX",
                "replyto": "LajyCQnoQG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6895/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6895/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response (part 1)"
                    },
                    "comment": {
                        "value": "Thank you for your time and effort in reviewing our paper! We would like to address the comments and questions you\u2019ve raised, and we\u2019d be very open to further discussion. We've updated the pdf and highlighted key changes in blue.\n\nWe first focus on clarifying our contribution, and its differences with prior work.\n\n> The paper proposes to use the difficulty of RASP-L programs as an indicator for length generalization performance. However, I'm not sure what this view can teach us. [...] I'm not sure whether these criteria provide new insights beyond findings in the existing literature.\n\nWe emphasize that our work focuses on the empirical learnability of tasks, rather than their theoretical representability.\nWhile there is a long line of work on the latter question (we have included more references in Appendix G), the former question is very much still open --- it is essentially about understanding the inductive bias of Transformers.\nOur work does not resolve this question, but we believe it takes an important and significant step forward.\n\nTo be concrete, we have added an explicit theoretical example (Appendix H) where our conjecture\u2019s prediction differs significantly from prior work. We consider the popular \u201cminimum-degree-interpolator\u201d conjecture proposed by Abbe et al (ICML 2023 Outstanding Paper: https://openreview.net/forum?id=3dqwXb1te4).\nWe give a simple theoretical setting (learning boolean conjunction) where our RASP-L conjecture correctly predicts successful length-generalization, but the min-degree conjecture does not. This simple example is emblematic of the value in our perspective.\n\nMoreover, as we discuss in the Intro (paragraph 2), the prior literature did not have a consistent answer to whether Transformers length-generalize. Most works found that standard Transformers typically fail catastrophically on even mild length-generalization, e.g. Del\u00e9tang et al. 2023,   Abbe et al 2023, and Liu et al. 2023(a).  However, it was also known that Transformers can sometimes length-generalize on particular tasks. Our contribution sheds light on which tasks: what is special about these length-generalizing tasks, that separates them from non-length-generalizing ones? We claim RASP-L representation is an empirically good heuristic for answering this question.\n\nWe hope these remarks help clarify the contribution of this work in the context of prior literature and address your concern regarding contribution. Please let us know if there are further concerns regarding this point.\n\n>It's unclear how to convert a task into RASP-L except for applying the definition directly, i.e. checking for whether certain criteria are satisfied. For example, two criteria provided in the paper are 1) whether the operation is non-causal and 2) whether precise index arithmetic is required. Hence one could also say that these criteria are indicative of length generalization performance, without resorting to the RASP-L language.\n\nThese two criteria you mentioned are implications of our RASP-L conjecture, but our conjecture is significantly stronger. \nTo give a simple example, the Parity task satisfies the two criteria you mentioned, but cannot be written in RASP-L.\n\nWe acknowledge that \u201cIt's unclear how to convert a task into RASP-L\u201d without actually trying to write the RASP-L program. This is however generally true of many computational complexity classes \u2014 for example, it's unclear whether how to determine if a task is in Polynomial time, without exhibiting a poly-time algorithm for the task. (Of course, certain well-studied complexity classes have structural features which help us determine membership. We do not yet have such a structural understanding of RASP-L, but we view the definition as an important first step). We have added a discussion of these limitations in Appendix H in the revised pdf.\n\n>It is well known that auto-regressive generation cannot uncover non-causal information.\n\nWe are unaware of a formalization of this claim in the literature. This claim has been made in informal/imprecise ways, but note that it is tricky to formally define what it means for a task to \u201crequire non-causal operations\u201d without a model of computation such as RASP-L.\n\nFor example, prior works postulated that forward-order addition would not length-generalize because the naive algorithm, on a von Neumann computer, involves non-causal operations. However, Transformers empirically actually can length generalize on forward-addition, and we show there exists a RASP-L algorithm which solves this task. This algorithm is not the naive one; it is specific to the Transformer architecture (not the von Neumann architecture), and it is thus able to \u201cwork around\u201d the causality restrictions by taking clever advantage of parallelism."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6895/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700009582909,
                "cdate": 1700009582909,
                "tmdate": 1700092925731,
                "mdate": 1700092925731,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "t6OLJPJIzo",
                "forum": "AssIuHnmHX",
                "replyto": "LajyCQnoQG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6895/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6895/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response (part 2)"
                    },
                    "comment": {
                        "value": ">Some experiments have been covered in prior work. The shifted position experiments are also included in Liu et al. 2023(a) [...] The scratchpad experiments are similar to those in Liu et al. 2023(a).\n\nWe first emphasize that the primary contribution of our work is not in the experiments, but in the conceptual framework which captures the results of those experiments (the RASP-L definition and conjecture). Indeed, our goal is to help understand the various experimental phenomena that have been observed in the past. Many papers, including Liu et al. 2023(a), have noticed the benefits of scratchpads \u2014 these papers motivate our work.\n\nAs a technical detail, note that the shifted-position experiments are not identical to Liu et al. 2023(a). Our paper does not propose a new shifting scheme \u2014 it simple replicates the standard practice in large-scale LLMs of training on random intervals from an iid stream of examples, and using learnt positional encodings for the entire context (e.g. Karpathy, 2023; Brown et al., 2020). Nevertheless, we agree that there is a structural similarity, so we have added a note about this in Section 3, Footnote 5.\n\n>it\u2019s not true that omitting positional encodings may lead to \"arbitrary length\" generalization, due to failures in both MLP and attention\n\nThank you for flagging this; we will clarify the statement in the paper. Note that the issue is subtle: there exist certain tasks where arbitrary-length generalization is provably possible, and such failures of MLP and attention do not manifest. For example, consider the simple Transformer that solves the boolean conjunction task in Appendix H. (The limitations of Hahn 2020 do not apply in our setting, since we allow weights to take values in the extended real line, as described in Footnote 3). \n\n>It would be better to include discussions on prior work that relate algorithmic reasoning tasks to formal languages.\n\nWe have added more discussion of such works in Appendix G, and we also have added a footnote about uniformity to Section 2 as suggested.\n\n>The paper studies length generalization, though it's unclear what is considered as successful length generalization in the paper.\n\nThank you for this question. In this paper, we consider successful length generalization to mean near perfect test EM on length that is at least 10 longer than the maximum training length, and we consider failures of length generalization to mean near 0 test performance on +10 length. We have added this clarification in a new figure (see Fig 1a) that summarizes the different tasks.\n\nThe specific choice of 10 is somewhat arbitrary. It reflects the level that all tasks which demonstrates non-trivial generalization can readily achieve without significant changes to model / data size and training strategy. Certain tasks such as count also exhibit significantly stronger length generalization than +10. However, it is also a reasonable proxy for capturing the differences between the set of tasks that are amenable to RASP-L solutions and the set that is not. Existing literature on length generalization reports many failure cases, including addition and parity, and in these cases performance starts to degrade as soon as the length goes out-of-distribution (see e.g. Nye et al Figure 3 and Anil et al). Being able to generalize perfectly for length of +10 or more suggests a qualitative difference from the failures cases reported in the literature. Nevertheless, how to predict the exact level of length generalization for a given task is still an open question. \n\n>Fig 3: How many test sequences are there for each trial?\n\nThe test data size is 5 * batch size for each task, which is typically 5 * 128 = 640 examples.\n\n>Fig 6(b): this is not really about length generalization, but more a failure of optimization?\n\nIndeed, for parity without scratchpad we see the difficulty of optimization, which has also been noted in prior work (e.g. Chiang & Cholak and Bhattamishra et al). We focus on comparing parity with different scratchpads, based on how simple the scratchpads are according to RASP-L. We observe that the tasks with simpler RASP-L solutions also show faster optimization, which lends some support to the intuition that simple RASP-L programs are \u201ceasier-to-learn\u201d.\n\n**Conclusion:**\n\nThank you for taking the time to read our rebuttal. If our response addresses your concerns, we kindly ask that you consider raising your score. Otherwise, please let us know about your remaining concerns/questions so we can make further improvements."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6895/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700009767040,
                "cdate": 1700009767040,
                "tmdate": 1700092951606,
                "mdate": 1700092951606,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "18fqbJtqDg",
                "forum": "AssIuHnmHX",
                "replyto": "1YdQdNPsFo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6895/Reviewer_Ho88"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6895/Reviewer_Ho88"
                ],
                "content": {
                    "title": {
                        "value": "Further clarifications"
                    },
                    "comment": {
                        "value": "Thank you for the detailed responses and clarifications! My main concern was that the main claim (i.e. the RASP-L conjecture) is stated stronger and more general than what the empirical results would support.\n\nThe authors have made several updates to the draft which I appreciate, including:\n  - a more detailed description on the experimental setups;\n  - an implication of RASP-L (Sec 5.1);\n  - discussions on the subtlety of various claims which I found were previously too strong, as well as limitations and comparison to prior work.\n\nI have a few more questions that I'd like to have the authors' thoughts on:\n- Length generalization can be considered as a specific type of OOD generalization. Is RASP-L informative for OOD generalization in general, or is it specifically for length generalization?\n  - A related work is Liu et al. 2023(b), which studies challenges in Transformer's OOD generalization under the same length. Could you please comment on the comparison?\n- For conjunction, what's the performance if we train on the training distribution in H1, but test on more numbers of 0s? Or is this type of generalization not supported by a short RASP-L program?\n- Maybe I missed this, but are there counterexamples, where the task has a short RASP-L program but cannot length generalize? Or, how to prove that there are no such tasks?\n  - For example, FIRST (i.e. whether the first token in a binary string is 1; please see Chiang & Cholak 22) can be implemented with 1 line of RASP-L (is this correct?), but 1) can length generalize with layernorm (eps=0) and 2) cannot length generalize without layernorm or with layernorm but eps>0. It seems that RASP-L (or RASP in general) is not sensitive to architecture details like this, which actually impacts the practical performance of Transformers. Could you comment on this please?\n- I'd like to have a clearer understanding on the applicability of RASP-L: the authors added discussions in Appendix G on the limitations of RASP-L, such as not able to represent numerical algorithms. Could you please comment on what is the set of tasks where RASP-L can determine length generalization abilities?"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6895/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700364377022,
                "cdate": 1700364377022,
                "tmdate": 1700364377022,
                "mdate": 1700364377022,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Gs8fYTXWeq",
                "forum": "AssIuHnmHX",
                "replyto": "LajyCQnoQG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6895/Reviewer_Ho88"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6895/Reviewer_Ho88"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the further discussions!\n\n- Re Liu et al. 23(b): I agree that improving data diversity will likely help, which Liu et al. 23(b) also pointed out (their R4) as well as Jelassi et al. 23 (\"priming\"). Note though this might be considered as a \"cheating\" solution, since introducing more data reduces the OOD problem into an in-distribution problem.\n\n- Chiang & Cholak 21's results are based on aggressive length generalization: I agree, and I think this again showcases the ambiguity in the definition of \"length generalization\". I hope this definition could be further highlighted in the paper since it will better clarify the implication/scope of the results; currently it seems that the only place that mentioned the \"generalizing to 10 more length\" is the caption of Figure 1 which I think is easy to miss.\n\n- Re counterexamples: It would be great if the authors could include this comment in the limitation or the conclusion as well. \n\nGiven these changes, I'm happy to raise my score to lean on the side of acceptance. The main reason for my rating is that I think the paper provides an interesting concept, and I hope it could be better tested out and the paper should discuss the limitations and relations to prior work more carefully."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6895/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700669617547,
                "cdate": 1700669617547,
                "tmdate": 1700669772841,
                "mdate": 1700669772841,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]