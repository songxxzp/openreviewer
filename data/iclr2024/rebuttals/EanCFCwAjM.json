[
    {
        "title": "Cameras as Rays: Sparse-view Pose Estimation via Ray Diffusion"
    },
    {
        "review": {
            "id": "y4HiTvFU07",
            "forum": "EanCFCwAjM",
            "replyto": "EanCFCwAjM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2052/Reviewer_mia4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2052/Reviewer_mia4"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a new method for sparse-view camera pose estimation. Instead of parameterizing a camera model as an intrinsic matrix and an extrinsic matrix, the authors propose to over-parameterize the camera as a collection of rays. The intrinsic and extrinsic are computed by solving a least-squares problem. A diffusion network is presented to predict the ray parameters. The method achieves state-of-the-art performance on the Co3D dataset."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "\u2022\tThe idea of over parameterization is novel. It enables robust camera pose estimation by involving least-squares optimization. Ideally, the method has the potential to predict the camera pose from a single RGB image since the ray representation does not rely on multi-view information.\n\n\u2022\tThe paper is well-written and easy to follow. The experimental results show much better camera pose estimation performance compared with some existing approaches."
                },
                "weaknesses": {
                    "value": "As reported in Table 1, it seems that the presented over parameterization method plays a crucial role in the framework. The performance of Ray Regression (Ours) surpasses that of R+T Regression by a considerable margin. The diffusion model only results in a 3.8% improvement in the case of two images. To my understanding, the superior performance is primarily attributed to the least-squares optimization which accounts for a robust estimation. However, it is still quite confusing why the pose estimation benefits from the ray representation. \n\nBasically, the idea is to regress a ray represented as a 6D vector for each patch in the RGB image. It is arguably more challenging than predicting R and T. The difficulty lies in two aspects. First, it is a dense prediction problem. Second, it regresses 3D information from RGB images. One could also predict the corresponding 2D coordinates in the right image for each patch in the left image as an alternative. Intuitively, it is easier to predict 2D coordinates than 6D ray vectors. The authors argue that such a method could struggle in sparse view settings due to insufficient image overlap to find correspondences. It is unclear why the presented method is able to achieve better robustness. \n\nMoreover, it is confusing why the presented method can recover the translation. According to Eq.3, m is coupled with the translation t. Predicting m then demands a requirement of capturing information about the camera translation. However, the actual input of the network is a cropped image. The information regarding t loses after the cropping.\n\nThe authors only conducted experiments on the Co3D dataset, which makes the evaluation not convincing enough. There are several datasets that have been widely used in the literature such as Megadepth, ScanNet, and HPatches. It would be beneficial if the authors could show the effectiveness of the over parameterization on such datasets.\n\nAccording to Eq.7, the patches of all available images are jointly processed, which is computationally expensive. As reported by the authors, training the diffusion model takes four days on 8 A6000 GPUs, which is much slower than RelPose and RelPose++."
                },
                "questions": {
                    "value": "\u2022\tMost of the equations in this paper make sense to me, but the explanation of Eq.5 is a bit confusing. What is the \u201cidentity\u201d camera? Are there any constraints on this equation? Does it still hold when the image depicts multiple planes? \n\n\u2022\tAs shown in Table 1, sometimes, the performance of the presented method decreases when more images are involved. By contrast, the performance of most competitors such as RelPose++ consistently becomes better with more images. I was wondering why the method is not compatible with multi-view images."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2052/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697814756340,
            "cdate": 1697814756340,
            "tmdate": 1699636137088,
            "mdate": 1699636137088,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NEYAHg74YV",
                "forum": "EanCFCwAjM",
                "replyto": "y4HiTvFU07",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2052/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2052/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response Part 1"
                    },
                    "comment": {
                        "value": "> As reported in Table 1, it seems that the presented over parameterization method plays a crucial role in the framework. The performance of Ray Regression (Ours) surpasses that of R+T Regression by a considerable margin. The diffusion model only results in a 3.8% improvement in the case of two images. To my understanding, the superior performance is primarily attributed to the least-squares optimization which accounts for a robust estimation. However, it is still quite confusing why the pose estimation benefits from the ray representation.\n\nWe agree with the reviewer\u2019s assessment that over-parameterization plays a crucial role, but the other important source of improvement is the tight coupling between the ray estimation and local features. We respectfully disagree with the reviewer\u2019s statement that performance is \u201cprimarily attributed to the least-squares optimization which accounts for a robust estimation.\u201d To verify this, we conduct an experiment that still makes use of our patch-predicted rays but reduces the \u201crobustness\u201d of the least squares optimization. Specifically, we use a randomly sampled subset of the 256 predicted rays per camera to recover the camera extrinsics. We compute the rotation accuracy for our Ray Regression model on unseen object categories for N=8 images:\n\n\n| # of Rays                        | 6    | 16   | 26   | 56   | 106    | 256  |\n|----------------------------------|------|------|------|------|------|------|\n| Ray Regression Rotation Accuracy | 73.8 | 81.2 | 81.2 | 81.4 | 81.8 | 81.8 |\n| Ray Regression Camera Center Accuracy | 60.7 | 61.8 |  61.9 |  61.4 | 61.8 | 61.6 |\n\nAs we can see, accuracy goes up very quickly, even with rather few rays. Of course, the least-squares optimization gives robustness (e.g. comparing 6 rays vs 16 rays), but we believe the primary gain is from the accurately estimated rays by the local-level joint reasoning of our network.\n\n> Basically, the idea is to regress a ray represented as a 6D vector for each patch in the RGB image. It is arguably more challenging than predicting R and T. The difficulty lies in two aspects. First, it is a dense prediction problem. Second, it regresses 3D information from RGB images. One could also predict the corresponding 2D coordinates in the right image for each patch in the left image as an alternative. Intuitively, it is easier to predict 2D coordinates than 6D ray vectors. The authors argue that such a method could struggle in sparse view settings due to insufficient image overlap to find correspondences. It is unclear why the presented method is able to achieve better robustness.\n\nWe agree that predicting dense 2D correspondences between image pairs is an interesting alternative. However, converting such 2D correspondences to 3D cameras (in the presence of uncertainty, partial visibility, etc) is still challenging. Moreover, this is fundamentally a pairwise prediction task and is hard to extend to multiple images where the question of which images should be matched is also uncertain. Because our solution is not restricted to such pairwise reasoning, it bypasses this issue altogether. In addition, the representation inferred is directly convertible to global cameras.\n\nWe also note that our COLMAP baseline essentially does the sparser version of such a 2D correspondence-to-camera pipeline using a state-of-the-art feature matcher. Such methods lack robustness because of insufficient 2D correspondences.\n\n> Moreover, it is confusing why the presented method can recover the translation. According to Eq.3, m is coupled with the translation t. Predicting m then demands a requirement of capturing information about the camera translation. However, the actual input of the network is a cropped image. The information regarding t loses after the cropping.\n\nThank you for pointing this out. We do provide the bounding box information of the crop to the network which is important for reasoning about translation (as the reviewer notes). Specifically, we concatenate the pixel coordinate of each patch (in normalized device coordinates with respect to the uncropped image) to the spatial features for both the ray regression and ray diffusion models that we train. \n\nWe apologize for missing this detail in the earlier version and have now clarified this in the text and added the pixel coordinates to Eqs 7 and 11."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2052/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700101199320,
                "cdate": 1700101199320,
                "tmdate": 1700101865466,
                "mdate": 1700101865466,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "K4NL10NcLh",
                "forum": "EanCFCwAjM",
                "replyto": "y4HiTvFU07",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2052/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2052/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response Part 2"
                    },
                    "comment": {
                        "value": "> According to Eq.7, the patches of all available images are jointly processed, which is computationally expensive. As reported by the authors, training the diffusion model takes four days on 8 A6000 GPUs, which is much slower than RelPose and RelPose++\n\nThe per-iteration training speed of our method is similar to that of RelPose++, but we train for half the iterations, so our total training time is about half that of RelPose++ (4 days vs 8 days of training on 8 A6000s). As we report in Table 6, the inference speed of our Ray Diffusion method is 2.6X faster than RelPose while our Ray Regression method (which requires a single forward pass) is 220X faster.\n\n> the explanation of Eq.5 is a bit confusing. What is the \u201cidentity\u201d camera? Are there any constraints on this equation? Does it still hold when the image depicts multiple planes?\n\nBy identity camera, we refer to a camera for which both the intrinsics and rotation are identity matrices: $K=I, R=I$. We have now clarified this in the text.\n\nIf we have a camera with rotation $R$ and intrinsics $K$, the direction of the ray corresponding to a (homogenous) pixel coordinate $u_i \\in \\mathbf{P}^2$ can be computed as $R^T K^{-1} u_i$. Given predicted directions $d_i$, we thus need to compute optimal $R, K$ such that $R^T K^{-1} u_i = d_i$,  or equivalently, $(K R) d_i = P d_i = u_i$. Note that this is an \u2018equality\u2019 in homogenous representations (up-to-scale). Solving this corresponds to finding the optimal homography matrix such that $ H d_i = u_i$.\n\nOur terminology stems from an alternate interpretation where \u2018directions\u2019 can be thought of as points on the plane at infinity (in projective 3D space) i.e. a direction $d_i  \\in \\mathbf{P}^2$ implies a point $(d_i,0) \\in \\mathbf{P}^3$. In this interpretation, finding $K,R$ is equivalent to finding the homography that relates  the images of this plane at infinity under the two cameras: identity camera $(K=I, R=I)$ and the regular camera $(K=K, R=R)$ (see Sec 8.5 in Hartley & Zisserman). \n\nPlease note that this does not have anything to do with planar surfaces present in the scene; and eq 5 holds for any scene geometry.\n\n\n> As shown in Table 1, sometimes, the performance of the presented method decreases when more images are involved.\n\nThis issue appears to be due to a bug when computing our camera intrinsics at training time (see top-level comment). Now that the bug has been fixed, the performance does not decrease with more images as expected, as shown in Table 1. We apologize for the confusion."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2052/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700101215832,
                "cdate": 1700101215832,
                "tmdate": 1700101778085,
                "mdate": 1700101778085,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "srKd7zCFYx",
                "forum": "EanCFCwAjM",
                "replyto": "y4HiTvFU07",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2052/Reviewer_mia4"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2052/Reviewer_mia4"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors"
                    },
                    "comment": {
                        "value": "I sincerely thank the authors for the rebuttal. Some of my concerns have been addressed, but my major concern still exists. The advantages compared to correspondence-based methods are still unclear to me. The authors argued that these methods need pairwise images as input while the presented method predicts the camera pose from the single-view image. However, to estimate the camera pose, the most straightforward idea is to somehow predict the pose from a single image. Since such a single-view prediction doesn't work well, pairwise images are utilized to make the problem easier. In the presented method, the predicted $\\mathbf{d}$ represents a direction in the canonical coordinate system, which means the network has to learn the camera rotation from the current view to the canonical view. Moreover, as I mentioned before, the authors propose to learn the direction for each image patch. Intuitively, such a dense prediction from a single view is more challenging. I don't understand why the method achieves promising results while the correspondence-based approach fails.\n\nAs updated by the authors, ray diffusion works worse than ray regression in camera rotation estimation, which weakens the claimed contribution. More importantly, the authors have found that there is a strong bias in the CO3D dataset and even predicting a constant rotation works better than RelPose++. This makes the conducted experiments less convincing. Given that the presented method predicts directions in the canonical frame, there is a potential risk of it being overfitted to this observed bias. The authors added some qualitative results on MegaDepth, which are not supportive enough."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2052/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700487707691,
                "cdate": 1700487707691,
                "tmdate": 1700487707691,
                "mdate": 1700487707691,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fPnBADs9jh",
                "forum": "EanCFCwAjM",
                "replyto": "6rXs3jgRO7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2052/Reviewer_mia4"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2052/Reviewer_mia4"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for the clarification and I am sorry for the previous misunderstanding. I acknowledge the novelty of this paper and I believe it would be a strong submission if more quantitative results on other datasets such as Megadepth and ScanNet could be reported. I understand it might be impractical to run these experiments in the rebuttal phase, but I have to keep my original rating because the experimental results on a single dataset co3d are not convincing enough to me."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2052/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700657499455,
                "cdate": 1700657499455,
                "tmdate": 1700657499455,
                "mdate": 1700657499455,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OMQuHJkeii",
                "forum": "EanCFCwAjM",
                "replyto": "y4HiTvFU07",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2052/Reviewer_mia4"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2052/Reviewer_mia4"
                ],
                "content": {
                    "comment": {
                        "value": "I disagree with the authors that the comments on the experiments are unreasonable and counterproductive! \n\nFirst, I don't think the learning-based camera pose estimation is an \"emerging\" research area. Even for the object-centric scenarios, some studies, such as [A, B, C], conducted experiments on other datasets. The authors argued that their work clearly represents a step forward for the area, but this is overclaimed to me with the experiments conducted on a single dataset.\n\nSecond, the authors have already reported some qualitative results on MegaDepth, which means they managed to test the method on MegaDepth's benchmark. The quantitative results are still missing, so I have a reason to wonder if the method can really work on the scene reconstruction dataset. Some studies like [D] reported experimental results on several datasets, using pairwise images as input. The authors asserted the advantages of their method compared to these competitors, but none of those datasets are examined in this paper.\n\nI agree with the authors that these additional experiments cannot be done overnight, so I keep my previous score based on the quality of the current submission. Again, I don't see a point that the rating is unreasonable.\n\n[A] Zhou, Xingyi, et al. \"Starmap for category-agnostic keypoint and viewpoint estimation.\" Proceedings of the European Conference on Computer Vision (ECCV). 2018.\n\n[B] Xiao, Yang, et al. \"Pose from shape: Deep pose estimation for arbitrary 3d objects.\" arXiv preprint arXiv:1906.05105 (2019).\n\n[C] Ahmadyan, Adel, et al. \"Objectron: A large scale dataset of object-centric videos in the wild with pose annotations.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021.\n\n[D] Sun, Jiaming, et al. \"LoFTR: Detector-free local feature matching with transformers.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2052/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700727613578,
                "cdate": 1700727613578,
                "tmdate": 1700727628431,
                "mdate": 1700727628431,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JxHlKDOvOd",
            "forum": "EanCFCwAjM",
            "replyto": "EanCFCwAjM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2052/Reviewer_wRVJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2052/Reviewer_wRVJ"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a distributed representation of camera pose which treats a camera as a bundle rays allowing for a tight coupling with spatial image features, which is naturally suited for set-level level transformers. Furthermore, the authors propose a regress-based approach to map image patches to associated rays. To further capture the inherent uncertainties in pose inference, the authors also develop a denoising diffusion model. The experiment on CO3D dataset demonstrate the performance of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThe authors propose a novel representation of pose that allows a bundle rays to denote camera in the field of sparse-view pose estimation.\n2.\tTo inference the rays, the authors develop a deterministic regression network and a probabilistic diffusion model, and the experiment on the CO3D demonstrates the superior performance."
                },
                "weaknesses": {
                    "value": "1.\tThe authors announce that the traditional representation of pose maybe suboptimal in neural learning in the part of introduction. However, no further discussion is given. More specific explanation is necessary, and the comparison with the proposed novel representation of pose is also required.\n2.\tThe punctuation is necessary at the end of each equation, please check it carefully.\n3.\tThe authors fail to state more details of the proposed network architecture. Moreover the training detail is also required.\n4.\tTo demonstrate the performance of the proposed novel representation, can authors undertake more experiments on more datasets?\n5.\tPlease check the format of REFERENCES."
                },
                "questions": {
                    "value": "See the weakness part"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2052/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2052/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2052/Reviewer_wRVJ"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2052/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698681682333,
            "cdate": 1698681682333,
            "tmdate": 1699636137007,
            "mdate": 1699636137007,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VJNGCLNOU0",
                "forum": "EanCFCwAjM",
                "replyto": "JxHlKDOvOd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2052/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2052/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">The authors announce that the traditional representation of pose maybe suboptimal in neural learning in the part of introduction. However, no further discussion is given. More specific explanation is necessary, and the comparison with the proposed novel representation of pose is also required.\n\nWe apologize if this was unclear, but by \u201ctraditional representation of pose\u201d in neural learning, we meant global parameterizations of 6D pose in the form of rotation and translation. Our experiments do highlight the benefits of our approach which represents cameras as rays compared to representative approaches which represent cameras using the \u201ctraditional representation.\u201d In particular, in Table 1, we demonstrate that our representation improves rotation accuracy by 58% for regression-based approaches (Ray Regression vs R+T Regression) and 22% for diffusion-based approaches (Ray Diffusion vs PoseDiffusion w/o GGS). The improvement of our representation over comparable methods that use the traditional global representation of cameras holds for camera center accuracies and all numbers of images.\n\nWe hypothesize that this improvement is due to the following factors:\n1. Predicting bundles of rays is particularly well-suited to transformer-based set-to-set inference.\n2. Local features enable reasoning about low-level features like correspondences that are not possible in existing global-feature parameterizations in prior work.\n\n> The authors fail to state more details of the proposed network architecture. Moreover the training detail is also required. \n\nWe use standard network architectures: DINOv2 for feature extraction and DiT for regressing and diffusing camera rays. We have included additional training details such as dataset preparation, model architecture, and diffusion hyperparameters in Section 3.4 Implementation Details. We would be happy to clarify any other details, and we will be releasing code to ensure reproducibility.\n\n> To demonstrate the performance of the proposed novel representation, can authors undertake more experiments on more datasets?\n\nWe have added experiments on zero-shot generalization to RealEstate10K (following PoseDiffusion) as well as newly trained results on MegaDepth (see top-level comment). We find that our method has significantly better zero-shot generalization compared to previous methods in this scene-centric setup.\n\n> The punctuation is necessary at the end of each equation, please check it carefully. Please check the format of REFERENCES.\n\nThank you for these suggestions. We have added punctuation to the end of equations and updated the reference format."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2052/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700101131430,
                "cdate": 1700101131430,
                "tmdate": 1700101131430,
                "mdate": 1700101131430,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nXJLyTa9F7",
                "forum": "EanCFCwAjM",
                "replyto": "VJNGCLNOU0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2052/Reviewer_wRVJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2052/Reviewer_wRVJ"
                ],
                "content": {
                    "comment": {
                        "value": "Great, the authors have addressed my concerns"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2052/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700487880005,
                "cdate": 1700487880005,
                "tmdate": 1700487880005,
                "mdate": 1700487880005,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Dwdrmti7Kr",
            "forum": "EanCFCwAjM",
            "replyto": "EanCFCwAjM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2052/Reviewer_v24e"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2052/Reviewer_v24e"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel method for estimating wide baseline camera poses from mutliview imagery by representing cameras as a bundle of rays through image pixels.  The rays are directly regressed from local image patches that they pass through using a vision transformer and then made more consistent with neighboring rays using diffusion applied to an image of the rays.  The bundle of rays can be converted to a standard pinhole camera model by a DLT fit of the camera parameters to the rays.  The authors train regression and diffusion models on data from the CO3Dv2 dataset and evaluate the models on held out data from that same dataset.  The method is compared to several other recent methods for camera pose estimation on the same data and demonstrates improvements in rotation and pose accuracy metrics compared to the prior work.  The primary contribution of the work is showing that regression of rays can result in more accurate camera models than trying to directly regress camera parameters as done in prior work."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The strengths of this paper are the novelty of the approach and the quality of results, which together are likely to have a significant impact in the field of wide baseline camera estimation.  Directly regressing rays intuitively makes sense as they more suited to regression by a neural network, since each ray depends on more local image information.  The paper makes this point clear and backs it up with experimental results.  Overall, the paper is written clearly and is easy to understand."
                },
                "weaknesses": {
                    "value": "The main weakness of this paper is the somewhat contrived and limited dataset and metrics used in the experimental results.  The CO3D dataset consists of many turntable-like videos with a camera orbiting in a circle around a single object of interest at an approximately fixed distance.  The variability of camera poses is quite limited compared to images in the wild.  Furthermore the image is tightly cropped around the object of interest.  This tight cropping ensures that most rays sampled pass through this common object in all views, which could provide added benefit to the propose approach.  The cropping also provides a disadvantage to feature matching approaches such as used by COLMAP.  COLMAP benefits from having a larger context of the scene with more features to match.  However, the authors are just duplicating the experimental setup from prior work (RelPose), so they are not entirely at fault for these decisions.\n\nThe proposed algorithm also, presumably, does not estimate precise camera parameters and would need a further bundle adjustment step to achieve sub-pixel accurate camera models with comparable accuracy to COLMAP (under the conditions where COLMAP succeeds).  The metrics only measure if the camera rotation is within 15 degrees of correct angle and within 10% of the scene scale in position.\n\nIn terms of clarity of the work, one concern I have is that the authors often say \"sparse-view\" when it would be more accurate to say \"wide-baseline\".  For example, the abstract states that 3D reconstruction remains challenging for sparse views (<10).  It's not the reduced number of views that are the challenge, it's the wide baseline between those images.  More traditional methods like COLMAP would do just fine on 10 images from more similar viewpoints.\n\nAlso, I found the camera visualization in Figure 5-9 to be confusing.  Without the context of the 3D object or the coordinate system and with only a few cameras, it's hard to interpret what I'm looking at.  In many cases it's not even clear which cameras belong to which algorithm's results.\n\nMinor issues:\n\nTypo on middle of page 4: \"the camera camera extrinsics\"\n\nReferences to Tab 10 and Tab 4 at the start of Section 4.3 seem to point to incorrect tables."
                },
                "questions": {
                    "value": "I'm curious whether the authors think that this method would be effective in more realistic multi-view imaging environments where the imagery is not tightly cropped to just one object and where the camera motion could be more general?  Have any experiments been run to see if this method works on images \"in the wild\"?\n\nIn the the experiments, while is COLMAP used with SuperPoint features and SuperGlue matching?  No justification is given.  Is this expected to perform better or worse than vanilla COLMAP on this dataset?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2052/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2052/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2052/Reviewer_v24e"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2052/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698724807379,
            "cdate": 1698724807379,
            "tmdate": 1699636136893,
            "mdate": 1699636136893,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NlrK7bqzRY",
                "forum": "EanCFCwAjM",
                "replyto": "Dwdrmti7Kr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2052/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2052/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> This tight cropping ensures that most rays sampled pass through this common object in all views, which could provide added benefit to the propose approach. The cropping also provides a disadvantage to feature matching approaches such as used by COLMAP.\n\nWe would like to clarify that for the COLMAP baseline, we use the entire image, and for the PoseDiffusion baseline, we center-crop the image (following their protocol). The reviewer is correct that our training with cropped images could provide additional contextual information, but we based our experimental setup on RelPose.\n\n> The metrics only measure if the camera rotation is within 15 degrees of correct angle and within 10% of the scene scale in position.\n\nIn Tables 10 and 11, we analyze the rotation and camera center accuracies at a variety of thresholds. We have also added a new measure of rotation and camera center using AUC which evaluates accuracies at all thresholds in Tables 8 and 9, and visualize the threshold vs accuracy curve in Figure 11.\n\n> one concern I have is that the authors often say \"sparse-view\" when it would be more accurate to say \"wide-baseline\"\n\nWe thank the reviewer for bringing up this discussion. We agree that \u201csparse-view\u201d may not be the optimal term. However, even the term \u201cwide-baseline\u201d may not apply in all scenarios, e.g. 2 cameras that are spatially close but viewing different directions. Perhaps a more technically accurate term would be \u201csparsely sampled views,\u201d e.g., if we think of a lightfield $L(x,y,u,v)$, sparse vs densely sampled views differ in the density of the sampling of $L$. We would be happy to emphasize this in the text, but if the reviewer does not have any objections, we would prefer to stick with the current title to follow convention in prior work (e.g. sparse-view reconstruction).\n\n> Also, I found the camera visualization in Figure 5-9 to be confusing. Without the context of the 3D object or the coordinate system and with only a few cameras, it's hard to interpret what I'm looking at. In many cases it's not even clear which cameras belong to which algorithm's results.\n\nWe have re-made all the qualitative results figures starting from Figure 5. We respectfully ask the reviewer for feedback on the new presentation of results.\n\n> I'm curious whether the authors think that this method would be effective in more realistic multi-view imaging environments where the imagery is not tightly cropped to just one object and where the camera motion could be more general? \n\nOur method can generalize to such setups. Even though our method was trained with tight crops on object-centric CO3D, we find that it generalizes zero-shot to RealEstate10K even though we used center crops (following PoseDiffusion) and the dataset has different camera trajectories. Similarly, our MegaDepth experiments use center cropping and the dataset has dramatically different camera motion.\n\n> Have any experiments been run to see if this method works on images \"in the wild\"?\n\nWe include results on self-captured data in Figure 6. We found that our method effectively generalizes to object-centric captures of object categories that are not found in CO3D.\n\n> In the the experiments, while is COLMAP used with SuperPoint features and SuperGlue matching? No justification is given. Is this expected to perform better or worse than vanilla COLMAP on this dataset?\n\nIn our early experiments, we found that COLMAP with SuperPoint features and SuperGlue matching performs better than COLMAP with SIFT features and nearest neighbors matching on CO3D."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2052/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700101110186,
                "cdate": 1700101110186,
                "tmdate": 1700101110186,
                "mdate": 1700101110186,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Sq1S2OqRs9",
            "forum": "EanCFCwAjM",
            "replyto": "EanCFCwAjM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2052/Reviewer_NPGi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2052/Reviewer_NPGi"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, authors tackle the problem of Sparse-View Pose Estimation by distributed \"ray\" based representation of cameras. By defining the camera as a bundle of rays via Plucker coordinates, authors formulate regression and diffusion-based approaches to predict camera rays from a set of sparse RGB images. With a predicted ray bundle, camera parameters (intrinsics and extrinsic) can be easily recovered. The authors test their method in a sparse view setup for the CO3Dv2 dataset and show that their regression and diffusion-based methods outperforms current learning-based and correspondence-based methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- I think this is a good method of formulating camera pose and intrinsic recovery using a bundle of rays. Furthermore, the authors' observation that ray-based representation is well-suited for set-level transformers is well backed by the results. \n- The authors' \"regression\" based method outperforms other \"diffusion\" based methods, which shows that over-parameterization is really helping solve for camera geometry accurately. \n- The results outperforms currently available \"leaning\" based and \"correspondence\" based method in sparse view settings on the CO3Dv2 dataset, that's a big encouragement.\n- The authors also show that the method generalizes to out-of-distribution, in-the-wild scenes."
                },
                "weaknesses": {
                    "value": "- One dataset is too small to see the applicability of a method. Since I see this method as superior to \"PoseDiffusion\", it would be great to see some results on the \"scene-centric\" dataset and compare it against PoseDiffusion.\n- It would be nice to see a \"memory\" requirement to run these models. Processing N image features together, I am assuming requires a good amount of GPU memory.\n- It would also be nice to see accuracy at different thresholds i.e. @5, @10, @15.\n- It would also be nice to see an ablation study where we do not scale the poses. Most of the applications require properly \"scaled\" poses.\n\n- The language is clear but I think the paper presentation is poor. Here are a few suggestions to improve the readability of the paper.\n1) For e.g., Fig 2. is really confusing where the authors are trying to show the camera to ray-bundle and ray-bundle to camera process.\n2) Fig 5. A qualitative comparison is hard to see and to make a good sense, as opposed to Fig 4 of PoseDiffusion paper for example. \n3) Also good to say in eq (3) that \"d\" is obtained by unprojecting rays from camera pixel coordinates, and \"m\" is obtained by considering point \"p\" as the camera-center since all rays intersect at the camera center.\n4) In section 4.3 evaluation Table numbers are wrong. Tab 10 -> Tab 1, Tab 4 -> Tab 2\n5) Fig 6 is very confusing. I think this needs to be redone."
                },
                "questions": {
                    "value": "- I see runtime in the appendix, but how does this method scale with adding a number of views? Instead of 8 what if I had 32? What are the memory requirements and inference runtime graphs?\n- If I am interested in \"scaled\" poses, how would I get it?  \n- Authors say they stopped the backward diffusion process early and found that those estimates were better. How early did they stop the diffusion? And how did they choose when to stop? It would be nice to see some ablation analysis.\n- What are your views of the applicability of this method for \"scene\" centric datasets?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2052/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2052/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2052/Reviewer_NPGi"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2052/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698825662237,
            "cdate": 1698825662237,
            "tmdate": 1700685145380,
            "mdate": 1700685145380,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "p0f8XS9DlU",
                "forum": "EanCFCwAjM",
                "replyto": "Sq1S2OqRs9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2052/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2052/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> It would be nice to see a \"memory\" requirement to run these models. Processing N image features together, I am assuming requires a good amount of GPU memory.\n\nWe thank the reviewer for this suggestion. We have added a GPU memory analysis for our Ray Diffusion model to the paper in Table 7. According to `nvidia-smi`, backward diffusion using our ray diffusion model on 8 images reaches a peak GPU memory usage of 3095 MiB of VRAM, which should fit on any standard GPU.\n\n> It would also be nice to see accuracy at different thresholds i.e. @5, @10, @15.\n\nIn Tables 10 and 11, we analyze the rotation and camera center accuracies at a variety of thresholds. We have also added a new measure of rotation and camera center using AUC which evaluates accuracies at all thresholds in Tables 8 and 9, and visualize the threshold vs accuracy curve in Figure 11.\n\n> It would also be nice to see an ablation study where we do not scale the poses. Most of the applications require properly \"scaled\" poses.\n\nWe would like the reviewer to clarify what is meant by not scaling poses. We are interpreting the reviewer\u2019s question as whether it\u2019s possible to not normalize camera scale at training time and instead learn metric scale. In our camera normalization procedure, the sampled minibatches of cameras are re-scaled such that the first camera always has a unit translation. This procedure is done because the ground truth camera poses are acquired using COLMAP which can produce scenes in an arbitrary scene scale, which is not consistent between sequences. Thus, it is not possible to learn metric scale since the magnitude of translations would be ill-defined. If we have incorrectly interpreted the reviewer\u2019s statement, we politely request clarification.\n\n> The language is clear but I think the paper presentation is poor. Here are a few suggestions to improve the readability of the paper.\n\nWe thank the reviewer for the feedback and have made revisions accordingly.\nWe have re-made all the qualitative results figures (Figure 5 onward). We respectfully ask the reviewer for feedback on the new presentation of results.\nFor Figure 2 specifically, we are trying to convey 1) that the camera and ray representation are easily interchangeable and 2) that even noisy rays (e.g. if they do not intersect at a single point) can still be converted into a valid camera. We would appreciate any feedback from the reviewer on how to make this more clear. We have updated the caption to add more context.\n\n> I see runtime in the appendix, but how does this method scale with adding a number of views? Instead of 8 what if I had 32? What are the memory requirements and inference runtime graphs?\n\nAlthough our model is trained with 8 images, we find that it can generalize effectively to more images by re-sampling mini-batches during the backward diffusion process. Specifically, at each iteration of DDPM, we can sample batches of 8 images (keeping the first image fixed), and predict the $X_0$s for each batch separately. With more images, the memory cost is constant but the runtime scales linearly. We find that our performance remains consistent with more images despite training with only 8 images, as we report below and added to Table 5 in the paper\n\n| # of Images                           | 8    | 15   | 22   | 29   | 36   | 43   |\n|----------------------------------------|------|------|------|------|------|------|\n| Rotation Acc. (Seen Categories)        | 93.3 | 93.1 | 93.3 | 93.1 | 93.4 | 93.4 |\n| Rotation Acc. (Unseen Categories)      | 88.1 | 88.2 | 89.2 | 88.7 | 89.0 | 88.9 |\n| Camera Center Acc. (Seen Categories)   | 84.1 | 78.3 | 76.5 | 75.3 | 74.7 | 74.2 |\n| Camera Center Acc. (Unseen Categories) | 71.4 | 62.7 | 61.1 | 59.3 | 59.2 | 58.9 |\n\n\n\n> What are your views of the applicability of this method for \"scene\" centric datasets?\n\nPlease refer to the top-level response. We have added experiments on zero-shot generalization to RealEstate10K (following PoseDiffusion) as well as newly trained results on MegaDepth.\n\n> Authors say they stopped the backward diffusion process early and found that those estimates were better. How early did they stop the diffusion? And how did they choose when to stop? It would be nice to see some ablation analysis.\n\nIn our initial experiments, we found that our accuracy metrics generally peak around T=30 (where T=100 is complete noise and T=0 is the \u201cnormal\u201d final diffusion timestep). We have added an ablation that shows performance at all timesteps in Figure 12.\n\n> I found the camera visualization in Figure 5-9 to be confusing.\n\nWe have re-made all the qualitative results figures (Figure 5 onward). We respectfully ask the reviewer for feedback on the new presentation of results."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2052/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700101070448,
                "cdate": 1700101070448,
                "tmdate": 1700101070448,
                "mdate": 1700101070448,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cUyOemRtTq",
                "forum": "EanCFCwAjM",
                "replyto": "p0f8XS9DlU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2052/Reviewer_NPGi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2052/Reviewer_NPGi"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the clarifications"
                    },
                    "comment": {
                        "value": "I thank authors for putting efforts in a nice rebuttal. \n\nRegarding scaled poses, I guess I misunderstood what you meant. I thought all translations were scaled to be unit norm vectors, and we were only getting relative distance. If we scale only the first pose to be unit-norm translation vector, its pretty standard.\n\nSince really basic oracle works better than any of the method on RealEstate10k, I can't really use it to rate the generalization of this method.\nThanks for providing results on MegaDepth though, any reason you don't have a numerical comparison? I see that SP+SG is more accurate when converges. When it doesn't converge, there are many heuristics one can apply, such as image retrieval and then coarse matching, is Ray Diffusion more accurate than such heuristics?\n\nThanks for ablation on memory requirements, runtimes, accuracy at different threshold. \n\nOverall, I am inclined to put this in acceptance threshold. My only concern is that there should have been numerical comparison for this method on scene centric dataset (not RealEstate10k because of bias), and it should really be part of the main paper (not the appendix). The idea of ray diffusion definitely shows novelty in a limited setting, hence I would be keeping my rating at acceptance. Had there been results on multiple datasets, my ratings would be in strong acceptance."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2052/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700685054946,
                "cdate": 1700685054946,
                "tmdate": 1700685054946,
                "mdate": 1700685054946,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]