[
    {
        "title": "Size Generalization of Graph Neural Networks on Biological Data: Insights and Practices from the Spectral Perspective"
    },
    {
        "review": {
            "id": "hB5IxRdXKN",
            "forum": "Rd1pjx84rk",
            "replyto": "Rd1pjx84rk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8382/Reviewer_Xenz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8382/Reviewer_Xenz"
            ],
            "content": {
                "summary": {
                    "value": "The paper investigates how graph neural networks (GNNs) handle graphs of different sizes, particularly focusing on their ability to generalize from smaller to larger graphs. Using biological datasets, the authors adopt a spectral analysis approach to show that differences in subgraph patterns, like cycle lengths, affect a GNN's performance when it encounters larger graphs. They propose and compare three model-agnostic strategies\u2014self-supervision, augmentation, and size-insensitive attention\u2014to enhance GNNs' size generalizability, finding that size-insensitive attention is the most effective method for improving performance on larger graphs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The paper starts with the study of types of distribution shifts happening real-world graphs and provides several insights, in particular to cycle importance. \n* The paper proposes and compares 3 different model-agnostic methods to enhance their performance in classification tasks. \n* The experiments on classification indicates that these methods are usually universally good across different models and datasets."
                },
                "weaknesses": {
                    "value": "* As the paper takes a data-driven approach, the main question is whether these empirical results are transferable to other domains,  other datasets, other models. \n* Augmenting model representations with different statistics is not novel. It's not clear how their enhancements correlate with previous approaches."
                },
                "questions": {
                    "value": "1. What is the time degradation when performing these augmentation? How much more time needed to perform classification? \n2. The models and datasets are academic. Is it possible to apply this model to more real-world datasets and showcase how this method can be used in biological scenarios?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8382/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698425745118,
            "cdate": 1698425745118,
            "tmdate": 1699637043165,
            "mdate": 1699637043165,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aSYWPLZbZU",
                "forum": "Rd1pjx84rk",
                "replyto": "hB5IxRdXKN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8382/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8382/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Xenz"
                    },
                    "comment": {
                        "value": "We thank the reviewer for acknowledging the insights of our paper and the effectiveness of our proposed methods. We also thank the reviewer for appreciating the soundness and presentation of our work. We appreciate all the questions and feedback the reviewer raised. In the following, we carefully respond to each of the concerns and suggestions that the reviewer raised.\n\n**Q4.1 Transferability of results**\n\nAs mentioned in the general response, one important insight brought up by this paper is that the covariate shifts caused by variations in sizes are dependent on the data domain. Thus, we restrict our attention to the biological domain. More importantly, most existing datasets for graph classification belong to the biological or chemistry domain.\n\n**Q4.2 Augmentation similar to prior approaches**\n\nWe would like to acknowledge that augmentation is not a unique technique proposed by us and neither is that the main contribution of this paper. Instead, it provides tools to validate the main insights of this paper --- leveraging cycle information in GNN can improve size generalizability on biological data. Our insights are orthogonal to different methodology-driven papers and can be integrated with those methods.\n\n**Q4.3 Additional time for augmentation**\n\nIn practice, our proposed strategies do not substantially increase training time. For cycle augmentation, we perform augmentations before training and thus do not incur overhead time during classification. For SIA, it incurs additional time due to the attention mechanism on each graph. We present per-epoch runtime statistics as below:\n\n\n|                               | bbbp | bace | proteins | NCI1 | NCI109 |\n| ----------------------------- | ---- | ---- | -------- | ---- | ------ |\n| Original (per-epoch time/sec) | 0.33 | 0.21 | 0.13     | 0.55 | 0.53   |\n| SSL (per-epoch time/sec)      | 0.33 | 0.21 | 0.12     | 0.57 | 0.54   |\n| CycleAug (per-epoch time/sec) | 0.34 | 0.21 | 0.13     | 0.54 | 0.53   |\n| SIA (per-epoch time/sec)      | 1.32 | 0.78 | 0.55     | 1.98 | 1.94   |\n\n\n**Q4.4 Other datasets**\n\nAs mentioned in the general response, our selection of datasets is based on the biological domain, and the dataset should not suffer significantly from other issues that blur the focus on size generalization. More details can be found in the datasets section in the general response."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8382/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700614913751,
                "cdate": 1700614913751,
                "tmdate": 1700615095987,
                "mdate": 1700615095987,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "EKh3asiT8m",
            "forum": "Rd1pjx84rk",
            "replyto": "Rd1pjx84rk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8382/Reviewer_dD3c"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8382/Reviewer_dD3c"
            ],
            "content": {
                "summary": {
                    "value": "This paper tackles distribution shifts caused by the graph sizes of training and test sets.\nFirst, through analysis of the spectrum distribution, it was shown that there is a correlation between the size of the graph and the distribution. It was empirically shown that the degree of correlation changes by adjusting the size of the cycle.\nBy this empirical evidence, this paper proposes three approaches to make GNNs aware of the existence and number of cycles.\nExperimental results demonstrate the potential of graph neural networks (GNNs) to enhance size generalization by understanding their substructure."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Spectral analysis of size generalizability of GNNs is novel.\n2. The proposed approaches to alleviate distribution shift are effective."
                },
                "weaknesses": {
                    "value": "1. Lack of mathematical proof of the relationship between spectrum changes depending on the size and number of cycles.\n2. GNNs that counts or can aware substructures were not compared.\n3. Inappropriate experimental settings."
                },
                "questions": {
                    "value": "1. The paper focuses on the relationship between cycle size and size generalizability. Could size generalizability be related to the number of cycles?\n\n2. Where is the theoretical evidence that reveals the relation between the size/number of cycles and spectrum distribution?\n\n3. Besides cycles, can there be any substructure that changes the spectrum according to changes in size and number?\n\n4. Where is a comparison with GNNs [1-8] that can understand the structure of the substructure and predict its number relatively accurately or consider out-of-distribution?\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [1] \"From stars to subgraphs: Uplifting any GNN with local structure awareness.\" ICLR 2022.\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [2] \"Building powerful and equivariant graph neural networks with structural message-passing\" NeurIPS 2020.\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [3] \"Understanding and extending subgraph gnns by rethinking their symmetries\", NeurIPS 2022.\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [4] \"Nested graph neural networks\", NeurIPS 2021.\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [5] \"From local structures to size generalization in graph neural networks\", ICML 2021.\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [6] \"Relational pooling for graph representations\", ICML 2019.\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [7] \"Size-invariant graph representations for graph classification extrapolations\", ICML 2021.\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [8] \"Improving graph neural network expressivity via subgraph isomorphism counting\", IEEE TPAMI 2022.\n\n5. The results in Tables 3 and 4 are the results after class imbalance and size imbalance have been corrected. What is the performance in the class imbalance setting of the original data?\n\n6. Is size generalizability using cycle applicable to other data domains beyond the biological domain?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8382/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698738034053,
            "cdate": 1698738034053,
            "tmdate": 1699637043050,
            "mdate": 1699637043050,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "caGeZQyigb",
                "forum": "Rd1pjx84rk",
                "replyto": "EKh3asiT8m",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8382/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8382/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dD3c"
                    },
                    "comment": {
                        "value": "We thank the reviewer for acknowledging the novelty of the spectral analysis and the effectiveness of our proposed methods. We also thank the reviewer for carefully reading our paper and detailed comments. We appreciate all the questions and feedback the reviewer raised, especially the one on drawing our attention to expressive models and the number of related works provided. In the following, we carefully respond to each of the concerns and suggestions that the reviewer raised.\n\n**Q3.1 Size generalizability and the number of cycles**\n\nThank you for your great suggestion. We present the following table to show the statistics of the average number of cycles that small and large graphs contain respective to their size. \n\n|          | num_cycles/num_nodes for smallest 50% graphs | num_cycles/num_nodes for largest 10% graphs |\n| -------- | -------------------------------------- | -------------------------------------- |\n| bbbp     | 0.1211 +- 0.05                         | 0.1103 +- 0.03                         |\n| bace     | 0.1335 +- 0.02                         | 0.0814 +- 0.02                         |\n| PROTEINS | 0.9237 +- 0.19                         | 0.8320 +- 0.26                         |\n| NCI1     |  0.1276 +- 0.05                         | 0.1103 +- 0.05                         |\n| NCI109   | 0.1282 +- 0.05                         | 0.1127 +- 0.05                         |\n\nIt can be seen that the average num_cycle/num_nodes are highly similar for small and large graphs for most datasets. Due to this reason, it is unlikely that the number of cycles is closely related to size generalization.\n\n**Q3.2 Besides cycles, can there be any substructure that changes the spectrum according to changes in size and number?**\n\nThere can be. The insights for our investigation into cycles are based on our discernment of variations in the peaks of specific frequencies within the graph spectrum, such as the frequency=1 in unnormalized Laplacian matrices. Many of these specific frequencies align with the frequencies of cycles, with cycles containing 5-7 nodes being particularly evident. As a reminder, the spectrum for a cycle with n nodes is given by the formula 2-2cos(2\u03c0j/n). We also computed the spectrum of other substructures such as lines, but their difference in small and large graphs is not consistent across different biological datasets.\n\n**Q3.4 Comparison with other baselines**\n\nWe have added more baselines in Table 4 (including some expressive GNN models). For more details, please refer to the baseline section in the general response and Table 4 in our updated submission. In general, the proposed method SIA still achieves best performance and is model-agnostic. Thank you for your valuable suggestions, we do find that expressive model that excels at detecting cycles have good size generalizability on certain datasets.\n\n**Q3.5 performance in the class imbalance setting of the original data**\n\nIn our experiments, we find that certain backbone models tend to be very biased towards the major class and thus result in an extremely skewed performance. For instance, the FAGCN and MLP backbone model always outputs a test accuracy (and thus unreliable F1 score) that is almost the same as the proportion of the larger class. In order to prevent such cases from happening, we decided to adopt upsampling.\n\n**Q3.6 Is size generalizability using cycle applicable to other data domains beyond the biological domain?**\n\nIt might be also applicable to the chemistry domain. For the other domains, it relies on further investigation of the spectrums. However, most existing datasets for graph classification belong to the biological or chemistry domain."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8382/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700614682460,
                "cdate": 1700614682460,
                "tmdate": 1700614682460,
                "mdate": 1700614682460,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GZGYqGUXNh",
            "forum": "Rd1pjx84rk",
            "replyto": "Rd1pjx84rk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8382/Reviewer_XnPU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8382/Reviewer_XnPU"
            ],
            "content": {
                "summary": {
                    "value": "This paper characterizes the size-induced distribution shifts and evaluated their influence on the generalizability of GNNs through the spectral perspective especially on biological data. It identifies that spectrum differences induced by size are related to differences in subgraph patterns and introduces three model-agnostic strategies to enhance GNNs\u2019 size generalizability."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper identifies that cycle-related information plays a pivotal role in reducing spectral differences between small and large graphs. It proposes three model-agnostic strategies\u2014self-supervision, augmentation, and size-insensitive attention\u2014to enhance GNNs\u2019 size generalizability and empirical results demonstrated that their effectiveness."
                },
                "weaknesses": {
                    "value": "1. Experiments are insufficient and lack of comparison with related methods, such as the size-generalization methods referenced in the related work. The baselines are not state-of-the-art methods in the relevant field. The authors need to add comparison experiments with methods from most recent years, which are related to this paper. Furthermore, the paper lacks experimental validation from other perspectives, such as the effect of different graph size settings in the training process.\n\n2. The contribution lacks novelty. In this paper, the authors identify cycle structures as a major factor affecting the generalization capacities of GNNs. This finding looks to be a special case of [1]. In Section 3, the authors observe that cycle structures have an impact on the spectrum differences between graphs, but it is difficult to ascertain the effect of graph size and cycle distribution on the generalization capacities of models. Also, the three proposed strategies in Section 4 lack novelty and could be combined into a single algorithm.\n\n[1] Gilad Yehudai, Ethan Fetaya, Eli Meirom, Gal Chechik, and Haggai Maron. From local structures to size generalization in graph neural networks. In International Conference on Machine Learning, pages 11975\u201311986. PMLR, 2021."
                },
                "questions": {
                    "value": "Why is the algorithm description incomplete in the text, such as the section 4.3? If not essential, it could be excluded as the part of the contributions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8382/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698765674896,
            "cdate": 1698765674896,
            "tmdate": 1699637042911,
            "mdate": 1699637042911,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "u1nLuxH51H",
                "forum": "Rd1pjx84rk",
                "replyto": "GZGYqGUXNh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8382/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8382/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer XnPU"
                    },
                    "comment": {
                        "value": "We thank the reviewer for acknowledging the insight of this paper in identifying cycle-related information. We value the constructive feedback that the reviewer provided. In the following, we carefully respond to each of the concerns and questions that the reviewer raised.\n\n\n**Q2.1 Insufficient experiments and comparison with baselines**\n\nWe have added more baselines in Table 4. For more details, please refer to the baseline section in the general response and Table 4 in our updated submission. In general, the proposed method SIA still achieves the best performance and is model-agnostic.\n\n**Q2.2 Novelty wrt. \"From local structures to size generalization in graph neural networks\"**\n\nActually, the empirical findings of our paper are contradictory to the assumptions made by the paper \"From local structures to size generalization in graph neural networks\". In their paper, they hold the assumption that the degree patterns change with graph sizes. However, in Appendix G of our updated paper, we find that the degree distribution does not show a clear correlation with the graph size on biological datasets. Our main contribution is that we find cycle-related information (existence of cycles, average cycle length) is critical for size generalization for biological data, which has not been proposed before.\n\n**Q2.3 Why is the algorithm description incomplete in the text, such as the section 4.3? If not essential, it could be excluded as the part of the contributions.**\n\nThis is primarily due to page limits. In fact, we have explained well about the motivation as well as the main function of this algorithm at the same paragraph. We leave the details of implementation in the Appendix for further interest."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8382/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700615328219,
                "cdate": 1700615328219,
                "tmdate": 1700615328219,
                "mdate": 1700615328219,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QwNJzaSWiZ",
            "forum": "Rd1pjx84rk",
            "replyto": "Rd1pjx84rk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8382/Reviewer_eDSS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8382/Reviewer_eDSS"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the size generalization of GNNs in biological networks. Through the spectral analysis, the authors find that spectrum differences induced by size are related to differences in subgraph patterns (e.g., average cycle lengths). Since regular GNNs can hardly capture the cycle features, they propose three strategies, including self-supervision, augmentation, and size-insensitive attention, to enable GNNs to learn cycle information thus improving the OOD generalization across different sizes. Experiments with various GNN backbones show the proposed solutions can effectively improve their size OOD generalization ability."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "(+) The spectral analysis along with the solutions are well-motivated and interesting to the community;\n\n(+) The paper is well-written and easy to follow;"
                },
                "weaknesses": {
                    "value": "(-) The analysis especially the solutions lacks theoretical guarantees.\n\n(-) The experiments focus on simple tasks and lack the comparison with several relevant baselines."
                },
                "questions": {
                    "value": "1. The analysis especially the solutions lacks theoretical guarantees. \n- Although the analysis shows that there is a connection between spectrum differences with the cycle lengths, there could be some underlying confounders that jointly affect the graph sizes and cycle lengths. For example, in the model by Bevilacqua et al. 2021, the graphon and the size of the graph will jointly affect the cycle lengths.\n- The proposed three solutions are well motivated, while mainly based on empirical observations. To what extent can the three methods resolve the cycle issue? Will the operations affect the expressivity of GNNs?\n\n2. The experiments focus on simple tasks and lack the comparison with several relevant baselines.\n- Why do the experiments adopt a different data split scheme from previous practice such as in Bevilacqua et al. 2021?\n- How well do the proposed methods perform on more realistic and large datasets such as OGB-molhiv with graph size shifts, and DrugOOD[1]?\n- Can the proposed methods perform better than previous solutions like Bevilacqua et al. 2021, and Buffelli et al. that are cited in the paper, and [2,3] that are the state-of-the-art in graph size OOD generalizations?\n- [4] analyzes the size generalization in link predictions, which is also a related work to discuss.\n- Can the proposed methods improve the size generalization in algorithmic reasoning tasks?\n\n\n**References**\n\n[1] DrugOOD: Out-of-Distribution (OOD) Dataset Curator and Benchmark for AI-aided Drug Discovery -- A Focus on Affinity Prediction Problems with Noise Annotations, AAAI\u201923.\n\n[2] Learning Causally Invariant Representations for Out-of-Distribution Generalization on Graphs, NeurIPS\u201922.\n\n[3] Wasserstein Barycenter Matching for Graph Size Generalization of Message Passing Neural Networks, ICML\u201923.\n\n[4] OOD Link Prediction Generalization Capabilities of Message-Passing GNNs in Larger Test Graphs, NeurIPS\u201922."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8382/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8382/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8382/Reviewer_eDSS"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8382/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698832420174,
            "cdate": 1698832420174,
            "tmdate": 1700728093996,
            "mdate": 1700728093996,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZPHTrxL1CI",
                "forum": "Rd1pjx84rk",
                "replyto": "QwNJzaSWiZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8382/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8382/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer eDSS"
                    },
                    "comment": {
                        "value": "We thank the reviewer for appreciating the novelty of the spectral analysis and the writing of this paper. We also thank the reviewer for carefully reading our paper and detailed comments. We appreciate all the questions and feedback the reviewer raised, especially on the number of related works provided. In the following, we carefully respond to each of the concerns and suggestions that the reviewer raised.\n\n**Q1.1 Other confounders that jointly affect the graph size and cycle lengths.**\n\nThere might be other confounders that jointly affect the graph size and cycle lengths. However, the method by Bevilacqua et al. 2021 cannot fully address the covariate shifts we found. In that paper, they assume that the density of induced k-sized subgraphs is invariant. However, based on our empirical observations, we actually found big cycles that do not appear in small graphs, suggesting that this density invariance does not hold for large cycles. In other words, their model cannot generalize from cycles of small lengths to cycles of large lengths.\n\n**Q1.2 To what extent can the three methods resolve the cycle issue? Will the operations affect the expressivity of GNNs?**\n\nExplicitly including cycle information can help distinguish more graphs, but will not fundamentally affect the expressiveness of the model. To understand this problem better, in Table 4, we also include two expressive models as our baselines: RPGNN[R2] and SMP[R3]. What we find is that the general expressive model can help size generalization to some extent, but is not as good as explicitly providing this information. The model that excels at cycle-related tasks, e.g. SMP, exhibits better size generalizability than other expressive models, such as RPGNN.  \n\n**Q1.3 Why do experiments adopt different data splits?**\n\nFirstly, it's important to note that we did not utilize identical datasets as those referenced in Bevilacqua et al. 2021. Secondly, the data files we obtained proved to be incompatible with the current versions of PyTorch and CUDA, preventing us from successfully loading the data into our environments. Thirdly, when attempting the data processing using the identical original code provided in Bevilacqua et al. 2021, we encountered errors. Consequently, we were unable to replicate the exact data split presented by Bevilacqua et al. 2021. Despite this deviation, our experimental setup closely aligns with the overarching concept of partitioning datasets into larger and smaller subsets.\n\n**Q1.4 Performance on other large-scale datasets**\n\nWe sincerely thank the reviewer for giving several dataset examples. Initially, we explored several other biological datasets, such as the HIV dataset you mentioned. However, we encountered a significant class imbalance issue (please refer to the dataset section in the general response for details). For instance, in the HIV dataset, the 50% smallest dataset contains 20054 class 0 samples and 510 class 1 samples, whereas its 10% largest dataset contains 3680 class 0 samples and 433 class samples. Other large-scale datasets from ogb and TU exhibit similar trends. Additionally, for the other dataset mentioned in your review, DrugOOD includes various covariate shifts other than graph sizes. For the one concerning size shift (DrugOOD-lbap-core-ic50-size), it adopts a different idea in splitting the data, where it uses the large datasets in the training and validation set and small datasets in the testing. Due to such differences, we did not conduct experiments on it.\n\n**Q1.5 Comparison with baselines**\n\nWe thank you for your great suggestions. We have added more baselines in Table 4. For more details, please refer to the baseline section in the general response and Table 4 in our updated submission. In general, the proposed method SIA still achieves the best performance and is model-agnostic.\n\n**Q1.6 Related work in link prediction OOD**\n\nThank you for your great suggestion! We have added this paper to our related work section. \n\n**Q1.7 Size generalization in algorithmic tasks**\n\nOne important view this paper takes is that the size-induced distribution shift depends on the data domain, and that is why contradictory conclusions appear in prior works. Though our main empirical findings apply mainly to biological data, we can adopt a similar spectral analysis to other fields, and identify other patterns that lead to the spectral differences."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8382/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700621107542,
                "cdate": 1700621107542,
                "tmdate": 1700621107542,
                "mdate": 1700621107542,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OwY7rqqSv0",
                "forum": "Rd1pjx84rk",
                "replyto": "QwNJzaSWiZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8382/Reviewer_eDSS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8382/Reviewer_eDSS"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed explanation, which addressed many of my concerns. However, I still have some concerns:\n- What are the exact errors for loading the splits by Bevilacqua et al. 2021?\n- As acknowledged by the authors, the focus of the paper is on the biological data. The class imbalance issue generically exists in biological data, and would affect all methods. If the proposed method could indeed address the graph size shifts, then there are still improvements expected to be observed. Otherwise, without experiments on real data, the scope of the paper could be very limited;\n- I appreciate the authors' efforts in making the comparison with [3], but why [2] is neglected in comparison?"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8382/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700661041011,
                "cdate": 1700661041011,
                "tmdate": 1700661041011,
                "mdate": 1700661041011,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "19qDSJojQl",
                "forum": "Rd1pjx84rk",
                "replyto": "QwNJzaSWiZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8382/Reviewer_eDSS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8382/Reviewer_eDSS"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the new results and discussion! I have increased my rating accordingly. I believe this work could be more impactful if the developed insights and methods could be generalized to realistic datasets."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8382/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700728071476,
                "cdate": 1700728071476,
                "tmdate": 1700728167374,
                "mdate": 1700728167374,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]