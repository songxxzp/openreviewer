[
    {
        "title": "AGILE3D: Attention Guided Interactive Multi-object 3D Segmentation"
    },
    {
        "review": {
            "id": "u8W1FfFPEr",
            "forum": "9cQtXpRshE",
            "replyto": "9cQtXpRshE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5206/Reviewer_Gxfu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5206/Reviewer_Gxfu"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces the novel interactive multi-object 3D segmentation task with a first multiple objects interactive segmentation approach, named AGILE3D.\nThe author also provides the setup, evaluation, and iterative training strategy for interactive multi-object segmentation on 3D scenes and conduct extensive experiments to validate the benefits of our task formulation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The interactive multi-object 3D segmentation task is interesting and novel.\n2. The author provides a complete task process, which will be helpful for subsequent research.\n3. The proposed AGILE3D appears to be concise and effective, and is able to achieve state-of-the-art performance. At the same time, the author performed Efficiency Comparison, which is necessary for user-interactive tasks."
                },
                "weaknesses": {
                    "value": "1. Is the contribution of this article incremental compared to 2D interactive segmentation?\nSince I am not an expert in this field, I cannot judge the extent of this article's contribution."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5206/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5206/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5206/Reviewer_Gxfu"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5206/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698669208793,
            "cdate": 1698669208793,
            "tmdate": 1699636517810,
            "mdate": 1699636517810,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Q4hOdsDE0q",
                "forum": "9cQtXpRshE",
                "replyto": "u8W1FfFPEr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5206/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5206/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Gxfu"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive feedback. We are glad to hear that the reviewer finds interactive multi-object 3D segmentation an \u201c*interesting and novel*\u201d task, and our model \u201c*concise and effective*\u201d.\nAnswering the reviewer\u2019s question, **our contribution is significant compared to 2D interactive segmentation**, which is also reflected by the comment of reviewer PRcH \u201c*The authors offer a cogent review of pertinent literature, **spanning interactive 3D and image segmentation domains**, accentuating their contribution's novelty.*\u201d In detail:\n\n**1. Step to multi-object handling**. Although interactive segmentation has a longer history in 2D, existing work has focussed on single-object segmentation. We show the advantage of jointly segmenting multiple objects. **The multi-object case is particularly useful in 3D, where the precise delineation of nearby and adjacent objects is harder than in an image** and we are convinced that the fresh view taken in our work will be explored further.\n\n**2. Network architecture**. Since the first deep-learning method [1] for interactive 2D segmentation, existing works have converged to a common architecture: user interactions are encoded as click maps, concatenated to the input image, and fed into a deep neural network for a binary mask prediction. Also, the state-of-the-art in 3D, InterObject3D, uses exactly that architecture (Fig. 1 in the paper). By contrast, our model encodes clicks as spatial-temporal queries and introduces a carefully designed click attention and query fusion module. This **new network layout** not only enables interactions between clicks with the scene, it also provides the flexibility needed for multi-object handling, and improves efficiency by avoiding repeated runs of the feature extraction backbone.\n\n**3. Real user study**. In 2D, the established practice is to evaluate interactive segmentation models with simulated clicks. Those simulated clicks are sampled with simple heuristics that look for the centers of the wrongly segmented regions, and do not fully reflect the actual behavior of human operators, who carefully choose where to click depending on context, contrast, point density, etc. Therefore, we have created a user interface and conducted real user studies (which support our multi-object formulation). **We hope to motivate the community in both 2D and 3D to follow that practice and go beyond the convenient, but limited proxy of simulating user clicks**.\n\n**4. Asking a new question**. We also see a contribution in preparing the ground for the 3D case. Arguably, interactive segmentation is at least as important, and more challenging in 3D, where there is a dire lack of datasets for large-scale learning, and annotation is more painful and needs automation support. In that sense, **it is also a contribution that our work formalizes the multi-object task, defines an evaluation protocol, and provides a benchmark**.\n\n[1] Xu, Ning, et al. Deep interactive object selection. CVPR. 2016."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5206/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699979574827,
                "cdate": 1699979574827,
                "tmdate": 1699985346835,
                "mdate": 1699985346835,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MWvvFhaFCN",
                "forum": "9cQtXpRshE",
                "replyto": "u8W1FfFPEr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5206/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5206/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Hope our answers are helpful"
                    },
                    "comment": {
                        "value": "Dear reviewer Gxfu, thank you for your interest in our work. We hope you are happy with our clarifications and answers. If you have any further questions we are happy to provide further clarification."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5206/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700666177134,
                "cdate": 1700666177134,
                "tmdate": 1700666177134,
                "mdate": 1700666177134,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wezzQrRlqj",
            "forum": "9cQtXpRshE",
            "replyto": "9cQtXpRshE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5206/Reviewer_3vqw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5206/Reviewer_3vqw"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new model for interactive 3D object segmentation called AGILE3D, which firstly encodes points to features. Multiple user clicks are converted to high-dimension queries, then lightweight decoder is used to segment multiple objects concurrently in 3D scene. Experimental results partly demonstrate the effectiveness of AGILE3D compared to previous models."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The interactive approach AGLIE3D can segment multiple objects simultaneously with limited user clicks. Compared to previous single-object iterative models, the proposed approach can reduce annotation time.\n2. Sufficient experiments have been conducted to show the promising results of the proposed method."
                },
                "weaknesses": {
                    "value": "1. How does the model correct wrongly segmented object instance? For example, in Figure 6, if the initial segmentation wrongly groups two chairs into one instance, how can a later click fix this mistake?\n2. In Table 9, results 2 and 7 show minor performance drops without the C2C attention and temporal encoding components. Does this indicates that modeling the relations between clicks is less important for the proposed method?\n3. During training and testing, the clicks are sampled at the center of the object regions. Were there any ablation experiments done using other click sampling methods? How does the choice of click sampling influence performance?"
                },
                "questions": {
                    "value": "Please see the weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5206/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698829817031,
            "cdate": 1698829817031,
            "tmdate": 1699636517728,
            "mdate": 1699636517728,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4PBuw6kKjD",
                "forum": "9cQtXpRshE",
                "replyto": "wezzQrRlqj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5206/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5206/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 3vqw (Part 1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive feedback. We are glad the reviewer agrees that our model is \u201c*new*\u201d, \u201c*can reduce annotation time*\u201d and is backed by \u201c*sufficient experiments*\u201d. Regarding the questions:\n\n**1. How does the model correct wrongly segmented object instance?**\n\nSorry if this was unclear. If the model produces a wrongly segmented object instance, users are expected to provide assistive clicks on the error regions. Then the model will take as input the newly added clicks together with the previous clicks and the 3D point cloud and produce an updated segmentation mask. Each click $c_k = ${$p_k, o_k $}  (where $k=1, 2, \u2026$ is the click index) has two attributes, one is its coordinates $p_k$, and another one is its region index $o_k$. $o_k = 0$ indicates the click comes from the background, $o_k=m (m>0)$ indicates the click comes from object $m$.\n\nIn Figure 6, **If** the user starts with one click $c_1= ${$p_1, o_1=1$} on chair 1 but the model wrongly groups chair 1 and chair 2 into one instance, then the error region is chair 2 and the user is expected to provide a second click $c_2 = ${$p_2, o_2=2$} on chair 2. After that, $c_1$ and $c_2$ are converted to queries and go through the click attention module and each click produces one region mask logits:\n\n-$c_1$ produces $\\mathcal{R}_{1} \\in \\mathbb{R}^{ {N}'}$, ${N}'$ is the number of points of the point cloud. \n\n-$c_2$ produces $\\mathcal{R}_{2} \\in \\mathbb{R}^{ {N}'}$ \n\n-We also have learned background mask logits $\\mathcal{R}_{0} \\in \\mathbb{R}^{ {N}'}$.\n\nThe above three mask logits are concatenated and softmaxed. Finally, an argmax will give a corrected segmentation map assigning each point a label belonging to [$0$ (background),$1$ (chair 1), $2$ (chair 2)].\n\n**2. Question on the effect of C2C attention and temporal encoding**\n\nThanks for the reviewer\u2019s careful observation. We agree that the self-attention between click queries (C2C attention) has a less pronounced effect compared to the cross-attention between click queries and point features, i.e., C2S attn and S2C attn. However, it does not mean that C2C attention is not important or can be safely removed. The ablation results in Table 9 are conducted in a setting where the model is trained on ScanNet40-train and evaluated on ScanNet40-val (smaller domain gap). We empirically find that C2C attn has a significant effect in the setting ScanNet40 (Training) -> KITTI-360 (Testing), **e.g. 37.4 (with C2C) vs 33.8 (wo. C2C)** on  $\\overline{\\textrm{IoU}}@\\overline{\\textrm{5}}$. Similarly, we find that the temporal encoding (TE) has an obvious effect on KITTI-360, especially for long click sequences, **e.g. 40.5 (with TE) vs 42.3 (wo. TE) on $\\overline{\\textrm{IoU}}@\\overline{\\textrm{10}}$**. This indicates that **C2C and temporal encoding play crucial roles in improving the model\u2019s generalization ability**. **We have updated the manuscript in page 9 to highlight this point**."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5206/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700142914847,
                "cdate": 1700142914847,
                "tmdate": 1700142914847,
                "mdate": 1700142914847,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "koRgDwvBPe",
                "forum": "9cQtXpRshE",
                "replyto": "wezzQrRlqj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5206/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5206/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 3vqw (Part 2/2)"
                    },
                    "comment": {
                        "value": "**3. How does the choice of click sampling influence performance?**\n\nYes, we did experiments to compare random sampling and iterative sampling.\n\nDuring **training**,  a naive way is **random sampling**, i.e. object clicks are randomly sampled from object regions, and background clicks are randomly sampled from the background. However, those randomly sampled clicks are **independent** of network errors, which is not consistent with testing conditions. We, therefore, propose **multi-object iterative training/sampling** to train our model, i.e. sampling new clicks from the centers of top error regions (one click per region) from the last iteration. \n\nWe show the comparison of iterative training/sampling with random sampling in **Table 9** (item textcircled 1). We also compare with two iterative training strategies[1,2] proposed in the image domain in **Table 8**. We find our iterative sampling is better than both random sampling and [1,2]. Please check the **Iterative training** paragraph in **Sec. 4.4 and B.3** in the appendix for details.\n\nThe established protocol for **testing** in the community is to simulate a user who always clicks at the center of the biggest error region. This protocol is deterministic and ensures reproducible scores. Like existing interactive segmentation works, we also adhere to this protocol when evaluating simulated clicks. However, we argue **this may not fully reflect real user behavior and that\u2019s why we also conducted real user studies**. For real user studies, **we did, on purpose, not give explicit instructions on where to click, in order to obtain realistic and representative user behavior**. We conducted the user studies with two models: one trained with random sampling, one trained with our iterative training. Results in **Table 6** show that users achieve better performance with the model trained with iterative training, and exhibit smaller variance. This indicates that iterative training can indeed better mimic real user behaviors.\n\n**We hope our response have helped answer the reviewer\u2019s questions. If so, we kindly ask the reviewer to reconsider the score. If something is still unclear, we are open to further discussions.**\n\n[1] Mahadevan, Sabarinath, et al. Iteratively trained interactive segmentation. BMVC. 2018.\n\n[2] Sofiiuk, Konstantin, et al. Reviving iterative training with mask guidance for interactive segmentation. ICIP. 2022."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5206/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700143035781,
                "cdate": 1700143035781,
                "tmdate": 1700143035781,
                "mdate": 1700143035781,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "J0jImlayVo",
                "forum": "9cQtXpRshE",
                "replyto": "wezzQrRlqj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5206/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5206/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A gentle reminder"
                    },
                    "comment": {
                        "value": "Dear reviewer 3vqw, A gentle reminder, as the discussion periods will end in less than one day, we hope you are happy with our clarifications and answers. If you have any further questions, we are happy to provide further clarification. Thank you."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5206/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700664733410,
                "cdate": 1700664733410,
                "tmdate": 1700664733410,
                "mdate": 1700664733410,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ri92Zh1FAR",
            "forum": "9cQtXpRshE",
            "replyto": "9cQtXpRshE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5206/Reviewer_PRcH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5206/Reviewer_PRcH"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces AGILE3D, an attention-guided interactive model for multi-object 3D segmentation. At its core, AGILE3D encodes user interactions as spatial-temporal queries, leveraging a unique click attention module to foster interplay between user-generated queries and the 3D scene. The model not only supports concurrent segmentation of multiple 3D entities but also champions efficiency, demanding fewer user inputs and providing swifter inference times. Comprehensive evaluations indicate its superiority over extant methods across both individual and multi-object interactive segmentation benchmarks."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The proposed approach pioneers in the realm of interactive multi-object 3D segmentation by introducing an avant-garde attention-centric model.\n- The real-world practicality of the proposed approach is convincing with achieving top-tier results on both the popular individual and multi-object interactive segmentation benchmarks, w.r.t. SOTA methods such as Mask3D. I also appreciate the detailed computational cost comparisons.\n- AGILE3D's potential to discern multiple entities with diminished user interaction and expedited inference, vis-\u00e0-vis its counterparts, is empirically substantiated.\n- The authors offer a cogent review of pertinent literature, spanning interactive 3D and image segmentation domains, accentuating their contribution's novelty.\n- The user study with a proper interactive interface provides convincing signals for the superiority of the model."
                },
                "weaknesses": {
                    "value": "- Discussion of limitations/drawbacks should be put in the main body of the manuscript instead of the appendix for a fairer portrayal.\n- More failure cases need to be showcased and analyzed if any, otherwise the paper seems to focus too much on the advantages of the presented method and does not always give the whole picture."
                },
                "questions": {
                    "value": "Please check the suggestions in the Weakness section above to further strengthen the paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5206/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698840331509,
            "cdate": 1698840331509,
            "tmdate": 1699636517633,
            "mdate": 1699636517633,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nxzENpQ21W",
                "forum": "9cQtXpRshE",
                "replyto": "ri92Zh1FAR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5206/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5206/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer PRcH"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive feedback. We appreciate the reviewer\u2019s generally positive view, and are particularly happy that they attest our paper \u201cnovelty\u201d, a \u201ccogent review\u201d and \u201ctop-tier results\u201d with \u201cconvincing signals for the superiority\u201d. The only real concern appears to be the discussion of limitations and failure cases.\n\n>Discussion of limitations/drawbacks should be put in the main body of the manuscript instead of the appendix for a fairer portrayal.\n\nThank you for the suggestion. We have now moved the limitations from the appendix to the main paper. \n\n>More failure cases need to be showcased and analyzed if any, otherwise the paper seems to focus too much on the advantages of the presented method and does not always give the whole picture.\n\nThank you, we agree that we did not pay enough attention to this. We have added failure cases and an accompanying discussion in the main paper, please see the updated manuscript for details. We hope the analysis will motivate further research into the under-explored topic of interactive 3D segmentation."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5206/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699977891512,
                "cdate": 1699977891512,
                "tmdate": 1699977891512,
                "mdate": 1699977891512,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8AZ7LbeO6A",
                "forum": "9cQtXpRshE",
                "replyto": "ri92Zh1FAR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5206/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5206/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Hope our revision is helpful"
                    },
                    "comment": {
                        "value": "Dear reviewer PRcH, thank you for your support and suggestions for our work, which further strengthen the paper. If you have any other questions, we are happy to provide more information."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5206/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700666886553,
                "cdate": 1700666886553,
                "tmdate": 1700666886553,
                "mdate": 1700666886553,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2AjtLSEzgr",
            "forum": "9cQtXpRshE",
            "replyto": "9cQtXpRshE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5206/Reviewer_byjT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5206/Reviewer_byjT"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents AGILE3D, the interactive 3D segmentation model that simultaneously segments multiple objects in context. The key idea is to use the attention-guided interactive 3D segmentation approach and also propose the click attention module to build the connection between different clicks, finally achieving multi-object segmentation via fast inference. The experimental results demonstrate and validate the proposed achievement of better performance on ScanNetV2 and outperform the competitive method InterObject3D."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) The paper provides detailed information and resources about AGILE3D, allowing readers to gain a deeper understanding of the framework and its implementation.\n2) The appendix includes additional materials such as network architecture, algorithms, and data samples, which further support the understanding and practical implementation of AGILE3D. The authors provide clear and detailed explanations of the concepts, algorithms, and techniques used in AGILE3D, ensuring that readers can follow and reproduce the framework effectively."
                },
                "weaknesses": {
                    "value": "1) Lack of Novelty: For the proposed methods, the novel point is the multi-object segmentation, and the fast inference is also a novel point to some degree. And for efficiency, the submission does not present a technical point on how to improve efficiency. Multi-object segmentation is also achieved by the previous methods (InterObject3D) via some small improvements. Actually, the performance on the multi-object segmentation is not the best, according to Table 4.\n2) Limited Evaluation: The appendix focuses more on providing information and supplementary materials than conducting extensive evaluations. A more thorough evaluation of the framework's performance and comparison with existing approaches would enhance the paper's credibility. The various datasets are also a strong evaluation, especially for the interactive tools. I suggest some results on the real captured and other challenging datasets should be presented in the paper.\n3) The running time should be reported in a fair manner; for table 5, it is not clear how to perform the comparison since the manuscript claims efficiency as a contribution. Or, the user study is a good choice to compare the efficiency.\nIn some challenging cases, there are a lot of occlusions between the segmented object and other objects in the scene.\n4) The significant difference between interobject3D and interobject3D++ should be discussed comprehensively since the published paper is very relative to the submission.\n5) From Figure 1, the input only presents the XYZ + RGB, i.e., the point cloud and point color. Why does the visualization have the faces? Could you use the faces as extra information to enhance the performance of the proposed methods? From the interactive demo, there is still the face visualization.\n6) The related works are very sparse, especially for the 3D instantce segmentations. I suggest the author refer to some surveys about the 3D segmentation and discuss more papers on it, especially for the pointnet and pointnet++.\n7) For the comparison, it is not clear to me: Are the fewer user clicks the same for the different methods?\n8) The failure cases and limitations are not discussed comprehensively in the submission; these parts are very important for the complete manuscripts, and some failure cases should be presented in the submission."
                },
                "questions": {
                    "value": "The strengths of the paper lie in the comprehensive information provided, the inclusion of supplementary materials, and the thorough explanations. However, the lack of novelty, limited evaluation, and other weak issues Although the appendix serves its purpose as a resource for implementing AGILE3D, it does not significantly contribute to the field. Considering these strengths and weaknesses, I am negative about the submission currently, but I look forward to the response to the above questions.\n\nsee weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5206/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698847538055,
            "cdate": 1698847538055,
            "tmdate": 1699636517536,
            "mdate": 1699636517536,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nJazIFT9ch",
                "forum": "9cQtXpRshE",
                "replyto": "2AjtLSEzgr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5206/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5206/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer byjT [Part 1/3]"
                    },
                    "comment": {
                        "value": "We thank the reviewer for spending time reviewing our paper, and for the advice regarding related work and failure cases. **We do, however, disagree with some of the other comments and believe they must be due to misunderstandings**. We therefore respond below to every individual question/concern:\n\n**1. Lack of Novelty**\n> For the proposed methods, the novel point is the multi-object segmentation, and the fast inference is also a novel point to some degree.\n\nWe are not sure what is meant here. The reviewer starts by saying, in their first sentence, that our multi-object segmentation and fast inference are novel. But then goes on to criticize a lack of novelty. We can only point out once more that\n\n(1) as far as we know, our work is the first to do interactive multi-object segmentation of 3D point clouds.\n\n(2) our architecture is significantly different from state-of-the-art (see Fig. 1).\n\n(3) our method yields high-quality instance masks with less human effort (fewer clicks), and also offers faster inference; thus making the labeling of 3D segmentation masks much more efficient.\n\n> And for efficiency, the submission does not present a technical point on how to improve efficiency\n\n**We respectfully disagree, this is not correct.**  The efficiency of our method comes from two sources: (1) an architecture designed with fast inference in mind; and (2) more importantly, fewer user clicks due to multi-object handling. In detail: \n\n(1) our proposed **architecture** minimizes computational cost, as described in the introduction and illustrated in Fig.1: the feature backbone only needs to be run once for a point cloud, whereas clicks are processed separately, so that for every interaction one must only run the lightweight decoder (not the complete forward pass, like for instance InterObject3D). The comparison in Sec. 4.2, respectively Tab. 5 confirms that our **inference is 2\u00d7 faster** than the baselines.\n\n(2) The **multi-object formulation** allows for concurrent segmentation of several objects (as opposed to prior work that processes one object at a time). Our scheme therefore needs fewer clicks than the baselines to achieve high IoU. In interactive segmentation, the crucial point is to **gain efficiency by saving user clicks**. \n\nIn this context **we note that our view is stand by all other reviewers**:\n\n-reviewer **PRcH**: *\u201cThe model ... champions efficiency, demanding fewer user inputs and providing swifter inference times\u201d, \u201cdetailed computational cost comparisons\u201d, \u201cdiminished user interaction and expedited inference ... is empirically substantiated.\u201d*\n\n-reviewer **3vqw**: *\u201cthe proposed approach can reduce annotation time\u201d*\n\n-reviewer **Gxfu**: *\u201cthe author performed Efficiency Comparison, which is necessary for user-interactive tasks.\u201d*\n\n>Multi-object segmentation is also achieved by the previous methods (InterObject3D) via some small improvements.\n\n**Also here we disagree**. Perhaps there was misunderstanding because we adapted InterObject3D so that it could serve as a baseline? Since there isn\u2019t any prior work about multi-object interactive segmentation, we had no baseline to compare to. Therefore, we upgraded InterObject3D, the state-of-the-art for single-object segmentation, such that it could be evaluated by our protocol. The \u201cbaseline\u201d paragraph in Sec. 4 explains our modifications. But although we modified InterObject3D for evaluation on our multi-object benchmark **it can, by design, only predict a binary mask for a single object** in every pass through the network, and those masks must be merged into a multi-object segmentation either heuristically or by hand.\n\n>Actually, the performance on the multi-object segmentation is not the best, according to Table 4.\n\n**This is not true**. We are not sure how the reviewer arrived at that conclusion. The fact is that **AGILE3D outperforms previous methods** by a significant margin reaching \u201c*top-tier results*\u201d [reviewer **PRcH**] and \u201c*state-of-the-art performance*\u201d [reviewer **Gxfu**]. Table 4 shows 6 metrics for each of the 3 datasets. Among the 18 comparisons in total, AGILE3D outperforms standard InterObject3D **in all cases**, and outperforms our enhanced InterObject3D++ in **17 out of 18 cases**. Please do not only check the metrics in isolation but overall for KITTI-360, in that particular case AGILE3D surpasses the baseline by **a factor of 4** and the enhanced baseline by **a factor of 2** on $\\overline{\\textrm{IoU}}@\\overline{\\textrm{5}}$."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5206/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699977317735,
                "cdate": 1699977317735,
                "tmdate": 1699984805257,
                "mdate": 1699984805257,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "elnw8BbbbO",
                "forum": "9cQtXpRshE",
                "replyto": "2AjtLSEzgr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5206/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5206/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer byjT [Part 2/3]"
                    },
                    "comment": {
                        "value": "**2. Limited Evaluation**\n> The appendix focuses more on providing information and supplementary materials than conducting extensive evaluations. A more thorough evaluation of the framework's performance and comparison with existing approaches would enhance the paper's credibility. The various datasets are also a strong evaluation, especially for the interactive tools. I suggest some results on the real captured and other challenging datasets should be presented in the paper.\n\nWe do not see why our evaluation would be lacking, and **our sufficient evaluation is appreciated by all other reviewers**: **PRcH**: \u201c*Comprehensive evaluations indicate its superiority \u2026*\u201d; **3vqw**: \u201c*Sufficient experiments have been conducted \u2026*\u201d; **Gxfu**: \u201c*The author \u2026 conduct extensive experiments to validate the benefits \u2026*\u201d.\n\nWe evaluate not only existing (single-object) interactive segmentation benchmark, and also provide a new multi-object benchmark. Both benchmarks include **multiple datasets of real scans**, among them **both indoor (ScanNet, S3DIS) and outdoor scans (KITTI-360)** that are all commonly used to evaluate 3D scene understanding. Since interactive 3D segmentation has only been studied recently, we have also run a state-of-the-art fully-supervised method (Mask3D) for a **complete and credible evaluation**. Finally, we have conducted **real user studies** to rule out biases due to simulated clicks, where real users worked with our interactive tools to annotate 5 real scenes that contain a variety of object categories. We speculate that the reviewer might be missing more qualitative results and have therefore moved them from the appendix to the main paper.\n\n\n**3. The running time should be reported in a fair manner**\n\nWe fully agree, and did our best to ensure a fair comparison. Run times for all methods were measured in the **same setting** on the **same hardware** (a single TITAN RTX GPU).\n\n>for table 5, it is not clear how to perform the comparison since the manuscript claims efficiency as a contribution.\n\nThe paragraph \u201cefficiency comparison\u201d in Sec. 4.2 describes this. **Inference time is measured after all objects in a given scene have received 5/10/15 clicks on average**. E.g., for a scene with 6 objects the t@$\\overline{5}$ is the total time spent on forward passes through the network after 6\u00d75=30 clicks. Throughout, our method is **2\u00d7 times faster** (1.0s vs 2.4s at t@$\\overline{10}$; 1.6s vs 3.5s at t@$\\overline{15}$; etc.).\n\n> Or, the user study is a good choice to compare the efficiency. \n\nWe fully agree, that is why we have conducted a real user study, see Tab. 7. Each user was asked to annotate 5 scenes with, in total, 30 objects. We report the final number of clicks ($\\overline{\\textrm{NoC}}$, average of all objects), the intersection-over-union achieved with those clicks ($\\overline{\\textrm{IoU}}$, average of all objects), and **the time spent by the user** to obtain that result ($\\overline{\\textrm{t}}$, average of all scenes). In the study, users achieved higher IoU with significantly fewer clicks, and in shorter times, with our proposed multi-object segmentation approach. I.e., the **user study confirms better efficiency with AGILE3D**.\n\n>In some challenging cases, there are a lot of occlusions between the segmented object and other objects in the scene.\n\nWe are not sure what the question is. Regarding missing data due to sensor occlusions: yes, the data has the characteristics of real scans. Regarding occlusions in the visualization: with our interactive tool users can use the mouse to rotate/translate/zoom in/zoom as needed to minimize occlusions. **All users quickly got familiar with the tool and we did not receive any feedback about occlusion problems**."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5206/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699977409723,
                "cdate": 1699977409723,
                "tmdate": 1699984691216,
                "mdate": 1699984691216,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UFPtIFgShJ",
                "forum": "9cQtXpRshE",
                "replyto": "2AjtLSEzgr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5206/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5206/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer byjT [Part 3/3]"
                    },
                    "comment": {
                        "value": "**4. The difference between interobject3D and interobject3D++**\n\nThe difference is explained in the \u201cbaselines\u201d paragraph in Sec. 4. **InterObject3D is the only recent published work** about (single-object) interactive 3D segmentation. **InterObject3D++ is our enhanced version** (i.e., a harder baseline) that we have derived from it. The reason for introducing it is that the iterative training strategy that we propose in our paper can be readily applied also to InterObject3D. **We found that, indeed, iterative training significantly improved InterObject3D. We call the enhanced version InterObject3D++**, and include it for a complete and fair comparison that properly disentangles our model design from the training strategy. **AGILE3D beats also InterObject3D++, indicating that its advantage is due to the multi-object model design and not only the result of a better engineered training strategy**.\n\n**5. Visualizations have the faces**\n\nFor all visualizations of ScanNet, e.g., Fig. 1 and for the interactive demo (Fig. 12), we visualize the meshes. This is common practice, and only serves to make the renderings easier to read. E.g., [1,2,3] also plot faces, but do not use them in their computational methods. We also do not use face information since not every 3D dataset contains faces. It is of course true that one could potentially calculate normals from the faces and use them as additional input to the network, but that would mean that the network could no longer be applied to scan data that does not contain faces (like S3DIS, KITTI-360, etc.).\n\n[1] Peng, Songyou, et al. OpenScene: 3D Scene Understanding with Open Vocabularies. CVPR. 2023.\n\n[2] Schult, Jonas, et al. Mask3D for 3D Semantic Instance Segmentation. ICRA. 2023.\n\n[3] Chibane, Julian, et al. Box2Mask: Weakly Supervised 3D Semantic Instance Segmentation using Bounding Boxes. ECCV, 2022.\n\n**6. Related works**\n\nThank you for the suggestion. We have rewritten the related work part for 3D instance segmentation, including more discussions on point-based and voxel-based learning backbones and more details on existing methods on 3D instance segmentations, as well as references to surveys. Please check the red-marked content in our revised manuscript.\n\n**7. Are the fewer user clicks the same for the different methods?**\n\nFor comparison using simulated clicks, only the first click is the same for all methods. This serves as a fair, common starting point. The following clicks will be sampled from regions where a method makes mistakes, which obviously differ between methods. \n\nIn the study with real user clicks, even the first click could be different, as it would defeat the purpose of a realistic user study to enforce specific click locations for everyone. We did, on purpose, also not give explicit instructions how to click, in order to obtain realistic and representative user behavior.\n\n**8. Advice on failure cases and limitations**\n\nWe thank the reviewer for this suggestion. We previously had put the discussion of limitations in the appendix, but have now moved it to the main paper. Moreover, we have also added discussions on failure cases, which can hopefully motivate future work. Please check the red-marked content in our revised manuscript. We believe this update has improved the manuscript.\n\n### **To conclude, we regret if the rather under-explored topic of interactive 3D object segmentation, and the associated lack of a canonical \u201cstandard vocabulary\u201d, has caused misunderstandings. We hope our explanations will alleviate the reviewer\u2019s concerns - if yes please reconsider the overall score. If there are further questions and concerns, we are open to continuing the discussion.**"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5206/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699977557999,
                "cdate": 1699977557999,
                "tmdate": 1699977557999,
                "mdate": 1699977557999,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BJmm2u1S84",
                "forum": "9cQtXpRshE",
                "replyto": "nJazIFT9ch",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5206/Reviewer_byjT"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5206/Reviewer_byjT"
                ],
                "content": {
                    "title": {
                        "value": "response"
                    },
                    "comment": {
                        "value": "In Figure 1, the illustration also claims that InterObject3D also supports interactions, which conflicts with the second contribution \"the first interactive approach\".\nAnd I agree with the fast inference of the proposed method from the reported numbers, but I don't see any technical points that are introduced in the paper to support the fast inference.\nFor InterObject3D, the methods can be used to finish the tasks, but the only different point of the proposed methods is the fast inference. And the paper is not focused on efficiency, and this part is only small.\nFor the performance in table 4, the reported number is not very significant over the previous methods (ie. just 0.x for NoC). And also, all the reported numbers are not the best, such as IoU@15 for KITTI-360 test data."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5206/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700030563241,
                "cdate": 1700030563241,
                "tmdate": 1700030563241,
                "mdate": 1700030563241,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Qa8JTYVuID",
                "forum": "9cQtXpRshE",
                "replyto": "elnw8BbbbO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5206/Reviewer_byjT"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5206/Reviewer_byjT"
                ],
                "content": {
                    "title": {
                        "value": "response"
                    },
                    "comment": {
                        "value": "I just curious about more challenging cases, such as when the segmented object is occluded by other objects, and more real-world captured data. Over all the presented  results, I don't see the diversity of the dataset that is evaluated by the proposed methods; there are only few examples."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5206/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700030818029,
                "cdate": 1700030818029,
                "tmdate": 1700030818029,
                "mdate": 1700030818029,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MLqY5Bm5j4",
                "forum": "9cQtXpRshE",
                "replyto": "2AjtLSEzgr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5206/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5206/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Further clarification on interactive multi-object segmentation (Part 2/2)"
                    },
                    "comment": {
                        "value": "> For the performance in table 4, the reported number is not very significant over the previous methods (ie. just 0.x for NoC). And also, all the reported numbers are not the best, such as IoU@15 for KITTI-360 test data.\n\nHere we show the results on table 4 and **explicitly mark the improvement** of our method over baselines in brackets. \n\nTable 4. Quantitative results on interactive multi-object segmentation. We adapt the state-of-the-art method in interactive single-object segmentation to be evaluated by our multi-object protocol for a complete comparison (Baseline paragraph of Sec. 4). **Note the baselines still predict binary masks for single-object and final masks must be merged manually**.\n\n| Methods |  Train $\\rightarrow$ Eval |   $\\overline{\\textrm{IoU}}@\\overline{\\textrm{5}} \\uparrow$ |  $\\overline{\\textrm{IoU}}@\\overline{\\textrm{10}} \\uparrow$| $\\overline{\\textrm{IoU}}@\\overline{\\textrm{15}} \\uparrow$|$\\overline{\\textrm{NoC}}@\\overline{\\textrm{80}} \\downarrow$ |  $\\overline{\\textrm{NoC}}@\\overline{\\textrm{85}} \\downarrow$| $\\overline{\\textrm{NoC}}@\\overline{\\textrm{90}} \\downarrow$|\n|----------|:------:|:------:|:------:|:------:|:------:|:------:|:------:|\n| InterObject3D |  | 75.1 | 80.3 | 81.6 | 10.2 | 13.5 | 16.6|\n| InterObject3D++ | ScanNet40 $\\rightarrow$ ScanNet40 |79.2 | 82.6 | 83.3 | 8.6 | 12.4 | 15.7 |\n| **AGILE3D (ours)** |   |**82.3 (+3.1)** | **85.0 (+2.4)** | **86.0 (+2.7)** | **6.3 (-2.3)** | **10.0 (-2.4)** | **14.4 (-1.3)** |\n| InterObject3D |  | 76.9 | 85.0| 87.3 | 6.8 | 8.8 | 13.5|\n| InterObject3D++ |  ScanNet40 $\\rightarrow$ S3DIS-A5 |81.9| 88.3 | 89.3 | 5.7 | 7.6 | 11.6 |\n| **AGILE3D (ours)** |   |**86.3 (+4.4)** | 88.3 | **90.3 (+1.0)** | **3.4 (-2.3)** | **5.7 (-1.9)** | **9.6 (-2.0)** |\n| InterObject3D |  | 10.5 | 22.1| 31.0 | 19.8 | 19.8 | 19.9|\n| InterObject3D++ |  ScanNet40 $\\rightarrow$  KITTI-360 |16.7| 37.1 | 52.2 | 18.3 | 18.9 | 19.3 |\n| **AGILE3D (ours)** | |**40.5 (+23.8)** | **44.3 (+7.2)** | 48.2 (-4.0) | **17.4 (-0.9)** | **18.3 (-0.6)** | **18.8 (-0.5)** |\n\nFirst, we agree the gap of NoC between our method and baselines on  KITTI-360 is not as significant as on ScanNet40 and S3DIS. However, **it is not correct to assume our method is not significant over the baselines**. We follow standard protocol in the interactive segmentation community to **threshold NoC metrics at 20 (to prevent unbounded click budgets)**, i.e. if one method didn\u2019t achieve 80 IoU after 20 clicks, then NoC@80 was thresholded at 20. Due to the biggest domain gap lying in KITTI-360, all the methods can have cases that didn\u2019t achieve the required IoU after 20 clicks. Even if we can achieve 80 IoU with 30 clicks but another one with 40, both of them are thresholded to 20 clicks, which would obviously narrow down their actual gaps. **To give the overall picture, here we also show NoC@50, 65, 70 and the challenging low-click regime IoU@1, 2, 3, which further verifies that we outperform baselines by a large margin.**\n\nTable 14. More results on low IoU regime and low lick regime on KITTI-360, which supplements tab. 4 to give an overall picture.\n| Methods |  Train $\\rightarrow$ Eval |   $\\overline{\\textrm{IoU}}@\\overline{\\textrm{1}} \\uparrow$ |  $\\overline{\\textrm{IoU}}@\\overline{\\textrm{2}} \\uparrow$| $\\overline{\\textrm{IoU}}@\\overline{\\textrm{3}} \\uparrow$|$\\overline{\\textrm{NoC}}@\\overline{\\textrm{50}} \\downarrow$ |  $\\overline{\\textrm{NoC}}@\\overline{\\textrm{60}} \\downarrow$| $\\overline{\\textrm{NoC}}@\\overline{\\textrm{70}} \\downarrow$|\n|----------|:------:|:------:|:------:|:------:|:------:|:------:|:------:|\n| InterObject3D |  | 1.9 | 3.7 | 6.0 | 17.8 | 18.8 | 19.3|\n| InterObject3D++ |  ScanNet40 $\\rightarrow$  KITTI-360 |3.7| 5.8 | 8.8 | 13.8 | 15.4 | 16.9 |\n| **AGILE3D (ours)** | |**34.9 (+31.2)** | **38.1 (+32.3)** | **39.4 (+30.6)** | **10.4 (-3.4)** | **13.1 (-2.3)** | **14.3 (-2.6)** |\n\n### **We kindly ask the reviewer to consider all the results as a whole - if the reviewer thinks our method is not the best, then which one is?**"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5206/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700227966446,
                "cdate": 1700227966446,
                "tmdate": 1700567593605,
                "mdate": 1700567593605,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "a1JMMAoiqQ",
                "forum": "9cQtXpRshE",
                "replyto": "2AjtLSEzgr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5206/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5206/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Further clarification on evaluation"
                    },
                    "comment": {
                        "value": "> I just curious about more challenging cases, such as when the segmented object is occluded by other objects, and more real-world captured data. Over all the presented results, I don't see the diversity of the dataset that is evaluated by the proposed methods; there are only few examples.\n\nFirst, the fact is that our evaluation datasets **ScanNet, S3DIS, and KITTI-360 are all challenging real-world captured datasets, widely benchmarked in literature.**\n\nSecond, **we want to know the justified reasons from the reviewer why you think ScanNet, S3DIS, and KITTI-360 are not challenging and not real-world enough**. Is there another dataset that you believe will offer something more significant than these real-world datasets we evaluated already? If yes please let us know which one and why it is better than the ones we used already.\n\n### **Now we included one-page more qualitative results (see Fig. 11 in page 15) in various challenging cases.**\n\nWe further included **one page more qualitative results in various challenging cases: target objects that are occluded by other objects, small objects, connected/overlapped objects, un-rigid objects, thin structures around objects, and extremely sparse outdoor scans**. Note here we visualize directly on the raw point clouds (not mesh). We hope now the reviewer can realize the challenging nature of the datasets we used. Now we included them in the appendix due to the limited pages in the main paper. If the reviewer recognizes our added results, we will add them to the main paper by squeezing more space.\n\n### **To conclude, regarding the reviewer\u2019s two main concerns on interactive multi-object 3D segmentation and evaluation:**\n(1) The **baseline** method can only predict **binary masks for single objects** by its network design and final masks must be merged by hand. **It can not be seen as interactive multi-object 3D segmentation**. **Our method is the first approach that supports interactive multi-object 3D segmentation, directly predicting multi-object masks end-to-end, offering high-efficiency, SOTA results, which are also recognized by other reviewers.**\n\n(2) **Our extensive evaluations on challenging and real-world captured datasetsScanNet, S3DIS, and KITTI-360 are recognized by all other reviewers**. **Now we further revised the manuscript to include one-page more qualitative results in various challenging cases.** Also if the reviewer has in mind other real-world datasets that can offer more insight than the ones we used already please let us know and we will add it in the final version of the paper.\n\n**If the reviewer still has concerns, we are open to further discussions.**"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5206/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700228201892,
                "cdate": 1700228201892,
                "tmdate": 1700228638140,
                "mdate": 1700228638140,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "o8jnVFU5KG",
                "forum": "9cQtXpRshE",
                "replyto": "MLqY5Bm5j4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5206/Reviewer_byjT"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5206/Reviewer_byjT"
                ],
                "content": {
                    "title": {
                        "value": "response"
                    },
                    "comment": {
                        "value": "I still have some concern about the overclaim for performance over InterObject3D++. From all reported numbers, the performance cannot achieve the best one fully (mentioned before), and for the evaluations, the presented visual result cannot cover the diversity of datasets. Hence, the manuscript are not above the acceptance bar of the conference and I will keep my origianl score. I am very happy to listen to the opinions of other reviewers.\nI suggest adding more data to evaluate the proposed methods during revision."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5206/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700241420578,
                "cdate": 1700241420578,
                "tmdate": 1700241420578,
                "mdate": 1700241420578,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]