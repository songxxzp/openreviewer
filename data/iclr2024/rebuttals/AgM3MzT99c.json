[
    {
        "title": "OMNI: Open-endedness via Models of human Notions of Interestingness"
    },
    {
        "review": {
            "id": "0gRMLgJelG",
            "forum": "AgM3MzT99c",
            "replyto": "AgM3MzT99c",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2863/Reviewer_CZLW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2863/Reviewer_CZLW"
            ],
            "content": {
                "summary": {
                    "value": "Paper introduces a new curriculum generation approach for reinforcement learning agents. From the high level, it includes 1) an LP model, which ranks the tasks based off the difficulty of the current learning progress of the agents; 3) an interestingness model (Mol), which ranks the tasks out of the \"interestingness\", a subjective view by human infused to the LLM. The proposed method, OMNI, is evaluated on life-long RL in two domains: crafter and babyAI. Basically, the agent will follow the task proposed by OMNI, an ablative version of OMNI, or a random task selector. The results show that agents learned with the curriculum produced by OMNI have clear advantages."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+The research topic presented here is relevant and important. There have been active explorations of how to turn the powerful LLM into general agents. Building a curricula generator is definitely very promising and should be of interest to a range of audiences from general agents to LLMs.\n\n+The paper is clear and well-written. The proposed method is technically sound -- LLM indeed has some infused human prior that could help determine the \"interestingness\" of the tasks and help build more reasonable curricula for RL.\n\n+The results on crafter and babyAI look quite promising. The advantages over random curriculum generators and ablative versions of OMNI are clear. The additional experiments in the appendix are very thorough."
                },
                "weaknesses": {
                    "value": "At this point, I do not have major concerns. But I do hope the authors can help address the following questions:\n\n-What if the LLM does not have the human prior or reasoning capacity about the task \"interestingness\" on the given domain?\n\n-Based on my personal experiences of training agents in Crafter, an agent with a random task generator (Uniform) should not be that bad as it should be able to master not just a limited set of tasks as the curve in Figure 3 suggested. Can the authors clarify this?\n\n-If my understanding is correct, LP seems to be more crucial to the success of mastering more challenging tasks as it takes the learning process into consideration, while \"interestingness\" is about not wasting time on tasks that could not lead to meaningful learning. Therefore, if an unlimited round of learning is allowed, an agent with LP only should ultimately match the results of full OMNI on the average success rate. However, this is not the case in all the experiments. Maybe the LP baseline is not run for enough rounds? I would like to learn more about your thoughts on this.\n\nMinor: some references on building open-ended agents in open-world environments should be cited [1-5] \n\n[1] open-world control: https://arxiv.org/abs/2301.10034\n\n[2] DEPS: https://arxiv.org/abs/2302.01560\n\n[3] STG transformer: https://arxiv.org/abs/2306.12860\n\n[4] Plan4MC: https://arxiv.org/abs/2303.16563\n\n[5] GITM: https://arxiv.org/abs/2305.17144"
                },
                "questions": {
                    "value": "See \"weaknesses\""
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2863/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2863/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2863/Reviewer_CZLW"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2863/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698659140970,
            "cdate": 1698659140970,
            "tmdate": 1700561831886,
            "mdate": 1700561831886,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "b8r9ggJbNG",
                "forum": "AgM3MzT99c",
                "replyto": "0gRMLgJelG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2863/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2863/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your very insightful review, and kind words about the paper's strengths. We now address each of your concerns and questions.\n\n> What if the LLM does not have the human prior or reasoning capacity about the task \"interestingness\" on the given domain?\n\nGiven the extensive dataset that LLMs have been trained on, it is unlikely that LLM would not already have some prior on any task that is human-generated. Regardless, we believe OMNI being helpful on many/ most tasks is still valuable, even if it is not helpful on all. \n\nIn the case where the LLM does not have a good enough notion of interestingness, we propose to use human feedback to train (or fine-tune) a model of interestingness. This is termed Open-Endedness with Human Feedback (OEHF) in Section 6 (Section 5 of the original manuscript), as part of future work. Similar to Reinforcement Learning with Human Feedback (RLHF), one could potentially train a model that can effectively capture an ineffable property that, although challenging to quantitatively measure, is readily identifiable upon observation (e.g., a backflip for RLHF, or whether a task is interesting for OEHF).\n\n> Based on my personal experiences of training agents in Crafter, an agent with a random task generator (Uniform) should not be that bad as it should be able to master not just a limited set of tasks as the curve in Figure 3 suggested. Can the authors clarify this?\n\nIf all of the tasks are interesting (i.e., only the 15 tasks designed in the original Crafter environment), using uniform sampling would be sufficient. This effectiveness can be attributed to the fact that these environments are often already designed with some human intuition for interestingness, only including tasks or features that we instinctively find interesting.\n\nHowever, in the broader context of open-ended learning where we want environments with infinite possible tasks, it is extremely difficult to design an environment or task space that only contains interesting features. Take a kitchen environment for example, there can be endless variations of possible tasks (e.g., putting a cup in slightly different positions), and it is almost impossible to curate a set of tasks that are all interesting. We modified Crafter and BabyAI environments by expanding the task space beyond the carefully selected few in order to emulate this phenomenon from vast, open-ended environments. It is because there are many boring or impossible tasks in the modified versions of these domains that uniform sampling fails.\n\nIn new experiments, we demonstrate OMNI\u2019s ability to handle just such an infinite task space (the set of all tasks specifiable in natural language). Please see our general reviewer response for a summary.\n\n> If my understanding is correct, LP seems to be more crucial to the success of mastering more challenging tasks as it takes the learning process into consideration, while \"interestingness\" is about not wasting time on tasks that could not lead to meaningful learning. Therefore, if an unlimited round of learning is allowed, an agent with LP only should ultimately match the results of full OMNI on the average success rate. However, this is not the case in all the experiments. Maybe the LP baseline is not run for enough rounds? I would like to learn more about your thoughts on this.\n\nIn an ideal scenario with infinite compute, an agent guided solely by LP could converge to performance levels of an agent utilizing the full OMNI approach. However, in reality, having infinite compute is impossible. In a truly open-ended setting, there are an infinite number of uninteresting tasks that LP will inevitably get distracted by. For example in the real world, there can be endless variations of mathematical, or even addition, questions (e.g. just by changing constants) that the agent could attempt. In the Crafter environment, LP is distracted by boring repetitive tasks, which require substantial compute and training steps, whereas OMNI is able to filter out the boring tasks and only focus on the interesting tasks to gain more meaningful learning.\n\n> Minor: some references on building open-ended agents in open-world environments should be cited [1-5]\n\nThank you for sharing these works, we have included them in the Related Works section of the revised manuscript.\n\nWe hope we have answered all your questions and concerns. We fear the current scores will preclude our paper from being published and shared with the ICLR community, despite the many positive reviewer comments on its originality and likely long term impact. If you are open to considering a higher score, that would improve the chances of the paper being accepted and we would deeply appreciate it."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2863/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700262196543,
                "cdate": 1700262196543,
                "tmdate": 1700262196543,
                "mdate": 1700262196543,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ibaaRszmuu",
                "forum": "AgM3MzT99c",
                "replyto": "b8r9ggJbNG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2863/Reviewer_CZLW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2863/Reviewer_CZLW"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you. My concerns have been addressed. I've raised my score accordingly."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2863/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700561811748,
                "cdate": 1700561811748,
                "tmdate": 1700561811748,
                "mdate": 1700561811748,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mAtSZcxqby",
            "forum": "AgM3MzT99c",
            "replyto": "AgM3MzT99c",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2863/Reviewer_LSLH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2863/Reviewer_LSLH"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on an interesting and practical scenario that the agent is exploring in an open-ended environment and learning new behaviors forever. Massive unlearned tasks exists in such environment, and how to choose the task to learn becomes a challenge. Beyond conventional choosing criteria, interestingness is an important aspect which will be largely considered by humans when exploring the open-ended environment, but is hard to be measured. This paper proposes to use large models, which already have encoded many human knowledge, as a model of interestingness, providing guidance for the agent to choose the task to learn. The authors further provide a simple implementation for such the above idea. Two environments (Crafter and BabyAI) are tested in experiments, and good results are obtained for both environments."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper targets an important problem. Open-endedness needs to be considered in many practical scenarios, where agents need to learn and improve themselves in interactive environments.\n\nThe intuition and idea behind this paper is very reasonable and promising. Large models offer us many potentials, making some previously impossible tasks feasible now, and this paper successfully finds such a task. Interesting tasks are hard to define, and now with the help of large models, one can ask what tasks are interesting and explore environments like a human. Also, finding interesting tasks is very important in open-endedness research.\n\nOverall, I like the idea of this paper, and believe that this principle will have impact on open-endedness research."
                },
                "weaknesses": {
                    "value": "The baselines compared in this paper are not strong enough. All baselines are intuitively set, and some previously proposed methods (some are discussed in related work, like curriculum learning for RL) need to be included. \n\nAlthough the authors claim that OMNI is a principle and this paper only presents one instance, practical implementation is essential to verify the proposed principle, and some designs in the current algorithm may need to be improved.\n- It might not be practical to know all candidate tasks in advance, and just let the large model choose one. In the RL setup, the agent needs to explore the environment and finds out all candidate tasks. \n- The downsampling method for boring tasks is quite simple now (multiplying by 0.001). The applicability of this method needs to be discussed and analyzed.\n\nHuman alignment also needs to be discussed. This paper just uses the interestingness of large models as that of human to do exploration. On the other hand, alignment is an unsolved question that targets aligning human and large models and avoiding dangerous actions. Methods like RLHF are proposed here. Considering that human alignment is not perfect now, how the interestingness of large models be used for choosing tasks and some important aspects, like safety, are also ensured at the same time? \n\nThe missing related work (https://arxiv.org/pdf/2302.06692.pdf) uses language model to suggest plausibly useful goals, which shares similar idea with this paper."
                },
                "questions": {
                    "value": "Why is the survival component removed in the Crafter environment? This is quite important in a practical open-ended environment. Without considering the survival component, the aspects that need to be considered during choosing the task become less adequate and complex, which goes against the motivation of using LLMs here.\n\nIn experiments, how does the large model know what tasks are interesting tasks, given that interesting tasks are pre-defined by the authors?\n\nIt\u2019s better to show instructions used in determining interesting tasks. How to query the large model and what kind of results are obtained from it? An example here might be helpful."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2863/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698818471307,
            "cdate": 1698818471307,
            "tmdate": 1699636229779,
            "mdate": 1699636229779,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0o0gd1lBiw",
                "forum": "AgM3MzT99c",
                "replyto": "mAtSZcxqby",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2863/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2863/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your feedback. We appreciate your recognition of the strengths in our approach and presentation. We now address each of your concerns and questions.\n\n> The baselines compared in this paper are not strong enough. All baselines are intuitively set, and some previously proposed methods (some are discussed in related work, like curriculum learning for RL) need to be included.\n\nLearning progress (LP) is an RL curriculum learning method, which has been demonstrated by previous works as a strong baseline (Kanitscheider et al., 2021). Using LP as an RL curriculum, Kanitscheider et al. (2021) could train Minecraft agents to collect diamonds from scratch, which has been recognized as an extremely challenging RL task. We will make this clearer in the manuscript.\n\nWe have added more experiments to our study. These new experiments, like related works (e.g., intrinsic motivation, novelty search, diversity), rely on pre-specified definitions of what counts as interestingly different. We compare OMNI with an alternative MoI which relies on predefined heuristics based on sentence embeddings. In this alternative, embedding-based MoI (eMoI), interestingness is determined by the distances between sentence embeddings of task descriptions. We show where such predefined metrics fail, and that OMNI outperforms the controls significantly. Please see Appendix P in the revised manuscript for the full details and results.\n\n> It might not be practical to know all candidate tasks in advance, and just let the large model choose one. In the RL setup, the agent needs to explore the environment and finds out all candidate tasks.\n\nThank you for pointing this out. We also knew that such a domain was the next frontier for OMNI (we planned on it being a follow-up paper, but are happy to add it to this paper instead). So immediately after submitting the original version, we began testing OMNI in an infinite, complex, robotics domain, and are pleased to report that OMNI far outperforms controls (the set of all tasks expressible in natural language). The revised manuscript includes these results, please see our general reviewer response for a summary.\n\n> Human alignment also needs to be discussed. [...] Considering that human alignment is not perfect now, how the interestingness of large models be used for choosing tasks and some important aspects, like safety, are also ensured at the same time?\n\nWe agree that human alignment and safety are important and interesting research challenges for open-endedness. We have added a discussion on this topic in Appendix R of the revised manuscript.\n\n> The missing related work (https://arxiv.org/pdf/2302.06692.pdf) uses language model to suggest plausibly useful goals, which shares similar idea with this paper.\n\nDu et al. (2023) was discussed in Section 2.3 of Related Works in the original manuscript. Please let us know if there are more comments regarding it that you feel we should add.\n\n> Why is the survival component removed in the Crafter environment? This is quite important in a practical open-ended environment. Without considering the survival component, the aspects that need to be considered during choosing the task become less adequate and complex, which goes against the motivation of using LLMs here.\n\nBy removing the survival component, task completion is less susceptible to external stochastic events like whether a skeleton has killed the player. It removes the need for the agent to learn and continually apply survival tactics against enemies or for food gathering. In other words, it allows us to focus on evaluating the tasks specified with less evaluation noise.\n\nFor completeness, based on your request, we conducted new experiments with the original Crafter environment, where survival components are retained. The findings from these experiment setup are consistent with our initial results, where OMNI outperforms Learning Progress and Uniform sampling significantly. For more details and results, please see Appendix O of the revised manuscript.\n\n> In experiments, how does the large model know what tasks are interesting tasks, given that interesting tasks are pre-defined by the authors?\n\nThere is a set of \u201cinteresting tasks\u201d that we manually chose, but that set of tasks is only for purposes of evaluating the trained model. The OMNI algorithm is not made aware of these choices. In OMNI, we tell the LLM which tasks are considered done well by the agent, and prompt the LLM to predict whether the remaining tasks are interesting or not.\n\n> It\u2019s better to show instructions used in determining interesting tasks. How to query the large model and what kind of results are obtained from it? An example here might be helpful.\n\nFull LLM prompts are provided in Appendices B and C. Appendix F (Appendix D in original manuscript) showcases a step-by-step example of how the model declares tasks as interesting or not. We have also added pseudocode in Section 3 of the revised manuscript for additional clarity."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2863/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700262155337,
                "cdate": 1700262155337,
                "tmdate": 1700262155337,
                "mdate": 1700262155337,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FicpoHjRkO",
            "forum": "AgM3MzT99c",
            "replyto": "AgM3MzT99c",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2863/Reviewer_qXzy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2863/Reviewer_qXzy"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a novel approach to enhance open-ended learning algorithms, which aim to continually learn new and interesting behaviors in vast environments with innumerable tasks. The core challenge addressed is the existing inability to quantify and prioritize tasks that are not only learnable but also inherently interesting\u2014deemed to be significant due to their worth and novelty. The authors propose the concept of Open-endedness via Models of human Notions of Interestingness (OMNI), leveraging large language models (LLMs) to serve as models of interestingness (MoIs). These LLMs are presumed to embody human perceptions of interestingness, having been trained on extensive human-generated data. The paper demonstrates through experiments that OMNI can direct open-ended learning towards tasks that are both learnable and interesting, showing superiority over baselines that rely solely on uniform task sampling or learning progress."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The idea of leveraging large language models to approximate human notions of interestingness in open-ended learning environments is novel and holds substantial promise.\n-  The paper includes detailed experiments across two domains that showcase the effectiveness of the OMNI framework in focusing learning on tasks that are both interesting and learnable."
                },
                "weaknesses": {
                    "value": "## Main concerns\n- The major weakness of this work is the lack of comprehensive comparitive analysis. The authors mention and properly cite a number of curriculum learning papers, and also works attempting to quantify interestingness. Why isn\u2019t any of these methods used for comparison? It is very obvious that OMNI would outperform uniform sampling and LP. The authors should try out LP + other metrics of interestingness, as described in Section 2.2. \n\n- The performance of OMNI and other baselines on BabyAI is extremely bad (success rate < 0.008). Have the authors taken a look at the policies and understood what is happening? I understand this includes hard tasks, but have the authors considered plotting the results for only interesting tasks, similar to Crafter?\n\n- In the experiments, the authors use a pre-defined set of interesting, boring and challenging tasks. However, it would also be interesting to try the methods in a more open-ended setting whereby they use a random task generator that procedurally generates virtually unlimited number of tasks.\n\n- The evaluation of the trained models is performed only on the previously seen tasks. I was wondering if the authors considered evaluating on held-out, and particularly out-of-distribution (OOD) tasks to assess the robustness of the trained methods.\n\n## Content and Writing Style Issues\n- A disproportionate emphasis on potential implications and future work in the paper might obscure the immediate contributions. For example, the Conclusion is > 1.25 page, mostly discussing future work. While this is very interesting, it feels to me that it takes the space from what actually was important in this paper and was moved to the Appendix. In the end, the reader has to go back and forth from main part of the paper to appendix to follow the ideas described in these different sections. It is my recommendation that the authors carefully consider what they want to include in the main part of the paper vs the appendix.\n\n> \"Perhaps, someday, unlimited energy and a cure for cancer can be added to the list\", \"ephemeral fuel ...\", \"eons of human experience\", \"shadows of what human sense could do\", \"intriguing prospect has arisen\", \"vast troves...\", \"to borrow from Newton\",...\n- Additionally, the stylistic choice of using lyrical language is atypical for scientific papers and could compromise the accessibility and clarity for a wider audience. \n\n> \"OMNI has the potential to significantly enhance the ability of AI to intelligently select which tasks to concentrate on next for endless learning and could pave the way for self-improving AI and AI-Generating Algorithms\"\n-  I think there is a giant gap between the what the authors presented in this work, compared to what they claim it has the potential of doing. I would recommend removing/rewarding such statements."
                },
                "questions": {
                    "value": "See above.\n\nUPDATE: I have updated the score following the rebuttal."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2863/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2863/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2863/Reviewer_qXzy"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2863/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699053367341,
            "cdate": 1699053367341,
            "tmdate": 1700565407729,
            "mdate": 1700565407729,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DSV8ddetVV",
                "forum": "AgM3MzT99c",
                "replyto": "FicpoHjRkO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2863/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2863/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your thoughtful and constructive feedback. We appreciate your positive remarks and critical analysis of our work. We now address each of your concerns and questions.\n\n> The major weakness of this work is the lack of comprehensive comparitive analysis. [...] The authors should try out LP + other metrics of interestingness, as described in Section 2.2.\n\nAs per your suggestion, we have added more experiments to our study. The new experiments, like the methods in section 2.2 (e.g., intrinsic motivation, novelty search, diversity), rely on pre-specified definitions of what counts as interestingly different. We compare OMNI with an alternative MoI which relies on predefined heuristics based on sentence embeddings. In this alternative, embedding-based MoI (eMoI), interestingness is determined by the distances between sentence embeddings of task descriptions. Please see Appendix P in the revised manuscript for the full details and results.\n\nWe show that the embedding-based MoI (eMoI) might suffice in distinguishing tasks with unique descriptions, but falls short in scenarios requiring more reasoning about the tasks. This exemplifies where pathologies might happen when we attempt to quantify the ineffable notion of interestingness. The additional results further reinforce the advantages of OMNI in focusing on learnable and interesting tasks, and highlights where autoregressive LMs might have an edge over predefined metrics of interestingness.\n\n>The performance of OMNI and other baselines on BabyAI is extremely bad (success rate < 0.008). Have the authors taken a look at the policies and understood what is happening? I understand this includes hard tasks, but have the authors considered plotting the results for only interesting tasks, similar to Crafter?\n\nAs you guessed, the average success rate is low because it is an average across the entire taskset of 1364 tasks. Most tasks are very difficult, which are not even completed at a success rate of 0.05 at any time step by any method. The reason why the tasks are so difficult in BabyAI is that each task does not depend on the target object(s) or environment configuration in that episode. For example, the target object distribution for a GoTo task can be any object in the environment distribution. If the object is in another locked room, the GoTo task would implicitly require the agent to find a key, unlock and open the door, before navigating to the target object.\n\nWe like your suggestion of plotting on interesting tasks only. We did a version of that by averaging success rates over tasks with instruction lengths of one or two only (see additional Figure 15 in Appendix K). OMNI significantly outperforms controls, and shows a more impressive completion rate of 16%.\n\n> In the experiments, the authors use a pre-defined set of interesting, boring and challenging tasks. However, it would also be interesting to try the methods in a more open-ended setting whereby they use a random task generator that procedurally generates virtually unlimited number of tasks.\n\nThank you for the suggestion. We are equally as excited to share the results of OMNI in an infinite, complex, robotics domain, and are pleased to report that OMNI far outperforms controls (the set of all tasks expressible in natural language). The revised manuscript includes these results, please see our general reviewer response for a summary.\n\n> The evaluation of the trained models is performed only on the previously seen tasks. I was wondering if the authors considered evaluating on held-out, and particularly out-of-distribution (OOD) tasks to assess the robustness of the trained methods.\n\nIn the current version, we are not claiming that OMNI can generalize to OOD tasks by training on interesting and learnable tasks. We agree that testing this would be an interesting future research direction, especially once OMNI is trained at scale.\n\n> A disproportionate emphasis on potential implications and future work in the paper might obscure the immediate contributions. [...] While this is very interesting, it feels to me that it takes the space from what actually was important in this paper and was moved to the Appendix. In the end, the reader has to go back and forth from main part of the paper to appendix to follow the ideas described in these different sections.\n\nWe apologize for the need to switch between the main text and the appendix. If accepted to ICLR, we will commit to moving some of the experimental details back to the main text, and some future work to the appendix. \n\n> the stylistic choice of using lyrical language is atypical for scientific papers and could compromise the accessibility and clarity for a wider audience. I think there is a giant gap between the what the authors presented in this work, compared to what they claim it has the potential of doing. I would recommend removing/rewarding such statements.\n\nWe have toned down some of the writing style and claims in the revised manuscript."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2863/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700262122119,
                "cdate": 1700262122119,
                "tmdate": 1700262122119,
                "mdate": 1700262122119,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "O6VpeMvyo7",
                "forum": "AgM3MzT99c",
                "replyto": "FicpoHjRkO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2863/Reviewer_qXzy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2863/Reviewer_qXzy"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal"
                    },
                    "comment": {
                        "value": "I thank the authors for a detailed response to my concerns and for adding new additional experiments that certainly improve the quality of the manuscript. I have updated my score accordingly."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2863/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700565489905,
                "cdate": 1700565489905,
                "tmdate": 1700565525715,
                "mdate": 1700565525715,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VjCGqGEO1W",
            "forum": "AgM3MzT99c",
            "replyto": "AgM3MzT99c",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2863/Reviewer_MxZx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2863/Reviewer_MxZx"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to use LLMs to measure how interesting tasks are for open-ended algorithms to learn. The problem which they tackle is that for open-ended algorithms, there are an infinite number of tasks which that agent could try to learn. This can be problematic for two reasons: the tasks may be unlearnable, or they may be uninteresting. The paper notes that the notion of \"interestingness\" is hard to measure exactly, but that humans know it when they see it. The paper proposes to combine measures two measures: learning progress (to ignore tasks which are unlearnable, and which has been explored in previous work), and interestingness (which is here measured using an LLM). They construct variants of the Crafter and BabyAI environments to test this approach, and find that it outperforms uniform sampling of tasks and prioritizing tasks by learning progress alone.\n\nOverall, this paper addresses an interesting problem and I think the approach has potential, but I do not think it's ready for publication due to the experiments which are not convincing enough and the presentation which needs work. I think this paper would benefit from another revision cycle giving time to add experiments on more convincing environments and improve the writing, after which it would make a strong submission."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is well motivated and the problem is interesting. They correctly identify one of the fundamental issues with open-ended learning algorithms, i.e. that if the agents choose tasks based on novelty, feasibility or information gain alone, they may end up choosing tasks which are completely uninteresting to humans, but which a  _tabular rasa_ agent has no reason to deprioritize."
                },
                "weaknesses": {
                    "value": "- Unfortunately, the experimental section of the paper does not live up to the ambitious ideas described in the intro. Only two (fairly simple) environments are considered (Crafter and one MiniGrid task), and the experiments feel contrived. For Crafter, \"Impossible\" tasks are generated by simply assigning the agent 0 reward. \"Boring\" tasks are generated by asking the agent to repeat the same task several times. Because of this, it is possible to solve the Crafter setup with very simple heuristics, and an LLM is not really needed. Granted, LLMs are more general, but it would be a lot more compelling if this method were evaluated on tasks which are not solvable with simple heuristics.\n- The paper's presentation needs improving. The paper is missing a lot of details about the algorithm. While Figure 1 gives a high-level overview of the system, it is still fairly vague and the rest of the description is all in the text and not very precise. It would be helpful to have the entire algorithm spelled out in pseudocode (in the main text), the exact process by which tasks are chosen described in equations or pseudocode, etc."
                },
                "questions": {
                    "value": "Here are my questions and suggestions:\n\n- My main suggestion for improving this paper is to test on one or more challenging environments. While artificial settings like the ones used in this paper are nice for proof of concepts or ease of interpretation, they need to be complemented by experiments on a more challenging domain where simple heuristics are insufficient. I would suggest trying this either on Minecraft, which has been used for open-ended research [1], or NetHack, which is another very complex open-ended game where the extrinsic reward is insufficient (it is also fast to run) [2].\n- My second suggestion is improving the presentation (see my comments above). It's also not clear what criteria the agent uses to decide it can do a task \"well\" - this this based on success rate or reward? Please clarify. In addition to this, I think the Conclusion/Discussion/Future Work section is too long at over 1 page. This saved space can be used for extra experiments in this paper. \n\n\n\n[1] Voyager: An Open-Ended Embodied Agent with Large Language Models (Wang et al)\n\n[2] The NetHack Learning Environment (Kuttler et al, NeurIPS 2020)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2863/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699137634235,
            "cdate": 1699137634235,
            "tmdate": 1699636229627,
            "mdate": 1699636229627,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vISrLwz717",
                "forum": "AgM3MzT99c",
                "replyto": "VjCGqGEO1W",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2863/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2863/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your thoughtful review. We greatly appreciate your acknowledgement of the novelty and potential of OMNI. We now address each of your concerns and questions.\n\n> Unfortunately, the experimental section of the paper does not live up to the ambitious ideas described in the intro. Only two (fairly simple) environments are considered (Crafter and one MiniGrid task), and the experiments feel contrived. For Crafter, \"Impossible\" tasks are generated by simply assigning the agent 0 reward. \"Boring\" tasks are generated by asking the agent to repeat the same task several times. Because of this, it is possible to solve the Crafter setup with very simple heuristics, and an LLM is not really needed. Granted, LLMs are more general, but it would be a lot more compelling if this method were evaluated on tasks which are not solvable with simple heuristics.\n> My main suggestion for improving this paper is to test on one or more challenging environments. While artificial settings like the ones used in this paper are nice for proof of concepts or ease of interpretation, they need to be complemented by experiments on a more challenging domain where simple heuristics are insufficient. I would suggest trying this either on Minecraft, which has been used for open-ended research [1], or NetHack, which is another very complex open-ended game where the extrinsic reward is insufficient (it is also fast to run) [2].\n\nIt is imperative to note that transitioning to such environments entails significant challenges. If we insist that the first paper of every new idea be showcased on a very complex domain, (1) we would really disadvantage academic labs, who do not have the compute resources often required for such complex domains, and (2) require many years of work before a student can publish their first paper.\n\nVoyager (Wang et al., 2023) was done by a team of full-time scientists from NVIDIA, and probably has orders of magnitudes more compute than any academic lab. We purposefully chose Crafter, the academic equivalent of Minecraft, because it has similar dynamics but is within budget for an academic lab. Furthermore, both Crafter and MiniGrid have been shown to be challenging RL domains (Hafner, 2021; Du et al., 2023; Chevalier-Boisvert et al., 2018; Mu et al, 2022).\n\nNonetheless, there is value in extending OMNI to a more complex, open-ended domain. Hence, our revision includes additional results on a very complex, open-ended, realistic embodied kitchen robotics domain, AI2-Thor (Kolve et al., 2017). Please see our general reviewer response for a summary.\n\n> The paper's presentation needs improving. The paper is missing a lot of details about the algorithm. While Figure 1 gives a high-level overview of the system, it is still fairly vague and the rest of the description is all in the text and not very precise. It would be helpful to have the entire algorithm spelled out in pseudocode (in the main text), the exact process by which tasks are chosen described in equations or pseudocode, etc.\n\nWe like your idea of including pseudocode for the entire algorithm, and have included it in Section 3 of the revised manuscript. Furthermore, as promised at the start of the Appendix, we will open-source the code to provide full transparency.\n\n> My second suggestion is improving the presentation (see my comments above). It's also not clear what criteria the agent uses to decide it can do a task \"well\" - this this based on success rate or reward? Please clarify. In addition to this, I think the Conclusion/Discussion/Future Work section is too long at over 1 page. This saved space can be used for extra experiments in this paper.\n\nAppendix F (Appendix D in original manuscript) explains how the agent decides which tasks are done well. On top of the details in the original manuscript, we hope that the additional pseudocode clarifies this. Reviewer LSLH indicated that the presentation was excellent and Reviewer CZLW commented that \u201cthe paper is clear and well-written\u201d. Given the additional updates, we hope you find the presentation better. If accepted to ICLR\u201924, we will commit to moving some of the experimental details back to the main text, and move some future work to the appendix. If there are any other missing details, please let us know and we will add them to the manuscript.\n\nYour score is substantially lower than the other reviews, and if not changed likely will lead to the paper not being published. Are our improvements enough for you to consider increasing your score? We deeply appreciate your consideration."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2863/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700262067004,
                "cdate": 1700262067004,
                "tmdate": 1700262067004,
                "mdate": 1700262067004,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]