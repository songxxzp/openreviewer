[
    {
        "title": "Towards Non-Asymptotic Convergence for Diffusion-Based Generative Models"
    },
    {
        "review": {
            "id": "La3vs3IDeZ",
            "forum": "4VGEeER6W9",
            "replyto": "4VGEeER6W9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4302/Reviewer_nMDV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4302/Reviewer_nMDV"
            ],
            "content": {
                "summary": {
                    "value": "This paper develops theoretical analyses for two variants of score-based generative models, those with deterministic (known as the probability flow ODE) and stochastic (known as denoising diffusion probabilistic models) reverse time processes, respectively. The deterministic variant is of particular interest because it has been difficult to analyze with existing techniques, despite being successful in practice. The main contribution of this work is to derive convergence guarantees for each of these processes under mild assumptions on the data distribution and the score function. Briefly, they provide an analysis of the deterministic algorithm which is the first to provide explicit rates of convergence, when the data distribution has bounded support (with only logarithmic dependence on the diameter) and when the score function and its Jacobian have been estimated. For the stochastic algorithm they recover similar results to the state-of-the-art, albeit with worse dimension dependence and stronger assumptions on the data distribution (compact support vs finite second moment)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This work studies an important problem, that of developing theoretical understanding of the efficacy of score-based methods, and focuses in particular on deterministic methods, which are not well understood theoretically. They develop the first analysis to achieve explicit rates of convergence for these methods in the literature, and achieve a strong bound under assumptions which are quite mild (except for the Jacobian assumption). For the stochastic method, they gain results which are close to the best known. Their analysis is necessarily novel and avoids technical difficulties that have forced previous works [1] to study (stochastic) modifications of the fully deterministic methods.\n\n[1] The Probability Flow ODE Is Provably Fast, by Chen et al 2023"
                },
                "weaknesses": {
                    "value": "From my point of view, the main weakness of the results is that they involve the error of the differential of the score function, and it is not clear why this would be well-controlled in general (since the score-matching objective doesn't involve the Jacobian). The prior work [2] didn't use such conditions, but then again achieved far weaker guarantees. It would be good to comment on the importance and plausibility of this condition. Less significant but nonetheless important is the authors' use of very specific step-size schemes, a remark on this would also be helpful. Finally, their convergence results are proved in TV (weaker than KL) and are not for the true data distribution, but for the distribution of the first step of the forward diffusion process.\n\nWith regard to correctness of their arguments, I was not able to carefully check this point due to the long and involved natured of their proofs but everything that I did read in the supplementary material seems correct.\n\n[2] \"Restoration-degradation beyond linear diffusions: A non-asymptotic analysis for DDIM-type samplers\" by Chen, Daras, and Dimakis 2023"
                },
                "questions": {
                    "value": "- Please add a remark about the fact that you prove your guarantees for convergence to the first step of the forward process (rather than the true data distribution). Of course, this is what allows for guarantees in TV with minimal assumptions on the data distribution since otherwise the data distribution could be singular, but I wonder how much this changes your results.\n- Are the step-size schemes you consider similar to those used in practice? How robust are your results to variations in the step-sizes?\n- Please discuss the utility and plausibility of the bound on the difference of Jacobians\n- Could you please clarify the meaning of \"continuous\" in the assumption on the data distribution when it is says \"$X_0$ is a continuous random vector, and\". In particular, are you assuming the data distribution has a density wrt Lebesgue here?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4302/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698619504182,
            "cdate": 1698619504182,
            "tmdate": 1699636398480,
            "mdate": 1699636398480,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NolSpmmL73",
                "forum": "4VGEeER6W9",
                "replyto": "La3vs3IDeZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4302/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4302/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks much for the valuable feedback!  Below is a point-by-point response to the reviewer's main comments.  \n\n**About the necessity of Jacobian error assumptions.**\n\nThanks for your comments regarding the assumption on the Jacobian errors. Note, however, that for deterministic samplers, having a small score estimation error alone is insufficient to guarantee convergence when measured by either the TV distance or the KL divergence; in other words, having additional assumptions on the Jacobian estimation error is unavoidable for the deterministic sampler in the presence of score errors. In comparison, [2] does not require this assumption mainly because they assume that the score estimation is noise-free. \n\nIn order to rigorize this crucial observation, we have come up with a hard instance as follows: \n\n**A hard example.** Consider the case where $X_{0}\\sim\\mathcal{N}(0,1)$, and hence $X_{1}\\sim\\mathcal{N}(0,1)$. Suppose that the reverse process for time $t=2$ can lead to the desired distribution if exact score function is employed, namely, \n$$Y_{1}^{\\star}\\coloneqq\\frac{1}{\\sqrt{\\alpha_{2}}}\\left(Y_{2}-\\frac{1-\\alpha_{2}}{2}s_{2}^{\\star}(Y_{2})\\right)\\sim\\mathcal{N}(0,1). $$\nNow, suppose that the score estimate $s_{2}(\\cdot)$ we have available obeys\n$$s_{2}(y_{2})=s_{2}^{\\star}(y_{2})+\\frac{2\\sqrt{\\alpha_{2}}}{1-\\alpha_{2}}\\left\\\\{ y_{1}^{\\star}-L\\left\\lfloor \\frac{y_{1}^{\\star}}{L}\\right\\rfloor \\right\\\\} \n\t\t\\qquad \\text{with } y_{1}^{\\star}\\coloneqq\\frac{1}{\\sqrt{\\alpha_{2}}}\\left(y_{2}-\\frac{1-\\alpha_{2}}{2}s_{2}^{\\star}(y_{2})\\right)$$\nfor some large integer $L>0$, where $\\lfloor z \\rfloor$ is the greatest  integer not exceeding $z$. It follows that\n$$Y_{1}=Y_{1}^{\\star}+\\frac{1-\\alpha_{2}}{2\\sqrt{\\alpha_{2}}}\\big[ s_{2}^{\\star}(Y_{2})-s_{2}(Y_{2})\\big]  =L\\left\\lfloor \\frac{Y_{1}^{\\star}}{L}\\right\\rfloor .$$\nClearly, the score estimation error $\\mathbb{E}\\_{X_2\\sim \\mathcal{N}(0,1)}\\big[|s_{2}(X_{2})-s_{2}^{\\star}(X_{2})|^2\\big]$\ncan be made arbitrarily small by taking $L$ to be sufficiently large. However, the discrete nature of $Y_{1}$ forces the TV distance to be\n$$\\mathsf{TV}(Y_{1},X_{1}) = 1. $$\nThis example demonstrates that, for the deterministic sampler, the TV distance between $Y_1$ and $X_1$ might not improve as the score error decreases, if only the score estimation error assumption is imposed. This is in stark contrast to the stochastic sampler. If we wish to eliminate the need of imposing Assumption 2 (about the Jacobian error), \none potential way is to resort to other metrics (e.g., the Wasserstein distance) instead of the TV distance between $Y_1$ and $X_1$.\n\nWe have added this hard example in Section 3 of the revised paper, in the hope of addressing the reviewer's concern.\n\n**About stepsize schemes.**\n\nThanks for your question regarding our choices of the learning rates. There are several remarks that we would like to make regarding the learning rate schedule.\n\n- It is worth noting that our analysis can be easily extended to accommodate a much broader class of learning rates, and the same convergence behavior can be achieved if the properties (43) are satisfied. If these properties are not satisfied,  our analysis framework is oftentimes still applicable, although the resulting convergence rates might be slower. \n\n- At a more technical level regarding our learning rate designs, we note that in general, the discretization error depends crucially upon the quantity $\\frac{1-\\alpha_t}{1-\\overline{\\alpha}_t}$, whereas the initialization error relies heavily upon $\\overline{\\alpha}_1$ and $\\overline{\\alpha}_T$. Based on these observations, our learning rate schedule (22) is designed to make $\\frac{1-\\alpha_t}{1-\\overline{\\alpha}_t}$ as small as possible, while  making sure $\\overline{\\alpha}_1$ (resp. $\\overline{\\alpha}_T$) is close to $1$ (resp. $0$). These properties are shown in (43). \n\nWe have added remarks to Section 3.1 of the revised version accordingly to address this comment. \n\n**About convergence to the first step of the forward process.**\n\nIndeed, our results are concerned with the first step of the forward process. Fortunately, given that $X_1 \\sim q_1$ and $X_0 \\sim q_0$ are quite close through taking $\\alpha_1 = 1 - \\frac{1}{\\mathsf{poly}(T)}$, focusing on the convergence w.r.t. $q_1$ instead of $q_0$ remains practically relevant while at the same time enabling a more effective theory. We have added a remark in the revised paper to explain this point. \n\n**About ''continuous'' random vectors.**\n\nThanks for raising this concern, and what we mean by \"continuous\" is exactly what the reviewer describes. We have changed it to \"absolutely continuous\" in the revised paper, meaning that there exists a probability density w.r.t. the Lebesgue measure."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4302/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700662627491,
                "cdate": 1700662627491,
                "tmdate": 1700662627491,
                "mdate": 1700662627491,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uFsejTl2VQ",
                "forum": "4VGEeER6W9",
                "replyto": "NolSpmmL73",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4302/Reviewer_nMDV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4302/Reviewer_nMDV"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your detailed response"
                    },
                    "comment": {
                        "value": "Thanks for your thoughtful and detailed response. I am satisfied with your additions and remarks other than those related to the Jacobian assumption.\n\nFor the Jacobian assumption, the \"hard example\" looks quite promising, but I am not clear why $\\mathbb{E}\\_{x_2 \\sim \\mathcal{N}(0, 1)}[ |s\\_2(x\\_2) - s\\_2^\\star(x\\_2)|^2]$ is small as $L \\to \\infty$? It seems that for each fixed $x\\_2$,\nas $L \\to \\infty$ the term $x\\_1^\\star - L\\lfloor \\frac{x\\_1^\\star}{L} \\rfloor \\to x\\_1^\\star \\neq 0$?"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4302/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700682613843,
                "cdate": 1700682613843,
                "tmdate": 1700682613843,
                "mdate": 1700682613843,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NdlScvNTfm",
            "forum": "4VGEeER6W9",
            "replyto": "4VGEeER6W9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4302/Reviewer_h2T8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4302/Reviewer_h2T8"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the deterministic probability flow ODE-based sampler often used in practice for score-based diffusion. It shows that the ODE-based sampler gets a $1/T$ convergence rate, improving upon the SDE-based sampling rate of $1/\\sqrt{T}$. The techniques are elementary, and do not rely on Girsanov's theorem and other techniques from the SDE/ODE toolboxes. While there was prior work (Chen et. al. 2023b) that obtains an improved guarantee for the ODE-based sampler, their analysis required the use of stochastic \"corrector steps\", while in practice, even just the ODE-based sampler without corrector steps seems to perform well. This is the first work that provides theoretical evidence that the vanilla ODE-based sampler can outperform the SDE-based sampler in practice."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Provides the first analysis of the (vanilla) ODE-based sampler for score-based diffusion models that gives some theoretical evidence for why it outperforms the SDE-based sampler in practice ($1/T$ convergence instead of $1/\\sqrt{T}$)\n- New analysis that doesn't make use of Girsanov's theorem/other results from the ODE/SDE literature.\n- Doesn't require Lipschitzness of score unlike (Chen et al 2023b), but pays in d dependence"
                },
                "weaknesses": {
                    "value": "- $d$ dependence is worse than (Chen et al 2023b). In particular, Chen gets a $\\sqrt{d}$ dependence for the ODE-based sampler when using stochastic corrector steps, which is better than the previous bound of $d$ for the SDE-based sampler. This paper on the other hand gets a $d^3$ dependence, which is significantly worse than both these bounds. \n- Requires Jacobian of score to be estimated accurately, rather than just the score\n- Requires the distribution $q_0$ to be bounded, and bound is stated in terms of this bound. In contrast, some of the prior works only required the second moment of $q_0$ to be bounded.\n- While new analysis is \"elementary\" in that it doesn't require SDE/ODE machinery, it seems much longer/more complicated than the previous analyses. Would really appreciate a condensation/proof overview in the main paper."
                },
                "questions": {
                    "value": "- Is it clear that $1/\\sqrt{T}$ is tight for the SDE-based sampler, and $1/T$ is impossible?\n- What are the barriers to getting a $d$ or $\\sqrt{d}$ dependence? Can you write something about this in the main paper?\n- Can you include a proof overview in the main paper?\n- Why is the proof so long? Can it be condensed?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4302/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4302/Reviewer_h2T8",
                        "ICLR.cc/2024/Conference/Submission4302/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4302/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698706091777,
            "cdate": 1698706091777,
            "tmdate": 1700698792987,
            "mdate": 1700698792987,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "T8b23fI7sc",
                "forum": "4VGEeER6W9",
                "replyto": "NdlScvNTfm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4302/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4302/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the valuable questions. Below is a point-by-point response to several remarks of the reviewer's. \n\n**About $d$ dependency.**\n\nWe agree that the dependency of $d$ in our results is likely loose (to be more precise, our result shows that the deterministic sampler yields $\\varepsilon$-accuracy as long as the iteration complexity exceeds $\\widetilde{O}\\big( \\frac{d^2}{\\varepsilon} + \\frac{d^3}{\\sqrt{\\varepsilon}}\\big)$, or $\\widetilde{O}\\big( \\frac{d^2}{\\varepsilon} \\big)$ for small enough $\\varepsilon$). \nWe would like to make two remarks. \n- On the one hand, the reviewer is absolutely correct that the abovementioned $d$-dependency is worse compared to Chen et al 2023b, the latter of which introduced a stochastic correction step to facilitate analysis.\n\n- On the other hand, the main purpose of our theory (for the deterministic sampler part) is to help establish non-asymptotic convergence guarantees for the probability flow ODE approach. Understanding such pure deterministic samplers is of fundamental importance, as this approach has been shown to be highly effective in practice\tand it also sheds light on the design of faster methods (e.g., the DPM solver, and the consistency model). However, existing theory for pure deterministic samplers was highly inadequate; for instance, the only non-asymptotic result before us was Chen et al 2023c, which did not provide concrete polynomial dependency and involved terms that scale exponentially in the Lipschitz constant of the score function. Our main contributions are therefore to establish improved convergence theory for the probability flow ODE approach. In comparison, while adding the stochastic correction step enables improved $d$-dependency (from the theoretical perspective), the resulting sampler studied in  Chen et al 2023b deviates from the deterministic sampler used in practice and hence falls short of explaining the effectiveness of this popular approach.\n\n**About Jacobian errors.**\n\nThanks for pointing out the additional requirement on the Jacobian estimation accuracy. Note, however, that for deterministic samplers, having a small score estimation error alone is insufficient to guarantee convergence when measured by either the TV distance or the KL divergence; in other words, having additional assumptions on the Jacobian estimation error is unavoidable for the deterministic case. In order to rigorize this crucial observation, we have come up with a hard instance as follows: \n\n**A hard example.** Consider the case where $X_{0}\\sim\\mathcal{N}(0,1)$, and hence $X_{1}\\sim\\mathcal{N}(0,1)$. Suppose that the reverse process for time $t=2$ can lead to the desired distribution if exact score function is employed, namely, \n$$Y_{1}^{\\star}\\coloneqq\\frac{1}{\\sqrt{\\alpha_{2}}}\\left(Y_{2}-\\frac{1-\\alpha_{2}}{2}s_{2}^{\\star}(Y_{2})\\right)\\sim\\mathcal{N}(0,1).$$\nNow, suppose that the score estimate $s_{2}(\\cdot)$ we have available obeys\n$$s_{2}(y_{2})=s_{2}^{\\star}(y_{2})+\\frac{2\\sqrt{\\alpha_{2}}}{1-\\alpha_{2}}\\left\\\\{ y_{1}^{\\star}-L\\left\\lfloor \\frac{y_{1}^{\\star}}{L}\\right\\rfloor \\right\\\\} \n\t\t\\qquad \\text{with } y_{1}^{\\star}\\coloneqq\\frac{1}{\\sqrt{\\alpha_{2}}}\\left(y_{2}-\\frac{1-\\alpha_{2}}{2}s_{2}^{\\star}(y_{2})\\right)$$\nfor $L>0$, where $\\lfloor z \\rfloor$ is the greatest  integer not exceeding $z$. It follows that\n$$Y_{1}=Y_{1}^{\\star}+\\frac{1-\\alpha_{2}}{2\\sqrt{\\alpha_{2}}}\\big[ s_{2}^{\\star}(Y_{2})-s_{2}(Y_{2})\\big]  =L\\left\\lfloor \\frac{Y_{1}^{\\star}}{L}\\right\\rfloor .$$\nClearly, the score estimation error $\\mathbb{E}\\_{X_2\\sim \\mathcal{N}(0,1)}\\big[|s_{2}(X_{2})-s_{2}^{\\star}(X_{2})|^2\\big]$ can be made arbitrarily small by taking $L \\to 0$. However, the discrete nature of $Y_{1}$ forces the TV distance to be\n$\\mathsf{TV}(Y_{1},X_{1}) = 1. $\nThis example demonstrates that, for the deterministic sampler, the TV distance between $Y_1$ and $X_1$ might not improve as the score error decreases, if only the score estimation error assumption is imposed. This is in stark contrast to the stochastic sampler. If we wish to eliminate the need of imposing Assumption 2 (about the Jacobian error), \none potential way is to resort to other metrics (e.g., the Wasserstein distance) instead of the TV distance between $Y_1$ and $X_1$.\n\nWe have added this hard example in Section 3 of the revised paper, in the hope of addressing the reviewer's concern."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4302/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700662388647,
                "cdate": 1700662388647,
                "tmdate": 1700697051605,
                "mdate": 1700697051605,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xK25Fszm8L",
                "forum": "4VGEeER6W9",
                "replyto": "NdlScvNTfm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4302/Reviewer_h2T8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4302/Reviewer_h2T8"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. I appreciate the inclusion of the hard example and the proof overview.\n\nWhat kind of guarantee would you be able to achieve for sampling with Wasserstein closeness, if you don't have the Jacobian guarantee? Would really appreciate it if you could include this result as well.\n\nI will raise my score to 8 regardless, but I would really appreciate it if this result appeared in the final version."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4302/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700698524724,
                "cdate": 1700698524724,
                "tmdate": 1700698777207,
                "mdate": 1700698777207,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9MWDNny1bC",
            "forum": "4VGEeER6W9",
            "replyto": "4VGEeER6W9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4302/Reviewer_ScYQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4302/Reviewer_ScYQ"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to provide a systematic analysis of the convergence rate of both deterministic and stochastic samplers of the diffusion models in the context of generative modeling. The authors proves, under certain assumptions, the former has a rate of $T^{-1}$\nwhile the latter has a rate $\\sqrt{T}$ which is consistent with the previous empirical observations. The authors also proposed some improvements, leading to a rate of $T^{-2}$ in the deterministic case, and a rate of $T^{-1}$ in the stochastic case."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "This paper provides an early systematic analysis on the convergence rate of samplers in the diffusion models. Both deterministic and stochastic cases are considered, and their results are almost optimal under the assumptions made. The presentation of the paper is good, and I really enjoyed reading it. Especially, I like Theorem 1."
                },
                "weaknesses": {
                    "value": "As for every (good) paper, there are always plenty of things remained to be done. For instance,\n\n(1) It is worthy to comment on the learning rate (22a)-(22b). I understand these rates are carefully chosen in order to match the rates in the theorems. The author may mention this, or provide some explanations/insights on it.\n\n(2) The paper, like Chen et al., deals with the TV (or KL) divergence. I understand that under these metrics, one can prove \"nice\" theoretical results using some specific algebraic identities (flow...) On the other hand, practitioners may care more about the FID (or Wasserstein distance) -- part of the reason is that Wasserstein distance is \"closer\" to how humans distinguish pictures. The authors may want to add a few remarks on this."
                },
                "questions": {
                    "value": "See the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4302/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698711551654,
            "cdate": 1698711551654,
            "tmdate": 1699636398327,
            "mdate": 1699636398327,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rzBq8vsZxH",
                "forum": "4VGEeER6W9",
                "replyto": "9MWDNny1bC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4302/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4302/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the positive appraisal of our paper and for helpful feedback! Please find below our point-by-point response to some of the main comments.  \n\n**About the learning rates.**\n\nThanks for suggesting more explanations about our choices of the learning rates. There are several remarks that we would like to make regarding the learning rate schedule. \n- As can be easily verified, in general the discretization error depends crucially upon the quantity $\\frac{1-\\alpha_t}{1-\\overline{\\alpha}_t}$, whereas the initialization error relies heavily upon $\\overline{\\alpha}_1$ and $\\overline{\\alpha}_T$. \nBased on these observations, our learning rate schedule (22) is designed to make $\\frac{1-\\alpha_t}{1-\\overline{\\alpha}_t}$ as small as possible, while  making sure $\\overline{\\alpha}_1$ (resp. $\\overline{\\alpha}_T$) is close to $1$ (resp. $0$). These properties are shown in (43). \n\n- On the other hand, it is worth noting that our analysis can be easily extended to accommodate a much broader class of learning rates, and the same convergence behavior can be achieved if the properties (43) are satisfied. If these properties are not satisfied,  our analysis framework is oftentimes still applicable, although the resulting convergence rates might be slower.\n\nWe have added remarks to Section 3.1 of the revised version accordingly to address this comment.\n\n**About TV/KL vs. the Wasserstein distance.**\n\nThanks a lot for your suggestion, and we fully agree with your comments regarding the practical importance of the Wasserstein distance as a performance metric. There are several remarks that we would like to make in this regard. \n- In fact, there have been several recent papers establishing appealing convergence guarantees based on the Wasserstein distance (e.g., Tang 2023, and Benton et al., 2023). One weakness of these Wasserstein-distance-based results, \nhowever, is that the iteration complexities of these results exhibit exponential dependency on the Lipstchiz constant of the score function. This is in stark contrast to our result and Chen et al., as these TV/KL-based convergence rates are nearly independent of the Lipschitz constant of the score function.  \n\n- For the deterministic sampler, our convergence results rely upon Assumption 2, which is concerned with the accuracy of Jacobian matrices. In Section 3 of the revised paper, we have included a hard example illustrating the necessity of such an assumption. However, we conjecture that Assumption 2 might be non-essential if the performance metric is chosen to be the Wasserstein distance (given that the Wasserstein distance allows one to transport probability mass in order to mitigate the mismatch between continuous and discrete random variables). In light of this conjecture, it would be valuable to develop a non-asymptotic convergence result for the deterministic sampler based on the Wasserstein distance.  \n\nWe have added some remarks regarding the Wasserstein-distance-type results in the revised paper.\n\n**References:**\n- Wenpin Tang. Diffusion Probabilistic Models. (2023). \n- Joe Benton, George Deligiannidis, and Arnaud Doucet. Error bounds for flow matching methods. arXiv preprint arXiv:2305.16860, 2023."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4302/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700662058653,
                "cdate": 1700662058653,
                "tmdate": 1700662058653,
                "mdate": 1700662058653,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "31R2kSZkbC",
            "forum": "4VGEeER6W9",
            "replyto": "4VGEeER6W9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4302/Reviewer_TgkF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4302/Reviewer_TgkF"
            ],
            "content": {
                "summary": {
                    "value": "The paper delves into the intricacies of diffusion models, a unique class of models capable of converting noise into fresh data instances through the reversal of a Markov diffusion process. While the practical applications and capabilities of these models are well-acknowledged, there remains a gap in the comprehensive theoretical understanding of their workings. Addressing this, the authors introduce a novel non-asymptotic theory, specifically tailored to grasp the data generation mechanisms of diffusion models in a discrete-time setting. A significant contribution of their research is the establishment of convergence rates for both deterministic and stochastic samplers, given a score estimation oracle. The study underscores the importance of score estimation accuracies. Notably, this research stands apart from prior works by adopting a non-asymptotic approach, eschewing the traditional reliance on toolboxes designed for SDEs and ODEs."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper offers a significant theoretical advancement by providing the convergence bounds for both stochastic and deterministic samplers of diffusion processes. I'm particularly struck by the elegance of their results, especially given the minimal assumptions required\u2014for instance, the results for stochastic samplers rely solely on Assumption 1. These theoretical findings provide a clear understanding of the effects of score estimation inaccuracies. Furthermore, the emphasis on the estimation accuracies of Jacobian matrices for deterministic samplers sheds light on potential training strategies, suggesting the incorporation of penalties in objective functions when learning the score function for these samplers. Additionally, the proof presented is elementary, and potentially useful in a broader context."
                },
                "weaknesses": {
                    "value": "- While this paper doesn't present any algorithmic advancements, it's understandable considering the depth of their theoretical contributions.\n\n- The proof appears to be procedural. I was hoping for deeper insights into how the proof was constructed. It might be beneficial for the authors to include a subsection detailing the outline and insights behind their proof."
                },
                "questions": {
                    "value": "- Eq. (14), just want to confirm: should it be $dX_t = \\sqrt{1-\\beta_t}X_td_t$ instead of $-\\frac{1}{2}\\beta(t)X_t d_t$? \n- Eq. (15), maybe I am missing something, but should it be $dY = (-f(...) + \\frac{1}{2}g^2 \\nabla) d_t$? The original paper was taking a reverse form. \n- Eq. (16), similar question to Eq. (15). \n- Eq. (21), I feel a bit weird about the notation: maybe remove the $R$. \n- The results for deterministic samplers may motivate a better training strategy (i.e., incorporating the requirement for Jacobian matrices accuracy). The first step is to verify that this phenomenon exists in experiments (i.e., a worse $\\epsilon_{Jacobi}$ does imply a worse sampling result). It may be worth adding a numerical experiment to confirm this if time permits."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4302/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4302/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4302/Reviewer_TgkF"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4302/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698812168849,
            "cdate": 1698812168849,
            "tmdate": 1699636398247,
            "mdate": 1699636398247,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lAOI9TkoXk",
                "forum": "4VGEeER6W9",
                "replyto": "31R2kSZkbC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4302/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4302/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks a lot for the helpful comments and valuable feedback. \nPlease find below our point-by-point response to the main comments, which has been incorporated into the revision version.\n\n**About proof outline.**\nWe agree that including a subsection detailing the proof outline would be quite beneficial. \nCurrently, mainly due to the space limitations, the main analysis steps of Theorems 1-2 are deferred to the supplementary material. \nWe have now added a new section in the supplementary material (see Appendix A of the revised paper) to provide a proof outline.   \n\n**About Eq. (14).**\nThanks for pointing out the confusion. \nNote that the continuous-time limit of (4) can be understood in the sense that \n$$X_{t}-X_{t-\\mathrm{d}t}=\\sqrt{1-\\beta_{t}}X_{t-\\mathrm{d}t}-X_{t-\\mathrm{d}t}+\\sqrt{\\beta_{t}}W_{t}\\approx-\\frac{1}{2}\\beta_{t}X_{t-\\mathrm{d}t}+\\sqrt{\\beta_{t}}W_{t},$$\n\nprovided that $\\beta_t$ is small; this is why the continuous-time limit reads\n$$\\mathrm{d}X_{t}=-\\frac{1}{2}\\beta(t)X_{t}+\\sqrt{\\beta_{t}}\\mathrm{d}W_{t}.$$\nWe have added a footnote in the revised paper to make this more clear. \n\n**About Eqs. (15) and (16).**\nThanks for pointing the typos!  Indeed, these should be $-f$ rather than $f$. We have fixed them in the revised version. \n\n**About Eq. (21).** Thanks for pointing out this notational issue. We have removed $R$ in the revised version as suggested by the reviewer.  \n\n**About potential training strategies with Jacobian requirements.**\nThanks a lot for the suggestion! Indeed, the additional requirement about the Jacobians in our theory motivates us to explore potential strategies to enforce such requirements in the training phase. \n\nAfter careful examination of this requirement, we realize that \n- The additional Jacobian requirement is largely due to the use of the TV distance (or KL divergence) as a performance metric; in other words, if only the score estimation error is assumed, then one can come up with a hard example such that the TV distance remains large even when the number of steps $T$ is large. We shall also include the hard example below, which is largely due to the mismatch between continuous random variables and discrete random variables.  \n\n- However, we conjecture that this fundamental requirement on Jacobian accuracy might be non-essential if the performance metric is chosen to be the Wasserstein distance (given that the Wasserstein distance allows one to transport probability mass in order to mitigate the mismatch between continuous and discrete random variables). If our conjecture were true, then it would be unclear whether enforcing the additional Jacobian estimation accuracy would be beneficial. \n\n We will conduct some experiments to test the utility of enforcing such Jacobian-based requirements in the training phase."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4302/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700661913282,
                "cdate": 1700661913282,
                "tmdate": 1700661913282,
                "mdate": 1700661913282,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "61w0DEzocO",
                "forum": "4VGEeER6W9",
                "replyto": "31R2kSZkbC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4302/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4302/Authors"
                ],
                "content": {
                    "title": {
                        "value": "The hard example"
                    },
                    "comment": {
                        "value": "The hard example mentioned above is described below, which has also been incorporated into Section 3 of the revised paper. \n\nConsider the case where $X_{0}\\sim\\mathcal{N}(0,1)$, and hence $X_{1}\\sim\\mathcal{N}(0,1)$. Suppose that the reverse process for time $t=2$ can lead to the desired distribution if exact score function is employed, namely, \n$$Y_{1}^{\\star}\\coloneqq\\frac{1}{\\sqrt{\\alpha_{2}}}\\left(Y_{2}-\\frac{1-\\alpha_{2}}{2}s_{2}^{\\star}(Y_{2})\\right)\\sim\\mathcal{N}(0,1).$$\nNow, suppose that the score estimate $s_{2}(\\cdot)$ we have available obeys\n$$s_{2}(y_{2})=s_{2}^{\\star}(y_{2})+\\frac{2\\sqrt{\\alpha_{2}}}{1-\\alpha_{2}}\\left\\\\{ y_{1}^{\\star}-L\\left\\lfloor \\frac{y_{1}^{\\star}}{L}\\right\\rfloor \\right\\\\} \n\t\t\\qquad \\text{with } y_{1}^{\\star}\\coloneqq\\frac{1}{\\sqrt{\\alpha_{2}}}\\left(y_{2}-\\frac{1-\\alpha_{2}}{2}s_{2}^{\\star}(y_{2})\\right)$$\nfor $L>0$, where $\\lfloor z \\rfloor$ is the greatest  integer not exceeding $z$. It follows that\n$$Y_{1}=Y_{1}^{\\star}+\\frac{1-\\alpha_{2}}{2\\sqrt{\\alpha_{2}}}\\big[ s_{2}^{\\star}(Y_{2})-s_{2}(Y_{2})\\big]  =L\\left\\lfloor \\frac{Y_{1}^{\\star}}{L}\\right\\rfloor .$$\nClearly, the score estimation error $\\mathbb{E}\\_{X_2 \\sim \\mathcal{N}(0,1)}\\big[|s_{2}(X_{2})-s_{2}^{\\star}(X_{2})|^2\\big]$\ncan be made arbitrarily small by taking $L \\to 0$. \nHowever, the discrete nature of $Y_{1}$ forces the TV distance to be\n$$\\mathsf{TV}(Y_{1},X_{1}) = 1. $$\nThis example demonstrates that, for the deterministic sampler, the TV distance between $Y_1$ and $X_1$ might not improve as the score error decreases, if only the score estimation error assumption is imposed. \nThis is in stark contrast to the stochastic sampler. \nIf we wish to eliminate the need of imposing Assumption 2 (about the Jacobian error), \none potential way is to resort to other metrics (e.g., the Wasserstein distance) instead of the TV distance between $Y_1$ and $X_1$."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4302/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700661969422,
                "cdate": 1700661969422,
                "tmdate": 1700697084228,
                "mdate": 1700697084228,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VhFBimHc8T",
                "forum": "4VGEeER6W9",
                "replyto": "61w0DEzocO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4302/Reviewer_TgkF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4302/Reviewer_TgkF"
                ],
                "content": {
                    "comment": {
                        "value": "This example is interesting as it underscores the importance of the Jacobian matrix requirement. I appreciate the thorough responses to my comments. Thank you!"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4302/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700717001206,
                "cdate": 1700717001206,
                "tmdate": 1700717001206,
                "mdate": 1700717001206,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]