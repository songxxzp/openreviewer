[
    {
        "title": "Deep Regression Representation Learning with Topology"
    },
    {
        "review": {
            "id": "oVQ4yrcWkP",
            "forum": "OeNcnlQPRz",
            "replyto": "OeNcnlQPRz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1763/Reviewer_iGsx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1763/Reviewer_iGsx"
            ],
            "content": {
                "summary": {
                    "value": "The paper uses the task of regression to study connections between the Information Bottleneck principle and the topology of feature space. Based on these connections, the authors introduce a regularisation scheme"
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper introduces an interesting approach to study representation learning from the point of view of the information bottleneck principle, but there are some problems in the presentation that prevent me from giving a fair evaluation of the analysis (see below).\n\nAdditionally, not being an expert in topology, I am not able to assess the soundness and value of the method introduced in section 5."
                },
                "weaknesses": {
                    "value": "There are some points of the theoretical analysis that I do not understand and prevent me from giving a proper evaluation. The analysis hinges on theorem 1, but there are two aspects of it that I do not understand:\n\n1. Minimising the difference is not generally equivalent to minimising the ratio, implying that the equivalence is a result of I being the mutual information. Can the author provide details on this equivalence? Is there a difference between I and \\mathcal{I} in the proof of theorem 10?\n\n2. Both Z and Y are deterministic functions of X. What does it imply for the entropy of Z conditioned on Y? Is it only meaningful when the mapping from X to Y is one-to-many? Does your theory suffer from the problems associated with having deterministic functions, as discussed in 'ON THE INFORMATION BOTTLENECK THEORY OF DEEP LEARNING' by Saxe et al, ICLR 2018?\n\nAdditionally, \n\n-The accessibility of Section 4 is limited to readers with advanced knowledge in topology.\n\n-The experimental evaluation is limited to a very simple architecture (two-layer network with 100 hidden units)"
                },
                "questions": {
                    "value": "See weaknesses above. In addition:\n\n1. What is the meaning of `regression representation learning'? Do you mean representation learning in a regression task?\n\n2. Are Swiss Roll and Mammoth standard jargon in topology?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1763/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1763/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1763/Reviewer_iGsx"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1763/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698747199386,
            "cdate": 1698747199386,
            "tmdate": 1699636105689,
            "mdate": 1699636105689,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9r5ZoM9oCo",
                "forum": "OeNcnlQPRz",
                "replyto": "oVQ4yrcWkP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1763/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1763/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer iGsx,\n\nThank you for your insightful and detailed comments.   \n\nFor the weaknesses:\n\n**W1. Minimizing the difference is not equivalent to minimizing the ratio:** Thanks for pointing this out. I and \\mathcal{I} both represent mutual information; we will revise the typos and theorem 1.\n\n**W2. Deterministic functions of $X$:**  There is a misunderstanding between the predicted $Y\u2019$ by neural networks and the target $Y$. The predicted $Y\u2019$ is a deterministic function of $X$, while $Y$ is not a deterministic function of $X$. Our theorem holds for many-to-one mapping from $X$ to $Y$. We will extend it to many-to-many as suggested by reviewer awEv. Saxe et al. discussed some problems associated with deterministic mapping when estimating mutual information. To estimate the mutual information, they suggest analyzing a new variable $Z\u2019$ with additive noise, let $Z\u2019 = Z + N$, where $N$ is the noise. The deterministic mapping assumption in theorem 1 is to cancel the term $\\mathcal H(Z|X)$. If N follows some fixed distribution, then $\\mathcal H(Z\u2019|X) = \\mathcal H(N)$ is a constant and thus also can be canceled.\n\n\n**Section 4 is limited to readers with advanced knowledge in topology:** Thanks for pointing this out. We will improve the readability. \n\n**Limited to a very simple architecture**: Experiments on real-world tasks using some complex architectures, like the EDSR model on DIV2K.\n\n**Meaning of `regression representation learning\u2019'**: Yes, it means representation learning in regression tasks.\n\n**Are Swiss Roll and Mammoth standard jargon in topology?**: Swiss Roll is widely exploited and we follow RTD [arxiv 2302.00136] to exploit Mammoth."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1763/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700657405660,
                "cdate": 1700657405660,
                "tmdate": 1700657405660,
                "mdate": 1700657405660,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RgJR9P2HFY",
            "forum": "OeNcnlQPRz",
            "replyto": "OeNcnlQPRz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1763/Reviewer_awEv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1763/Reviewer_awEv"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces Persistent Homology Regression Regularizer (PH-Reg) as a novel regularization strategy inspired by the Information Bottleneck (IB) principle with the goal of regularizing intermediate representation to preserve the topological structure of the (regression) targets while reducing the intrinsic dimensionality. A theoretical section relates the intrinsic dimensionality of the representation to the IB principle and generalization error, justifying the need for PH-Reg. An experimental section validates the effectiveness of the proposed method by analyzing the effect of the regularization components that encourage lower intrinsic dimension $\\mathcal{L}_d$ and preserve the topology of the target space $\\mathcal{L}_t$ on both synthetic and real-world tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1) The paper draws interesting and novel connections between the Information Bottleneck principle and the topology and intrinsic dimensions of the representations with respect to the targets\n\n\n2) The experimental section covers a significant number of diverse datasets, integrating quantitative results with qualitative visualizations\n\n\n3) The authors include an estimation of the additional computational and memory cost of Ph-Reg demonstrating that the overhead of the proposed method is small compared to the cost of training large architectures."
                },
                "weaknesses": {
                    "value": "## Main Concerns\n\n1) **Clarity** \n   1) The underlying assumptions used to prove the statements in Section 3 are not fully clarified. Theorem 1 implicitly restricts the considered representations to deterministic functions of $\\bf x$. Previous literature [1] has shown the benefit of using stochastic encoders, for which the result from Theorem 1 is not applicable. \n   2) Theorem 3 assumes a uniform (conditional) distribution on the manifold $\\mathcal{M}_i$ but the paper does not elaborate on under which conditions this assumption is reasonable.\n   3) Section 4 is extremely difficult to follow because of the large number of definitions. Additional intuition on the roles of the terms in $\\mathcal{L}'_d$, $\\mathcal{L}_d$, $\\mathcal{L}_t$ would greatly improve the readability.  \n\n2) **Soundess**\n   1) The definition of the optimal representation and the statement in proposition 2 seem not to be applicable to continuous $\\bf z$ and $\\bf y$ without further clarifications. The value of the (differential) entropies $\\mathcal{H}({\\bf Y}|{\\bf Z})$ and $\\mathcal{H}({\\bf Z}|{\\bf Y})$ approach $-\\infty$ whenever there exists an invertible mapping ${\\bf y}=f({\\bf z})$. As a result, I believe the proof for proposition 2 needs to be revised.\n   2) The existence of a homeomorphic mapping between $\\bf z$ and $\\bf y$ and definition 1 rely on the existence of an underlying mapping ${\\bf y} = f({\\bf x})$. In other words, this assumption seems to restrict the setting to targets $\\bf y$ that can be fully determined from $\\bf x$. This aspect is not directly discussed in the main text and the real-world experiments consider distributions in which $p({\\bf y}|{\\bf x})$ has non-negligible aleatoric uncertainty (super-resolution, depth-estimation, age estimation).\n  \n\n3) **Experiments**\n   1) No measure of standard deviation is reported, which makes it difficult to assess the significance of the reported results.\n   2) The authors mention that \"several widely used regularizers like weight decay and dropout effectively reduce the last hidden layer's intrinsic dimension\", but these simple baselines are not included in any comparison.\n\n\n\n## Minor Issues\n4)  **Presentation**\n    1) The distribution $\\mathcal{D}$ defined in theorem 2 seems to depend on the specific sample ${\\bf y}_i$ but the index $i$ is dropped in the notation.\n    2) Figure 2b is helpful in supporting section 4, but I was unable to parse the plot on the right side since there is no direct reference to it in the description.\n    3) The differences between plots (b) to (e) in Figure 3 are difficult to relate to the description in the main text since the plots appear quite similar to each other.\n    4) Section 5.2 introduces a large number of abbreviations without previous mentions.\n\n### References\n[1] Alemi, Alexander A., et al. \"Deep variational information bottleneck.\" arXiv preprint arXiv:1612.00410 (2016)."
                },
                "questions": {
                    "value": "1) Can the proposed method be extended to stochastic representations? What is the benefit of considering only deterministic encoders?\n\n2) Under which conditions is the assumption used in Theorem 3 (uniformity) justified?\n\n3) Does the proposed theory address solely tasks with negligible aleatoric uncertainty? In tasks such as super-resolution, it seems that the predictive distribution $p({\\bf y}|{\\bf x})$ could map a low resolution ${\\bf x}$ into multiple possible \"correct\" outputs ${\\bf y}$. Does that mean that an \"optimal representation\" of $\\bf x$ does not exist for this task? How could a homeomorphic mapping between $\\bf y$ and $\\bf z$ exist in these settings?\n\n4) How are the values for $\\lambda_t$ and $\\lambda_d$ determined?\n\n5) How consistent are the results reported in Tables 1, 2, 3, and 4 across multiple runs?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1763/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698763356907,
            "cdate": 1698763356907,
            "tmdate": 1699636105586,
            "mdate": 1699636105586,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fB9QxNMlkg",
                "forum": "OeNcnlQPRz",
                "replyto": "RgJR9P2HFY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1763/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1763/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer awEv,\n\nThank you for your insightful and detailed comments. \n\nFor the concerns:\n\n**Uncertainty, proposition 2 and definition 1 (Soundness & Question 3):** Thanks for these very valuable points for Proposition 2 and Definition 1. You are correct. The defined optimal representation and the corresponding homeomorphism are currently limited to the setting with negligible aleatoric uncertainty. When considering aleatoric uncertainty, i.e., $\\mathcal H(Y|X)>0$, the \u2018optimal representation\u2019 exists but can not be obtained under our current definitions, as $\\mathcal H(Y|Z)$ can never equal 0. By the way, Proposition 2 and Definition 1 hold under epistemic uncertainty.\n\nProposition 2 and Definition 1 show the homeomorphism between the target and the feature spaces in the limited setting, which currently serves as a hint to explain why we should encourage the topological similarity between the target and the feature spaces. We will extend Proposition 2 and Definition 1, and give some discussion accordingly.\n\n**Extended to stochastic representations (Clarity 1 & Question 1):** Appreciate for pointing this out. Our method can be extended to stochastic representations if $p(z|x)$ follows some fixed distributions like the standard Gaussian distribution which is commonly exploited in VAE. The deterministic mapping assumption in theorem 1 is to cancel the term $\\mathcal H(Z|X)$. If $p(z|x)$ follows some fixed distributions, then $\\mathcal H(Z|X)$ is a constant and thus also can be canceled. \n\n**Uniform distribution assumption in Theorem 3 (Clarity 2 & Question 2):** We follow [Ghosh & Motani (2023)] to make this assumption. Thanks for this question. We realized this assumption is unnecessary: the entropy of uniform distribution has the largest entropy over all distributions over the support $\\mathcal M_i$, thus $\\mathcal H(Z|Y)$ is bounded by the right part of Eq 4, which also suggests reducing the intrinsic dimension for a lower $\\mathcal  H(Z|Y)$.\n\n**Report standard deviation & include baselines like weight decay& difficult to follow Section 4 (Experiments 1, 2 & Clarity 3):** Thanks for pointing these out. We will revise our paper accordingly.\n\n**How are the values for $\\lambda_t$ and $\\lambda_d$ determined (Question 4):** Their values are mainly determined by the the value of the task loss. For a high value task loss,  $\\lambda_t$ and $\\lambda_d$ should also be set to high values.\n\n**How consistent are the results reported in Tables 1, 2, 3, and 4 across multiple runs? (Question 5)**: The standard deviation is reported in Table 1; we will consider reporting standard deviations for experiments on real-world tasks, i.e., Table 2,3,4. In particular, age estimation results are inconsistent, which should be due to the large variance of the \u2018few\u2019 subset."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1763/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700657087512,
                "cdate": 1700657087512,
                "tmdate": 1700657087512,
                "mdate": 1700657087512,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iZ6wMoS5uB",
            "forum": "OeNcnlQPRz",
            "replyto": "OeNcnlQPRz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1763/Reviewer_pU4H"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1763/Reviewer_pU4H"
            ],
            "content": {
                "summary": {
                    "value": "This work considers deep learning models trained for regression from the perspective of information bottleneck and the topology of the model\u2019s latent space. The work notes that the concept of the information bottleneck (IB) imposes constraints on the relationship between the latent space of a model and the target space. Through a number of propositions and theorems, the work claims to show that (i) the latent space of a regression model should have the same intrinsic dimension as the target space and (ii) the latent space of a regression model should have the same topology as the target space. To nudge models in this direction, the work develops regularizers that encourage these properties in the latent space. Finally, the work describes both synthetic and real-data experiments that suggest that these regularizers are both helpful for building robust regressors."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- **Advancing the science of deep learning for regression:** The fact that so much research into the science of deep learning has focused exclusively on classification at the expense of other tasks (such as regression), is a weakness of the field. This work which investigates IB, intrinsic dimension, and the topology of latent space specifically for models trained to perform regression thus represents a welcome research direction. \n- **Utilizing interesting mathematics:** The paper brings together interesting mathematics (topology) with ideas about the information bottleneck. Though some of the formalism of that connection needs to be ironed out, the underlying idea is interesting and seems worth pursuing."
                },
                "weaknesses": {
                    "value": "**Writing correctness and clarity:** There were a number of issues with the writing that made it more challenging to read the work than it should have been. For instance, typos such as:\n1. In the introduction, \u201cThe homeomorphic between two\u2026\u201d $\\mapsto$ \u201cThe homeomorphism between two\u2026\u201d.\n2. In the introduction, \u201c\u2026and in the topology view,\u2026\u201d $\\mapsto$ \u201c\u2026and from the topological viewpoint,\u2026\u201d\n3. In Section 5.1, \u201cIn contrast, naively lowering the intrinsic dimension ($+L\u2019_{d}$ ) performs poorly and even worse than the baseline, i.e., Tours\u201d $\\mapsto$ \"...i.e., torus.\u201d\n\nThere were also a number of mathematical statements that were made that either didn\u2019t make sense or were not true. For example:\n1. The sentence \u201cThe continuity represents the $0$th Betti number in topology\u2026\u201d is incorrect. The $0$th Betti number captures the number of connected components in the topological space but has little to do with continuity (otherwise). For instance, a set of three unique (and discrete) points in $\\mathbb{R}^3$ has $0$th Betti number $3$ and three disjoint spheres in $\\mathbb{R}^3$ also has $0$th Betti number $3$.\n2. Pretty much all topological data analysis is based on algebraic topology, so the word \u2018algebraic\u2019 in \u2018algebraic topological data analysis\u2019 is unnecessary.\n3. \u201cHowever, unlike classification, regression\u2019s target space is naturally a topology space, rich in topology information crucial for the detailed task.\u201d It isn\u2019t clear why the target space of a classification task is less naturally a topological space. Perhaps what the paper means is that there is less topological structure in the discrete target space of classification tasks. Then again, if one is only using the $0$th homology groups (as this paper does), one is not capturing any higher dimensional structure anyway.\n4. Proposition 2 concerns continuous maps between $\\mathbf{Z}$ and $\\mathbf{Y}$. What are $\\mathbf{Z}$ and $\\mathbf{Y}$? Continuous maps only make sense when one has a defined topology on the domain and target space, this doesn\u2019t seem to currently exist in the work. \n\nThere were also a few cases where notation was used that was never defined.\n1. What is $\\mathcal{Y}$ in Theorem 2?\n2. If $\\mathbf{X}$, $\\mathbf{Z}$, and $\\mathbf{Y}$ are going to be used in proofs, it should be stated what they are.\n\nFinally, the reviewer had a hard time understanding some of the figures\n1. Figure 1(b) is used to illustrate a point about intrinsic dimension in the introduction. The reviewer had trouble understanding what the task is here and what the reader is meant to see in this scatter plot. More context should probably be added.\n\n**Use of the word \u2018topological\u2019 when only $H_0$ is used:** In the cases considered in this paper, the only topological property that $H_0$ measures is the number of connected components of a space (it has been shown that persistent homology also captures geometric information like curvature, so such features may also be leaking into regularization). As such, it captures only a small fraction of the total topological characterization of the space (particularly for high-dimensional spaces). It is this reviewer\u2019s opinion that this fact should be more explicitly stated than what is currently found on the bottom of page 5. For instance, it would be more accurate to call this \u2018connectedness regularization\u2019. It is probably not necessary to actually make this shift, but it does highlight that what is being advertised in much of the work is only marginally captured by the actual algorithms. Also, the paper makes a point of noting that regression tasks have richer topology than classification tasks, but if one is only using $H_0$, the target spaces are effectively the same.\n\n**Concern about where improvements are coming from:** One interpretation of the proposed method is that by constraining topology and intrinsic dimension (even on $\\mathbf{Z}$) based on ground truth, one is imposing built-in priors to the regression model. With this extra information, it is not surprising that the new model achieves better performance. One way to test this would be to impose the same regularization on model outputs in $\\mathbf{Y}$ rather than $\\mathbf{Z}$. Are the same improvements seen? I am not sure that the results are less interesting, but the connection to IB might be less compelling."
                },
                "questions": {
                    "value": "- How does this regularization method scale to higher-dimensional target spaces? \n- Is it possible to incorporate higher homology groups into the regularization terms?\n- Do intrinsic dimension estimators outside of the approach leveraged in the regularizer capture a decrease in intrinsic dimension of the latent space representations?\n- How important is it to apply these regularizers to the latent space? Would a similar effect be seen if the outputs were regularized instead?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1763/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698791869490,
            "cdate": 1698791869490,
            "tmdate": 1699636105476,
            "mdate": 1699636105476,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "res1hwC4qI",
                "forum": "OeNcnlQPRz",
                "replyto": "iZ6wMoS5uB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1763/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1763/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer pU4H,\n\nThank you for your insightful and detailed comments.   \n\nFor the weaknesses:\n\n**Writing correctness and clarity**:  Thanks for the detailed remarks. We will revise our paper accordingly. We should define the topology for the target and latent space as the topology induced by the metric, Euclidean distance. Since regression\u2019s target space is naturally a metric space (while classification\u2019s is not), the topology of the target space for classification is harder to exploit than regression.\n\n**Use of the word \u2018topological\u2019 when only $H_0$ is used**: While our designed regularizer only consider $H_0$, our theoretical analysis is not limited to $H_0$. Besides, it of course can consider higher homology groups, and some methods already exist, like the RTD [arxiv 2302.00136]. The only reason why we just consider $H_0$ is that we exploit the topological autoencoder as our topology part $L_t$, and higher (1-dimensional) topological features merely increase runtime for topological autoencoder. \n\n**Concern about where improvements are coming from**: Thanks for this interesting point. 1) The topology and intrinsic dimension can be regarded as built-in priors, and we are imposing the two built-in priors on the regression model. 2) The model outputs, i.e. predicted Y\u2019, can also be regarded as the representation Z, where the intrinsic dimension is commonly forced to be the same as Y, and the MSE loss or other loss clearly encourages Y\u2019 and Y to be the same (including the topology aspect).\n\nFor the questions:\n\n**Scale to higher dimensional**: Our designed regularizer is based on the topological autoencoder and Birdal\u2019s regularizer. Both methods work well for high-dimensional space, e.g., dimension equals the number of pixels for image input. Based on their good performance when scaled to high dimensional, our method should work well for high-dimensional target space. \n\n**incorporate higher homology groups**: of course can. Please see above the weaknesses part.\n\n**decrease in intrinsic dimension of the latent space representations**: Do you mean whether the intrinsic dimension of the latent space really decreased? Based on the visualization and the estimated intrinsic dimension estimated by Birdal\u2019s estimator, the intrinsic dimension is decreased. For exploiting other intrinsic dimension estimators, we will report these results in our future submission.\n\n**apply to the outputs**:  please see above the weaknesses part."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1763/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700656739923,
                "cdate": 1700656739923,
                "tmdate": 1700656739923,
                "mdate": 1700656739923,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "abJZa1TxLR",
            "forum": "OeNcnlQPRz",
            "replyto": "OeNcnlQPRz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1763/Reviewer_5Zit"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1763/Reviewer_5Zit"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the IB principle for regression problems. Then, the connection between the IB principle and topological properties is established. Basing on these observations, the PH-reg regularizer is proposed to harmonize topology of Z space and target space Y and to minimize the intrinsic dimension of latent space Z. Finally, experiments with both synthetic and real-world datasets show that the proposed PH-reg brings some improvement in quality measures."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Only a few papers study application of topology to machine learning and only a few papers study differentiable topological losses.\n2. The paper is mostly well written and clear.\n3. New theoretical results are presented.\n4. Experiments show improvements in quality measures for super-resolution, depth estimation and age prediction problems.\nAblation studies are provided."
                },
                "weaknesses": {
                    "value": "1. I have concerns about the IB principle. Neural networks are deterministic functions, and, thus H(Y|Z) = 0 always.\nH(Z|Y) > 0 when different Z map to the same Y. This can happen when different X map the the same Y, because for real large networks different input objects X have different embeddings Z. So, some H(Z|Y)>0 depends only on the dataset itself, not the network for realistic scenarios.\n\n2. $min_Z \\left( I(Z,X) - \\beta I(Z,Y) \\right) $ and $min_Z \\frac{I(Z,X)}{ \\beta I(Z,Y) }$ are different problems. Thus, Theorem 1 is wrong.\n\n3. The regularizer in eq. 8 is the topological loss from Moot et. al, 2020. You should explicitly mention it.\n\n4. Sometimes the narration is clumsy, ex.: \"Figure 1(a) provides a t-SNE visualization\nof the 100-dimensional feature space with a \u2019Mammoth\u2019 shape target space.\"\nHere I don't understand what does 100-d space mean and what is \"Mammoth target space\".\nAlso, I think for your case visualization with t-SNE is not a good choice since t-SNE often tears a manifold apart.\nTry to use PCA.\n\nminor:\n1. Caption to Figure 3: model' - typo\n2. In Section 3, you write \"Consider a dataset S = {xi, yi} with N samples xi , which typically is an image (xi \\in Rdx1\u00d7dx2 )\"\ncolor channels of image are missing."
                },
                "questions": {
                    "value": "1. what do functions f1, f2, f3, f4 from Section 5.1 mean?\n2. I don't understand the purpose of experiments from Section 5.1.\nYou embed 3D point cloud into a high dimensional space by adding 96 extra noise coordinates.\nThen, you try to predict 3 original coordinates, right?\nSo, ideally, the encoder must learn to ignore 96 noise coordinates and learn identity mapping for the rest of them.\nWhen you enforce the topological regularizer from (Moor et. al, 2020), you enforce pairwise distances in input and target spaces to be the same. Obviously, noise coordinates are forced to be ignored. \n3. What is the target space in the age prediction problem, R ? Then, the homeomorphism from image space R is not possible.\n4. What are the target spaces and Z space for depth estimation and super-resolution problems?\n5. For experiments with DIV2K, the improvement is very small (~0.1%-0.2%). Also std. intervals are not provided.\nHave you optimized hyperparams on the validation dataset and evaluated final quality on the test set?\nOtherwise, results are not valid."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1763/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698844390301,
            "cdate": 1698844390301,
            "tmdate": 1699636105396,
            "mdate": 1699636105396,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EMdIhxe7Qk",
                "forum": "OeNcnlQPRz",
                "replyto": "abJZa1TxLR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1763/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1763/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer 5Zit,\n\nThank you for your insightful and detailed comments.   \n\nFor the weaknesses:\n\n**W1. $\\mathcal H(Y|Z) = 0$ always. $\\mathcal H(Z|Y) > 0$ depends only on the dataset itself:**\n1) There is a misunderstanding between the predicted Y\u2019 by neural networks and the target Y. For predicted Y\u2019, $\\mathcal H(Y\u2019|Z) = 0$ always, since neural networks are deterministic functions. But this does not hold for $\\mathcal H(Y|Z)$. \n2) It seems you mean different X -> different Z -> the samy Y, thus $\\mathcal H(Z|Y) > 0$ and this depends only on the dataset itself. This should be correct for general purpose representation learning, which targets a wide range of downstream tasks. However, for a specific task, in an extreme case, you can treat the predicted Y\u2019 as the representation Z, which clearly shows $\\mathcal H(Y\u2019|Y) =0$ is desirable and is the learning target. By the way, from the invariance representation learning point of view, lowering $\\mathcal H(Z|Y)$ is learning invariance representation with respect to Y.\n\n**W2. minimizing the difference is not equivalent to minimizing the ratio:**  Thanks for pointing this out. We will revise Theorem 1. \n\n**W3. Should explicitly mention the topology autoencoder:** Thanks for pointing this out. We will emphasize it in the main paper.\n\n**W4. Try PCA:**  Appreciate for pointing this out. The PCA works better than t-SNE for visualization on the synthetic dataset. The \u2018100-d space\u2019: the feature space is 100 dimensional. The \u2018Mammoth target space\u2019: the target space is a mammoth. Some descriptions are given in Section 5.1.\n\nFor the questions:\n\n**Q1. Meaning of functions f1-4:**  Functions f1-4 are functions to encode the coordinates. Detailed expressions for these functions can be found in Appendix B.\n\n\n**Q2. experiments from Section 5.1:** It is not learning identity mapping, as the original coordinates are encoded by f1-4. The key purposes are: 1) show the feature space is topologically similar to the target space and regression potentially captures the topology of the target space. 2) verify our methods. The objects in Section 5.1 are topologically different, and experiments with those objects can clearly verify our methods. For real-world tasks, we lack datasets with different \u2018shape\u2019 target space.\n\n**Q3. target space of age prediction:** The target space for age estimation is discrete points. Besides, the homeomorphism is between Z and Y rather than between image space and Y.\n\n**Q4. target spaces of depth estimation and super-resolution:** Depth estimation: 1-dimensional line; super-resolution: 3-dimensional space. \n\n**Q5. Experiments on DIV2K** \n:  1) Most methods have \u2018similar\u2019 performance with DIV2K, and the improvements for many other methods targeted at super-resolution are also very small. The small improvement should be due to the task/dataset itself. 2) Usually, the computational cost is high for super-resolution, but we will consider reporting the std and intervals if possible. 3) We follow the setting of EDSR for the hyper-parameters, apart from the $\\lambda_t$ and $\\lambda_d$."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1763/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700656558277,
                "cdate": 1700656558277,
                "tmdate": 1700656558277,
                "mdate": 1700656558277,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LgE843qdtS",
            "forum": "OeNcnlQPRz",
            "replyto": "OeNcnlQPRz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1763/Reviewer_5oRh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1763/Reviewer_5oRh"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers implementing information bottleneck (IB) principle for regression problem. To this end, authors first derive a new generalization error bound, showing that in regression scenario, the generalization error is upper bounded by a function of the conditional entropy of $Z$ given $Y$, i.e., $H(Z|Y)$, in which $Z$ is the learned feature. Given this, authors mention the connection between $H(Z|Y)$ with respect to the intrinsic dimension of $Z$. Hence, to control the generalization error, it makes sense to minimize the intrinsic dimension of $Z$.\n\nFurther, based on the optimization representation condition that $H(Y|Z)=H(Z|Y)$, authors emphasized the necessarity to add an additional topology preserving regularization terms. Experiments are conducted on three real world data to demonstrate the effectiveness of the two regularization terms."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The IB principle for regression problem is less considered. The motivation of this work is promising.\n2. The new generalization error bound in Eq.~(2) is interesting, although it still has some issues (see below)."
                },
                "weaknesses": {
                    "value": "Overall, I have some concerns regarding Theorems 1, 2 and 3, especially Theorem 2. \n\n1. I have several concerns regarding the new generalization error bound in Eq.~(2).\n\n1.1 how this bound is connected to the new bound in [1], which emphasized the role of I(X;Z|Y) in classification tasks.\n\n[1] Kawaguchi, Kenji, et al. \"How Does Information Bottleneck Help Deep Learning?.\" arXiv preprint arXiv:2305.18887 (2023).\n\n1.2 Is the bound tighter or not? or does H(Z|Y) a good indicator on the generalization error in regression problems? Can you validate this point?\n\n1.3 In fact, the Theorem 2 is a bit unclear, especially the description on Q. Are there some properties that Q should have?\n\n1.4 It is unclear how to derive from Eq.(28) to Eq.(29). I understand that $(E(|z-\\bar{z}|_2))^2 \\leq E |z-\\bar{z}|_2^2$. However, I do not understand why $E |z-\\bar{z}|_2^2$ can be further bounded by $Q(H(Z|Y))$. Does $Q$ exist for general case? In fact, authors only illustrated that $Q$ exists for Gaussian case and uniform case. I am not sure if the derivation from Eq.(28) to Eq.(29) holds for general case.\n\n2. Regarding Theorem 1, it only holds for a deterministic mapping function from $X$ to $Z$, since you assume that $H(Z|X)=0$? This should be emphasized in the main paper, since majority of deep IB approaches use stochastic representations and $H(Z|X)\\neq 0$.\nActually, this also decreases the applicability of Theorem 1. \n\n3. Theorem 3 is actually a straightforward result of [Ghosh & Motani (2023), Proposition 1]. This should also be emphasized in the main paper. \n\n4. Apart from the above concerns, I also feel the two regularization terms are not novel. \nIn fact, the connection between entropy and intrinsic dimension has been carefully discussed in [Ghosh & Motani (2023)]. \nThe way that authors evaluate the intrinsic dimenion seems to be directly from Birdal et al. (2021).\nAs for another regularization term on topology perserving, it also shares rather similar to that in (Moor et al., 2020).\n\n5. Finally, regarding the experiments. It would be much helpful if authors can compare their method to other prevalent deep IB approaches. For example, nonlinear information bottleneck also considers a regression setup. Another example is the convex information bottleneck [2].\n\n[2] Rodr\u00edguez G\u00e1lvez, Borja, Ragnar Thobaben, and Mikael Skoglund. \"The convex information bottleneck lagrangian.\" Entropy 22."
                },
                "questions": {
                    "value": "please see weaknesses 1, 4, and 5."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1763/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698886157267,
            "cdate": 1698886157267,
            "tmdate": 1699636105330,
            "mdate": 1699636105330,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IabBjOlKd2",
                "forum": "OeNcnlQPRz",
                "replyto": "LgE843qdtS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1763/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1763/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer 5oRh,\n\nThank you for your insightful and detailed comments.   \n\nFor the weaknesses:\n\n**W1.1 & 1.2 & 1.3. Connections with the bound in [1], and illustrations of the function Q**: The bound in [1] emphasizes two terms, i.e., $I(X; Z|Y)$ and $I(\\phi; S)$, where the second term measures the effect of overfitting the encoder $\\phi$. $S$ here represents the dataset. By contrast, our bound only emphasizes one term, i.e., $\\mathcal H(Z|Y)$, which equals $I(X; Z|Y)$ with deterministic encoders: \n> $I(X; Z| Y) = I (X; Z) - I(Y; Z) $(shown in [1]) $= H(Z) - H(Z|X) - (H(Z) - H(Z|Y)) =  H(Z|Y) - H(Z| X) =  H(Z|Y)$. \n\nNote, the linear dependence on H(Z| Y, X) in the bound [1] is also mainly for deterministic encoders. For stochastic representations, $H( Z| X, Y) \\approx 0$ if the injected noise is small, and thus the linear dependence also holds.\n\nThe tightness of our bound is determined by the function Q, which aims bound the dispersion/standard deviation of a distribution by its entropy. Compared with the bound in [1], with the Q suggested in proposition 1,  bound [1] is tighter than ours if we ignore the term $I(\\phi; S)$. Because the Q we suggested is exponential dependence while their bound is linear dependence. Q exists for general cases, as the dispersion/standard deviation and the entropy commonly can be estimated for a specific distribution. We thus can find a function Q to scale the entropy to larger than its dispersion/standard deviation.\n\nIt is worth mentioning that we are not targeting a tight/advanced bound. The bound is mainly introduced to support our claim: \u2018minimizing $\\mathcal H(Z|Y) $can be seen as learning a minimal representation by reducing noise. The minimality can reduce the complexity of Z and prevent neural networks from overfitting.\u2019 Comparing it with other advanced bounds and demonstrating it is a good indicator are somehow out of our scope. We will take these points into consideration to revise the manuscript.\n\n\n**W1.4. From Eq.(28) to Eq. (29):**\nBecause $z$ in Eq. (28) follows the distribution $(Z| Y=y_i)$, and based on Eq. (14) we can thus bound $E ||z-\\bar z||_2$ by $\\mathcal H(Z|Y=y_i)$.\n\n**W2. deterministic mapping between Z and X**: Appreciate for pointing this out. We will emphasize this in the main paper. Although our analysis is based on IB, we target neural networks, which are commonly deterministic functions. Besides, our method can be extended to stochastic representations if $p(z|x)$ follows some fixed distributions like the standard Gaussian distribution, which is commonly exploited in VAE. The deterministic mapping assumption in theorem 1 is to cancel the term $\\mathcal H(Z|X)$. If $p(z|x)$ follows some fixed distributions, then $\\mathcal H(Z|X)$ is a constant and thus also can be canceled. \n\n**W3.** Thanks for pointing this out. We will emphasize this in the main paper\n\n**W4. The novelty of the two regularizer terms**: Our main contribution is the two established connections between the topology and IB for regression representation learning, and the two regularizer terms are mainly introduced to verify the connections. Besides, although the two regularizers are designed based on Birdal\u2019s regularizer and the topological autoencoder, designing the regularizer in such a way, that follows our established connections, is novel. \n\n**W5. Compared to other IB approaches.** Thanks for the suggestion. We will revise the manuscript accordingly."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1763/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700656086164,
                "cdate": 1700656086164,
                "tmdate": 1700656086164,
                "mdate": 1700656086164,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pupMtYV5HL",
            "forum": "OeNcnlQPRz",
            "replyto": "OeNcnlQPRz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1763/Reviewer_7gBF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1763/Reviewer_7gBF"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes to use a regularizer PH-Reg, with aim to lower the intrinsic dimension of the feature space while trying to preserve the \"0-dimensional topologically relevant distances\", in the context of representation learning. Some arguments, favoring  reducing a quantity which could be related to the intrinsic dimension of the feature space, in terms of information bottleneck approach, are given. Some  empirical verifications of the method are described, an access to the source code for experiments for reproducibility purpose is not provided."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper tries to relate quantitative characteristics of data representations from information theory with topological data characteristics, following several recent approaches."
                },
                "weaknesses": {
                    "value": "1) The proposed \"intrinsic dimension lowering\" loss term $\\mathcal{L}_d$  actually disturbs the intrinsic dimension in an unclear way, it can both increase and decrease it, since, for a given data representation $Z$, the term   $\\mathcal{L}_d$  involves the ratio of logarithms $\\log E(Z_n)/ \\log E(Y_n)$ where $Y$ is another data representation. There is no $\\log E_n(Y)$, $Y-$dependent part, in the formula for intrinsic dimension of $Z$, see eg arXiv:2306.04723 or J. M. Steele, Growth rates of euclidean minimal spanning trees with power weighted edges, The Annals of\nProbability, 16(4):1767\u20131787, 1988.  So the paper narrative concerning the lowering of intrinsic dimension is seemingly in contradiction with the reported experiments setup. \n\n2) Reproducibility check concerning the reported experiments in the manuscript (Section 5) could not be performed. No source code was made available during the review phase. \n\n3) Theoretical results reported in Section 3 seem to have strong intersection with previously  published papers, in particular, Theorem 3 from Section 3 seems to be similar to Proposition 1 from  [Ghosh & Motani (2023)], cf Appendix A.3. \n\n4) The definition of the intrinsic dimension employed in Section 3, which is the same as the definition in  [Ghosh & Motani (2023)],  is completely different from the definition of intrinsic dimension via the 0-th persistent homology, employed in Section 4. So, a priori, there might be no connection between the two quantities. \n\n3) Training details are  lacking in the description of the experiments reported in Table 1, Table 2, Table 3, Table 4.   In particular, how many iterations/epochs were used, what were the stopping criteria? It might be that the experiments, if they can be reproduced, cf above, could be explained by overfitting of the baselines, since it seems that the standard regularizers, such as weight decay, were not used. \n\n4) Standard deviations are not reported in Table 2, Table 3, Table 4. \n\n5) Only a single baseline method is employed for comparison in each case in Table 2, Table 3, Table 4. \n\n6) The loss $\\mathcal{L}_t$ employed \"to preserve topology\", has been recently shown to have the following drawbacks: firstly, this loss is not continuous, secondly, moreover, diminishing   $\\mathcal{L}_t$  can lead to bigger difference in topology, cf  arxiv2302.00136, Appendix J, Figure 10. \n\n7)  Related work section does not mention many relevant papers, eg arXiv:2106.04024, arXiv:2201.00058, arXiv:2306.04723, J. M. Steele, Growth rates of euclidean minimal spanning trees with power weighted edges, The Annals of\nProbability, 16(4):1767\u20131787, 1988.\n\n9) The writing could be improved, there are some vague statements and not very clearly defined notions. \n\nBelow are some specific remarks: \n\npage 2 : \"intrinsic dimension of the feature space\" - what is this ? From what follows a reader can guess that seemingly it is  what can be  called intrinsic dimension of the dataset points  in the feature space, is it?\n\npage 2 :  Figure 1a - The right picture with supposedly \"topology similar\" has clearly too many clusters, eg a separate leg cluster, and, on the contrary, it does not preserve topology. The preserving topology representations exist in the literature for this mammooth dataset and they look completely different. \n\npage 2 : Figure 1b \"Lower the intrinsic dimension\" picture - Is the paper saying that a good representation of the 3d mammoth for regression task should be 1dimensional? First of all, the 3d object smashed into 1d space does not seem to be a good representation for the vast majority of tasks including many other regression tasks.  Secondly, such a smashing into 1d space representation would depend heavily on the regression task, which is also usually considered to be bad for a data representation. \n\npage 2:  \"The homeomorphic between two spaces\" - should it be homeomorphism ?\n\npage 2:  \" t-SNE visualization of the 100-dimensional feature space\" - the 100d feature space is described only in Section 5.1, several pages down. It would be better to mention it here on page 3, otherwise it is impossible to understand what are the 100 dimensions mentioned here. \n\npage 2: \"we are the first to explore topology in the context of regression representation learning\"- actually, the autoencoders learn the regression task of reproducing the coordinates of the data points, so no, the topology has been heavily explored in previous works in the context of regression representation learning. \n\npage 3: \"The intrinsic dimension of the last hidden layer\" -  what is this ? From other contexts a reader can probably guess that, seemingly, it is  what can be called intrinsic dimension of the dataset points  in the last hidden layer, is it?\n\npage 3:  \"1-dimensional (Trofimov et al 2023) topologically relevant distances\" - actually the method from (Trofimov et al 2023) preserves both 0-dimensional and 1-dimensional topological features, and even arbitrary $k-$dimensional features. \n\npage 3:  \"However, unlike classification,\" - as already mentioned, the autoencoders learn the regression task of reproducing the coordinates of dataset points. \n\npage 4: \"$Dim_{ID} M_i$ is the intrinsic dimension of the manifold\"(corresponding to the distribution)   - which definition of intrinsic dimension is used here? \n\npage 5: \"is a homomorphism between ... and\" - is a homomorphism from... to ...\n\npage 5: \"that the set of points S sampled from\" - that the set of points S _is_ sampled from ?"
                },
                "questions": {
                    "value": "What is the relation between the quantity called \"intrinsic dimension\" in the Theorem 3, Section 2, seemingly based on \u03f5-neighborhood intrinsic dimension and the \"intrinsic dimension\" calculated via asymptotic of the minimal spanning trees in Section 3?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 6,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1763/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699558912033,
            "cdate": 1699558912033,
            "tmdate": 1699636105249,
            "mdate": 1699636105249,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "boK7QrCxjR",
                "forum": "OeNcnlQPRz",
                "replyto": "pupMtYV5HL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1763/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1763/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer 7gBF,\n\nThank you for your insightful and detailed comments.   \n\nFor the weaknesses:\n\n**W1. The term $\\mathcal L_d$ can both increase and decrease the intrinsic dimension:** Yes, this term can do both.  We aim to encourage a feature space with an intrinsic dimension (ID) equal to the target space rather than only lowering the ID. Therefore, we apply $\\mathcal L_d$ to increase or decrease as necessary.  We will revise the manuscript to state this more explicitly.\n\n**W2. Release the Code**: We will release the code upon paper acceptance. \n\n**W3. Theorem 3 Intersection with [Ghosh & Motani (2023)]**: Theorem 3 is a result of [Ghosh & Motani (2023), Proposition 1]. This is shown in the appendix A.3, and we will emphasize it in the main paper.\n\n**W4. Different definitions for intrinsic dimension**:  Yes, the definitions for the 0-th persistent homology (employed in the regularizer) and the $\\epsilon$-neighborhood intrinsic dimension (employed in our theoretical analysis)  are different. However, this difference does not invalidate our conclusions, as our theoretical analysis is purely based on the $\\epsilon$-neighborhood intrinsic dimension. In addition, although the 0-th persistent homology has a different mathematical definition and its value can be different, it is also defined for the intrinsic dimension. It is thus reasonable to exploit the 0-th persistent homology to constrain the intrinsic dimension. \n\nA similar concern with corresponding discussions can be found here: https://openreview.net/forum?id=099uYP0EKsJ&noteId=zMiYTAyu9ol .\n\n\n**W5, 6, 7, 9, 10. Lacking training details, standard deviations, and related works; only a single baseline;  writing could be improved**: Thanks for the detailed remarks. We will revise the manuscript accordingly. \n\n**W8. Drawbacks of the topology autoencoder**\n**A**: We also noticed this and once considered the RTD [arxiv 2302.00136]. However, the topology autoencoder works well in reality, and its drawbacks do not invalidate our conclusion.\n\n**We thank you for your detailed remarks. We want to clarify the remark 2 (about the Figure 1a) and the remark 3 (about the Figure 1a):**\n- **Figure 1a, topology similar**: We tried the PCA as suggested by reviewer 5Zit, and found the \u2018too many clusters\u2019 is due to the t-SNE, and the results obtained by PAC do not present such \u2018clusters\u2019. \n- **Figure 1b, 1d space for 3d mammoth**: This figure is not for 3d mammoth. It is for the depth estimation task, where the target space is 1d.\n\n**Question:** Please see the response for the W4."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1763/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700655421836,
                "cdate": 1700655421836,
                "tmdate": 1700655421836,
                "mdate": 1700655421836,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]