[
    {
        "title": "Zero-Mean Regularized Spectral Contrastive Learning"
    },
    {
        "review": {
            "id": "UXvhbFb8TG",
            "forum": "RZBy8oHTz4",
            "replyto": "RZBy8oHTz4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission224/Reviewer_41KY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission224/Reviewer_41KY"
            ],
            "content": {
                "summary": {
                    "value": "By incorporating an additive factor into the SpeCL term that involves negative pairs, this paper enforces the mean of representations to be zero. The experimental results and related theoretical analysis suggest that introducing $\\tau$ can improve performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.This paper addresses the issue of negative pairs in contrastive learning, which is a hot topic in the research community. The authors aim to provide a solution to this problem that arises in related CL works.\n\n2.The authors of this paper present a theoretical foundation to support the proposed zero-mean regularization in both the unsupervised domain adaptation (UDA) task and supervised classification with noisy labels.\n\n3.The proposed method's robustness is improved by the impressive results achieved on the supervised classification task, particularly when dealing with symmetric label noise."
                },
                "weaknesses": {
                    "value": "1. This work appears to be relatively incremental based on Ref. [18] and [19]. Basically, what caught my interest is the representation of spectral embeddings using affinities and features; however, it was previously proposed in [18].\n\n2. Although the authors describe the proposed additive factor as simple yet effective, this work can still be considered progressive research. However, when compared to [18] and [19], it is unclear whether significant progress has been made for this conference.\n\n3. The effectiveness of the proposed method is demonstrated through experiments that involve fewer state-of-the-art methods."
                },
                "questions": {
                    "value": "1. On page 4, there is a missing space in \"thatSpeCL\".\n\n2. It is suggested to include more benchmark methods in the experiments to avoid categorizing it as an enhanced/improved version of SpeCL method, and instead position it as a robust SSL method.\n\n3. The factor $\u03c4$ effectively relaxes the orthogonal constraint on negative pairs. It would be interesting to explore the integration of other methods, such as Barlow Twin, in different ways.\n\n4. In the experiments, it is recommended to analyze the results achieved with different values of $\\tau$ due to its previous detailed analysis. This analysis can provide insights into what can be inferred from the results obtained with different $\\tau$ values."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission224/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission224/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission224/Reviewer_41KY"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission224/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698480367106,
            "cdate": 1698480367106,
            "tmdate": 1699635948217,
            "mdate": 1699635948217,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JcQ0wsnzGY",
                "forum": "RZBy8oHTz4",
                "replyto": "UXvhbFb8TG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission224/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission224/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Reviewer 41KY (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive comments and insightful suggestions.\n\n**Comment 1:** *This work appears to be relatively incremental based on Ref. [18] and [19]. Basically, what caught my interest is the representation of spectral embeddings using affinities and features; however, it was previously proposed in [18].* and *Although the authors describe the proposed additive factor as simple yet effective, this work can still be considered progressive research. However, when compared to [18] and [19], it is unclear whether significant progress has been made for this conference.*\n\n**Response:** Thank you for your kind comment.\nAt first glance, the formulation of the proposed zero-mean regularized spectral contrastive loss (ZM-SpeCL) may appear similar to the vanilla SpeCL [18] and [19], with the only difference being the introduction of $\\tau$ in the term involving negative pair. However, it cannot be simply justified that our work is an incremental improvement over SpeCL. Otherwise, following the same logic, some recent well-known works in self-supervised learning, such as BYOL and SimSiam, could also be considered incremental compared to SimCLR.\n\nMore importantly, ZM-SpeCL possesses a clear theoretical motivation and support. It is specifically designed to modify pairwise similarities between positive pairs, which intuitively relaxes the\northogonality of representations between negative pairs and implicitly alleviates\nthe adverse effect of wrong connections in the positive-pair graph, leading to better\nperformance and robustness. Our work is not only simple yet effective, but also theoretically sound.\n\nIn Section 3.2, we delve into the role of zero-mean regularization on spectral contrastive pretraining-based unsupervised domain adaptation and provide theoretical proof that zero-mean regularization can tighten the error bound by multiplying a factor less than one. In Section 3.3, we establish the supervised version of spectral contrastive loss and derive the closed-form optimal representations, which resembles the Neural Collapse phenomenon and suggests using class-mean features as a classifier. We further prove that zero-mean regularization can mitigate label noise by implicitly reducing mislabeled weights in the noise transition matrix.\n\n------\n\n**Comment 2:** *The effectiveness of the proposed method is demonstrated through experiments that involve fewer state-of-the-art methods.* and *It is suggested to include more benchmark methods in the experiments to avoid categorizing it as an enhanced/improved version of SpeCL method, and instead position it as a robust SSL method.*\n\n**Response:** Thank you for your constructive comment. In response to your suggestion, we have expanded our experimental evaluation to include comparisons with additional state-of-the-art methods, We have incorporated self-supervised learning methods such as MoCo, SimCLR, and BYOL into our comparative analysis. For these experiments, we employed a PreAct-ResNet18 architecture and conducted training over a total of 200 epochs. The training process utilized the SGD optimizer with a momentum of 0.9, a learning rate of 0.1, and a weight decay of 5e-4. To dynamically adjust the learning rate over the course of the 200 epochs, we implemented the cosine decay learning rate schedule.\n\nThe comprehensive experimental results, including comparisons with the aforementioned state-of-the-art methods, are presented in the table below. We believe that this expanded evaluation provides a more thorough understanding of the effectiveness of our proposed method in the context of self-supervised learning.\n\n| Method             | CIFAR-10  | CIFAR-100 | SVHN      |\n| :----------------- | :-------- | :-------- | :-------- |\n| SimCLR             | 83.84     | 55.04     | 88.07     |\n| MoCo               | 73.63     | 51.88     | 82.25     |\n| BYOL               | 86.32     | 54.73     | 90.24     |\n| SpeCL              | 86.25     | 52.44     | 89.58     |\n| SpeCL ($\\tau$=0.1) | 86.77     | 53.98     | 90.36     |\n| SpeCL ($\\tau$=0.2) | 86.93     | 55.25     | 90.80     |\n| SpeCL ($\\tau$=0.5) | 87.05     | 56.61     | 91.10     |\n| SpeCL ($\\tau$=1.0) | **87.24** | **56.77** | **91.12** |\n\n------\n\n**Comment 3:** On page 4, there is a missing space in \"thatSpeCL\".\n\n**Response:** Thank you for your careful review. We have rectified the mentioned typos and addressed some other potential errors."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission224/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700368755304,
                "cdate": 1700368755304,
                "tmdate": 1700368755304,
                "mdate": 1700368755304,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yqHYskocQm",
                "forum": "RZBy8oHTz4",
                "replyto": "UXvhbFb8TG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission224/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission224/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Reviewer 41KY (2/2)"
                    },
                    "comment": {
                        "value": "**Comment 4:** The factor $\\tau$ effectively relaxes the orthogonal constraint on negative pairs. It would be interesting to explore the integration of other methods, such as Barlow Twin, in different ways.\n\n**Response:** Thank you for your insightful comment and suggestion. We have explored the integration of other methods, specifically Barlow Twins, by incorporating the relaxation factor $\\tau$ to effectively relax the orthogonal constraint on negative pairs. The modified Barlow Twins loss function, denoted as $L_{BT}$, is expressed as follows:\n\\begin{equation}\n    L_{BT}=\\sum_i (1-C_ii)^2 + \\lambda \\sum_{i\\neq j} (C_{ij}+\\tau)^2,\n\\end{equation}\nwhere $C$ is the cross-correlation matrix computed between the outputs of two batches of samples. The hyperparameter $\\tau$ is introduced to control the relaxation of the orthogonality constraint.\n\nTo provide empirical evidence of the impact of incorporating $\\tau$ into Barlow Twins, we trained the models for 200 epochs using the open-source code available at https://github.com/IgorSusmelj/barlowtwins. The results, presented in the table below, demonstrate the KNN accuracy for different values of $\\tau$:\n\n| $\\tau$       | 0.0   | 0.1   | 0.2   | 0.5   | 1.0   |\n| ------------ | ----- | ----- | ----- | ----- | ----- |\n| KNN Accuracy | 74.63 | 74.99 | 74.89 | 74.87 | 74.20 |\n\nThe results indicate that the incorporation of $\\tau$ in Barlow Twins improves downstream KNN classification accuracy. We appreciate your suggestion, and these findings suggest that the introduced relaxation factor can positively influence the performance of Barlow Twins in the context of our experiments.\n\n\n\\hspace*{\\fill}\n\n\\noindent\\textbf{Comment 5:} In the experiments, it is recommended to analyze the results achieved with different values of $\\tau$ due to its previous detailed analysis. This analysis can provide insights into what can be inferred from the results obtained with different values.\n\n\\noindent\\textbf{Response:} We appreciate your valuable feedback and would like to provide additional insights into the role of the regularization parameter $\\tau$ and the trade-offs associated with its selection. In this paper, we propose setting $\\tau\\in[0,1]$ to balance the regularization strength of zero-mean regularization. Although in Section 3, we do not explicitly constrain $\\tau\\le 1$, theoretical and empirical evidence supports the necessity of such a constraint:\n\n\n- In Theorem 3.1, we assume that $\\tau<\\frac{\\tilde{\\lambda}_1(0)-\\tilde{\\lambda}_d(0)}{\\tilde{\\lambda}_1(0)}<1$ to ensure that the introduction of $\\tau$ will not alter the value of the $d$-th eigenvalue in Eq. (C.23). In Proposition 3.2, we assume that $\\tau\\le \\frac{\\tilde{\\lambda}_1(0)-\\tilde{\\lambda}_2(0)}{\\tilde{\\lambda}_1(0)}< \\frac{\\tilde{\\lambda}_1(0)-\\tilde{\\lambda}_d(0)}{\\tilde{\\lambda}_1(0)}<1$ to guarantee that the first eigenvalue of $\\tilde{A}'$ is $\\tilde{\\lambda}_1(\\tau)=(1-\\tau)\\tilde{\\lambda}_1(0)$ (as can be seen, $\\tau\\le 1$ is essential to avoid the presence of negative eigenvalues of $\\tilde{A}'$) in Eq. (C.32), thus facilitating a more intuitive comparison with the bound at $\\tau=0$. While it is possible to draw conclusions within the range of $[\\frac{\\tilde{\\lambda}_1(0)-\\tilde{\\lambda}_d(0)}{\\tilde{\\lambda}_1(0)}, 1]$, it requires a more nuanced comparison of the order of these eigenvalues, predictably resulting in an obviously different bound than at $\\tau=0$.\n\n- In Theorem 3.3, we prove that the global minimum of the supervised spectral contrastive loss is achieved at $\\hat{H}=\\arg\\min_{H}\\|(rI-\\tau11^\\top)-H^\\top H\\|_F^2$. When $r\\le d+1$, $\\|(rI-\\tau11^\\top)-H^\\top H\\|_F^2$ has a minimum of zero only for $\\tau\\in [0,1]$, corresponding to $\\hat{H}^\\top \\hat{H}=rI-\\tau11^\\top$, where the angle between representations of different classes is $\\arccos\\frac{-\\tau}{r-\\tau}>90^\\circ$. However, for $\\tau > 1$, there exists no $\\hat{H}$ that satisfies $\\hat{H}^\\top \\hat{H}=rI-\\tau11^\\top$, as it would lead to a paradoxical situation where $0\\le 1^\\top\\hat{H}^\\top \\hat{H} 1= 1^\\top(rI-\\tau 11^\\top)1=r^2(1-\\tau)<0$.\n\n- We conducted experiments for $\\tau>1$ as presented in Tables 9 and 11 of the Appendix. As observed, consistently better results are obtained with $\\tau\\in[0,1]$, while excessively large $\\tau$ values hinder overall performance. Thus, we suggest limiting $\\tau$ to the range of $[0,1]$ to balance the regularization and discriminative capabilities.\n\nMany thanks to the reviewer for the valuable suggestion, we have added this part to Section A.4 in the revised version."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission224/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700368883829,
                "cdate": 1700368883829,
                "tmdate": 1700368883829,
                "mdate": 1700368883829,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CHQI5ceecP",
                "forum": "RZBy8oHTz4",
                "replyto": "UXvhbFb8TG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission224/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission224/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to your feedback"
                    },
                    "comment": {
                        "value": "Dear reviewer 41KY,\n\nThanks again for your valuable time and insightful comments. As the deadline for the Author/Reviewer discussion is approaching, it would be nice of you to let us know whether our answers have solved your concerns so that we can better improve our work. We are looking forward to your feedback.\n\nBest regards,\n\nAuthors of Paper 224"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission224/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700729845209,
                "cdate": 1700729845209,
                "tmdate": 1700729845209,
                "mdate": 1700729845209,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6wDUTP8QfB",
            "forum": "RZBy8oHTz4",
            "replyto": "RZBy8oHTz4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission224/Reviewer_9yLv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission224/Reviewer_9yLv"
            ],
            "content": {
                "summary": {
                    "value": "The authors provide a new regularizer for (spectral) contrastive\nlearning which supposedly improves the quality of the representation.\nThe authors provide experiments to show the quantitative improvements\ndue to their method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The regularization is well-motivated and intuitively explained.  The\nexperiments seem convincing and suggest that the improvement is\nconsisent across datasets.\n\nThe appendix is extensive."
                },
                "weaknesses": {
                    "value": "The results on the CIFAR datasets are not state-of-the-art for\ncontrastive learning.  This begs the question whether the regularizer\ncould also be applied to other CL techniques such as SimCLR or others.\nHopefully the regularization would also help in that case.\n\nTable 1 & 2 only report single numbers.  It would be more convincing\nif there was a mean +- std reported or some other statisic computed\nover multiple runs."
                },
                "questions": {
                    "value": "Could the approach be extended to other CL techniques?  Do you expect\nit to improve the results similarly?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission224/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission224/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission224/Reviewer_9yLv"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission224/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698743830484,
            "cdate": 1698743830484,
            "tmdate": 1699635948133,
            "mdate": 1699635948133,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "L3xkAjxUQz",
                "forum": "RZBy8oHTz4",
                "replyto": "6wDUTP8QfB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission224/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission224/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 9yLv (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive comments and insightful suggestions.\n\n**Comment 1**: The results on the CIFAR datasets are not state-of-the-art for contrastive learning. This begs the question whether the regularizer could also be applied to other CL techniques such as SimCLR or others. Hopefully the regularization would also help in that case.\n\n**Response**: Thank you for your insightful comment.  We would like to provide further clarification on the applicability of the zero-mean regularization to other contrastive learning (CL) techniques.\n\nThe primary concept behind zero-mean regularization is to implicitly reduce the weights of incorrect connections in the positive-pair graph, as emphasized throughout the work. This idea is fully depicted in the unsupervised domain adaptation (see Eq. 3.3) and learning with noisy labels (see Theorem 3.4) in Section 3.3.2. Our focus in this work has been on the spectral contrastive loss due to its theoretical soundness and ease of analysis. The manifestation is succinctly expressed as the loss-specific \"zero-mean regularization\".\n\nTo extend the core idea of ``weight reduction in positive pairs\" to other popular CL techniques, such as SimCLR, MoCo, and CPC, we propose a modification to the InfoNCE loss by uniformly reducing the importance of positive pair as follows:\n\\begin{equation}\n-\\log \\frac{(1-\\tau)e^{sim(x,x^+)}}{(1-\\tau)e^{sim(x,x^+)}+\\sum_{i=1}^ke^{sim(x,x_i^-)}}\n=-\\log \\frac{e^{sim(x,x^+)-\\tau'}}{e^{sim(x,x^+)-\\tau'}+\\sum_{i=1}^ke^{sim(x,x_i^-)}}\n=-\\log \\frac{e^{sim(x,x^+)}}{e^{sim(x,x^+)}+\\sum_{i=1}^ke^{sim(x,x_i^-)+\\tau'}},\n\\end{equation}\nwhere $sim(x_1,x_2)$ denotes the similarity between $x_1$ and $x_2$, $\\tau'=\\log\\frac{1}{1-\\tau}>0$ and $\\tau\\in[0, 1)$. As can be seen, the derived form that adds a margin term is similar to margin-based losses, particularly the negative-margin softmax loss [1]. From this point of view, weight reduction in positive pairs coincides with the motivation of margin-based losses  that enlarges the discriminativeness with intuitive decision boundaries. \n\nFor empirical validation of the efficacy of the modified InfoNCE loss, we conducted experiments on self-supervised and supervised learning using CIFAR-10/-100 and SVHN datasets. These additional experiments aim to provide further evidence of the versatility inherent in the concept of \"weight reduction in wrong positive pairs\" and demonstrate its potential to enhance the performance of various contrastive learning algorithms on real-world datasets.\n**Self-Supervised Learning:**\n\n|                          | CIFAR-10  | CIFAR-100 | SVHN       |\n| :----------------------- | :-------- | :-------- | :--------- |\n| infoNCE                  | 86.56     | 55.51     | 90.769     |\n| infoNCE with $\\tau=0.05$ | **86.64** | **55.68** | **91.049** |\n| infoNCE with $\\tau=0.1$  | **86.97** | **56.07** | **90.934** |\n| infoNCE with $\\tau=0.2$  | **86.65** | **55.55** | 90.769     |\n| infoNCE with $\\tau=0.5$  | **87.32** | **56.01** | **90.953** |\n| infoNCE with $\\tau=0.7$  | 86.41     | **55.98** | **90.896** |\n| infoNCE with $\\tau=0.9 $ | 86.48     | **55.88** | **90.953** |\n| infoNCE with $\\tau=0.95$ | **86.80** | **55.56** | **91.288** |\n\n**Supervised Contrastive Learning**:\n\n|                          | CIFAR-10  | CIFAR-100 | SVHN       |\n| :----------------------- | :-------- | :-------- | :--------- |\n| infoNCE                  | 94.18     | 71.43     | 96.105     |\n| infoNCE with $\\tau=0.05$ | 94.04     | **71.80** | **96.235** |\n| infoNCE with $\\tau=0.1$  | **94.27** | **71.56** | **96.166** |\n| infoNCE with $\\tau=0.2$  | **94.19** | **72.07** | **96.201** |\n| infoNCE with $\\tau=0.5$  | 94.17     | **72.44** | **96.182** |\n| infoNCE with $\\tau=0.7$  | **94.54** | **72.52** | **96.136** |\n| infoNCE with $\\tau=0.9 $ | **94.43** | **72.29** | **96.155** |\n| infoNCE with $\\tau=0.95$ | **94.33** | **71.82** | **96.097** |\n\n> [1] Bin Liu, Yue Cao, Yutong Lin, Qi Li, Zheng Zhang, Mingsheng Long, and Han Hu. Negative margin matters: Understanding margin in few-shot classification. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part IV 16, pages 438\u2013455. Springer, 2020.\n\n-------"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission224/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700368519336,
                "cdate": 1700368519336,
                "tmdate": 1700368519336,
                "mdate": 1700368519336,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uoGw8THffD",
                "forum": "RZBy8oHTz4",
                "replyto": "6wDUTP8QfB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission224/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission224/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 9yLv (2/2)"
                    },
                    "comment": {
                        "value": "**Comment 2:** Table 1 \\& 2 only report single numbers. It would be more convincing if there was a mean +- std reported or some other statisic computed over multiple runs.\n\n**Response**: Thank you for your kind suggestion. In the revised manuscript, we have updated the comparison results by including mean values along with their corresponding standard deviations (mean $\\pm$ std).\n\n**Self-Supervised Learning:**\n\n|                       | CIFAR-10             | CIFAR-100            | SVHN                 |\n| :-------------------- | :------------------- | :------------------- | :------------------- |\n| Vanilla SpeCL         | 86.22 $\\pm$ 0.12     | 52.66 $\\pm$ 0.16     | 89.77 $\\pm$ 0.19     |\n| SpeCL with $\\tau=0.1$ | **86.83 $\\pm$ 0.12** | **53.97 $\\pm$ 0.19** | **90.26 $\\pm$ 0.07** |\n| SpeCL with $\\tau=0.2$ | **86.84 $\\pm$ 0.10** | **55.27 $\\pm$ 0.02** | **90.78 $\\pm$ 0.07** |\n| SpeCL with $\\tau=0.5$ | **87.15 $\\pm$ 0.12** | **56.37 $\\pm$ 0.27** | **91.10 $\\pm$ 0.13** |\n| SpeCL with $\\tau=1.0$ | **87.09 $\\pm$ 0.16** | **56.63 $\\pm$ 0.10** | **91.24 $\\pm$ 0.09** |\n\n**Supervised Contrastive Learning:**\n\n|                       | CIFAR-10             | CIFAR-100            | SVHN                 |\n| :-------------------- | :------------------- | :------------------- | :------------------- |\n| Vanilla SpeCL         | 93.47 $\\pm$ 0.46     | 65.59 $\\pm$ 0.75     | 96.05 $\\pm$ 0.16     |\n| SpeCL with $\\tau=0.1$ | **94.21 $\\pm$ 0.13** | **68.15 $\\pm$ 0.19** | **96.22 $\\pm$ 0.05** |\n| SpeCL with $\\tau=0.2$ | **94.52 $\\pm$ 0.05** | **68.04 $\\pm$ 0.18** | **96.28 $\\pm$ 0.05** |\n| SpeCL with $\\tau=0.5$ | **94.36 $\\pm$ 0.12** | **69.06 $\\pm$ 0.34** | **96.22 $\\pm$ 0.09** |\n| SpeCL with $\\tau=1.0$ | **94.38 $\\pm$ 0.06** | **69.59 $\\pm$ 0.11** | **96.25 $\\pm$ 0.10** |"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission224/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700368571554,
                "cdate": 1700368571554,
                "tmdate": 1700368571554,
                "mdate": 1700368571554,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Kipxd73gup",
                "forum": "RZBy8oHTz4",
                "replyto": "6wDUTP8QfB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission224/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission224/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to your feedback"
                    },
                    "comment": {
                        "value": "Dear reviewer 9yLv,\n\nThanks again for your valuable time and insightful comments. As the deadline for the Author/Reviewer discussion is approaching, it would be nice of you to let us know whether our answers have solved your concerns so that we can better improve our work. We are looking forward to your feedback.\n\nBest regards,\n\nAuthors of Paper 224"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission224/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700729825288,
                "cdate": 1700729825288,
                "tmdate": 1700729825288,
                "mdate": 1700729825288,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VMfXDaljqZ",
                "forum": "RZBy8oHTz4",
                "replyto": "Kipxd73gup",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission224/Reviewer_9yLv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission224/Reviewer_9yLv"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response.  I appreciate the additional detail.  I will keep my score (as it already is positive)."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission224/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700737195237,
                "cdate": 1700737195237,
                "tmdate": 1700737195237,
                "mdate": 1700737195237,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "M39EwyeOLb",
            "forum": "RZBy8oHTz4",
            "replyto": "RZBy8oHTz4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission224/Reviewer_Ef5A"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission224/Reviewer_Ef5A"
            ],
            "content": {
                "summary": {
                    "value": "In this point, the authors try to extend spectral contrastive learning with negative pairs. By adding a zero-mean regularization, the loss function relaxes the orthogonality of representations between negative pairs and implicitly alleviates the adverse effect of wrong connections in the positive-pair graph, leading to better performance and robustness. Beyond that, this paper gives a solid theoretical analysis for the regularization."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper is very easy to follow. The motivation is very clear, and the methodology is elegant.\n\n2. The perspective of UDA is novel. It provides a different way to observe contrastive learning.\n\n3. The perspective of supervised classification with a noise label is also helpful for contrastive learning."
                },
                "weaknesses": {
                    "value": "1. The novelty is an issue. According to the Equ 3.2, it is the same as contrastive laplacian eigenmaps (NeurIPS 2021). In contrastive laplacian eigenmaps, they have the same three terms. The main point is the fully connected adjacent matrix (the all-one matrix)."
                },
                "questions": {
                    "value": "I would like to hear from authors about the relationship between this one and contrastive laplacian eigenmaps. I think the authors give a totally different theoretical analysis based on a very similar thing."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission224/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698796598437,
            "cdate": 1698796598437,
            "tmdate": 1699635948050,
            "mdate": 1699635948050,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "066SDq5oDY",
                "forum": "RZBy8oHTz4",
                "replyto": "M39EwyeOLb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission224/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission224/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Ef5A"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive comments and insightful suggestions.\n\n**Comment:** *The novelty is an issue. According to the Equ 3.2, it is the same as contrastive laplacian eigenmaps (NeurIPS 2021). In contrastive laplacian eigenmaps, they have the same three terms. The main point is the fully connected adjacent matrix (the all-one matrix).* and *I would like to hear from authors about the relationship between this one and contrastive laplacian eigenmaps [1]. I think the authors give a totally different theoretical analysis based on a very similar thing.*\n\n**Response:** Thank you for your insightful comment. While there are certain similarities between Zero-mean regularized spectral loss (Zero-SpeCL) in Eq. 3.2 and contrastive Laplacian eigenmaps (COLES) [1], there are distinct differences between them:\n\n- `Different Negative Components.` The contrastive objective (to be maximized) can generally be formulated as $J(f)=E_{(x,x^+)\\sim p_{pos}} s(f(x),f(x^+)) + \\eta E_{(x,x^-)\\sim p_{data}} \\tilde{s}(f(x),f(x^-))$. While the positive components of Zero-SpeCL and COLES are the same, their negative components are different. The negative component in COLES is $\\tilde{s}(f(x),f(x^-))=-f(x)^\\top f(x^-)$, whereas in Zero-SpeCL, it is $\\tilde{s}(f(x),f(x^-))=-(f(x)^\\top f(x^-)+\\tau)^2$.\n- `Different Reasons for Covariance Term.` Both Zero-SpeCL and COLES introduce the term $\\Vert \\mathbb{E}_x f(x) f(x)^\\top-I\\Vert_F^2$. The motivation of COLES is to softly satisfy the constraint $F^\\top F=I$ that removes an arbitrary scaling factor in the embeddings [2]. Additionally, it helps avoid collapsed solutions since, without the constraint $F^\\top F=I$, the derived graph Dirichlet energy $-Tr(F^\\top\\Delta W F)$ would be minimized when all representations collapse to a constant vector. In contrast, the covariance term $\\mathcal{R}_1(f)=\\Vert \\mathbb{E}_x f(x) f(x)^\\top-I\\Vert_F^2$ in Zero-SpeCL appears as a part of an equivalent form in Eq. 3.2.\n- `Different Overall Objective.` COLES represents a constrained graph Dirichlet energy minimization problem (contrastive Laplacian eigenmaps), i.e.,  $\\min_{F^\\top F=I}-Tr(F^\\top\\Delta W F)$. On the other hand, Zero-SpeCL depicts the low-rank matrix approximation problem $\\min_F|A-F^\\top F|_F^2$ (as shown in Eq. 3.3).\n- `Different Focuses.` Though both works revolve around contrastive learning, our work pays more attention on discussing in theory how zero-mean regularization is helpful in downstream unsupervised domain adaptation (UDA) and supervised classification with noisy labels. We theoretically prove that zero-mean regularization can tighten the error bound by multiplying a factor less than one for UDA (Section 3.2), and prove that zero-mean regularization can mitigate label noise by implicitly reducing mislabeled weights in the noise transition matrix (Section 3.3).\n\nThanks again to the reviewers for their comments. In order to better clarify our contribution, we have added the relevant comparison to contrastive laplacian eigenmaps in the revised version.\n\n>[1] Zhu H, Sun K, Koniusz P. Contrastive laplacian eigenmaps[J]. Advances in Neural Information Processing Systems, 2021, 34: 5682-5695.\n>\n> [2] Belkin M, Niyogi P. Laplacian eigenmaps and spectral techniques for embedding and clustering[J]. Advances in neural information processing systems, 2001, 14."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission224/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700368376931,
                "cdate": 1700368376931,
                "tmdate": 1700368376931,
                "mdate": 1700368376931,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Xk0vB4uinU",
                "forum": "RZBy8oHTz4",
                "replyto": "M39EwyeOLb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission224/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission224/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to your feedback"
                    },
                    "comment": {
                        "value": "Dear reviewer Ef5A,\n\nThanks again for your valuable time and insightful comments. As the deadline for the Author/Reviewer discussion is approaching, it would be nice of you to let us know whether our answers have solved your concerns so that we can better improve our work. We are looking forward to your feedback.\n\nBest regards,\n\nAuthors of Paper 224"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission224/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700729799216,
                "cdate": 1700729799216,
                "tmdate": 1700729799216,
                "mdate": 1700729799216,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]