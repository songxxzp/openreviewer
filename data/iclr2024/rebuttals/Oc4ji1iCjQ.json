[
    {
        "title": "Catch the Shadow: Automatic Shadow Variables Generation for Treatment Effect Estimation under Collider Bias"
    },
    {
        "review": {
            "id": "Gd20fhq53j",
            "forum": "Oc4ji1iCjQ",
            "replyto": "Oc4ji1iCjQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5512/Reviewer_Y4SN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5512/Reviewer_Y4SN"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new method to address collider bias in causal effect estimation. The main idea is to use representation learning to generate a shadow variable, in order to alleviate the difficulty in finding a well-defined shadow variable in real-world applcations. The proposed algorithm has three steps which are inspired by exisitng theoretical results on identification using shadow variables, in the first step it generate shadow variable by imposing conditional independence constraints in the learned representation, in the second step it use hypothesis test to ensure the generation is valid, and in the last step it utilize existing CATE estimator for causal effect estimation. Experiments on conducted on IHDP and Twins datasets, and the proposed algorithm is compared with algorithms that are designed to address confounding bias."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Motivation. The motivation of this work is very good. As most work (especially in AI/ML community) on causal effect estimation focus on confounder bias, it is nice to see a work that attempt to address collider bias.\n2. Writing. The structure and writing of this work is well-organized and clear."
                },
                "weaknesses": {
                    "value": "1. Insufficient evaluation. The selected baseline methods/datasets are not designed for collider bias evaluation. Please see questions for more details.\n2. Limited technical contribution. The proposed method mainly follows the theoretical results outlined by previous works, especially those by Miao and d\u2019Haultfoeuille. Although this is not a critical problem by itself, it is amplified by the fact that the designed algorithm is fragmented into three components. The first and second components (which forms the \"Shadow-Catcher\"), mandates multiple testing which is subsequently proposed to be addressed by p-value correction. Then the third component comes in and uses existing CATE estimators to obtain the final estimation. There is little to none discussions on the guarantee of the algorithm procedure when putting these three components together."
                },
                "questions": {
                    "value": "1. How is the CATE estimations evaluated on Twins dataset? The Twins dataset is usually used only for evaluating ATE estimations as there is no ground truth counterfactual outcomes.\n2. Have the authors compared with more recently proposed CATE/ATE estimators? For example TEDVAE [1], DR-CFR [2]. As the currently compared methods are mainly not designed for collider and are a bit outdated, comparing with more recent methods can further demonstrate if the proposed approach is effective. As we are now at ICLR 2024, it seems insufficient to have most of the baselines proposed in 2016/2017.\n3. Also is there any reason for selecting the datasets used in the manuscript? IHDP itself is a semi-synthetic dataset with only one data generation process, which only covers rather limited real-world scenarios. Twins is commonly used for ATE estimation instead of CATE estimation.\n\n[1] Treatment effect estimation with disentangled latent factors. AAAI 2021.\n\n[2] Learning Disentangled Representations for CounterFactual Regression. ICLR 2020."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5512/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5512/Reviewer_Y4SN",
                        "ICLR.cc/2024/Conference/Submission5512/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5512/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697519386307,
            "cdate": 1697519386307,
            "tmdate": 1700737115107,
            "mdate": 1700737115107,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4bszkUKM4W",
                "forum": "Oc4ji1iCjQ",
                "replyto": "Gd20fhq53j",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5512/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5512/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses by Authors (Part 1): Added experiments on the **Jobs** and **ACIC 2016** datasets"
                    },
                    "comment": {
                        "value": "We sincerely appreciate the reviewer\u2019s great efforts and insightful comments to improve our manuscript. In below, we address these concerns point by point and try our best to update the manuscript accordingly.\n\n> **[Q1] How is the CATE estimations evaluated on Twins dataset? The Twins dataset is usually used only for evaluating ATE estimations as there is no ground truth counterfactual outcomes.**\n\n**Response:** The reviewer raises an interesting concern. Following previous studies [1, 2], we use the Twins dataset to evaluate CATE estimation performance. Nonetheless, **we agree with the reviewer that \u201cthe Twins dataset is usually used only for evaluating ATE estimations as there is no ground truth counterfactual outcomes\u201d.**\n\nAs suggested by the reviewer, **we conducted experiments on two additional datasets with more comprehensive DGPs,** namely **Jobs** and **ACIC 2016**, and **removed CATE evaluation results on Twins dataset** in Table 2. Since the Jobs dataset contains no counterfactual outcomes, we follow [3, 4] to use the policy risk for evaluating the CATE estimation performance, which measures the expected loss if the treatment is taken according to the CATE estimation. The results are shown as below.\n\n| Method | ACIC within-sample | ACIC out-of-sample |Jobs within-sample | Jobs out-of-sample |\n|:---------| :---------: | :---------: | :---------: | :---------: |\n| Heckit | 3.106$\\pm$0.444 | 3.340$\\pm$0.111 | 0.328$\\pm$0.050 | 0.331$\\pm$0.052  |\n| DR | 2.346$\\pm$0.129 | 2.653$\\pm$0.222  | 0.316$\\pm$0.007 | 0.317$\\pm$0.036 |\n| IPSW    |  4.244$\\pm$0.145 | 5.411$\\pm$0.073 | 0.284$\\pm$0.051  | 0.289$\\pm$0.063 |\n| BNN   | 2.173$\\pm$0.150 | 2.586$\\pm$0.486 | 0.303$\\pm$0.025 | 0.304$\\pm$0.041|  \n| TARNet    | 2.275$\\pm$0.756 | 2.805$\\pm$0.766 | 0.315$\\pm$0.012 | 0.316$\\pm$0.050 |\n| CFR   | 2.107$\\pm$0.297 | 2.361$\\pm$0.587 | 0.313$\\pm$0.018 | 0.314$\\pm$0.072|\n| CForest     | 4.137$\\pm$0.295 | 4.605$\\pm$0.137  | 0.326$\\pm$0.012 | 0.326$\\pm$0.059|\n| DR-CFR     | 2.240$\\pm$0.691 | 2.340$\\pm$0.663 | 0.322$\\pm$0.022 | 0.323$\\pm$0.099|\n| TEDVAE     | 3.501$\\pm$0.708 | 4.468$\\pm$0.813  | 0.296$\\pm$0.046 | 0.300$\\pm$0.031  |\n| DeR-CFR| 2.214$\\pm$0.204| 2.246$\\pm$0.598 | 0.309$\\pm$0.023 | 0.311$\\pm$0.029|\n| DESCN   | 2.185$\\pm$0.150 | 2.306$\\pm$0.236 | 0.331$\\pm$0.010 | 0.331$\\pm$0.051 |\n| ES-CFR     | 3.875$\\pm$0.224 | 4.494$\\pm$0.214   | 0.290$\\pm$0.045   | 0.293$\\pm$0.046 |\n| Ours (New)    | **1.911$\\pm$0.126** | **2.047$\\pm$0.351** | **0.279$\\pm$0.017** | **0.280$\\pm$0.018**|  \n\nFrom the above results, we observe that: (1) The performance of all the estimators is not as good as that on other datasets like the IHDP dataset because the data generation process of the ACIC 2016 datasets is more complex and comprehensive. (2) DR-CFR, DESCN, and ES-CFR show the most competitive performance among all baseline methods. (3) The proposed method stably outperforms all baselines addressing confounding bias or sample selection bias, since the previous methods are limited to the cases in which $X$ and $T$ cause $S$, and cannot handle the cases with addition $Y$ cause $S$. Instead, our ShadowCatcher and ShadowEstimator can address collider bias for accurate CATE estimation. (4) The proposed method on the Jobs dataset shows the lowest policy risk, which demonstrates the effectiveness of our methods in real-world applications."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5512/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700595533811,
                "cdate": 1700595533811,
                "tmdate": 1700595533811,
                "mdate": 1700595533811,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GGvUsKhAm9",
                "forum": "Oc4ji1iCjQ",
                "replyto": "4bszkUKM4W",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5512/Reviewer_Y4SN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5512/Reviewer_Y4SN"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their effort in the rebuttal. These addressed most of my concerns. As there is not much time left in the author-reviewer discussion period, I will only ask two follow-up questions.\n\n1. When using the IHDP, ACIC semi-synthetic datasets, have the authors examined the different settings' data-generating process (DGP)? Do these DGPs actually induce collider bias? If so, roughly what proportion of these DGPs induce collider bias? If not, what is the reason behind the performance advantages gained by the proposed? These questions would shed more light on the performance gain. The DGPs of these datasets are readily available on Github.\n\n2. Regarding removing the Twins dataset results, can you give more details on how the evaluation was done in the original submission?"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5512/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700607653105,
                "cdate": 1700607653105,
                "tmdate": 1700607653105,
                "mdate": 1700607653105,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1KXIv2BZC7",
            "forum": "Oc4ji1iCjQ",
            "replyto": "Oc4ji1iCjQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5512/Reviewer_5YRg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5512/Reviewer_5YRg"
            ],
            "content": {
                "summary": {
                    "value": "The paper considers the problem of causal estimation with collider bias, that is, in situations where whether the individual is sampled or not (S=1/0) depends on the treatment T and the outcome Y. Previously, it has been shown that causal estimation can be done when there is collider bias if there exists a \u201cshadow variable\u201d (Z), which depends on the outcome variable (Y) among the sampled individuals (S=1), conditionally on the treatment (T) and covariates (X). Notably, Y can be missing for individuals that do not belong in the sample (S=0). Additionally, the shadow variable Z should be independent of the selection indicator S conditionally on all other variables. The main problem in using shadow variables is that they don\u2019t always exist. The innovation of the paper is to learn suitable shadow variables from the observed covariates (X)."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1) Most of the prior works addressing bias in causal estimation have focused on resolving the problem of hidden confounders and little work exists on collider bias. Therefore, the topic is fresh and important, and I found the approach based on shadow variables interesting.\n2) The method seems technically solid and clearly presented. Rationale of the different steps of the method are justified properly.\n3) The empirical validation seems appropriate, containing multiple reasonable baselines, two real-world datasets, and simulations."
                },
                "weaknesses": {
                    "value": "I did not identify any major weaknesses. Some smaller ones:\n1) The assumption that X and T are observed when the individual is not sampled seems quite strong and probably not true in many real-world use cases (see Question 1 below to address this).\n2) Overall, the the method appears a bit hacky (a combination of multiple steps), but all the steps are well-motivated.\n3) When printed out, fonts in Figures 2 and 3 are barely readable."
                },
                "questions": {
                    "value": "1) I guess quite often if the individual is not sampled, it\u2019s not only Y that is not observed but also X (and T), in which case the shadow variable approach would not be applicable. Could the authors discuss how this affects the usefulness in practice, and give representative real-world examples about situations in which they expect the method to be useful and where not?\n\n2) I don\u2019t understand the equation for the loss function in the \u201cHypothesis Test Phase\u201d paragraph. Could there be a typo?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5512/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698321792589,
            "cdate": 1698321792589,
            "tmdate": 1699636564385,
            "mdate": 1699636564385,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JpD9O5et4Z",
                "forum": "Oc4ji1iCjQ",
                "replyto": "1KXIv2BZC7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5512/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5512/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses by Authors: More discussions on real-world use cases and formula correction"
                    },
                    "comment": {
                        "value": "We sincerely appreciate the reviewer\u2019s great efforts and insightful comments to improve our manuscript. In below, we address these concerns point by point and try our best to update the manuscript accordingly.\n\n> **[W1 & Q1] If the individual is not sampled, it\u2019s not only Y that is not observed but also X (and T), in which case the shadow variable approach would not be applicable. How does this affect the usefulness in practice. Are there any representative real-world examples about situations in which they expect the method to be useful and where not?**\n\n**Response:** Thank you for the comment. We agree with the reviewer that there are also cases \"if the individual is not sampled, it's not only $Y$ that is not observed but also $X$ (and $T$), in which case the shadow variable approach would not be applicable\". **Instead, the proposed method (and most methods on sample selection bias) is applicable when the target population is determined, which also covers a wide range of real-world scenarios.**\n\nFor example, when studying the effect of vaccines ($T$) on disease prevention ($Y$), the target population is determined as, for example, the population of a country or region. Given the target population, governments or institutions would collect all information in terms of their covariates ($X$), and vaccine status ($T$). However, only a portion of the population will remain in follow-up ($S=1$), making the outcome ($Y$) able to be collected, while others will be lost ($S=0$), due to emigration, death, etc.\n\nAnother example is semi-supervised learning, in which one of the covariates is regarded as the treatment ($T$), and the rest of the covariates are denoted as $X$. Then the labeled samples are marked as $S=1$ with fully observed $X$, $T$, and $Y$, whereas the unlabeled samples are marked as $S=0$ with only $X$ and $T$ observed.\n\n> **[W2] Overall, the method appears a bit hacky (a combination of multiple steps), but all the steps are well-motivated.**\n\n**Response:** Thank you for the comment. We agree that the proposed method is a bit hacky but well-motivated. To make the entire framework of ShadowCatcher and ShadowEstimator more connected and clearer, **we briefly summarize the overall process** of using ShadowCatcher and ShadowEstimator:\n\n-\tShadowCatcher first takes the fully observed $X$ and $T$, and the observed $Y$ of $S=1$ units as inputs to generate shadow-variable representations $Z$, and then tests whether the generated $Z$ satisfies Assumption 1.\n-\tIf the generated $Z$ does not pass the hypothesis test, ShadowCatcher should re-generate new shadow-variable representations until the generated $Z$ finally passes the test.\n-\tFinally, ShadowEstimator uses the generated $Z$ to estimate treatment effects with observational samples.\n\n**We also add an overall flowchart as shown in Figure 4 on page 14.**\n\n> **[W3] When printed out, fonts in Figures 2 and 3 are barely readable.**\n\n**Response:** We thank the reviewer for pointing out this issue. **We have adjusted them in our revised version** on page 5 and Figure 3 on page 7 to make them clearer.\n\n> **[Q2] There is a typo in the equation for the loss function in the \u201cHypothesis Test Phase\u201d paragraph.**\n\n**Response:** We thank the reviewer for pointing out typos here. **The correct equation should be $L_q = 1/n * \\sum_{i=1}^n||(s_i / q(x_i, t_i, y_i) - 1) \\cdot (x_i, z_i, t_i)||_2^2$, where $(x_i, z_i, t_i)$ is a vector and $||.||_2$ is the L2-norm.** $L_q$ aims to learn a solution q of Q by Theorem 2 in Equation (6). Specifically, if $E[S / Q(X, T, Y) - 1 | X, Z, T] = 0$ (by Theorem 2 in Equation (6)), then $E[E[S / Q(X, T, Y) - 1 | X, Z, T] * (X, Z, T) ]=0$. The left hand side equals to $E[(S / Q(X, T, Y) - 1) * (X, Z, T)] = 0$. Then $L_q$ is just to minimize the square of the L2-norm of that Equation. We have corrected that on page 6, and added a more detailed explanation of $L_q$ in Appendix A.3.3 on page 18.\n\n***\n\n**We hope the above discussion will fully address your concerns about our work.** We look forward to your insightful and constructive responses to further help us improve the quality of our work. Thank you!"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5512/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700597732394,
                "cdate": 1700597732394,
                "tmdate": 1700597732394,
                "mdate": 1700597732394,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "N6oc9aEoIu",
            "forum": "Oc4ji1iCjQ",
            "replyto": "Oc4ji1iCjQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5512/Reviewer_G11c"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5512/Reviewer_G11c"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors focus on the collider bias problem, which is one of the important challenges of causal inference. They propose a novel method that can automatically generate shadow variable representations from observed covariates and propose an estimator to estimate CATE with the help of the generated representations. They conduct extensive experiments, including comparing different choices of the hyper-parameter $alpha$ and ablation studies. The main contribution is that they relax the strong assumptions of previous works on collider bias and make CATE estimation under collider bias feasible in most real-world observational studies."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. Collider bias is an important and easily ignored problem of causal inference in observational studies. The main difficulty for previous works to be applied in real-world scenarios is the strong assumptions they made. Therefore, if the assumptions are relaxed, the proposed method will make significant contributions to the causality community and will have high application value in real-world scenarios.\n2. This paper is mainly based on the shadow variables identification framework of collider bias, which strongly assumes that valid shadow variables are well-defined. Interestingly, the authors propose a novel idea that the shadow-variable representations can be learned from the observed covariates and propose a novel ShadowCatcher to \"catch the shadow\" from the covariates. The success of ShadowCatcher significantly relaxes the strong assumptions of previous works and makes CATE estimation possible under collider bias with the help of the proposed ShadowEstimator. This contributes a lot to causal inference research because, finally, collider bias can be addressed without any strong and even untestable assumptions as confounding bias in observational studies.\n3. The proposed method is clearly stated and reasonable to me. For the testable conditional dependence assumption of shadow variables, they directly constrain it in the representation learning phase, and for the untestable conditional independence assumption, they constrain it and do additional hypothesis tests to guarantee it. The entire learning process of ShadowCatcher is theoretically feasible. ShadowEstimator is based on the shadow variables estimation framework, whose correctness is also theoretically guaranteed. There is still one concern about ShadowCatcher, as I will state in the weaknesses part.\n4. The experiments are detailed and persuasive. The authors conduct experiments under different strengths of collider bias, proving the ability to reduce collider bias of their methods. They also conduct ablations to prove the effectiveness of the conditional dependence constraint in ShadowCatcher and compare the performance and efficiency under different choices of the hyper-parameter $alpha$. The results and analysis seem reasonable to me."
                },
                "weaknesses": {
                    "value": "1. Since the conditional independence assumption is not strictly constrained due to missing data, the authors use a hypothesis test phase to test whether this assumption is satisfied and only the generated representations pass the test can ShadowCatcher finish learning. As the authors also state in the paper, the choice of $alpha$ will significantly affect the efficiency of ShadowCatcher. As the results in Table 5 and Figure 4 show, the smaller $alpha$ is (which means the test is more strict), the better the performance is. But the number of iterations also gets bigger when the test is too strict. Therefore, the tradeoff between the efficiency and performance of the proposed method should be considered carefully in real-world applications.\n\n2. A minor concern is that the figure size is somewhat small."
                },
                "questions": {
                    "value": "See weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5512/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5512/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5512/Reviewer_G11c"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5512/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698646000029,
            "cdate": 1698646000029,
            "tmdate": 1699636564280,
            "mdate": 1699636564280,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kcnVMosWZM",
                "forum": "Oc4ji1iCjQ",
                "replyto": "N6oc9aEoIu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5512/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5512/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses by Authors: Experiments and discussions in terms of trade-off between the efficiency and performance"
                    },
                    "comment": {
                        "value": "We sincerely appreciate the reviewer\u2019s great efforts and insightful comments to improve our manuscript. In below, we address these concerns point by point and try our best to update the manuscript accordingly.\n\n> **[W1] The tradeoff between the efficiency and performance of the proposed method should be considered carefully in real-world applications.**\n\n**Response:** Thank you for your constructive suggestion. **We have conducted additional experiments to explore the impact of different choices of $\\alpha$ on the efficiency and performance**. The results are shown as below.\n\n| Reject threshold | $\\sqrt{\\mathrm{PEHE}}$ selected data |  $\\sqrt{\\mathrm{PEHE}}$ unselected data | mean number of iterations |\n|:---------| :---------: | :---------: | :---------: |\n|$1e-4$| 0.235$\\pm$0.003 | 0.235$\\pm$0.008 | **1.2** |\n| $5e-5$| 0.228$\\pm$0.003 | 0.231$\\pm$0.004 | 2 |\n| $1e-5$| 0.228$\\pm$0.002 | 0.231$\\pm$0.001 | 4.6 |\n| $5e-6$| 0.227$\\pm$0.002 | 0.230$\\pm$0.002 | 7.9 |\n| $1e-6$| **0.227$\\pm$0.001** | **0.229$\\pm$0.001** | 10 |\n| | | | | \n\nThe above results show that the performance of ShadowCatcher improves as the reject threshold decreases, because the hypothesis test gets more strict, and the constraint gets more reliable. However, the number of iterations required for ShadowCatcher to pass the hypothesis test also increases rapidly, reducing the efficiency of ShadowCatcher. Therefore, choosing an appropriate $\\alpha$ is a trade-off between efficiency and performance, which depends on the real-world application scenarios.\n\n> **[W2] The figure size is somewhat small.**\n\n**Response:** We thank the reviewer for pointing out this issue. **We have adjusted them in our revised version on page 5 and Figure 3 on page 7 to make them clearer.**\n\n***\n\n**We hope the above discussion will fully address your concerns about our work.** We look forward to your insightful and constructive responses to further help us improve the quality of our work. Thank you!"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5512/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700597591146,
                "cdate": 1700597591146,
                "tmdate": 1700597591146,
                "mdate": 1700597591146,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GDwxjVzlmq",
            "forum": "Oc4ji1iCjQ",
            "replyto": "Oc4ji1iCjQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5512/Reviewer_JJeK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5512/Reviewer_JJeK"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents an approach for learning representations that can take the role of \"shadow variables\", which allow for CATE identification in the presence of selection bias. They describe the process of learning these variables and empirically show with synthetic and semi-synthetic data that they produce more accurate CATE estimates."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- clean and clear contribution, seems like an inventive and well motivated approach for applying ML tools to improve causal inference with good grounding in causal literature\n- paper is mostly well presented\n- experiments seem to support the main idea of the paper and contrast with other causal methods for slightly different tasks, showing improvement"
                },
                "weaknesses": {
                    "value": "- I found myself getting just a little lost in the preliminaries of the shadow variable, particularly at the bottom of page 4. I found it confusing to say that f(Z | X, S) was identifiable from the observed data, and then to still say that we needed to find \\tilde(OR) - I thought the f(Z|...) functions eliminated the need to calculate \\tilde(OR) according to equation 4. I also don't quite see why Eq 5 is true and think this could use more explanation.\n- I found some of the loss functions through 3.3 and 3.4 a little unintuitive. 1) I found it a little odd to try to look at -Z to maximize MSE - this means that the function will behave differently for Z close to 0 since Z and -Z are near each other. Would it be reasonable or more sensible to take random Z instead? or why is -Z the best idea? 2) I'm not sure why h_r, a function that aims to predict Z well, will also help to move the Z0 and Z1 distributions towards each other. 3) what is the notation \\dot (x_i, z_i, t_i) in the loss function for Q? it's not clear if there's a typo here, since this just looks like a tuple. 4) I'm not sure why distilling h_z0 / h_z1 into a \\tilde(or) function is necessary\n- I'm confused by the comment right at the end of 3 around deconfounding methods: I thought there was an assumption around unconfoundedness. Is this still a fair comparison to other methods if deconfounding methods are used?\n- In the synthetic data in Table 1, I was surprised that some of the wins were not that big. Given that this is fully synthetic data presented for this method, I'd expect the results to be outside the confidence bands, but in a few spots they're highly overlapping - it makes me wonder if some of the practical choices aren't as effective\n- there's a lot of experimental info missing from section 4 around what learning algorithm and models are used\n\n\nSmaller comments:\n- end of Sec 3.3: it says \"the final generated Z that passes the test\" - should this be the first generated Z which passes the test? it seems like that test is a \"stopping criteria\" to me"
                },
                "questions": {
                    "value": "- What are the failure modes of this method - under what circumstances will learning a shadow variable be harder than others?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5512/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698702049831,
            "cdate": 1698702049831,
            "tmdate": 1699636564155,
            "mdate": 1699636564155,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qKQ4Iafh0k",
                "forum": "Oc4ji1iCjQ",
                "replyto": "GDwxjVzlmq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5512/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5512/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses by Authors (Part 1): Thanks for your insightful suggestion to take random $Z$ instead, which results in significant performance improvements"
                    },
                    "comment": {
                        "value": "We sincerely appreciate the reviewer\u2019s great efforts and insightful comments to improve our manuscript. In below, we address these concerns point by point and try our best to update the manuscript accordingly.\n\n> **[W1.1] Why do the $f(Z|...)$ functions not eliminate the need to calculate $\\widetilde{OR}$ according to Equation (4)?**\n\n**Response:** Thank you for the comment. The necessity for further calculating $\\widetilde{OR}(X, T, Y)$ given $f(Z | X, T, S)$ comes from that: for identification of $f(Y | X, T, S=0)$, we need to obtain **both $OR(X, T, Y)$ and $E[\\widetilde{OR}(X, T, Y) | X, Z, T, S=1]$** by Equation (2). The latter can be easily calculated by Equation (4) using $f(Z | X, T, S)$. However, **the former still needs $\\widetilde{OR}(X, T, Y)$ and $\\widetilde{OR}(X, T, Y=0)$** by Equation (5). Thus, it is still necessary to know what $\\widetilde{OR}(X, T, Y)$ as a function of $X, T, Y$ is to calculate $\\widetilde{OR}(X, T, Y=0)$, and we need to **further solve for $\\widetilde{OR}(X, T, Y)$ as a function of $X, T, Y$ from the calculated $E[\\widetilde{OR}(X, T, Y) | X, Z, T, S=1]$** by Equation (4).\n\n> **[W1.2] Why is Equation (5) true?**\n\n**Response:** By Equation (2), $OR(X, T, Y=0) = 1$ (just substitute $Y=0$ into Equation (2)). Therefore, by the definition of $\\widetilde{OR}(X, T, Y)$ that $\\widetilde {OR}(X, T, Y) = OR(X, T, Y) / E[OR(X, T, Y) | X, T, S=1]$, **the right-hand side of Equation (5) equals to $OR(X, T, Y) / OR(X, T, Y=0)$**. Since $OR(X, T, Y=0) = 1$, we have **$OR(X, T, Y) / OR(X, T, Y=0)$ equals the left-hand side of Equation (5) (i.e., $OR(X, T, Y)$)**. To make this clearer, **we make a detailed explanation of Equation (5) in Appendix A.3.2 on page 18**.\n\n> **[W2.1] Would it be reasonable or more sensible to take random $Z$ as the implementation of $Z^-$ instead?**\n\n**Response:** Thank you for such insightful advice. **It is absolutely reasonable and we have incorporated this suggestion throughout our paper.** In fact, the original description of $Z^-$ in our article that \"the opposite value of $Z$\" is not accurate since we misuse the word \"opposite.\" **We have corrected it in our revised version (on Page 6) that $Z^-$ indeed denotes a value that differs from $Z$ to constrain the conditional dependence assumption**, which any strategy can implement. We choose $-Z$ as $Z^-$ for continuous $Z$s just because this is easy to implement. We do agree that **taking a random Z instead is a better idea** since when $Z$ is close to 0, $-Z$ does not satisfy the requirement of $Z^-$. **We use this new implementation to re-run all experiments and achieve obvious performance improvements**. The results are shown as below.\n\n| Method | $\\beta=1$ selected |  $\\beta=1$ unselected |  $\\beta=3$ selected | $\\beta=3$ unselected | $\\beta=5$ selected | $\\beta=5$ unselected |\n|:---------| :---------: | :---------: | :---------: | :---------: | :---------: | :---------: |\n| Ours (Old) | 0.241$\\pm$0.014 | 0.248$\\pm$0.009 | 0.305$\\pm$0.013 | 0.326$\\pm$0.015 | 0.333$\\pm$0.040 | 0.404$\\pm$0.053 |\n| Ours (New) | **0.227$\\pm$0.001** | **0.229$\\pm$0.001** | **0.249$\\pm$0.013** | **0.255$\\pm$0.021** | **0.299$\\pm$0.008** | **0.300$\\pm$0.008** |\n||||||||\n\n| Method | IHDP within-sample |  IHDP out-of-sample |  ACIC within-sample | ACIC out-of-sample | Jobs within-sample | Jobs out-of-sample |\n|:---------| :---------: | :---------: | :---------: | :---------: | :---------: | :---------: |\n| Ours (Old) | 1.039$\\pm$0.069 | 1.065$\\pm$0.099 | 2.078$\\pm$0.333 | 2.142$\\pm$0.390 | 0.283$\\pm$0.018 | 0.284$\\pm$0.080 |\n| Ours (New) | **0.703$\\pm$0.106** | **0.723$\\pm$0.102** | **1.911$\\pm$0.126** | **2.047$\\pm$0.351** | **0.279$\\pm$0.017** | **0.280$\\pm$0.018** |\n||||||||\n\nThe original implementation of our method is denoted as Ours (Old), whereas the new implementation of our method is denoted as Ours (New). Note that we add an additional real-world dataset - Jobs to better evaluate the performance, where the evaluation metric is the policy risk $\\hat{R}_{\\mathrm{Pol}}=1-(\\mathbb{E}[Y(1)\\mid \\tau(\\mathbf{x}) > 0,T=1] \\cdot \\mathbb{P}(\\tau(\\mathbf{x}) > 0) + \\mathbb{E}[Y(0)\\mid \\tau(\\mathbf{x}) \\leq 0,T=0]\\cdot \\mathbb{P}(\\tau(\\mathbf{x}) \\leq 0)$ instead of $\\sqrt{\\mathrm{PEHE}}$ for other datasets. **The new implementation (taking a random $Z$ instead of $-Z$ as $Z^{-}$) shows obvious performance improvements on all datasets.** We really appreciate your great advice!\n\n> **[W2.2] Why will $h_r$, a function that aims to predict Z well, also help to move the $Z_0$ and $Z_1$ distributions towards each other?**\n\n**Response:** The reviewer raises an interesting concern. Please kindly note that $h_r$ itself is not the key to moving the $Z_0$ and $Z_1$ distributions towards each other. Instead, **as $h_r$ is the estimate of $f(Z | X, T, Y, S=1)$, if $h_r$ can also predict $Z$ well for $S=0$ data, then $f(Z | X, T, Y, S=1) = f(Z | X, T, Y, S=0)$** and the assumption that $Z$ is independent of $S$ given $X, T, Y$ is satisfied."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5512/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700596376215,
                "cdate": 1700596376215,
                "tmdate": 1700597374595,
                "mdate": 1700597374595,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oGSRtm79WL",
                "forum": "Oc4ji1iCjQ",
                "replyto": "GDwxjVzlmq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5512/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5512/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses by Authors (Part 2): Notation correction and missing experimental details"
                    },
                    "comment": {
                        "value": "> **[W2.3] What is the notation $\\cdot (x_i, z_i, t_i)$ in the loss function for $Q$? it's not clear if there's a typo here.**\n\n**Response:** We thank the reviewer for pointing out typos here. **The correct equation should be $L_q = 1/n * \\sum_{i=1}^n||(s_i / q(x_i, t_i, y_i) - 1) \\cdot (x_i, z_i, t_i)||_2^2$, where $(x_i, z_i, t_i)$ is a vector and $||.||_2$ is the L2-norm.** $L_q$ aims to learn a solution q of Q by Theorem 2 in Equation (6). Specifically, if $E[S / Q(X, T, Y) - 1 | X, Z, T] = 0$ (by Theorem 2 in Equation (6)), then $E[E[S / Q(X, T, Y) - 1 | X, Z, T] * (X, Z, T) ]=0$. The left hand side equals to $E[(S / Q(X, T, Y) - 1) * (X, Z, T)] = 0$. Then $L_q$ is just to minimize the square of the L2-norm of that Equation. We have corrected that on page 6, and added a more detailed explanation of $L_q$ in Appendix A.3.3 on page 18.\n\n> **[W2.4] why is distilling $h_{z_0} / h_{z_1}$ into a $\\widetilde{or}$ function is necessary?**\n\n**Response:** Thank you for the comment, but we believe this question comes from [W1.1] in which Proposition 1 and Equation (5) confuse you. Please kindly refer to our **response to [W1.1]**, it is necessary because we need to **further solve for $\\widetilde{OR}(X, T, Y)$ as a function of $X, T, Y$ from the calculated $E[\\widetilde{OR}(X, T, Y) | X, Z, T, S=1]$ by Equation (4)**, so that we can obtain $OR(X, T, Y)$ from $\\widetilde{OR}(X, T, Y)$ by Equation (5) and further identify $f(Y | X, Z, T, S=0)$ by Equation (2).\n\n> **[W3] I thought there was an assumption around unconfoundedness. Is this still a fair comparison to other methods if de-confounding methods are used?**\n\n**Response:** The reviewer raises an interesting concern. However, the de-confounding way is just an implementation of the outcome estimators in ShadowCatcher and ShadowEstimator, which **will not affect (either positive or negative) the ability to address collider bias**. As you may noticed, we do need the unconfoundedness assumption on the target population (but not on only $S=1$ data because this assumption can be violated due to collider bias). This means we assume that there are no unobserved confounders, but there can possibly be confounding bias caused by fully observed covariates. The de-confounding method (an IPM to balance treated and controlled representations following [1]) we use is just to **avoid the impact of such confounding bias on CATE estimation, thus we suppose the evaluation is fair to see each method's ability to address collider bias**.\n\n> **[W4] In the synthetic data in Table 1, some of the wins were not that big.**\n\n**Response:** We agree with you and **have adopted your kind suggestion in [W2.1]**. We think it mainly results from [W2.1] that the implementation of ShadowCatcher could be improved. We have adopted your suggestion to **use a random $Z$ instead of $-Z$ as the implementation of $Z^-$**. We re-run all experiments and achieve **obvious performance improvements.** Please kindly find the detailed results in the **response to [W2.1]**.\n\n> **[W5] There's a lot of experimental info missing from section 4 around what learning algorithm and models are used.**\n\n**Response:** We thank the reviewer for pointing out this issue. In the following, we **report the missing implementation details**, including what learning algorithms and models are used, as well as the choice of hyperparameters on different datasets in our revised version. \n\nSpecifically, we adopt 3-layer neural networks to implement each module in ShadowEstimator and ShadowCatcher. We use the Adam optimizer with batch normalization in the training process, and we use the Wasserstein distance as the Integral Probability Metric (IPM) to implement all the methods that need IPM to balance representations. We implement all the methods in the PyTorch environment with Python 3.9. The CPU we use is 13th Gen Intel(R) Core(TM) i7-13700K, and the GPU we use is NVIDIA GeForce RTX 3080 with CUDA 12.1. **For the ease of reproducibility, the hyper-parameters of our methods on different datasets are detailed in the table below.**\n\n| Dataset | epochs | batch size | learning rate | weight decay | IPM weight | reject threshold |\n|:---------| :---------: | :---------: | :---------: | :---------: | :---------: | :---------: |\n|Synthetic datasets| 100 | 1024 | 0.03 | 0.01 | 0.001 | 1e-6 |\n| The IHDP dataset | 100 | 128 | 0.03 | 0.01 | 0.001 | 0.01 |\n| The Twins dataset| 100 | 1024 | 0.03 | 0.01 | 0.1 | 0.1 |\n| The Jobs dataset | 100 | 256 | 0.003 | 0.001 | 0.1 | 0.1 |\n| The ACIC 2016 datasets| 100 | 256 | 0.01 | 0.001 | 0.001 | 100 |\n| | | | | | | |"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5512/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700597079237,
                "cdate": 1700597079237,
                "tmdate": 1700597148989,
                "mdate": 1700597148989,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3gs4bGXtWU",
                "forum": "Oc4ji1iCjQ",
                "replyto": "57OGi2NNKD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5512/Reviewer_JJeK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5512/Reviewer_JJeK"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thanks for the rebuttal - I found the clarifications helpful and I'm glad to see the suggestion improved your results! At the moment I will keep my score as is but will consider changing it in conversation with other reviewers."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5512/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700663540637,
                "cdate": 1700663540637,
                "tmdate": 1700663540637,
                "mdate": 1700663540637,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]