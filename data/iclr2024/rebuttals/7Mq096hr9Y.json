[
    {
        "title": "OpenMixup: A Comprehensive Mixup Benchmark for Visual Classification"
    },
    {
        "review": {
            "id": "VBJeczykoe",
            "forum": "7Mq096hr9Y",
            "replyto": "7Mq096hr9Y",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission157/Reviewer_nCz4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission157/Reviewer_nCz4"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a comprehensive benchmark for image classification methods based on *mixup*. The benchmark is constructed from a unified codebase and offers a detailed overview of the codebase itself, the implemented methods, available datasets, and the evaluation metrics available. Moreover, the paper includes an empirical analysis that compares the performance (accuracy) of various models across diverse datasets, as well as a comparison of the trade-off between performance, total training time and memory usage."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- **Comprehensive Analysis with practical relevance**: The paper presents an extensive examination of various mixup methods for image classification. It assesses these methods across a wide range of classification datasets and employs multiple architectures. This comprehensive approach ensures that the study's findings are robust and applicable to various real-world scenarios. By considering the trade-off between performance, training time, and GPU memory usage, it provides valuable insights for implementing mixup techniques. \n- **Open-Source Codebase**: Open-source code promotes transparency and reproducibility in research. This contributes to the credibility and reliability of further study on mixup methods."
                },
                "weaknesses": {
                    "value": "- **Lack of clear Take-Home Message**: The paper is a tech report that describes the functionalities and the different components in the library. While I do not dismiss the massive work put behind building the library, the paper is missing a clear take-home message from the \"interesting observations\" claimed by the authors. There is no substantial findings or actionable insights beyond the existence of the associated codebase.\n- **Misalignment of Evaluation Metrics**: The paper introduces several evaluation metrics which are not applied in the experiments, such as calibration or robustness. The primary focus remains on accuracy, which as already been extensively studied in the papers associated with each individual method. This limits the paper's potential to offer a fresh perspective on the methods by not exploring alternative criteria beyond accuracy, especially as other evaluation metrics are already described as available in the benchmark. \n\n### Remarks:\n\n- The color gradient in the plots make it difficult to really differentiate two methods.\n- The double-blind nature of the review is broken by keeping the name of the contributors and authors of the associated arxiv paper in the code provided as supplementary material ..."
                },
                "questions": {
                    "value": "- What would be the main take-home message from the empirical study presented ? Beyond the existence of the open-source library, what should we remember from this paper ?\n- I really think that the paper would be improved with a comparison of the methods on other evaluation metrics than accuracy, it might provide a different perspective on the implemented methods. Though I totally understand that this would require a lot of new experiments."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission157/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698657873439,
            "cdate": 1698657873439,
            "tmdate": 1699635941327,
            "mdate": 1699635941327,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aY2JwlfLfW",
                "forum": "7Mq096hr9Y",
                "replyto": "VBJeczykoe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission157/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission157/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer nCz4"
                    },
                    "comment": {
                        "value": "We would like to extend our gratitude for raising the constructive points to help us improve the manuscript. Based on your questions and suggestions, we have presented our revision and general response. All revised information in our revision is highlighted for your convenience. \n\nWe would like to address your concerns and the points you raised for improvement:\n\n### **1. On the Clear Take-Home Message.**\n\nThank you for your valuable comments. Based on your advice, we have summarized and analyzed a great number of benchmarking results to compare and rank all the included mixup methods in terms of **performance, applicability,** and **overall capability**. \n\nSpecifically, regarding performance, we averaged the accuracy rankings of all mixup algorithms for each downstream task, as well as averaging their robustness and calibration (supplementary experiments) rankings separately. Finally, these ranking averages were averaged again to produce the comprehensive range of performance ranking results. As for speed, we adopt a similar ranking computation scheme. With the overall capability ranking, we combined the performance and applicability rankings with a 1:1 weighting to obtain the final take-home rankings.\n\nFinally, we provide the **take-home rankings** as shown in the following tables. Again, we appreciate your constructive suggestion.\n\n\n|               | Mixup | CutMix | DeiT | SmoothMix | GridMix | ResizeMix | ManifoldMix | FMix | AttentiveMix | SaliencyMix | PuzzleMix | AlignMix | AutoMix | SAMix | TransMix | SMMix |\n|---------------|:-----:|:------:|:----:|:---------:|:-------:|:---------:|:-----------:|:----:|:------------:|:-----------:|:---------:|:--------:|:-------:|:-----:|:--------:|:-----:|\n| Performance   |   13  |   11   |   5  |     16    |    15   |     8     |      12     |  14  |       7      |      9      |     6     |    10    |    2    |   1   |     4    |   3   |\n| Applicability |   1   |    1   |   1  |     1     |    1    |     1     |      1      |   1  |       3      |      1      |     4     |     2    |    7    |   6   |     5    |   5   |\n| Overall       |   8   |    6   |   1  |     11    |    10   |     4     |      7      |   9  |       5      |      5      |     5     |     6    |    4    |   2   |     4    |   3   |\n\n\n### **2. Misalignment of Evaluation Metrics.**\n\nThank you for raising the issue we had with evaluation metrics. Based on your suggestion, we have revised the article where the presentation was not rigorous. Meanwhile, for robustness and calibration evaluation, we have added the related experiments, as shown in the table below. \n\nThe results show that although these new evaluation metrics provide a variety of interpretable perspectives, the conclusions they provide are still consistent with those of accuracy. This is also consistent with what we know about mixup as an augmentation method. An augmentation method is good as long as it is effective, fast, and can be applied to multiple scenarios concurrently. Furthermore, we have provided comprehensive radar plots in the paper, which clearly reflect the performance of each algorithm in different scenarios. We suppose this is a comprehensive evaluation of existing mixup algorithms. Nonetheless, we have added the corresponding experiments you mentioned and have come up with consensus results.\n\n### **3. The Color Gradient in the Plots.**\n\nWe appreciate that you pointed out the lack of differentiation of color gradients. Since we cover a lot of algorithms, selecting various color gradients can be more or less difficult to differentiate. We have tried to make changes in our revision to make the color gradients in plots more distinguishable. Again, thank you for your valuable suggestions.\n\n### **4. Comparison with Other Evaluation Metrics.**\n\nAs mentioned in the second response, based on your suggestion, we have added the related robustness and calibration evaluation experiments, as shown in Table A8 in the revised version. The results show that although these new evaluation metrics provide a variety of interpretable perspectives, the conclusions they provide are still consistent with those of accuracy. This is also consistent with what we know about mixup as an augmentation method. An augmentation method is good as long as it is effective, fast, and can be applied to multiple scenarios concurrently. Furthermore, we have provided comprehensive radar plots in the paper, which clearly reflect the performance of each algorithm in different scenarios. We suppose this is a comprehensive evaluation of existing mixup algorithms. However, we have added the corresponding experiments you mentioned and have come up with consensus results.\n\nOverall, we sincerely appreciate all your time and valuable feedback. We hope that you will reconsider our submission with the revision and response. If you are satisfied with our response, please consider updating your score. If you need any clarification, please feel free to contact us."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission157/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668925746,
                "cdate": 1700668925746,
                "tmdate": 1700669255884,
                "mdate": 1700669255884,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "teo2CLtuvt",
            "forum": "7Mq096hr9Y",
            "replyto": "7Mq096hr9Y",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission157/Reviewer_x5Ek"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission157/Reviewer_x5Ek"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents the mixup visual classification benchmark, called OpenMixup, which also offers a unified modular mixup-based model design and training codebase framework for visual representation learning. Extensive experiments on both static and dynamic mixup baselines are conducted across various visual classification datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well-organized and easy to follow.\n2. The experimental assessment is comprehensive."
                },
                "weaknesses": {
                    "value": "1. This article is a summary of the previous Mixup-based data augmentation approaches and does not propose new tasks and methods.\u00a0\n\n2. Lack of important references: \"Zhao Q, Huang Y, Hu W, et al. MixPro: Data Augmentation with MaskMix and Progressive Attention Labeling for Vision Transformer[C]//The Eleventh International Conference on Learning Representations. 2022.\"\n\n3. Lack of latest ViT's backbone results for benchmarks.\nSuch as FastViT: \"Vasu P K A, Gabriel J, Zhu J, et al. FastViT: A Fast Hybrid Vision Transformer using Structural Reparameterization[J]. //Proceedings of the IEEE/CVF International Conference on Computer Vision (2023)\""
                },
                "questions": {
                    "value": "None."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission157/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698773782775,
            "cdate": 1698773782775,
            "tmdate": 1699635941254,
            "mdate": 1699635941254,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1IAfXHUMQL",
                "forum": "7Mq096hr9Y",
                "replyto": "teo2CLtuvt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission157/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission157/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer x5Ek"
                    },
                    "comment": {
                        "value": "We sincerely appreciate all the time you took to thoroughly review our work and your valuable feedback. We are delighted about your assessment of our paper as a well-organized and comprehensive assessment. \n\nAgain, we would like to express our appreciation to you for pointing out the issue of academic advancements (new tasks and methods). Regarding the points you raised for improvement:\n\n### **1. No New Tasks and Methods.**\n\nFirst, we have elaborated on\u00a0**why we need a comprehensive mixup benchmark.**\u00a0and\u00a0**why we need a standardized open-source platform.**\u00a0in the abstract and Sec.1. Briefly, mixup is a widely-used data augmentation method in modern Convolutional Neural Networks (CNNs) and Visual Transformers (ViTs). To the best of our knowledge, however, as a fundamental building block of today's computer vision architecture, there has been no such comprehensive benchmarking study (which is both\u00a0**time-consuming**\u00a0and\u00a0**computationally resource-intensive**) for the mixup community to drive progress in a standard manner. As researchers, we believe that we can all agree that an area will\u00a0**struggle to advance without a comprehensive benchmark**. This is the first reason that we focus on mixup, especially for building a comprehensive mixup visual classification benchmark. **It is quite urgent work for the community**. **This area needs a benchmark.** In this context, our work is indeed **the\u00a0first**\u00a0mixup visual classification benchmark. Even though we do not present new algorithms or define new tasks, we are still highly meaningful to the community as the first benchmarking study and a unified codebase platform. \n\nSecond, we have to emphasize that most current mixup methods are crafted with diverse angles of enhancement, applying distinct configurations and even different coding styles\u00a0**without an all-in-one open-source coding platform**\u00a0for streamlined pre-processing, method design, training, and evaluation. The lack of standardization\u00a0**poses obstacles to fair comparison and effortless development**, necessitating\u00a0**costly trial-and-error for researchers**. This is the second reason that we focus on mixup augmentations, especially for providing a unified model design and training platform.\n\nIn summary, we conclude our main contributions and values as follows:\n\n**(a)**\u00a0Mixup is an augmentation method that is widely employed nowadays. However, there is no comprehensive benchmark for mixup algorithms so far. This will lead to the **absence of a standardized criterion** for numerous mixup classification methods, and researchers have no idea how to choose a proper method. Personally, there are many researchers who reached out to me and stated that the existing mixup classification algorithms are **varied and full of pitfalls**. \n\nTo address this issue, OpenMixup is presented as **the\u00a0first**\u00a0comprehensive mixup visual classification benchmark. It first enables researchers to\u00a0**fairly compare**\u00a0the performance of their models against a broad set of mixup baselines, providing a\u00a0**clear and objective measurement**\u00a0of the method's effectiveness. Such a first-of-its benchmarking study is both\u00a0**time-consuming**\u00a0and\u00a0**resource-intensive**, but we still carried it out\u00a0**from scratch**. \n\nI suppose this makes our work truly meaningful and full of value, even though the scope is objectively limited.\n\n**(b)**\u00a0Apart from the benchmarking study, we provide\u00a0**a standardized model design and training codebase platform**\u00a0for mixup-based visual classification. The overall framework is streamlined and modular, which can enhance the\u00a0**accessibility and customizability of downstream deployment and applications**.\n\n**(c)\u00a0Interesting observations, insights, and take-home conclusive rankings**\u00a0on comprehensive performance, robustness, training stability, and convergence are obtained (**In Sec.4**) with diverse analysis toolkits (*e.g.*\u00a0**accuracy radar plots**,\u00a0**trade-off plots**, loss landscape, CAM visualization,\u00a0*etc.*) from our OpenMixup platform, enabling researchers to identify what specific properties contribute to the success of different methods. In the revision, we aim to provide take-home ranking results that offer a clear message, helping researchers select the proper mixup methods. Archers to identify what specific properties contribute to the success of different methods.\n\n### **2. The Important Reference.**\n\nThanks for pointing out the missing reference. we have added this important work in the revision. Thanks again.\n\n### **3. The FastViT Backbone Results.**\n\nWe appreciate you pointing out our missing backbone. Due to the limited time and computational resources we have in the rebuttal period, we would add this work to the reference first and conduct related experiments on this work in our subsequent revisions. Thanks again.\n\nIn conclusion, we sincerely appreciate all your valuable feedback to help us improve our work. We hope that you will reconsider our submission with the revised version."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission157/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668581245,
                "cdate": 1700668581245,
                "tmdate": 1700669428487,
                "mdate": 1700669428487,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1QJTxHNdiA",
            "forum": "7Mq096hr9Y",
            "replyto": "7Mq096hr9Y",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission157/Reviewer_kQcu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission157/Reviewer_kQcu"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors proposed OpenMixup, a mixup based benchmark for visual classification. Multiple algorithms are implemented in a unified framework and evaluated on a wide spectrum of classification datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. It is very helpful to have a unified and comprehensive benchmark for mixup algorithms given that it has been widely used in model training.\n2. Extensive implementation and evaluation are conducted for mixup based models. The authors also provide good visualizations and analysis on the results.\n3. Writing is good and easy to follow."
                },
                "weaknesses": {
                    "value": "1. The main contribution seems to be the implementation of mixup based models. I am not sure ICLR is a good venue for this kind of work.\n2. Detection and Segmentation are currently implemented as Transfer Learning from classification trained backbones which is quite limited as there are also mixup models for detection."
                },
                "questions": {
                    "value": "1. How this engineering focused and narrowed application benchmark benefit the wide community of ICLR?\n2. How do you extend the current framework to add the mixup algorithms for object detection that are not based on transfer learning?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission157/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698820273872,
            "cdate": 1698820273872,
            "tmdate": 1699635941160,
            "mdate": 1699635941160,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NFxTjlqFde",
                "forum": "7Mq096hr9Y",
                "replyto": "1QJTxHNdiA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission157/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission157/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer kQcu"
                    },
                    "comment": {
                        "value": "We first express our gratitude for your time and effort in reviewing our submission. We have made numerous adjustments to our submission and provide a comprehensive general response. We invite you to first go through our general responses because they may answer most of your questions.\n\nAgain, we would like to express our appreciation to you for pointing out the issue of the scope and the value to the community. Here, we will address your concern from the following two aspects:\n\n### **1. On the Value and Contributions of OpenMixup.**\n\nWe appreciate your valuable comments. We present a comprehensive overview of our contributions and novelties to the community.\n\n**(1)**\u00a0Mixup is an augmentation method that is widely employed nowadays. However, there is no comprehensive benchmark for mixup so far. This will lead to the **absence of a standardized criterion** for numerous mixup classification methods, and researchers have no idea how to choose a proper method. Personally, there are many researchers who reached out to me and stated that the existing mixup classification algorithms are **varied and full of pitfalls**. As researchers, we believe that we can all agree that an area will\u00a0**struggle to advance without a comprehensive benchmark**. It is quite urgent for the community. This area needs a benchmark.\n\nTo address this issue, OpenMixup is presented as **the\u00a0first**\u00a0comprehensive mixup visual classification benchmark. It first enables researchers to\u00a0**fairly compare**\u00a0the performance of their models against a broad set of mixup baselines, providing a\u00a0**clear and objective measurement**\u00a0of the method's effectiveness. Such a first-of-its benchmarking study is both\u00a0**time-consuming**\u00a0and\u00a0**resource-intensive**, but we still carried it out\u00a0**from scratch**. \n\nI suppose this makes our work truly meaningful and full of value, even though the scope is objectively limited.\n\n**(2)**\u00a0Apart from the benchmarking study, we provide\u00a0**a standardized model design and training codebase platform**\u00a0for mixup-based visual classification. The overall framework is streamlined and modular, which can enhance the\u00a0**accessibility and customizability of downstream deployment and applications**.\n\n**(3)**\u00a0**Interesting observations and insights**\u00a0on comprehensive performance, robustness, training stability, and convergence are obtained (**In Sec.4**) with diverse analysis toolkits (*e.g.*\u00a0**accuracy radar plots**,\u00a0**trade-off plots**, loss landscape, CAM visualization,\u00a0*etc.*) from our OpenMixup platform, enabling researchers to identify what specific properties contribute to the success of different methods. In the revision, we aim to provide take-home ranking results that offer a clear message, helping researchers select the proper mixup methods.\n\n### **2. Without Transfer Learning Detection and Segmentation Tasks.**\n\nWe appreciate your valuable comment. First, as emphasized in our revision (**in Sec.1**), although mixup exerts its power in other downstream tasks like object detection, it is derived from its supervised classification counterpart with **shared attributes**. Meanwhile, mixup augmentations utilized in segmentation tasks need to be analyzed and specially designed on a case-by-case basis since most of the mixup methods for classification tasks cannot be directly used in segmentation tasks. This is less relevant to the study of the properties of the mixup algorithm itself. We suppose that the study of mixup classification is sufficient to provide a comprehensive understanding of mixup. \n\nSecond, while our main focus is indeed on supervised visual classification tasks with mixup augmentations, the platform we provided, OpenMixup, can be extended to other tasks, as you mentioned, such as object detection and semantic segmentation. We have provided\u00a0**Transfer Learning to Semantic Segmentation**\u00a0and more detailed benchmarking results in the revised supplementary materials to better evaluate the mixup methods. We believe that the unified streamlined platform we provided could be beneficial for these downstream tasks, and we intend to extend our work in these directions in the future.\n\nIn conclusion, we appreciate all your constructive feedback and the opportunity given to improve our OpenMixup. We believe that addressing these points has significantly enhanced our paper and its contributions to the mixup community. We hope that you will reconsider our submission with the revision and response. If you are satisfied with our response and effort, please consider updating your score. If you need any clarification, please feel free to contact us."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission157/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668384310,
                "cdate": 1700668384310,
                "tmdate": 1700668384310,
                "mdate": 1700668384310,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jSe1ZCKqFN",
                "forum": "7Mq096hr9Y",
                "replyto": "NFxTjlqFde",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission157/Reviewer_kQcu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission157/Reviewer_kQcu"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the feedback. I am not convinced that \"time-consuming\", \"resource-intensive\", and \"from scratch\" would be good evidence for value to ICLR. I think there is value of this work but as mentioned in my initial comments \"I am not sure ICLR is a good venue for this kind of work.\" I am sticking to my initial rating."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission157/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670022951,
                "cdate": 1700670022951,
                "tmdate": 1700670022951,
                "mdate": 1700670022951,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dFQ25Haqbh",
            "forum": "7Mq096hr9Y",
            "replyto": "7Mq096hr9Y",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission157/Reviewer_rFDa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission157/Reviewer_rFDa"
            ],
            "content": {
                "summary": {
                    "value": "The authors present the first benchmark framework which employ various published mixup methods for visual classification tasks. Not only on classification dataset, they also publish the downstream task baselines which mixup researches usually evaluate their methods as a strong and effective regularizer. To evaluate mixup researches on various tasks, they provide visualization tools, various dataset module, and various models which are widely used on research."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tThis work is well-motivated. Since the programs used in the studies of the mixup methodologies differed, it was difficult to compare them to each other. This benchmark framework is useful to the mixup research community and practical field.\n2.\tThe diversity of images and learning models generated by the proposed method, as well as the diversity of downstream tasks, can provide a complete picture of the performance of the training intervals and, in particular, reveal weaknesses."
                },
                "weaknesses": {
                    "value": "1.\tFor the installation of OpenMixup, detailed settings are required, such as the version that the authors tested. Because the framework requires pytorch-based frameworks \u2018mmcv\u2019, it requires more specifics like author-tested versions of pytorch, mmcv, and mmclassification. Including other tasks, such as detection and segmentation, the description of specific versions would make it more accessible to other researchers.\n2.\tExtensibility to add new mixup method. As the authors mentioned, we hope this benchmark can be extended to other mixup methods and scenarios not only in research fields but also in practical applications. To this end, instructions to add the mixup method could be included in the document. Recent mixup methods include additional modules (e.g. AutoMix, SAMix, RecursiveMix, SMMix,. Etc); therefore, the guideline for the methodologies that require additional modules would be better to be included.\n3.\tTo measure the robustness metric using OpenMixup, the performances on the corrupted dataset (Cifar-100-C or ImageNet-C) could be included."
                },
                "questions": {
                    "value": "1.\tIn the latest version, we cannot find TransMix\u2019s implementation of Swin-tiny, while the authors describe it as the supported method in OpenMixup. Then, how can the authors get TransMix score of Deit-T on ImageNet-1k in Table A3, A7? (including SMMix)\n2.\tWhy does CutMix's hyper-parameter alpha vary from experiment to experiment, and why doesn't it utilize the officially utilized value of 1? Is there a reason for writing it differently for different methods?\n3.\tAs far as I know, the official code of OpenMixup provides training segmentation tasks using pre-trained backbone model. Why do not the authors include the results?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission157/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission157/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission157/Reviewer_rFDa"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission157/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698830863896,
            "cdate": 1698830863896,
            "tmdate": 1699635941057,
            "mdate": 1699635941057,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]