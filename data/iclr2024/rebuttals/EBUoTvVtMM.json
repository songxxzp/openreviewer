[
    {
        "title": "User Inference Attacks on Large Language Models"
    },
    {
        "review": {
            "id": "4CxSQAPT7A",
            "forum": "EBUoTvVtMM",
            "replyto": "EBUoTvVtMM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6787/Reviewer_UDKj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6787/Reviewer_UDKj"
            ],
            "content": {
                "summary": {
                    "value": "This paper  investigates **user inference** attacks against LLMs fine-tuned on user-stratified data. The goal is to predict if a particular user's data is used in the model's fine-tuning process. The adversary has access to a subset of a user's data and uses a simple attack that applies a threshold on the average loss of a user\u2019s documents.  The authors also design canaries to study the influence of shared substrings. The authors also explore factors that affect the attack's success rate as well as mitigation strategies."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.This paper first defines user-level inference attack against fine-tuned LLMs. In the threat model, the adversary\u2019s knowledge could be different from the user\u2019s training samples. This makes the threat model more realistic.\n\n2.The authors demonstrate the attack's efficacy across three real-world datasets."
                },
                "weaknesses": {
                    "value": "1.This paper only explores one simple threshold-based attack. Although the authors show that this simple attack works well, it is better to test other popular attacks, e.g., fitting a NN, in the membership inference literature.\n\n2.The authors assume the documents of one user are independent (in Equation 4). This is a reasonable assumption for sample-level inference attacks, but it may not be the right assumption for user inference attacks. Consider emails, the content in email $x_{i+1}$ probably depends on the previous email $x_{i}$.\n\n3.There are two intuitive mitigations that may be worth exploring. The first is to clip user-level gradients instead of sample-level gradients, this helps control user contribution even if one user has many more documents than others. The second is to deduplicate documents from a single user (motivated by the finding in Figure 6)."
                },
                "questions": {
                    "value": "The attack success rate on the Enron email dataset significantly surpasses the success rate on other two datasets. Why is this the case? Although each user contributes more documents in the email dataset, Figure 3 shows that attacks against the email dataset are still much easier even when the number of documents is limited."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6787/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697942903045,
            "cdate": 1697942903045,
            "tmdate": 1699636784225,
            "mdate": 1699636784225,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "v1xuYv4LKv",
                "forum": "EBUoTvVtMM",
                "replyto": "4CxSQAPT7A",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6787/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6787/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer UDKj"
                    },
                    "comment": {
                        "value": "We thank the reviewer for appreciating the practicality of our threat model and for the review. We answer each of the questions below.\n\n**[Other approaches to the attack design]**\n\nState-of-the-art membership inference attacks  (e.g., LiRA by Carlini et al. 2022) require training hundreds of shadow models during the attack stage to learn the distribution of model confidences and create a test statistic. This approach would not be feasible for LLMs due to the high computational complexity incurred in training the shadow models. One of our main requirements is to design an efficient attack statistic that does not train additional shadow models. This motivates the use of the pretrained model as a reference model, resulting in the calibrated loss on the fine-tuned model normalized by the loss on the pretrained model as a test statistic.  \n\nOf course, there could be other attack designs even stronger than ours with similar efficiency, but that would only make privacy leakage a bigger concern. Our attack already indicates an unacceptable level of privacy leakage in LLM finetuning and we hope that future work will address the need for developing and evaluating defenses, as well as potentially designing stronger privacy attacks. \n\n**[Independence of user\u2019s documents]**\n\nThis is a great question! In the email case, independence is a modeling assumption that, even if incorrect, makes the problem more computationally tractable. Even with this simplifying assumption, we still get an attack that is relatively strong in most cases. Relaxing this assumption could likely lead to stronger attacks, but those won't necessarily change the conclusions/observations we make in the paper. \n\nTo reiterate, we do not aim to propose the strongest attacks in the paper. Rather, we focus on highlighting that even simple attacks (that are computationally efficient) can already identify privacy leakages in LLM fine-tuning.\n\n**[Mitigations: Clipping user-level gradients]**\n\nClipping user-level gradients is a promising approach. In fact, the gold standard defense is differential privacy at the *user-level* (see Section 5), which involves clipping user-level gradients and adding noise.\n\nUnfortunately, existing LLM finetuning infrastructure does not allow for a straightforward way of implementing clipping at the user-level efficiently at scale. Our results can be seen as strong evidence in favor of (a) developing software that supports per-user clipping, and (b) augmenting training workflows to keep track of the \u201cowner\u201d of each data point.\n\n**[Mitigations: Deduplicate documents from a single user]**\n\nGreat question! We ran some preliminary analyses on the number of duplicates in each dataset. Here are the results:\n- Nearly 10% of examples of CC News are _exact_ duplicates within users.\n- ArXiv is too large to look for exact duplicates. 2.4% of all examples in ArXiv are _approximate_ duplicates _across_ users (meaning the number the _exact_ duplicates _within users_ will likely be significantly smaller).\n- We are still in the process of analyzing the Enron email dataset and we don\u2019t yet have final statistics.\n\nYour suggestion of deduplicating data at the user-level is a promising idea for mitigation. We will implement and evaluate it in the next revision of the paper (as this requires longer to execute). \n\n**[Attack success rate on the Enron dataset]**\n\nThere are two key reasons why the user inference attack enjoyed high success on the Enron dataset, also highlighted in our analysis in Proposition 1:\n- Each user contributes a large amount of data to the training of the model. Figure 6 (right) shows that this allows for a greater attack efficacy. (Note that Figure 3 shows the effect of the attacker\u2019s knowledge: the above result is true even when the attacker\u2019s knowledge is minimal).\n- Second, the statistical divergence between the samples contributed by the target user compared to other training data (i.e., the KL divergence between the user\u2019s distribution and other users\u2019 training distributions): The larger the distance, the higher the attack success. There is more separation between users in the Enron dataset compared to ArXiv: for instance,  emails contain more user-level information in the signatures, and salutations. Other details such as addresses are also commonly found in the body of the email (see some examples [here](https://huggingface.co/datasets/aeslc))"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6787/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700197187981,
                "cdate": 1700197187981,
                "tmdate": 1700197187981,
                "mdate": 1700197187981,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "drd6YX2lHb",
                "forum": "EBUoTvVtMM",
                "replyto": "v1xuYv4LKv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6787/Reviewer_UDKj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6787/Reviewer_UDKj"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response. I have two follow-up questions.\n\nRegarding clipping user-level gradients. Could the authors provide more details about the challenges of clipping user-level gradients? Given recent advances in DP training of transformers, I expect that the implementation of computing sample-level gradients is mature. If one can compute sample-level gradients, then it should also be feasible to aggregate the results to get user-level gradients. \n\nRegarding deduplicating documents. Thanks for the new results. What do exact/approximate duplicates mean? I'm surprised that CC News has a non-trivial duplication rate. Could you please share more details about what types of contents are duplicated? I was only expecting there are a non-trivial number of duplicates in the Enron email dataset (because of the shared signatures, as the authors have mentioned)."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6787/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700442333883,
                "cdate": 1700442333883,
                "tmdate": 1700442333883,
                "mdate": 1700442333883,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IDxKrzWWOc",
                "forum": "EBUoTvVtMM",
                "replyto": "Irf1XvTS9b",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6787/Reviewer_UDKj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6787/Reviewer_UDKj"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the update. I appreciate the authors' attempt to incorporate the two suggested mitigations. I am maintaining my original score as the submission has not yet addressed the three points raised in my initial review. Nonetheless, I still think the idea of user-level inference attack is intriguing, and I believe this work would be stronger if it could include the results addressing any of these comments."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6787/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700720095272,
                "cdate": 1700720095272,
                "tmdate": 1700720095272,
                "mdate": 1700720095272,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "u6fwIVYb6N",
            "forum": "EBUoTvVtMM",
            "replyto": "EBUoTvVtMM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6787/Reviewer_7zUW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6787/Reviewer_7zUW"
            ],
            "content": {
                "summary": {
                    "value": "The paper defines a threat model, called user inference, wherein an attacker infers whether or not a user\u2019s data was used for fine-tuning. It also proposed a practical attack that determines if a user participated in training by computing a likelihood ratio test statistic normalized relative to a reference model.\n\nThe paper empirically studied the susceptibility of models to user inference attacks, the factors that affect their success, and potential mitigation strategies, using three datasets: Arxiv abstracts, CC News, and Enron Emails. Experimental results have shown that the proposed attack can achieve non-trivial performance. It also conducted canary experiments to study the privacy leakage of worst-case users. The paper also studied several mitigation methods, including gradient clipping, early stopping, and limited data per user."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper introduced a new realistic threat model called user inference, and proposed a practical attack using likelihood ratio test. Experimental results show the effectiveness of the proposed attack method.\n- The paper is well-written and enjoyable to read. Sufficient insights and informative figures are provided to help make the point."
                },
                "weaknesses": {
                    "value": "There is no obvious weakness in the draft."
                },
                "questions": {
                    "value": "1. In the current setup, the attacker is assumed to have access to a fine-tuned language model, as well as a similar reference model, which is the pretrained GPT-Neo in the experiments. If the reference model is less similar (which can be realistic, as attackers may not always have access to the base model), would it be much harder to perform the attack?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6787/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6787/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6787/Reviewer_7zUW"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6787/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698734493375,
            "cdate": 1698734493375,
            "tmdate": 1699636784093,
            "mdate": 1699636784093,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CTADXLUBDj",
                "forum": "EBUoTvVtMM",
                "replyto": "u6fwIVYb6N",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6787/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6787/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 7zUW"
                    },
                    "comment": {
                        "value": "Thank you so much for your positive comments and suggestions! We are glad that you found the paper enjoyable to read. \n\n**[Question 1] choices of the reference model**  \nThat is a great question! We have performed an experiment in which we consider an attacker having access to a different pretrained reference model (Table 3 in Appendix). We obtained the best attack success when the attacker model is the same (e.g., 93.1% on Enron with GPT-Neo), but we only see a small reduction in attack success for a different pretrained model (e.g., 87.6% on OPT and 87.2% on GPT-2 for Enron). Fortunately, we found that the attack is not very sensitive to the knowledge of the reference model, and we believe that access to a public pretrained model is a reasonable assumption for the attacker."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6787/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700196986840,
                "cdate": 1700196986840,
                "tmdate": 1700196986840,
                "mdate": 1700196986840,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CrZXk7ELCg",
            "forum": "EBUoTvVtMM",
            "replyto": "EBUoTvVtMM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6787/Reviewer_TnAv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6787/Reviewer_TnAv"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes the threat model of _user inference_, aiming to determine whether a user\u2019s data exists in the fine-tuning dataset using only a small set of the user\u2019s samples. _User inference_ relaxes the assumption in previous privacy attacks, such as membership inference, which requires direct access to the specific user data from the training set. With _user inference_, the authors further investigate factors correlated with the privacy risk concerning user participation in training. They find that the uniqueness, the quantity, and the attacker\u2019s knowledge of the user\u2019s data can affect the attack effectiveness. This indicates how to mitigate the privacy risk using user-level differential privacy techniques during training."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper presents a clear problem statement and convincingly demonstrates how user inference, grounded in a more realistic assumption regarding access to user data, fills gaps in existing privacy attacks. The core idea is clearly illustrated through figures and well elaborated with theoretical and technical details in the paper. \n\nAdditionally, the paper provides thorough experiments and analysis of the attack performance across different datasets and explores factors that may influence the performance. It also offers insights into mitigating user-level privacy leakage during training. In my opinion, it sheds light on future works in privacy defenses, particularly in scaling up the user-level differential privacy for LLMs."
                },
                "weaknesses": {
                    "value": "The basic assumption of user inference -- \"samples from the same user are more similar on average than those from different users\" -- could limit the applicability of the method. When merging samples from diverse tasks or domains during finetuning (_e.g._, blogs of different topics or on different media platforms), samples from the same domains might exhibit greater similarity than those from the same user. Also, since the pre-trained model is used as the reference model to calculate the test statistic, it\u2019s also necessary to control and analyze the similarity among the pre-training data, the fine-tuning data, and the attacker\u2019s knowledge, considering these potentially influential factors. \n\nIn this case, the disparity in attack performance on the three datasets may not solely be attributed to the amount of each user's data. For example, content in ArXiv Abstracts is usually more objective and its characteristics might be more reflective of the specific topics/subjects rather than the individual user identities/styles. The paper might benefit from an extended analysis or discussion on this aspect."
                },
                "questions": {
                    "value": "A. Besides the amount of user data, are there other factors that could significantly influence the attack effectiveness? For example, while the unique email signatures can serve as distinct identity markers, would there be other (potentially spurious) features (_e.g._, email/article subjects) that could be captured for user identification?\n\nB. How would the proposed attack technique perform on recent Chatbot models, such as Llama-2-chat?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6787/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6787/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6787/Reviewer_TnAv"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6787/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698765491362,
            "cdate": 1698765491362,
            "tmdate": 1699636783975,
            "mdate": 1699636783975,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6QY11P5S7o",
                "forum": "EBUoTvVtMM",
                "replyto": "CrZXk7ELCg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6787/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6787/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer TnAv"
                    },
                    "comment": {
                        "value": "Thank you so much for your insightful comments and supportive suggestions on our work. We are glad that you found our threat model realistic and our attack a foundation for future work in privacy-preserving LLM fine-tuning.\n\n**\"user inference \u2026 merging samples from diverse tasks or domains\"**\n\nGreat question! Note that user inference is not possible when two users have the same distribution (e.g. split a set of documents randomly into two). Intuitively, some statistical similarity between samples from the user is necessary for general user inference (not just our attack). In particular, user inference is only possible if the attacker\u2019s i.i.d. samples are closer to the user\u2019s fine-tuning data than to any other user\u2019s data.\n\nThis is what we meant by \"samples from the same user are more similar on average than those from different users\" \u2014 we have adjusted the phrasing to be more clear.\n\n**\"disparity in attack performance \u2026 may not solely be attributed to the amount of each user's data [but to the text content. [Question A] other factors that could significantly influence user inference\"**\n\nGreat question! Indeed, the attack performance disparity on different datasets is not only due to the varying number of users (even though that is an important factor).  We identified two additional factors that contribute to the attack performance, through our analysis in Proposition 1, also confirmed experimentally, \n\n1. _The number of documents contributed by each user_: The Enron dataset has fewer users, each contributing more samples and a larger fraction of the training set, resulting in higher privacy leakage. Figure 6 (right) shows that the attack efficacy increases with the number of fine-tuning documents.\n1. _The statistical divergence between the samples contributed by the target user compared to other training data_ (i.e., the KL divergence between the user\u2019s distribution and other users\u2019 training distributions): The larger the distance, the higher the attack success. There is more separation between users in the Enron dataset compared to ArXiv: for instance, emails contain more user-level information in the signatures, and salutations. Other details such as addresses are also commonly found in the body of the email (see some examples [here](https://huggingface.co/datasets/aeslc)). On the other hand, the scientific writing style of ArXiv abstracts, which is predominantly impersonal and formal, makes the users less distinct. All these factors resulted in a more successful attack on Enron compared to ArXiv and CC News. \n\n**[Question B] proposed attack technique on recent chatbots**\n\nThat is an interesting question and could be a really good topic of future research. Chatbots such as ChatGPT and even Llama-2-chat are trained with more complex pipelines, including fine-tuning on human-labeled data, training a reward model, and optimizing a policy using reinforcement learning -\u2013 quantifying the effect of each of these steps on user inference would indeed be interesting. \n\nAs with any other LLM, an attacker with multiple writing samples from a target user should be able to mount an attack to extract information about the user by carefully prompting the model and using the generated text. The main issues with this approach are (a) the lack of large user-stratified chat datasets to validate the success of such an attack, and (b) the lack of software support to reproduce the multi-stage fine-tuning and alignment pipeline of chatbots. Addressing these shortcomings and running controlled user inference experiments on chatbots is an interesting direction for future work."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6787/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700196880430,
                "cdate": 1700196880430,
                "tmdate": 1700196880430,
                "mdate": 1700196880430,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "j8aawnOgRE",
            "forum": "EBUoTvVtMM",
            "replyto": "EBUoTvVtMM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6787/Reviewer_hrNQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6787/Reviewer_hrNQ"
            ],
            "content": {
                "summary": {
                    "value": "The goal of the paper is to evaluate the extent to which large language models leak whether any of the data generated by a user was used to fine-tune them. The authors propose a black-box attack based on the output probabilities of the model and evaluate it on three different datasets, showing their attack success to be quite high. They then study the impact of mitigations and factors such as outlier and amount of user\u2019s data on the attack performance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- 1) Understanding the extent to which fine-tuned LLMs leak the membership of users in a relaxed threat model (i.e., was any data of a user used to train the model) is an interesting question.\n- 2) Extensive evaluation of the attack\u2019s performance under different settings and mitigation techniques."
                },
                "weaknesses": {
                    "value": "-  1) Lack of novelty: the black-box methodology used is standard and https://arxiv.org/pdf/2304.02782.pdf already proposes the same threat model. Framing existing attack terminology (\u201cuser inference\u201d) as something new is confusing as there are already multiple works proposing the stronger threat model where either X texts of a user\u2019s data or none were used to train the model (e.g., Song and Shmatikov). To the best of my understanding, it seems that user-level + fine-tuning + LLM is new, but the current wording doesn't reflect that, instead suggesting that the paper introduces user-level MIAs. \n- 2) The arXiv abstracts dataset seems unsuitable for user-level MIAs. Indeed, the authors assign the user who submitted the paper as the \u201cuser\u201d (i.e., the author). However, papers have multiple authors which may rewrite or edit the abstract, e.g., the PI. Hence, an abstract could be assigned as belonging to an individual even though it was largely written by someone else. I see two issues with this: (a) Potential authorship overlap between different users' documents, e.g., the same author (PI)\u2019s data could be labeled as belonging to multiple users and (b) The task being evaluated looks like PI/research group inference attack. The \u201cuser\u201d terminology is thus misleading, and this should be acknowledged early on.\n- 3) Problematic evaluation due to non-member users\u2019s data having already been used to pre-train the LLM. The LLMs used in this work are pre-trained in 2021 on The Pile, whose description suggests that it contains both arXiv papers and the Enron Emails dataset. The results are thus biased because the LLM already saw both members and non-members\u2019 data in the pre-training stage. To address this, the authors could evaluate their attack on a dataset known not to have been part of training. This can be done by using arXiv abstracts from 2022 onwards (although these suffer from the problem highlighted above) or another dataset from the well-studied authorship attribution problem, see e.g., Luyckx et al. https://aclanthology.org/C08-1065.pdf)"
                },
                "questions": {
                    "value": "- 1) What are possible reasons for the difference in attack performance on the three datasets (Figure 2)?\n- 2) Have the authors considered the connections between their work and the authorship attribution (AA) problem? One appeal of the relaxed user inference threat model is that it allows to study questions such as: what in a user\u2019s writings (topic, style, both) is constant enough across different texts to enable the attack to succeed. These questions are widely studied in the AA literature and there are many publicly available controlled datasets, see e.g. Luyckx et al. (https://aclanthology.org/C08-1065.pdf) that this work could use in their evaluation."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6787/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698772200374,
            "cdate": 1698772200374,
            "tmdate": 1699636783860,
            "mdate": 1699636783860,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "b2kGEACJxu",
                "forum": "EBUoTvVtMM",
                "replyto": "j8aawnOgRE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6787/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6787/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer hrNQ: Part 1"
                    },
                    "comment": {
                        "value": "Thank you so much for your careful reading of the paper and your detailed comments and suggestions! We believe we have addressed all your concerns below. Please do not hesitate to reach out in case of further clarifications or comments.\n\n**Lack of novelty: black-box methodology used is standard and (Chen et al. 2023) already proposes the same threat model**\n\nThank you for pointing out the additional reference (Chen et. al. 2023). We have improved the positioning of the paper relative to existing work on user-level privacy risks in the introduction, and related work (Section 2). While user inference attacks have been studied in the literature for other application domains such as face recognition and speech recognition, we are the first to study user inference for LLMs where the attacker does _not_ have access to the exact training samples.\n\nSpecifically, **all existing work in the context of generative text models study more restrictive threat models** than what we study in this paper. Most similar to our work are the following papers:\n- [Song and Shmatikov 2019](https://arxiv.org/abs/1811.00513) study user-level inference under the stringent assumption that the  **attacker has access to the training set** of the text generation model. In contrast, our threat model assumes that the attacker may not have full knowledge of the user\u2019s training samples, and is thus more realistic. Additionally, their attack trains multiple shadow models on subsets of the training set and a meta-classifier to differentiate users participating in training from other users. The meta-classifier-based methodology does not apply to LLMs due to the large computational cost. \n- [Shejwalkar et al. 2021](https://openreview.net/pdf?id=74lwg5oxheC) performs user inference for text-based classification models, but also assumes that the attacker has access to the user\u2019s training samples. They do not consider generative text models in this work. \n\nThus, to the best of our knowledge, our user inference threat model has not been considered for generative text models. This is practically relevant to many settings today, such as Smart Compose, GBoard, and GitHub Copilot.\n\nWe also wish to point out that any paper published after May 28, 2023, is considered contemporaneous work as per the [ICLR policy](https://iclr.cc/Conferences/2024/ReviewerGuide). The **Chen et al. paper** you pointed out was published in August 2023 and is thus **considered contemporaneous**. We ask that you assess the merits of our work independently.\n\n\n**[ArXiv dataset] multiple authors which may rewrite or edit the abstract**\n\nWe acknowledge the limitations of the ArXiv dataset you have identified. Despite them, we still believe that analyzing the attack on ArXiv shows some interesting results. \n\nWhile the user labeling might not be perfect, the fact that we still have significant privacy leakage suggests that the attack can only get stronger if we have perfect ground truth user labeling and non-overlapping users. Further, our experiments on canary users are not impacted at all by the possible overlap in user labeling, since we create our synthetically-generated canaries to evaluate worst-case privacy leakage. We have added a discussion in the revision in Appendix C. \n\nAt a more practical level, few datasets are large enough with user annotation for evaluating our privacy attack. Thus, despite its flaws, the ArXiv dataset provides a valuable data point (in addition to our other two datasets) in the evaluation of user inference attacks."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6787/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700196459299,
                "cdate": 1700196459299,
                "tmdate": 1700196719413,
                "mdate": 1700196719413,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AMuHyNyiUB",
                "forum": "EBUoTvVtMM",
                "replyto": "nNezKnigzE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6787/Reviewer_hrNQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6787/Reviewer_hrNQ"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your detailed response. I appreciate the more careful re-positioning of the paper within the related works.\n\nConnections to authorship attribution (AA): I wasn't referring to potential similarities between the problem statements, which I agree are different (I don't think there's a need to compare them in the paper and as far as I am concerned, the authors may remove the corresponding paragraphs from the Appendix if they wish to). What I meant is that in the AA field, researchers have explored the impact of various distributional factors on the linkability of users, curating controlled datasets which the authors may use in their evaluation. For instance: impact of number of authors (https://aclanthology.org/D13-1193.pdf), is it an author's topic or their style that makes them more re-identifiable, what happens if the target's training data is from one domain but the test data is in a different domain (https://petsymposium.org/2016/files/papers/Blogs,_Twitter_Feeds,_and_Reddit_Comments__Cross-domain_Authorship_Attribution.pdf). All of these factors are relevant to the user inference problem, since the hypothesis is that a user's data remains sufficiently consistent between the fine-tuning step and the UIA query step to enable good attack performance. Hence, my question was whether the authors looked into this literature.\n\narXiv dataset: It's unclear whether results will only get stronger if the dataset was cleaned. Consider two arXiv users A and B and a paper with abstract X co-written by B but labeled as belonging to A because A submitted it. Consider an LLM fine-tuned on data labeled as A but not on data labeled as B. If the current attack method predicts that no sample from B was used to train the model, the prediction would be considered correct when it's not, inflating the performances.\n\nI still doubt that the datasets are clean enough for a correct evaluation of UIA risks and it seems doable to collect or curate suitable datasets with clean user labels and a variety of distribution assumptions between train and test MIA data allowing to correctly test various hypotheses."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6787/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700642469099,
                "cdate": 1700642469099,
                "tmdate": 1700642469099,
                "mdate": 1700642469099,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PimIdn43Gd",
                "forum": "EBUoTvVtMM",
                "replyto": "j8aawnOgRE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6787/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6787/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We really appreciate your detailed comments! In this response, we argue that:\n- the arxiv dataset are still valid, and,\n- the existing authorship attribution benchmarks are unsuitable for user inference.\n\n**Arxiv dataset**: We appreciate the reviewer\u2019s point but we believe that the noisy labels will, on average, lead to reduced attack performance.\n\nRecall that our theoretical analysis shows that the attack is most successful when the users are well separated. Mislabeling examples leads to a mixture of data distributions (e.g. documents listed under a user A are written both by user A as well as their co-authors), which brings the data distributions closer together (this is proved below). Overall, this reduces attack efficacy as per Proposition 1.\n\n**Claim [Mixing distributions brings distributions closer]**: Let $P, Q$ be two user distributions. Suppose the mislabeling of documents leads to distributions $P\u2019 = \\lambda P + (1 - \\lambda) Q$ and $Q\u2019 = \\mu Q + (1-\\mu) P$ for some $\\lambda, \\mu \\in (0, 1)$. We always have $\\text{KL}(P\u2019 \\Vert Q\u2019) \\le \\text{KL}(P \\Vert Q)$. \n\n**Proof**: This can be seen by convexity of the KL divergence. Indeed, we have\n$$\n\\text{KL}(P \\Vert \\mu Q + (1 -\\mu)P) \\le \\mu \\cdot \\text{KL}(P \\Vert Q) + ( 1- \\mu) \\cdot \\text{KL}(P \\Vert P)  = \\mu \\cdot \\text{KL}(P \\Vert Q)\n$$\nThus, $ \\text{KL}(P \\Vert \\mu Q + (1-\\mu)P) <  \\text{KL}(P \\Vert Q) $. \nThe same holds for the first argument by the same reasoning.\n\nWe note that this argument also holds authorship attribution. If the documents are randomly mislabeled, the accuracy of any classification model only reduces the classification accuracy (see e.g. the influential paper by [Zhang et al. (ICLR 2017)](https://openreview.net/pdf?id=Sy8gdB9xx)).\n\n\n**Authorship attribution datasets**: Unfortunately, the existing curated datasets in the authorship attribution literature are **not interesting for user inference** as they are _too small_ or have _very few samples per user_. \n\nSpecifically, here are the curated datasets we found and their drawbacks.\n- CCAT50 (50 authors), CMCC has (21 authors), Guardian has (13 authors) and IMDB62 (62 authors) have even fewer authors than enron, our smallest dataset, where we also have near perfect attack accuracy.\n- Project Gutenberg (29k documents and 4.5k authors) has enough users, but each user only has 6 documents on average. This is not enough to mount a user inference attack, as we need enough documents for fine-tuning (see Figure 6, right) and attacker\u2019s knowledge (see Figure 3) for the attack results to be non-trivial.\n\nWhile we appreciate the reviewer\u2019s suggestion to explore the authorship attribution datasets, we conclude that the benchmarks are unsuitable for our experiments.\n\n**Summary**: Thank you again for your time and consideration."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6787/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700702290603,
                "cdate": 1700702290603,
                "tmdate": 1700702575482,
                "mdate": 1700702575482,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]