[
    {
        "title": "Neural Networks and Solomonoff Induction"
    },
    {
        "review": {
            "id": "jJwnm0HUQk",
            "forum": "tJDlRzQh7x",
            "replyto": "tJDlRzQh7x",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7375/Reviewer_YTUE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7375/Reviewer_YTUE"
            ],
            "content": {
                "summary": {
                    "value": "- This paper investigates \"amortizing Solomonoff Prediction into a neural network\". \n- They then \"introduce a generalized Solomonoff prior\". \n- Lastly, they conduct experiments to show predictive performance on sequence prediction tasks, where they use meta learning with log-loss on a heterogeneous set of string related tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper draws connections between Solomonoff Prediction and meta learning.\n\nThe paper seems to have a formal grasp on some concepts in computational complexity that are useful formalism for describing tasks in machine learning, for example ranking them according to the Chomsky hierarchy (does the task require a stack to solve? or a more complicated data structure)"
                },
                "weaknesses": {
                    "value": "Overall, the paper is hard for me to follow. In summary the issues are:\n\n- Many definitions given up front (up to beginning of page 4) are fairly non-standard, given the general body of work that shows up at ICLR. At the same time, the presentation features little discussion of definitions after they are given, with most details relegated to appendix. \n\n\n- Theorem statements in main text contain uncommon terms \"probability gap\" without definition. \n\n- Definitions that are given contain other undefined terms within definition, e.g.:\n  - An algorithmic data generating source \u00b5 is simply a computable data source by\" A \"data source\" was not defined.\n  - SI: Inductive inference aims to find a universally valid approximation to \u00b5. What's \"universally valid\"?\n\n- Propositions (e.g. prop 4, prop 8) are given and followed immediately  by a next section with no concluding sentence on what the takeaway should be or what the theorem means in words.\n\n- Many data details (e.g.\"Variable-order Markov Source\", one of the 3 experiments) are not defined in main text and details are relegated to appendix, and, as mentioned, are generally not particularly well known within the ICLR community.\n\n- Many baselines / models not defined in main text: Stack-RNNs, Tape-RNNs,  Context Tree Weighting, where the last one is used as the main baseline.\n\n- Important experimental details that are glossed over, e.g. there is a test distribution described as \"out-of-distribution\" in passing in the analysis of results without a formal experimental setup given for precisely what the shift between in- and out-distribution is.\n\n\nI will be glad to raise my score if a major rewrite of this paper is undertaken. In particular it must be readable to wider audience without having to refer to the appendix for interpretation of main contributions or for understanding basic setup like datasets and baselines. As mentioned in \"Questions\" below, it is also necessary to clarify whether the experimental results are something distinct from running basic meta-learning on existing datasets. If not, is the significance in the connection to the theoretical results? If so, what is that connection?"
                },
                "questions": {
                    "value": "- It is stated that $\\pi_\\theta$  approximates the predictive distribution for each task $p(x_{t+1}|x_{\\leq t}, \\tau)$ for each task $\\tau$ . However $\\pi_\\theta(x_{\\leq t})$ is not notated to be a function of $\\tau$. If $\\pi_\\theta$ is optimized with log loss it will learn a mixture of the predictive distribution across tasks rather than each task, unless extra assumptions are stated, such as that the support of $x_{\\leq t}$ is disjoint across tasks for each $t$. Could the authors clarify  whether $\\pi_\\theta$ is also a function of $\\tau$, and if not, what are the assumptions on the data that make this statement true?\n\n\n- \"Out-of-distribution\" appears twice in the main text, including in the qualification of a test distribution. However, no particular definition of what is in versus out, or what the distribution shift is precisely, was given. A few sentences later, length-generalization is mentioned in passing, so I had to infer what in versus out meant. Usually it's really important to mention training versus test distribution details up front rather than in passing in the results. Could you please explain the precise experimental setup including data generation in more detail, in the main text?\n\n\n- Finally, experimentally, it's not clear that the experiments run were anything different than running log likelihood optimization on a mixture of datasets. What's the practical/algorithmic difference or significance in what was run, and what should the takeaways be? If there is no difference, is the significance in the connection to the theoretical results? If so, what is that connection?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7375/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698810089688,
            "cdate": 1698810089688,
            "tmdate": 1699636882219,
            "mdate": 1699636882219,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Hhetso07xL",
                "forum": "tJDlRzQh7x",
                "replyto": "jJwnm0HUQk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7375/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7375/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your insightful comments about our paper. We streamlined the paper thanks to your comments making it more readable and understandable. In particular, we have added to the revised manuscript:\n\n* We added more information on why some of our definitions are needed.\n* What we mean by \u201cprobability gap\u201d: Let $\\mu(x)$ be the probability that an (in)finite sequence \\emph{starts} with $x$.  While probability distributions satisfy $\\sum_{a\\in\\mathcal{X}}\\mu(xa) = \\mu(x)$, semimeasures exhibit \\emph{probability gaps} and only satisfy  $\\sum_{a\\in\\mathcal{X}}\\mu(xa)\\leq\\mu(x)$. \n\n* We added what we mean by \u201cdata source\u201d, which is simply an object that generates data, and replaced  \u201cuniversally valid approximation to \\mu\u201d with a better description of Solomonoff Induction.\n* We added a Summary to Section 3 explaining why Propositions 4, 6 and 8 are useful.\n* Added more intuition to the Variable Order Markov Source in the main manuscript and in the appendix. See response to reviewer Mfzf for an expanded intuition for the variable-order Markov source. \n* We added a sentence explaining how Stack-rnn and Tape-rnn are just RNNs augmented with a stack and tape memory.\n* We do have a definition of what we mean by in- and out-of-distribution in the \u201cEvaluation procedure\u201d paragraph of Section 4. In the revised version we emphasize it better.\n* We respond to all your questions below.\n\n**\u201c[...] Could the authors clarify whether $\\pi_\\theta$ is also a function of $\\tau$, and if not, what are the assumptions on the data that make this statement true?\u201d**\n\n$\\pi$ has no dependency on $\\tau$. In addition, the aim of $\\pi$  is to approximate the mixture distribution (over $\\sum_\\tau p(x | \\tau)p(\\tau)$). Since this is fairly standard we talked about this in the Background section on Meta-learning. \n\n**\u201cOut-of-distribution\" appears twice in the main text, including in the qualification of a test distribution.[\u2026] Usually it's really important to mention training versus test distribution details [...] Could you please explain the precise experimental setup including data generation in more detail, in the main text?\u201d**\n\nOur definition of in- and out-of-distribution is located on \u201cEvaluation procedure\u201d in Section 4. We highlighted this better.\nWe added more intuition and detail  in the main text about the experimental setting.\n\n**\u201cFinally, experimentally, it's not clear that the experiments run were anything different than running log likelihood optimization on a mixture of datasets. What's the practical/algorithmic difference or significance in what was run, and what should the takeaways be? If there is no difference, is the significance in the connection to the theoretical results? If so, what is that connection?\u201d**\n\nThanks for the comment, this is exactly the point that we are trying to make. That is, learning from the log-loss on a distribution of tasks leads to an approximation of Solomonoff Induction in a similar fashion as Ortega et al 2019, but only when the \u201cdata-mixture\u201d is of a particular form, generated by UTMs, as described by our Section 3. This is exactly the theoretical takeaway. Furthermore, Thm 9 allows for more flexibility in generating this type of data, in which changing the distribution over programs may unblock generating the most important data quicker, which could lead to better performance. We use this fact in our experiments to generate better outputs while maintaining universality.  On the experimental side, we show mainly two things: 1) some networks are capable of achieving close to the Bayes-optimal solution for data sources that involve a mixture over programs (in this case Variable-Order  Markov Data is generated by mixing trees which can be effectively thought of as programs). 2) Training networks on UTM data exhibits transfer to the other tasks we considered. This is a hint that the UTM data exhibits rich transferable patterns. We believe that designing better UTMs or distribution over programs might lead to even more transfer to more complex domains."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7375/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738190781,
                "cdate": 1700738190781,
                "tmdate": 1700738190781,
                "mdate": 1700738190781,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uIFe6ssSNe",
            "forum": "tJDlRzQh7x",
            "replyto": "tJDlRzQh7x",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7375/Reviewer_iUvX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7375/Reviewer_iUvX"
            ],
            "content": {
                "summary": {
                    "value": "In this work, authors theoretically investigate how universal approximators using a dataset converge to SP in some limit and show that universality is maintained even when underlined distribution shifts. They experiment with Transformers and LSTM NNs to show model complexity increases with increase in parameters, such that convergences can be seen on challenging dataset."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well written\n2. Good set of experiments"
                },
                "weaknesses": {
                    "value": "1. Novelty is limited\n2. Several key papers are not cited\n\nBelow, I provide my detailed review."
                },
                "questions": {
                    "value": "It is true only for first-order RNNs that they are Turing complete with infinite precision and time, however, tensor RNNs with and without memory are shown to be equivalent to TM with finite precision [5,6] and also UTM\n\nLemma 10, Corollary 11, Lemma 12-14 [ 1-2] \u2013 Theorem 9 in paper shares some similarities, slightly different ways to prove the same property. \n\nTheorem 3 Linear separation [4] \u2013 the paper could benefit by showing how various layers in transformers cause linear separation using hard attention and would lie in Banach space dual. Again, with some assumption, it is trivial to show how their approach is also universal. \n\nGeneralized Solomononff semimeasures definition and Theorem 9 in the paper also share similarity with [3]; second majority of suggestions and claims are given in [3]. Furthermore they have shown some experiments and multiple hypothesis generation can be seen as a case of meta-learning. There are several lemmas on recursive functions, that can be extended with modern RNNs such as LSTM and even for Transformers (assuming within a finite length, they approximate RNN).\n\nAuthors should cite these line of work. Thus it seems the current manuscript is more incremental aligned with the experimental setup in the meta-learning space using modern NNs.\n\nFinally, it is hard for me to see what values the current method provides to the community; I will briefly discuss why I feel this,\n\n* theorem 11 in [7] proves that equivalence between two RNN is undecidable, Theorem 6 shows that consistency problem in RNN is also undecidable, Theorem 7 shows 2 layer RNN using BPTT on a finite corpus is necessary not consistent, furthermore Theorem 11 and 8 points out best string problem is NP-hard and in some cases undecidable. Given we know above properties for RNN, that is also true for transformers with some conditions, thus I am not sure Solomonoff induction would help in getting universal capability of the modern day NNs\n\n* Second RNN and transformers are turing complete comes from a unrealistic assumptions where entire tape is encoded into a tape. Based on bignum arithmetic we can see there is infinitely many hierarchies across various natural numbers, and works in infinite space. Therefore, what practical benefits it offers is still a open question.\n\n* Third when we move to UTM space and show RNN is equivalent to UTM will also work in infinite space and time\n\n* Fourth Solomonoff induction also requires infinite samples, given everything or in simple words all components are working in infinite space, how can one show practical universality? Nor can it be claimed that the model trained on the dataset is universal. So, I would advise authors to lower down the claim as it is highly misleading.\n\n\nIt would benefit if authors can provide insight how transformers and RNNs LM can benefit. For instance, by showing how they work when state space is small vs large, symbols are increased, model is trained on short strings and tested on longer. How do attention weights attend in such scenarios, how does LSTM memory adapt to these changes? Do you observe any tape-like or even stack-like behaviour etc. showing these analyses would further benefit the paper and will help understand how using SI can help LLMs reason about the world in some finite space. \n\n\n\n1.\tSterkenburg, T.F., 2017. A generalized characterization of algorithmic probability. Theory of Computing Systems, 61, pp.1337-1352.\n\n2.\tWood, I., Sunehag, P. and Hutter, M., 2013. (Non-) equivalence of universal priors. In Algorithmic Probability and Friends. Bayesian Prediction and Artificial Intelligence: Papers from the Ray Solomonoff 85th Memorial Conference, Melbourne, VIC, Australia, November 30\u2013December 2, 2011 (pp. 417-425). Springer Berlin Heidelberg.\n\n3.\tLi, M. and Vitanyi, P.M., 1992. Inductive reasoning and Kolmogorov complexity. Journal of Computer and System Sciences, 44(2), pp.343-384.\n\n4.\tSunehag, P. and Hutter, M., 2013. Principles of Solomonoff induction and AIXI. In Algorithmic Probability and Friends. Bayesian Prediction and Artificial Intelligence: Papers from the Ray Solomonoff 85th Memorial Conference, Melbourne, VIC, Australia, November 30\u2013December 2, 2011 (pp. 386-398). Springer Berlin Heidelberg.\n\n5.\tStogin, J., Mali, A. and Giles, C.L., 2020. A provably stable neural network Turing Machine. arXiv preprint arXiv:2006.03651.\n\n6.\tMali, A., Ororbia, A., Kifer, D. and Giles, L., 2023. On the Computational Complexity and Formal Hierarchy of Second Order Recurrent Neural Networks. arXiv preprint arXiv:2309.14691.\n\n7.\tChen, Y., Gilroy, S., Maletti, A., May, J. and Knight, K., 2017. Recurrent neural networks as weighted language recognizers. arXiv preprint arXiv:1711.05408."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7375/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699023219927,
            "cdate": 1699023219927,
            "tmdate": 1699636882071,
            "mdate": 1699636882071,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5txKlfz99U",
                "forum": "tJDlRzQh7x",
                "replyto": "uIFe6ssSNe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7375/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7375/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the insightful comments and references. We cited all suggested papers, added a limitation section in the Discussion and highlighted better our novel contributions. \n\n**\u201c... tensor RNNs with and without memory are shown to be equivalent to TM with finite precision [5,6]...\u201d**\nThank you for the comment; we will add these references to our manuscript.\n\n**\u201cLemma 10, Corollary 11, Lemma 12-14 [ 1-2] \u2013 Theorem 9 in paper shares some similarities \u2026\u201d**\nand\n**\u201cGeneralized Solomononff semimeasures definition and Theorem 9 in the paper also share similarity with [3]...\u201d**\n\nThank you for the references. While references 2 and 3 indeed talk about basic and fundamental ideas around Solomonoff Induction they only contain the statement that M is universal, they do not contain any discussion of M^Q (the non uniform case). With respect to reference 1, which we were unaware of, indeed shows similarity to our Thm 9. This means that the conclusions obtained by our Thm 9 are not novel. It seems we have independently discovered similar conclusions to reference 1, though our proof seems to be more self-contained and short when compared to ref 1.  We have adapted our claims in the revised version of the manuscript stating the above.\n\n\n**Theorem 3 Linear separation [4] \u2013 the paper could benefit by showing\u2026**\nThis would definitely be an interesting element to investigate to better understand transformers, however, we think it falls outside the scope of this paper.\n\n**\u201cAuthors should cite these line of work. Thus it seems the current manuscript is more incremental aligned with the experimental setup in the meta-learning space using modern NNs.\u201d**\nWe added all citations since, indeed, they are related to our work. It is true that a major part of our work is to experimentally try to get closer to Solomonoff Induction by meta-learning neural networks (see our general response for an argument why we believe the empirical work is important). We think that this is aligned with ICLRs audience and topics. We also believe the ICLR audience would greatly benefit from thinking more about the links between empirical ML and the strong theory of Solomonoff Induction, which is one of the subgoals of this paper.\n\n**\u201ctheorem 11 in [7] proves that equivalence between two RNN is undecidable [\u2026] Given we know above properties for RNN [\u2026]  I am not sure Solomonoff induction would help in getting universal capability of the modern day NNs\u201d**\nIf the reviewer\u2019s comment refers to the fact that a non-universal model could fail in approximating SI when training on our universal data, we agree that this is the case and it is not a surprise (i.e. non-realizability). If, instead, the reviewer means that this could also happen even if we have universal architectures, we are not so sure about this. The mentioned properties for RNNs (useful citations we add to the revised version), can possibly translate to transformers, however, we note that our work simply takes current neural networks architectures and tests them empirically on various data sources including UTMs. In this work, we are not concerned about how to develop easily-trainable universal architectures as we mention in the introduction, which would be a major breakthrough and a paper on its own. We note that while neural network universality is necessary for full Solomonoff Induction, universality won\u2019t help with the fact that Solomonoff Induction is uncomputable/undecidable.  Nevertheless, more compute and bigger universal architectures should help when aiming at approximating Solomonoff Induction (that is computable/decidable for bounded time/space versions). This is the aim of our paper, and in line with our experiments, we show that increasing neural network size leads to better performance and transfer."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7375/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700737791037,
                "cdate": 1700737791037,
                "tmdate": 1700737791037,
                "mdate": 1700737791037,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fEoTcKAJS4",
                "forum": "tJDlRzQh7x",
                "replyto": "uIFe6ssSNe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7375/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7375/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part (2/2)"
                    },
                    "comment": {
                        "value": "**\u201cSecond RNN and transformers are turing complete comes from a unrealistic assumptions [...]\u201c**\n\nWe wholeheartedly agree with the reviewer. Unfortunately, there is no universally accepted universal architecture that practitioners use effectively without unrealistic assumptions. In fact, we believe this is an actual open problem and not the focus of our paper. We are simply concerned with \u201chow good widely used architectures (rnns, lstms transformers etc) are when approximating our data generating sources?\u201d. \n\n**\u201cThird when we move to UTM space and show RNN is equivalent to UTM will also work in infinite space and time.\u201d**\nWe agree that this could be the case. \n\n**\u201cFourth Solomonoff induction also requires infinite samples [...]\u201d**\nThis is an interesting question. We agree that our way of approximating Solomonoff Induction requires infinite samples, and that even if we would have a universal architecture (realizability) which can be easily trained (learnability), one can  we can never guarantee that  any approach (including ours) is exactly mimicking the universality of a Solomonoff Inductor (since it is uncomputable). However, our theory shows that in the limit this universality should be achieved. We also consider the finite space case in Definition 5 that uses limited resources (bounded time and space). We added a \u201cLimitations\u201d section explaining these nuances. Note that, we are not aware of any practical work that generates samples directly from a UTM according to our Definition 5, and then uses these samples to learn a predictor. So, our way of approximating Solomonoff Induction is novel, as far as we can tell.\n\n\n**\u201cIt would benefit if authors can provide insight how transformers and RNNs LM can benefit. For instance, by showing how they work when state space is small vs large, symbols are increased,** \n\n\nWe conducted experiments with small and large state spaces (our architectures range from small , medium and large sizes). W.r.t. Alphabet sizes we opted for a fixed alphabet size of 17 for most tasks since this allows comparing transfer performance across tasks as we do in our experiments. When having small alphabet sizes, certain computations might require longer sequences when compared to the same setting with large alphabet sizes, this is because large alphabets in a sense have more bandwidth.  \n\n**\u201cmodel is trained on short strings and tested on longer. \u201c**\nWe do have these experiments for all tasks, see Figures 1,2 and 3 right panels. Basically, our conclusions are that LSTMs do very well when tested on longer sequences, whereas transformers fail to do so.\n\n**How do attention weights attend in such scenarios, how does LSTM memory adapt to these changes? Do you observe any tape-like or even stack-like behaviour [...]\u201d**\nThese are interesting questions, but they fall beyond the scope of this paper, and we refer the reviewer to (Deletang et al 2022) where there is an exploration of such kinds of questions. In summary, given that RNNs cannot solve the tasks that a Stack-RNN or Tape-RNN can solve, this means that they are likely not implementing a stack or a tape under the hood in their internal activations."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7375/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700737922884,
                "cdate": 1700737922884,
                "tmdate": 1700737922884,
                "mdate": 1700737922884,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "O34rH4HpgL",
            "forum": "tJDlRzQh7x",
            "replyto": "tJDlRzQh7x",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7375/Reviewer_Mfzf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7375/Reviewer_Mfzf"
            ],
            "content": {
                "summary": {
                    "value": "The authors explore approximate versions of Solmonoff induction via meta-learning and neural networks.  Their experimental setup compares the performance of a variety of deep neural network architectures within their framework on several algorithmically generated data sets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is clearly written and motivates the general problem.\n\nThe discussion, although it is primarily focused on Turing machines, is relevant broadly to themes in modern ML, e.g., large language models and other increasing large DNNs trained on massive data sets.  As a result, this work might help make connections between more classical AI and modern ML."
                },
                "weaknesses": {
                    "value": "While the discussion is clear in many places, it also assumes quite a bit a background without references, e.g., \"Kolmogorov's probability axioms\".  As this is a submission to an ML conference, I suggest that the authors provide the necessary context to aid unfamiliar readers.  In the same vein, not many participants at ICLR are likely to be familiar with Solmonoff induction. So, the fit might be better at a more traditional AI venue -- the advances here are more from about using existing NN tools rather than pushing the state-of-the-art in deep NNs.\n\nThe experimental setup is missing some details, e.g., how many training examples are there?"
                },
                "questions": {
                    "value": "- I don't really have good intuition about how varied the prediction tasks are.  Can you provide a bit more intuition here?\n\n- Like large language models, I expected that you would need a significant amount of data for training in this case.  Can you talk a bit more about data sizes, and why the results are or are not expected?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7375/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699419592082,
            "cdate": 1699419592082,
            "tmdate": 1699636881950,
            "mdate": 1699636881950,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QZKTIJE7GB",
                "forum": "tJDlRzQh7x",
                "replyto": "O34rH4HpgL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7375/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7375/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the helpful comments and positive feedback. Below we provide detailed responses.\n\n**\u201c... I suggest that the authors provide the necessary context to aid unfamiliar readers\u2026\u201d** :  \nWe rephrased the description of Semimeasures to be more accessible to readers.. We also provide further explanations about Solomonoff Induction to be more easily understood by ML practitioners. We emphasize in the introduction that the focus of the paper is not to develop a novel architecture but to study more empirical versions of Solomonoff Induction using modern neural networks.\n\n**\u201c... The experimental setup is missing some details, e.g., how many training examples are there?..\u201d**:  In the Neural Predictors section we describe how we used 500k iterations with a batch size of 128. Since we use data generators (not fixed datasets) we effectively generated 500k*128 sequences of various lengths depending on the experiments. We are happy to clarify other details that the reviewer thinks are missing.\n\n**\u201cI don't really have good intuition about how varied the prediction tasks are. Can you provide a bit more intuition here?\u201d:**\nWe added intuitions in the revised version of the manuscript for all tasks, see below.\n\n* The Chomsky tasks are tasks that manipulate strings, for example, reversing an input string, or computing modular arithmetic operations. More details n our Appendix D3. \n\n* For the UTM (BrainF*ck/BrainPhoque) tasks we generate random programs (that can effectively encode any structured sequence generation process) and run them in our UTM and get the outputs. For example, in principle a program could generate the image of a cow, a chess program, or the books of Shakespeare. But of course these programs are extremely unlikely to be sampled. In practice, the generated programs are short and their outputs bear more resemblance with regular or context-free languages, which explains partly why transfer to the Chomsky tasks is possible at all. See Figure 5 in the Appendix for some examples.\n\n* A Markov model of order k sequentially assigns probabilities to a string of characters by looking, at step t in the sequence, at the suffix string from t-k to t. This suffix is used to lookup the model parameters to make a prediction of the next character. A variable Markov model (VMM) is a Markov model where the value of k can vary depending on the suffix. A VMM makes its prediction using a suffix tree. CTW is a variable Markov model predictor that performs Bayesian inference efficiently over all possible suffix trees as it reads and predicts the sequence. Additionally, any predictor (such as CTW) can be used as a generator, by sampling sequentially from the predictive probabilities.  \nLastly, the task on variable-order Markov sources considers data generated from tree structures. For example, given the binary tree \n```\n         Root\n  0/                \\1\nLeaf_0       Node\n\t\t0/         \\1\n        Leaf_10       Leaf_11\n```\n\nAnd given the history of data \u201c011\u201c (where 0 is the first observed datum and 1 is the last one) the next sample uses Leaf_11 (because the last two data points in history were 11) to draw the next datum using a sample from a Beta distribution with parameter Leaf_11. Say we sample a 0, thus history is then transformed into \u201c0110\u201d and Leaf_10 will be used to sample the next datum (because now the last two datapoints that conform to a leaf are 10), and so forth.  This way of generating data is very general and can produce many interesting patterns ranging from simple regular patterns like  01010101 or more complex ones that can have stochastic samples in it. Larger trees can encode very complex patterns indeed. For more information about the way we generate this data you can check the Appendix D2. \n\n**\u2026 Can you talk a bit more about data sizes, and why the results are or are not expected?\u201d** \nWe train for 500k iterations with batches of 128 sequences of length 256 in most experiments. That means we use around 16 Billion \u201ctokens\u201d to train our models. Large language models are trained using datasets sizes of the order of Trillions of tokens e.g. Mistral 7B uses about 8 Trillion tokens. Given this context, our models could still be trained further if we make them bigger, specially in the case of UTM data where we can generate at will any number of sequences. Our results on Variable Order Markov Sources are novel and somewhat reassuring since no previous work has shown the capabilities of neural networks to reach Bayes-optimal performance on such type of algorithmic universal data. The results on UTM data are a positive surprise since they demonstrate our hypothesis that practical UTM data contains useful transferable patterns to improve performance on other tasks. What we expect is that using better UTMs and training on the order of Trillions of tokens could significantly improve transfer to more \u201creal-world\u201d tasks including vision and language."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7375/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700737638969,
                "cdate": 1700737638969,
                "tmdate": 1700737638969,
                "mdate": 1700737638969,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]