[
    {
        "title": "Deceptive-NeRF: Enhancing NeRF Reconstruction using Pseudo-Observations from Diffusion Models"
    },
    {
        "review": {
            "id": "hLk1E6xeJh",
            "forum": "va9hbzIggi",
            "replyto": "va9hbzIggi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission480/Reviewer_zKFv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission480/Reviewer_zKFv"
            ],
            "content": {
                "summary": {
                    "value": "This work introduces a new method, called Deceptive-NeRF,  which leverages diffusion models to synthesize pseudo observations to improve the few-shot NeRF reconstruction. This approach first reconstructs a coarse NeRF from sparse input data, and then utilizes the coarse NeRF to render images and subsequently generates pseudo-observations based on them. Last, a refined NeRF model is trained utilizing input images augmented with pseudo-observations. A deceptive diffusion model is proposed to adeptly convert RGB images and depth maps from coarse NeRFs into photo-realistic pseudo-observations, while preserving scene semantics for reconstruction. Experiments on the synthetic Hypersim dataset demonstrate that the proposed approach is capable of synthesizing photo-realistic novel views with very sparse inputs."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper introduces an approach for the few-shot novel view synthesis that leverages diffusion models to generate pseudo-observations to provide training signals.\n\nTo generate photo-realistic pseudo-observations that faithfully preserve scene semantics and input view consistency, an RGB-D conditioned diffusion model is trained on a synthetic indoor scene dataset (Hypersim).\n\nAn ablation study is conducted to verify the design of the method, including progressive training, depth condition, image captioning, and textual inversion.\n\nResults on the Hypersim dataset show that the proposed method outperforms the existing method on the few-shot setting."
                },
                "weaknesses": {
                    "value": "The idea of introducing pseudo observations to enhance the reconstruction of few-shot Neural Radiance Fields (NeRF) is a promising concept. If the diffusion model can effectively generate pseudo observations that align with the data distribution of a given scene, it has the potential to improve the quality of the refined NeRF reconstructions.\n\nOne of the primary concerns is the generalization capacity of the proposed deceptive diffusion model. The real-world scenes' data distribution is often highly intricate and diverse. However, the diffusion model is only trained on a limited dataset consisting of 40 scenes and 2000 synthetic images from the Hypersim dataset during the second stage. As the primary experiment relies on the Hypersim dataset, which shares similarities with the training data, the method's performance on the real LLFF dataset is disappointing. In specific metrics and view-number configurations, it even falls short of the freeNeRF (note that the proposed method also uses the same frequency regularization as in freeNeRF). These outcomes indicate that the proposed approach struggles to generalize to the complexities of real-world scenes.\n\nIt would be valuable to include a comparative analysis between the generated pseudo observations and the ground-truth images. This could provide insights into the fidelity of the pseudo observations and their accuracy in replicating the real data.\n\nInformation regarding the optimization time required for scene reconstruction is crucial for understanding the method's practicality and efficiency. Including this information in the paper would be helpful for readers seeking to assess the computational demands of implementing this approach.\n\nIn Table 1, a more robust baseline for scene reconstruction might be considered, such as the monoSDF method that utilizes monocular depth and normal maps as additional sources of supervision. Comparing the proposed method's performance to such a strong baseline would provide a clearer picture of its relative merits and limitations."
                },
                "questions": {
                    "value": "- Further discussion on the generalization of the proposed methods.\n- It is important to assess the quality of the generated pseudo observations. Detailed evaluations, including visual comparisons with ground-truth data, can help validate the effectiveness of this component in improving the NeRF reconstruction.\n- Discussion for the runtime?\n- Stronger baseline for scene reconstruction.\n Please refer to weakness for details."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission480/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission480/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission480/Reviewer_zKFv"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission480/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697961289313,
            "cdate": 1697961289313,
            "tmdate": 1699668502923,
            "mdate": 1699668502923,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ct0CNMNNzL",
                "forum": "va9hbzIggi",
                "replyto": "hLk1E6xeJh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission480/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission480/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank the reviewer for regarding our idea as a \"promising concept\" and providing constructive criticism that helped us identify areas that need improvement. Below is our response to your concerns.\n\n**Generalization Capacity (Question 1).** Please refer to the discussion in the general response. We believe that our method has the potential to achieve good few-shot novel view synthesis results on any 3D scene when large-scale general 3D scene datasets are available for training. We will soon report the results of generalizing our model to other indoor datasets beyond Hypersim to demonstrate its generalization ability. \n\n**Quality of Pseudo-Observations. (Question 2)** Thank you for your thoughtful suggestion to include a comparative analysis between our generated pseudo-observations and the ground-truth images. We have been working on it and will provide these visual comparisons soon to further strengthen our arguments.\n\n**Runtime analysis (Question 3).** Please refer to the discussion in the general response and we will soon provide runtime analysis and comparison.\n\n**Stronger Baselines (Question 4).**  Thanks for pointing out our missing baselines and we'll report their results soon."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission480/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699865078163,
                "cdate": 1699865078163,
                "tmdate": 1699865078163,
                "mdate": 1699865078163,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "X4uQh5w4ew",
                "forum": "va9hbzIggi",
                "replyto": "hLk1E6xeJh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission480/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission480/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to express our gratitude once again for your precise feedback in improving our work. Please review our revised manuscript. We have further addressed your concerns as follows:\n\n**Generalization Capacity.** In Section E, we report the performance of our model, trained on the HyperSim dataset, on two additional datasets. This demonstrates that our method not only surpasses the baseline but also confirms that models trained on the HyperSim dataset are effective on other indoor datasets. As you rightly pointed out, we have emphasized in our limitations section the performance decline when models trained on HyperSim are generalized to arbitrary 3D scenes beyond indoor environments.\n\n**Runtime Analysis.** We have included a discussion about the runtime of our method, with a time breakdown. Please refer to Section C or the general response provided above for more information.\n\n\n**Quality of Pseudo-Observations.**  We have included a comparative analysis between our generated pseudo-observations and the ground-truth images in Section D of our revised manuscript. Additionally, as suggested by Reviewer QhhZ, we provide a comparison with image restoration models, along with quantitative results. We hope they will aid in understanding the behavior of the deceptive diffusion model.\n\n**MonoSDF.** Thank you for suggesting MonoSDF [1] as an additional baseline. We experimented with MonoSDF integrated into SDFStudio [2], but in our few-shot setting (5-20 input views), MonoSDF failed to provide reasonable reconstruction. This is likely due to the fact that MonoSDF, as a neural implicit surface reconstruction approach, prioritizes accurate reconstruction of the object surface and therefore requires dense observations to achieve reasonable results.\nThe comparison in Table 1 covers most of the state-of-the-art methods in the field of few-shot novel view synthesis at the time of submission. While approaches for the task of neural implicit surface reconstruction [1,3,4] are highly relevant to our task, they have a different focus. Specifically, these methods prioritize the reconstruction of object surfaces, while our focus is on photo-realistic novel view rendering directly without recovering the surface. Additionally, neural implicit surface reconstruction often requires additional inputs such as normals, depths, or masks, in addition to RGB inputs, to ensure the accuracy of the surface reconstruction, while our method does not.\nWe have revised the paper to include a discussion on MonoSDF in Section 4.1.\n\n\n**References**    \n[1] Yu et al., \u201cMonoSDF Exploring Monocular Geometric Cues for Neural Implicit Surface Reconstruction\u201d. (NeurIPS 2022)     \n[2] Yu et al., \u201cSDFStudio: A Unified Framework for Surface Reconstruction\u201d. (https://github.com/autonomousvision/sdfstudio)    \n[3] Wang et al., \u201cNeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction\u201d. (NeurIPS 2021)    \n[4] Oechsle et al., \u201cUNISURF: Unifying Neural Implicit Surfaces and Radiance Fields for Multi-View Reconstruction\u201d. (ICCV 2021)"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission480/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700725119791,
                "cdate": 1700725119791,
                "tmdate": 1700725119791,
                "mdate": 1700725119791,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vGePK8eXni",
            "forum": "va9hbzIggi",
            "replyto": "va9hbzIggi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission480/Reviewer_beZg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission480/Reviewer_beZg"
            ],
            "content": {
                "summary": {
                    "value": "This method proposed a few-shot NeRF training with pseudo samples from the diffusion models as the training corpus, and a series of training strategy that can boost the performance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The overall idea is reasonable, effective and well elaborated."
                },
                "weaknesses": {
                    "value": "1. One biggest concern of this idea is that some diffusion-synthesized samples are view-inconsistent. This method proposed a 50% filtering strategy to alleviate this issue, but I don't know whether this issue can be fully bypassed. Introducing confidence score as in NeRF-W may help.\n2. Computational cost. Training a diffusion model for 10 days to further finetune a NeRF sounds inefficient to me. Also, can this finetuned CLDM be applied to any in-the-wild NeRF reconstruction dataset?"
                },
                "questions": {
                    "value": "Beside the concerns stated above, I wonder whether you can apply your method to other popular nerf benchmars, such as DTU and LLFF? Also, what is the key limitation of your method and under which setting will this pipeline fails."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission480/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission480/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission480/Reviewer_beZg"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission480/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698704316587,
            "cdate": 1698704316587,
            "tmdate": 1699635974460,
            "mdate": 1699635974460,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Bz1oeqdDXn",
                "forum": "va9hbzIggi",
                "replyto": "vGePK8eXni",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission480/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission480/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your constructive feedback and for acknowledging our work as 'reasonable, effective, and well-elaborated'. Below is our response to your concerns: \n\n**Filtering Strategy (Weakness 1).** Based on our experience, our filtering strategy, together with our other designs, has to some extent alleviated the occurrence of view inconsistency. We appreciate the reviewer's suggestion to adopt the confidence score as in NeRF-W and will implement it as soon as possible and report the results.\n\n**Computational Cost (Weakness 2).** Please refer to our discussion in the general response. Please kindly note that the 10-day fine-tuning time was achieved using personal computer-level computing resources (a single NVIDIA GeForce RTX 3090 Ti GPU) and can be greatly improved.\n\n**Other benchmarks (Weakness 2 and Question).** Please refer to our discussion in the general response. We have included the experimental results on LLFF in the appendix, and our method outperforms most of the baselines. We will report our method's results on other indoor scene datasets to demonstrate its generalization ability."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission480/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699865041478,
                "cdate": 1699865041478,
                "tmdate": 1699865041478,
                "mdate": 1699865041478,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZRUJtYAKiJ",
                "forum": "va9hbzIggi",
                "replyto": "vGePK8eXni",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission480/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission480/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to express our gratitude once again for your constructive feedback on improving our paper. Please review our updated manuscript, which includes the following revisions based on your suggestions:\n\n**Confidence Score as in NeRF-W.** We have updated Table 2 to report the performance of a filtering strategy using the confidence score used in NeRF-W [1]. This approach yielded results comparable to our original design. Specifically, following [1]\u2019s pre-filtering step, we discarded low-quality pseudo-observations that scored below the 50% threshold in the NIMA [2] assessment.\n\n**Computational Cost.** We have discussed the computational cost of our method. Please see Section C or the general response above. \n\n**Other Benchmarks.** In response to your inquiries about \u201cCan this finetuned CLDM be applied to any in-the-wild NeRF reconstruction dataset\u201d and \u201cWhether you can apply your method to other popular NeRF benchmarks\u201d, we have evaluated our method on two additional datasets. As outlined in Section E, these evaluations demonstrate that our method not only surpasses the baseline but also confirms that models trained on the HyperSim dataset can also work effectively on other indoor datasets. In Section F, we report our method's performance on the popular NeRF benchmark LLFF. While our approach exceeds most baselines, it does exhibit a performance decline due to the domain gap from our indoor training data. Nevertheless, our method shows promise for broader generalization to diverse in-the-wild scenes, a potential that should be further realized as large-scale datasets of general 3D scenes become increasingly available.\n\n\n**References**    \n[1] Martin-Brualla et al., \u201cNeural Radiance Fields for Unconstrained Photo Collections\u201d. (CVPR 2021)     \n[2] Milanfar et al., \u201cNIMA: Neural Image Assessment\u201d. (IEEE TIP, 2018)"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission480/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700724945761,
                "cdate": 1700724945761,
                "tmdate": 1700724992423,
                "mdate": 1700724992423,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "czLYrOla2c",
            "forum": "va9hbzIggi",
            "replyto": "va9hbzIggi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission480/Reviewer_QhhZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission480/Reviewer_QhhZ"
            ],
            "content": {
                "summary": {
                    "value": "The present paper proposes a diffusion model training scheme for neural radiance field-based high-quality novel view synthesis from a small number of input views. It's core idea is to train an initial radiance field based on the small number of views, and then generate novel views in this low quality field which are subsequently enhanced using a diffusion model. The resulting higher quality views can then be used to train a higher quality neural radiance field. In terms of technical novelty, most of the substance focuses on training the enhancement diffusion model. Ideally, such training would require the availability of neural fields (both degraded and high quality) for a big number of scenes which is computationally prohibitive. Thus, the authors train the diffusion model to restore a noise corrupted input image (+ depth). The work is shown to outperform other baselines using neural fields."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* This work considers an important problem following the theme of leveraging 2d generative models for 3d. \n* It outperforms its considered baselines for few-shot novel view synthesis"
                },
                "weaknesses": {
                    "value": "* Some of the writing requires improvements, for example in related work the first paragraph ends with the statement that NeRFs require numerous images which is followed by an entire paragraph falsifying that very statement presenting recent advances on few-sample learning with NeRFs. There are also some language issues.\n* The evaluation seems insufficient. Given that the diffusion model has been trained on a denoising task to reduce computational burden, it would seem meaningful to evaluate this concept on some existing image restoration networks in comparison to the proposed fine-tuning of a diffusion model.\n* the approach seems to be computationally burdensome as its iterative variant requires a sequence of radiance field learning processes."
                },
                "questions": {
                    "value": "* what is meant by \"we use a linearly increasing frequency mask\"?\n* how is the depth map obtained from the nerf?\n* It is surprising to me that the optimization scheme does not result in inconsistency issues. Could the authors provide some intuition for why there is no issue?\n* What is meant by \"We optimize a shared latent text embedding s [...]\"?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission480/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699252770912,
            "cdate": 1699252770912,
            "tmdate": 1699635974358,
            "mdate": 1699635974358,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1auQSS2ldu",
                "forum": "va9hbzIggi",
                "replyto": "czLYrOla2c",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission480/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission480/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate your thoughtful comments and the attention to detail you gave to our paper.  Below is our response to your concerns.\n\n**Writing issue (Weakness 1).** Thanks for pointing that out. We will improve writing to ensure logical flow and minimize grammatical and vocabulary errors.\n\n**Image Restoration Networks (Weakness 2).** Comparing our model with the fine-tuning process with existing image restoration networks is a reasonable approach to demonstrate the effectiveness of our proposed method. We will conduct the corresponding experiments and report the results as soon as possible.\n\n**Computational Burden (Weakness 3).** Please refer to the discussion in the general response. I will soon provide more experimental analysis on the runtime.\n\n**Frequency Mask (Question 1).** When we refer to \"we use a linearly increasing frequency mask,\" we are describing the utilization of a technique that resembles the frequency regularization in [1]. To be specific we initiate training without any positional encoding and progressively enhance the visibility of certain frequency components as the training process advances. This strategy is employed to prevent overfitting to high-frequency elements within the input data. We will clarify this in the modified version to assist readers in understanding.\n\n**Depth from NeRFs (Question 2).** We compute the depth estimation of a pixel as\n$\n   \\hat{z}(\\mathbf{r}) = \\sum_{k=1}^{K} w_k t_k,\n$\n   where $t_k$ are the sampling locations, and the rendering weights\n$\n   w_k = T_k (1 - \\exp(-\\sigma_k \\delta_k)).\n$\n $\\sigma_k$ is the volume density value inferred from the NeRF MLP. $\\delta_k = t_{k+1} - t_k$ is the distance between adjacent sampled points, and \n$\n   T_k = \\exp\\left(-\\sum_{i=1}^{k-1} \\sigma_i \\delta_i\\right).\n$\n\n**View consistency (Question 3).** In our pipeline, the diffusion model does not generate a new view image from scratch, which can easily lead to inconsistent views (as seen in many 3D diffusion generation efforts). Instead, it takes as input an image rendered from a coarse NeRF and refines it to remove artifacts. The coarse image processed by the diffusion model is consistent with other views in nature and the view consistency can still be maintained after diffusion\u2019s refinement. Moreover, our progressive strategy involves gradually moving away from the input view to synthesize a new view. As a result, the artifacts encountered by the diffusion model at any given time are limited, reducing the difficulty of the refinement task it faces.\n\n**Shared text embedding (Question 4).** As described in [2], we optimize for a new embedding vector within the embedding space of a frozen text-to-image model. This vector represents elements that appear in each input image but cannot be directly described using natural language, such as the stylistic information of a room. This embedding can be treated like any other word and is utilized to compose novel textual queries for our generative models.\n\n \n**References**    \n[1] Yang et al., \u201cFreeNeRF: Improving Few-shot Neural Rendering with Free Frequency Regularization\u201d.     \n[2] \u200b\u200bGal et al., \u201cAn Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion\u201d."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission480/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699864999386,
                "cdate": 1699864999386,
                "tmdate": 1699864999386,
                "mdate": 1699864999386,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nOXATokADa",
                "forum": "va9hbzIggi",
                "replyto": "1auQSS2ldu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission480/Reviewer_QhhZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission480/Reviewer_QhhZ"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for this response. I will wait for the new updates and then decide if and how much I can increase my scores. Also, it would be cool, if you could use latexdiff or at least some color emphasis in the updated version of the paper to visualize which parts changed in response to the review."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission480/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700238231995,
                "cdate": 1700238231995,
                "tmdate": 1700238231995,
                "mdate": 1700238231995,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]