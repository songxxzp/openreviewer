[
    {
        "title": "DreamClean: Restoring Clean Image Using Deep Diffusion Prior"
    },
    {
        "review": {
            "id": "x4O2VrxW7N",
            "forum": "6ALuy19mPa",
            "replyto": "6ALuy19mPa",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission331/Reviewer_6gxh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission331/Reviewer_6gxh"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a novel generic image restoration (IR) approach called DreamClean that recovers degraded images back to high-quality reconstructed images without heavily relying on or knowing degraded models. The DreamClean method consists of two essential steps to restore corrupt images (e.g., noisy images, low-resolution images, or artifacts generated by JPEG compression)to the reconstructed images. First, the method utilizes a pre-trained diffusion model to encode the corrupt image images to latent embeddings in the diffusion models. The paper then presents a novel sampling strategy, Variance Preservation Sampling (VPS), mimicking a process of generating clean images to guide low-probability latent toward nearby high-probability regions for high-quality image synthesis. The contribution of this paper is threefold: 1. the paper introduces a novel aspect of restoring clean images without knowing prior degraded model information. The proposed method could be an IR technique for image restoration applications with variations of camera devices. 2. the proposed method could be generalized to diverse degraded images without training multiple neural network models. 3. the paper evaluates the proposed method on multiple datasets and types of degraded images. The experiment shows DreamClean achieves competitive quantitative results compared with other IR methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The strengths of this paper are as follows:\n\n1. This paper proposes a generic IR method that can tackle corrupt images generated by variations of degraded models without training multiple network models. The paper provides theoretical and empirical proof to demonstrate the DreamClean method can synthesize high-quality clean images given the corrupt images.\n\n2. The paper proposed a novel sampling strategy, the VPS method, in diffusion-based models to improve the chance of yielding clean images. The paper demonstrates the capability of VPS through theoretical proof and empirical results. The experimentation shows that DeamClean achieves higher quantitative results under different image quality metrics.\n\n3. This paper contains comprehensive quantitative results on image quality comparisons for distinct IR methods. The DreamClean method outperforms other methods on most degraded models on different datasets."
                },
                "weaknesses": {
                    "value": "This paper contains two weaknesses in terms of the proposed method:\n\n1. The DreamClean method can synthesize clean images given the corrupt images; however, the proposed method does not consider consistency between the corrupt images and the reconstructed clean images due to a lack of prior degraded models. The proposed method can fail some restored images by synthesizing unexpected clean images.\n\n2. The proposed method utilizes the pre-trained diffusion model on a particular large-scale dataset (i.e., ImageNet) to embed the corrupt images to the latent embeddings. Then, the method applies VPS to sample clean images during the sampling phase in diffusion models. Due to using the pre-trained model, the proposed method can generate unexpected images."
                },
                "questions": {
                    "value": "There is a question I would like to ask to clarify all doubts in the paper:\n\n1. In section 3.3, which is quantitative experiments, the paper uses 100 NFEs rather than a configuration in the original paper for DDRM. Does it affect the quantitative comparisons because 100 NFEs may not be the configuration for DDRM?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission331/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission331/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission331/Reviewer_6gxh"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission331/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698307861579,
            "cdate": 1698307861579,
            "tmdate": 1699635959981,
            "mdate": 1699635959981,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dfifIZRryp",
                "forum": "6ALuy19mPa",
                "replyto": "x4O2VrxW7N",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission331/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission331/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 6gxh"
                    },
                    "comment": {
                        "value": "**Q:** The DreamClean method can synthesize clean images given the corrupt images; however, the proposed method does not consider consistency between the corrupt images and the reconstructed clean images due to a lack of prior degraded models. The proposed method can fail some restored images by synthesizing unexpected clean images.\n\n**A:** We agree that the consistency with corrupt images is important for IR problem. In this work, we also consider the consistency issue carefully but under much more challenging setting: the degradation model is unknown. Based on the fact that ODE sampling is approximately invertible, the inverted latents should contain desirable information about the input image. Hence, we leverage DDIM inversion to achieve approximate consistency. We admit that ODE inversion cannot ensure strict consistency with the input image without knowing the degradation model. It is an open problem and we are excited about the coming future that in-depth reseaches that can solve this problem thoroughly or mostly.\nBesides, orthogonal to previous methods, when the degradation model is known, DreamClean can also accomplish more faithful image restoration.\n\n**Q:** The proposed method utilizes the pre-trained diffusion model on a particular large-scale dataset (i.e., ImageNet) to embed the corrupt images to the latent embeddings. Then, the method applies VPS to sample clean images during the sampling phase in diffusion models. Due to using the pre-trained model, the proposed method can generate unexpected images.\n\n**A:** The concern that generating unexpected content because of the pre-trained model is reasonable. Like other generative methods for IR [1], our method also inherits the in-built stochasticity of diffusion models, which benefits diversity but with the risk of yielding undesirable results. As show in limitation (Sec. 5), diffusion models pretrained on ImageNet struggle to generate daily scenes such as altitude view of buildings. We figure out this problem clearly in limitation section (Sec. 5). It is an open problem that deserves further in-depth inverstigations.\n\n**Q:** In section 3.3, which is quantitative experiments, the paper uses 100 NFEs rather than a configuration in the original paper for DDRM. Does it affect the quantitative comparisons because 100 NFEs may not be the configuration for DDRM?\n\n**A:** We use $100$ NFEs for DDRM to keep the comparison in the context of the same or similar computation. In fact, as revealed in Table 1, DDRM with $100$ NFEs produces slightly better results than that with original $20$ NFEs. \n\n**Table 1: DDRM performace with different NFEs.**\n|CelebA|PSNR$\\uparrow$|SSIM$\\uparrow$|LPIPS$\\downarrow$|NFEs$\\downarrow$|\n|:----|:----:|:----:|:----:|:----:|\n|DDRM|29.16|0.83|0.11|20|\n|DDRM|29.21|0.83|0.09|100|\n\n|ImageNet|PSNR$\\uparrow$|SSIM$\\uparrow$|LPIPS$\\downarrow$|NFEs$\\downarrow$|\n|:----|:----:|:----:|:----:|:----:|\n|DDRM|25.22|0.71|0.32|20|\n|DDRM|25.67|0.73|0.30|100|\n\n[1] Zero-Shot Image Restoration Using Denoising Diffusion Null-Space Model. ICLR 2023."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission331/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700358347952,
                "cdate": 1700358347952,
                "tmdate": 1700358347952,
                "mdate": 1700358347952,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vS67nJSTEI",
                "forum": "6ALuy19mPa",
                "replyto": "dfifIZRryp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission331/Reviewer_6gxh"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission331/Reviewer_6gxh"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for author's response. The author's response has addressed my concerns."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission331/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700645161312,
                "cdate": 1700645161312,
                "tmdate": 1700645161312,
                "mdate": 1700645161312,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "or9zcWoIrr",
            "forum": "6ALuy19mPa",
            "replyto": "6ALuy19mPa",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission331/Reviewer_HBur"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission331/Reviewer_HBur"
            ],
            "content": {
                "summary": {
                    "value": "This paper works on unsupervised image restoration using the pre-trained diffusion models as deep diffusion prior. It relaxes the linear degradation model and extends to non-linear and bad weather degradation. The faithfulness of the image restoration is achieved via the rich image prior embedded in the pre-trained models and DDIM inversion. The realness is satisfied through the proposed variance preservation sampling, where the latents are driving to the high probability set. Experiments for classical super-resolution, deblurring, colorization, non-align JPEG compression, and bad weather restoration show the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper works on an important problem: unsupervised image restoration.\n- The paper proposes a method to drive the latent variables to a high probability set to generate high-quality restoration results and provide theatrical analysis.\n- The proposed method generalizes to non-linear degradation and bad weather as well as applies to diffusion models with latent space, e.g., Stable Diffusion.\n- Well written and easy to follow."
                },
                "weaknesses": {
                    "value": "- For the proposed variance preservation sampling (VPS), the latent variables fall into the high probability set with a strong assumption that $M$ is sufficiently large (Theorem 2.2). However, $M$ is empirically set to $1$ in this paper, and there is no detailed discussion about this important parameter.\n- The qualitative results are only shown for the proposed method, and there are no visual comparisons to existing methods.\n- For the bad weather degradation, there are no quantitative measurements. The only evidence is a set of visual examples in Fig. 2.\n- The ablation study is only conducted on one type of degradation and is insufficient.\n- The paper should discuss and compare with more recent blind image restoration works, e.g., GDP.\n\nFei, Ben, et al. \"Generative Diffusion Prior for Unified Image Restoration and Enhancement.\" CVPR. 2023."
                },
                "questions": {
                    "value": "- Is the proposed method sensitive to some parameters, e.g., $\\gamma$ and strength?\n- The paper should give more visual illustrations for the sampling algorithm of different timesteps to understand the effectiveness of the two steps in the proposed VPS.\n- For the bad weather, the training of diffusion models often encounters such images in high image quality. It is not evident that the images with bad weather are categorized as degenerated or clean from the perspective of the diffusion prior.\n- The authors should provide more implementation details.\n- It is better to explain $x$ and $y$ in Fig. 2."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission331/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission331/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission331/Reviewer_HBur"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission331/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698673176237,
            "cdate": 1698673176237,
            "tmdate": 1699635959884,
            "mdate": 1699635959884,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hP58REmHA4",
                "forum": "6ALuy19mPa",
                "replyto": "or9zcWoIrr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission331/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission331/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer HBur"
                    },
                    "comment": {
                        "value": "**Q:** For the proposed VPS, the latent variables fall into the high probability set when M is sufficiently large (Theorem 2.2). However, M is empirically set to 1 in this paper, and there is no detailed discussion about this important parameter.\n\n**A:**  $M=1$ works well because VPS is conducted for each timestep. This does not conflicts with Theorem 2.2 which states that when $M$ is sufficiently large, latents converges to a High Probability Set for certain timestep $t$. In other words, **the correcting effect of VPS can be accumulated along the DDIM sampling trajectory**. We have conducted noisy SR experiments on CelebA with different $M$ and the tendency of performance is plotted in Figure A2. The results suggests that increasing $M$ cannot significantly boost performance but with more compute. Therefore, we set $M=1$ for most cases.\n\n**Q:** There are no visual comparisons to existing methods.\n\n**A:** We follow the reviewer's advice to add visual comparisons with existing methods. Figures A7 and A8 presents visual comparisons of noisy SR and JPEG artifacts correction respectively. Figure A9 shows comparisons of $32$ SR with $\\sigma=0.1$ using Stable Diffusion XL. It can be found that without knowing degradation model, DreamClean (ours in Figures A7, A8 and A9) can improve images' quality and produce visually pleasing results. When equipped with specific degradation, DreamClean (ours* in Figure A7) yields more faithful results.\n\n**Q:** For the bad weather degradation, there are no quantitative measurements. The only evidence is a set of visual examples in Fig. 2.\n\n**A:** We follow the reviewer's advise and include quantitative scores of image deraining on Rain100L. Table 1 shows the quantitative results. Baseline refers to original unprocess data. It can be found that without any specific training, DreamClean can effectively process complex rainy degradation.  We also add more bad weather corrupted cases. Figures 2 and A12 present more bad weather degradation cases. Besides, Figure A1 presents more real-world bad weather cases.\n\n**Table 1: Quantitative results on Rain100L.**\n|Rain100L|PSNR$\\uparrow$|SSIM$\\uparrow$|LPIPS$\\downarrow$|\n|:----|:----:|:----:|:----:|\n|Baseline|28.53|0.89|0.06|\n|Ours|28.68|0.92|0.04|\n\n**Q:** The ablation study is only conducted on one type of degradation.\n\n**A:** \nWe follow the suggestion to perform ablation experiments on more degradation types. The below table shows that our VPS achieves the best score compared with other schemes. The reason is that VPS optimizes latents to High Probability Set, which conforms with the sampling dynamics of diffusion models. These results are consistent with noisy SR cases.\n\n**Table 2: Ablation study on more degradation types.**\n|Schedule|Deblurring(Gauss) PSNR$\\uparrow$/SSIM$\\uparrow$/LPIPS$\\downarrow$|Inpainting PSNR$\\uparrow$/SSIM$\\uparrow$/LPIPS$\\downarrow$|\n|:----|:----:|:----:|\n|0|32.20/0.85/0.12|24.24/0.83/0.10|\n|$\\sqrt{2\\gamma(1-\\bar{\\alpha}_t)}$|32.68/0.90/0.10|24.67/0.85/0.09|\n|Ours|32.71/0.91/0.09|24.71/0.87/0.07|\n\n**Q:** The paper should discuss and compare with more recent blind image restoration works, e.g., GDP.\n\n**A:** We follow the reviewer's suggestion to add comparison with GDP. For reviewer's convinience, we copy the quantitative results of noisy $4\\times$ super-resolution on imagenet and CeleBA.\n\n**Table 3: Quantitative comparison with GDP.**\n|CeleBA|PSNR$\\uparrow$|SSIM$\\uparrow$|LPIPS$\\downarrow$|NFEs$\\downarrow$|\n|:----|:----:|:----:|:----:|:----:|\n|GDP|24.38|0.71|0.15| 1000|\n|Ours| 27.23|0.77 |0.12|90|\n|Ours*| 30.19| 0.84 | 0.08|60|\n\n|Imagenet|PSNR$\\uparrow$|SSIM$\\uparrow$|LPIPS$\\downarrow$|NFEs$\\downarrow$|\n|:----|:----:|:----:|:----:|:----:|\n|GDP|24.33|0.67|0.39|1000|\n|Ours|24.31|0.67|0.40|90|\n|Ours*|25.84|0.74|0.23|60|\n\n**Q:** Is the proposed method sensitive to some parameters, e.g., step size $\\gamma$ and strength $\\tau$?\n\n**A:** The effective range of $\\gamma$ and $\\tau$ is relatively loose. Empirically, we find that $\\gamma \\in [0.01, 0.1]$ and strength $\\tau \\in [300, 500]$ works well in most cases. To validate this, we conduct noisy SR experiments on CelebA with a range of $\\gamma$ and strength $\\tau$. As shown in Figures A3 and A4, our method works well when $\\gamma$ and $\\tau$ are in the above interval."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission331/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700358096572,
                "cdate": 1700358096572,
                "tmdate": 1700358096572,
                "mdate": 1700358096572,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pC5HHnAC17",
                "forum": "6ALuy19mPa",
                "replyto": "oLNLva2EGj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission331/Reviewer_HBur"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission331/Reviewer_HBur"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Reviewer HBur"
                    },
                    "comment": {
                        "value": "I sincerely thank the authors for the elaborate responses to my questions. Nevertheless, I still have some concerns. \nFor the statement \"the correcting effect of VPS can be accumulated along the DDIM sampling trajectory,\" is there any theoretical analysis?\nIt is better to include GDP for visual comparison.\nFor Figure A1 (a), the hallucination, deviation from the original contents, in bad weather is more evident than in other applications. Is there any trade-off between the restoration and regeneration?"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission331/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700653783644,
                "cdate": 1700653783644,
                "tmdate": 1700653783644,
                "mdate": 1700653783644,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Oc5Gqh7yGG",
                "forum": "6ALuy19mPa",
                "replyto": "or9zcWoIrr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission331/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission331/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer HBur"
                    },
                    "comment": {
                        "value": "Thanks for your feedback! Below are our detailed responses.\n\n**Q:** The correcting effect of VPS.\n\n**A:** The form of VPS: $y_t^m = y_t^{m-1}+\\eta_l\\nabla\\log p_t(y_t^{m-1})+\\eta_g\\epsilon_g^m$. We consider the degraded input consists of clean and corrupted components.  We assume that it is the corrupted component that causes the low probability of latents and the amount of corrupted information remains constant. \nFor arbitrary $t$, the first term $\\eta_l\\nabla\\log p_t(y_t^{m-1})$ aims to increase its probability by removing low-probability corrupted component and the second term $\\eta_g\\epsilon_g^m$ adds a finely tuned amount of Gaussian noise to ensure that $y_t$ adheres to the formulation of diffusion models.\nIt is worth noting that the addition of fresh Gaussian noise does not contribute to an increase in the level of corrupted information. This is because the amount of noise is carefully adjusted by $\\eta_g$ to ensure that it can be totally removed after the sampling process of diffusion models.\nTherefore, in essence, VPS exchanges low-probability corrupted component for Gaussian noise, The amount of exchange is adjusted to match the dynamics of diffusion models. As this exchange occurs along the ODE trajectory and the total amount of corrupted information remains constant, the correcting effect accumulates gradually. \nTo support this, please refer to visualization in Figure A5. Comparing latents of the interval $t\\in [240, 400]$ of Figure a (without VPS) and Figure b (with $M=1$ VPS), we can observe the accumalated correcting effect. That is, at $t=400$, artifacts are still apparent after VPS. After accumulation along the sampling trajectory, at $t=240$, the artifacts are significantly reduced and the latent is like the clean image perturbed with Gaussian noise, which conforms the formulation of diffusion models. Additionally, we provide a theoretical analysis in Sec. A. 12 (Page 21).\n\n\n**Q:** Include GDP for visual comparison.\n\n**A:** We follow the advice to include GDP for visual comparison.\n\n\n**Q:** The hallucination in bad weather.\n\n**A:** The more hallucination in real-world bad weather cases can be explained that the degradation model is more complex and unknown. Therefore, the coupling of clean and corrupted component of the input image becomes more intricate in such scenarios. As we discuss in Limitation section (Secion 5, Page 9), DreamClean cannot guarantee strict faithfulness, i.e., without the knowledge of the degradatin model, DreamClean cannot provide complete separation between the clean and corrupted components of the input image.\nDreamClean provides a ''soft'' separation from the probabilistic prespective: the corrupted component should be evaluated as low probability using the metric of the generative prior learned from clean images. Therefore, the hallucination emerges because DreamClean resamples the low-probability component according to its generative prior learned from clean training images. The mentioned trade-off exists and can be implemented by the step size $\\gamma$. In the limit that $\\gamma=0$, VPS vanishes and DreamClean reproduces the input image using the DDIM inversion. As $\\gamma$ increases, DreamClean regenerates the low-probability component."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission331/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733544620,
                "cdate": 1700733544620,
                "tmdate": 1700733667889,
                "mdate": 1700733667889,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BGUWWIw2dW",
            "forum": "6ALuy19mPa",
            "replyto": "6ALuy19mPa",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission331/Reviewer_NQCS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission331/Reviewer_NQCS"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a novel unsupervised diffusion based approach called DreamClean, which does not rely on specific degradation types, for Image Restoration tasks. The superiority lies in restoring clean images without specific finetuning or assuming known degradation. Specifically, DreamClean exploits the approximately inversible property of DDIM to ensure faithfulness. For realness, the paper introduces Variance Preservation Sampling (VPS) to guide latents to nearby high probability region which conforms statistics of the pretrained diffusion model to produce high-quality images. The paper gives a solid theoretical support for the convergence of VPS. The experiments and analysis are sufficient and the results seems promising."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- The paper gives a simple but insightful conclusion for the existing IR methods and points out the advantage of unsupervised method for generalization. Based on this, DreamClean is proposed to mitigate the reliance on the underlying degradation model or data-specific finetuning, which is critical to improve the generalization and the practical. \n- DreamClean can improve image quality using pre-trained diffusion models without finetuning or assuming specific degradation formulation. This method fills this gap in IR field. The results on various degradation types and latent diffusion model seems to support its strong robustness. \n- The analysis of the paper is mathematically complete. The paper constructs a high probability set according to the diffusion\u2019s statistical characteristic and proves that latents will converge to the high probability set under VPS. This perspective is interesting and novel.\n- The experiments are sufficient enough to support the claims and the results are promising. In particular, the result in Figure 1 has much higher visual quality compared with previous methods. The paper validates the efficacy on various IR tasks including single and multiple degradation cases, and well compatibility with latent diffusion models."
                },
                "weaknesses": {
                    "value": "- The proposed method leverages ODE inversion to ensure faithfulness with motivation that ODE can approximately reconstruct the input image. Given that, why is ODE inversion needed rather than directly conduct VPS on the input image y?\n- The authors should explain the reason that fresh noise is need in VPS. In other words, without the noise, VPS returns latents with locally maximal probability density. Why do higher-density latents perform inferior compared to latents with noise?"
                },
                "questions": {
                    "value": "- In section 3.2, the authors should give a more detailed interpretation about  log\u2061(q(x_t |x_0)) can be alternative score for log\u2061(p_\u03b8 (x_t))."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission331/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698694025517,
            "cdate": 1698694025517,
            "tmdate": 1699635959818,
            "mdate": 1699635959818,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BUWHEtKN8Y",
                "forum": "6ALuy19mPa",
                "replyto": "BGUWWIw2dW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission331/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission331/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer NQCS"
                    },
                    "comment": {
                        "value": "**Q:** Why is ODE inversion needed rather than directly conduct VPS on the input image y?\n\n**A:** ODE inversion indeed perturbs the data with noise. As revealed in Song et at [1], perturbing data can effectively mitigate the issue of inaccurate score estimation in low data density regions. With progressively decreasing noise, high-quality images can be generated. To further support this,  experiments of noisy SR based on CelebA are conducted. Table 1 reveals that inversion provides more accurate score estimation, thereby boosting the final IR performance.\n\n**Table 1: Performance tendency with different ODE inversion strength.**\n|$\\tau$|10|100|300|500|\n|:----|:----:|:----:|:----:|:----:|\n|PSNR$\\uparrow$|23.65|24.24|27.23|27.25|\n|SSIM$\\uparrow$|0.51|0.57|0.77|0.77|\n\n**Q:** The authors should explain the reason that fresh noise is need in VPS. Why do higher-density latents perform inferior compared to latents with noise?\n\n**A:** For certain timestep $t$, according to diffusion's formulation, latents should locate in the High Probability Set. As we state in Theorem 2.2, the amount of fresh noise is vital to ensure that the corrected latents is driven to the High Probability Set. Therefore, fresh noise is needed to conform the sampling dynamics of diffusion models, which produces high-quality images. Besides, as shown in Figure 8, without fresh noise ($\\eta_g=0$), many details are discarded in the generated image.\n\n[1] Generative modeling by estimating gradients of the data distribution. NeurIPS 2019."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission331/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700356704930,
                "cdate": 1700356704930,
                "tmdate": 1700356704930,
                "mdate": 1700356704930,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6fSrkdXX3x",
            "forum": "6ALuy19mPa",
            "replyto": "6ALuy19mPa",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission331/Reviewer_aVXX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission331/Reviewer_aVXX"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents DreamClean, a training-free method for high-fidelity image restoration without prior knowledge of degradation. DreamClean embeds the degraded image into pre-trained diffusion models, uses Variance Preservation Sampling (VPS), and outperforms previous methods, especially in challenging tasks without degradation priors."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The innovation is impressive, providing a fresh perspective on the field of image restoration\u3002"
                },
                "weaknesses": {
                    "value": "1. Insufficient experiments. The performance needs to be tested on a broader range of real-world degradation data. The data used in the paper are all synthetic, which lacks convincing power.\n2. The methods for comparison are insufficient. For example, other training-free methods like 'Generative Diffusion Prior for Unified Image Restoration and Enhancement' are also available."
                },
                "questions": {
                    "value": "Refer to weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission331/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission331/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission331/Reviewer_aVXX"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission331/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698759114307,
            "cdate": 1698759114307,
            "tmdate": 1699635959749,
            "mdate": 1699635959749,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KkyvMfq4qN",
                "forum": "6ALuy19mPa",
                "replyto": "6fSrkdXX3x",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission331/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission331/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer aVXX"
                    },
                    "comment": {
                        "value": "**Q:** The performance needs to be tested on a broader range of real-world degradation data.\n\n**A:** We follow the reviewer's suggestion to add more experiments on real-world degraded images. First, we evaluate quantitative performance of image denoising on the real-world SIDD dataset [2]. For comparison, we include two baselines: the scores of input noisy image and the classic denoising method BM3D [1]. Table 1 reveals the efficacy of DreamClean in real-world image denoising ($+8.26$ dB compared with Input).\nBesides, we also extend DreamClean to more real-world applications, ranging from real-world image deraining, desnowing and old photo restoration and real-world image denoising. As show in Figure A1, DreamClean can process complex real-world corrupted images. We have added related content in Sec. A.3.\n\n**Table 1: Quantitative results of real-world image denoising on SIDD.** \n|Method|PSNR$\\uparrow$|SSIM$\\uparrow$|LPIPS$\\downarrow$|\n|:----|:----:|:----:|:----:|\n|Input|23.66|0.35|0.58|\n|BM3D|25.65|0.68|N/A|\n|Ours|31.92|0.76|0.23|\n\n**Q:** DreamClean should compare with recent training-free method GDP.\n\n**A:**: We follow the reviewer's advice and add comparison with GDP. For reviewer's convinience, we copy the quantitative results of noisy SR on Imagenet and CeleBA.\n\n**Table 2: Quantitative comparison with GDP.**\n|CeleBA|PSNR$\\uparrow$|SSIM$\\uparrow$|LPIPS$\\downarrow$|NFEs$\\downarrow$|\n|:----|:----:|:----:|:----:|:----:|\n|GDP|24.38|0.71|0.15|1000|\n|Ours|27.23|0.77|0.12|90|\n|Ours*|30.19|0.84|0.08|60|\n\n|Imagenet|PSNR$\\uparrow$|SSIM$\\uparrow$|LPIPS$\\downarrow$|NFEs$\\downarrow$|\n|:----|:----:|:----:|:----:|:----:|\n|GDP|24.33|0.67|0.39|1000|\n|Ours|24.31|0.67|0.40|90|\n|Ours*|25.84| 0.74|0.23|60|\n\n[1] Image Denoising by Sparse 3-D Transform-Domain Collaborative Filtering. TIP 2007.\n\n[2] A High-Quality Denoising Dataset for Smartphone Cameras. CVPR 2018."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission331/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700356578953,
                "cdate": 1700356578953,
                "tmdate": 1700356578953,
                "mdate": 1700356578953,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qSDkjVtq4a",
                "forum": "6ALuy19mPa",
                "replyto": "6fSrkdXX3x",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission331/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission331/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Eager to hearing from you!"
                    },
                    "comment": {
                        "value": "Dear Reviewer aVXX,\n\nThe ICLR rebuttal discussion period is near the end. Does our response successfully address all your concerns? If so, would you please raise your rating? Your support is really important to us. If not, do please tell us your remained concerns. We are eager to discuss them with you.\n\n\nBest wishes,\n\nICLR Submission 331 Authors."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission331/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700641692218,
                "cdate": 1700641692218,
                "tmdate": 1700642023639,
                "mdate": 1700642023639,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]