[
    {
        "title": "Improved Algorithms for Replicable Bandits"
    },
    {
        "review": {
            "id": "SHsQTuahh7",
            "forum": "pA4s793lcB",
            "replyto": "pA4s793lcB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5545/Reviewer_apnD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5545/Reviewer_apnD"
            ],
            "content": {
                "summary": {
                    "value": "This paper designs replicable algorithms for multi-armed bandits and linear bandits, i.e., with a fixed inner random variable, the vector of the pulled arms will remain the same with high probability, even if the outcomes of arms are also random. The authors adapt the RASMAB algorithm framework in Esfandiari et al. (2023), and modify the way of eliminating arms to get new algorithms REC and RSE. In REC, the algorithm only tries to eliminate all the arms in one single phase. This leads to an $O(\\sum_i {\\Delta_i \\log T \\over \\Delta_{\\min}^2 \\rho^2})$ regret upper bound, which is better than existing algorithms when $\\Delta_{\\min} \\ge \\Delta_i/K$ for all arm $i$. Then the authors mix REC and RASMAB to get RSE, which achieves a regret upper bound that equals the minimum of the regret upper bounds of REC and RASUCB. As for the linear bandits case, the authors also provide an RLSE algorithm. Finally, they use some simulation results to demonstrate the effectiveness of their algorithms."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is clearly written, and there are some new regret upper/lower bounds."
                },
                "weaknesses": {
                    "value": "My main concern is that the writing of proofs is not good enough. In fact, I only check one proof in the appendix (Theorem 5), but I find it hard to understand some of the steps.\n\nSpecifically, Eq. (75) and Eq. (76) only say that $P_{-\\Delta, U}[N_2(T) \\ge N_C] \\ge {1\\over 4}$ and $P_{\\Delta, U}[N_2(T) \\ge N_C] \\ge {3\\over 4}$. This cannot ensure that there exists some $r \\in [-\\Delta, \\Delta]$ such that $P_{r, U}[N_2(T) \\ge N_C] = {1\\over 2}$. For example, perhaps $P_{r, U}[N_2(T) \\ge N_C] = 1$ for all $r \\in [-\\Delta, \\Delta]$.\n\nThere are also many typos in the proof (which makes it more complicated to understand the steps), e.g., in the line after Eq. (75), I think it should be $Regret_{-\\Delta,U}(T) \\ge N_2(T)\\Delta$ but not $Regret_{-\\Delta,U}(T) \\ge N_2(T)\\Delta/4$, and it should be $N_C \\le {T\\over 2}$ but not $N_2(T) \\le {T\\over 2}$. \n\nI really suggest the authors revise their proofs to make sure that they are correct and easy to understand. \n\nAlso, I think some of the proof sketches should be given in the main text.\n\nBesides, both the analysis and experiments show that REC (or RSE) does not dominate RASMAB, and I am also wondering the experiments of performance comparison between RLSE and existing benchmarks. \n\n=============================\n\nI would like to raise my score if my concern on the proof is addressed.\n\n\n=============================\n\nThe proof after revision looks clear now. I change my score to 6."
                },
                "questions": {
                    "value": "See the above \"Weaknesses\""
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5545/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5545/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5545/Reviewer_apnD"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5545/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698209031623,
            "cdate": 1698209031623,
            "tmdate": 1700451628214,
            "mdate": 1700451628214,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Dgkg0OgSp7",
                "forum": "pA4s793lcB",
                "replyto": "SHsQTuahh7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5545/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5545/Authors"
                ],
                "content": {
                    "title": {
                        "value": "On the error of the lower bound proof"
                    },
                    "comment": {
                        "value": "Thank you for your careful reading and for correctly pointing out our mistake. We fixed the proof in the revision by defining $N_C$ to be the minimum of a set of integers (Appendix E, changes are colored red). Let us know if this is unclear.\n\n> Also, I think some of the proof sketches should be given in the main text.\n\nThank you for your suggestion. We could not put them due to page limitation."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5545/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700186623882,
                "cdate": 1700186623882,
                "tmdate": 1700186623882,
                "mdate": 1700186623882,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CIuSi9xFRG",
            "forum": "pA4s793lcB",
            "replyto": "pA4s793lcB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5545/Reviewer_Hx4P"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5545/Reviewer_Hx4P"
            ],
            "content": {
                "summary": {
                    "value": "This work analyzed the replicable bandits. The replicable bandit algorithms were first analyzed by Esfandiari et al. (2023). This work proposed the REC and RSE algorithm to solve the regret minimization problem in the standard replicable bandits and also studied the linear setting. It provided theoretical analysis on for the proposed algorithm and numerical comparison among algorithms.\n\n\n==========\n\nThanks for response from author(s). I prefer to keep my score so far."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is generally well organized and easy to follow.\n1. The problem is clearly formulated.\n2. Numerical experiments are conducted to evaluate the performance of algorithms.\n1. This work provides extensive study on the replicable bandits."
                },
                "weaknesses": {
                    "value": "Title:\n1. The title of this work starts with 'Improved algorithms'. I doubt whether it is an appropriate title. I feel that this work is more likely to be an extensive study on replicable bandits.\n    1. According to Table 1, the REC and RSE algorithms are only better than the existing RASMAB in the instances where $K^2/\\Delta^i$ is larger than $\\Delta^2_i/\\Delta$. In other cases, the order of distribution-dependent upper bounds of these three algorithm seem to be the same.\n    1. Numerical results imply that RASMAB algorithm outperforms the REC and RSE algorithms in Model 1. I appreciate some explanation regrading this point.\n\nApplication:\n1. I understand it is a follow-up work on the topic of replicable bandits. However, if the aim is to handle situations where we have simply vague data, I think we can also apply differentially private stochastic bandits (https://proceedings.mlr.press/v180/hu22a/hu22a.pd). May you clarify the difference between application of these two settings? What are their individual strengths/weaknesses?\n\nContributions:\n1. The distribution-independent bound is only provided for the RSE algorithm. However, it is usually not so difficult to derive a distribution-independent bound for the other algorithms when the distribution-dependent ones already exist. I suggest the author(s) to provide distribution-independent bounds for other algorithms and a comparison or explain the analytical challenge.\n1. As the lower bound for the standard setting is provided, may the author(s) discuss the gap between upper and lower bounds?\n1. I appreciate the efforts to study the linear setting, may the author(s) also provide a lower bound (or explain the difficulty)?\n1. As the linear algorithm is proposed for an instance with a large number of arms, may the author(s) compare(s) the performance of linear and nonlinear algorithms in a numerical experiment with a large $K$?\n1. Section 1.2 claimed that 'we introduce the Replicable Successive Elimination (RSE) algorithm whose regret bound is the minimum\nof those of REC and the existing algorithms.'\n    1. From Table 1, I cannot tell RSE is obviously better than REC.\n    1. According to experiments, REC seems to be always slightly better than RSE.\n    1. If RSE is better than REC, why are we interested in the REC algorithm?\n\nMinor suggestion:\n1. I appreciate the comparison among bounds in Table 1 but have a minor suggestion: it would be better to include the name of algorithms( and the theorems index), and rearrange the table."
                },
                "questions": {
                    "value": "Please refer to the **Weaknesses** section for questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5545/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5545/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5545/Reviewer_Hx4P"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5545/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698723365230,
            "cdate": 1698723365230,
            "tmdate": 1700622056983,
            "mdate": 1700622056983,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ndPsn7dXkb",
                "forum": "pA4s793lcB",
                "replyto": "CIuSi9xFRG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5545/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5545/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review"
                    },
                    "comment": {
                        "value": "Thank you for your time and reading. We feel the review score is a bit harsh given the reviewer agrees on the novelty of the paper. It would be great if you raised the score to match the actual sentiment.\n\n> The title of this work starts with 'Improved algorithms'. I doubt whether it is an appropriate title. I feel that this work is more likely to be an extensive study on replicable bandits\n\nWe agree with this. In particular, Section 3 formalizes the way to guarantee the reproducibility that is implicitly (and inprecisely in terms of the multiplication count) used in (Esfandiari et al.'23). We revised the paper so that this contribution is clear. We have not changed the title, but we welcome your suggestion.\n\n> Bingshan Hu, Nidhi Hegde. \"Near-optimal Thompson sampling-based algorithms for differentially private stochastic bandits.\" UAI 2022.\n\nThank you for pointing out relevant work. DP considers the change of decision against the change of a single data point, whereas in our case, we have more than one change of data points between two datasets that are generated from the identical data-generating process. We added this to the revision of the related work section (Section 9, changes are colored red).\n\n> The distribution-independent bound is only provided for the RSE algorithm. However, it is usually not so difficult to derive a distribution-independent bound for the other algorithms when the distribution-dependent ones already exist. I suggest the author(s) to provide distribution-independent bounds for other algorithms and a comparison or explain the analytical challenge.\n\nThank you for your suggestion. Following your recommendation, we removed the claim to be \"the first distribution-independent bound\".\n\n> As the lower bound for the standard setting is provided, may the author(s) discuss the gap between upper and lower bounds?\n\nThank you for clarifcation. The gap is tight up to a logarithmic factor for $K=2$. We added this in Section 6 (colored red).\n\n> I appreciate the efforts to study the linear setting, may the author(s) also provide a lower bound (or explain the difficulty)?\n\nThank you for pointing it out. The difficulty here is not from the linear model but from extending the lower bound to $K>2$ arms. Note that even $K=2$, ours is the first lower bound in this problem. \n\n> From Table 1, I cannot tell RSE is obviously better than REC.\n\nYou are completely right, for most practical cases REC is better than RSE. If $\\Delta_2$ is very small compared with $\\Delta_3,...,\\Delta_K$, then RSE would outperform, which is model 1. In model 1, RSE eliminated arms $3--5$ after $T=1000$ and thus outperforms REC, but the difference looks subtle due to the exploration before $T<1000$. \n\n> I appreciate the comparison among bounds in Table 1 but have a minor suggestion: it would be better to include the name of algorithms( and the theorems index), and rearrange the table.\n\nRevised accordingly; thank you for your suggestion. \n\nThank you for many other suggestions. We highlighted the changes in the red lines."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5545/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700186425313,
                "cdate": 1700186425313,
                "tmdate": 1700186425313,
                "mdate": 1700186425313,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HoWInQoDJu",
                "forum": "pA4s793lcB",
                "replyto": "ndPsn7dXkb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5545/Reviewer_Hx4P"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5545/Reviewer_Hx4P"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response"
                    },
                    "comment": {
                        "value": "I appreciate the response from the author(s). Still, some of my concerns are not resolved:\n1. I still feel that is not that appropriate to have the title of this work start with 'Improved algorithms'. Again, I suggest the author(s) to revise the title.\n2. The similarity and difference between the replicable bandits and DP bandits should be clarified in the section of 'related works'.\n3. As I mentioned before, I suggest the author(s) to provide distribution-independent bounds for other algorithms and a comparison or explain the analytical challenge.\n\nBesides, I wonder if the concern of Reviewer tkZN is well resolved.\n\nConsidering the current situation, I may keep my score now."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5545/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700621998191,
                "cdate": 1700621998191,
                "tmdate": 1700621998191,
                "mdate": 1700621998191,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0Os0VzHetI",
            "forum": "pA4s793lcB",
            "replyto": "pA4s793lcB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5545/Reviewer_a2Sd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5545/Reviewer_a2Sd"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers the problem of replicability in the context of stochastic bandits. Following the definition of Impagliazzo et al. '22, and its adaptation to the bandit setting of Esfandiari et al. '23, a bandit algorithm is called replicable if, with probability at least 1-$\\rho$, it pulls the exact same sequence of arms when executed twice in the same bandit environment when its internal randomness is shared but the rewards of the arms are drawn independently across the two executions.\n\nIn the multi-armed bandit setting, the authors provide algorithms with an instance-dependent bound of $O(K)$, where $K$ is the number of different arms, and instance-independent bound of $O(K^{1.5})$. Moreover, they provide lower bounds which show that the dependence on the replicability parameter $\\rho$ is tight. \n\nThen, the authors consider the linear bandit setting and they provide algorithms whose instance-dependent regret scales as $O(d)$ and the instance-independent regret scales as $O(K^{1.5}d^{0.5})$."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "-The paper presents an algorithm that gets $O(K)$ instance-dependent bound in the multi-armed bandit setting.\n\n-The authors provide a best of both worlds type of algorithm that achieves the best bound between their algorithm and the one in Esfandiari et al. '23.\n\n-An instance-independent regret analysis of the algorithm is provided. \n\n-A lower bound on the regret is given which establishes the optimality on the dependence of the replicability parameter $\\rho$. In my opinion, this is the most technically interesting part of the paper.\n\n-Similar upper bounds are provided in the linear bandit setting. The instance-independent analysis provided in this work uses a different regret decomposition from Esfandiari et al. '23, which allows the authors to get rid of a factor $K/\\rho$ in the regret bound."
                },
                "weaknesses": {
                    "value": "After reading the abstract and the introduction of the paper, I was excited to learn more about the results. However, I think there are several issues that make the results less exciting than they appear to be.\n\nLet me start by explaining the main technical challenges in this line of work on replicable algorithms. The bandit algorithms of Esfandiari et al. '23 and of this paper use multiple calls to the replicable mean estimation algorithm of Impagliazzo et al. '22. Let $N$ be the number of times this algorithm is called. Because of a union bound over the probability of non-replication, the sample complexity overhead scales as $O(1/(\\rho/N)^2)$. For this reason, both Esfandiari et al. '23, and this paper using a batching approach and call the replicable mean estimation subroutine (or a randomized thresholding scheme that closely resembles it) at the end of each batch. The reason why Esfandiari et al. '23 incur an extra overhead of $O(K^2)$ in the regret compared to the non-replicable setting is, essentially, a union bound over the number of arms. Let me now move on to pointing specific weaknesses of the paper.\n\n-Abstract: \"Existing algorithms require a regret scale of $O(K^3)$, which increases much faster than the number of actions (or \u201carms\u201d), denoted as K. We introduce an algorithm with a distribution-dependent regret of $O(K)$\" -> both bounds are distribution-dependent, but are incomparable since the bound of Esfandiari et al. has better dependence on the distribution-dependent parameters than the one in the current paper. I think the way it is stated in the abstract is a bit confusing.\n\n-Abstract: \"Additionally, we propose an algorithm for the linear bandit with regret of $O(d)$, which is linear in the dimension of associated features, denoted as d, and it is independent of K.\" -> this bound is distribution-dependent, I think it should be mentioned.\n\n-Abstract: \"Our algorithms exhibit substantial simplicity compared to existing ones\" -> I don't agree with the statement, I will elaborate shortly.\n\n-Page 2: \"Upon closer examination of the problem, we discovered that $K^2$ factor can be eliminated\" -> as I mentioned before, the bounds are incomparable. I think at first read this sentence gives the impression that the regret bound is improved.\n\n-Page 2-3: \"Furthermore, due to the simplicity of the algorithm, we establish the first distribution-independent regret bound for the replicable K-armed bandit problem.\" -> this claim is imprecise. Esfandiari et al.'23 have already established distribution-independent bounds for linear bandits (without any restriction on the relationship between d, K), which is a more general setting than multi-armed bandits. As I said before, I don't agree with the statement that the algorithms are \"simpler\" either.\n\n-Section 3: I believe that the non-replication framework is inspired by the long line of work on batched bandits and bandits with low adaptation, so I think some citations are needed. Moreover, the same batching framework was essentially used in Esfandiari et al. '23 (even though it wasn't stated as such, its essence remains the same). I don't the benefit of stating it as an abstract framework. I think the authors could move some of this discussion to the appendix and elaborate more on their proofs technique in the main body.\n\n-Section 3: The set $|A_{p+1}|$ which I assume denotes the set of active arms is not defined (please correct me if I have missed it).\n\n-Section 4: The explore-then-commit algorithm was mentioned in the warm-up section of Esfandiari et al. for the case where the suboptimality gap $\\Delta$ is given to the designer and a sketch of the regret analysis was also provided. Essentially, Algorithm 2 of the current submission gets rid of the assumption that $\\Delta$ is known by using the doubling-trick approach. I believe some discussion about it would be useful,\n\n-Section 5: Essentially, this algorithm is a best of both worlds combination of Algorithm 2 and the replicable arm elimination algorithm of Esfandiari et al. '23. It is easy to see that if the algorithm detects a large gap between the two best arms, it just eliminates all of them except for the best, otherwise it performs the replicable arm elimination process of Esfandiari et al. '23. I think this comparison should be mentioned in the text. Given this discussion, I don't see where the simplicity of this algorithm compared to Esfandiari et al. '23 lies. \n\n-Section 5: \"Here, eliminating all but one arm is equivalent\nto switching to the exploration period.\" -> exploitation.\n\n-Section 5: \"Moreover,\nthis algorithm is the first replicable algorithm that has a distribution-independent regret bound in the\nK-armed bandit problem.\" -> As I mentioned before, this is not correct. Since the distribution-independent bounds for linear bandits of Esfandiari et al. hold even when $K = d$ they already imply distribution-independent regret bound for the K-armed bandit problem.\n\n-Section 7: \"Next, we consider the linear bandit problem, a special version of the K-armed bandit problem\nwhere associated information is available.\" -> if $d = K$ then the linear bandit problem can express the multi-armed bandit problem, so maybe it should be emphasized that $d << K.$\n\n-Section 7: \"The main innovation here is to use the G-optimal design that\nexplores all dimensions in an efficient way.\" -> I think it should be mentioned that this innovation has already been done in the literature since it is a very well-known technique and is not an innovative element of the current submission.\n\n-Lemma 7: The definition of $\\hat{\\theta}_p$ is missing. I think it's worth discussing that even though $\\hat{\\theta}_p$ is different across two executions, because of the different observed rewards, the algorithm is still replicable. \n\n-Related work: Throughout the draft, the citation to Esfandiari et al. '23 is not the indented one. In fact, the citation to the correct paper is absent from the references. \n\n-Conclusion: \"This represents a significant advancement over existing algorithms.\" -> as I explained before, the bounds are not comparable. \n\n-In general, the authors could try to add some proof sketches in the main body. If need be, they could try to shorten the discussion about the replicability framework, which I don't think adds much to the paper.\n\n-Not a weakness, but relevant the the related works section: Some other references that could be useful:\n\"List and Certificate Complexities in Replicable Learning\", Peter Dixon, A. Pavan, Jason Vander Woude, N. V. Vinodchandran\n\"Replicability and stability in learning\", Zachary Chase, Shay Moran, Amir Yehudayoff\nBoth of these works study a different notion of replicability that has to do with the number of different outputs of a learning algorithm.\n\n\"Replicable Reinforcement Learning\", Eric Eaton, Marcel Hussing, Michael Kearns, Jessica Sorrell\n\"Replicability in Reinforcement Learning\", Amin Karbasi, Grigoris Velegkas, Lin F. Yang, Felix Zhou\nBoth of these papers study replicable algorithms for RL.\n\n\"Statistical Indistinguishability of Learning Algorithms\", Alkis Kalavasis, Amin Karbasi, Shay Moran, Grigoris Velegkas\nThis paper provides a relaxation of the replicability definition and extends some of the equivalences shown in Bun et al. '23 to uncountable domains."
                },
                "questions": {
                    "value": "-See weaknesses section.\n\n-Instance-independent regret bound for linear vs. multi-armed bandits: for linear bandits the regret bound is $O(K/\\rho \\sqrt{dKT\\log T})$ whereas for multi-armed bandits it is $O(K/\\rho \\sqrt{KT\\log T})$. Isn't the former always worse even though there is more structure?\n\n-Appendix F.2, proof of the (A) case. Isn't there an additive term O(K) missing in the regret bound?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5545/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5545/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5545/Reviewer_a2Sd"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5545/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698821006749,
            "cdate": 1698821006749,
            "tmdate": 1700709741027,
            "mdate": 1700709741027,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "eNJckZ0IOx",
                "forum": "pA4s793lcB",
                "replyto": "0Os0VzHetI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5545/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5545/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for many comments"
                    },
                    "comment": {
                        "value": "Thank you for your careful reading. We updated the paper accordingly (colored red in the revision). Below are our updates.\n\n> Incompativility of $O(K^3)$ and $O(K)$ bounds\n\nYou are correct. We denote this several times, but revised the paper to clarify that \"when the suboptimality gaps for each arm are within a constant factor\". \n\n> We establish the first distribution-independent regret bound for the replicable $K$-armed bandit problem\n\nWe agree that and drop this statement.\n\n> Section 3: I believe that the non-replication framework is inspired by the long line of work on batched bandits and bandits with low adaptation.\n\nThank you for the reference. We added batched bandit papers in the related work (Section 9).\n\n> Section 4: The explore-then-commit algorithm was mentioned in the warm-up section of Esfandiari et al. for the case where the suboptimality gap is given to the designer and a sketch of the regret analysis was also provided. Essentially, Algorithm 2 of the current submission gets rid of the assumption that is known by using the doubling-trick approach. \n\nThank you for the reference. We added this at the end of Section 4. Their algorithm is $O(T \\sum_i \\Delta_i)$, and our EtC clearly provides a better bound.\n\n> Section 5: Essentially, this algorithm is a best of both worlds combination of Algorithm 2 and the replicable arm elimination algorithm of Esfandiari et al. '23. It is easy to see that if the algorithm detects a large gap between the two best arms, it just eliminates all of them except for the best, otherwise it performs the replicable arm elimination process of Esfandiari et al. '23. I think this comparison should be mentioned in the text. Given this discussion, I don't see where the simplicity of this algorithm compared to Esfandiari et al. '23 lies.\n\nWe agree with the word \"simplicity\" is somewhat imprecise. We think Section 3 in our paper clarified what is somewhat implicitly used in [Esfandiari et al. '23], and we revised the paper so that our contributions are more clearly mentioned.\n\n> Not a weakness, but relevant the the related works section: Some other references that could be useful: \"List and Certificate Complexities in Replicable Learning\", Peter Dixon, A. Pavan, Jason Vander Woude, N. V. Vinodchandran \"Replicability and stability in learning\", Zachary Chase, Shay Moran, Amir Yehudayoff Both of these works study a different notion of replicability that has to do with the number of different outputs of a learning algorithm.\n> \"Replicable Reinforcement Learning\", Eric Eaton, Marcel Hussing, Michael Kearns, Jessica Sorrell \"Replicability in Reinforcement Learning\", Amin Karbasi, Grigoris Velegkas, Lin F. Yang, Felix Zhou Both of these papers study replicable algorithms for RL.\n> \"Statistical Indistinguishability of Learning Algorithms\", Alkis Kalavasis, Amin Karbasi, Shay Moran, Grigoris Velegkas This paper provides a relaxation of the replicability definition and extends some of the equivalences shown in Bun et al. '23 to uncountable domains.\n\nThank you for pointing them out. We added this to the related work (Section 9). \n\n> Instance-independent regret bound for linear vs. multi-armed bandits: for linear bandits the regret bound is \n whereas for multi-armed bandits it is. Isn't the former always worse even though there is more structure?\n\nIt is always worse in terms of distribution-independent bound because we use a naive bound $N_i^{lin}(p) \\le N^{lin}(p)$ for linear bandits, which still outperforms Esfandiari et al. '23. \n\n> Appendix F.2, proof of the (A) case. Isn't there an additive term O(K) missing in the regret bound?\n\nWe consider it correct, but let us know if some steps are unclear. We are happy to revise it.\n\nThank you for many other corrections. We updated the paper accordingly. All changes in revisions are colored in red."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5545/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700185593802,
                "cdate": 1700185593802,
                "tmdate": 1700185622852,
                "mdate": 1700185622852,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JASjomvNuY",
                "forum": "pA4s793lcB",
                "replyto": "eNJckZ0IOx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5545/Reviewer_a2Sd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5545/Reviewer_a2Sd"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to thank the authors for addressing my points and revising their draft accordingly. I have also followed the discussion with Reviewer tkZN. Given the new results that are presented by authors and the updated version of the manuscript, I am increasing my score to 6 since I have a more positive view of the paper."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5545/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700709708715,
                "cdate": 1700709708715,
                "tmdate": 1700709708715,
                "mdate": 1700709708715,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2wSpeSfOs1",
            "forum": "pA4s793lcB",
            "replyto": "pA4s793lcB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5545/Reviewer_tkZN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5545/Reviewer_tkZN"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers the problem of replicability in stochastic and linear stochastic bandits. Specifically, the $\\rho$-replicability criterion requires the sequence of actions chosen by the algorithm in two independent runs over the same data generating process be identical with probability at least $1-\\rho$, where $\\rho$ is a given replicability parameter, and the probabilities are taken with respect to the internal randomization of the algorithm and randomness in the data generating process. This work considers this criterion in the context of regret minimization in stochastic, and linear stochastic bandits. This paper provides 3 key results: (a) they provide a natural algorithm that achieves an instance dependent expected cumulative regret of $O\\left(\\rho^{-2}\\log T\\cdot\\min\\left(\\{\\sum_{i\\neq i^*} \\frac{\\Delta_i}{\\Delta^2},\\sum_{i\\neq i^*}\\frac{K^2}{\\Delta_i}\\}\\right)\\right)$, which, depending on the instance improves upon the existing known regret upper bound of $O(\\rho^{-2}\\log T\\cdot\\sum_{i\\neq i^*} \\frac{K^2}{\\Delta_i})$. They additionally provide an instance independent regret upper bound of $O(\\rho^{-1}K\\sqrt{KT\\log T})$ achieved by the same algorithm. (b) They provide a 2-arm regret lower bound of $\\Omega(\\rho^{-2} \\Delta^{-1}\\log^{-1}(1/\\rho\\Delta))$ for any $\\rho$-replicable algorithm for stochastic bandits. (c) Lastly, for the case of linear stochastic bandits, they provide an algorithm that builds upon the replicable algorithm for stochastic bandits that achieves an instance dependent regret upper bound of $O(\\rho^{-2}\\Delta^{-2}d\\log T)$, and an instance independent regret upper bound of $O(\\rho^{-1}K\\sqrt{dKT\\log T})$."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "Overall, the paper is well written and easy to read. The algorithms designed are natural and quite easy to understand."
                },
                "weaknesses": {
                    "value": "To be very honest, I am not too enthusiastic about this result. I have serious questions about the motivation for this particular problem - I don't see the practical need for exact replicability in the bandit setting, as it seems too stringent a condition, and existing ideas (which I shall elaborate shortly) already provide \"near\"-replicable algorithms for this particular problem at no additional loss in regret. From a technical perspective, I don't see any new technical ideas being developed in this piece of work. The algorithms provided are pretty much a straightforward application of the batched bandits idea of Perchet et. al. (Annals of Statistics, 2016) + its other follow-ups. This is also an obvious connection because the batched bandits framework already provides near-replicability: very arm $i\\neq i^*$ essentially has a fixed optimal epoch when the number of plays exceeds $\\tilde{O}(\\Delta_i^{-2})$ for the first time so it becomes \"distinguishable\" from the best arm. We have the high probability guarantee that every arm will necessarily be played up until the optimal epoch and will necessarily be discarded at the end of the epoch following the optimal epoch (by slightly strengthening the rejection condition for an arm to be something like the empirical means are separated by 3x the size of the confidence interval in that epoch, and increasing the number of samples obtained from active arms such that the size of the confidence interval shrinks by a factor of 1/5 across successive epochs - this will effectively guarantee that there is only one \"uncertain\" epoch when an arm may or may not be rejected, before which the arm will necessarily be played and after which the arm will necessarily be rejected. This slight modification of the rejection condition affects cumulative regret by only a small constant). Therefore for every arm, the behavior of the algorithm is near-deterministic and hence replicable (with polynomially high probability in $T$) for all epochs except one epoch where anything can happen (the optimal epoch where this arm becomes distinguishable for the first time. In this epoch, it is hard to argue the nature of the overlap/non-overlap in the confidence intervals since the size of the confidence intervals is at the same scale -- up to small constants -- as the gap between the rewards). One might argue that this is already good enough for most practical purposes - the number of times any arm will be played will be within constant factor of each other across any two runs of the algorithm with any desired polynomially large probability (in $1/T$). In fact, across any two runs of the algorithm, we have the additional guarantee that for any arm, the number of times the arm is played takes only 2 possible values! One can add additional randomness by randomizing the width of the confidence interval in this one epoch (you don't need to know the identity of the bad epoch. you can essentially randomize confidence intervals in each epoch and the strong separation or overlap in confidence intervals in other epochs guarantees that this randomization effectively only kicks in at this one potentially bad epoch where it is hard to argue what the algorithm does) to explicitly get a handle on what the probability of non-replicability looks like. The instance-independent regret bounds follow by an identical analysis (essentially the same analysis as batched bandits), the algorithm and analysis for linear bandits follows pretty much the same way. The lower bound is also extremely weak. These are not at all novel ideas, and I find it hard to argue acceptance for these results."
                },
                "questions": {
                    "value": "What is novel in this work? I would request the authors to improve their upper bounds (which I am almost certain is possible with a finer analysis exploiting subgaussianity and a cleverer randomization strategy for rejection in an epoch), as well as provide tighter lower bounds."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5545/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5545/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5545/Reviewer_tkZN"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5545/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699127325875,
            "cdate": 1699127325875,
            "tmdate": 1699636569517,
            "mdate": 1699636569517,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sjD40m0gkh",
                "forum": "pA4s793lcB",
                "replyto": "2wSpeSfOs1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5545/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5545/Authors"
                ],
                "content": {
                    "title": {
                        "value": "On the relevance to batched bandit problem"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful comments. The main point is the novelty comparison with the literature by Vianney Perchet, Philippe Rigollet, Sylvain Chassang, Erik Snowberg (2016). \"Batched Bandit Problem.\" Annals of Statistics 2016, Vol. 44, No. 2, 660\u2013681 [PRCS2016].\n\nThe batched bandit problem [PRCS2016] utilizes an explore-then-commit policy that conducts uniform exploration for each batch. As you correctly mentioned, if it is applied with geometric size of batches (Sec 4.2 therein), one can obtain the high probability guarantee that the rejection of arms occurs in one of two rounds; we use this fact in our algorithms as well. \nIn view of this, our novelty is as follows: \n* The analysis in [PRCS2016] focuses on the case of $2$ arms, whereas our algorithm extends to $K$ arms. \n* We use randomization of the confidence bound so that the reproducibility is guaranteed with probability more than $1-\\rho$ for $\\rho < 0.5$, unlike the algorithm in [PRCS2016]. \n* We formalize the sufficient condition for a sequential algorithm to be replicable, which is implicitly used in [EKKKMV2023]. Thank you for clarifying the multiple correction on replication. In fact, [EKKKMV2023] incorrectly counts the correction factor needed for the multiplity, and the formalization in our Section 3 has a significant contribution.\n* Furthermore, we unify the explore-then-commit [PRCS2016] and successive rejects [GHRZ2019,EKKKMV2023] for $K$ arms in a way that guarantees reproducibility (Algorithm 2).   \n\n[GHRZ2019] Zijun Gao, Yanjun Han, Zhimei Ren, Zhengqing Zhou. \"Batched Multi-armed Bandits Problem.\" NeurIPS 2019.\n\n[EKKKMV2023] Hossein Esfandiari and Alkis Kalavasis and Amin Karbasi and Andreas Krause and Vahab Mirrokni and Grigoris Velegkas. \"Replicable Bandits.\" ICLR 2023.\n\nSince these two papers [PRCS2016,GHRZ2019] as well as other papers in batched bandits are clearly relevant to our work, we revised our related work (Section 9) to add a discussion (highlighted in red color). Thank you again for introducing these materials."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5545/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700185002460,
                "cdate": 1700185002460,
                "tmdate": 1700187826432,
                "mdate": 1700187826432,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RUbhiD2xNq",
                "forum": "pA4s793lcB",
                "replyto": "sjD40m0gkh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5545/Reviewer_tkZN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5545/Reviewer_tkZN"
                ],
                "content": {
                    "title": {
                        "value": "Still not convinced"
                    },
                    "comment": {
                        "value": "Thank you for looking into the batched bandit literature. However, I am still not convinced regarding the novelty in this work. The extension to the $K$ arm case was established in GHRZ2019 following the work of PRCS2016, which is essentially the same as what is done in the current submission. That hardly counts as novelty. I agree that the analysis of batched bandits only guarantees that each arm gets rejected in one of two possible epochs, but I dont believe the leap from that to $1-\\rho$ replicability is that novel. As I said earlier, I dont even see a strong motivation for this type of a replicability result, especially given that the batched bandits framework gives \"near\" replicability anyway (see the comments from my first response). I am really tired of seeing these A+B type results, which are essentially - researchers have historically studied problem A, and in recent years concept B has been developed, so why not study A+B - especially when no new techniques or tools are being developed that would be of broader applicability other than the narrow scope considered in that paper. With regards to this work, at the very least I would have liked to see a tighter lower bound, if not a tighter upper bound, both of which I strongly believe are possible. \n\nBut that being said, I think novelty is a subjective thing, and although I will not be changing my score, I would like to see what the other reviewer's thoughts are regarding the matter."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5545/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700673787691,
                "cdate": 1700673787691,
                "tmdate": 1700673787691,
                "mdate": 1700673787691,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "88JUHPooZ5",
                "forum": "pA4s793lcB",
                "replyto": "2wSpeSfOs1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5545/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5545/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Re: Still not convinced"
                    },
                    "comment": {
                        "value": "Dear Reviewer tkZN:\n\nIn our understanding, your second comment is summarized as follows:\n\n* You are not convinced of the novelty of the replicable bandit problem setting (K-armed bandits with replicability).\n* You are unsatisfied with the gap between the upper bound and the lower bound.\n\nLet us set the first one aside because it is somewhat subjective as you mentioned. Regarding the second question, you strongly believe you can have a better bound in both lower and upper bound. Please tell us some idea of how the tight bound should looks like? I think you understand the problem structure well, and it would be great if you shared your insight.\n\nBest,\nAuthors"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5545/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674815980,
                "cdate": 1700674815980,
                "tmdate": 1700675046762,
                "mdate": 1700675046762,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ed6A1KHiij",
                "forum": "pA4s793lcB",
                "replyto": "2wSpeSfOs1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5545/Reviewer_tkZN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5545/Reviewer_tkZN"
                ],
                "content": {
                    "comment": {
                        "value": "1. There are two components to my opinion regarding novelty, and both arise from the fact that the batched bandits work is already an established result in this area. (a) Subjective - since batched bandits already provides \"near\" replicability, I am not convinced about the motivation for $1-\\rho$ replicability in the stochastic bandits setting. It is an extremely stringent condition and I cannot think of any practical situation where one might desire these stronger properties over the weaker guarantees already provided by the batching approach which importantly come at no asymptotic loss in regret. (b) Not as subjective - I do not believe there are any new techniques being developed in this work that would be more broadly applicable beyond the scope of the problem considered here, which further limits the impact of this paper. These two factors together make me question the novelty here.\n\n2.  The point is, there is a big gap between the upper and lower bounds. I find it quite hard to believe that the new replicability lower bound you have introduced must be independent of $T$. If you strongly believe that it must be the case, then that means the correct upper bound you should be aiming toward must contain two terms (additive instead of multiplicative), the first being the usual MAB regret, and the other additive term being the excess regret due to replicability. This seems implausible to me, though I cannot say for certain. I certainly think there is scope for improvement in your upper bounds though, through a more careful analysis exploiting subgaussianity and anti-concentrations to bound the probability of non-replication. That being said, this is as far as I can get based on intuition alone.\n\nLastly, I do understand that some of my reservations regarding this work are quite subjective. I would like to see both the authors as well as other reviewer's thoughts on the matter."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5545/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700676441785,
                "cdate": 1700676441785,
                "tmdate": 1700676469742,
                "mdate": 1700676469742,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EHkHH09b0Y",
                "forum": "pA4s793lcB",
                "replyto": "2wSpeSfOs1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5545/Reviewer_tkZN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5545/Reviewer_tkZN"
                ],
                "content": {
                    "comment": {
                        "value": "I was not arguing for the lower bound to be multiplicative, though looking back at my comment I understand it came off that way. We are essentially operating with two different \"scales\" here - one is the replicability parameter $\\rho$, and the other is the time horizon $T$, which are independent of each other, due to which the penalty due to replicability will be independent of $T$. I apologize for the carelessness in the writing of my response. The point I was trying to make is that you should be trying to improve your upper bound to $\\sum \\log T/\\Delta_i + 1/(\\Delta\\rho^2)$, which is substantially stronger than the $O(\\sum \\Delta_i\\log T/\\Delta^2)$ that you prove in this paper, which I think follows straightforwardly from existing analyses in the batching space. Moreover, you should at least try to prove a $K$ armed lower bound rather than a 2 armed one. Regarding further improvements to the upper bound, a $\\log\\log T$ dependence in the term that contains the replicability parameter is something I believed would be possible due to the union bound over only the $\\log T$ epochs, but I did not want to commit to it since I have not formally worked out the analysis. If it is truly possible to get a regret that looks like $\\sum_i \\log\\log T/(\\rho^2\\Delta_i) + \\log T/\\Delta_i$, it would be a massive improvement over the $O(\\sum \\Delta_i\\log T/(\\Delta^2\\rho^2))$ regret proved in this work. This would also substantially shrink the gap between your upper and lower bounds, to just a $O(\\log\\log T)$ factor, which I think is near negligible. I also wonder whether its possible to achieve an even stronger result for worst-case regret (additive, with a $\\log\\log\\log T$ dependence in the term that contains the replicability parameter $\\rho$), since you can achieve near-optimal $O(\\sqrt{KT\\log T})$ regret in just $\\log\\log T$ batches in the usual non-replicable case, as opposed to $\\log T$ batches required to achieve the optimal $O(\\sum_i \\log T/\\Delta_i)$ instance-dependent regret. If this is indeed the case, I would indeed be willing to change my evaluation of this work. I still have my reservations about the motivation behind this setting, and that even this further improvement comes from a more careful analysis of existing batching ideas rather than the introduction of truly new tools, the fact that this work would then provide a near-complete resolution (up to low order terms) to the replicability question would make up for these deficiencies."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5545/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700680713440,
                "cdate": 1700680713440,
                "tmdate": 1700681748894,
                "mdate": 1700681748894,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uRWG8SttYm",
                "forum": "pA4s793lcB",
                "replyto": "2wSpeSfOs1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5545/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5545/Authors"
                ],
                "content": {
                    "title": {
                        "value": "What we can further do easily and challenging"
                    },
                    "comment": {
                        "value": "Thank you for your reply.\n\n* We think we can extend our lower bound from $2$ to $K$-arms relatively easily to get $\\sum_i (\\frac{\\log T}{\\Delta} + \\frac{1}{\\Delta \\rho^2})$ bound that you consider ideal.\n* We hypothesize (though we are not very sure) that it is impossible to have this upper bound of $\\sum_i (\\frac{\\log T}{\\Delta} + \\frac{1}{\\Delta \\rho^2})$ because it implies $K-1$ decisions of eliminating each suboptimal arm at some round, and it is subject to $O(K)$ different decision points. We guess there are some limitations (Algorithm 1 focused on deleting all arms at the same time to the cost of $\\Delta_K/\\Delta_2$ coefficient as you correctly pointed out). But we are not sure; our current framework is just incapable of removing this $K$ multiplication.\n* Worst-case bound is easily improvable to replace $\\sqrt{\\log T}/\\rho$ of the current paper with $\\sqrt{\\log \\log T}\\rho + \\sqrt{\\log T}$. Moreover, if we can reduce the number of batches to $\\log \\log T$ as you suggested, then we can further improve upon it. This was not in our mind, thank you for your suggestion."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5545/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700681991366,
                "cdate": 1700681991366,
                "tmdate": 1700682070850,
                "mdate": 1700682070850,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8HSTkQtHSl",
                "forum": "pA4s793lcB",
                "replyto": "2wSpeSfOs1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5545/Reviewer_tkZN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5545/Reviewer_tkZN"
                ],
                "content": {
                    "comment": {
                        "value": "For worst-case regret, you can be a lot more aggressive with batch sizes - using a squaring trick to get $\\log\\log T$ batches/adaptive rounds in total achieving $\\tilde{O}(\\sqrt{KT})$ regret rather than the instance-dependent case which requires a more conservative doubling trick to get $\\log T$ batches/adaptive rounds in total achieving $\\sum \\log T/\\Delta_i$ regret.\n\nIn any case, I think we have made all our points, and it is up to other reviewers and the meta reviewers to weigh in with their opinions."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5545/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700682821582,
                "cdate": 1700682821582,
                "tmdate": 1700682889281,
                "mdate": 1700682889281,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]