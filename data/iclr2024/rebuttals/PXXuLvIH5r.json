[
    {
        "title": "From Matching to Mixing: A Graph Interpolation Approach for SAT Instance Generation"
    },
    {
        "review": {
            "id": "FtHfAsgLjC",
            "forum": "PXXuLvIH5r",
            "replyto": "PXXuLvIH5r",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7734/Reviewer_SBgK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7734/Reviewer_SBgK"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method for automatically generating SAT instances. Automatic generation of SAT instances is highly demanded since we need a large amount of SAT instances for estimating parameters of learning-based SAT solvers or tuning hyperparameters of traditional SAT solvers. While previous state-of-the-art learning-based SAT instance generation methods generate instances from a single reference, the proposed algorithm, MixSAT, generates SAT instances by interpolating two SAT instances. MixSAT first makes an assignment map of variables of two SAT instances. Then, it uses the map to select similar clauses and finally exchanges these similar clauses to make new SAT instances. On making an assignment map, the proposed method puts random noise for preserving difficulties. Experimental results show that MixSAT can preserve more graph structural properties than baseline methods, except for UNSAT instances. Moreover, MaxSAT preserves the difficulty of SAT instances, in hard satisfiable instances."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "**Important topic:**\nI agree with the motivation that we need more real-world SAT instances to improve the performance of Boolean SAT solvers. I agree that to generate SAT instances, preserving difficulty is a challenging task since SAT instances seem sensitive to small changes. \n\n**Clearly written paper:**\nThe paper is generally clearly written and easy to read. Figures 1-3 help us understand the complex procedure of MaxSAT.\n\n**The idea of using interpolation is interesting:**\nThe idea of using interpolation is interesting. It seems a reasonable way of data augmentation. Moreover, the proposed method seems\ncarefully designed to perform interpolation between two CNFs.\n\n**Experimental results show the superiority of the proposed method:**\nAlthough not always better than HardSATGEN, the proposed method outperforms other baseline methods."
                },
                "weaknesses": {
                    "value": "**Experimental results are not strong:**\n1. Experimental results show that the proposed method is not always superior to HardSATGEN. Especially, the running time evaluation results in Tab.2 show that the proposed method fails to preserve the computational difficulties of hard instances. The results seem to contradict the claim of the paper.\n1. Li et al. (2023) reported that hyperparameter tuning with instances generated by HardSATGEN improves performance on real-world problems. Since a primal objective of generating SAT instances is to tune the parameters of solvers, the paper should report the results of using the instances generated by MixSAT to tune the parameters of existing solvers. \n1. Runtime evaluations were conducted with CaDiCaL. I think the paper should also report the performance of different solvers like Kissat since different solvers sometimes show completely different performances. In performance evaluation, Li et al. (2023) compared the performance of three different solvers. It seems a safer way to evaluate the running time of SAT solvers.\n\n\n\n**Some important details of experiments are missing:**\nSome important details of experiments are not reported in the paper:\n1. The proposed method generates instances by interpolating two instances. Therefore, the performance of the proposed method depends on how we make a pair. However, the details of making pairs are not reported in the paper.\n2. It is said that the dataset was taken from SATLIB and SAT Competition 2021. However, how the authors select easy and hard [a-h] instances from a large amount of instances seems not explained in the paper. The paper should show the reason behind selecting these instances to make the evaluation fair.\n\n\n\n**There are claims not supported by experimental results:**\nSome claims of the paper seem not supported by the experimental results:\n1. The paper says the proposed method is more efficient (page 2), but no experimental results support the claim.\n2. The paper says that MaxSAT introduces some randomness to maintain computational hardness. However, experimental results reported in Table 2 show that randomness does not help much in preserving hardness.\n\nThese points make me feel that the proposed approach is not well-motivated."
                },
                "questions": {
                    "value": "I'd be happy if the authors addressed my concerns mentioned in the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7734/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7734/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7734/Reviewer_SBgK"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7734/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698679980331,
            "cdate": 1698679980331,
            "tmdate": 1699636943728,
            "mdate": 1699636943728,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MjcmQzdIxV",
                "forum": "PXXuLvIH5r",
                "replyto": "FtHfAsgLjC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7734/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7734/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response by Authors (1/3)"
                    },
                    "comment": {
                        "value": "Thanks for the thorough review and valuable comments. We are encouraged with your acknowledgment of our topic importance, writing clarity, methodology design, and empirical results. Below we respond to your specific comments.\n\n> **Q1: Experimental results show that the proposed method is not always superior to HardSATGEN.**\n\nPlease note that reproducing computational hardness in the SAT instance generation task is considerably challenging which has not been achieved until HardSATGEN [1]. Its success stems from its fine-grained control over the instance structure, particularly for the unsatisfiable core structures, as well as the postprocessing design involved to maintain the dominant effect of the unsat core structure.\n\n\nWhile the design in [1] is effective, it confines the method to unsatisfiable instances only. Since real-world datasets generally incorporate both satisfiable and unsatisfiable instances, merely learning from the unsat instances can lead to certain negative impacts on downstream tasks like hyperparameter tuning, as evidenced by Table 3 in the new revision. In contrast, MixSAT surpasses this limitation, demonstrating generality to both satisfiable and unsatisfiable instances. MixSAT is also free from the node merge-split framework of previous learning-based methods [1-3], naturally meeting the hardness analysis and requirements set forth in [1].  Notably, our performance on satisfiable instances is significantly superior to all other methods. \n\nWe acknowledge that our effectiveness on unsatisfiable instances is not as strong, especially considering [1] already provides a tailored solution for unsatisfiable problems. However, we believe the two methods can complement each other. As evidence, we have supplemented the study with hyperparameter optimization experiments and present the combined effects of both methods in Table 3, which show the significant effectiveness of our methods in practice as a boosting tool complementary to HardSATGEN.\n\n\n> **Q2: Hyperparameter tuning with the generated instances.**\n\nThanks for the valuable point. We supplement the hyperparameter tuning results on the state-of-the-art SAT solver Kissat in Table 3 of the new manuscript. Due to the complexity of the table format, we refrain from presenting it in this response. \n\nWe adopt one of the Bayesian optimizers HEBO to tune the solver. The tuned hyperparameters include restart interval, reduce interval, and decay, which pose impacts on the frequency of restarting, frequency of learned clause reduction, and per mille scores decay respectively. For each method, the Bayesian optimizer tries 200 sets of hyperparameters and we select the top 5 hyperparameters to evaluate on test instances. The best runtime results are presented in the table. Due to the costly Bayesian optimization process, we select instances with runtimes under 500 seconds for experiments. \n\nNote in Table 2, MixSAT reproduces computational hardness well on satisfiable instances, while HardSATGEN [1] can only learn from unsatisfiable instances. Table 3 shows the matched performance where MixSAT and HardSATGEN perform the best for satisfiable and unsatisfiable instances respectively. Thus, we also consider simultaneously adopting MixSAT learned from satisfiable instances and HardSATGEN learned from unsatisfiable instances for tuning, which presents the best overall performance. In general, the generated instances by the proposed methods bring a significant solver runtime reduction of over 50% to the solver.\n\n\n> **Q3: Runtime evaluations conducted with more modern solvers.**\n\nThanks for the comment. We supplement runtime evaluation on Kissat SBVA-Cadical (the overall main-track winner of the 2023 SAT competition) solvers over the generated formulas as well as the original benchmarks in Tables 6 and 7 of the new revision. We set the runtime limit for Kissat at 5000 seconds, while for SBVA-Cadical, no runtime limit is imposed due to its slower execution on the Hard dataset. As seen, MixSAT performs consistently regarding the computational hardness across different solvers."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7734/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700410192483,
                "cdate": 1700410192483,
                "tmdate": 1700410520262,
                "mdate": 1700410520262,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "syo6N7YStP",
                "forum": "PXXuLvIH5r",
                "replyto": "FtHfAsgLjC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7734/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7734/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response by Authors (2/3)"
                    },
                    "comment": {
                        "value": "> **Q4: The performance of the proposed method depends on how we make a pair. However, the details of making pairs are not reported in the paper.**\n\n\nFirstly, it is essential to note that the core methodology of our approach centers around the design of the interpolation method, which is non-trivial when applied to graph structures, especially in the case of large-scale bipartite graphs with distinctive structures. In the paper, we elaborate on how we design the graph interpolation method to ensure consistency in generating data that aligns with the original distribution, in structural (through matching) and hardness (through matching with randomness) aspects.\n\n\nOn the other hand, of course pair selection method is an integral part of our methodology. The process of how we filter pairs has been detailed in Sec. 2.3 of the original paper, with the specific design outlined as follows: \n\nWe introduce an entropy-based filter to first select potentially valuable pairs for mixing (since the similarity of different pairs can vary much, not all pairs can yield quality matching results), which assesses and filters pairs using the row-wise entropy of the soft alignment matrix $\\mathbf{S}\\in\\mathbb{R}^{n_1\\times n_2}$:\n$$h = -\\sum_{i=0}^{n_1} \\sum_{j=0}^{n_2}s'\\_{ij}\\log{s'\\_{ij}} \\quad \\text{where} \\quad s'\\_{ij} = \\frac{s\\_{ij}}{\\sum\\_{k}s\\_{ik}}$$\nwhich implies the sharpness of the soft alignment matrix between $G_1$ and $G_2$. A smaller value of $h$ denotes more distinct matching relations, leading to a potentially better pair for mixing.\n\n> **Q5: The reason behind selecting these instances as datasets to make the evaluation fair.**\n\nWhen selecting instances to construct the dataset for evaluation, we consider the following factors: \n\n1) It should include instances derived from real-world application scenarios to ensure the practical value of our experimental validation. Therefore, we choose instances from SAT competition benchmarks, which are encoded from real-world scenarios. \n \n2) We aim to maintain an equal proportion of satisfiable and unsatisfiable instances in our data to verify the general applicability of MixSAT. Hence, we maintain a 1:1 ratio of sat to unsat instances in the dataset. \n\n3) Recognizing the significance of computational hardness in experimental validation, we specifically select instances with higher computational hardness to constitute the Hard dataset.\n\n\n> **Q6: Experimental support for \"the proposed method is more efficient\".**\n\n\nTo demonstrate the efficiency advantage of MixSAT over previous state-of-the-art learning-based methods that generate from scratch, we compared G2SAT[2] with MixSAT ($\\lambda=0.1, \\tau=1.0$) for single instance generation, which is adopted as the standard training scheme in [3].\n\n\nThe pipelines consist of two processes: training and generating. The training process only needs to be performed once for a particular dataset, while the generating process can produce formulas repeatedly according to the target quantity. We measure the time for training and generating one specific formula. As shown below and also in Table 8 of the updated revision,  MixSAT is significantly more efficient than G2SAT in both processes. Specifically, for the generating phase, MixSAT costs only approximately 1/2000 runtime compared to G2SAT.\n\n\n|       |     **Training**      |   **Generating**    |\n|:----------:|:---------------------:|:-------------------:|\n| **G2SAT**  | 4467.05 $\\pm$ 2315.81 | 2242.07 $\\pm$ 1280.28 |\n| **MixSAT** | 1760.03 $\\pm$ 457.89  |   1.44 $\\pm$ 0.22   |"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7734/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700410504441,
                "cdate": 1700410504441,
                "tmdate": 1700410739930,
                "mdate": 1700410739930,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Lq90qJbPWU",
                "forum": "PXXuLvIH5r",
                "replyto": "FtHfAsgLjC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7734/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7734/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response by Authors (3/3)"
                    },
                    "comment": {
                        "value": "> **Q7: Experimental support for \"MixSAT introduces some randomness to maintain computational hardness\".**\n\n\nNote Table 2 illustrates the average solver runtime of all pairs across each instance. In our analysis, a tunable level of randomness is introduced to prevent hardness degradation caused by the interaction and conflicts of substructures. However, some deterministically generated formulas ($\\lambda=0$) have already effectively maintained the hardness, surpassing the entirely random ones (due to inherent randomness). Consequently, the average solver runtime cannot effectively reflect the improvement of hardness led by introducing randomness.\n\nTo straightforwardly reiterate the effect of randomness on the computational hardness of generating formulas, below and also in Table 5 of the new revision presents the results on instance pairs where MixSAT exhibits some degree of hardness degradation. We gradually increased the $\\lambda$ value to enhance the randomness. As seen, when the randomness increases,  the hardness of the initially degraded pairs is recovered to a certain extent compared with the ground truth. Note although pair $e$-$f$ fails to maintain hardness on both deterministic ($\\lambda=0$) and purely random generations, it gains some hardness when partial randomness is introduced ($\\lambda=0.5$).\n\n| **Randomness**      | **pair $a$-$c$**  | **pair $b$-$d$**    | **pair $d$-$c$**   | **pair $e$-$f$** |\n|:-------------------:|:-----------------:|:-------------------:|:------------------:|:----------------:|\n| **Ground Truth**    | 254.22            | 1382.71             | 852.85             | 2629.03          |\n| **$\\lambda=0$**     | 0.09 $\\pm$ 0.01     | 52.73 $\\pm$ 3.87      | 117.83 $\\pm$ 129.39  | 0.31 $\\pm$ 0.01    |\n| **$\\lambda=0.1$**   | 0.29 $\\pm$ 0.01     | 138.11 $\\pm$ 18.68    | 199.63 $\\pm$ 190.70  | 33.95 $\\pm$ 0.03   |\n| **$\\lambda=0.5$**   | 0.19 $\\pm$ 0.02     | 23.14 $\\pm$ 0.74      | 122.32 $\\pm$ 12.33   | 500.12 $\\pm$ 4.60  |\n| **$\\lambda=1.0$**   | 3.59 $\\pm$ 0.06     | 442.11 $\\pm$ 3.53     | 188.93 $\\pm$ 18.49   | 21.42 $\\pm$ 2.41   |\n| **$\\lambda=10.0$**  | 39.88 $\\pm$ 6.91    | 332.05 $\\pm$ 25.57    | 1530.78 $\\pm$ 838.56 | 2.05 $\\pm$ 1.80    |\n| **$\\lambda=100.0$** | 130.32 $\\pm$ 63.88  | 472.43 $\\pm$ 13.31    | 1429.93 $\\pm$ 308.41 | 0.69 $\\pm$ 0.68    |\n| **Random**          | 215.73 $\\pm$ 120.95 | 1116.42 $\\pm$ 1203.59 | 695.24 $\\pm$  799.58  | 11.49 $\\pm$ 9.12   |\n\n\n---\n\nWe hope this response could help address your concerns. We sincerely wish that you could reconsider the novel contribution and potential impact of our work on the community. We look forward to receiving your valuable feedback soon.\n\n---\n\n**Reference:**\n\n[1] HardSATGEN: Understanding the Difficulty of Hard SAT Formula Generation and A Strong Structure-Hardness-Aware Baseline. KDD 2023.\n\n[2] G2SAT: Learning to Generate SAT Formulas. NeurIPS 2019.\n\n[3] On the Performance of Deep Generative Models of Realistic SAT Instances. SAT 2022."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7734/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700410647020,
                "cdate": 1700410647020,
                "tmdate": 1700455671915,
                "mdate": 1700455671915,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ROmkRtSqet",
                "forum": "PXXuLvIH5r",
                "replyto": "Lq90qJbPWU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7734/Reviewer_SBgK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7734/Reviewer_SBgK"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response. I have some additional questions."
                    },
                    "comment": {
                        "value": "Thank you for the very detailed response. I have some additional questions:\n\n1. The new results in Table 3 are interesting. Could you please tell me more details about the experimental settings? How do you prepare training and test data?\n\n2. Table 8 shows a significant improvement over G2SAT.  Why do you compare with G2SAT? What happens if we compare MixSAT with other baselines?"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7734/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700562846307,
                "cdate": 1700562846307,
                "tmdate": 1700562846307,
                "mdate": 1700562846307,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KP9sLIqFbV",
                "forum": "PXXuLvIH5r",
                "replyto": "FtHfAsgLjC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7734/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7734/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer SBgK"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your prompt reply and the acknowledgment of our response. Below we respond to your new questions.\n\n> Q1: The new results in Table 3 are interesting. Could you please tell me more details about the experimental settings? How do you prepare training and test data?\n\nWe adhere closely to HardSATGEN's experimental settings of its solver tuning experiment. The data used for tuning the solver aligns with the datasets employed in previous experiments concerning graph structure (Sec. 3.2) and computational hardness (Sec. 3.3). The training set is consistent with previous experiments, specifically the HARD dataset. Trained models are then applied to generate new instances based on reference instances hard-a, hard-d, hard-f, and hard-g,  which are selected from the HARD dataset with runtimes of less than 500 seconds. This selection is motivated by the very time-intensive nature of hyperparameter optimization, involving testing all augmented instances with 200 sets of hyperparameters. Opting for instances with shorter runtimes, such as the aforementioned four, helps mitigate experiment overhead. The reference instances collectively form the test data. The underlying expectation is that the augmented data could effectively guide the solver to enhance its performance on the original data.\n\n> Q2: Table 8 shows a significant improvement over G2SAT. Why do you compare with G2SAT? What happens if we compare MixSAT with other baselines?\n\n\nPrevious learning-based methods, such as G2SAT, GCN2S, and HardSATGEN, share a common foundation built on the node merge-split framework brought out by G2SAT, employing the same main generation process involving a sequence of node-merging operations (differing mainly in the use of differently trained networks to select the node pairs for merging). Consequently, the runtime results of G2SAT serve as the representative for these approaches operating within the same paradigm. The remaining baselines under the split-merge paradigm show no significant difference in terms of time efficiency compared to G2SAT.\n\nGiven that the efficiency comparison primarily centers around learning-based methods, traditional hand-crafted methods are not included in Table 8. However, we can still provide discussions of these methods in this response below:\n\nTraditional generative methods including CA and PS can generate instances within mere seconds and require no training. They are more efficient than MixSAT. However, since the paradigm of these methods basically follows the pipeline of first identifying one or two structural metrics and then specifying an algorithm capable of controlling these metrics, these models fail to unravel specific data characteristics adaptively, merely matching partial structural metrics through manual control, as discussed in G2SAT and HardSATGEN. This can also be evidenced by Tables 1-3.\n\n---\n\nWe hope this response could help address your concerns, and look forward to receiving your feedback soon."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7734/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700573605631,
                "cdate": 1700573605631,
                "tmdate": 1700611985820,
                "mdate": 1700611985820,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MYTm2Kw2sG",
                "forum": "PXXuLvIH5r",
                "replyto": "KP9sLIqFbV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7734/Reviewer_SBgK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7734/Reviewer_SBgK"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for answering my questions."
                    },
                    "comment": {
                        "value": "Thank you for the response. I appreciate the hard work of the authors, and I feel that the paper is significantly improved. However, there remain some concerns:\n\n1. The experimental results on runtime evaluation (Table 2) are not strong. This result mismatches with the claim described in the introduction. In the introduction, the proposed method is said to achieve hardness reproducing and structure resemblance. However, the MixSAT does not work well on preserving the hardness of UNSAT instances, and it is unclear why it does not work well. \n\n2. The proposed method is evaluated with a few instances whose selection procedure (Appendix D) seems somewhat subjective. From the information described in Appendix D, I cannot judge whether the selection process of instances is fair or not.\n\n3. The newly added results on parameter tuning are interesting. However, I think the experimental settings explained by the authors are not convincing since they use instances augmented from hard-[adfg] for tuning parameters and evaluate the performance of the tuned models using the original hard-[adfg] as the test data. This is unrealistic since we usually cannot access test data when we train a model; it seems easy to select the best hyperparameters if we know the test instances when we tune the parameter. The paper should use different instances for training and testing."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7734/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633577471,
                "cdate": 1700633577471,
                "tmdate": 1700633577471,
                "mdate": 1700633577471,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9HI1u1r8Xx",
                "forum": "PXXuLvIH5r",
                "replyto": "FtHfAsgLjC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7734/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7734/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Further Response to Reviewer SBgK"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your valuable engagement and the time you dedicated to reviewing our work. However, we may hold differing opinions on certain points you raised. Below we respond to your concerns.\n\n> **The experimental results on runtime evaluation (Table 2) are not strong. This result mismatches with the claim described in the introduction. In the introduction, the proposed method is said to achieve hardness reproducing and structure resemblance.** \n\nWe admit that Table 2's results are not strong. But as we noted before, the resemblance in computational properties is very challenging (as listed as one of the ten key challenges in propositional reasoning and search [1]), and in fact MixSAT's results have already been the best among all other hand-crafted methods and learning-based methods **with general applicability**. Please note again HardSATGEN is a tailored method for UNSAT instances, whose success is at the cost of its ability to learn from SAT instances. Moreover, we respectfully believe underperforming one baseline under a specific task for partial data should not directly lead to rejection. We have revised the expression \"achieve hardness reproducing\" in the introduction to \"achieve better hardness resemblance\" to more accurately align with the observed empirical results.\n\n[1] Ten challenges in propositional reasoning and search. IJCAI 1997.\n\n> **The proposed method is evaluated with a few instances whose selection procedure (Appendix D) seems somewhat subjective. From the information described in Appendix D, I cannot judge whether the selection process of instances is fair or not.**\n\n\nWe assert that the principles outlined in Appendix D are devoid of any subjective bias. We never cherry-picked the instances with the aim of maximizing MixSAT's performance. We are in the process of organizing our code and plan to upload it before the rebuttal deadline to ease your concerns. The rationale behind the three rules of selection in Appendix D is as follows: \n\n1) The objective is to augment real-world SAT data to solve the data bottleneck in real-world scenarios, it is natural to select real-world data rather than random instances. \n\n2) To test the applicability generality of the methods (i.e. performance on SAT and UNSAT sources), we select an equal number of SAT and UNSAT instances to compose the datasets. This is natural to approximate real-world application data that encompasses both SAT and UNSAT instances (otherwise we may not need a SAT solver to decide instances' satisfiability). \n\n3) The imposition of a hardness requirement serves to guarantee the verifiability of the solver runtime experiment. We initially selected the dataset of G2SAT to evaluate our methods, but we observed that most of its instances could be solved in nearly 0 seconds, which made the solver runtime comparison meaningless. Thus, for solver runtime evaluation, we select hard instances from G2SAT and supplement additional instances from SAT competition to form the Hard dataset. While the instances in the Easy dataset are all from G2SAT. \n\n\nWe consider these selections to be fundamental and intuitive requirements. If you are still confused about some of the specific selection rules, please let us know.\n\n> **However, I think the experimental settings explained by the authors are not convincing since they use instances augmented from hard-[adfg] for tuning parameters and evaluate the performance of the tuned models using the original hard-[adfg] as the test data.**\n\nThis design is out of the consideration that the objective is to augment SAT instances that follow the original data distribution, thereby solving the data bottleneck. Please kindly note that this is not a training-testing paradigm, but rather a reference-augmentation paradigm similar to Tables 1 and 2 for structure and hardness resemblance evaluation.  The performance of the solver tuned on instances augmented from hard-[adfg] and tested on the original hard-[adfg] instances shows the higher-level resemblance of the generated instances to the reference instances (obeying the same underlying distribution), such that the augmented data could effectively guide the solver to enhance its performance on the original data sources. This achievement is non-trivial, as can be evidenced by the failures of all previous methods in this setting including HardSATGEN. Please also note that this experimental setting exactly follows the solver tuning results in HardSATGEN. \n\n---\n\nWe hope this response could help address your remaining concerns. We believe our work could contribute to the community as a novel framework for SAT instance generation, distinct from previous methods adhering to the split-merge framework. We sincerely wish that you could reconsider the methodology novelty, efficiency superiority, and general empirical superiority on structure and hardness, as well as the potential impact of our work on the community. We look forward to receiving your feedback soon."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7734/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700657865375,
                "cdate": 1700657865375,
                "tmdate": 1700670566863,
                "mdate": 1700670566863,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PocoBZTFeS",
                "forum": "PXXuLvIH5r",
                "replyto": "9HI1u1r8Xx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7734/Reviewer_SBgK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7734/Reviewer_SBgK"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the further response"
                    },
                    "comment": {
                        "value": "Thank you for the further response. \n\n---\n\nThe data selection procedure explained by the authors is the following:\n\n1.  The number of SAT/UNSAT instances is equal.\n2.  Select real-world and hard instances.\n\nSince these rules would not be enough to identify the hard instances used in experiments from SATLIB and SAT 2021,  I think that there are some implicit rules in the selection procedure.\n\n---\n\nI also agree that the results of the hyperparameter tuning task are non-trivial. However, these results do not show that the proposed method can be used to solve the data scarcity problem described in the introduction:\n> Both model training for modern learning-based SAT solvers or hyperparameter tuning for traditional solvers require relevant instances as many as possible to either tune the trainable model parameters or the hyperparameters. Unfortunately, SAT instances are often scarce in practice\n\nI think both use cases described above follow a training-testing paradigm."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7734/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700690212166,
                "cdate": 1700690212166,
                "tmdate": 1700690212166,
                "mdate": 1700690212166,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "x5Fl0XQRRM",
                "forum": "PXXuLvIH5r",
                "replyto": "FtHfAsgLjC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7734/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7734/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Further Response to Reviewer SBgK"
                    },
                    "comment": {
                        "value": "Thank you for your timely reply and your valuable engagement in this tight rebuttal window, which helps a lot for us to refine our paper. \n\n> **Since these rules would not be enough to identify the hard instances used in experiments from SATLIB and SAT 2021, I think that there are some implicit rules in the selection procedure.**\n\nIndeed, our instances are originally sourced from G2SAT, a dataset derived from SATLIB and SAT 2021. The Easy dataset is directly from G2SAT. The Easy dataset directly originates from G2SAT. However, given the scarcity of challenging instances in G2SAT (only 4 hard instances), we have supplemented the remaining 4 instances from SAT 2021, ensuring they share similar problem scales with G2SAT instances. The underlying implicit rule may be that current graph network based methods may not be able to scale to very large graphs (SAT 2021 often contains problems with 10k variables and 100k clauses). While also, our affinity matrix construction is somehow resource-consuming, thus MixSAT still cannot exceed the scale limit of current learning-based methods. Thanks again for noting, we are sorry that we omitted this underlying scale requirement, we will supplement this in Appendix D. On the other hand, we have uploaded our code to https://anonymous.4open.science/r/MixSAT/, which can be evaluated by any data to prove MixSAT's performance.\n\n> **I also agree that the results of the hyperparameter tuning task are non-trivial. However, these results do not show that the proposed method can be used to solve the data scarcity problem described in the introduction.**\n\nThe ultimate objective of SAT instance generation is to generate instances that resemble the original data, regarding structure and hardness. As guided in *Ten challenges in propositional reasoning and search (IJCAI 1997)*:\n\n\"CHALLENGE 10: Develop a generator for problem instances that have computational properties that are more similar to real-world instances.\"\n\nWe believe the requirement of the resemblance between the generated and original data is sufficient to resolve the data scarcity problem. After all, that is the whole point of SAT instance generation methods in the first place. Our newly supplemented solver tuning results can support this data resemblance on a higher level, which surpasses all other baselines. From the application perspective, we admit it should better follow the training-testing paradigm, while our current solver tuning results may not be supportive enough for this point. In your initial review, you mentioned HardSATGEN as an example for the hyperparameter tuning experiments. It is per your request to follow its setting and supplement the new results to show the superiority of MixSAT. We believe the new results are still meaningful and can strengthen our paper as evidence for a higher level of resemblance between the generated and original data. Please also note that previous baselines are all ineffective for the current solver tuning setting, let alone the solver tuning performance for application purposes. We will continue to make efforts to another version of solver tuning results, but since the discussion window is closing, we wonder whether our efforts in the rebuttal period have met your requests and addressed most of your concerns, we would sincerely appreciate it if you could reconsider the current overall paper contribution and the massive new results supplemented in this very tight rebuttal window to strengthen our paper.\n\nThanks again for your interest and dedicated efforts in reviewing our work. We truly appreciate that and look forward to receiving your feedback soon."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7734/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700707112575,
                "cdate": 1700707112575,
                "tmdate": 1700725087184,
                "mdate": 1700725087184,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "SjYPUwOPZN",
            "forum": "PXXuLvIH5r",
            "replyto": "PXXuLvIH5r",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7734/Reviewer_5Kyc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7734/Reviewer_5Kyc"
            ],
            "content": {
                "summary": {
                    "value": "This paper is devoted to machine learning assisted generation of\nfamilies of benchmark formulas for propositional satisfiability (SAT)\nsolvers. The paper builds on the use of graph representations of\nformulas in conjunctive normal form (CNF) and utilizes a graph\ninterpolation approach, which modifies the graph structure by\nreplacing some of its substructures from other similar instances. The\npaper argues that the proposed approach is able to maintain the\noriginal hardness of the corresponding families of benchmarks, thanks\nto the use of Gumpel noise. The presented experimental results claim\nto demonstrate the advantage over the state of the art in SAT\nbenchmark generation from the perspective of (1) maintaining the\nstructure of the benchmark family and (2) its hardness."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The entire flow of the proposed approach are provided one after\n  another.\n- The overall presentation is nice, especially the figures look\n  beautiful and help a reader understand the ideas.\n- To the best of my understanding, the proposed approach is novel."
                },
                "weaknesses": {
                    "value": "- Although all the steps of the approach are listed, they are not\n  augmented with clear arguments for why they are used. This seems to\n  be a standard issue with all the works on applying ML methods in/for\n  combinatorial problem solvers where the authors propose to apply\n  ~50-100 steps with no clear justification. This trend is clearly\n  detached from the mainstream SAT research where each idea has to be\n  clearly articulated and justified. Granted it may be just me getting\n  lost in matters that are straightforward to any ML expert.\n\n- Experimental results look rather weak to me as they seem to be a\n  mixed bag. At least, I don't see a clear advantage of the proposed\n  approach over the state of the art in terms of the hardness of the\n  generated formulas.\n\n- Minor: the authors say that SAT is a combinatorial optimization (CO)\n  problem while it isn't. There is no optimization in the original\n  decision formulation of SAT.\n\n- Minor: I would suggest rewording the sentence (in the introduction):\n  \"resemble the computational complexity\". Computational complexity of\n  SAT is well understood."
                },
                "questions": {
                    "value": "- A key question is this line of research is whether the formulas\n  generated automatically exhibit the properties intrinsic to\n  \"industrial instances\". This is important given the widespread use\n  of modern SAT solvers in industrial settings. So I would like to ask\n  the authors whether the ability of their approach to capture this.\n\n- Can you comment on the rationale of (at least some of) the steps\n  your approach performs?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7734/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698737452425,
            "cdate": 1698737452425,
            "tmdate": 1699636943565,
            "mdate": 1699636943565,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YgdqMCibf2",
                "forum": "PXXuLvIH5r",
                "replyto": "SjYPUwOPZN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7734/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7734/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response by Authors (1/2)"
                    },
                    "comment": {
                        "value": "Thanks for the valuable comments, and recognition of our presentation and novelty. Below we respond to your specific comments.\n\n> **Q1: The rationale of the steps the approach performs.**\n\nThanks for the note. We totally agree with your assertion that the rationale behind every effort of the methodology design should be articulated and elucidated in the paper. \n\nWe believe this principle should be a consensus across both the mainstream SAT research and the ML4SAT research. In fact, we try to make efforts to achieve such clarity in the original paper. Indeed, we acknowledge that certain points may not have been sufficiently clarified, leading to confusion. Here we would like to elucidate the underlying rationale for each design choice in our methodology. We hope this clarification addresses your concerns and fortifies the overall clarity of our work.\n\n- **Why mixing?** Previous node split-merge framework for SAT instance generation exhibits poor performance in computational hardness resemblance. Though more fine-grained controls in [1] on structures can improve the performance, it also introduces applicability limitations. Thus we resort to a different graph interpolation approach that directly manipulates the raw structures of SAT instances in place, which is achieved by mixing substructures of SAT instances. This design is more efficient, more adaptive, and can naturally avoid the hardness degradation causes in the split-merge framework as analyzed in [1]. (Page 2)\n\n- **Why matching?** The exchanged substructures should be matched such that the mixing process can largely preserve the original essential patterns and the mixed instances remain within the underlying distribution of reference instances regarding both structural similarity and computational hardness. (Page 3)\n\n- **Why adopt the neural SAT solving task as a pretraining task?** It is for the sake of hardness-awareness. This pretraining task enables the model to not only capture the graph structure but also naturally discover the inherent properties regarding the computational hardness of the instances. (Page 4)\n\n- **Why perform online optimization with matching loss?** The pretraining task guarantees plausible initial node features, but it is not directly related to the matching purpose. The online optimization with matching loss is to directly maximize the similarity between the node features of the two SAT instances. (Page 5)\n\n- **Why introduce randomness?** Learning-based methods excel at identifying similarities within structures. With the replacement on top of the learned variable correspondence map, similar substructures from two reference graphs can easily interact and probably lead to conflicts and consequent hardness degradation. Thus, beyond the regular matching pipeline, a tunable level of randomness could avoid hardness degradation. (Page 5)\n\n- **Why use Gumbel trick to introduce randomness?** Introducing randomness to assignments is non-trivial, while Gumbel trick on discrete assignment distributions is well studied in [4] with satisfying sampling property that $H(\\mathbf{M}+\\mathbf{\\Gamma})\\sim\\mathcal{G.M.}(\\mathbf{M})$, where $\\mathbf{\\Gamma}$ denotes a Gumbel noise matrix, $\\mathcal{G.M.}(\\mathbf{M})$ is the Gumbel-Matching distribution with parameter $\\mathbf{M}$, and $H(\\cdot)$ denotes the matching operator providing the best matching results, which can be achieved by the Hungarian algorithm for the linear matching case. (Page 5)\n\n- **Why this mixing algorithm?** The mixing algorithm we propose in this paper is simple and natural. It directly utilizes the variable matching confidence obtained in the matching process, which increases the probability of replacing similar structures. We perform substitutions at the clause level because clauses are the smallest substructure of the SAT formula, thus they correspond to the smallest interpolation step in the interpolation scenario. (Page 6)\n\nWe have revised the manuscript accordingly to increase the presentation clarity of our paper. Please let us know if you have any other confusion regarding the approach rationale."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7734/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700409741452,
                "cdate": 1700409741452,
                "tmdate": 1700409741452,
                "mdate": 1700409741452,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "T0h1C8fWPy",
                "forum": "PXXuLvIH5r",
                "replyto": "SjYPUwOPZN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7734/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7734/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response by Authors (2/2)"
                    },
                    "comment": {
                        "value": "> **Q2: Experimental results on hardness seem weak.**\n\nAlthough MixSAT performs weaker than HardSATGEN [1] on unsat instances, it outperforms other methods. Please note that reproducing computational hardness in the SAT instance generation task is considerably challenging which has not been achieved until [1]. Its success stems from its fine-grained control over the instance structure, particularly for the unsatisfiable core structures, as well as the postprocessing design involved to maintain the dominant effect of the unsat core structure.\n\nWhile the design in [1] is effective, it confines the method to unsatisfiable instances only. Since real-world datasets generally incorporate both satisfiable and unsatisfiable instances, merely learning from the unsat instances can lead to certain negative impacts on downstream tasks like hyperparameter tuning, as evidenced by Table 3 in the current revision. In contrast, MixSAT surpasses this limitation, demonstrating generality to both satisfiable and unsatisfiable instances. MixSAT is also free from the node merge-split framework of previous learning-based methods [1-3], naturally meeting the hardness analysis and requirements set forth in [1].  Notably, our performance on satisfiable instances is significantly superior to all other methods. \n\nWe acknowledge that our effectiveness on unsatisfiable instances is not as strong, especially considering [1] already provides a tailored solution for unsatisfiable problems. However, we believe the two methods can complement each other. As evidence, we have supplemented the study with hyperparameter optimization experiments and present the combined effects of both methods in Table 3, which shows the significant effectiveness of our methods in practice as a boosting tool complementary to HardSATGEN.\n\n> **Q3: Whether the formulas generated automatically exhibit the properties intrinsic to \"industrial instances\"?**\n\nThe data we adopt in our experiments are exactly derived from real-world scenarios (from SAT Competition Benchmarks). Note the disparity between industrial data and real-world data can be viewed as minimal, as both originate from practical application scenarios. The experiments are designed to evaluate whether the generated instances exhibit the properties intrinsic to real-world instances. The evaluation for the real-world resemblance is two-fold: 1) structural properties compared to the reference real-world instances (Sec. 3.2); 2) computational properties compared to the reference real-world instances (Sec. 3.3). In the rebuttal period, we also supplement solver hyperparameter tuning results in Sec. 3.4, which indicates that the generated instances maintain close properties to the reference real-world instances so that hyperparameter tuning led by them could bring a significant solver runtime reduction of over 50% to the solver. The evaluation has already shown the effectiveness of MixSAT in generating instances that exhibit the properties intrinsic to real-world instances.\n\n> **Q4: Minor issues and suggestions.**\n\nWe sincerely appreciate your thorough review. We will revise the manuscript accordingly to elevate it into a more refined paper.\n\n---\nWe hope this response could help address your concerns. We sincerely wish that you could reconsider the novel contribution and potential impact of our work on the community. We look forward to receiving your valuable feedback soon.\n\n---\n\n**Reference:**\n\n[1] HardSATGEN: Understanding the Difficulty of Hard SAT Formula Generation and A Strong Structure-Hardness-Aware Baseline. KDD 2023.\n\n[2] G2SAT: Learning to Generate SAT Formulas. NeurIPS 2019.\n\n[3] On the Performance of Deep Generative Models of Realistic SAT Instances. SAT 2022.\n\n[4] Learning latent permutations with gumbel-sinkhorn networks. ICLR 2018."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7734/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700409844551,
                "cdate": 1700409844551,
                "tmdate": 1700455714397,
                "mdate": 1700455714397,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DG7WZZ51xv",
                "forum": "PXXuLvIH5r",
                "replyto": "YgdqMCibf2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7734/Reviewer_5Kyc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7734/Reviewer_5Kyc"
                ],
                "content": {
                    "title": {
                        "value": "Reply to the authors"
                    },
                    "comment": {
                        "value": "Thanks for the reply and for revising the paper. I appreciate the effort you've put in this."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7734/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700608897019,
                "cdate": 1700608897019,
                "tmdate": 1700608897019,
                "mdate": 1700608897019,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YEY5fDWhih",
                "forum": "PXXuLvIH5r",
                "replyto": "T0h1C8fWPy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7734/Reviewer_5Kyc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7734/Reviewer_5Kyc"
                ],
                "content": {
                    "title": {
                        "value": "Reply to the authors 2"
                    },
                    "comment": {
                        "value": "Again, thank you for revising the manuscript. My understanding is that the main benefit claimed in the paper now is that it somewhat outperforms the competition in the case of satisfiable instances although it loses to the competition for the unsatisfiable ones. The new discussion should clearly facilitate a reader's understanding of how far/close the generated instances are from/to those they are meant to resemble."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7734/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700609144128,
                "cdate": 1700609144128,
                "tmdate": 1700609144128,
                "mdate": 1700609144128,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7pagZd5VZv",
            "forum": "PXXuLvIH5r",
            "replyto": "PXXuLvIH5r",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7734/Reviewer_KdVD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7734/Reviewer_KdVD"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes MixSAT, a ML-based generation approach for SAT instances. Instead of generating formulas ex novo, MixSAT interpolates exising pairs of formulas using:\n\n-    GNN-based representation learning for capturing structural properties in a latent representation\n-    Differentiable Gumbel-Sinkhorn stochastic matching between instances\n-    Graph interpolation via iterative clause replacement\n\nThe generated formulas statistically retain similar structural properties and preserve the hardness of the original instances."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "-    Overall very clear presentation\n-    Well-motivated problem\n-    The proposed technique seems sound and novel\n-    Promising initial results"
                },
                "weaknesses": {
                    "value": "-    Minor points related to the presentation and evaluation\n-    I would also discuss the limitations of MixSAT"
                },
                "questions": {
                    "value": "1) Besides structural properties and hardness, does MixSAT preserve the SAT/UNSAT ratio of the original data?\n2) What are the limitations of your approach?\n3) Do you observe a degratation in performance when interpolating formulas generated by MixSAT?\n4) Is it possible to learn a binary classifier that accurately discriminates original vs. MixSAT instances?\n\nMinors:\n\n\"The Boolean satisfiability problem (SAT) stands as a canonical NP-complete combinatorial optimization (CO) problem\"\n\nSAT is a decision problem, not a CO. Max-SAT is the optimization counterpart.\n\n\"there exists an assignment of Boolean variables that satisfies a Boolean formula and in general is NP-hard.\"\n\nSAT is NP-complete. I understand that, in principle, the formulas that MixSAT generates could be used in non-decision settings too. I would then rephrase the sentences above, making it clear that you address generation of propositional logic instances for decision (SAT) and other problems (e.g. Max-SAT)\n\n\"e.g. in EDA.\"\n\nWhat EDA stands for?\n\nAdding the pseudocode of MixSAT would greatly help."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7734/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698763476087,
            "cdate": 1698763476087,
            "tmdate": 1699636943448,
            "mdate": 1699636943448,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oCBSC3r0Ns",
                "forum": "PXXuLvIH5r",
                "replyto": "7pagZd5VZv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7734/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7734/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response by Authors (1/2)"
                    },
                    "comment": {
                        "value": "Thanks for the valuable comments, nice suggestions, and for acknowledging the presentation clarity, methodology novelty, and technical soundness of our paper. Below we respond to your specific comments.\n\n> **Q1: Besides structural properties and hardness, does MixSAT preserve the SAT/UNSAT ratio of the original data?**\n\nThanks for the comment. The results of satisfiability phase preservation for generated formulas are presented below and also in Table 10 of the current revision. It is observed that despite being trained on a dataset with a one-to-one ratio of satisfiable to unsatisfiable instances, previous methods exhibit a tendency to generate a specific satisfiability phase. Notably, instances generated by G2SAT, GCN2S, and HardSATGEN are all unsatisfiable, while PS generates exclusively satisfiable instances. Among all the methods, MixSAT maintains the closest phase ratio to the original reference instances.\n\nUnsatisfiable ratio of the generated instances:\n\n|  **Method**  | **EASY** | **HARD** |\n|:-:|:-:|:-:|\n| Ground Truth | 50.00%  | 50.00%  |\n|  CA  |  87.5%  | 75.00%  |\n|  PS  |  0.00%  | 0.00%  |\n|  G2SAT | 100.00% | 100.00% |\n| GCN2S | 100.00% | 100.00% |\n|  HardSATGEN  |  100%   | 100.00% |\n| MixSAT | 23.61%  | 43.41%  |\n\n> **Q2: What are the limitations of your approach?**\n\nCurrently, the primary limitation of our approach lies in the fact that our hardness reproducing performance on unsatisfiable examples, though better than other methods, is not as effective as that achieved by HardSATGEN [1]. However, please note that reproducing computational hardness in the SAT instance generation task is considerably challenging which has not been achieved until [1]. Its success stems from its fine-grained control over the instance structure, particularly for the unsatisfiable core structures, as well as the postprocessing design involved to maintain the dominant effect of the unsat core structure.\n\nWhile the design in [1] is effective, it confines the method to unsatisfiable instances only. Since real-world datasets generally incorporate both satisfiable and unsatisfiable instances, merely learning from the unsat instances can lead to certain negative impacts on downstream tasks like hyperparameter tuning, as evidenced by Table 3. In contrast, MixSAT surpasses this limitation, demonstrating generality to both satisfiable and unsatisfiable instances. MixSAT is also free from the node merge-split framework of previous learning-based methods [1-3], naturally meeting the hardness analysis and requirements set forth in [1].  Notably, our performance on satisfiable instances is significantly superior to all other methods. \n\nOn the other hand, we believe the two methods can complement each other. As evidence, we have supplemented the study with hyperparameter optimization experiments and present the combined effects of both methods in Table 3 of the current revision, which shows the significant effectiveness of our methods in practice as a boosting tool complementary to HardSATGEN.\n\n\n> **Q3: Do you observe a degratation in performance when interpolating formulas generated by MixSAT?**\n\nTo evaluate the effect of secondary interpolation on the performance of formulas generated by MixSAT, we select two representative instance pairs and apply MixSAT ($\\lambda=0.1,\\tau=1$) to obtain their 5% mixing ratio formulas. We then use these formulas as the start formulas and interpolate them with their corresponding goal formulas, resulting in 10% mixing ratio formulas. We compare these formulas with the original formulas, the 5% start formulas, and the 10% formulas obtained by one interpolation. The comparison is shown below and also in Table 9 of the current revision, where we can see that the secondary interpolation does not degrade the performance of the formulas, as they maintain the same level of hardness as the one-interpolated formulas. This demonstrates the robustness of MixSAT in generating hard formulas through interpolation.\n\n|   | **Ground Truth** |  **5%**  | **Interpolated 10%** | **10%**  |\n|:-:|:-:|:-:|:-:|:-:|\n| **hard $b$-$a$** | 1382.71 | 1023.25 $\\pm$ 4.69  | 1001.97 $\\pm$ 7.28 | 1042.53 $\\pm$ 4.78 |\n| **hard $d$-$c$** | 852.85 | 199.63 $\\pm$ 190.70 | 6.13 $\\pm$ 3.14 | 3.56 $\\pm$ 1.69  |"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7734/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700409353475,
                "cdate": 1700409353475,
                "tmdate": 1700409367356,
                "mdate": 1700409367356,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kQKDcD5YwL",
                "forum": "PXXuLvIH5r",
                "replyto": "7pagZd5VZv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7734/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7734/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response by Authors (2/2)"
                    },
                    "comment": {
                        "value": "> **Q4: Is it possible to learn a binary classifier that accurately discriminates original vs. MixSAT instances?**\n\nDue to the limited data availability in specific SAT scenarios (which is also the motivation for designing generation algorithms), the current dataset size is insufficient to support the training of a classification network. In fact, we believe this situation is widely acknowledged in literature [1,2].\n\nWe understand your concern, as classification is a method to discern whether the generated distribution approximates the real one. Indeed, in our experiments, out of the same consideration, we have already validated the structural and hardness similarities between generated and real instances. Additionally, during the rebuttal period, we have highlighted MixSAT's superior performance in solver tuning. We believe these results sufficiently support the approximation of the distribution between the data generated by our method and real-world data.\n\n> **Q5: What EDA stands for?**\n\nEDA stands for \"Electronic Design Automation\", a practical application scenario of SAT problem, where SAT instances exhibit scarcity. Sorry for the confusion and we have fixed it in the revision.\n\n> **Q6: Pseudocode of MixSAT.**\n\nThanks for the comment. We supplement the pseudocode in Appendix C to further enhance our methodology presentation clarity.\n\n\n\n> **Q7: Minor issues and suggestions.**\n\nWe sincerely appreciate your thorough review. We will revise the manuscript accordingly to elevate it into a more refined paper.\n\n\n---\nWe hope this response could help ease your concern and wish to receive your further feedback soon.\n\n---\n\n**Reference:**\n\n[1] HardSATGEN: Understanding the Difficulty of Hard SAT Formula Generation and A Strong Structure-Hardness-Aware Baseline. KDD 2023.\n\n[2] G2SAT: Learning to Generate SAT Formulas. NeurIPS 2019.\n\n[3] On the Performance of Deep Generative Models of Realistic SAT Instances. SAT 2022.\n\n[4] Locality in random SAT instances. IJCAI 2017."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7734/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700409498939,
                "cdate": 1700409498939,
                "tmdate": 1700409953129,
                "mdate": 1700409953129,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]