[
    {
        "title": "($\\texttt{PASS}$) Visual Prompt Locates Good Structure Sparisty through a Recurent HyperNetwork"
    },
    {
        "review": {
            "id": "UW71otmdxv",
            "forum": "Uavy4DLrXR",
            "replyto": "Uavy4DLrXR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6393/Reviewer_dU7f"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6393/Reviewer_dU7f"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a recurrent mechanism with LSTM to acquire layer-wise sparse masks, considering both the sparse masks from previous layers and visual prompts.\n\nThe paper has achieved commendable performance on CIFAR and Tiny ImageNet datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The presentation of this paper is excellent, with professional handling of formulas, images, and expressions.\n\n2. The paper has achieved commendable performance on CIFAR and Tiny ImageNet datasets."
                },
                "weaknesses": {
                    "value": "1. The novelty of this paper is relatively limited. Several methods have already been proposed to address the intricate dependencies arising from channel elimination across layers with sequence network, such as the RNN-based SkipNet[1]. To enhance the novelty, the author is encouraged to explore a broader range of dynamic neural network literature, as numerous ideas and methods have been introduced in this domain over the years. \n\nIt is necessary to compare these methods and elucidate their differences.\n\nThe reviewer possesses a deep understanding of dynamic networks with sequence modeling. Any potential misconceptions in the reviewer's understanding can be clarified during the rebuttal phase.\n\n2. Visual prompts are typically designed for fine-tuning with limited data and domain transfer scenarios (e.g., transform the ImageNet model to CIFAR), but the author claims that the visual prompt plays a key role in pruning. However, the experiments in this work seem challenging to support this argument, as all the gains from visual prompts appear to be very marginal, less than or equal to 1%. Such experimental results are hard to be convincing. \n\nAdditionally, prompt learning relies on a strong foundation of pre-trained models. To demonstrate its effectiveness in network pruning, favorable experiments and analyses are essential.\n\n In cases where pruning a model without fine-tuning, the visual prompt is unnecessary, in such a scenario, it seems that the paper may not work.\n\n3. The experiments conducted on small datasets, such as CIFAR and Tiny-ImageNet, with very low resolution and data scale are not entirely convincing. The reviewer suggests including experiments on at least ImageNet-1k or ImageNet. In the era of big data, ImageNet is considered a relatively small dataset.\n\n[1] Wang, Xin, Fisher Yu, Zi-Yi Dou, Trevor Darrell, and Joseph E. Gonzalez. \"Skipnet: Learning dynamic routing in convolutional networks.\" In Proceedings of the European Conference on Computer Vision (ECCV), pp. 409-424. 2018.\n\n\n----------------------\n\nAfter reading the rebuttal, the reviewer raised the score to 5."
                },
                "questions": {
                    "value": "1. Could the author explain that why the visual prompts improve channel pruning? Since the visual prompts are static across a task or a dataset, why the author state their pruning method as \u201cfrom a data-centric perspective\u201d while it is not even input dependent?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6393/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6393/Reviewer_dU7f",
                        "ICLR.cc/2024/Conference/Submission6393/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6393/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697956664896,
            "cdate": 1697956664896,
            "tmdate": 1700550757126,
            "mdate": 1700550757126,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "B3CrMSIWe8",
                "forum": "Uavy4DLrXR",
                "replyto": "UW71otmdxv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6393/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6393/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dU7f (1/4)"
                    },
                    "comment": {
                        "value": "**We sincerely appreciate your detailed comments. We are glad that you found our presentation, formulas, images, and expressions professional. We provide point-wise responses to address your concerns below.**\n\n**Q1:** *The novelty of this paper is relatively limited. Several methods have already been proposed to address the intricate dependencies arising from channel elimination across layers with sequence network, such as the RNN-based SkipNet[1]. To enhance the novelty, the author is encouraged to explore a broader range of dynamic neural network literature, as numerous ideas and methods have been introduced in this domain over the years. It is necessary to compare these methods and elucidate their differences.*\n\n**Reply:**\n\n**Cons: Limited novelty:** We respectfully disagree with this assessment and would like to emphasize the technical novelty of our paper, which is primarily manifested in two aspects:\n\n- **Exploring visual prompts in pruning vision models is novel**: As appreciated by **Reviewer F4Cf** and **Reviewer NTwF**, our exploration of visual prompts to network pruning is novel and innovative. Recent advances like in-context learning and prompting have demonstrated great success in the field of LLMs. [1] demonstrates that learning post-pruning prompting can improve the performance of pruned LLMs without changing their weights. However, the effect of visual prompts on sparsifying vision models remains mysterious, due to the fact that visual prompts are inherently more complex to comprehend and typically pose greater challenges in terms of both design and learning, in comparison to their linguistic counterparts. To this end, we propose a novel framework that enabling incorporating visual prompting into the sparsification process, opening the door to the exciting possibility to leveraging visual prompting to prune vision models.\n\n- **The framework for fusing visual prompt and model information is novel**: We introduce a novel framework designed to fuse visual prompt data with model information specifically for channel pruning. The complexity of this challenge stems from the disparate nature of visual prompts and model information, each embodying unique data structures and characteristics. Integrating these two elements effectively for structural pruning is not trivial. Through our thorough ablation studies, we provide empirical evidence showcasing the efficacy of our framework in seamlessly incorporating visual prompts with model information for better structural pruning.\n\n**Cons: Difference between our proposed PASS and dynamic neural network?:** There are two fundamental differences between our proposed PASS and dynamic neural network.\n\n- **The hyper-network in our proposed PASS is not 'dynamic'.** Dynamic neural networks, as categorized in the literature, are networks capable of adapting their structures or parameters conditioned in a sample-dependent manner, as outlined in [2]. In contrast, the hyper-network within our PASS framework does not exhibit this 'dynamic' nature. It is designed to be dependent on a visual prompt (task-specific), as opposed to dynamically adjusting to input samples. This hyper-network's role is confined to the channel mask identification phase and is not employed during the inference phase. Therefore, it is fundamentally different from dynamic neural networks.\n\n- **Their goals are different.** The fundamental goal of the hyper-network in PASS is distinct from that of dynamic neural networks. While the latter focuses on adapting their architecture or parameters based on input samples, our hyper-network is specifically engineered for the integration of visual prompts with the statics of model weights.\n\n\nAdditionally, the selection of LSTM as the hyper-network was a deliberate choice, driven by its proven ability to effectively manage inherent channel dependencies via the recurrent mechanism. The efficacy of this choice has been rigorously validated through our ablation studies, as detailed in Section 5.1 (In our submission). \n\nWe will update the description for differences of our proposed PASS and dynamic neural network in the revised version (in Appendix F).\n\n[1] Xu, Zhaozhuo, Zirui Liu, Beidi Chen, Yuxin Tang, Jue Wang, Kaixiong Zhou, Xia Hu, and Anshumali Shrivastava. \"Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt.\" arXiv preprint arXiv:2305.11186 (2023).\n\n[2] Han, Yizeng, et al. \"Dynamic neural networks: A survey.\" IEEE Transactions on Pattern Analysis and Machine Intelligence 44.11 (2021): 7436-7456."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6393/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700421289512,
                "cdate": 1700421289512,
                "tmdate": 1700470194371,
                "mdate": 1700470194371,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "r4O4CF5bIG",
                "forum": "Uavy4DLrXR",
                "replyto": "UW71otmdxv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6393/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6393/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dU7f (2/4)"
                    },
                    "comment": {
                        "value": "**Q2:** *Visual prompts are typically designed for fine-tuning with limited data and domain transfer scenarios (e.g., transform the ImageNet model to CIFAR), but the author claims that the visual prompt plays a key role in pruning. However, the experiments in this work seem challenging to support this argument, as all the gains from visual prompts appear to be very marginal, less than or equal to 1%. Such experimental results are hard to be convincing.*\n\n**Reply:** Thank you for your detailed comments. \n\n- **Gains of visual prompt are not ``Marginal\":** While the improvements observed with visual prompts in our ablation study (in Section 5.2 and Table 2) are between 0.6% to 1.3%, it is crucial to **contextualize these gains within the domain of network pruning**. In this field, the improvement of 1% is already very appealing, constrained by the limited parameter count. For example, even the current state-of-the-art such as DepGraph[1] and GrowRegularization[2] only outperform their baselines by less than 1%. The gains from visual prompts, although seemingly modest, are non-trivial when considering the challenges associated with further optimizing highly efficient models. Furthermore, these improvements are consistent across different sparsities, indicating a reliable enhancement rather than a statistical anomaly.\n\n\n    **[1]** Fang, Gongfan, et al. \"Depgraph: Towards any structural pruning.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n    **[2]** Wang, Huan, et al. \"Neural pruning via growing regularization.\" ICLR, (2021).\n\n- **Gains of our overall Framework (PASS) are not ``marginal\".** Our proposed PASS demonstrates significant performance enhancements across a range of architectures, datasets, and sparsity levels. Notably, with ResNet-18, our PASS consistently surpasses baseline models by **1% to 3%** in accuracy across six downstream tasks under 1000M FLOPs (from Figure 2 in the submission). Additionally, at a modest speed up, the subnetwork identified by our PASS outperforms fully-finetuned dense networks by **0.5% to 1%** in accuracy, as illustrated in **Table 1**. These results clearly indicate that the benefits of our method are not 'marginal'.\n\n\n    **Table 1: Comparison of Accuracy for PASS and dense neural network under different Datasets**\n\n    | Dataset      | ACC (PASS) | ACC (Dense) | FLOPs (PASS) | FLOPs (Dense) | \u0394 ACC |\n    |--------------|------------|-------------|--------------|---------------|-------|\n    | CIFAR-10     | 96.77      | 96.02       | 1644M        | 1822M         | 0.75  |\n    | CIFAR-100    | 83.45      | 82.40       | 1647M        | 1822M         | 1.05  |\n    | TinyImageNet | 73.64      | 73.10       | 1491M        | 1822M         | 0.54  |\n    | DTD          | 70.16      | 69.15       | 1677M        | 1822M         | 1.01  |\n    | Food101      | 82.13      | 81.07       | 1700M        | 1822M         | 1.06  |\n\n**Q3:** *Additionally, prompt learning relies on a strong foundation of pre-trained models. To demonstrate its effectiveness in network pruning, favorable experiments and analyses are essential.*\n\n**reply**: Thank you for your insightful feedback. We totally agree with the significance of a strong foundation of pre-trained models for prompt learning. Due to the time limitation, we could not extend to evaluating the effectiveness of VP on our proposed PASS using a very strong foundation of pre-trained vision models such as CLIP[1]. However, we provide a similar analysis to study the effect of model size on the pruning performance caused by visual prompts, by varying the pre-trained model size from ResNet-18 to ResNet-34, and to ResNet-50. The results are shown in **Table 2**.\n\n- The results show an increase in the efficacy of VP as we progress from ResNet-18 to more advanced models like ResNet-34 and ResNet-50. This observation suggests that the impact of VP on the PASS framework might be more pronounced when implemented on stronger foundational vision models.\n\n    **Table 2: Performance of subnetworks (Channel Sparsity: 30%) found by PASS w/ VP and w/o VP with varying different scales of the pre-trained models. The  task is CIFAR-100.**\n    |                | ResNet-18 | ResNet-34 | ResNet-50 |\n    |----------------|-----------|-----------|-----------|\n    | LSTM+Weights   | 81.13     | 82.68     | 82.84     |\n    | LSTM+Weights+VP(PASS) | 81.72 | 83.69 | 83.91 |\n    | \u0394              | 0.59      | 1.01      | 1.07      |\n\n[1] Radford, Alec, et al. \"Learning transferable visual models from natural language supervision.\" International conference on machine learning. PMLR, 2021."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6393/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700421581821,
                "cdate": 1700421581821,
                "tmdate": 1700421581821,
                "mdate": 1700421581821,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CjYgaEhvFW",
                "forum": "Uavy4DLrXR",
                "replyto": "UW71otmdxv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6393/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6393/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dU7f (3/4)"
                    },
                    "comment": {
                        "value": "**Q4:** *In cases where pruning a model without fine-tuning, the visual prompt is unnecessary, in such a scenario, it seems that the paper may not work.*\n\n**Reply:** Thank you for your comments. We would like to clarify that our proposed PASS still works in cases where pruning a model without fine-tuning. It is because our proposed PASS concurrently optimizes both the visual prompt and the hypernetwork for finding the channel topology.\n\n- To substantiate our claim, we conducted additional experiments comparing the performance of subnetworks identified by PASS with and without the visual prompt (VP). Our results in **Table 3** demonstrate that subnetworks pruned using PASS with the visual prompt outperform those pruned without it, underscoring the visual prompt's value in structural pruning.\n\n    **Table 3: Performance of subnetworks before fine-tuning. The experiments are based on CIFAR-100 and ResNet-18.**\n\n    |Channel Sparsity | 10%   | 30%   | 50%   | 70%   |\n    |---------------|-------|-------|-------|-------|\n    | LSTM+Weights  | 33.22 | 32.80 | 21.19 | 8.42  |\n    | LSTM+Weights+VP(PASS) | 35.64 | 33.79 | 22.38 | 9.15  |\n\n\n**Q5:** *The experiments conducted on small datasets, such as CIFAR and Tiny-ImageNet, with very low resolution and data scale are not entirely convincing. The reviewer suggests including experiments on at least ImageNet-1k or ImageNet. In the era of big data, ImageNet is considered a relatively small dataset.*\n\n**Reply:** Thank you for your comments. In response, we expanded our empirical investigation to include the ImageNet-1k dataset. Following [5], we adopt the Vision Transformer (ViT-B/16) and ResNeXt-50 architectures. We utilized the pre-trained models from PyTorch Vision ([1]). \n\n- The results are shown in **Table 4**. The results show that our method PASS achieves superior performance over baselines with significant speed-ups with only minimal impacts on accuracy, affirming the effectiveness of our proposed PASS on large datasets.\n\n    **Table 4: Pruning results  on ImageNet with Advanced models.**\n\n    | Arch.      | Method       | Base  | Pruned | \u0394 Acc. | FLOPs |\n    |------------|--------------|-------|--------|--------|-------|\n    | ResNeXt-50 | ResNeXt-50   | 77.62 | -      | -      | 4.27  |\n    |            | SSS [2]      | 77.57 | 74.98  | -2.59  | 2.43  |\n    |            | GFP [3]      | 77.97 | 77.53  | -0.44  | 2.11  |\n    |            | DepGraph [4] | 77.62 | 76.48  | -1.14  | 2.09  |\n    |            | Ours (PASS)  | 77.62 | 77.21  | -0.41  | 2.01  |\n    | ViT-B/16   | ViT-B/16     | 81.07 | -      | -      | 17.6  |\n    |            | DepGraph [4] | 81.07 | 79.17  | -1.90  | 10.4  |\n    |            | Ours (PASS)  | 81.07 | 79.77  | -1.30  | 10.7  |\n\n[1] [TorchVision](https://pytorch.org/vision/stable/index.html)\n\n[2] Huang, Zehao, and Naiyan Wang. \"Data-driven sparse structure selection for deep neural networks.\" Proceedings of the European conference on computer vision (ECCV). 2018.\n\n[3] Liu, Liyang, et al. \"Group fisher pruning for practical network compression.\" International Conference on Machine Learning. PMLR, 2021.\n\n[4] Fang, Gongfan, et al. \"Depgraph: Towards any structural pruning.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6393/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700421812651,
                "cdate": 1700421812651,
                "tmdate": 1700421812651,
                "mdate": 1700421812651,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nAa5ouPTta",
                "forum": "Uavy4DLrXR",
                "replyto": "UW71otmdxv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6393/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6393/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dU7f (4/4)"
                    },
                    "comment": {
                        "value": "**Q6:** *(1) Could the author explain why the visual prompts improve channel pruning? Since the visual prompts are static across a task or a dataset, why does the author state their pruning method as \u201cfrom a data-centric perspective\u201d while it is not even input dependent?*\n\n**Reply:** Thank you for your insightful questions. We answer your questions step by step.\n\n**Why visual prompts help channel pruning.** \n- Several studies demonstrated that model sparsification is influenced by the nature of tasks or datasets[1][2]. Visual prompts act as a form of meta-information, encapsulating crucial insights about the task or dataset. It provides the task-dependent information for the pruning process, leading to the identification of more relevant and efficient channel structures. The robust performance of our proposed PASS, demonstrated across six diverse datasets and multiple architectures, further substantiates this claim. Additionally, our ablation studies provide concrete evidence of the efficacy of visual prompts in enhancing channel pruning.\n\n- Furthermore, the increasing volume of research indicates that incorporating information from inputs is crucial for enhancing the performance of pruning algorithms [3][4][5][6]. For example, the recent study [6] highlights that leveraging prompts post-pruning can significantly improve the efficiency of sparse LLMs. Inspired by these insights, we explored how to integrate visual prompts into the pruning process of the vision models and proposed our PASS, showing its superiority over other baselines.\n\n[1] Liu, Shiwei, et al. \"Sparsity May Cry: Let Us Fail (Current) Sparse Neural Networks Together!.\" ICLR (2023).\n\n[2] Yin, Lu, et al. \"Junk DNA Hypothesis: A Task-Centric Angle of LLM Pre-trained Weights through Sparsity.\" arXiv preprint arXiv:2310.02277 (2023).\n\n[3] Sun, Mingjie, et al. \"A Simple and Effective Pruning Approach for Large Language Models.\" arXiv preprint arXiv:2306.11695 (2023).\n\n[4] Frantar, Elias, and Dan Alistarh. \"SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot.\" ICML (2023).\n\n[5] Yin, Lu, et al. \"Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity.\" arXiv preprint arXiv:2310.05175 (2023).\n\n[6] PROMPT, TRANSFERABLE. \"Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt.\" arXiv preprint arXiv:2305.11186 (2023).\n\n**Our proposed PASS is from a data-centric perspective.** \n\n- Firstly, let us elucidate the concept of a data-centric technique. A data-centric technique **does not necessarily hinge on being input sample-dependent**. Instead, it can also be task-specific or dataset-dependent[1]. This kind of technique primarily focuses on engineering and managing data to boost model performance. A notable manifestation of this in the data-centric paradigm is the utilization of prompts, including visual prompts, which are specific to a task or dataset[1]. In our proposed PASS, the reliance on visual prompts categorically aligns it with a data-centric methodology, as these prompts are dataset-specific. Therefore, our method's designation as data-centric is aptly justified.\n\n[1] Zha, Daochen, et al. \"Data-centric artificial intelligence: A survey.\" arXiv preprint arXiv:2303.10158 (2023)."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6393/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700421961845,
                "cdate": 1700421961845,
                "tmdate": 1700421961845,
                "mdate": 1700421961845,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "py5vuwgdbi",
                "forum": "Uavy4DLrXR",
                "replyto": "nAa5ouPTta",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6393/Reviewer_dU7f"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6393/Reviewer_dU7f"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Thank you for the author's response. The author has addressed most of my concerns.\n\nHowever, the reviewer still expresses concerns about novelty. Although this work focuses on a different task and has a different objective compared to dynamic networks, the network architecture appears to be similar.\n\nAdditionally, in Table 4: Pruning results on ImageNet with Advanced models, compared to previous works, the gain in this study appears to be marginal, with an increase of approximately 0.6%.\n\nFurthermore, the authors only provided experiments for ResNeXt-50 with 50% sparsity and experiments for ViT-B/16 with around 40% sparsity. Although the reviewer acknowledges the challenges of conducting additional experiments in a short time frame, suggesting that the authors provide results for more challenging and higher sparsity settings, such as 60% or 70%.\n\nIn summary, the reviewer gives a final score of 5: marginally below the acceptance threshold."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6393/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700550675179,
                "cdate": 1700550675179,
                "tmdate": 1700550675179,
                "mdate": 1700550675179,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7SJWMMpUVV",
            "forum": "Uavy4DLrXR",
            "replyto": "Uavy4DLrXR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6393/Reviewer_dN4F"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6393/Reviewer_dN4F"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors study how to use visual prompts for channel pruning. The authors argue that the layer-wise mask should consider the sequential dependency between adjacency layers, network weights and visual prompts. Motivated by this argument, the authors propose PASS to learn sparse mask using a recurrent LSTM network. The authors conduct experiments on six target datasets with four different backbones."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed method achieves better performance over the baselines on most of the proposed settings.\n2. The authors provide the code in the appendix."
                },
                "weaknesses": {
                    "value": "1. Model Complexity: While the channel pruning reduces size, the added LSTM network introduces new parameters. An analysis of its impact on model parameters considering the LSTM and training/testing time would be beneficial.\n2. Backbone Networks: this paper uses ResNet and VGG as the backbone networks. I recommend the authors also explore more contemporary and potentially powerful architectures, such as ResNeXT and ViT used in DepGraph.\n3. Benchmarks: The paper's benchmarks are limited in size. Testing on larger datasets like ImageNet, as used in GrowReg, DepGraph, and other baselines, is recommended."
                },
                "questions": {
                    "value": "1. Please correct the typos in the title, \"sparsity\" and \"recurrent\".\n2. It's preferable to place figures and tables at the top of a page.\n3. The authors may consider switching the sequence of Figure 4 and Figure 5 to align with their respective mentions in the text."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6393/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698630114636,
            "cdate": 1698630114636,
            "tmdate": 1699636708307,
            "mdate": 1699636708307,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vDBQoweR89",
                "forum": "Uavy4DLrXR",
                "replyto": "7SJWMMpUVV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6393/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6393/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dN4F (1/2)"
                    },
                    "comment": {
                        "value": "**We sincerely appreciate your constructive suggestions and detailed comments. We provide point-wise responses to your concerns below.**\n\n**Q1:** *Model Complexity: While the channel pruning reduces size, the added LSTM network introduces new parameters. An analysis of its impact on model parameters considering the LSTM and training/testing time would be beneficial?*\n\n**Reply:** Thank you for your insightful suggestion regarding the analysis of model complexity in relation to the LSTM network used in our approach. We appreciate the emphasis on understanding the overall impact on model parameters and time efficiency during training and testing. In response, we have conducted a comprehensive analysis, the details of which we will incorporate into our revised manuscript (in Appendix E).\n\n- **Regarding the impact on time complexity:** Our recurrent hyper-network is designed for efficiency. The channel masks are pre-calculated, eliminating the need for real-time generation during both the inference and subnetwork fine-tuning phases. Therefore, the recurrent hyper-network does not introduce any extra time complexity during the inference and the fine-tuning phase. The additional computing time is limited to the phase of channel mask identification.\n\n- **Moreover, the hyper-network itself is designed to be lightweight:** The number of parameters it contributes to the overall model is minimal, thus ensuring that any additional complexity during the mask-finding phase is negligible. This claim is substantiated by empirical observations: the hyper-network accounts for only about **0.2% to 6%** of the total model parameters across various architectures such as ResNet-18/34 and VGG-16, as illustrated in **Table 1**.\n\n    **Table 1: The number of parameters for our Hypernetworks.**\n\n    |                           | ResNet-18 (11M) | ResNet-34 (21M) | ResNet-50 (25M) | VGG-16 (138M) |\n    |---------------------------|-----------------|-----------------|-----------------|---------------|\n    | #Parameters-HyperNetwork  | 0.31M (2.8%)    | 0.56M (2.6%)    | 1.5M (6%)       | 0.34M (0.2%)  |\n\n- Additionally, we assessed the training time per epoch with and without the hyper-network during the channel mask identification phase. Our findings in **Table 2** indicate that the inclusion of the LSTM network has a marginal effect on these durations, further affirming the efficiency of our approach.\n\n    **Table 2: Training Time (s) per Epoch w/ and w/o Hypernetworks during Channel Mask Identification Phase with single A100 GPU.**\n\n    |                  | ResNet-18 (11M) | ResNet-34 (21M) | ResNet-50 (25M) |\n    |------------------|-----------------|-----------------|-----------------|\n    | w/o HyperNetwork | 70.05           | 73.95           | 95.65           |\n    | w/ HyperNetwork  | 72.2            | 76.95           | 111.6           |"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6393/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700419280300,
                "cdate": 1700419280300,
                "tmdate": 1700419280300,
                "mdate": 1700419280300,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dkSKwydBia",
                "forum": "Uavy4DLrXR",
                "replyto": "7SJWMMpUVV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6393/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6393/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dN4F (2/2)"
                    },
                    "comment": {
                        "value": "**Q2:** *Backbone Networks: this paper uses ResNet and VGG as the backbone networks. I recommend the authors also explore more contemporary and potentially powerful architectures, such as ResNeXT and ViT used in DepGraph.*  **Q3:** *Benchmarks: The paper's benchmarks are limited in size. Testing on larger datasets like ImageNet, as used in GrowReg, DepGraph, and other baselines, is recommended.*\n\n**Reply:** We appreciate your suggestion to explore more contemporary architectures and larger datasets. \n\n- In response, we have extended our experiments to include the Vision Transformer (ViT-B/16) and ResNeXt-50 on ImageNet. We utilized pre-trained models from PyTorch Vision ([1]), ensuring that our experiments are replicable and grounded in standard benchmarks. The results are reported in **Table 3**.\n\n- Notably, our method PASS demonstrates a significant speed-up with minimal accuracy loss, as indicated by the $\\Delta$Acc., which are superior to existing methods like SSS, GFP, and DepGraph. the resulting empirical evidence robustly affirms the effectiveness of PASS across both advanced neural network architectures and large-scale datasets.\n\n    **Table 3: Pruning results based on ImageNet and Advanced models.**\n\n    | Arch.      | Method      | Base  | Pruned | \u0394 Acc. | FLOPs |\n    |------------|-------------|-------|--------|--------|-------|\n    | ResNeXt-50 | ResNeXt-50  | 77.62 | -      | -      | 4.27  |\n    |            | SSS [2]     | 77.57 | 74.98  | -2.59  | 2.43  |\n    |            | GFP [3]     | 77.97 | 77.53  | -0.44  | 2.11  |\n    |            | DepGraph [4]| 77.62 | 76.48  | -1.14  | 2.09  |\n    |            | Ours (PASS) | 77.62 | 77.21  | -0.41  | 2.01  |\n    | ViT-B/16   | ViT-B/16    | 81.07 | -      | -      | 17.6  |\n    |            | DepGraph [4]| 81.07 | 79.17  | -1.90  | 10.4  |\n    |            | Ours(PASS)  | 81.07 | 79.77  | -1.30  | 10.7  |\n\n[1] [TorchVision - Stable](https://pytorch.org/vision/stable/index.html)\n\n[2] Huang, Zehao, and Naiyan Wang. \"Data-driven sparse structure selection for deep neural networks.\" Proceedings of the European conference on computer vision (ECCV). 2018.\n\n[3] Liu, Liyang, et al. \"Group fisher pruning for practical network compression.\" International Conference on Machine Learning. PMLR, 2021.\n\n[4] Fang, Gongfan, et al. \"Depgraph: Towards any structural pruning.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n\n**Q4:** *Please correct the typos in the title, \"sparsity\" and \"recurrent\". It's preferable to place figures and tables at the top of a page.*\n\n**Reply:** Thank you very much for highlighting these important details. \n\n- We have carefully reviewed the manuscript and corrected the typos in the title.\n\n- Additionally, in line with your suggestion for improved readability and standard formatting practices, we have adjusted the layout to position figures and tables at the top of the pages in our revision. \n\n**Q5:** *The authors may consider switching the sequence of Figure 4 and Figure 5 to align with their respective mentions in the text.*\n\n**Reply:** Thank you so much for pointing it out. Following your helpful suggestion, we have rearranged these figures in the revised version.\n\n**If you have any other suggestiongs, please let us know. We are more than happy to address them.**"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6393/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700419666184,
                "cdate": 1700419666184,
                "tmdate": 1700419666184,
                "mdate": 1700419666184,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QFXjKDWxHB",
                "forum": "Uavy4DLrXR",
                "replyto": "7SJWMMpUVV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6393/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6393/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Last 40 hours reminder"
                    },
                    "comment": {
                        "value": "Dear Reviewer **dN4F**,\n\nThanks a lot for your constructive reviews! We really hope to have a further discussion with you to see if our response solves your concerns.\n\nIn our response, we have **(1)** provided a comprehensive analysis of the extra model complexity introduced by the recurrent hypernetwork. **(2)** presented experimental results with **ImageNet** and advanced architectures such as **ViT-B/16 and ResNext-50**. **(3)** corrected the typos and rearranged the tables and figures as you requested.\n\nWith only 40 hours remaining until the ICLR rebuttal deadline, we sincerely hope you can review our response and look forward to your further feedback! Your feedback and support is very important to us! We greatly appreciate that!\n\nBest wishes,\n\nThe authors"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6393/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700606402985,
                "cdate": 1700606402985,
                "tmdate": 1700606540563,
                "mdate": 1700606540563,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AJyOq4Qoh1",
                "forum": "Uavy4DLrXR",
                "replyto": "7SJWMMpUVV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6393/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6393/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A friendly reminder"
                    },
                    "comment": {
                        "value": "Dear Reviewer **dN4F**,\n\nThis is a friendly reminder that there are only 12 hours left before the Author-Reviewer Discussion Period closes. We are more than happy to provide more information if you still have any concerns. We are looking forward to your further feedback!\n\nThank you again for your time spent on our rebuttal.\n\nBest regards,\n\nThe authors"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6393/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700691993789,
                "cdate": 1700691993789,
                "tmdate": 1700691993789,
                "mdate": 1700691993789,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mh1J4mfVQq",
            "forum": "Uavy4DLrXR",
            "replyto": "Uavy4DLrXR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6393/Reviewer_NTwF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6393/Reviewer_NTwF"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to solve the problem of estimating the channel significance in structural pruning task. It leverages the visual prompts in in-context learning to capture the channel significance and derive high-quality structural sparsity. A novel network which takes visual prompts and weight statistics as input will output layer-wise channel sparsity recurrently. Experiments have demonstrated effectiveness of proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. It is novel to take the visual prompts into the channel pruning problem.\n2. The theoretical analysis is solid and convincing for me.\n3. Experimental results are sufficient to demonstrate the effectiveness of proposed method."
                },
                "weaknesses": {
                    "value": "No obvious weakness for me."
                },
                "questions": {
                    "value": "Is the proposed method effective to vision transformer based models?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6393/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6393/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6393/Reviewer_NTwF"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6393/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698772376854,
            "cdate": 1698772376854,
            "tmdate": 1699636708141,
            "mdate": 1699636708141,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AtynoJ7cMf",
                "forum": "Uavy4DLrXR",
                "replyto": "mh1J4mfVQq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6393/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6393/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer NTwF"
                    },
                    "comment": {
                        "value": "**We sincerely appreciate your detailed comments and positive ranking. We provide point-wise responses to your concerns below.**\n\n**Q1:** *Is the proposed method effective for vision transformer-based models?*\n\n**Reply:** Thanks for your concern in transformer-based architecture. We report the performance of PASS on the ViT-B-16 and Swin-T models on ImageNet where the pre-trained ViT-B-16 and Swin-T are from torchvision [1]. The results in **Table 1** show that our method PASS achieves higher accuracy than baselines with similar FLOPs, validating PASS is a general method that can be applied to various architectures.\n\n**Table 1: Pruning results Based on vision transformers on ImageNet.**\n\n| Arch.    | Method      | Base  | Pruned | \u0394 Acc. | FLOPs |\n|----------|-------------|-------|--------|--------|-------|\n| ViT-B/16 | ViT-B/16    | 81.07 | -      | -      | 17.6  |\n|          | DepGraph [2]| 81.07 | 79.17  | -1.90  | 10.4  |\n|          | Ours (PASS) | 81.07 | 79.77  | -1.30  | 10.7  |\n| Swin-T   | Swin-T      | 81.4  | -      | -      | 4.49  |\n|          | X-Pruner [3]| 81.4  | 80.7   | -0.7   | 3.2   |\n|          | STEP [4]    | 81.4  | 77.2   | -4.2   | 3.5   |\n|          | Ours (PASS) | 81.4  | 80.9   | -0.5   | 3.4   |\n\n[1] [TorchVision - Stable](https://pytorch.org/vision/stable/index.html)\n\n[2] Fang, Gongfan, et al. \"Depgraph: Towards any structural pruning.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n[3] Yu, Lu, and Wei Xiang. \"X-Pruner: eXplainable Pruning for Vision Transformers.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n[4] Li, Jiaoda, Ryan Cotterell, and Mrinmaya Sachan. \"Differentiable subset pruning of transformer heads.\" Transactions of the Association for Computational Linguistics 9 (2021): 1442-1459."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6393/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700418789364,
                "cdate": 1700418789364,
                "tmdate": 1700418789364,
                "mdate": 1700418789364,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jnmYSqNRbe",
                "forum": "Uavy4DLrXR",
                "replyto": "AtynoJ7cMf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6393/Reviewer_NTwF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6393/Reviewer_NTwF"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Thank you for your response. Your response have addressed my concerns and I will keep my score."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6393/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700622062619,
                "cdate": 1700622062619,
                "tmdate": 1700622062619,
                "mdate": 1700622062619,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AA2bBQT5cL",
            "forum": "Uavy4DLrXR",
            "replyto": "Uavy4DLrXR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6393/Reviewer_F4Cf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6393/Reviewer_F4Cf"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses the inefficiency of large-scale neural networks by proposing a pruning method named PASS, which stands for Prune to Achieve Sparse Structures.PASS utilizes visual prompts as an innovative means to identify crucial channels for pruning, seeking to enhance model efficiency without sacrificing performance. This framework adopts a recurrent hypernetwork to generate sparse channel masks in an auto-regressive manner, leveraging both visual prompts and weight statistics of the network. The authors provide extensive experimental evidence showing that PASS achieves better accuracy with fewer computational resources across multiple datasets and network architectures. They also highlight that the hypernetwork and sparse channel masks generated by PASS have superior transferability for subsequent tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. PASS introduces a novel use of visual prompts to determine channel importance.\n2. Using recurrent hyper networks allows efficient learning of sparse masks, considering the inter-layer dependencies.\n3. Experiment results show the advantage of the proposed method over baselines on convolution baseline over small benchmarks."
                },
                "weaknesses": {
                    "value": "1. The recurrent hyper network approach might introduce complexity, especially in the LSTM network. Does the FLOPs computation involve the hyper-network? This requires more clear explanation in the paper. \n2. This paper only experiments with the convolution-based method. While the transformer-based approach, such as vision transformers and swin-transformers, has no investigations. To validate the generalization of the proposed approach, the authors need to provide more experiments on transformer-based networks. \n3. The experiments performed in small-scale datasets, such as cifar10, cifar100. It is worth reporting results on large datasets such as imagenet."
                },
                "questions": {
                    "value": "Please refer to the questions in the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No concern on Ethics."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6393/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698821404690,
            "cdate": 1698821404690,
            "tmdate": 1699636707960,
            "mdate": 1699636707960,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WccPTPhe0s",
                "forum": "Uavy4DLrXR",
                "replyto": "AA2bBQT5cL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6393/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6393/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer F4Cf (1/2)"
                    },
                    "comment": {
                        "value": "**We sincerely appreciate your detailed comments. We provide point-wise responses to your concerns below.**\n\n**Q1:** *The recurrent hyper-network approach might introduce complexity, especially in the LSTM network. Does the FLOPs computation involve the hyper-network? This requires more clear explanation in the paper.*\n\n**Reply:** Thank you for your helpful feedback. We give detailed explanations here and will incorporate this detailed explanation into the revised version of our paper (in Appendix E).\n\n- We acknowledge the concern about potential complexities arising from the incorporation of an LSTM network. It's crucial to emphasize that our paper primarily **focuses on achieving inference speedup**. The role of the hyper-network is solely dedicated to generating sparse masks. After that, **it will not be used during both the inference and fine-tuning phases**. Therefore, the FLOPs computation does not involve the hyper-network.\n\n- Moreover, the hyper-network itself is designed to be lightweight. The number of parameters it contributes to the overall model is minimal, thus ensuring that any additional complexity during the mask finding phase is negligible. This claim is substantiated by empirical observations: the hyper-network accounts for only about **0.2% to 6%** of the total model parameters across various architectures such as ResNet-18/34/50 and VGG-16, as illustrated in **Table 1**.\n\n    **Table 1:** The number of parameters for our Hypernetworks.\n\n    |                           | ResNet-18 (11M) | ResNet-34 (21M) | ResNet-50 (25M) | VGG-16 (138M)  |\n    |---------------------------|-----------------|-----------------|-----------------|----------------|\n    | #Parameters-HyperNetwork  | 0.31M (2.8%)    | 0.56M (2.6%)    | 1.5M (6%)       | 0.34M (0.2%)   |\n\n- Additionally, we assessed the training time per epoch with and without the hyper-network during the channel mask identification phase. Our findings in Table 2 indicate that the inclusion of the LSTM network has a marginal effect on these durations, further affirming the efficiency of our approach.\n\n    **Table 2:** Training Time (s) per Epoch w/ and w/o Hypernetworks during Channel Mask Identification Phase with single A100 GPU.\n\n    |                  | ResNet-18 (11M) | ResNet-34 (21M) | ResNet-50 (25M) |\n    |------------------|-----------------|-----------------|-----------------|\n    | w/o HyperNetwork | 70.05           | 73.95           | 95.65           |\n    | w/ HyperNetwork  | 72.2            | 76.95           | 111.6           |"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6393/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700418315237,
                "cdate": 1700418315237,
                "tmdate": 1700418315237,
                "mdate": 1700418315237,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RnOwv8aNAu",
                "forum": "Uavy4DLrXR",
                "replyto": "AA2bBQT5cL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6393/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6393/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer F4Cf (2/2)"
                    },
                    "comment": {
                        "value": "**Q2:** *This paper only experiments with the convolution-based method. While the transformer-based approach, such as vision transformers and swin-transformers, has no investigations. To validate the generalization of the proposed approach, the authors need to provide more experiments on transformer-based networks.* **Q3:** *The experiments performed in small-scale datasets, such as CIFAR-10, CIFAR-100. It is worth reporting results on large datasets such as ImageNet.*\n\n**Reply:** Thank you for your valuable suggestions emphasizing the need for broader experimentation to validate the generalizability of our proposed PASS methodology.\n\n- In line with your recommendations, we have explored our approach to transformer-based networks and larger dataset, i.e., Vision Transformers (ViT-Base-16) and Swin Transformers (Swin-Tiny) on ImageNet-1K. The pre-trained weights are obtained from torchvision [1].\n\n- The results from these extended experiments, as shown in **Table 3**, illustrate that PASS achieves higher accuracies than baselines with similar FLOPs on ImageNet with various advanced architectures, affirming that our proposed PASS can be generalized to advanced architectures and large datasets.\n\n**Table 3: Pruning results based on ImageNet and Advanced models.**\n\n| Arch.      | Method       | Base  | Pruned | \u0394 Acc. | FLOPs |\n|------------|--------------|-------|--------|--------|-------|\n| ResNeXt-50 | ResNeXt-50   | 77.62 | -      | -      | 4.27  |\n|            | SSS [2]      | 77.57 | 74.98  | -2.59  | 2.43  |\n|            | GFP [3]      | 77.97 | 77.53  | -0.44  | 2.11  |\n|            | DepGraph [4] | 77.62 | 76.48  | -1.14  | 2.09  |\n|            | Ours (PASS)  | 77.62 | 77.21  | -0.41  | 2.01  |\n| ViT-B/16   | ViT-B/16     | 81.07 | -      | -      | 17.6  |\n|            | DepGraph [4] | 81.07 | 79.17  | -1.90  | 10.4  |\n|            | Ours (PASS)  | 81.07 | 79.77  | -1.30  | 10.7  |\n| Swin-T     | Swin-T       | 81.4  | -      | -      | 4.49  |\n|            | X-Pruner [5] | 81.4  | 80.7   | -0.7   | 3.2   |\n|            | STEP [6]     | 81.4  | 77.2   | -4.2   | 3.5   |\n|            | Ours (PASS)  | 81.4  | 80.9   | -0.5   | 3.4   |\n\n\n\n[1] [TorchVision - Stable](https://pytorch.org/vision/stable/index.html)\n\n[2] Huang, Zehao, and Naiyan Wang. \"Data-driven sparse structure selection for deep neural networks.\" Proceedings of the European conference on computer vision (ECCV). 2018.\n\n[3] Liu, Liyang, et al. \"Group fisher pruning for practical network compression.\" International Conference on Machine Learning. PMLR, 2021.\n\n[4] Fang, Gongfan, et al. \"Depgraph: Towards any structural pruning.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n[5] Yu, Lu, and Wei Xiang. \"X-Pruner: eXplainable Pruning for Vision Transformers.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n[6] Li, Jiaoda, Ryan Cotterell, and Mrinmaya Sachan. \"Differentiable subset pruning of transformer heads.\" Transactions of the Association for Computational Linguistics 9 (2021): 1442-1459."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6393/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700418466868,
                "cdate": 1700418466868,
                "tmdate": 1700418466868,
                "mdate": 1700418466868,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RTbOP5HlvO",
                "forum": "Uavy4DLrXR",
                "replyto": "AA2bBQT5cL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6393/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6393/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Last 40 hours reminder"
                    },
                    "comment": {
                        "value": "Dear Reviewer **F4Cf**,\n\nThanks a lot for your constructive reviews! We really hope to have a further discussion with you to see if our response solves your concerns.\n\nIn our response, we have provided **(1)** a  comprehensive analysis of the model complexity introduced by the recurrent hypernetwork.  **(2)**  experimental results with ImageNet and transformer-based models.\n\nWith only 40 hours remaining until the ICLR rebuttal deadline, we sincerely hope you can review our response and look forward to your further feedback! Your feedback and support is very important to us! We greatly appreciate that!\n\nBest wishes,\n\nThe authors"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6393/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700605743888,
                "cdate": 1700605743888,
                "tmdate": 1700606500082,
                "mdate": 1700606500082,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xlGYH0Kuyp",
                "forum": "Uavy4DLrXR",
                "replyto": "AA2bBQT5cL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6393/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6393/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A friendly reminder"
                    },
                    "comment": {
                        "value": "Dear Reviewer **F4Cf**,\n\nThis is a friendly reminder that there are only 12 hours left before the Author-Reviewer Discussion Period closes.  We are more than happy to provide more information if you still have any concerns. We are looking forward to your further feedback!\n\nThank you again for your time spent on our rebuttal.\n\nBest regards,\n\nThe authors"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6393/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700691887394,
                "cdate": 1700691887394,
                "tmdate": 1700691887394,
                "mdate": 1700691887394,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]