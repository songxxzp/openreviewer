[
    {
        "title": "Understanding In-context Learning with a Pelican Soup Hypothesis"
    },
    {
        "review": {
            "id": "EIVr3uVfUn",
            "forum": "aaYBsuGRne",
            "replyto": "aaYBsuGRne",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4423/Reviewer_rNJR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4423/Reviewer_rNJR"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on understanding the in-context learning ability of large language models. The authors propose the Pelican soup hypothesis. It explains the in-context learning ability originating from learning the knowledge via the next token prediction. To support this hypothesis, the authors build a dataset and demonstrate the linkage between linguistic phenomena and in-context learning."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper provides substantial numerical results to support the proposed hypothesis. The linguistic phenomena analysis is also interesting to the community. In addition, the built dataset may be of independent interest."
                },
                "weaknesses": {
                    "value": "1. The claim related to the knowledge base needs more clarification. The experiments in [1] demonstrate that input-output mapping is not very important to the ICL. If the label space is correct, LLMs can even implement efficient ICL given wrong mapping. However, this wrong mapping conflicts with the knowledge base. More discussions are needed here.\n\n2. In Section 5.1, some assumptions are presented, but there is a notable absence of justification for these assumptions within the paper. This absence makes it challenging to ascertain the realism of these assumptions.\n3. I would greatly appreciate further elucidation on the distinction between the hypothesis presented in this paper and that discussed in [2]. Specifically, the variance between the \"atomic elements of NLP tasks\" and \"a set of atom concepts\" requires additional clarification.\n\n4. It is advantageous to include more highly relevant works in the related works. For example, besides HMM, implicit Bayesian inference is modeled for ICL in many different data assumptions [3,4,5]. [6] also studies the optimization side of ICL.\n\n[1] Min S, Lyu X, Holtzman A, et al. Rethinking the role of demonstrations: What makes in-context learning work?[J]. arXiv preprint arXiv:2202.12837, 2022.\n\n[2] Sanjeev A. and Anirudh G. A theory for emergence of complex skills in language models. arXiv preprint arXiv:2307.15936, 2023.\n\n[3] Jiang H. A latent space theory for emergent abilities in large language models[J]. arXiv preprint arXiv:2304.09960, 2023.\n\n[4] Zhang Y, Zhang F, Yang Z, et al. What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization[J]. arXiv preprint arXiv:2305.19420, 2023.\n\n[5] Wang X, Zhu W, Wang W Y. Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning[J]. arXiv preprint arXiv:2301.11916, 2023.\n\n[6] Dai D, Sun Y, Dong L, et al. Why can gpt learn in-context? language models secretly perform gradient descent as meta optimizers[J]. arXiv preprint arXiv:2212.10559, 2022."
                },
                "questions": {
                    "value": "Questions are specified in Weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4423/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698428079499,
            "cdate": 1698428079499,
            "tmdate": 1699636416867,
            "mdate": 1699636416867,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "fXSoklalgu",
            "forum": "aaYBsuGRne",
            "replyto": "aaYBsuGRne",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4423/Reviewer_hbQa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4423/Reviewer_hbQa"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes the Pelican Soup Hypothesis to explain in-context learning. It says that the in-context learning in language models can be explained as generalisation under several types of distribution shifts. It provides a formalism of NLP classification tasks in the context of in-context learning and constructs a dataset in formal language demonstrating the hypothesis."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- It proposes a general formalism for NLP classification tasks in the context of in-context learning. As the paper says, it may facilitate future NLP theory research.\n- The Pelican Soup hypothesis provides a potential explanation of in-context learning in language models.\n- The Calcutec dataset may also facilitate future research on explaining in-context learning."
                },
                "weaknesses": {
                    "value": "In general, I am not an expert in this line of work, but I have a strong feeling that the hypothesis and the experiment are more about mimicking, or more precisely, producing an environment, with which in-context learning still works, rather than explaining how/why in-context learning works in language models. Intuitively, for me, they are different things or at least an insufficient explanation.\n\nSome of the reasonings are hard for me to follow, for example, \n- why the yes/no questions are similar to the demonstrations for in-context learning? Or, such similarities had already been considered distribution shifts?\n- (In page 3) How do you know the process of figuring out that \"she\" may be a person to whom something unexpected happened is similar to recovering z for class y? I understand the outcome would be similar, but why also the process? \n- And if the above one is the actual process, I somehow feel that this suggests that LM should be good at handling anaphora but not catephora, which intuitively, is different from in-context learning."
                },
                "questions": {
                    "value": "See my questions above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4423/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698573684655,
            "cdate": 1698573684655,
            "tmdate": 1699636416792,
            "mdate": 1699636416792,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Wd2Vq2g7KN",
                "forum": "aaYBsuGRne",
                "replyto": "fXSoklalgu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4423/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4423/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your positive review. \n\nWe would like to point out that producing an environment, with which in-context learning still works, is a practice used in most (if not all) theoretical analyses such as [1] and [2] and even some empirical analyses such as [3]. We consider one of our main contributions to be proposing an environment that is closer to the real-world scenario than previous works.\n\nWith regard to the questions you stated:\n\n- The yes/no questions in a Pelican Soup riddle have a similar function as the demonstrations for in-context learning, because based on the answer of those yes/no questions, the participants are able to figure out the latent story and thus are able to answer other following questions. Note that we use Pelican Soup riddles just to motivate the importance of the role of commonsense.\n- Our argument is that solving these two problems (ICL and modeling coreference) requires similar capabilities. This argument is to draw the intuition for the framework/assumptions in Section 5.1. Surely the two problems are not identical, thus we characterize the difference between these two problems with the distribution shifts discussed in Section 4. To show that LMs may be able to generalize under these distribution shifts, we replicate these distribution shifts in our experiments, as discussed in Section 5.3 and results in Section 5.5 are aligned with our hypothesis.\n- We understand ICL as generating verbalizers based on the meaning of the verbalizers, which is recovered according to the preceding context, so we think it is similar to modeling anaphora. Could you please elaborate more on why you think ICL is more like handling catephora?\n\nPlease let us know if you have any other comments. We are happy to discuss more!\n\n[1] Xie, Sang Michael, et al. \"An Explanation of In-context Learning as Implicit Bayesian Inference.\" International Conference on Learning Representations. 2021.\n\n[2] Hahn, Michael, and Navin Goyal. \"A theory of emergent in-context learning as implicit structure induction.\" arXiv preprint arXiv:2303.07971 (2023).\n\n[3] Chan, Stephanie, et al. \"Data distributional properties drive emergent in-context learning in transformers.\" Advances in Neural Information Processing Systems 35 (2022): 18878-18891."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4423/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699841706339,
                "cdate": 1699841706339,
                "tmdate": 1699841706339,
                "mdate": 1699841706339,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8obOAMZVtv",
                "forum": "aaYBsuGRne",
                "replyto": "Wd2Vq2g7KN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4423/Reviewer_hbQa"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4423/Reviewer_hbQa"
                ],
                "content": {
                    "title": {
                        "value": "Response to Rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for your response.\n\n> We would like to point out that producing an environment, with which in-context learning still works, is a practice used in most (if not all) theoretical analyses such as [1] and [2] and even some empirical analyses such as [3]. We consider one of our main contributions to be proposing an environment that is closer to the real-world scenario than previous works.\n\nThanks! This gives me a clearer picture of how this line of studies works.\n\n> Surely the two problems are not identical, thus we characterize the difference between these two problems with the distribution shifts discussed in Section 4. To show that LMs may be able to generalize under these distribution shifts, we replicate these distribution shifts in our experiments, as discussed in Section 5.3 and results in Section 5.5 are aligned with our hypothesis.\n\nI still feel that an explanation about this approximation or assumption is missing here, which should be seen as a limitation later on. \n\n> ICL is more like handling cataphora?\n\nbtw. I didn't say ICL is more like cataphora. Maybe you misread my review. (Maybe because I misspelled cataphora as catephora. My apologise!)"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4423/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700133360080,
                "cdate": 1700133360080,
                "tmdate": 1700133360080,
                "mdate": 1700133360080,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "jS1e9y8XnV",
            "forum": "aaYBsuGRne",
            "replyto": "aaYBsuGRne",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4423/Reviewer_5jds"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4423/Reviewer_5jds"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces the Pelican Soup Hypothesis to formalize and explain large language models' ability for in-context learning. The paper claims that in-context learning can be seen as a model's ability to generalize linguistic phenomena under distribution shifts. The authors identify several contributions in the paper, including:\n1. A new formalism for approaching natural language classification problems, specifically aimed at understanding in-context learning.\n2. A new dataset, \"Calcutec,\" replicates specific linguistic phenomena. The authors report that training on this dataset allows models to develop in-context learning abilities and improve their performance on chain-of-thought reasoning.\n3. The paper reports experiments with the GPT-2 model on various NLP tasks. These experiments connected certain linguistic phenomena and the model's in-context learning capabilities.\n4. The authors use a digit addition task to study a specific type of distribution shift. This experiment revealed that larger models are more capable of generalizing and adapting to such shifts."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Linking in-context learning to the model's coreference learning ability is an interesting and novel idea. It serves the vast interest of the community in understanding the underlying mechanism of LLMs' in-context learning and chain-of-thought ability.\n2. Overall, the Pelican Soup Hypothesis and the accompanying experiments provide insights into why and how in-context learning works in large language models. The introduction of the Calcutec dataset and the digit addition task as experimental tools paves the way for further research in this area."
                },
                "weaknesses": {
                    "value": "1. Many claims are made without citing prior sources or supporting evidence. For example, in 3.1, the author claims that \u201clanguage models may be able to acquire the KB by modeling general text.\u201d However, no clear evidence is provided via citations or experiments, and frankly, this is still an ongoing question the community aims to answer; it would be an important work itself to show these claims.\n\n2. The pretraining and in-context learning setting in the proposed dataset is different from common LLM settings, in which the synthetic setting here loses some of the information that LLM encodes, such as contextual information and domain information. This mismatched setting seems not ideal and limits the generalizability of this study. In particular, in-context learning has been found to be highly sensitive to context-label and domain-label biases, which is not clear in a context-free & domain-free setting.\n\n3. The main assumption of this paper seems to be that the text in pretraining corpora for LLMs consists of clear reasoning steps (potentially with some intermediate steps dropped). However, this assumption normally requires structured and domain-specific training data such as math text or academic papers. On the other hand, data like dialogues or other internet content may contain completely implicit reasoning steps that are hidden in the text space. So, I don't think the proposed pretraining data here, which includes some reasoning steps explicitly in the sequence, is very representative of the overall LLM pretraining setting.\n\n4.  The experiments are poorly designed, and the implementation details are generally missing, but the main experiment on Calcutec: dataset design is too complicated, but the experimental design and analysis are too simple, although the fact that it can do in-context learning is interesting. In addition, what is discussed in section 6 as real-world evidence does not directly support their main hypothesis.\n\n5. The logic of the paper is weak, and the paper is poorly organized. The arguments are not supported by rigorous experimental evidence. Almost all arguments around using the word \"**Therefore**\" are not rigorous (either the conclusion is not supported by the evidence or the things after, therefore, are logically irrelevant to things before). A large number of arguments are based on the author's thinking that A is **similar** to B, where first the similarity is poorly defined, and how from such similarity can we conclude their conclusion is usually unclear. For, in section 3.2, the author claims that predicting the correct pronoun in the next token completion using the information in the context is \"**similar**\" to inferring the class description z_y for y in text classification. \"**Therefore**\" modeling general text is similar to performing in-context learning. This may \"explain\" the linkage between in-context learning and emergent abilities of LLMs.\n\n6. The title of the work or the main motivation: human solving Pelican Soup riddles is similar to LLM doing in-context learning is based on some poorly defined subjective similarity.\n\n7. Could design more controlled experiments to study the importance of each individual aspect of the dataset (the current construction of the dataset is too complicated) and also to rule out other possibilities. For instance, the binary classification problem seems a bit too easy. Can the model learn shortcuts instead of using their \"world\" knowledge to solve the problem?"
                },
                "questions": {
                    "value": "1. What's your view on the mesa-optimization view of in-context learning based on your Pelican Soup Hypothesis? Do they complement each other, and can one explain the other one?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4423/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4423/Reviewer_5jds",
                        "ICLR.cc/2024/Conference/Submission4423/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4423/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698809758689,
            "cdate": 1698809758689,
            "tmdate": 1700740401626,
            "mdate": 1700740401626,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zZgk8ET7C6",
                "forum": "aaYBsuGRne",
                "replyto": "jS1e9y8XnV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4423/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4423/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank you for your review. We would like to address the weakness below:\n\n1. Please let us know what are the arguments that you think are not substantiated. For the example you provided, we apologize for not including proper citations. We think that LMs are able to acquire some knowledge about commonsense (though imperfectly) has been widely accepted. Many studies can be found in previous works, e.g. [1][2].\n2. Validating theories with a synthetic dataset is a common practice. We admit that our setting cannot explain domain bias. It may require more assumptions to theoretically explain. We would like to mention that, we think it will make the construction of the toy data even more complicated, which based on your 4th point, is not acceptable. \n3. We justify our assumptions made in Section 5.1 in Section 3. As argued in Section 3.1, we posit that training data usually contain text that has some logical structure and/or follows chronicle order, which can contribute to LMs\u2019 reasoning capability. The dataset Calcutec is a formal abstraction of such structure.\n4. Could you be more specific on what details are missing and how we should improve? We will appreciate your suggestion. The construction of Calcutec is more complicated than the toy data used in previous theoretical works such as [3] because Calcutec relies on milder assumptions and is more realistic. \n5. We apologize that we didn\u2019t make the purpose of Section 3 clearer. We treat Section 3 as the justification/intuition for the abstraction/assumptions/framework we made in Section 5.1. We then instantiate the abstraction/framework with Calcutec and show that LMs trained with it can do in-context learning.\n6. We use Pelican Soup riddles only to motivate the intuition that commonsense plays an important role for ICL. We would not say our hypothesis is *based on* this similarity.\n7. Please let us know if you think any experiment is important but missing. Also, could you elaborate more on the possibility that the models solve the task by learning shortcuts (and maybe also your definition of shortcuts)?\n\nOur answer to the question\n\n1. We think mesa-optimization mainly explains why models of Transformer architecture trained with autoregressive loss have the capacity to model data that involves ICL-like phenomena. However, in works such as [4], they only study on some toy data. It is unclear how their toy data is relevant to natural language data. Our work starts from another direction, aiming to understand what characteristics of training data lead to the ICL ability.\n\n\n[1] West, Peter, et al. \"Symbolic knowledge distillation: from general language models to commonsense models.\" arXiv preprint arXiv:2110.07178 (2021).\n\n[2] Li, Xiang Lorraine, et al. \"A systematic investigation of commonsense knowledge in large language models.\" Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022.\n\n[3] Xie, Sang Michael, et al. \"An explanation of in-context learning as implicit bayesian inference.\" arXiv preprint arXiv:2111.02080 (2021).\n\n[4] von Oswald, Johannes, et al. \"Uncovering mesa-optimization algorithms in transformers.\" arXiv preprint arXiv:2309.05858 (2023)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4423/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699835312830,
                "cdate": 1699835312830,
                "tmdate": 1699835312830,
                "mdate": 1699835312830,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ceyF8rKOzv",
                "forum": "aaYBsuGRne",
                "replyto": "zZgk8ET7C6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4423/Reviewer_5jds"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4423/Reviewer_5jds"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed response! \n> More statements that need evidence and citation\n1. 'language models may learn to do reasoning with the rules in the KB.\" Yes, some models do mimic this kind of rule-based reasoning from purely pretraining, but a lot of models fail to show a robust ability for rule-based reasoning. This statement doesn't seem to generalize well enough to support the border scope of the paper's main motivation, i.e., explain in-context learning.\n2. \"Therefore, by modeling these articles, a language model can not only learn the commonsense rules in KB but also\nlearn to utilize these rules for induction.\" How can you be sure that the model learns the commonsense rules and the ability to reason based on these rules purely from the articles? There are other possibilities as well, such as code data and textbooks, etc. How is the model's ability to \"utilize these rules for induction\" being evaluated?\n3. \"Such kind of articles may be pervasive in the training data. Essays arguing some claims are one\nexample.\" How can you be sure that articles in the pretraining data are pervasive? Most of the models with strong in-context learning ability do not open source their pretraining data.\n\n> The construction, assumptions, and motivations of Calcutec\nI agree that the construction of this dataset is complex, but I fail to see that this dataset is more realistic and the relevance between building a formal abstraction of the logical structure and the pretraining data that are used in current work for Large Language Model pretraining.  I think you would need to show that (1) such logical structure largely exists in the pretraining data of current LLMs and (2) the data of such logical structure show certain levels of impact on the model's behavior post-pretraining.\n\n> Models solve the task by learning shortcuts\nMainly domain-label bias, context-label bias, and vanilla-label bias. See https://arxiv.org/abs/2305.19148 and https://arxiv.org/abs/2102.09690.\n\nOverall, I will raise my soundness to 2 while keeping my original rating."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4423/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700742394471,
                "cdate": 1700742394471,
                "tmdate": 1700742394471,
                "mdate": 1700742394471,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GGaLNzAXOW",
            "forum": "aaYBsuGRne",
            "replyto": "aaYBsuGRne",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4423/Reviewer_MUGa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4423/Reviewer_MUGa"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a new theoretical account of the in-context learning (ICL) abilities of large language models.\nSection 2 describes a formal framework for NLP classification tasks, inspired by commonsense knowledge bases.\nSection 3 intuitively discusses, using this framework how the structure of language may lead to ICL abilities.\nSection 4 specifically describes three ways in which ICL shows a distribution mismatch relative to general language modeling.\nSections 5--7 adduce experimental evidence from three domains: a new synthetic dataset (\"Calcutec\"), evidence from a small LMM, and a digit addition task."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- I found the toy dataset (\"Calcutec\") quite interesting, and to improve in some ways over prior synthetic setups for ICL, such as Xie et al 2022 or Chan et al 2022, in that it includes a simple kind of logical reasoning.\n\n- Provides evidence that even smaller LLMs (GPT-2) can perform ICL with artificial/task-agnostic label symbols (which Wei et al 2023 argued only large LLMs can do).\n\n- provides empirical results from different domains"
                },
                "weaknesses": {
                    "value": "- While I found the Calcutec experiment in particular to be innovative, the theoretical arguments in Sections 2--3 are quite hand-wavy and unspecific. There is no rigorous theoretical statement of the assumptions and conclusions made in the theoretical framework and the reasoning of how language modeling may lead to ICL.\n\n- While I believe the Calcutec toy dataset is an interesting contribution and a strength of the paper, it is limited in that the training dataset appears to bake in the repetitive nature of prompts by assuming that each \"paragraph\" in a document is about one of two latent concepts (\"topics\"), as in the prompting downstream tasks. A potential concern about the CoT evaluation is mentioned as a Question.\n\n- In the Digit Addition Task, the ability of the LM to complete the task in one go, whereas the training set usually had intermediate steps, is interpreted as an ICL ability representing a domain shift. However, as the training set also had intermediate steps stochastically dropped (independently, as far as I got from the paper -- so it is possible for all steps to be dropped simultaneously), it is not clear in which sense the test examples are out-of-domain relative to the training distribution. The same concern applies to the Calcutec dataset."
                },
                "questions": {
                    "value": "- How exactly is Chain-of-thought evaluated in Calcutec? Does the prompt only include the first step in the chain? And under what circumstances is the LM's answer counted as correct -- are predictions rolled out until the \";\" paragraph appears? This question is crucial for assessing the meaningfulness of the CoT results."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4423/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4423/Reviewer_MUGa",
                        "ICLR.cc/2024/Conference/Submission4423/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4423/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698839439880,
            "cdate": 1698839439880,
            "tmdate": 1700322617829,
            "mdate": 1700322617829,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "h5y20ydCVy",
                "forum": "aaYBsuGRne",
                "replyto": "GGaLNzAXOW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4423/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4423/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your thoughtful comments. We would like to address the weakness below:\n\n1. In Section 2, we propose a formalism for NLP classification tasks while the main purpose of Section 3 is to justify the assumptions made in Section 5. We see the assumptions in Section 5 as the formal statement/abstraction of our theorem in Section 3. We are sorry that we didn\u2019t describe it more clearly. We will improve this part in our later revision. Please let us know if we need to make more clarifications.\n2. As far as we know, all theoretical analyses on ICL depend on some repetitive nature of prompts or training data, e.g. Xie et al and Hahn & Goyal. One difference in our work is that we focus on the repetitive usage and the consistent meaning of pronouns, which is very realistic in real-world data. In Figure 2, we also show that the model can still do ICL when the task description is more complicated than the meaning of the pronouns seen in the training set, i.e., when the task description is composed of 3 atomic concepts (the green lines for \u201ctriple\u201d in Figure 2).\n3. We discuss this point below:\n    - The digit addition task: Please refer to Figure 8 for the distribution of the number of steps in the datasets. For the 5-digit addition task, when the dropping rate is 0.2, there are only 265 (0.265%) training samples where all the intermediate steps are dropped. We think it is reasonable to say the testing samples are out-of-domain in this case. (And GPT-2-sized model can achieve ~80% accuracy with only 265 training samples in the testing domain.) Meanwhile, considering that the real-world data also has a small fraction of text that drops all the reasoning steps, we think this setting is not unrealistic.\n    - Calcutec: Please refer to Figure 5. It shows that even when no steps are dropped in the training set, the model still can do ICL, though the performance is not as good.\n\nQuestion:\n\n- The prompt is as the one in Table 1. We only include the reasoning steps of some (3) randomly selected examples and the premise of the testing input in the prompt. (For the example in Table 1, the testing input is `x55 x76 x84 x99`). We unroll until the first verbalizer is generated. \n\nPlease let us know if you have any other questions. We are more than happy to discuss with you."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4423/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699775634544,
                "cdate": 1699775634544,
                "tmdate": 1699775954248,
                "mdate": 1699775954248,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Yy3jFQ4TAW",
                "forum": "aaYBsuGRne",
                "replyto": "h5y20ydCVy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4423/Reviewer_MUGa"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4423/Reviewer_MUGa"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response.\n\nThe Question is answered to my satisfaction.\n\nRegarding the Weaknesses, I take your point that other theoretical studies of ICL also need to assume repetition in the training set. I also appreciate the explanations regarding the number of steps. In response to this, I have increased the 'soundness' score to 3."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4423/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700322534950,
                "cdate": 1700322534950,
                "tmdate": 1700322534950,
                "mdate": 1700322534950,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "deEg7asXhs",
            "forum": "aaYBsuGRne",
            "replyto": "aaYBsuGRne",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4423/Reviewer_cdtQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4423/Reviewer_cdtQ"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes the \"Pelican Soup Hypothesis\" to explain in-context learning in large language models. The key idea is that in-context learning relies on models acquiring commonsense knowledge and reasoning skills from pretraining on general text. The paper formalizes NLP classification tasks as mapping inputs to output concepts based on commonsense rules and knowledge. Experiments on a synthetic dataset Calcutec show models can acquire in-context learning abilities."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper provides a clear and intuitive conceptual framework based on the Pelican Soup analogy to explain in-context learning.\n2. The proposed formalism for NLP tasks is simple yet quite general. It could be a useful tool for future theory research.\n3. Evidence from synthetic data, language modeling, and a toy task provide empirical support for the central hypothesis.\n4. The Calcutec dataset offers a nice testbed for studying in-context learning and model architectures.\n5. Analysis of the digit addition task sheds light on how model scale impacts reasoning abilities."
                },
                "weaknesses": {
                    "value": "1. The explanations are conceptual. More formal theoretical analysis could better elucidate the mechanisms.\n2. More analysis could be done on how different pretraining corpora impact in-context abilities.\n3. The hypothesis focuses on classification; generative tasks may involve additional factors."
                },
                "questions": {
                    "value": "1. Can we quantify the relative importance of different distribution shifts identified?\n2. How well does the formalism proposed capture more complex real-world reasoning?\n3. Is it possible to design pretraining objectives to better acquire commonsense and reasoning?\n4. How can we test if models learn explicit commonsense rules and reasoning versus pattern matching?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4423/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698959315287,
            "cdate": 1698959315287,
            "tmdate": 1699636416397,
            "mdate": 1699636416397,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XaxpSv0ajx",
                "forum": "aaYBsuGRne",
                "replyto": "deEg7asXhs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4423/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4423/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank you for the positive feedback. With regard to the weakness:\n\n1. While Section 3 is purely conceptual, we provide rigorous characterization in Section 5.1. We consider this as one of our main contributions, an instantiation for the assumptions made in previous theoretic works, such as [1] and [2]. For [1], our framework constituted with the assumptions we made in Section 5.1 satisfies the requirements of Corollary 4.2, therefore we can use their analysis to get an $O(1/T)$ regret bound ($T$ is the number of examples in the demonstration). For [2], please refer to our reply to reviewer rNJR.\n2. Thanks for the suggestion! We can run more experiments. Could you let us know what experiments you think are important are missing?\n3. We admit that we only focus on classification tasks in this work. However, we would like to stress that even for the classification setting, the mechanism of ICL is not yet clear. Future work may also extend our framework to generation tasks.\n\nWith regard to the questions:\n\n1. By preprocessing the training data for LLMs, we think it\u2019s possible. However, training an LLM requires lots of computational resources. We leave it for future work.\n2. The formalism in Section 2 can handle arbitrarily difficult reasoning tasks as long as there is a logic system that can handle it and the induction searching process is computationally feasible, e.g. not NP-hard. We believe that most human-solvable tasks are in this class.\n3. We think this is an important open question. A possible solution would be augmenting the training data, e.g. [3].\n4. Our conjecture would be that for many commonsense rules, LLMs are trained with many instances for each of them. Because the training data may cover a large portion of common surface forms, even though it\u2019s possible that LLMs are doing no more than pattern matching, they are still able to do \u201creasoning\u201d following the commonsense rules.  \n\n[1] Zhang Y, Zhang F, Yang Z, et al. What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization[J]. arXiv preprint arXiv:2305.19420, 2023.\n\n[2] Sanjeev A. and Anirudh G. A theory for emergence of complex skills in language models. arXiv preprint arXiv:2307.15936, 2023.\n\n[3] Zhou, Wangchunshu, Ronan Le Bras, and Yejin Choi. \"Commonsense Knowledge Transfer for Pre-trained Language Models.\" arXiv preprint arXiv:2306.02388 (2023)."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4423/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699851936073,
                "cdate": 1699851936073,
                "tmdate": 1699851936073,
                "mdate": 1699851936073,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]