[
    {
        "title": "DORSal: Diffusion for Object-centric Representations of Scenes $\\textit{et al.}$"
    },
    {
        "review": {
            "id": "BEJtOpRKo1",
            "forum": "3zvB14IF6D",
            "replyto": "3zvB14IF6D",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5701/Reviewer_J43q"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5701/Reviewer_J43q"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel approach for controllable scene synthesis with an object-centric representation. It first extracts object slots, the object-centric representation, using an auto-encoder, and then trains a multi-view diffusion model conditioned on these object slots for novel view synthesis. Extensive experiments on MultishapeNet and Street View datasets support the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper demonstrates the effectiveness of using an object-centric representation for controllable scene synthesis. The object slots can be learned in an unsupervised manner and represent the scene decompositionally. Training a multi-view diffusion model conditioned on such representation can support high-quality scene synthesis, and enable many applications like object removal and transferring. \n\n2. This paper is well-written. It is easy-to-follow and contains much details."
                },
                "weaknesses": {
                    "value": "1. The paper only shows results with low resolution. Experiments with higher resolution can further reveal the potential of the proposed method.\n\n2. The proposed scene editting scheme can only support object removal and transferring. Can the framework be extended to support more diverse scene editing operations (like translation, rotation)."
                },
                "questions": {
                    "value": "1. Since you incorporate the Street View dataset, which features complex and unbounded outdoor scenes, I am particularly interested in the OSRT performance in this dataset. Could you please provide object-decomposed visualization as illustrated in the OSRT paper (Fig. 4). These visualizations would reveal how different portions of the street view are represented by individual slots.\n\n2. What is the rationale of incorporating a video (multi-view) diffusion model for synthesis? If the capacity of OSRT is strong enough, will a single-view diffusion model (conditioned on OSRT and single view direction) be enough to generate results with good multi-view consistency?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5701/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5701/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5701/Reviewer_J43q"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5701/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698734492351,
            "cdate": 1698734492351,
            "tmdate": 1699636596424,
            "mdate": 1699636596424,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "r7zouh1ptA",
                "forum": "3zvB14IF6D",
                "replyto": "BEJtOpRKo1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5701/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5701/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer J43q (part 1)"
                    },
                    "comment": {
                        "value": "Thank you for your review and suggestions to improve our work! We are pleased that you recognize how our extensive experiments on MultishapeNet and Street View datasets support the effectiveness of our approach, and how this research can enable many applications.\n\nIn terms of the Weaknesses (W) and Questions (Q) you list, our response is as follows:\n\n**W1: The paper only shows results with low resolution.**\n\nIndeed, we agree that experiments with higher resolution would be even more impactful. However, the resolution of 128x128 we report results on is typical for relevant prior work. In fact, both the diffusion-based 3DiM baseline and the SRT-based baselines report only up to 128x128, as does the video diffusion architecture we base our decoder on.\n\nGenerally speaking, high-resolution, consistent video generation (or in our case generating multiple views of the same scene) is a challenging open problem (Hoogeboom et al., \u201cEnd-to-end diffusion for high resolution images\u201d, 2023). Fortunately, a straightforward approach to increase the resolution is via cascading: having multiple stages of diffusion, where later stages perform super-resolution to arbitrarily increase the resolution (Ho et al., \u201cCascaded diffusion models for high fidelity image generation\u201d, 2022). This approach is equally applicable to DORSal, though we have not considered it in practice, as high resolution generation would require an additional standalone diffusion model and is not the main focus of our work.\n\n**W2: The proposed scene editting scheme can only support object removal and transferring.**\n\nIndeed, we acknowledge that this is a limitation of our current approach and supporting more fine-grained scene editing operations is an important direction for future work. For the rotation and translation you mentioned in particular, it is foreseeable how supervised co-training with language can provide an interface for this as in \u201cObject 3DIT\u201d (Mochel et al., 2023), which we cite in our work. Alternatively, the object representations themselves could be disentangled to a point, where information about the rotation or position of an object is isolated, and can thus be manipulated independently during generation. In general, fine-grained, object-level control is an active area of research, and approaches that offer such capabilities like DisCoScene (Xu et al., 2023) requires a lot more prior knowledge about the scene to be able to work.  \n\nWe recognize that the current discussion of limitations is perhaps too lenient in this regard. In the revision, we will update it to comment on fine-grained control.\n\n**Q1: Could you please provide object-decomposed visualization as illustrated in the OSRT paper (Fig. 4).**\n\nThank you for your interest. Figure 1 in the current submission contains a representative example of how OSRT assigned different parts of the scene to different slots (like Figure 4 in the OSRT paper). For completeness, we have uploaded the same visualization for the other target views as well, and for an additional scene, in the supplementary material (*osrt_streetview_decomposition.pdf*). It can be seen how OSRT (trained fully unsupervised here) usually offers a coarse decomposition of the background into broad segments and partially succeeds at decomposing some of the objects. \n\nWe note how OSRT was never evaluated on Street View in the first place, and is clearly pushing the limits of what it is capable of. For this reason we show interesting cases in Figure 6 and exhaustive edits including failure cases in Figure 13 in Appendix C. In Appendix C.2 we comment on how \u201cThese failure modes likely originate in part from the unsupervised nature of the OSRT base model, which sometimes assigns multiple slots to a single object, or does not decompose the scene well. Fully \u201cimagined\u201d objects (i.e. objects which are not visible in the input views and therefore not encoded in the Object Slots) further generally cannot be directly edited in this way.\u201d. In that same section we also hint at possible solutions for addressing this."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5701/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700082589369,
                "cdate": 1700082589369,
                "tmdate": 1700082589369,
                "mdate": 1700082589369,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fkEOzRBH8k",
                "forum": "3zvB14IF6D",
                "replyto": "m4kFego0Vl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5701/Reviewer_J43q"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5701/Reviewer_J43q"
                ],
                "content": {
                    "comment": {
                        "value": "Hi, authors. Apologies for my delayed response as I just recovered from the deadlines of ICLR and CVPR. Thank you for your reply, which has addressed most of my concerns. However, I still have some reservations regarding the editing capacity.\n\nYou mentioned that \"it is foreseeable how supervised co-training with language can provide an interface for this as in 'Object 3DIT' (Mochel et al., 2023).\" However, Object 3DIT relies on a paired dataset for supervised training with language. I am uncertain how this technique can be applied to more general cases without language captions, such as the street view dataset you used. Additionally, you mentioned that \"the object representations themselves could be disentangled to a point, where information about the rotation or position of an object is isolated, and can thus be manipulated independently during generation.\" I am also unclear about how to achieve this kind of disentanglement based on the current method without significant modification."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5701/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633166914,
                "cdate": 1700633166914,
                "tmdate": 1700633166914,
                "mdate": 1700633166914,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dLHKmK15kE",
            "forum": "3zvB14IF6D",
            "replyto": "3zvB14IF6D",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5701/Reviewer_dJMA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5701/Reviewer_dJMA"
            ],
            "content": {
                "summary": {
                    "value": "The paper extends the Object Scene Representation Transformer (OSRT) with a diffusion-based decoder, proposing DORSal.\nIt enables a model to render precise images while maintaining properties of OSRT, i.e., (unsupervised) decomposition of objects.\nThe experiments were conducted with appropriate baselines, metrics, and datasets."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Straightforward success of the proposal idea to an important problem, i.e., diffusion decoder for OSRT-based NVS. It enables the models to generate sharp images with object-centric properties. This is a nice contribution to the research of SRT because the SRT family has been likely to render very blurred images, which is one of the largest weaknesses.\n- Easy to read and well-written manuscript. The text is very fluent and informative."
                },
                "weaknesses": {
                    "value": "- A little reserved technical novelty. The usage and its effectiveness of diffusion-based decoders for precise rendering of NVS models are already widely spread this year (Watson et al., 2023; Chan et al., 2023; Tewari et al., 2023). In terms of that, this paper could be seen as a simple attempt to borrow the idea into another form of NVS, OSRT. While the video diffusion-like multi-frame architecture for considering consistency might be a novelty, the ablation test or comparisons on the decoder architecture are missing.\n    - While the core idea is straightforward, some implementation details or hyperparameters for better performance may be informative and implicitly have novelty if the codebase is released. Do you have any plans to release it publicly?\n- 3D inconsistency. SRTs do not have 3D consistency in design. Furthermore, DORSal could have worse 3D inconsistency due to its decoding process, in my understanding. From supplementary rendering videos, DORSal's results seem a little unstable when changing viewpoints. An example of objects changing their shapes continuously is shown in the center video of dorsal_multishapenet_3.gif.\n    - If possible, more investigation on 3D consistency is nice (while I understand the evaluation protocol is not obvious depending on the settings). The current test (only Sec. 5.3's) is quite limited and might underestimate failure cases by DORSal.\n    - (It could be a nice defense to suggest NVS applications requiring less 3D consistency if they exist.)\n\n\n----\n\nAdditional citations\n- Generative Novel View Synthesis with 3D-Aware Diffusion Models, Eric R. Chan et al., ICCV 2023\n- Diffusion with Forward Models: Solving Stochastic Inverse Problems Without Direct Supervision, Ayush Tewari et al., NeurIPS 2023"
                },
                "questions": {
                    "value": "- Conditioning for diffusion models uses frozen OSRT's representations. Is freezing required for better performance? Any experiments on joint training or two-stage training (i.e., 1. training OSRT -> 2. training OSRT and diffusion)? Appendix A says, \"End-to-end training comes with additional challenges (e.g. higher memory requirements), but is worth exploring in future work.\" Is the memory requirement extremely high?\n- Can DORSal faithfully reproduce images from the observed (input) viewpoints? Although OSRT was not able to reproduce inputs due to its blurred reconstruction, DORSal could do that. Even if it cannot, the result is very informative for analyzing the behavior. It may indicate that DORSal is still very unfaithful in terms of reference ability in addition to 3D consistency or estimation.\n- (a little out of interest) Does DORSal work in out-domain scenes? While I guess that OSRTs may not be good at out-domain scenes, DORSal could possibly be more generalized to them thanks to diffusion-based training. If true, it would be a new and significant strength of the DORSal.\n- Sec. 3.2 says, \"To obtain instance segments from edits with DORSal, we propose the following procedure...\" Actually, is the procedure also used for OSRT baselines in comparison? Or did baselines use different methods?\n- Sec. 5.3. says, \"This is also reflected in our quantitative ... mixed views.\" This description seems unclear. Could you provide the details of the test procedure? Is PSNR calculated by a rendered view and a +360 degree re-rendered view?\n- Fig. 6 says, \"Notably, the encircled tree in the final row is generated upon removal of a slot to fill up the now-unobserved facade previously explained by this slot. The original scene does not contain a tree in this position.\" Could you explain this more? I didn't understand whether this was success or failure (I felt it was bad behavior).\n- Fig. 5 shows combination-based editing. Did the results render only objects which are shown in one of the target scenes? Or, are there many unintended (or unintended-shape) objects? And, does each column is rendered from the same camera pose? In other words, I wonder whether I can assume an object is transferred to the same position (in the image) after the composite. If not, checking the result is pretty difficult."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5701/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698742783420,
            "cdate": 1698742783420,
            "tmdate": 1699636596292,
            "mdate": 1699636596292,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XYfSgQhhbK",
                "forum": "3zvB14IF6D",
                "replyto": "dLHKmK15kE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5701/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5701/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer dJMA (part 1)"
                    },
                    "comment": {
                        "value": "Thank you for your review and suggestions to improve our work! We are excited that you agree that this work addresses an important problem and is a \u201cnice contribution to the research of SRT\u201d. We were also glad to learn that the paper is easy to read, well-written and very fluent and informative. Thank you also for the additional citations, which we will make sure to include.\n\nIn terms of the Weaknesses (W) and Questions (Q) you list, our response is as follows:\n\n**W1: A little reserved technical novelty.**\n\nWe mostly agree with your assessment regarding technical novelty, and your summary of our work as a \u201cStraightforward success of the proposal idea to an important problem\u201d is well put. The focus of this work is on making progress towards NVS from few observations in real-world scenes, while also opening up (or preserving, depending on how you look at) the possibility for scene editing. We achieve this by reusing and repurposing existing techniques from the literature, i.e. object-based representations of 3D scenes and video diffusion decoders. In demonstrating the feasibility of this approach (and what\u2019s more, how well it actually fares against the baselines), we expect follow-up work to be able to make further progress toward achieving these ambitious goals.\n\nBecause of this, we did not prioritize experiments that specifically target the design of the decoder or other architectural parts. In fact, at the pace at which video diffusion models progress, it is not unreasonable to expect a future version of DORSal to incorporate a slightly different decoder design (e.g. using latent diffusion). In terms of code, there is a publicly available implementation of OSRT (https://github.com/stelzner/osrt) and video diffusion models (https://github.com/lucidrains/video-diffusion-pytorch) already available. Hence, it is unlikely that we will release our own implementation of these. That said, we are happy to update the paper with additional details or hyperparameters if you find that anything is missing. We would also be happy to include pseudo-code in the paper if you think that would be useful. Generally, we have tried to take care in putting as much detail in the paper as possible.\n\n**W2: 3D inconsistency. SRTs do not have 3D consistency in design. Furthermore, DORSal could have worse 3D inconsistency due to its decoding process, in my understanding.**\n\nWe acknowledge that DORSal is not perfectly 3D consistent for the same reasons that SRT is not. And indeed, the fact that DORSal is stochastic adds an additional significant challenge, as any new information the model adds during the generation process needs to be synchronized between all generated views. Video diffusion models offer a promising solution for this problem for short, consistent video clips (and in the future likely for much longer videos) by jointly modeling a full sequence of frames. Achieving perfect consistency across a long camera path is still highly challenging as it goes beyond the context length that can be modeled directly via attention in the video diffusion model.\n\nTo provide some further insight into the issue of 3D consistency, we have re-computed the edit-segmentation scores on a subset of the samples for DORSal, where we measure both Edit FG-ARI across all the views simultaneously and a \u201c2D Edit FG-ARI\u201d where we compute Edit FG-ARI for each view individually and then average the results. Note that the latter does not penalize inconsistencies between the views, hence the gap between the two scores is indicative of any inconsistencies taking place. A similar approach to evaluating 3D consistency was carried out in the OSRT paper. We obtain 0.702 Edit FG-ARI and 0.721 2D Edit FG-ARI. The small gap between these scores indicates that the segmentations obtained via this procedure are highly consistent across views.\n\nIn terms of possible applications, a more 3D consistent approach is typically preferred, and in our paper we have suggested two possible techniques for improving this. Firstly, as shown on MultiShapeNet, having a view distribution that mixes close by and far apart cameras is more desirable. Secondly, having additional stages of \u201cframe shuffling\u201d throughout denoising can help address this. In the limit, where each stage corresponds to a single denoising step, this becomes similar to the \u201cstochastic conditioning\u201d technique deployed in 3DiM for this purpose. Finally, although we haven\u2019t shown this, it is foreseeable that a cascaded diffusion approach using additional super-resolution stages can help resolve tiny inconsistencies."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5701/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700082195962,
                "cdate": 1700082195962,
                "tmdate": 1700082195962,
                "mdate": 1700082195962,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RkqrP4mIAF",
                "forum": "3zvB14IF6D",
                "replyto": "XYfSgQhhbK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5701/Reviewer_dJMA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5701/Reviewer_dJMA"
                ],
                "content": {
                    "title": {
                        "value": "Thank you"
                    },
                    "comment": {
                        "value": "Thank you for your careful replies and additional information! They deepened my understanding.\n\nW1:\nThank you. One more thing is that it is nice to add the two diffusion x NVS papers in related work to clarify novelty (and what is not novelty).\n- Generative Novel View Synthesis with 3D-Aware Diffusion Models, Eric R. Chan et al., ICCV 2023\n- Diffusion with Forward Models: Solving Stochastic Inverse Problems Without Direct Supervision, Ayush Tewari et al., NeurIPS 2023\n\nW2:\nThe experiment on 3D consistency is quite helpful. While the setup may underestimate 3D inconsistency because 3D consistency of appearance (maybe, typically referred to as \"3D consistency\") is more sensitive than that of segmentation, its result is great for the paper.\n\nQ1:\nI understand the issues with the resources and the separate implementations. I hope that future work challenges it.\n\nQ2:\nThat is a great experiment! While the input-view scores are better than the novel-view scores (in Table 1), I had expected that even PSNR was much higher. I began to guess that the frozen OSRT representations had already failed to maintain input-view information a lot, so DORSal also cannot reproduce the input views in detail and (almost) deterministically.\nAnyway, thank you for providing the result!\n\nQ3, 4, 5, 6, 7:\nThank you!"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5701/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700093543418,
                "cdate": 1700093543418,
                "tmdate": 1700093543418,
                "mdate": 1700093543418,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "riXdZ3uYs0",
                "forum": "3zvB14IF6D",
                "replyto": "Kif1dVQbwm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5701/Reviewer_dJMA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5701/Reviewer_dJMA"
                ],
                "content": {
                    "title": {
                        "value": "I see"
                    },
                    "comment": {
                        "value": "Fortunately, after the comments, you got new responses from two of the reviewers. Given them, I hope that a well-considered decision will be made by the AC (and us). Thank you!"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5701/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700698329447,
                "cdate": 1700698329447,
                "tmdate": 1700698329447,
                "mdate": 1700698329447,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PgRueEZwae",
            "forum": "3zvB14IF6D",
            "replyto": "3zvB14IF6D",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5701/Reviewer_jJcr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5701/Reviewer_jJcr"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers an object-centric representation for efficient scene representation and rendering. The work builds on earlier work on object based scene representations (OSRT) but considers a different decoder based on video diffusion models instead of a simple decoder network trained with an l2 loss. Experiments show that the novel view synthesis is much sharper leading to a lower FID score, and that the representation allows for scene editing."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The methodology is sound, by using a video diffusion model for the decoding/rendering which results in sharp and consistent images. This addresses issues with previous works where the renderings in general are quite blurry and/or not consistent.\n- Experiments show that the method can obtain significantly lower FID (but not PSNR and LPIPS) for novel view synthesis compared to existing work based on scene representations, although not as significant when compared to existing methods based on diffusion models (3DiM).\n- The renderings of the scenes appear consistent across views and over time. Furthermore, for scene editing, the filled in regions when objects are removed appear realistic, and the object slots seem to mostly correspond to specific objects (e.g. a car or a tree) in the scene."
                },
                "weaknesses": {
                    "value": "- The scene editing is more a property of OSRT than the proposed method. The object-slots are pre-trained from OSRT and not refined or learned in this paper.\n- One main limitation of the method is that we can not control specific object slots. If we transfer a slot from one scene to the other, we can not (to my understanding, please correct if it is incorrect) e.g. move or rotate it in an easy way. It is placed in exactly the same position as in the original scene, which in general is not very useful.\n- The videos in the supplementary material would have been more informative if they would have shown results for all methods compared to as well, and also the input image/images."
                },
                "questions": {
                    "value": "See weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5701/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5701/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5701/Reviewer_jJcr"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5701/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698759514000,
            "cdate": 1698759514000,
            "tmdate": 1699636596179,
            "mdate": 1699636596179,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WT81CRd7XF",
                "forum": "3zvB14IF6D",
                "replyto": "PgRueEZwae",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5701/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5701/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reponse to reviewer jJcr"
                    },
                    "comment": {
                        "value": "Thank you for your review and suggestions to improve our work! We appreciate that you recognize that our methodology is sound, that DORSal can obtain significantly better FID, that the renderings appear consistent across views and over time, and that the presented edits are mostly realistic.\n\n**W1: The scene editing is more a property of OSRT than the proposed method. The object-slots are pre-trained from OSRT and not refined or learned in this paper.**\n\nWe agree that the editing capabilities we present here are mainly a consequence of having object slots. However, other aspects of the model play an important role too. For example, because DORSal is a probabilistic approach, it is better equipped to handle the inconsistencies that can arise due to removing or combining different slots and the uncertainty that arises from this. Indeed, compare the plane in the middle frame in the top row in Scene 2 to the same frame in the Combination scene directly to the right. Notice how DORSal generates plausible looking shadows under the plane to match the lighting when viewed from this angle as in Scene 1. This impressive feat is a direct consequence of using a diffusion-based decoder and not something that the deterministic OSRT decoder can achieve. Indeed, this observation alone already might indicate that a diffusion-based decoder might be more suitable for the editing operations presented here.\n\n\n**W2: One main limitation of the method is that we can not control specific object slots**\n\nIndeed, we acknowledge that this is a limitation of our current approach and supporting more fine-grained scene editing operations is an important direction for future work. For the rotation and translation you mentioned in particular, it is foreseeable how supervised co-training with language can provide an interface for this as in \u201cObject 3DIT\u201d (Mochel et al., 2023), which we cite in our work. Alternatively, the object representations themselves could be disentangled to a point, where information about the rotation or position of an object is isolated, and can thus be manipulated independently during generation. In general, fine-grained, object-level control is an active area of research, and approaches that offer such capabilities like DisCoScene (Xu et al., 2023) requires a lot more prior knowledge about the scene to be able to work.  \n\n\nWe will improve our current discussion of limitations to put more emphasis on this, and highlight ways in which future research may address this.\n\n**W3: The videos in the supplementary material would have been more informative if they would have shown results for all methods compared to as well, and also the input image/images.**\n\nThank you for pointing this out. We will add video results for baselines to the supplementary results in the updated version of our paper. In the meantime, we recommend viewing the website of the OSRT baseline (https://osrt-paper.github.io/) for representative video results."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5701/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700082056230,
                "cdate": 1700082056230,
                "tmdate": 1700082056230,
                "mdate": 1700082056230,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VveYxijN19",
                "forum": "3zvB14IF6D",
                "replyto": "WT81CRd7XF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5701/Reviewer_jJcr"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5701/Reviewer_jJcr"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the answers. I keep my rating as 6, and below are some brief comments.\n\nW1: I agree with the raised points that a probabilistic decoder can e.g. handle inconsistencies better, but the possible scene edits are still exactly the same as the baseline method, but with better/more consistent renderings which somewhat limits the technical novelty of the paper.\n\nW2: These are all good and valid suggestions, but as pointed out by reviewer J43q it would require significant modifications of the proposed method.\n\nW3: Indeed, the videos look less sharp than DORSal. It seems clear from the paper that the proposed method is better qualitatively and by the evaluated metrics."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5701/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700659507996,
                "cdate": 1700659507996,
                "tmdate": 1700659507996,
                "mdate": 1700659507996,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xz7BeuZqN3",
            "forum": "3zvB14IF6D",
            "replyto": "3zvB14IF6D",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5701/Reviewer_16Uh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5701/Reviewer_16Uh"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents DORSal, a 3D scene generation model that leverages object-centric representations. The approach consists of two stages: first, it pre-trains an Object Scene Representation Transformer (OSRT) to encode multi-view images into slot-based representations that capture the objects in the scene. Second, it trains a diffusion-based conditional multi-view decoder that takes the frozen slot representations as input and renders novel views of the scene. The paper demonstrates that DORSal can generate 3D scenes with higher image quality than the baseline models, and also perform scene manipulation tasks such as object removal.\n\n---\n\npost rebuttal:\n\nrating 3 -> 5"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The generation quality of the proposed model is much better than the baseline models. This is a meaningful improvement as it can help scale up object-centric learning to large-scale applications.\n2. The experiments in object-level scene manipulations are quite interesting. The paper demonstrates that the learned representation allows object-level scene editing by removing or transferring slots between scenes. This is a promising result that could lead to further research in this area."
                },
                "weaknesses": {
                    "value": "1. The model reliance on pre-trained object-centric representations instead of end-to-end training with diffusion models is a potential weakness. \n    * The quality of the object slots provided by OSRT may not capture the object representation, including the appearance information, well. Eventually, as the decoder becomes stronger and stronger, the quality of the slot representations will become the bottleneck of improving the generation quality. This brings limitation to the model in scaling up to more realistic scenes.\n\n2. Given that the representations are pre-trained, the model seems to be too straightforward and limited in its technical contribution. \n    * The improvement of image quality is obvious when the slots are pre-computed and frozen, one might expect the introduction of the diffusion decoder to have some effect on the representation learning process. However, the paper was not able to demonstrate this."
                },
                "questions": {
                    "value": "1. What are the benefits and drawbacks of training DORSal end-to-end? How would it affect the quality of the object representations?\n2. In cases where end-to-end training poses challenges, such as collapsing issues, could it achieve better performance by finetuning the pre-trained slots encoder during the decoder training stage?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5701/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5701/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5701/Reviewer_16Uh"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5701/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699136628253,
            "cdate": 1699136628253,
            "tmdate": 1700716291202,
            "mdate": 1700716291202,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cprLHV52wB",
                "forum": "3zvB14IF6D",
                "replyto": "xz7BeuZqN3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5701/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5701/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer 16Uh (part 1)"
                    },
                    "comment": {
                        "value": "Thank you for your review and for your constructive comments. We appreciate that you recognize the strengths of our method, in particular the substantial improvement in generation quality compared to prior work, and the novel conceptual contribution of object-level scene editing in diffusion generative models enabled by conditioning on structured, object-centric scene representations (\u201cobject slots\u201d).\n\n\nWe address your comments regarding your highlighted weaknesses of the paper in the following.\n\n\n**W1: The model reliance on pre-trained object-centric representations instead of end-to-end training with diffusion models is a potential weakness.**\n\n\nWe\u2019d like to respectfully push back on this argument (in the context of this being a weakness of our paper), for the following reasons:\n* Joint (end-to-end) representation learning and generative modeling with diffusion models is an active area of research and achieving competitive representation learning performance this way (as opposed to using deterministic reconstruction objectives or contrastive learning) is an open research problem. Solidly addressing this problem goes far beyond the scope of our investigation. In the future, as the community makes progress on this problem, e.g. in the setting of single-image generative models, it would be great to revisit the end-to-end setting in the context of DORSal.\n* We rely on the ability to inject segmentation supervision into the OSRT base model on MSN-H for significantly improved scene editing performance. While this is straightforward for OSRT, where the decoder provides a clear (soft) segmentation prediction, it is unclear how to achieve this when training the OSRT encoder end-to-end through a DORSal diffusion decoder, which does not provide an obvious modeling path for segmentations.\n* Separating the problem into a representation learning stage for conditioning information (here: object slots) is common practice in text-conditioned diffusion models. It has several benefits: 1) significantly reduced memory, compute, and infrastructure complexity; 2) modularity: our two-stage training recipe has the advantage of directly benefiting from future advances in scene representation learning methods as well as diffusion generative models.\n\nRegarding your comment \u201cthe quality of the slot representations will become the bottleneck of improving the generation quality\u201d: this is true irrespective of whether the model is trained end-to-end with a diffusion-based decoder or a deterministic decoder. We believe that there is no reason to assume that a diffusion-based decoder would necessarily result in better object representations compared to a deterministic decoder (which we chose). Running such an end-to-end experiment is non-trivial with the existing models used in our work since they live in different code bases and are optimized to make maximum use of available device memory (in addition to the issue of segmentation supervision injection). We leave this investigation for future work, once joint representation learning and generative modeling with diffusion models has advanced as a research field.\n\n**W2: Given that the representations are pre-trained, the model seems to be too straightforward and limited in its technical contribution.**\n\nWe agree that the novelty solely in terms of architecture contribution for the individual method components is limited. In our view, the novelty and significance lies in the elegance/simplicity of the combined approach (which has not been demonstrated before), as our recipe allows for (almost) direct use of established components and thus also benefits directly from future development in these areas. Indeed, we demonstrate that conditioning a video diffusion architecture on (structured) scene representations and pose encodings allows us to overcome limitations of a prior state-of-the-art 3D generative diffusion model (3DiM) and introduce a sampling process to achieve improved consistency for sampling long camera path videos (up to 200 frames), which is an extremely difficult task.\n\nAside from NVS, consistent object or asset transfer between scenes in video/3D generative models is a major unsolved problem, and DORSal shows promising progress on this problem. DORSal demonstrates that this recipe can achieve new forms of control (object removal, object transfer) via simple manipulation of the conditioning information without the need to collect paired editing data or the need to perform pixel-level masking and inpainting.\n\nWe would appreciate it if you could reconsider your assessment regarding novelty (and significance to the community) to focus on aspects beyond architectural novelty, as discussed above."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5701/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700081924867,
                "cdate": 1700081924867,
                "tmdate": 1700081924867,
                "mdate": 1700081924867,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "U7XUN0liM1",
                "forum": "3zvB14IF6D",
                "replyto": "xz7BeuZqN3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5701/Reviewer_16Uh"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5701/Reviewer_16Uh"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response"
                    },
                    "comment": {
                        "value": "I thank the authors for their response and apologies for the delay.\n\nTo clarify, my question did not pertain to the comparison between representation learned by diffusion model or OSRT, which would indeed be beyond the scope of this paper. Rather, I am concerned whether the information contained in the pre-trained, frozen representations was sufficient for the diffusion decoder to accurately render the fine details in image generation. This is questionable given the fact that the representation learning approach, OSRT, demonstrates suboptimal reconstruction quality in complex images. This could suggest that it might not capture the finer details necessary for generating new viewpoints of the same objects later when using the diffusion model.\n\nHowever, after reviewing the feedback from other reviewers and the authors' responses, I do agree that it is interesting to see that the high-quality image generation can be achieved by the simple combination of existing approaches. Moreover, it is also quite surprising to see that the frozen representations could provide generations that are far beyond the detail level demonstrated by the results of the representation learning model OSRT.\n\nConsidering these points, I am raising my rating."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5701/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700716225165,
                "cdate": 1700716225165,
                "tmdate": 1700716835727,
                "mdate": 1700716835727,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]