[
    {
        "title": "Minimum Edit Distance Training for Conditional Language Generation Models"
    },
    {
        "review": {
            "id": "vppGXj6LIe",
            "forum": "V4fyVlX13y",
            "replyto": "V4fyVlX13y",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2176/Reviewer_DVwK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2176/Reviewer_DVwK"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes minimum edit distance training for machine translation (MT) and speech recognition (ASR) in the context of attention-based encoder-decoder models. The authors suggest that the conventional maximum likelihood training with teacher forcing results in exposure bias for these models, and as such, sequence-level objectives like minimum edit distance should work better since they are closely related to the evaluation metric. They also use the proposed objective to train a post-hoc calibration function. Experiments are mostly conducted using MT datasets, and a few using LibriSpeech (100h) for ASR, demonstrating improvements over the conventional training methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. While minimum Bayes risk (MBR) training is a well-known method for ASR, it has not received adequate attention for MT, perhaps because it is hard to agree upon a good definition of \u201crisk\u201d for this task which is efficient to compute. As such, this paper should fill this gap. Since MT can have multiple correct \u201calignments\u201d, the authors define a segment-level Levenshtein distance function (sMED) for this task.\n\n2. Section 2 is effective (at the cost of being a little too verbose) in explaining the discrepancy between training and evaluation metrics, in terms of Hamming distance and Levenshtein distance.\n\n3. The proposed MED training gives small BLEU score improvements on IWSLT data, and significant WER improvements on LibriSpeech (100h)."
                },
                "weaknesses": {
                    "value": "The two main weaknesses in the paper are novelty and empirical results, which I will describe in detail below.\n\n### MBR training is well known\n\nMinimum edit distance training is essentially a form of minimum Bayes risk (MBR) training where the risk function is Levenshtein distance between the reference and predicted sequences. MBR training has a long history in ASR, with instantiations in minimum phone error (MPE) [1], state-level minimum bayes risk (sMBR) [2], and MED training. In fact, the authors have cited [2] in the \u201cRelated Works\u201d section (which is unfortunately delegated to the Appendix), but they fail to make the connection. Instead, they have strangely focused more on \u201cscheduled sampling,\u201d which is just a training hack to reduce overconfidence in models trained with teacher forcing. Aside from choices of the risk function, MBR training has been explored in the context of hybrid HMM-based models [2], RNN-transducers [3], and attention-based encoder-decoders [4]. The last one, in particular, uses MED training for the same type of models as proposed in the paper. The main difference between the strategy proposed in the paper versus these in literature is that the authors compute the risk against a single sequence obtained using autoregressive decoding. I would argue that this is, in fact, a weaker version of traditional MBR training, which computes the \u201cexpected\u201d risk.\n\nEven in machine translation, MBR has a rich history. It has been primarily used for decoding [5] but has also seen use in training of statistical MT models [6]. The authors have not situated their paper against any of these well-known works, which comes across as ignorance of prior work.\n\nOnce we take into account all these existing (but not cited) work, the remaining contribution of the paper is two-fold: (i) segment-level edit distance for MT, and (ii) using MED training for the calibration function. In my opinion, while (i) is a clever idea, it stands against the original motivation of reducing discrepancy between training and evaluation metrics \u2014 for example, [6] performed MBR training using BLEU score and showed that it improved BLEU metrics most, compared to edit distance based metrics. This only leaves (ii) \u2014 it is interesting to see improvement in calibration errors, but this is not sufficient by itself considering lack of novelty in the main training method.\n\n[1] Povey, Daniel and Philip C. Woodland. \u201cMinimum Phone Error and I-smoothing for improved discriminative training.\u201d 2002 IEEE International Conference on Acoustics, Speech, and Signal Processing 1 (2002): I-105-I-108.\n\n[2] Vesel\u00fd, Karel et al. \u201cSequence-discriminative training of deep neural networks.\u201d Interspeech (2013).\n\n[3] Weng, Chao et al. \u201cMinimum Bayes Risk Training of RNN-Transducer for End-to-End Speech Recognition.\u201d Interspeech (2019).\n\n[4] Cui, Jia et al. \u201cImproving Attention-Based End-to-End ASR Systems with Sequence-Based Loss Functions.\u201d 2018 IEEE Spoken Language Technology Workshop (SLT) (2018): 353-360.\n\n[5] Kumar, Shankar and William J. Byrne. \u201cMinimum Bayes-Risk Decoding for Statistical Machine Translation.\u201d North American Chapter of the Association for Computational Linguistics (2004).\n\n[6] Och, Franz Josef. \u201cMinimum Error Rate Training in Statistical Machine Translation.\u201d Annual Meeting of the Association for Computational Linguistics (2003).\n\n### MT results show only minor BLEU improvements\n\nThe experimental results section is rather sparse, and the provided MT results do not suggest significant improvements. From Table 1, on most test sets, the BLEU score only improves by 0.5-1.0, with the relative improvement only about 2% in most cases. Furthermore, the relatively simple \u201cscheduled sampling\u201d already recovers 50% of this improvement in most cases, so I don\u2019t see why practitioners would implement MED training at all.\n\n### ASR experiments are done on a \u201ctoy\u201d task\n\nASR results are only shown on the 100h subset of LibriSpeech, which is almost a toy task at this point. The authors should perform training and evaluation at least on the full 960h LibriSpeech. In Appendix A, they have mentioned that they used 4 A100 GPUs for the MT experiments \u2014 this hardware should be quite sufficient to conduct experiments also on the full LibriSpeech. Otherwise, readers may wonder whether these results were not shown because MED provided no improvement in this setting (which may very well be the case from my experience).\n\nBesides the above limitations, I think it may be useful to reduce the space given to Section 2 (Background), since the train-eval discrepancy is already well known, and instead conduct more thorough evaluation."
                },
                "questions": {
                    "value": "In Section 3.1, the LBP is essentially the Levenshtein alignment, which means that equation (11) would compute the edit distance (or WER) between the reference and decoded hypothesis. Is this correct?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2176/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698594115087,
            "cdate": 1698594115087,
            "tmdate": 1699636150922,
            "mdate": 1699636150922,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "BXf8k5YEFg",
            "forum": "V4fyVlX13y",
            "replyto": "V4fyVlX13y",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2176/Reviewer_kjTK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2176/Reviewer_kjTK"
            ],
            "content": {
                "summary": {
                    "value": "Context: Attention-based encoder-decoder (AED) models (also referred to as conditional language generation (CLG) models) for speech recognition and machine translation.\n\nA new training method is proposed to fix the exposure bias problem, in which the model only sees the reference label sequence as prefix during conventional training, maximum likelihood estimation (MLE).\n\nThe motivation is to avoid the exposure bias problem and to have the training loss more consistent with the evaluation metric, namely the word error rate, i.e. the edit distance / Levenshtein distance.\n\nDuring training, beam search is performed to get a recognized sequence out. Then, the Levenshtein distance and alignment is calculated between the reference label sequence and the recognized label sequence. This gives a target for each position of the recognized sequence prefix, and that is used as an additional loss to the standard MLE training with teacher-forcing."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Interesting approach which should help for regularization and better generalization."
                },
                "weaknesses": {
                    "value": "A lot of mathematical formulations are introduced, which are rather confusing and distracting from the method, and also have some problems (see details for an example). The method could be explained in a much simpler way.\n\nThe Optimal Completion Distillation (OCD) conceptually leads to a very similar loss. The Optimal Completion for any given prefix should be exactly what any possible Levenshtein alignment would give as the next possible targets for a given prefix. The Optimal Completion even solves the problem of ambiguity when there are multiple targets possible, by mapping that to a target probability distribution instead of having a single target. Given that this method is so similar, even looks almost the same (not taking the derivation/formulation into account, but just what you actually do in the end), this should be directly compared to, when presenting the method, to see the actual differences, and also in the experiments. This is missing here.\n\nThe novelty is also limited, given that OCD is so similar.\n\nThe ASR experiments are only done on the Librispeech 100h subset, which is way too small to be relevant. The whole Librispeech 960h should be used.\n\nMBR training and OCD should be done for comparisons. I.e. not just citing results from another paper, but this should be done on the exact same model, within the same software, such that we really have a direct comparison.\n\nNo code is published?\n\nASR baseline is not good. (I did not really check the translation baseline too much. Maybe also not good.)\nMany details missing."
                },
                "questions": {
                    "value": "Definition 3.1: Something is wrong. You use union there, which is an operation for sets, but you use union on sequences, namely you take the union of \u03c8 with the tuple (t+1,t\u02c6+1). I don't know what this is supposed to mean. Is this concatenation of sequences? Or is it union of sets?\n\nDefinition 3.3 and Section 3.3. I did not really understand this. I understand the problem, and I was also wondering how you choose the Levenshtein alignment actually when it is ambiguous, which it often is. I guess this is some sort of smoothing, but the way it is defined is too complicated.\n\n\n\n> Because the inferred label sequence Y\u02c6 i is used to calculate the LBP, the CLG model trained using 1:t\u02c6 only NLL_{MED} in an offline scenario has a trivial solution that outputs the input label of the current time step.\n\nI don't fully understand this. Why would it learn to just output the input label? That would lead to an insertion error.\n\n\n> We trained the NMT model with the online scenario shown in Figure 2, and the ASR model with the offline scenario.\n\nWhy?\n\nAnd what is really the difference in the definition mathematically? Is this just an implementation detail, or is there a real difference?\n\n> In these experiments, we sampled the prefix from an interpolated distribution \u03b5SS P (Y\u02c6 ) + (1 \u2212 \u03b5SS )P (Y )\n\nI don't understand this. Is this on sequence level (Y being the whole sequence), or on label level (Y being one label)? What are the two distributions here? P is just one model, i.e. one distribution? Or it's on label level but using different prefixes? But this also does not make sense.\n\nWhat models are used exactly? This seems to use existing recipes from Fairseq and ESPnet. So it would be helpful to reference them exactly? Or if these are not existing recipes, why not? And why are those specific models chosen? E.g. for the ASR model, you would rather choose a Conformer-based baseline, not a Transformer.\n\n\nRuntime? Beam search during training might be expensive? So how much slower is the proposed training? This seems like a very relevant aspect, but there are no numbers on that?\n\n\n\nThe CY LIN. ROUGE... reference capitalization is wrong, it's Chin-Yew Lin. 2004. ROUGE..."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2176/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698794744286,
            "cdate": 1698794744286,
            "tmdate": 1699636150850,
            "mdate": 1699636150850,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "YfABJ47LZN",
            "forum": "V4fyVlX13y",
            "replyto": "V4fyVlX13y",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2176/Reviewer_FobE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2176/Reviewer_FobE"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a modification to the conventional Maximum Likelihood Estimate (MLE) training method, involving teacher forcing for encoder-decoder architectures, with a focus on minimum edit distance-based training. The paper's argument centers on the distinction between MLE training and inference, emphasizing that, during MLE training with teacher forcing, only corrections and substitutions are addressed, while deletion and insertion errors are not considered. In contrast, evaluation metrics, such as edit distances (WER) or n-gram overlaps (Bleu), differ from the training-time evaluation.\n\nThe core concept of this paper involves the training process, where a target sequence is initially sampled during training, using a beam search either from the same model or a different one. Subsequently, a Levenshtein alignment path is computed between these sequences. The MLE loss is then calculated over the Levenshtein backward path, accounting for insertion and deletion errors. For Neural Machine Translation (NMT) tasks, the paper introduces a segment-level modification, where the original sequence is segmented into n-grams, and alignment is established accordingly. The paper contends that Minimum Edit Distance (MED) training also enhances the model's calibration.\n\nEmpirical results from experiments conducted on NMT and ASR tasks underscore the practical benefits of the proposed minimum edit distance-based training approach."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is well-written and easy to follow.\n\n2. The empirical results show the usefulness of the approach. \n\n3. The calibration results are useful."
                },
                "weaknesses": {
                    "value": "1. Lack of comparison with minimum edit distance based training in ASR systems. The idea of using the edit distance criteria for training the ASR model is quite well explored [1,2]. \n\n\n\n\nReferences:\n\n[1] Prabhavalkar, Rohit, et al. \"Minimum word error rate training for attention-based sequence-to-sequence models.\" 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018.\n\n[2] Tian, Jinchuan, et al. \"Integrating lattice-free MMI into end-to-end speech recognition.\" IEEE/ACM Transactions on Audio, Speech, and Language Processing 31 (2022): 25-38."
                },
                "questions": {
                    "value": "1. How does sMED compare against MED for the NMT tasks?\n\n2. How much is the training speed impacted by MED computations? For MBR training, it is one of the major problem?\n\n3. What are the results of online training with MED for ASR task?\n\n4. Can you share examples where sMED objective lead to superior target sequences?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2176/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698897442837,
            "cdate": 1698897442837,
            "tmdate": 1699636150701,
            "mdate": 1699636150701,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "u7Z67so9qh",
            "forum": "V4fyVlX13y",
            "replyto": "V4fyVlX13y",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2176/Reviewer_eZjX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2176/Reviewer_eZjX"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposed an approach to train AED models to minimize the edit distance between the predicted sequence and the reference sequence. It changes the teacher forcing training to condition on the beam search decoding history instead of ground-truth history, and predict the next correct token along the alignment path with the minimum edit distance between the beam search decoded sequence and the reference sequence. By doing this, it can mitigate the expose bias and optimize the evaluation metric during training, leading to a better generalization performance. The proposed method is experimented in NMT and ASR tasks, showing improved performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The proposed idea is a new approach to mitigate the exposure bias issue, and optimize sequence level evaluation metric.\n- The proposed method is intuitive and technically sound.\n- It is a relatively straight-forward change in the training implementation.\n- Consistent improvement over the MLE approach across all NMT and ASR tasks in the experiments.\n- Calibration function performance also benefits from the proposed minimum edit distance loss, which in turn helps reduce WER as well.\n- The analysis showing the proposed approach reduces the accumulated errors due to exposure bias, especially for longer sequences, is promising.\n- Presentation is clear."
                },
                "weaknesses": {
                    "value": "- There are already many methods to optimize the evaluation metrics in the sequence-level training, as the authors mention some of them. E.g. there are minimum WER and MBR based approaches for ASR, see some examples below. But they are not compared in the ASR experiments in the paper. The proposed method was mainly compared to MLE, which is a weaker baseline. Even for NMT, only one task shows MBR result. This seems to be a major drawback of the paper.\n\nPrabhavalkar et al., Minimum Word Error Rate Training for Attention-based Sequence-to-Sequence Models, 2018\nShannon, Optimizing expected word error rate via sampling for speech recognition, 2017\nKingsbury, Lattice-based optimization of sequence classification criteria for neural-network acoustic modeling, 2009\n\n- Table 2 shows the results for different calibration functions. It seems they are all done in the ASR model that was trained with MLE. Can the same comparisons be done in the ASR model trained with MED as well? Would the MED trained calibration still improve the MED trained ASR model performance further?"
                },
                "questions": {
                    "value": "- The paper mentions \"Because the inferred label sequence is used to calculate the LBP, the CLG model trained using\nonly NLL_MED in an offline scenario has a trivial solution that outputs the input label of the current time step.\" But according to Eq (11), by definition NLL_MED trains the model to predict the correct token at the next time step in the LBP based on the previous predicted sequence. How come the model will trivially learn to output the input label of the current time step? Please elaborate.\n\n- Some brackets in Eq (13) seem to be missing. The notation is not clear enough."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2176/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699620953288,
            "cdate": 1699620953288,
            "tmdate": 1699636150640,
            "mdate": 1699636150640,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]