[
    {
        "title": "Efficient Network Embedding in the Exponentially Large Quantum Hilbert Space: A High-Dimensional Perspective on Embedding"
    },
    {
        "review": {
            "id": "pzPWozAHGn",
            "forum": "lROh08eK6n",
            "replyto": "lROh08eK6n",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5870/Reviewer_8cxP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5870/Reviewer_8cxP"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes two high-dimensional network embedding methods called node2ket and node2ket+ that outperform standard methods such as word2ket. As evidence of these claims, they perform experiments studying several tasks, obtaining better compressive ratio than other approaches. Additionally, they provide an implementation in a library called LIBN2K."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper improves upon existing network embedding techniques by proposing a high-dimensional embedding with better efficiency over previous quantum-inspired methods such as word2ket.\n- The theoretical analysis is clear, showing that node2ket gets a high-rank approximation of the information matrix\n- The experiments appear to be quite thorough and shows advantages over existing methods, as promised.\n- It is nice that the code is made available."
                },
                "weaknesses": {
                    "value": "- This is a quantum-inspired algorithm for classical machine learning, which suffers from the lack of a clear connection to quantum computation (see Questions).\n- There are gaps for actually making this algorithm \"quantum-friendly,\" as it is generally not easy to load classical information into a quantum device."
                },
                "questions": {
                    "value": "- What is the significance of the embeddings being designed for \"quantum Hilbert space\"? To me, the relationship to quantum computation is not clear and seems more like an afterthought. Quantum computers are known to be good at problems with certain structure, and it's not clear to me what structure is being leveraged here (and what benefits are obtained as a result).\n- The fact that pure quantum states are normalized leads to the constraint $\\\\|\\mathbf{x}_i\\\\|=1$. Is there any consideration for eliminating a global phase, which I assume would affect the embedding? Also, I wonder what embedding might be developed for more general quantum states, such as mixed states.\n- Forgive my ignorance, but what is the definition of positive node pairs and negative nodes? Does it just mean the inner product is positive or negative?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5870/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5870/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5870/Reviewer_8cxP"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5870/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698266126203,
            "cdate": 1698266126203,
            "tmdate": 1699636622040,
            "mdate": 1699636622040,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5y6MlnLhnS",
                "forum": "lROh08eK6n",
                "replyto": "pzPWozAHGn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5870/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5870/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer 8cxP"
                    },
                    "comment": {
                        "value": "Many thanks for your comments and appreciation. Your extensive knowledge in quantum computing will help us further improve our paper.\n\n---\n\n\n### **Q1: What is the significance of the embeddings being designed for \"quantum Hilbert space\"? To me, the relationship to quantum computation is not clear and seems more like an afterthought. Quantum computers are known to be good at problems with certain structures, and it's not clear to me what structure is being leveraged here (and what benefits are obtained as a result).**\n\nA1: Thanks for your feedback. We would like to emphasize that our approach is so far a classical one, aligning with the principles of quantum mechanics and potentials to be run on a quantum machine. We will provide a more detailed explanation below.\n\nWe agree with the reviewer's observation that amplitude encoding theoretically allows the use of $n$ quantum bits to encode $q=p^n$ dimensional inputs, but often requires deep quantum circuits, where $p$ denotes the dimension of a single quantum bit. This implies an exponential increase in circuit depth with the input dimension $q$, demanding all quantum bits to be entangled and be well-connected physically [1].\n\nIn contrast, in our paper, all quantum bits in the quantum system are organized into $C$ columns each containing $k$ qubits. **It is only required that each subsystem consisting of $k$ quantum bits has the capability to generate entanglement, while the $C$ subsystems are independent of each other.** Additionally, we appropriately configure parameters to ensure $q=(p^k)^C$, meaning we in fact use a subspace of the entire Hilbert space.\n\nAs a result, the depth of the circuit for data loading is **independent of $q$ and only depends on $C$**, which is adjustable. It is possible to leverage circuit depth affordable by NISQ by restricting $C$ as a small value (e.g. 8 in our paper). We hold the view that this demonstrates the quantum-friendly in terms of data loading in our approach.\n\n[1] Plesch M, Brukner \u010c. Quantum-state preparation with universal gate decompositions[J]. Physical Review A, 2011, 83(3): 032302.\n\n---\n\n### **Q2: Is there any consideration for eliminating a global phase in the constraint $\\|\\mathbf{x}_i\\|$, which I assume would affect the embedding?** \n\nA2: Though intuitively constraining $\\|\\mathbf{x}_i\\|=1$ would erase part of information, **the erased information could be toxic for embedding learning**:\n\nAs proved by [3], with the skip-gram embedding loss, as the training goes on, the embedding model will converge at:\n\n$$\\lim\\_{t\\rightarrow \\infty} \\mathbf{x}\\_i^{(t)\\top} \\mathbf{x}\\_j^{(t)} = +\\infty, \\\\\\\\ \\lim\\_{t\\rightarrow \\infty} \\|\\mathbf{x}\\_i^{(t)} - \\mathbf{x}\\_j^{(t)}\\| = 0.$$\n\nwhere the nodes i and j are a positive node pair, and t denotes the training iteration. The results lead to:\n\n$$\\lim\\_{t\\rightarrow \\infty} \\|\\mathbf{x}\\_i^{(t)}\\|^2 + \\|\\mathbf{x}\\_i^{(t)}\\|^2 = \\lim_{t\\rightarrow \\infty} 2\\mathbf{x}\\_i^{(t)\\top} \\mathbf{x}\\_j^{(t)} +\\|\\mathbf{x}\\_i^{(t)} - \\mathbf{x}\\_j^{(t)}\\|^2 = +\\infty,$$\n\nwhich indicates that the modulus of some embeddings would go infinity. By constraining $\\|\\mathbf{x}\\_i\\|=1$, the above problem would fade automatically. Also, some works [2] show that the constraint empirically works.\n\n**Moreover, the most importantly, to make the node2ket program runnable, the constraint $\\|\\mathbf{x}\\_i\\|=1$ is empirially a must** as shown in the Table 11 (the row ''Remove constraint i)''), after removing which the program would crash.\n\n[2] Meng, Yu, et al. \"Spherical text embedding.\" Advances in neural information processing systems 32 (2019).\n\n[3] H. Xiong, J. Yan and Z. Huang, \"Learning Regularized Noise Contrastive Estimation for Robust Network Embedding,\" in IEEE Transactions on Knowledge and Data Engineering, vol. 35, no. 5, pp. 5017-5034, 1 May 2023, doi: 10.1109/TKDE.2022.3148284.\n\n---\n\n### **Q3: What embedding might be developed for more general quantum states, such as mixed states.**\n\nA3: As the quantum states go general, the difficulty of training them on classic machines will greatly increase. So far the scheme of representing embeddings by product state is the one that simultaneously can be run by classical machines with super efficiency and also can be well interpreted with quantum mechanics. A ''slightly'' entangled state might be more classical-friendly than a mixed state. \n\nThanks for your suggestions and we would consider trying that on a real quantum machine."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5870/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700512712475,
                "cdate": 1700512712475,
                "tmdate": 1700513567509,
                "mdate": 1700513567509,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GILmrAXHKl",
                "forum": "lROh08eK6n",
                "replyto": "pzPWozAHGn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5870/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5870/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Conti. page"
                    },
                    "comment": {
                        "value": "### **Q4: Definitions of ''positive node pairs and negative nodes''** \n\nA4: These are concepts from contrastive learning, which, given a batch of samples, aims to embed positive pairs of samples close but negative pairs of samples distant in the embedding space. In the embedding learning, such positive sample pairs are called ''positive node pairs'', and for a node in the node pair, we would sample some nodes (e.g. by random sampling from the whole networks) to form negative node pairs, and we call these nodes ''negative nodes''. \n\nThanks for your suggestion and we will make it more clear in the updated version.\n\n---\n\nFinally, hope you enjoy your Thanksgiving!"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5870/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700513233422,
                "cdate": 1700513233422,
                "tmdate": 1700513578047,
                "mdate": 1700513578047,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "m1Yyvorvk1",
                "forum": "lROh08eK6n",
                "replyto": "GILmrAXHKl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5870/Reviewer_8cxP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5870/Reviewer_8cxP"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the responses. I am certainly not an expert in this area, but I still feel the connection to quantum is lacking, and it doesn't make sense to call this quantum friendly (this is not something actually designed to be run on a quantum device). I'll keep the current score though, as this work seems interesting enough from the perspective of classical machine learning."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5870/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700675624393,
                "cdate": 1700675624393,
                "tmdate": 1700675624393,
                "mdate": 1700675624393,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3qVsrXwsbb",
            "forum": "lROh08eK6n",
            "replyto": "lROh08eK6n",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5870/Reviewer_RkWJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5870/Reviewer_RkWJ"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a groundbreaking paradigm for network embedding (NE), departing from traditional low-dimensional embeddings and exploring high-dimensional quantum state representations. The authors propose two NE methods, node2ket and node2ket+, and implement them in a flexible, efficient C++ library (LIBN2K). Experimental results showcase the good performance of their proposal, boasting advantages in parameter efficiency, running speed, and memory usage."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is well organized. The insights offered in the paper have the potential to inspire the development of other quantum-inspired methods and contribute to the broader application of quantum computing in the field of network embedding."
                },
                "weaknesses": {
                    "value": "The primary concern in this submission pertains to the technical contribution. First, the extension of the word2ket concept to product states appears relatively straightforward. Second, the utilization of product states might limit the embedding's expressivity since these states occupy a smaller portion of the Hilbert space and result in a low-dimensional representation."
                },
                "questions": {
                    "value": "No questions at the moment."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5870/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698745950197,
            "cdate": 1698745950197,
            "tmdate": 1699636621919,
            "mdate": 1699636621919,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "A5ytX7k101",
                "forum": "lROh08eK6n",
                "replyto": "3qVsrXwsbb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5870/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5870/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer RkWJ"
                    },
                    "comment": {
                        "value": "Many thanks for your comments and appreciation. Your suggestions are very valuable for the further works especially in the embedding theory.\n\n---\n\n### **Q1: The extension of the word2ket concept to product states appears relatively straightforward.** \n\nA1: It is true that node2ket and word2ket construct embeddings in a similar way. We believe that the embedding method approach created by word2ket is a great invention in the history of embedding research and would be a new paradigm for constructing embeddings in different fields. Compared to word2ket, our further contributions lie in the following aspects:\n\n- Node2ket is a successful trial of transferring word2ket from low-dimensional space to high-dimensional space, and from word embedding to node embedding (and possibly others with the toolkit LIBN2K).\n\n- We theoretically show the strong ability of the embedding construction as quantum states in preserving high-rank information.\n\n- By experiments, we show that representing nodes as product states is a more efficient and effective way to construct embeddings compared with the pseudo entangled states with the same amount of parameters.\n\n---\n\n### **Q2: The utilization of product state might limit model expressivity since these states occupy a smaller portion of the Hilbert space and result in a low-dimensional representation.** \n\nA2: i) ''The utilization of product state might limit model expressivity.'' It is true that, _**theoretically**_, product state (but not an entangled one) might limit model expressivity. However, so far, an efficient method to train embeddings as **strictly entangled states** has not been developed yet. We have compared the performance of product states (node2ket) and **pseudo entangled states** (w2k+n2k) in experiments, which shows that with the same number of parameters, embedding as product states is more efficient and effective compared with the pseudo entangled states.\n\nWe will leave it as future works to explore how to train the strictly entangled states as embeddings efficiently and how well it performs in the tasks.\n\nii) ''The product states occupy a smaller portion of the Hilbert space.'' We also consider this when studying the representation ability of the product quantum states. We want to know how much of the entire Hilbert space the product state occupies. However, it is a non-trivial mathematical problem to give an analytic result to answer the question. We only have an empirical conclusion: Although the product state does not occupy a significant portion of the entire Hilbert space, for any entangled state, there can always be a product state in its neighborhood that is very close to it. In fact, this question is very similar to asking how close the optimal solution of the rank-1 approximation by CP decomposition of an arbitrary tensor is to that tensor -- it is likely to be NP-hard.\n\nWe also sincerely hope this question can be well answered by future research.\n\n---\n\nFinally, hope you have a happy Thanksgiving!"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5870/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700512584829,
                "cdate": 1700512584829,
                "tmdate": 1700513716130,
                "mdate": 1700513716130,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "51yF7rYCbE",
                "forum": "lROh08eK6n",
                "replyto": "3qVsrXwsbb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5870/Reviewer_RkWJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5870/Reviewer_RkWJ"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your rebuttal addressing my concerns. I will keep my score at 6."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5870/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700655030958,
                "cdate": 1700655030958,
                "tmdate": 1700655030958,
                "mdate": 1700655030958,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "W7W6mBaWwa",
            "forum": "lROh08eK6n",
            "replyto": "lROh08eK6n",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5870/Reviewer_VRn5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5870/Reviewer_VRn5"
            ],
            "content": {
                "summary": {
                    "value": "For the standing and important task of network embedding in data mining and machine learning, the paper proposes explores the exponentially high embedding space for network embedding, which largely differs from existing works dewelling on the low-dimensional embedding. This is achieved by product quantum states in a super high-dimensional quantum Hilbert space. The experiments show surprisingly strong performance of the approach, in terms of both high memory and running efficiency with strong robustness across different tasks of network reconstruction, link prediction, and node classification. The authors also provide the source code to ensure the soundness of the experiments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1) This paper innovatively resorts to the high-dimensional embedding space for network embedding, which quite departures from existing literature.\n2) The paper is well presented and the overview plot in Fig. 1 is very informative and useful to readers. The paper is well organized with strong content in appendix that signifcantly enriches the paper.\n3) The experiments are impressive. Provided with the source code, I am convinced by the strong performance.\n4) The authors give strong theoretical understanding of the essence of their approaches, which I really appreciate."
                },
                "weaknesses": {
                    "value": "As the authors emphasized, the presented techniques are mainly suited for the structure networks, without considering the attributes. I understand this setting and think it is reasonable in practice. It also fits with many previous works in literature that have also been compred in this paper."
                },
                "questions": {
                    "value": "1) Comaperd with Fig. 1, can the authors provide a more succinct plot to convey the main idea of the paper? Fig. 1 is still a bit busy which is useful yet a more direct illustration in the begining of the paper is welcomed. Something like Fig. 2 is better.\n2) Can the approach be useful for solving combinatorial problems especially for large-scale ones? As there is little attributes need to be considered in these cases thus it seems suited to the proposed methods?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5870/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5870/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5870/Reviewer_VRn5"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5870/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698758956369,
            "cdate": 1698758956369,
            "tmdate": 1699636621812,
            "mdate": 1699636621812,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OknMLiJlAz",
                "forum": "lROh08eK6n",
                "replyto": "W7W6mBaWwa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5870/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5870/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer VRn5"
                    },
                    "comment": {
                        "value": "Many thanks for your comments and appreciation. I feel so surprised to see that some points of your comments are exactly what I am doing right now or plan to give it a try recently. I am glad to exchange the ideas (maybe still preliminary) with you:\n\n---\n\n### **Q1: Potentials of being applied on attributed networks.**\n\nA1: Though the proposed node2ket cannot be directly applied to attributed networks, it does provide some insights in GNN model design. The common part of GNN and Network Embedding (NE) is that both of them aim to learn node representations, while one through an embedding look-up table (NE) and the other through neural networks (GNN). A typical application of our method in GNN is on the graph attention -- where the attention is conducted with the following two steps (simplified for illustration, not strictly the same as the literature): \n\n1.compute the attention matrix $\\mathbf{A}\\in \\mathbb{R}^{N \\times N}$ by:\n\n$$\\mathbf{A}=\\mathbf{Q}\\mathbf{K}=(\\mathbf{W}\\_Q\\mathbf{X}\\_{in})^T (\\mathbf{W}\\_K\\mathbf{X}\\_{in}),$$\n\nwhere the $\\mathbf{Q},\\mathbf{K}\\in \\mathbb{R}^{N\\times d\\_{hidden}}$ are named as the 'query' and the 'key' matrix respectively, $\\mathbf{X}\\_{in} \\in \\mathbb{R}^{d\\_{in} \\times N}$ is the input of the attention layer, and $\\mathbf{W}\\_Q, \\mathbf{W}\\_K \\in \\mathbb{R}^{d\\_{hidden}\\times d\\_{in}}$ are the two weight matrices. \n\n2.compute the output by:\n\n$$\\mathbf{X}\\_{out} = \\mathbf{V} \\textit{softmax} (\\mathbf{A}) = (\\mathbf{W}\\_V \\mathbf{X}\\_{in})\\textit{softmax} (\\mathbf{A}),$$\n\nwhere the weight matrix $\\mathbf{W}\\_V\\in\\mathbb{R}^{d\\_{out}\\times d\\_{in}}$ and the output $\\mathbf{X}\\_{out} \\in \\mathbb{R}^{d\\_{out} \\times N}$.\n\n\nWe can find that the attention construction is exactly the same as the type-II Information Matrix Factorization (Eq. 5 in the paper) which is defined as follows:\n\n$$\\mathbf{A}(G) \\approx \\mathbf{X}^T\\mathbf{H},$$\n\nwhere $\\mathbf{A}(G)$ is the latent information matrix, $\\mathbf{X}, \\mathbf{H} \\in \\mathbb{R}^{d\\times N}$ are node embeddings and hidden embeddings respectively.\n\n\n**It is obvious that the attention $\\mathbf{A}$ is a low-rank matrix whose rank $\\textit{rank}(\\mathbf{A}) \\leq d_{hidden}$ just as the constructed latent information matrix $\\textit{rank}(\\mathbf{A}(G))\\leq d$.** Though the low-rank bottleneck of attention has been pointed out in literature [1], and solutions to obtain high-rank attention also have been proposed in previous works e.g. [1,2], constructing the attention by representing the matrices $\\mathbf{Q},\\mathbf{K}$ in the way of embeddings by node2ket, i.e. representing $\\mathbf{Q},\\mathbf{K}$ by a Katri-Rao product (illustrated as the Fig. 2b in the paper):\n\n$$\\mathbf{Q} = (\\mathbf{W}_Q^{(1)}\\mathbf{X}\\_{in}) \\circ (\\mathbf{W}\\_Q^{(2)}\\mathbf{X}\\_{in}) \\circ \\cdots \\circ (\\mathbf{W}\\_Q^{(C)}\\mathbf{X}\\_{in})$$\n\nstill has two strong advantages: i) **no extra parameters** are introduced, and ii) under certain conditions (Theorem 1 in the paper) the attention can be **full-rank**.\n\n\n[1] Bhojanapalli S, Yun C, Rawat A S, et al. Low-rank bottleneck in multi-head attention models[C]//International conference on machine learning. PMLR, 2020: 864-873.\n\n[2] Zhang Z, Shao N, Gao C, et al. Mixhead: Breaking the low-rank bottleneck in multi-head attention language models[J]. Knowledge-Based Systems, 2022, 240: 108075.\n\n---\n\n### **Q2: Potentials in solving CO problems.** \n\nA2: Indeed solving CO problems with the network embedding technique is what I am trying to do right now. For example, in solving the very large traveling salesman problem, especially in a non-Euclidean space (which means the distances are not derived from Euclidean coordinates), even the strongest heuristic LKH will face the challenge of a prohibitively large searching space. I believe that NE which has a long history in dimension reduction [3] will play a strong role in shrinking the search space. \n\n[3] Yan S, Xu D, Zhang B, et al. Graph embedding and extensions: A general framework for dimensionality reduction[J]. IEEE transactions on pattern analysis and machine intelligence, 2006, 29(1): 40-51.\n\n---\n\n### **Q3: Fig. 1 is still a bit busy.**\n\nA3: Thanks for the suggestion, we will modify the paper accordingly.\n\n\n---\n\nIn the last, I sincerely wish you a happy Thanksgiving!"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5870/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700512481026,
                "cdate": 1700512481026,
                "tmdate": 1700513827097,
                "mdate": 1700513827097,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]