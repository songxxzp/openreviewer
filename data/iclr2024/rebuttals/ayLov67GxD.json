[
    {
        "title": "Video2Demo: Grounding Videos in State-Action Demonstrations"
    },
    {
        "review": {
            "id": "br4F5EliOC",
            "forum": "ayLov67GxD",
            "replyto": "ayLov67GxD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8346/Reviewer_RP75"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8346/Reviewer_RP75"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes to use GPT-4 to interactively query a VLM to construct temporally coherent state-action sequences. Then it uses a prior method, Demo2Code, to generate robot task code that faithfully imitates the demonstration. Experiments on EPIC-Kitchens show it outperforms prior VLM-based approaches."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The paper proposes an effective way to convert human video demonstrations to state-action sequences, which are useful for generating executable robot policies.\n* The paper conducts extensive experiments on EPIC-Kitchens.\n* The paper is well-written and easy to follow."
                },
                "weaknesses": {
                    "value": "* **Other LLMs and VLMs**: How do other LLMs and VLMs perform on this task? I am curious to see how this framework is generalized to other models.\n* **GPT4-V**: It would be good to include some results of GPT-4V. I know its API is not released yet, but some quick experiments through ChatGPT's UI are sufficient.\n* **Execution-based evaluation**: I wonder whether you can provide some execution-based results of robot code to prove the generated state-action sequences are really useful.\n* **Prior works**: It would be good to discuss the paper's relationship to some additional prior works:\n\n[1] ProgPrompt: Generating Situated Robot Task Plans using Large Language Models. Singh et al.\n\n[2] VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models. Huang et al.\n\n[3] Voyager: An Open-Ended Embodied Agent with Large Language Models. Wang et al."
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8346/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698594268327,
            "cdate": 1698594268327,
            "tmdate": 1699637037910,
            "mdate": 1699637037910,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jpl0EyDKHC",
                "forum": "ayLov67GxD",
                "replyto": "br4F5EliOC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8346/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8346/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their feedback, the comparison with newer vision model APIs and appreciate the questions raised about the the evaluation of our pipeline\n\n## **Questions and Weaknesses**\n### **Clarification on Prior Work:**\nWe thank the reviewer for their suggestion to consider ProgPrompt[5], Voyager[6] and Voxposer[7] as related works. In our next revision, we will duly acknowledge them.\n\nWe wish to emphasize that `Video2Demo` serves a different purpose from the mentioned works.\n\n1. ProgPrompt is an LLM-based task planner translating language instructions to code\n2. Voxposer defines affordances and rewards through code for motion planning\n3. Voyager builds a skill library for playing Minecraft using LLMs. \n\n`Video2Demo` however, is not a planner. We focus on extracting state-action predicates through vision-language demonstrations. We use Demo2Code[8] as an example of how `Video2Demo`\u2019s output can be used in downstream task planners like [5-7]. We aim to explore this in future work with more embodied planners.\n \n\n### **Other VLMs and LLMs:**\nOur choice for the generative LLM was straightforward: there are currently no other LLMs that match the context capacity, instruction following and logical coherence capabilities that GPT-4 has [1]\n\nOur generative VLM choice was based on recent literature surveys, underlying simplicity of the architecture, and the choice of pre-trained models in LLaVA [2,3]. Due to time and cost limitations, we could not ablate the choice of the VLMs. However, the method is plug and play for both the choice of LLM and VLM, similar to the Socratic Model approach.\n\nFinally, OpenAI has recently enabled GPT4-vision as part of their API[4]. For future work, we are interested in evaluating this in our setting as:\n1. An alternative generative VLM in the `Video2Demo` pipeline, where we can expect better visual grounding compared to LLaVA.\n2. A competing method to `Video2Demo`\n\n### **On execution-based evaluation** \nIn the global response, we elaborate on how we primarily focus on the problem of extracting rich information from offline vision-language demonstrations. In future work, we plan to evaluate `Video2Demo`\u2019s generated state-action with task and motion planners [5,11] in simulators [9-10].\n\n\n\n### References - \n\n[1] OpenAI, R. \"GPT-4 technical report.\" arXiv (2023): 2303-08774.\n\n[2] Liu, Haotian, et al. \"Visual instruction tuning.\" arXiv preprint arXiv:2304.08485 (2023).\n\n[3] Touvron, Hugo, et al. \"Llama 2: Open foundation and fine-tuned chat models.\" arXiv preprint arXiv:2307.09288 (2023).\n\n[4] OpenAI: New models and developer products announced at DevDay. https://openai.com/blog/new-models-and-developer-products-announced-at-devday, 2023\n\n[5] Singh, Ishika, et al. \"Progprompt: Generating situated robot task plans using large language models.\" 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2023.\n\n[6] Huang, Wenlong, et al. \"Voxposer: Composable 3d value maps for robotic manipulation with language models.\" arXiv preprint arXiv:2307.05973 (2023).\n\n[7] Wang, Guanzhi, et al. \"Voyager: An open-ended embodied agent with large language models.\" arXiv preprint arXiv:2305.16291 (2023).\n\n[8] Wang, Huaxiaoyue, et al. \"Demo2Code: From Summarizing Demonstrations to Synthesizing Code via Extended Chain-of-Thought.\" arXiv preprint arXiv:2305.16744 (2023).\n\n[9] Eric Kolve et al. AI2-THOR: An Interactive 3D Environment for Visual AI. arXiv:1712.05474, 2022\n\n[10] Todorov, Emanuel, Tom Erez, and Yuval Tassa. \"Mujoco: A physics engine for model-based control.\" 2012 IEEE/RSJ international conference on intelligent robots and systems. IEEE, 2012.\n\n[11] Yu, Wenhao, et al. \"Language to Rewards for Robotic Skill Synthesis.\" arXiv:2306.08647, 2023"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8346/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700169927319,
                "cdate": 1700169927319,
                "tmdate": 1700169927319,
                "mdate": 1700169927319,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fKwFPYYY9f",
                "forum": "ayLov67GxD",
                "replyto": "tNZ0e2AItU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8346/Reviewer_RP75"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8346/Reviewer_RP75"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks a lot for the rebuttal! I am still concerned with the lack of execution-based evaluation, which is also a universal concern among all the reviewers. I really hope to see systematic experiments in either a simulator or a real robot."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8346/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700587388343,
                "cdate": 1700587388343,
                "tmdate": 1700587388343,
                "mdate": 1700587388343,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "J6DvR3I6JD",
            "forum": "ayLov67GxD",
            "replyto": "ayLov67GxD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8346/Reviewer_51er"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8346/Reviewer_51er"
            ],
            "content": {
                "summary": {
                    "value": "The paper \"Video2Demo\" addresses the challenge of teaching robots everyday tasks through a novel approach using a combination of GPT-4 and Vision-Language Models (VLMs) like LLaVA. The approach consists of 2 phases. First, GPT-4 is prompted to  interact with the VLM to create temporally coherent state-action sequences from descriptive VLM captions. Moreover, GPT-4's capacity to follow up with the VLM for additional information or clarification further enhances the quality of the obtained responses. Second,  GPT-4 is prompted to generate robot task code that imitates the demonstrated actions. The approach is evaluated on the EPIC-Kitchens dataset, outperforming other methods. Key contributions include a new framework for transforming vision-language demonstrations into state-action sequences, annotated data for benchmarking, and superior performance in both state-action and code generation tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Originality:  Video task description requires a combination of object identification, contextual analysis over time, and the application of common knowledge and reasoning to provide a comprehensive and coherent account of the video's content and events.\nThe paper introduces a useful system design with prompt engineering for interactive dialog between Vision-Language Models (VLMs) and Language Models (LLMs), providing a fresh perspective on task planning based on video data.\n\nQuality:  The paper frames the problem of decoding what is happening in a video in the form of iterative dialog between VLM (like LLaVA) to answer queries about a frame and LLM (like GPT-4) for asking questions, and  deciding state-action predicates. \nThe research is validated on real-world data (EPIC-Kitchens), and outperforms baselines in both state-action and code generation.\n\nClarity: The paper is well-structured and accessible, making it easy for a wide audience to understand. The key research questions and the failure cases are well discussed. The paper brings the problem of spatial grounding and hallucinations in LLMs and VLMs to the community's attention. This is reflected in the lower accuracy in symbolic state recall and action prediction. \n\nSignificance: The paper addresses a significant challenge in robotics and AI, with potential applications in various domains, and introduces a valuable approach for interactive AI systems."
                },
                "weaknesses": {
                    "value": "1. The paper motivates the problem of \"teach robots everyday tasks\". But there are no simulated or real robots experiments which makes it hard to assess the practicality of proposed approach and the possible failure scenarios. For example, how would the generated task plan compare to execute task in simulated environments like ALFRED [1].  The scope and possible future implication can be clear, like the proposed solution seems well suited for video comprehension, that can facilitate task planning.  \n1. One of the reasons why the proposed approach may be unsuitable for robot is the possibility of compounding error over interactive dialog and the corresponding latency. \n1. Video2Demo relies heavily on prompt engineering, which requires considerable effort. It is unclear if the presented prompts are applicable to just EPIC kitchen videos only, or can be applied more broadly to other activity videos."
                },
                "questions": {
                    "value": "1. Can it scale to beyond egocentric videos in EPIC kitchen to third-person tutorial videos? How would the prompt change, especially in the phase 2 where the prompts and state-action predicates seems to be centered on the human in the videos?\n1. How does ChrF compare to other code generation metrics [1]? Does the generated code with high ChrF score correlate with human preference? How much of the generated code follow required syntax and physical feasibility to execute successfully on a simulator?  \n[1] Zhou et al, 2023. CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code. https://arxiv.org/abs/2208.03133"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8346/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698813241294,
            "cdate": 1698813241294,
            "tmdate": 1699637037803,
            "mdate": 1699637037803,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LCWjWLVS5m",
                "forum": "ayLov67GxD",
                "replyto": "J6DvR3I6JD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8346/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8346/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for a thorough review of our paper, enumerating its strongest points, asking us clarifying questions and providing new ways to test and evaluate our approach.\n\n## **Questions:**\n\n**Q1: Can it scale to beyond egocentric videos in EPIC kitchen to third-person tutorial videos? How would the prompt change, especially in the phase 2 where the prompts and state-action predicates seems to be centered on the human in the videos?**\n\n\nWhile we did not evaluate our system on third-person videos in this paper, `Video2Demo` can easily be applied to generate state and action predicates for third-person tutorial videos.\n\n`Video2Demo`'s VLM and LLM prompts are mostly domain-independent. To gain better performance in new environments, one only needs to modify the following:\n\n* Domain information in the LLM's prompt, which could include\n    + Overall setting or theme of the vision language demonstrations (e.g. house chores)\n    + Examples of the state predicates\n    + Examples of the action predicates\n    + Domain-specific rules (e.g. we forbid the LLM from directly referring to the human in its state and action predicates). \n* Domain information in the VLM\n    + Whether the images are first-person view or third-person view\n\n**Q2: How does ChrF compare to other code generation metrics?**\n\nWe thank the reviewer for suggesting CodeBERTscore as another code evaluation metric. We will integrate this in the final version of the paper and strengthen our evaluation of the generated code.\n\n**Does the generated code with high ChrF score correlate with human preference?**\n\nOur qualitative testing showed a correlation between generalized code with loops and relevant conditions, and a high ChrF score\n\n**How much of the generated code follow required syntax and physical feasibility to execute successfully on a simulator?**\n\nAs we provide in the paper, the results from states and actions generated by baselines and `Video2Demo` imply that the demonstrations provided by our perception system will be noisy. However, we qualitatively show that the code generated is close to expert-generated code. In addition, we note that:\n1. Demo2Code is robust to these noisy states and actions, and manages to capture the user\u2019s preference\n2. The code generated is valid Python code without any undefined functions\nAs also mentioned in our global response, in future work, we will test the generated code for validity and feasibility in simulators like AI2-Thor [2] and VirtualHome [3]. \n\n## **Weaknesses:**\n\n### **On more Evaluations**\nIn our global response, we expand more on closing the loop with `Video2Demo` and evaluate using simulators [1-3]. We thank the reviewer for suggesting the ALFRED [1] benchmark to test our method. Since they also use high-level actions and egocentric vision, it provides the right testing ground. Our aim with this paper is to motivate the need for perception and generation of state-action sequences from task videos, and in future work, we hope to evaluate `Video2Demo` on the simulator.\n\n### **On the reliance on prompt engineering**\n\nMost section of `Video2Demo`'s prompt is domain-independent, so it can be applied to other activity videos. For domains significantly different EPIC-Kitchens, the only parts of the prompts that need to be changed are about the domain. Please refer to our answer to your 1st question for details. \n\n\n### References -\n[1] Mohit Shridhar, et al. ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks. arXiv:1912.01734, 2020\n\n[2] Eric Kolve et al. AI2-THOR: An Interactive 3D Environment for Visual AI. arXiv:1712.05474, 202\n\n[3] Xavier Puig et al. VirtualHome: Simulating Household Activities via Programs. arXiv:1806.07011, 2018\n\n[4] Zhou et al, 2023. CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code. https://arxiv.org/abs/2208.03133"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8346/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700170364758,
                "cdate": 1700170364758,
                "tmdate": 1700170490532,
                "mdate": 1700170490532,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "D5ZJqC4hmL",
                "forum": "ayLov67GxD",
                "replyto": "pna7VXt1cN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8346/Reviewer_51er"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8346/Reviewer_51er"
                ],
                "content": {
                    "title": {
                        "value": "Thanking authors for clarifications!"
                    },
                    "comment": {
                        "value": "I would thank the authors for evaluating the baselines with other metrics and clarifications for all the reviewers' concerns. While the paper explores an interesting question of making use of LLM and VLM for video understanding, the proposed solution needs improvement in terms of scalability across different domains, and evaluation on sim/real robot setups. Even for offline video processing, the proposed solution seems to create one state/action predicate based on action segments labelled in EPIC-kitchen (response to reviewer 8paA) and it is unclear how an image will be selected as a state at test time. \nSo, I would maintain my current score."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8346/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700692292703,
                "cdate": 1700692292703,
                "tmdate": 1700692292703,
                "mdate": 1700692292703,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9QIAn6xaUC",
            "forum": "ayLov67GxD",
            "replyto": "ayLov67GxD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8346/Reviewer_qcN7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8346/Reviewer_qcN7"
            ],
            "content": {
                "summary": {
                    "value": "The paper aims to propose a model that can extract temporal-consistent (text) state and (text) action pairs from videos. In order to do so, the paper proposes the let VLM and LLM talk with each other to extract descriptions. Since LLM can see past communications and can be prompted to take VLM output critically, the extracted description can consistently track objects. To evaluate the method, the authors provides a human annotated state-action predicates for EPIC-Kitchens. Finally, the authors show that LLM can use such extraction as demonstrations to prompt LLM to synthesize robot code."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is well written and is easy to understand.\n\n- The paper not only shows the perception power but also demonstrates downstream applications like code synthesis and planning.\n\n- The paper contributes a small human-annotated validation set for EPIC-Kitchens, a nice contribution to the community who hopes to do similar work.\n\n- All design choices are logical and sounds."
                },
                "weaknesses": {
                    "value": "- My main criticism for this paper is its contribution's significance. Using LLM and VLM with text as history seems like an obvious design choice. The techniques the authors introduced over the Socratic Model, namely the way to structure and prompt the LLM / VLM interaction doesn't seem to constitute enough contribution to be an ICLR paper.\n\n- The evaluation itself relies on GPT, which is a bit weak despite the human annotation the authors provide. If the authors had proposed a structured output format like those used in VQA and have more annotations the evaluation would be much stronger. \n\n- There are also a few misleading claims. Throughout the paper, the authors talks about constructing \"state-action\" pairs, while in reality what they extract are some loose-form text predicates as well as loose-form text actions. Such abusive use of terms misleads the readers when they read the abstract.\n\nOverall, I think the current status of the paper lacks the significance an ICLR paper would need."
                },
                "questions": {
                    "value": "1. How big is the \"Human-annotated state and action predicates\" in claimed contribution 2? I think this is a nice contribution but from what I read in the paper, this doesn't seem to be very big. Could you clarify?\n\n2. Could you clearly define \"structure and temporal consistency\" in the abstract?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8346/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8346/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8346/Reviewer_qcN7"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8346/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698828688185,
            "cdate": 1698828688185,
            "tmdate": 1699637037661,
            "mdate": 1699637037661,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Px2pfEbtCB",
                "forum": "ayLov67GxD",
                "replyto": "9QIAn6xaUC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8346/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8346/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their comprehensive review commending the strengths of the paper as well as constructive criticism about the contributions.\n\n### **Questions**\n\n**Q1: How big is the \"Human-annotated state and action predicates\" in claimed contribution 2? I think this is a nice contribution but from what I read in the paper, this doesn't seem to be very big. Could you clarify?**\n\nWe annotate 20 videos from the EPIC-Kitchens validation dataset, where each video is 5 minutes in length on average\n* an average of 3 predicates per frame\n* 1 action per frame, which is already provided by the dataset.  \n* 1027 image frames in total \n\nWe also partially annotate 5 videos to only focus on the dish-washing segment of the videos. \nAll annotations are double-checked by a second person to ensure quality.\n\nWe estimated that the time required for manual labeling per video is 2 hours, resulting in a total estimated annotation time of 50 hours. Thus, we built a user-friendly web-based annotation tool that allows users to cross-reference frames and autocomplete predicates based on past annotation and reduces annotation time.\n\n**Q2: Could you clearly define \"structure and temporal consistency\" in the abstract?**\n\nThe term structure and temporal consistency refers to the importance of understanding past actions and the relevance of objects in past timesteps (i.e. state of the environment) to predict state-actions for the current timestep. Referring to past state-actions to infer the current state and action is evident during video annotations as human annotators as well. For example, if a cook is seen holding a pan in their hand at some time t:\n* If the pan was brought over to the stove-top from elsewhere, it is likely to be put down in the next few frames\n* If the pan was not in hand earlier, it was picked up right before time t.\n* The pan, as part of the environment, may stay relevant in future actions in the demonstration.\n\nIf we use a VLM to generate captions without context, it may fail to recognize what is going on in the scene. However, responses from VLMs along with GPT-4\u2019s ability to reason what states precede actions and what actions follow other actions enhances the consistency in actions and the state of the observed environment.\n\n## **Weaknesses**\n\n### **On the contribution in the paper**\n\nThe contribution of the paper is not just in the method of LLM/VLM interaction, but also in solving the problem of extracting symbolic states and actions from long vision language demonstration. The baseline result shows that\n* The captions that VLM generates are not sufficient in extracting salient states about objects in the frame. (see SM-Caption)\n* Despite including history, a predefined, fixed interaction between the LLM and the VLM is also not sufficient in accurately extracting salient information because the LLM has no way to query VLM about inconsistencies in its answer. (see SM-Fixed-Q) \n\nNaive application of LLM and VLM cannot effectively solve this problem. `Video2Demo`, despite its simplicity, is able to significantly outperform the baselines by extracting symbolic states and actions with higher quality.\n\n### **Clarification on Evaluation**\n\nWe use GPT-3.5 to evaluate our method\u2019s generated state and action predicates as they come from:\n1. open-vocabulary based predicate generation and\n2. expert annotations from different humans\nmeaning that it's possible two predicates match semantically without matching exactly in text string. Since it is taxing to enumerate all possible states in a complex environment like EPIC-Kitchens, we allow GPT-4 to come up with its own predicates. We argue that in this setting a soft match in the semantic space is fair to evaluate the model. In addition to this, a natural-language planner should be able to generate similar plans for semantically similar predicates.\n \nIn order to make the evaluation more robust, we ask GPT-3.5 to give reasoning before giving an answer (of whether two predicates are \u201cequal\u201d in semantic space)\n\nText evaluations are often done using other AI models, especially more recently with reliable models like GPT3.5 or GPT4. [1,2]\n\n### **Clarification on Outputs**\nIn the global response under \u201cRepresentation of States and Actions\u201d, we clarify the choice of how we represent states and actions. While they may be text-based, they are structured in predicate format, and a majority of the most commonly used state predicates and actions are provided to GPT-4 as part of its prompt. Examples of predicates:\n\n* state: `is_inside(<A>, <B>)`, `human_is_holding(<A>)`, `is_on_top_of(<A>, <B>)`, etc.\n* action: `pick_up(<A>)`, `open(<A>)`, `turn_off(<A>)`, etc.\n\nSince it is taxing to design a comprehensive pddl-like specification for every possible predicate in a complex setting like EPIC-Kitchens, we allow the model to come up with its own predicates similar to the examples. This can be restricted in the prompt if `Video2Demo` is to be used in a simpler enumerable setting."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8346/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700173167848,
                "cdate": 1700173167848,
                "tmdate": 1700173167848,
                "mdate": 1700173167848,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wBCnAuIwXC",
                "forum": "ayLov67GxD",
                "replyto": "9QIAn6xaUC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8346/Reviewer_qcN7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8346/Reviewer_qcN7"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the clarifications.\n\nI just went through the response written by peer reviewers, as well as the author's response. Unfortunately, I don't think the author's rebuttal would address my concerns, without heavy modifications to the project. I reiterate the need for stronger technical contribution for this paper to be accepted at any top ML conference."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8346/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700718985209,
                "cdate": 1700718985209,
                "tmdate": 1700719014683,
                "mdate": 1700719014683,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LhSoyIckI3",
            "forum": "ayLov67GxD",
            "replyto": "ayLov67GxD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8346/Reviewer_8paA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8346/Reviewer_8paA"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose using a video language model and a large language model in tandem to label videos from the Epic-Kitchens dataset with descriptions of the various subtasks demonstrated in the video in pseudo-code. They evaluate the ability of their method on this task using a handful of videos with hand-labeled pseudo-code descriptions."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "**Well-written**: The paper is clear with good presentation, sound descriptions of the idea and clear experiments. The authors clearly state their setups."
                },
                "weaknesses": {
                    "value": "Unfortunately, I am not fully convinced of the motivation behind this work. I enlist the weaknesses of this work below:\n1. **Not enough evaluation**: it's very difficult to get a signal for the method's abilities given that evaluations are performed on contrived code for 7 videos.\n2. **Choice of action space perhaps makes this task too easy**: Generating code is generally useful but in this setup, it's difficult to apply to real-life scenarios due to the level of abstraction. For instance, the reference code uses functions like \u201ccheck_if_dirty(object)\u201d, a level of abstraction for which we do not have good robot behaviors. In a sense, the method performs a kind of task-level planning. But the level of abstraction of this planning makes it impossible to test in a control setup. \n3. **Unclear motivation**: The authors claim that this work tries to ground videos into state actions and states for robot demos. Unfortunately, this is simply not true. They describe the state in videos using text and ground actions into pseudo-code. This far from the promise of a state-action demonstration. \n4. **No robotic evaluations**: The paper does not run any experiments on robots - neither in simulation nor on real robots. Therefore, I believe calling this a method to generate state-action demonstrations is an overclaim.\n5. **Extremely expensive to deploy on a robot**: The method requires making several calls per time step of execution to GPT-4 making this very very expensive."
                },
                "questions": {
                    "value": "1. What was the cost of running these experiments?\n2. Do you have one state for every image?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8346/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698959532831,
            "cdate": 1698959532831,
            "tmdate": 1699637037536,
            "mdate": 1699637037536,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lL20ZTDzFj",
                "forum": "ayLov67GxD",
                "replyto": "LhSoyIckI3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8346/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8346/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their constructive criticism, and we acknowledge their concerns about our paper\u2019s motivation, evaluation, and practical deployment.\n\n## **Questions:**\n\n**Q1: What was the cost of running these experiments?**\n\nThe costs for these experiments are mainly the cost of OpenAI\u2019s GPT-4 API usage.\n\n* The total cost was approximately $2200.\n* Each demonstration costs about $15. \n\nWe ran approximately 6 experiments on each of the 26 videos, encompassing several trials with the baselines, `Video2Demo`, ablations, and qualitative experiments for code generation. \n\nLower costs for GPT-4 were recently announced by OpenAI [6], and we can accordingly expect the cost of this method to be reduced in the future. \n\n\n**Q2:Do you have one state for every image?**\n\nWe do not generate states for every frame in the video because two consecutive frames are highly similar. Instead, `Video2Demo` generates a set of state predicates and one action for each action segment specified in the EPIC-Kitchens data. \n\n\n## **Weaknesses**\n\n### **On insufficient evaluation**\n\nIn quantitative evaluation, we focus on the quality of the generated state and action predicates. In Table 1 and Figure 3 in our paper, we evaluated the approaches on 19 videos that have human state-action annotation. We show how our generated predicates are closest to human annotations compared to other baselines across all kitchen activity categories. \n\nOur evaluation for code generation on 7 dishwashing videos is a qualitative example of a downstream application of `Video2Demo`. We show that [2], which takes text-based state-action trajectories as input, can generate expert-like code when it uses the state-actions generated by `Video2Demo`.\n\n### **Clarification on the choice of action space**\n\nIn global response, we clarify why we picked a high-level action space. We would like to add that despite the high-level resolution of this task, predicting these high-level actions is still not easy, as is evident from the accuracy and recall scores from the baseline VLM-based methods and even `Video2Demo`.\n\nWhile we acknowledge that code generated by task planners may not be feasible or affordable by the embodiment in that environment, we note that it is not the focus of our work. We aim to be able to generate states and actions that can increase the success rate for language-based planners compared to only providing language instructions. Qualitatively, we see that our demonstrations allow a code-generation pipeline based on [1-2] to generate expert-like code with loops and conditions that are non-trivial to infer from simple language instructions.\n\n\n### **Clarification on Motivation**\n\nIn the global response, we clarify our motivation behind this work and why we represent our state-action sequences in text-predicate form. \n\nOur method aims to ground the visual information available in hours of offline demonstration data such as EPIC-Kitchens by converting it into text-based states and actions. We hope that language-based task planners [2-5] can learn the high-level task plan from symbolic state-action trajectory\u2014bridging the gap between language instruction and planning, as seen in [1-2].\n### **On Robotic Evaluation**\n\nIn the global response, we elaborate on how the paper focuses on the problem of extracting rich information from offline vision-language demonstrations. In future work, we plan to evaluate downstream applications of `Video2Demo`\u2019s generated state-action in simulations (e.g. ALFRED, AI2-THOR [7-8]) and on real robots. \nOn Cost and Speed of Deployment\nWe acknowledge the importance of fast perception models during the deployment and online rollout of learned policies. However, **`Video2Demo`'s applications are for offline settings, so it does not need to be deployed on a robot.**\n \nConcretely, `Video2Demo` provides a framework for learning high-level task plans from few-shot demonstrations. We hope that with future work, we can:\n1. Collect one or more demonstrations of a human performing a task.\n2. Run `Video2Demo` to extract the high-level states and actions from these demonstrations\n3. Rely on task-planning and motion-planning methods to leverage these generated states and actions to replicate the tasks, e.g., generate code to perform this task, conditioned on the robot embodiment, environment affordances etc.\n\nThus, `Video2Demo` can run offline asynchronously before the policy is learned then run on the robot."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8346/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700171512324,
                "cdate": 1700171512324,
                "tmdate": 1700510894117,
                "mdate": 1700510894117,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]