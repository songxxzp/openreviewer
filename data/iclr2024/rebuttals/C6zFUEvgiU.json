[
    {
        "title": "Feedback-guided Data Synthesis for Imbalanced Classification"
    },
    {
        "review": {
            "id": "vLKHY6hgZa",
            "forum": "C6zFUEvgiU",
            "replyto": "C6zFUEvgiU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6233/Reviewer_Ajag"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6233/Reviewer_Ajag"
            ],
            "content": {
                "summary": {
                    "value": "This thesis utilises recent advances in generative modelling to address the shortcomings of synthetic data in representation learning and introduces feedback from downstream classifier models to guide the data generation process. To augment static datasets with useful synthetic samples, the research designs a framework that utilises pre-trained image generation models to provide useful and diverse synthetic samples that are close to the support of real data distributions to improve the representation learning task. This paper lays the groundwork for the effective use of state-of-the-art text-to-image models as data sources that can be queried to improve downstream applications."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Originality. The paper designs a diffusion model sampling strategy that uses the feedback of the pre-trained classifier to generate samples that help improve its own performance, which improves the classification performance to a certain extent. Has a certain degree of innovation.\n- Quality. The experimental design of the paper is reasonable, and the feasibility of the method is verified in ImageNet-LT and NICO++. \n- Clarity. The paper well-organized and clearly written. \n- Significance. The ideas proposed in this paper have certain contributions to this field."
                },
                "weaknesses": {
                    "value": "1. The font format of the article is not uniform. Do the words in italics want to express any special meaning? Make it difficult for readers to read.\n2. The charts are mixed up, for example, Figure 5. Is it a table or a graph? The sizes of some pictures also don\u2019t match.\n3. How about the time complexity of this method?\n4. Are there more evaluation metrics to evaluate the performance of the proposed method versus the baseline method?"
                },
                "questions": {
                    "value": "Please refer to the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6233/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6233/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6233/Reviewer_Ajag"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6233/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698463872449,
            "cdate": 1698463872449,
            "tmdate": 1699636681156,
            "mdate": 1699636681156,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gg84XNu5Yn",
                "forum": "C6zFUEvgiU",
                "replyto": "vLKHY6hgZa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6233/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6233/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer Ajag"
                    },
                    "comment": {
                        "value": "Thank you for your feedback on our paper. We note a mismatch between your positive evaluation and your recommendation. Below, we address all your concerns and believe that upon reviewing our responses, you might consider revising your score.\n\n\n>The font format of the article is not uniform. Do the words in italics want to express any special meaning? Make it difficult for readers to read.\n\nWe have followed ICLR 2024's official style guidelines, including the use of italics for introducing either new concepts (e.g., \u201cusefulness\u201d) or emphasizing key points (e.g. \u201cclose to the support of the real data distribution\u201d). If any instances create readability issues please let us know, we will modify them. However, these formatting concerns could be easily rectified and do not reflect the quality of the research presented.\n\n\n> The charts are mixed up, for example, Figure 5. Is it a table or a graph? The sizes of some pictures also don\u2019t match.\n\nWe have now made the size of the tables consistent across the paper and modified Figure 5 (now Table 1) to include only the table and moved the figure to appendix. \n\n> How about the time complexity of this method?\n\nThank you for raising this question. We have now included an additional Section (G1) in the Appendix which compares the time-complexity of feedback guidance versus regular sampling. \n\n>Are there more evaluation metrics to evaluate the performance of the proposed method versus the baseline method?\n\nWe used standard metrics used to report results on ImageNet-LT and NICO++ [1, 2] such as average accuracy, many-med-few accuracy and worst group accuracy. In addition to the benchmark\u2019s metrics, in our submission we include standard  generative model metrics to evaluate the effect of our criteria on the quality of the generated images such as FID, coverage and density \u2013 see Table 2. Thus, we argue that our evaluation metrics are in-line with both representation learning and generative model literatures. If the reviewer has suggestions on which metrics they would like to see included in the paper, please let us know and we would be happy to include them.\n\n\u2014---------------\n\n[1] Ziwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang, Boqing Gong, and Stella X Yu. Largescale long-tailed recognition in an open world. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\n\n[2] Xingxuan Zhang, Yue He, Renzhe Xu, Han Yu, Zheyan Shen, and Peng Cui. Nico++: Towards better benchmarking for domain generalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16036\u201316047, 2023."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6233/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700685788422,
                "cdate": 1700685788422,
                "tmdate": 1700685788422,
                "mdate": 1700685788422,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "KWe7NH3Rop",
            "forum": "C6zFUEvgiU",
            "replyto": "C6zFUEvgiU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6233/Reviewer_tjGi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6233/Reviewer_tjGi"
            ],
            "content": {
                "summary": {
                    "value": "With the recent advances in generative models, researchers have started augmenting these static datasets with synthetic data, reporting moderate performance improvements on long-tailed classification tasks. \nThe authors hypothesize that these performance gains are limited by the lack of feedback from the classifier to the generative model, which would promote the usefulness of the generated samples to improve the classifier\u2019s performance.\nIn this work, the authors introduce a framework for augmenting static datasets with useful synthetic samples, which leverages one-shot feedback from the classifier to drive the sampling of the generative model. \nFor the framework to be effective, they find that the samples must be close to the support of the real data of the task at hand and be sufficiently diverse. \nThe authors validate three feedback criteria on a long-tailed dataset (ImageNet-LT) and a group-imbalanced dataset."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The problem definition to encourage the generated samples to be helpful to the classifier, inspired by active learning frameworks, is novel.\n2. The proposed method performs better than the previous sample synthesis-based imbalance classification methods."
                },
                "weaknesses": {
                    "value": "- The proposed solution for the problem definition is too na\u00efve. For active learning methods, in addition to the confidence-based or entropy-based approach, margin margin-based approach is also possible. For the recent active learning criteria, such as BALD [1], VAAL [2], or MCDAL [3]. To claim the contribution of a complete research paper, the authors should devise an idea to leverage such recent active learning methods to find more novel solutions suitable for this problem.\n[1] Deep Bayesian Active Learning with Image Data. ICML 2017.\n[2] Variational Adversarial Active Learning. ICCV 2019.\n[3] MCDAL: Maximum Classifier Discrepancy for Active Learning. TNNLS 2022.\n\n- Also, instead of simply comparing among na\u00efve active learning criteria, how about combining multiple losses (at least linear combination in the loss)? That would be more novel than the proposed solution.\n\n- The experiment is also too weak. For the datasets, The authors only use ImageNet and NICO++. However, according to other recent Long-tailed recognition papers, they usually evaluate their methods on iNaturalist and Place-LT datasets to demonstrate the scalability. At least the authors should have evaluated their method on CIFAR datasets to show the effectiveness of their methods on other datasets.\n\n- Also, a comparison with more recent state-of-the-art long-tailed recognition papers is missing. For example, CMO [4] is one of the recent long-tailed recognition methods based on sample synthesis. To claim the usefulness of the proposed method, the authors should compare the proposed method with recent long-tailed recognition papers, including [4].\n[4] The Majority Can Help The Minority: Context-rich Minority Oversampling for Long-Tailed Classification. CVPR 2022.\n\n- More analysis of the detailed design choices. For example, how are the hyper-parameters decided, such as w in Eqns (5), (6), (8)? As the authors proposed to add additional criteria, it would be necessary to analyze the effect of w on the performance."
                },
                "questions": {
                    "value": "Please refer to the questions in the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6233/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698777596825,
            "cdate": 1698777596825,
            "tmdate": 1699636681045,
            "mdate": 1699636681045,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dOkUaVITPt",
                "forum": "C6zFUEvgiU",
                "replyto": "KWe7NH3Rop",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6233/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6233/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer tjGi"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for their insightful review. We are glad to see that the reviewer finds our framework novel. We would like to highlight important differences between Active Learning (AL) and our method. The goal of AL is to interactively query a user (or a model) to label new data points from a given static dataset of unlabelled data points, thus the dataset is known but the labels are unknown. However, in our setup the labels are given and the dataset is unknown. The dataset has to be obtained by sampling from a parametric generative model \u2013 our framework describes how to class-conditionally sample from a generative model to obtain useful data points to train a classifier. We agree with the reviewer that our criteria bear some similarities with AL acquisition functions. However, we argue that our main contribution is the design of a queryable framework for generative models that \u201csmartly\u201d balances the training data for a classification model, and not the criteria itself. Note that we do not claim the criteria to be our contribution \u2013 see contributions list at the end of the introduction. Moreover, in our experiments we show that the criteria we use are effective enough to obtain state-of-the-art results on three challenging datasets (ImageNet-LT and NICO++ and Places-LT). Thus, we disagree with the main premise of the reviewer\u2019s critique that our simple and effective criteria weakens our main contributions. Contrarily, we see this as a proof of strength for our framework as it can reach the state-of-the-art with simple and broadly used criteria. Thus, we kindly ask the reviewer to reconsider their initial recommendation. Below, we provide detailed responses to the reviewer's questions.\n\n> .. using the recent active learning criteria, such as BALD [1], VAAL [2], or MCDAL [3]\n\nThank you for suggesting the relevant literature. We have included them in the future work section (6) in our paper. In general, any differentiable criterion which is a function of the classifier can be applied as a feedback guidance in the sampling process, as long as it is computationally efficient. Among the mentioned methods, in theory, both BALD and MCDAL could be applied as a feedback criterion. However, compared to the entropy, they are computationally 10x and 3x more expensive, respectively, limiting their application in practice. The suggested VAAL method is not applicable in our framework since it is task-agnostic and its acquisition function does not depend on the classifier of the task.\n\n> ...according to other recent Long-tailed recognition papers, they usually evaluate their methods on iNaturalist and Place-LT datasets to demonstrate the scalability. At least the authors should have evaluated their method on CIFAR datasets...\u201d\n\nThank you for your feedback regarding our experimental approach. Our paper focused on ImageNet-LT, which has a substantial number of classes (1000), and NICO++, featuring 360 groups. To further provide evidence for our methodology's effectiveness on large-scale datasets, we have applied our framework for the Places-LT[1] dataset. This is a highly imbalanced dataset where the smallest class only has 5 examples and the largest class has 4980 examples with 365 classes overall. We upsample this dataset such that every class has 4980 samples resulting in a dataset of size 1.8 million examples. We compare against the Fill-up [2] work which also uses 1.8 million synthetic samples and other baselines that do not use synthetic data. We observe that feedback guidance with entropy achieves SOTA results on this dataset. Below is the summary of our results and see full details in Section G.4 of the paper.\n\n\n| Method | # Syn. data | Overall | Many | Medium | Few |\n| --- | --- | --- | --- | --- | --- |\n| ERM | No | 30.2 | 45.7 | 27.3 | 8.2 |\n| Decouple-LWS | No | 37.6 | 40.6 | 39.1 | 28.6 |\n| Balanced Softmax | No | 38.6 | 42.0 | 39.3 | 30.5 |\n| ResLT | No | 39.8 | 39.8 | 43.6 | 31.4 |\n| MiSLAS | No | 40.4 | 39.6 | 43.3 | 36.1 |\n| PaCo | No | 41.2 | 37.5 | 47.2 | 33.9 |\n| Fill-Up | 1.8M | 42.6 | **45.7** | 43.7 | 35.1 |\n| LDM-FG (Entropy) | 1.8M | **42.8** | 41.7 | **44.9** | **40.0** |"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6233/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700687207534,
                "cdate": 1700687207534,
                "tmdate": 1700687207534,
                "mdate": 1700687207534,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FpvWDQzGb5",
            "forum": "C6zFUEvgiU",
            "replyto": "C6zFUEvgiU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6233/Reviewer_SFA5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6233/Reviewer_SFA5"
            ],
            "content": {
                "summary": {
                    "value": "The effectiveness of utilizing synthesized data is limited by the lack of feedback. This work proposes a framework to drive the sampling process of a generative model, thereby improving the usefulness of the generated samples."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "\u25cf the experimental results were stunning, achieving state-of-the-art on ImageNet-LT\n\u25cf the writing is clear and easy to follow\n\u25cf the experiment is comprehensive, comparing three types of feedback criteria"
                },
                "weaknesses": {
                    "value": "ImageNet-LT is essentially a pseudo long-tail dataset, where the tail classes may not necessarily be the minority in the actual data distribution. Therefore, generative models can sample relatively well. However, for real-world long-tail distributions, is it also difficult for generative models to obtain sufficiently good samples?"
                },
                "questions": {
                    "value": "ImageNet-LT is essentially a pseudo long-tail dataset, where the tail classes may not necessarily be the minority in the actual data distribution. Therefore, generative models can sample relatively well. However, for real-world long-tail distributions, is it also difficult for generative models to obtain sufficiently good samples?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6233/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6233/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6233/Reviewer_SFA5"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6233/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699076596481,
            "cdate": 1699076596481,
            "tmdate": 1699636680930,
            "mdate": 1699636680930,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1ygA6oXGfK",
                "forum": "C6zFUEvgiU",
                "replyto": "FpvWDQzGb5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6233/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6233/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer SFA5"
                    },
                    "comment": {
                        "value": "Thank you for your valuable feedback. We are glad that you found our experimental results \u201ccomprehensive/stunning\" and the writing \u201ceasy to follow\u201d. Below we clarify your question and hope that you consider increasing your score. \n\n> ImageNet-LT is essentially a pseudo long-tail dataset, where the tail classes may not necessarily be the minority in the actual data distribution. Therefore, generative models can sample relatively well. However, for real-world long-tail distributions, is it also difficult for generative models to obtain sufficiently good samples?\n\nThanks for raising this point. As discussed in the limitations paragraphs in the conclusion section of our initial submission: \u201c...our guidance mechanism can only explore the data manifold already captured by the generative model\u201d and this limitation is imposed by the generative models.\n\nYet, we would like to add that state-of-the-art generative models are capable of generating very unlikely data points under the real world distribution of images - e.g. they are able to generate the rare concept of \"an astronaut riding a horse\" by combining the common concepts of \u201castronaut\u201d and \u201chorse\u201d. This is mostly what we observed in our NICO++ experiments, where we saw that generative models can indeed generate new combinations of object and background which is useful to improve generalization.\n\nOur findings on Imagenet-LT, NICO++, and Places-LT (recently added - refer to new results in reviewer\u2019s tjGi answer) show that this inherent potential can be harnessed towards achieving tangible improvements in classification tasks."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6233/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700688056550,
                "cdate": 1700688056550,
                "tmdate": 1700688056550,
                "mdate": 1700688056550,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]