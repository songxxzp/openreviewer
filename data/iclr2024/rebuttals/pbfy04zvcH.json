[
    {
        "title": "OceanGPT:  A Large Language Model for Ocean  Science Tasks"
    },
    {
        "review": {
            "id": "V9AuiGVcj1",
            "forum": "pbfy04zvcH",
            "replyto": "pbfy04zvcH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission882/Reviewer_5Gh6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission882/Reviewer_5Gh6"
            ],
            "content": {
                "summary": {
                    "value": "This paper provides a fine-tuned version of LLaMa 2, OceanGPT, which is meant to be specifically used for ocean studies and related tasks. It was also finetuned by a series of GPT 3.5 derived agents, allowing it to carry out a variety of tasks, such as providing instructions for underwater robots and extracting relevant parts of literature."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- The authors aim to train a model specifically for an under-served field of study, that of oceanography\n- They create a benchmark for testing AI models capabilities on ocean-adjacent tasks\n- They create a corpus of open-access literature about ocean studies."
                },
                "weaknesses": {
                    "value": "- The 'instruction seed generation' approach described in section 3.2. is fundamentally flawed because it utilizes other LLMs to generate this data, meaning that it is liable to contain hallucinations and not be reliable from a scientific perspective\n- The evaluation carried out has several significant shortcomings (see my questions below) - notably that there is a lack of details regarding how it was carried out, or how reliable GPT4 is as an evaluator.\n- The insistence of the authors on \"embodied intelligence\" is not proven or described in detail, and the whole section about the instructions for underwater robots is unclear to me - how this testing was carried out, how it was evaluated, etc. (see questions below)"
                },
                "questions": {
                    "value": "- Do you check licenses of content that you use? \n- \"The seed instruction dataset comes from annotations by ocean science researchers\" - which researchers? what was the annotation procedure?\n- It's unclear to me: you use 'gpt-3.5-turbo' to enrich training samples? what about hallucination issues? (in general, all the agents that you use will have this issue, so for me all the data gathered during this step (described in Section 3.2.) is unverifiable and therefore cannot be trusted\n- You say that you \"leverage GPT-4 as the evaluator for our experimental results\" - that's an unacceptable form of evaluation, because GPT-4 is lacking the specific domain knowledge you seek to represent, and because it suffers from the hallucination issues that all LLMs suffer from.\n- When you compare models and say that \"the result shows that OCEANGPT demonstrates a higher level of knowledge expertise when describing the content of radioactive nuclide research.\", how do you measure this?\n- You state that \"the experimental result suggests that OCEANGPT has the potential to acquire embodied intelligence.\" - without providing any proof of formal evaluation of the model's ability to write instruction code for underwater robots. You need to develop a procedure for evaluating this before you can make such statements."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission882/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698424699083,
            "cdate": 1698424699083,
            "tmdate": 1699636014991,
            "mdate": 1699636014991,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oZ48SS7kj9",
                "forum": "pbfy04zvcH",
                "replyto": "V9AuiGVcj1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission882/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission882/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely thank you for your insightful feedback. Below are our detailed responses to your concerns:\n\n**Q1: Hallucinations and not be reliable from a scientific perspective**\n\nThanks for your comment. It is a very important issue. In fact, the issue of hallucinations in large scientific models cannot be completely avoided. In our paper, we use LLMs to generate data in order to address the issue of difficult access to ocean-specific data. Meanwhile, we also use a fine-tuned agent to automate the extraction of knowledge from text, specifically to mitigate such hallucinations. In future research, we will continue to focus on this issue.\n\n**Q2: Licenses of content that you use**\n\nDuring the process of corpus collection, we only use literature that was accessible and meets the requirements.\n\n**Q3: Collecting annotation of the seed dataset**\n\nFor the construction of the seed dataset, we employ annotators with rich backgrounds in marine science to assist in building the dataset. Each annotator is responsible for several topics, and they first manually write some high-quality data. Then, we use LLMs to mimic this existing data to generate a large number of similar samples. All these samples are ultimately manually screened and modified by the annotators\n\n**Q4:  The use of evaluator in our work**\n\nThanks for your suggestions. We use GPT-4 primarily because it is well-suited for comparing the quality of content generated by two language models. However, to avoid the drawbacks (such as hallucination or lack of domain knowledge) of automated evaluation, we also employed manual assessment methods.\n\n**Q5: The measure for the content of radioactive nuclide research**\n\nIn fact, when analyzing the content generated by language models, we consider three aspects: quality, expertise, and diversity (as shown in Figure 7). Additionally, in Figure 8, we have marked the expertise demonstrated by OceanGPT in blue.\n\n**Q6: The embodied intelligence for OceanGPT**\n\nThanks for your comment. Regarding our model for controlling robots, in the paper we have provided proof (in the Gazebo platform) that it is capable of interacting with underwater robots. We will continue to focus on this issue and explore the potential of OceanGPT."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission882/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700591405244,
                "cdate": 1700591405244,
                "tmdate": 1700591405244,
                "mdate": 1700591405244,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "u7238FenGE",
            "forum": "pbfy04zvcH",
            "replyto": "pbfy04zvcH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission882/Reviewer_bjnA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission882/Reviewer_bjnA"
            ],
            "content": {
                "summary": {
                    "value": "This paper builds a large language model (LLM) for ocean science tasks, namely OceanGPT, which is the first attempt of concerning LLM with ocean science. OceanGPT is firstly pre-trained on the collected open-access literature of ocean corpus based on the LLaMA-2 model,  then fine-tuned on the instruction data generated by the proposed DoInstruct framework based on multi-agent collaboration and five specific ocean topics, and lastly evaluated on the constructed ocean-specific benchmark OceanBench."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- It is the first attempt of building a large language model (LLM) for ocean science, which is helpful for the related study.\n- The proposed DoInstruct seems flexible for the LLM model on other fields to generate instruction data."
                },
                "weaknesses": {
                    "value": "Major concerns:\n- Although the authors made a great effort on building the OceanGPT, the key contributions seem not strong. Most of the operations on building OceanGPT are general for current LLM models. The authors claim that the DoInstruct is novel, but actually the multi-agent collaboration has already been concerned in the field of LLM, for example, the papers collected in the URL of https://github.com/AGI-Edgerunners/LLM-Agents-Papers, and the authors didn't discuss the essential contributions different from them.\n- The Appendix seems not finished without any content (text) from A.2 to A.6. \n\nMinor concerns: \n- The authors are suggested to pay attention to the writing like typos and grammar errors, for example, the sentence in Page 5: \"We use the retrieved texts are used as high-quality candidate samples\"."
                },
                "questions": {
                    "value": "- What are the contributions that can consider that this work is meaningful to the LLM community?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission882/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698640238766,
            "cdate": 1698640238766,
            "tmdate": 1699636014911,
            "mdate": 1699636014911,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "C3CaOp4FjW",
                "forum": "pbfy04zvcH",
                "replyto": "u7238FenGE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission882/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission882/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely thank you for your insightful feedback. Below are our detailed responses to your concerns:\n\n**Q1: Discussion about multi-agent collaboration**\n\nThanks for your constructive suggestion. As you say, the multi-agent collaboration actually has already been concerned in the field of LLMs. Different from those work in the general domain, we introduce the (science) domain instruction generation framework via multi-agent collaboration.\n\n**Q2: Mistakes in the Appendix**\n\nSorry for our mistakes in the Appendix and we have corrected all the points.\n\n**Q3: Typos and grammar errors**\n\nSorry for the mistakes and we have corrected it.\n\n**Q4: The contribution to the LLM community**\n\nThanks for your advice. In fact, we believe that progress of foundational LLMs and innovative applications in science domains are also very import. OceanGPT is the first-ever LLM in the ocean domain. It not only demonstrates a higher level of knowledge expertise for oceans science tasks but also gains preliminary embodied intelligence capabilities."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission882/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700591308226,
                "cdate": 1700591308226,
                "tmdate": 1700591308226,
                "mdate": 1700591308226,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Ge4ovAfvde",
            "forum": "pbfy04zvcH",
            "replyto": "pbfy04zvcH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission882/Reviewer_E237"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission882/Reviewer_E237"
            ],
            "content": {
                "summary": {
                    "value": "This study introduces a large language model for ocean science tasks to explore the potential of LLMs for ocean science."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This study introduces a large language model for ocean science tasks to explore the potential of LLMs for ocean science. It is a great research topic and beneficial to experts in the ocean science domain."
                },
                "weaknesses": {
                    "value": "While the proposed OceanGPT shows great potential for ocean science tasks, certain details appear to be absent from the manuscript, and valuable information seems to be dispersed throughout the text."
                },
                "questions": {
                    "value": "1.\tAdditional related work should be included to provide a comprehensive background  and necessity of the proposed work\n2.\tI recommend including \"DoINSTRUCT\" in Figure 2.\n3.\tIn Section 3.1, the authors mention that they collected a raw corpus of 67,633 documents. While the source journals are listed in the Appendix, detailed information is missing, such as the criteria for selecting these journals, the specific volumes chosen, and the types of articles included.\n4.\tIn Section 3.2, the authors state that over 10,000 data entries across 500 sub-categories were provided by ocean science researchers. However, they do not explain how these annotations were collected.\n5.\tIn Section 3.2, the introduction to the fine-tuned agent is unclear. What does it mean to \"automatically generate questions from the unsupervised ocean science corpus\"?\n6.\tIn the title of Figure 4, the authors mention that 15,000 instructions were generated from data seeds. If I understand correctly, they collected 10,000 data seeds. Does this mean that DoINSTRUCT generated an additional 15,000 instructions from the seeds?\n7.\tIn Section 3.2, please provide more details about the quality control steps, such as the number of samples evaluated.\n8.\tIn Section 4, what is meant by \"For each testing question, given two responses from two different LLMs\"?\n9.\tIn Section 5, could you explain how the win rate is calculated?\n10.\tThe content in the appendix should be reviewed for accuracy and completeness, as there are a few issues:\n(1)\tSections A.3, A.4, and A.5 are empty.\n(2)\t Some content is missing or cannot be located. For example, in the title of Figure 6, the authors refer to Table 10 in the Appendix, but I was unable to find Table 10."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission882/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission882/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission882/Reviewer_E237"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission882/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698961990006,
            "cdate": 1698961990006,
            "tmdate": 1699636014809,
            "mdate": 1699636014809,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wQtgqmHdHp",
                "forum": "pbfy04zvcH",
                "replyto": "Ge4ovAfvde",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission882/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission882/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely thank you for your insightful feedback. Below are our detailed responses to your concerns:\n\n**Q1: Additional related work**\n\nThanks for your advice. We will add more related work in the revised version and provide a comprehensive background.\n\n**Q2: Revision about \"DoINSTRUCT\" in Figure 2**\n\nThanks for your suggestion and we will revise the Figure 2.\n\n**Q3: Detailed procedure about documents collection**\n\nFor the thematic part, we only select literature related to the field of marine science. For the specific volumes we choose, we prefer to consider publications from recent years to ensure the inclusion of the latest research and developments. At the same time, we will select some historically significant literature to help the model understand the developmental history of the field. Regarding diversity, we choose articles from different publishers and journals to ensure coverage of various research perspectives and methods.\n\n**Q4: The collecting annotations**\n\nFor the construction of the seed dataset, we employ dozens of annotators with rich backgrounds in marine science to assist in building the dataset. Each annotator is responsible for several topics, and they first manually write some high-quality data. Then, we use LLMs to mimic this existing data to generate a large number of similar samples. All these samples are ultimately manually screened and modified by the annotators\n\n**Q5: Unclear points about the fine-tuned agent**\n\nSorry for the confusion. We want to convey that the fine-tuned agent has the ability to extract information from unannotated corpus. We have corrected the mistake in the new version.\n\n**Q6: Instructions generated from data seeds**\n\nSorry for the mistake here. We collect 10,000 data seeds and our proposed DoINSTRUCT generates an additional 150,000 (not 15,000) samples.\n\n**Q7: Details about the quality control steps**\n\nSorry for the unclear points here. We randomly sample 10% instances from the generated instruction dataset and ask the trained domain experts to validate if there are potential errors in the sampled instances.\n\n**Q8: The meaning \u201cFor each testing question, given two responses from two different LLMs**\n\nSorry for the confusion here. For each testing question, we query the GPT4 to obtain the comparison result when given two outputs from two LLMs.\n\n**Q9: how the win rate is calculated**\n\nFor the task-level calculation, we compare the effectiveness of two models for each task. When one model performs better on the majority of test samples in a single task, it is considered to 'win' that task. For the instance-level computation process, we do not differentiate between specific tasks and instead calculate overall metrics.\n\n**Q10: Incorrect points in the Appendix**\n\nSorry for our mistakes in the Appendix and we have corrected all the points you mentioned."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission882/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700591184881,
                "cdate": 1700591184881,
                "tmdate": 1700591262271,
                "mdate": 1700591262271,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]