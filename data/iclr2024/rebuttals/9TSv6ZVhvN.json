[
    {
        "title": "Improving Accelerated Federated Learning with Compression and Importance Sampling"
    },
    {
        "review": {
            "id": "6fcyhAwxQO",
            "forum": "9TSv6ZVhvN",
            "replyto": "9TSv6ZVhvN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1818/Reviewer_AQbc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1818/Reviewer_AQbc"
            ],
            "content": {
                "summary": {
                    "value": "The paper studied the communication problem in the federated learning. The authors proposed a local training strategy which combine the communication compression and importance client sampling. The authors provided theoretical analysis to validate their proposed methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors introduced an innovative federated training method that integrates client importance sampling. Additionally, the paper offers a thorough and comprehensive theoretical analysis of their approach."
                },
                "weaknesses": {
                    "value": "The paper appears to encompass a broad range of topics, making it challenging to follow. The theoretical analysis is primarily confined to strongly convex scenarios, which may not be applicable to intricate models such as deep neural networks (DNNs). Moreover, the simplicity of the numerical experiments detracts from their overall persuasiveness. I would suggest to reorganize the presentation and include more experiments."
                },
                "questions": {
                    "value": "1. In Algorithm 1&2 line 7, what if we can not optimize the problem in K iteration? How would the optimization error on  Eq(4) or Eq(6)  affect the final convergence? Has this be considered in Theorem 4.1 or 5.1?\n\n2. How will the client's data heterogeneity affect the convergence as well as the sampling mechanism?\n\n3. The assumption 2 seems not to be very clear. Can you explain how to achieve it in the algorithm? \n\n4. The client sampling mechanism seems to be unclear. Is it informative? Which information is leveraged for the sampling? \n\n5. Could the results be extended to non-convex cases? I would suggest to both include analysis and experiment on non-convex learning problems."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1818/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697914364791,
            "cdate": 1697914364791,
            "tmdate": 1699636111221,
            "mdate": 1699636111221,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2AhYTFAzDN",
                "forum": "9TSv6ZVhvN",
                "replyto": "6fcyhAwxQO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1818/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1818/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Review"
                    },
                    "comment": {
                        "value": "**Strengths:**\n\n>The authors introduced an innovative federated training method that integrates client importance sampling. Additionally, the paper offers a thorough and comprehensive theoretical analysis of their approach.\n\nThank you for providing your valuable feedback! Your insights and comments are greatly appreciated as they contribute to the ongoing refinement and improvement of our work. If you have any further thoughts, suggestions, or questions, please feel free to share them. Your engagement is instrumental in the continued development of our project, and we look forward to any additional input you may have. Once again, thank you for taking the time to offer your feedback.\n\n**Weaknesses:**\n\n>The paper appears to encompass a broad range of topics, making it challenging to follow. The theoretical analysis is primarily confined to strongly convex scenarios, which may not be applicable to intricate models such as deep neural networks (DNNs). Moreover, the simplicity of the numerical experiments detracts from their overall persuasiveness. I would suggest to reorganize the presentation and include more experiments.\n\nThank you for dedicating your time and effort to providing comments. Allow us to address the weakness section first. We acknowledge the reviewer's observation that our method may not be directly applicable to optimize Deep Neural Networks (DNNs). This limitation arises due to the non-convex nature of the optimization landscape in DNNs, while our theoretical assumption necessitates strong convexity. Despite this, we believe the identified limitation holds significance. Our work introduces two novel algorithms, each designed to address critical challenges in Federated Learning, specifically compression and arbitrary sampling. Both methods achieve accelerated convergence rates by leveraging a widely used mechanism\u2014local training.\n\nWe wish to express our respectful disagreement with the viewpoint suggesting that the current landscape of Federated Learning is solely concentrated on Deep Neural Networks (DNNs). In reality, Federated Learning finds its predominant application in on-device Machine Learning environments, where each device is often constrained by limited computational resources. This inherent limitation implies that smaller devices may struggle to accommodate the training and execution demands associated with large-scale models like DNNs.\n\nMoreover, it's important to highlight that logistic regression, despite its simplicity, remains one of the most popular models in practical applications, especially within the context of mobile devices. The practicality and efficiency of logistic regression make it a preferred choice, particularly when considering the resource constraints often inherent in on-device Machine Learning settings. Therefore, we believe it is essential to recognize the diversity of models and applications within the Federated Learning paradigm, acknowledging the prevalent use of simpler models like logistic regression in real-world scenarios.\n\nIt is important to note that the theoretical analysis of this mechanism, local training, is still in its early stages of development. Consequently, we aim to contribute to this evolving field by providing a concise summary of the generations of local training methods in our work. We consider our contribution to be novel in its analytical approach, as it addresses and resolves important problems while offering a comprehensive overview of local training methods.\n\nWhile we acknowledge that the experiments in our work are relatively straightforward, we assert that the paper's primary value lies in its theoretical results. The experiments serve the purpose of visually illustrating these results rather than constituting standalone findings. We appreciate your consideration of these aspects in evaluating the overall merit of our work."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1818/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700479300894,
                "cdate": 1700479300894,
                "tmdate": 1700479300894,
                "mdate": 1700479300894,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "i1PqlBZgUy",
                "forum": "9TSv6ZVhvN",
                "replyto": "6fcyhAwxQO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1818/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1818/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Review 2"
                    },
                    "comment": {
                        "value": "**Questions:**\n>1. In Algorithm 1&2 line 7, what if we can not optimize the problem in K iteration? How would the optimization error on Eq(4) or Eq(6) affect the final convergence? Has this be considered in Theorem 4.1 or 5.1?\n\nIn the event that we fail to meet the Local Training assumption, it becomes challenging to assert that we will theoretically converge at the promised speeds. The potential arises for certain clients to address their local optimization problems more effectively than others, possibly employing distinct optimization strategies that result in varying convergence rates. Given that the Local Training Assumption is global, there exists the possibility of still satisfying it even if not every device can effectively solve its respective optimization problem.\n\n>2. How will the client's data heterogeneity affect the convergence as well as the sampling mechanism?\n\nWe tackle the challenge of client data heterogeneity through our innovative algorithm, 5GCS-AB. This algorithm enables the utilization of sampling techniques, such as Importance Sampling (IS), which specifically accounts for the diversity present in the data. In contrast to other Local Training methods that mandate the use of upper and lower bounds on L-smoothness and mu-convexity of the functions, often resulting in substantial slowdowns in convergence rates, 5GCS-AB stands out. It accommodates arbitrary differences in the data while still achieving the optimal convergence rate.\n\n>3. The assumption 2 seems not to be very clear. Can you explain how to achieve it in the algorithm?\n\nAssumption 2 establishes a category of sampling schemes that encompasses a broad range of methods commonly employed in practical applications. This classification is designed to be inclusive, covering most existing sampling schemes that are prevalent in real-world scenarios. As part of our analytical framework, we possess the ability to rigorously examine and verify whether a given sampling scheme adheres to the conditions outlined in Assumption 2 and, subsequently, determine its inclusion within this specified class of samplings. This approach facilitates a systematic evaluation of the suitability of various sampling techniques within the framework of our analysis.\n\n>4. The client sampling mechanism seems to be unclear. Is it informative? Which information is leveraged for the sampling?\n\nThe client sampling mechanism delineated in our approach exhibits a high degree of generality, offering substantial flexibility in its application. This versatility is further underscored by the weighted AB assumption, which introduces a nuanced level of adaptability to accommodate various scenarios and requirements. The weighted AB assumption allows for the incorporation of different weights, providing a framework that can be tailored to specific contexts, thereby enhancing the adaptability and applicability of the mechanism.\n\nIn order to employ Importance Sampling effectively, it is imperative to possess information about the individual smoothness constants denoted as $L_i$. These constants play a crucial role in guiding the sampling mechanism, allowing for a targeted and informed approach to variance reduction during the optimization process. The incorporation of individual smoothness constants enhances the precision and adaptability of Importance Sampling, contributing to more efficient and effective optimization strategies tailored to the specific characteristics of each component in the system.\n\n>5. Could the results be extended to non-convex cases? I would suggest to both include analysis and experiment on non-convex learning problems.\n\nOur ambition is to broaden the scope of our acceleration methods to encompass non-convex scenarios. However, achieving this goal requires a fundamentally different analysis and approach, which is presently absent from both our knowledge and the existing literature. While our proofs successfully demonstrate convergence concerning the distance to the optimum\u2014a distinctive feature in cases like ours\u2014it is imperative to recognize that such a claim is not universally applicable.\n\nIn the context of non-convex optimization, the intricacies of the landscape necessitate a novel perspective and analytical framework that currently eludes us. The proofs we provide, grounded in the specifics of convex optimization, cannot be seamlessly extended to the non-convex case due to the inherent disparities in optimization dynamics. As we strive to advance our understanding and methodologies, addressing the challenges posed by non-convex scenarios remains an avenue for future exploration and research."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1818/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700480977107,
                "cdate": 1700480977107,
                "tmdate": 1700492218122,
                "mdate": 1700492218122,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xIlp9i49op",
                "forum": "9TSv6ZVhvN",
                "replyto": "i1PqlBZgUy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1818/Reviewer_AQbc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1818/Reviewer_AQbc"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response."
                    },
                    "comment": {
                        "value": "I would like to thank authors for the detailed response. However, my primary concerns for the paper are not addressed. I would suggest the authors to further elaborate:\n\n1. Most of the optimization problem cannot be solved with limited iteration (i.e. K iterations). From this point, can the global theoretical results be based on the precision of the sub-problem?\n\n2. I agree that the IS can help handle the data heterogeneity issue. But the proposed 5GCS-AB algorithm doesn't show how to set up the IS based on the heterogeneity.\n\n3. Can you provided a few examples which satisfies the Assumption 2?\n\n4. The current study and analysis are only on the strongly convex case. But Almost of the real-world problems are non-convex, especially under FL context. This limits this paper's contribution.\n\nI feel that there are many aspects of this article that still need improvement, including but not limited to the completeness of the theory, the connection between theory and real-world scenarios, reasonable assumptions, and more empirical evidence. Thus, I would keep my current score."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1818/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700528040806,
                "cdate": 1700528040806,
                "tmdate": 1700528040806,
                "mdate": 1700528040806,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mhduJ2d3sI",
                "forum": "9TSv6ZVhvN",
                "replyto": "esFbWDsuH9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1818/Reviewer_AQbc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1818/Reviewer_AQbc"
                ],
                "content": {
                    "comment": {
                        "value": "1. `our approach centers around the careful consideration of a specific sub-problem that we have purposefully constructed to align with the requirements of our method` -- If the algorithm only targets a sub-set of optimization problem, then the contribution would be limited. Strongly convexity and L-smoothness would be some general assumption to capture the objection function's landscape but identifying a solution with limited iteration would be unrealistic assumption on the algorithm. \n\n2. `It is worth noting that our methodology does not necessitate an exact solution to the aforementioned sub-problem. Instead, we find that obtaining an approximate solution suffices for our purposes.` -- I agree with this point and that's why I encourage the authors to improve the theoretical results by considering the precision of the solution of subproblem. \n\n3. `Heterogeneity is a distinguishing characteristic marked by significant variations in the smoothness constants` -- I disagree with this statement. Heterogeneity will definitely affect but not just the smoothness. Take LSE problem as an example $E_{(y,X)~P_i}\\|y- X\\beta\\|^2$. The heterogeneity of $X$ will affect the smoothness and convexity and that of $y$ will affect the location of solution. \n\n4. Even though we only consider the heterogeneity affects the smoothness. $L_i$ for each local loss might be hard to capture."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1818/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700669203053,
                "cdate": 1700669203053,
                "tmdate": 1700669203053,
                "mdate": 1700669203053,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yMd9oJ3Fcx",
                "forum": "9TSv6ZVhvN",
                "replyto": "6fcyhAwxQO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1818/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1818/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to comment 2"
                    },
                    "comment": {
                        "value": ">3. Can you provided a few examples which satisfies the Assumption 2?\n\nThank you for your question! \n\nIndeed, we have presented several examples of sampling schemes in the main body of the paper. Allow me to briefly recapitulate them here:\n\n**SAMPLING WITH REPLACEMENT (MULTISAMPLING) (Section 5.1)**\n\nLet $\\underline{p}=\\left(p_1, p_2, \\ldots, p_M\\right)$ be probabilities summing up to 1 and let $\\chi_m$ be the random variable equal to $m$ with probability $p_m$. Fix a cohort size $C \\in\\{1,2, \\ldots, M\\}$ and let $\\chi_1, \\chi_2, \\ldots, \\chi_C$ be independent copies of $\\chi$. Define the gradient estimator via\n$$\nS\\left(a_1, \\ldots, a_n, \\psi, \\underline{p}\\right):=\\frac{1}{C} \\sum_{m=1}^C \\frac{a_{\\chi m}}{M p_{\\chi_m}} .\n$$\n\nBy utilizing this sampling scheme and its corresponding estimator, we gain the flexibility to assign arbitrary probabilities for client participation while also fixing the cohort size. However, it is important to note that under this sampling scheme, certain clients may appear multiple times within the cohort. \n\n**Lemma 5.2.** The Multisampling with estimator 5 satisfies the Assumption 2 with $A=B=\\frac{1}{C}$ and $w_m=p_m$\n\n**SAMPLING WITHOUT REPLACEMENT (INDEPENDENT SAMPLING) (Section 5.2)**\n\nIn the previous example, the server had the ability to control the cohort size and assign probabilities for client participation. However, in practical settings, the server lacks control over these probabilities due to various technical conditions such as internet connections, battery charge, workload, and others. Additionally, each client operates independently of the others. Considering these factors, we adopt the Independent Sampling approach. Let us formally define such a scheme. To do so, we introduce the concept of independent and identically distributed (i.i.d.) random variables:\n\n$$\n\\chi_m= \\begin{cases}1 & \\text { with probability } p_m \\\\ 0 & \\text { with probability } 1-p_m\\end{cases}\n$$\nfor all $m \\in[M]$, also take $S^t:=\\left\\lbrace m \\in[M] \\mid \\chi\\_m=1\\right\\rbrace$ and $\\underline{p}=\\left(p_1, \\ldots, p_M\\right)$. The corresponding estimator for this sampling has the following form:\n$$\nS\\left(a_1, \\ldots, a_M, \\psi, \\underline{p}\\right):=\\frac{1}{M} \\sum_{m \\in S} \\frac{a_m}{p_m},\n$$\n\nThe described sampling scheme with its estimator is called the Independence Sampling. Specifically, it is essential to consider the probability that all clients communicate, denoted as $\\Pi_{m=1}^M p_m$, as well as the probability that no client participates, denoted as $\\Pi_{m=1}^M\\left(1-p_m\\right)$. It is important to note that $\\sum_{m=1}^M p_m$ is not necessarily equal to 1 in general. Furthermore, the cohort size is not fixed but rather random, with the expected cohort size denoted as $\\mathbb{E}\\left[S^t\\right]=\\sum_{m=1}^M p_m$.\n\n**Lemma 5.6.** The Independent Sampling with estimator 7 satisfies the Assumption 2 with $A=$ $\\frac{1}{\\sum_m^M \\frac{p_m}{1-p_m}}, B=0$ and $w_m=\\frac{\\frac{p_m}{1-p_m}}{\\sum_{m=1}^M \\frac{p_m}{1-p_m}}$.\n\n**TAU-NICE SAMPLING (Section E.4)** \nThis is uniform sampling without replacement considered in the 5GCS work:\n\nGrudzie\u0144, M., Malinovsky, G., & Richt\u00e1rik, P. (2023, April). Can 5th Generation Local Training Methods Support Client Sampling? Yes!. In International Conference on Artificial Intelligence and Statistics (pp. 1055-1092). PMLR.\n\nAdditionally, various examples, including Stratified Sampling and Extended Tau Nice Sampling, are noteworthy. A comprehensive survey of these methods can be found in the referenced paper, providing a broad overview.\n\nTyurin, A., Sun, L., Burlachenko, K., & Richt\u00e1rik, P. (2022). Sharper rates and flexible framework for nonconvex SGD with client and data sampling. arXiv preprint arXiv:2206.02275."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1818/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670816711,
                "cdate": 1700670816711,
                "tmdate": 1700679479200,
                "mdate": 1700679479200,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RtpTGcaX13",
            "forum": "9TSv6ZVhvN",
            "replyto": "9TSv6ZVhvN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1818/Reviewer_SCZT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1818/Reviewer_SCZT"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposed a Federated learning framework that combines the state-of-art techniques in Federated learning. Thorough theoretical results upon smooth and strongly convex objectives are presented."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper gives an excellent review of existing techniques from different perspectives of FL. The motivation of the paper is well-presented.\n2. The theoretical part looks sound and solid."
                },
                "weaknesses": {
                    "value": "1. The work focuses only on strong convex and smooth cases. How does it perform (theoretically) in convex / non-convex cases?\n2. The authors are encouraged to give more detailed comparisons to existing approaches on convergence and communication cost.\n3. It will be interesting to see how well the proposed algorithms perform empirically."
                },
                "questions": {
                    "value": "See weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1818/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698753128054,
            "cdate": 1698753128054,
            "tmdate": 1699636111130,
            "mdate": 1699636111130,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8HHylajrVt",
                "forum": "9TSv6ZVhvN",
                "replyto": "RtpTGcaX13",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1818/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1818/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Review"
                    },
                    "comment": {
                        "value": "**Strengths:**\n\n>1. The paper gives an excellent review of existing techniques from different perspectives of FL. The motivation of the paper is well-presented.\n\n>2. The theoretical part looks sound and solid.\n\nThank you sincerely for your positive evaluation and the endorsement of our work. Your appreciation is highly motivating and reinforces our commitment to delivering quality results. If you have any additional insights, questions, or suggestions, we welcome them with gratitude. Once again, we are grateful for your positive feedback and support.\n\n**Weaknesses:**\n\n>1. The work focuses only on strong convex and smooth cases. How does it perform (theoretically) in convex / non-convex cases?\n\nIn accordance with the current trend observed in recent academic papers, which places a strong emphasis on accelerating training through local methods, our analysis heavily leans on the assumption of strong convexity in the underlying functions. We are actively involved in ongoing efforts to relax these assumptions, acknowledging the importance of extending the applicability of the proposed methods.\n\nIt is noteworthy, however, that in our specific case, the theoretical framework does not offer guarantees for convergence in non-convex scenarios. Despite the collective efforts to broaden the scope of these techniques, the challenges associated with ensuring convergence in non-convex optimization settings remain an active area of exploration and a subject of ongoing research within the broader scientific community.\n\nWe are steadfast in our belief that the comprehensive analysis of non-convex objectives is a topic deserving of its own dedicated paper. The complexities and nuances associated with non-convex optimization merit in-depth exploration, requiring a focused examination that can thoroughly delve into the unique challenges and insights specific to this realm. As such, we envision a separate publication that delves into the intricacies of analyzing and optimizing non-convex objectives, contributing valuable perspectives and advancements to the broader scientific discourse.\n\n>2. The authors are encouraged to give more detailed comparisons to existing approaches on convergence and communication cost.\n\nWe express our gratitude for your suggestion regarding a more detailed comparison. Your input is highly valuable to us. Should there be specific aspects that we might have inadvertently overlooked, we are fully committed to integrating them into our comparison table. Your insights play a pivotal role in enhancing the completeness and accuracy of our analysis, and we welcome any further feedback you may have. Thank you for contributing to the refinement of our work.\n\n>3. It will be interesting to see how well the proposed algorithms perform empirically.\n\nThe paper provides an empirical comparison, albeit presented in a concise manner. The overarching goal was to demonstrate the efficacy of Importance Sampling (IS) by showcasing its ability to yield substantial improvements over non-IS methods. This emphasis on IS underscores its significance in the context of the research.\n\nA key observation is that Local Training (LT) methods, especially those without support for Client Sampling (CS) or Importance Sampling (IS), exhibit a comparatively diminished level of competitiveness. This observation is critical, as it has the potential to influence the overall fairness of the comparison presented in the paper. The inclusion of this insight aims to provide a nuanced understanding of the performance dynamics among various methodologies, acknowledging the impact of IS support in enhancing the overall effectiveness of Local Training methods."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1818/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700475159023,
                "cdate": 1700475159023,
                "tmdate": 1700475159023,
                "mdate": 1700475159023,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VgDzmv1drC",
            "forum": "9TSv6ZVhvN",
            "replyto": "9TSv6ZVhvN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1818/Reviewer_ii5V"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1818/Reviewer_ii5V"
            ],
            "content": {
                "summary": {
                    "value": "Per the authors and reviewing the history of optimization methods on FL, this paper combines three techniques that help with the communication burden in FL rounds: Local training, Compression, and Partial Participation."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper introduces different algorithmic and theoretical tools for the final solution. I.e., if FL was a setting targeting convex problems mostly, the fact that the paper presents and exploits dual spaces + new theoretical tools like AB Inequality is a plus."
                },
                "weaknesses": {
                    "value": "- From an optimization perspective, assuming the logistic regression as an experimental setting is ok, but this is a machine learning venue; it has been a norm to consider more difficult objectives to test the hypotheses. It is a weakness not to consider a setting similar to what most of the FL algorithms are tested in.\n\n- Similarly to the experimental case, providing theory in the convex case, given that FL is mainly applied in nonconvex settings with neural networks, could be improved. While the reviewer appreciates that there is a continuation of works (from specific research groups) that aim to cover every possible problem setting (as summarized in Table 1 of the paper), the current work (theoretically and practically) cannot be readily appreciated (and put among other works) on nonconvex neural networks FL setting.\n\n- I might have missed it, but the difference between this work (along with the accompanying difficulties in completing the algorithm and proof) and the 5GCS is not clear from the text. Table 1 claims that that work did not satisfy the CC column. However, how difficult was it to complete the CC column on top of the work of 5GCS? What were the challenges? What was the amount of additional difficulty in completing this work? Was it incremental or substantial?\n\n- The paper does not explain why assumptions 2,3,4 should hold in practice and under which conditions. They are hard to digest and read like proof, enabling assumptions\n\nOverall, This reads like rigorous work. Yet, this score is due to the lack of generalizability of the results to more FL settings, lack of experimental results on FL scenarios, and lack of proper description of (differences with) prior work."
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1818/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1818/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1818/Reviewer_ii5V"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1818/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699892807169,
            "cdate": 1699892807169,
            "tmdate": 1699892807169,
            "mdate": 1699892807169,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gGhjc98Tw7",
                "forum": "9TSv6ZVhvN",
                "replyto": "VgDzmv1drC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1818/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1818/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Review"
                    },
                    "comment": {
                        "value": "**Strengths:**\n\n>The paper introduces different algorithmic and theoretical tools for the final solution. I.e., if FL was a setting targeting convex problems mostly, the fact that the paper presents and exploits dual spaces + new theoretical tools like AB Inequality is a plus.\n\nThank you very much for the positive feedback! We greatly appreciate your encouraging words. Your support motivates us to continually strive for excellence. If you have any further comments, suggestions, or questions, please feel free to share them. Your input is valuable to us, and we are grateful for your positive impressions of our work.\n\n**Weaknesses:**\n\n>From an optimization perspective, assuming the logistic regression as an experimental setting is ok, but this is a machine learning venue; it has been a norm to consider more difficult objectives to test the hypotheses. It is a weakness not to consider a setting similar to what most of the FL algorithms are tested in.\n\nOur stance is grounded in the conviction that a paper characterized by this level of theoretical depth, particularly emphasizing strongly convex functions, does not require extensive experimentation to demonstrate the theorems. The mathematical rigor underpinning our proofs establishes the validity of the results with a probability of 1, rendering exhaustive experiments less imperative for showcasing these established principles. The robustness of our theoretical framework, substantiated by rigorous mathematical analysis, is deemed sufficient to convey the validity and reliability of our theorems without a heavy reliance on exhaustive experimental validation.\n\nFurthermore, it is crucial to emphasize that logistic regression, despite its simplicity, continues to be widely adopted in practical applications, particularly in the realm of mobile devices. The practicality and efficiency of logistic regression position it as a preferred choice, especially when dealing with resource constraints common in on-device Machine Learning environments. Consequently, it is imperative to acknowledge the varied landscape of models and applications within the Federated Learning paradigm, recognizing the prevalent utilization of simpler models like logistic regression in real-world situations.\n\n>Similarly to the experimental case, providing theory in the convex case, given that FL is mainly applied in nonconvex settings with neural networks, could be improved. While the reviewer appreciates that there is a continuation of works (from specific research groups) that aim to cover every possible problem setting (as summarized in Table 1 of the paper), the current work (theoretically and practically) cannot be readily appreciated (and put among other works) on nonconvex neural networks FL setting.\n\nAligning with the prevalent trend in recent academic literature, which underscores the acceleration of training through local methods, our analysis predominantly relies on the assumption of strong convexity in the underlying functions. We actively participate in ongoing initiatives to ease these assumptions, recognizing the significance of expanding the applicability of our proposed methods.\n\nHowever, it is crucial to highlight that, in our specific case, the theoretical framework does not furnish assurances for convergence in non-convex scenarios. Despite collaborative endeavors to broaden the scope of these techniques, the challenges associated with ensuring convergence in non-convex optimization settings persist as an active area of exploration and ongoing research within the broader scientific community.\n\nOur unwavering belief lies in the notion that a comprehensive analysis of non-convex objectives warrants its own dedicated paper. The intricacies and subtleties inherent in non-convex optimization demand thorough investigation, necessitating a focused examination that can delve deeply into the unique challenges and insights specific to this domain. Consequently, we envision a distinct publication that delves into the complexities of analyzing and optimizing non-convex objectives, contributing valuable perspectives and advancements to the broader scientific discourse."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1818/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700494983844,
                "cdate": 1700494983844,
                "tmdate": 1700494983844,
                "mdate": 1700494983844,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]