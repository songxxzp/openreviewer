[
    {
        "title": "Estimating Shape Distances on Neural Representations with Limited Samples"
    },
    {
        "review": {
            "id": "Ce2b5iDPFe",
            "forum": "kvByNnMERu",
            "replyto": "kvByNnMERu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7957/Reviewer_emQE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7957/Reviewer_emQE"
            ],
            "content": {
                "summary": {
                    "value": "This paper extends some works from shape analysis into the space of neural networks. That is, how does one measure the distance between two NNs where a NN is represented as a mapping h:features ->R^N. The main idea is to think of this mapping into the space of point clouds and hence one can use a Kendall Shape space type distance. The key difference contribution I see is the fact the point clouds have some stochastic properties to them so the shape distance is measuring the distance between expectations. The authors spend a great deal with considering the bounds of these distances."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "A key component of this paper is the bounds on the distances when using an estimator for the cross covariance estimator. The authors present Lemma 1, 2 and Theorem 1 to do this (and theorem 2). The authors explain this bound well as it makes sense that the higher the dimension, the more landmarks need to be observed. \n\nThe authors show how under a variety of scenarios their estimator is performing. The variety is suitable."
                },
                "weaknesses": {
                    "value": "When referencing the shape distances such as (2) and (3) the authors cite Williams 2021 but these are simply common shape distances which have been around for much longer. Unless this specific formulation is novel to Williams 2021 I don't see how it's different than just measuring the distance on a circle.\n\nWhen considering the \"failure modes of plug-in...\" there is the missing component of mentioning scale. If h_i differ in scale their $\\rho$ distance can be large but their $\\theta$ distance can be very small as it rescales in the denominator.\n\nSome of the figure axis are impossible to read."
                },
                "questions": {
                    "value": "When setting up the background, the authors define $h_i:\\mathcal{Z}\\rightarrow \\mathbb{R}^n$ as a function representing each neuron. I have no problem with this definition. My question is, why do the authors limit their writing to that of NNs while this is true for a much wider class of functions. I may have missed something in an assumption but it seems these functions can be quite generally considered.\n\nFor (5) isn't the last equation enough?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7957/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7957/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7957/Reviewer_emQE"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7957/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698431335108,
            "cdate": 1698431335108,
            "tmdate": 1699636978275,
            "mdate": 1699636978275,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FUOnGcxjJs",
                "forum": "kvByNnMERu",
                "replyto": "Ce2b5iDPFe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7957/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7957/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer emQE"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their positive and thoughtful comments on our work.\n\n> When referencing the shape distances such as (2) and (3) the authors cite Williams 2021 but these are simply common shape distances which have been around for much longer. Unless this specific formulation is novel to Williams 2021 I don't see how it's different than just measuring the distance on a circle.\n\nWe now clarify that the shape metric estimator is a common formulation and include older citations in the introduction.  Specifically, we emphasize in a new paragraph 3 of the introduction that although shape distances are drawn from an older, established literature, they were, to the best of our knowledge, proposed as a method to measure similarity in neural network representations by (Williams et. al. 2021).  We then cite this paper frequently due to the fairly self-contained appendix that explains these concepts while using notation that is similar to ours.  \n\n> When considering the \"failure modes of plug-in...\" there is the missing component of mentioning scale. If $h_i$ differ in scale their distance can be large but their  distance can be very small as it rescales in the denominator.\n\nThe reviewer\u2019s understanding is correct. We have now added a sentence to convey this explicitly to the reader under the definitions in equations (2) and (3).  .\n\n**Answers to questions:**\n\n> When setting up the background, the authors define as a function representing each neuron. I have no problem with this definition. My question is, why do the authors limit their writing to that of NNs while this is true for a much wider class of functions. I may have missed something in an assumption but it seems these functions can be quite generally considered.\n\nWe agree that the estimator is broadly applicable, we only need IID responses from a function where the responses have finite covariance. We have crafted our exposition to make the paper more accessible to practitioners studying neural networks. However, we have edited the text to clarify that the theory we develop can be considerably generalized to measure shape distances between any two functional mappings (not only neural nets).\n\n> For (5) isn't the last equation enough?\n\nWe have notated the covariance matrices (for systems i and j) and cross-covariance matrices (between systems i and j) separately to make their distinction and definitions in equation (6) clear. We have also reformulated the text above equation (5) to make the definitions easier to parse for the reader."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7957/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700637520807,
                "cdate": 1700637520807,
                "tmdate": 1700638711325,
                "mdate": 1700638711325,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iGMB4kx4kE",
                "forum": "kvByNnMERu",
                "replyto": "FUOnGcxjJs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7957/Reviewer_emQE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7957/Reviewer_emQE"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the comments, albeit a bit late in the process.\n\nSince the estimator is more broadly applicable than NN's, it would be useful to mention other applications. I do not expect more experiments, simply stating other areas of application would suffice. My issue is that the paper almost handicaps itself into a very particular scenario which could hinder its impact."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7957/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674149153,
                "cdate": 1700674149153,
                "tmdate": 1700674149153,
                "mdate": 1700674149153,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ENPtYDfxR4",
            "forum": "kvByNnMERu",
            "replyto": "kvByNnMERu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7957/Reviewer_GC8w"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7957/Reviewer_GC8w"
            ],
            "content": {
                "summary": {
                    "value": "The paper theoretically quantifies the performance of some typical shape distances between neural representations and the dependence of the performance with respect to the number of samples. The authors find that typical methods have low variance but high variance, and instead propose a new method that enables a tunable bias-variance tradeoff."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- The paper contains significant insight into the performance of 'plug-in' estimators of shape distance between two neural representations and the performance dependence on the number of samples and the dimensionality of the ambient space.\n- The paper identifies the key component behind the non-ideal performance of these estimators (the $||\\Sigma_{ij}||_\\ast$ term) and proposes a simple but intuitive and effective estimator to allow a tunable bias-variance tradeoff.\n- There are adequate experiments on synthetic data to validate this new estimator."
                },
                "weaknesses": {
                    "value": "- The paper should contain some more intuition-building sentences so that the mathematical formulation is more easily digestible. However, the authors have done a really nice job of making the mathematical foundations and derivations themselves clear.\n- One small experiment with VAEs (even if it is confined to the appendix) might be useful for quantifying the performance of stochastic networks."
                },
                "questions": {
                    "value": "NA"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "10: strong accept, should be highlighted at the conference"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7957/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7957/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7957/Reviewer_GC8w"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7957/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698673765157,
            "cdate": 1698673765157,
            "tmdate": 1699636978123,
            "mdate": 1699636978123,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kOApJtdFPn",
                "forum": "kvByNnMERu",
                "replyto": "ENPtYDfxR4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7957/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7957/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer GC8w"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their positive and thoughtful comments on our work.  We agree that the mathematical details of this paper can be difficult to digest.  To address the reviewers point that our paper should contain more intuition-building sentences to help the reader digest the mathematical formulation, we have added the following: \n\t\n* Edited wording in front of eq. 1 to make clear that these are assumptions.\n* Beginning of appendix A:  \u201cWe can intuitively think of the Procrustes distance as the Euclidean distance between two vectors remaining when the rotations and reflections have been \u201cremoved\u201d. Similarly, the Riemannian shape distance can be thought of as the angle between two vectors after these rotations and reflections are removed. These definitions in eq. (2) and eq. (3) also make clear that Procrustes distance, like Euclidean distance, is sensitive to the overall scaling of $h_i$ or $h_j$, while the Riemannian shape distance, like the angle between vectors, is scale-invariant.\u201d\n* Added wording above eq. (5) to improve clarity.\n* We have revamped the introduction and edited the text (in response to this and other reviewer\u2019s comments) to improve clarity as much as possible while remaining within the page limit.  \n\t\n\nWe have now included an experiment where we compare the representation of two neural networks trained on the same task but from different initialization points and with different training procedures (Appendix E: Applications to deep learning). Interestingly we find that the plug-in estimator, similarly to simulations, shows a high bias that slowly converges as we increase the number of samples, but the moment estimator maintains little bias regardless of the number of samples (Fig 5). We discuss how the ways in which the plug-in estimator\u2019s bias can depend on irrelevant nuisance variables including number of samples and effective dimensionality may lead to erroneous scientific conclusions about the similarity between networks."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7957/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700637116873,
                "cdate": 1700637116873,
                "tmdate": 1700637116873,
                "mdate": 1700637116873,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "D2Ea5I7tdK",
            "forum": "kvByNnMERu",
            "replyto": "kvByNnMERu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7957/Reviewer_sJ3L"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7957/Reviewer_sJ3L"
            ],
            "content": {
                "summary": {
                    "value": "In this paper the authors study the estimators of the so-called \"shape distance\", more precisely Procrustes size-and-shape distance $\\rho$, and Riemannian shape distance $\\theta$., that are distances defined on data manifold. They measure the uncertainty (bias and variance) of the empirical estimates of these distances, relying on centration inequalities, and design a new estimator whose bias/variaqnce tradeoff can be controlled. Their theoretical results are illustrated on:\n* a synthetic experiment\n* measure of calcium in the neural activity of a mouse"
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "### Clarity\n\nThe paper is well written, and the and theoretical results are explained in a way that make it accessible to a broad range of readers. The proof techniques are well explained. \n\n### Quality\n\nExperiments are convincing and support"
                },
                "weaknesses": {
                    "value": "### Out of scope: no link with neural networks\n\nThis work has nothing to do with artificial neural networks. In the statement of the problem, the authors assume that thew observations are $X=h_i(Z)$ with $Z$ the input data  and $h_i$ the neural network. No hypothesis is made on $h_i$ nor $Z$. The whole paper could be rewritten with $X$ to get rid of the assumption that the measures are the outputs of a neural network. Even Theorem 2 mentions \"neural\" for no good reason: authors only meant to talk about the existence of some r.v $X$.  \n\nThe only experiment linked with neural network is in Sec 4.2, and the \"neural data\" is actually *calcium measurements* from mouse primary visual cortex, which are typical tabular data.  \n\nOverall, a conference or journal focused on statistics seems to be a better match for the content of the article than ICLR.  \n\n### Novelty\n\nThe novelty is poor: all the proofs are classical bias-variance decomposition and concentration inequalities. \n\nMoreover, there exists previous work on the topic that are not covered in the literature review:   \n   \nKent, J.T. and Mardia, K.V., 1997. Consistency of Procrustes estimators. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 59(1), pp.281-290.\n  \nGoodall, C., 1991. Procrustes methods in the statistical analysis of shape. Journal of the Royal Statistical Society: Series B (Methodological), 53(2), pp.285-321."
                },
                "questions": {
                    "value": "### Application to artificial neural networks\n\nWhich results your method yield in the latent space of an *artificial* neural network?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7957/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7957/Reviewer_sJ3L",
                        "ICLR.cc/2024/Conference/Submission7957/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7957/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699099403195,
            "cdate": 1699099403195,
            "tmdate": 1700736219512,
            "mdate": 1700736219512,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lRLNvSQqrN",
                "forum": "kvByNnMERu",
                "replyto": "D2Ea5I7tdK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7957/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7957/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer sJ3L (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their feedback and for their positive comments regarding clarity (\"the paper is well-written\" and \"proof techniques are well-explained\") and quality (\"experiments are convincing\"). Our reading of their review shows two major concerns which we address below.\n\n**First,** It seems like the major concern is that the work is \"out of scope\" because there \"is no link with neural networks.\" While we think this is overstated, this was useful feedback that helped us revise and re-frame the paper to appeal to a broader audience.\n\n* In our revision, we emphasize that Procrustes/shape distances are *actively being applied to artificial neural networks in the literature* (see: [here](https://arxiv.org/abs/2307.07654), [here](https://arxiv.org/abs/2206.10999), [here](https://arxiv.org/abs/2210.06545), [here](https://arxiv.org/abs/1806.09277), [here](https://arxiv.org/abs/2108.01661), and [here](https://arxiv.org/abs/2110.14739)). More broadly, there is vast literature on comparing hidden layer representations in artificial networks (see 24 page review by [Klabunde et al.](https://arxiv.org/abs/2305.06329)). Notably, this interest is highly interdisciplinary between machine learning, neuroscience, and psychology as reviewed by [Sucholutsky et al](https://arxiv.org/abs/2310.13018). This interdisciplinary interest is why we chose to submit this paper to the **Neuroscience & Cognitive Science track** of ICLR, and why our original submission contained an application to biological networks. Given all of this, plus the positive feedback from the other three reviewers, we think it is clear that our work is a good fit for ICLR.\n\n* To make our envisioned applications more clear and concrete for the ML community, we now include a new direct application of our method to artificial networks (Appendix E: Applications to deep learning ). We show that plug-in estimators of shape distance converge slowly, and our new method provides a viable alternative (Fig 5). The details of this new experiment are described in our global response to reviewers.\n\n* The reviewer says our paper has \"nothing to do with artificial neural networks\" because our theorems are applicable to a more general setting (i.e. they could apply to \"any r.v. X\"). In one sense this is true\u2014our theorems apply to a fairly broad variety of problems. We argue that this generality is a strength not a weakness!   We agree with the comments from reviewer emQE who suggested that we edit the text to explain that our analysis (and the shape distance) can be used to compare any two functions, not just neural networks. Such comparisons between functions may be useful in other contexts within machine learning (e.g. quantifying differences between functions sampled from a Gaussian Process).  \n\n**Second,** the reviewer brings up a concern about \"novelty.\" However, the concern is less about novelty and more about the technical sophistication (\"all proofs are classical bias-variance decomposition and concentration inequalities\"). We show below this concern is based on a fundamental misunderstanding of what we proved; the techniques involved are considerably more complex than the reviewer suggests. The reviewer also requests that we add two citations, but does not explain why these prior references reduce novelty of our results. Overall, we think these concerns are not generous to our efforts.\n\n* First, simple but novel mathematical proofs should be viewed as a strength! Contrary to the reviewer, we believe that results are particularly valuable and accessible when the mathematical techniques are established, but the conclusion is novel.  This is especially true for an applications-focused venue like ICLR. It would be fair to count novelty as a weakness if our theorems / practical guidance afforded practitioners were already widely known. But the reviewer does not make this case against the paper.\n\n* We put considerable effort into refining the proof arguments and explaining their intuition to a broad audience. But this clarity should not be confused with the proofs being obvious. Furthermore, we use a broad range of mathematical techniques rooted in several fields. We use random matrix theory (not mentioned by the reviewer) to prove Theorem 2. We use principles from convex optimization (epigraph reformulation) to arrive at a tunable bias-variance tradeoff in our method-of-moments estimator. We use the Matrix Bernstein inequality to prove theorem 1; this inequality is not a \"classical concentration inequality\" in the usual sense of the term---it was derived in contemporary literature by [Tropp (2010)](https://arxiv.org/abs/1004.4389), and the proof is quite involved (not just a simple application of Markov's inequality). Altogether, we believe the combination of approaches is non-trivial and merits the community's interest."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7957/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700636839707,
                "cdate": 1700636839707,
                "tmdate": 1700636839707,
                "mdate": 1700636839707,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YudFITTi2n",
                "forum": "kvByNnMERu",
                "replyto": "D2Ea5I7tdK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7957/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7957/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer sJ3L (2/2)"
                    },
                    "comment": {
                        "value": "* We thank the reviewer for the references, and we have added them to our literature review in section 2, \u2018Background and Problem Setting\u2019, as well as in paragraph 3 of our introduction.  These classic papers both motivate the importance and heritage of the problem we consider here, but are largely concerned with the problem of estimating form and shape from noisy and nuisance-transformed data, as opposed to estimating shape distances, and in particular, the convergence properties and uncertainty of estimating shape distances. Specifically, Goodall 1991 covers various methods for mean shape estimation in special cases, but contains only a brief discussion of shape differences, and no discussion of the statistical estimation of shape distances in the same form as we do here.  Kent 1997 studies the asymptotic consistency of Procrustes estimators, again of shape, not shape distances, and in the special, restricted case of 2-dimensional shapes (configurations of points in a plane). Our results are non-asymptotic and provide explicit rates of convergence. This is a significant advance over proving asymptotic consistency.\n\n\n**Responses to questions:**\n\n> Which results would your method yield in the latent space of an artificial neural network?\n\nIn our original draft we did not apply the estimator to artificial networks. We have now run the requested experiments and shown in this setting the plug-in estimator shows a significantly larger bias than the moment estimator (see Appendix E and Fig 5)."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7957/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700636912292,
                "cdate": 1700636912292,
                "tmdate": 1700636912292,
                "mdate": 1700636912292,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XBrUrN8SoK",
                "forum": "kvByNnMERu",
                "replyto": "YudFITTi2n",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7957/Reviewer_sJ3L"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7957/Reviewer_sJ3L"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the additional experiment regarding neural networks.  \n\nAs mentioned by reviewer emQE, the position of the paper is a bit weird because it narrows the contribution to neural networks (for no good reason), with no experiments on neural networks in the initial submission, whereas the nature of the contribution itself is more focused on statistics than neurosciences, despite the **Neuroscience & Cognitive Science** track you mention.    \n\nI'd like to clarify that I did not want to understate the efforts of the authors regarding the proofs, for which I praised the clarity, and the fact it was well explained in my initial review. My criticism had nothing to do with the technical sophistication, I believe this is a misunderstanding of the authors.    \n\nI am just wondering why a paper whose major contribution belongs to the field of statistics would position itself in the Neuroscience & Cognitive Science track of ICLR, while offering no new insights in neurosciences or neural networks.  \n\nIt seems that the submission met at least partially its public (by judging the other reviews), but I will stand by my rating. I am lowering my confidence since I am not familiar with the field."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7957/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700675006551,
                "cdate": 1700675006551,
                "tmdate": 1700675006551,
                "mdate": 1700675006551,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2zXPj80l50",
                "forum": "kvByNnMERu",
                "replyto": "xGohrPHc3A",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7957/Reviewer_sJ3L"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7957/Reviewer_sJ3L"
                ],
                "content": {
                    "comment": {
                        "value": "> The litmus test is whether the ICLR community would find this work useful\n\nI agree; and in this regard, all reviewers (including myself) expressed their view on it. ICLR community is quite broad.   \n\nI don't consider myself part of the \"Neuroscience & Cognitive Science\"  sub-community, so I won't speak in its name. Defining the scope is a choice that I prefer to delegate to the AC. \n\nRegarding technical correctness, I don't have much more to say: the paper is well written.  \n\nI lowered my confidence score to reflect this: I don't want to remain \"grumpy reviewer 2\" for too long if other readers found the paper useful."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7957/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700736156846,
                "cdate": 1700736156846,
                "tmdate": 1700736156846,
                "mdate": 1700736156846,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TaC8xlkSq7",
            "forum": "kvByNnMERu",
            "replyto": "kvByNnMERu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7957/Reviewer_ATPd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7957/Reviewer_ATPd"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a novel estimator for distances between neural representations from both biological and artificial neural networks. The key technical contribution is a new moment based estimator of the nuclear norm of the cross covariance matrix which has significantly lower bias than the plug in estimator. The authors provide both theoretical and empirical analysis to illustrate the benefits of the proposed estimator."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The problem is clearly identified and analysed theoretically, and the estimator proposed to solve it is principled and technically sound. The theoretical analysis in Section 3 is fairly detailed and appears to be novel. It clearly explains the flaws in the plugin estimator and the subsequent derivation of the new estimator is also well grounded in theory.\n\n2. Simulations and experiments match the intuition used to derive the estimator (Fig 2A) and clearly highlight the strengths of the estimator over the plug-in baseline."
                },
                "weaknesses": {
                    "value": "1. The overall motivation of the problem is a bit weak. The experiments don't clearly illustrate the downstream benefits of this work. Further experiments showing tangible gains on at least one real world application would greatly strengthen the paper in my opinion.\n\n2. The main technical weakness of the approach appears to be its effect on the variance. While deriving the estimator the authors seem to primarily focus on reducing the bias compared to the plug-in estimator and acknowledge at the end of Section 3.3 that the bias is being bounded at the expense of the variance. This is also seen in the experiments (Fig 2) where the proposed estimator appears to have a significantly higher variance than the moment estimator. This raises questions about the reliability of the estimator, specifically it is not clear if it can reduce the mean squared error for all distributions. This is further illustrated in Fig 4 where on real neural data it appears that other than the case where similarity is set to 0 (A) the plug-in estimator performs comparable to (C, D) or better than (B) the proposed estimator."
                },
                "questions": {
                    "value": "1. In the last equation on page 5, why is $x \\in [0,1]$?\n\n2. In Section 4.2 it is acknowledged that the proposed estimator is highly variable in low SNR regimes and so neurons with the highest SNR are selected. Can you provide the relative proportion of such neurons in the presented scenario and also (if possible) comment on their prevalence in general? This is important because if the relative proportion is small then it means that the proposed estimator can only be applied in very few cases."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7957/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699335138380,
            "cdate": 1699335138380,
            "tmdate": 1699636977897,
            "mdate": 1699636977897,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hftwBcAMAQ",
                "forum": "kvByNnMERu",
                "replyto": "TaC8xlkSq7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7957/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7957/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ATPd (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their thoughtful comments about our work, in particular for highlighting our novel theoretical analysis of this problem.  \n\nWe disagree with the reviewer\u2019s comment that \u201cexperiments showing tangible gains on at least one real world application would greatly strengthen the paper\u201d since our original submission did include an application to experimental neuroscience where data are noisy and sample-limited. Nonetheless we  acknowledge that the importance of studying the uncertainty in estimating shape distances may not have come across clearly in our introduction.  We have added paragraph 3 in the introduction to make clear that, although the Procrustes and shape distances brought to light by Williams et. al. (2021) are currently being applied in this field to measure representational similarity between both artificial and biological neural networks (see: Schuessler et. al. 2023 (https://arxiv.org/abs/2307.07654), Lange et. al. 2022 (https://arxiv.org/abs/2206.10999), Boix-Adsera et. al. 2022 (https://arxiv.org/abs/2210.06545), Alvarez-Melis et. al. 2018 (https://arxiv.org/abs/1806.09277), and Ding et. al. 2021 (https://arxiv.org/abs/2108.01661)), there has been little progress in understanding the theoretical and empirical behavior of the estimators for these distances.  This is a major gap in understanding, because\u2014as we show\u2014the naive plug-in estimators are biased and can converge slowly, and there are circumstances when less naive estimators such as the estimator we propose can be advantageous.  For instance, Figure 2B shows us that in the fixed dimensionality case, one may reduce the variance of an essentially unbiased estimate by increasing the number of stimuli sampled, faster than the bias of the plug-in estimator can be reduced in the same way.  \n\nIn our additional experiments on an artificial neural network (introduced in the rebuttal period), we can see that the bias of the naive plug-in converges slowly with respect to increasing samples whereas the moment estimator maintains small bias (Fig 5, Appendix E).  These findings are relevant to the active research being performed to compare representations in neural networks using Procrustes/shape distances (see citations in previous paragraph). Furthermore, the bias of the plug-in estimator may depend on nuisance properties of the populations being compared leading to confounds. We concretely demonstrate this by showing that the bias of the plug-in estimator depends on the effective dimensionality of the neural populations (Fig 6, Appendix E). Thus plug-in estimated differences across experimental conditions could simply result from differences in effective dimensionality and not the actual similarity. The quantification of bias in the moment estimator allows the practitioner to precisely determine if differences cannot be explained by estimator bias \u2013 a highly practical scientific benefit to the novel estimator.\n\nLastly, we would like to emphasize that the objective of this paper is not to provide a universally \u2018correct\u2019 choice of estimator for shape distances, and we have added wording to make this fact more clear (introduction, final paragraph).  As the reviewer points out, there are cases when the plug-in estimator performs comparably or better than the method-of-moments estimator.  The method-of-moments estimator then provides the practitioner a way to study the uncertainty of shape estimates and explicitly trade off bias and variance by imposing a bias constraint. The reviewer is correct that this can mean for some choices of bias constraint that the variance becomes so large that the estimator is not useful. However, this ability to trade off bias and variance with a tunable parameter is, we believe, a significant contribution.  We also emphasize that this estimator is just one of the results of our paper.  Theorems 1 and 2 theoretically characterize the plug-in estimator and provide worst-case performance guarantees, potentially guiding the design of biological experiments which use shape distances.  We also provide intuition regarding the bias of the plug-in estimator and how the error decreases as a function of neurons and input samples, and how this can converge slowly.   We believe that the culmination of these results and novel estimator methodology is an important step toward a thorough understanding of the statistical challenges of estimating shape distances."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7957/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700636228711,
                "cdate": 1700636228711,
                "tmdate": 1700636228711,
                "mdate": 1700636228711,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mp3Gf01h56",
                "forum": "kvByNnMERu",
                "replyto": "TaC8xlkSq7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7957/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7957/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ATPd (2/2)"
                    },
                    "comment": {
                        "value": "**Answers to questions:**\n\n> In the last equation on page 5, why is $x \\in [0,1]$?\n\n$x$ is in $[0, 1]$ for mathematical convenience. To optimize the polynomial approximation we must choose a domain \u2013 ideally this domain would be between the min and the max singular value of the cross-covariance matrix. Because both are unknown we set the lower end of the domain to be 0 (singular values are positive)  and we estimate the maximum singular value then scale the data so that it is 1. This is the same approach taken in Adams et al. (2018).\n\n> In Section 4.2 it is acknowledged that the proposed estimator is highly variable in low SNR regimes and so neurons with the highest SNR are selected. Can you provide the relative proportion of such neurons in the presented scenario and also (if possible) comment on their prevalence in general? This is important because if the relative proportion is small then it means that the proposed estimator can only be applied in very few cases.\n\nThese 40 neurons are the maximal SNR units so they are exceedingly rare, 40 out of 10,000 neurons recorded. This is a wide-field calcium imaging experiment\u2014 known to be noisy. These results suggest that accurate shape metric estimation in large neural populations will require higher SNR, potentially with electrophysiological recordings. The confidence intervals of our moment estimator quantify this whereas the plug-in estimator naively interpreted would have led to biased results."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7957/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700636474674,
                "cdate": 1700636474674,
                "tmdate": 1700636474674,
                "mdate": 1700636474674,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9IaSaRNvim",
                "forum": "kvByNnMERu",
                "replyto": "TaC8xlkSq7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7957/Reviewer_ATPd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7957/Reviewer_ATPd"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response"
                    },
                    "comment": {
                        "value": "Thank you for the detailed response. I appreciate the honest acknowledgement of the limitations of the proposed estimator and the modification of the wording in the introduction to reflect this and I do not think that this takes anything away from the merits of the work. I had already recommended accepting the paper and so I will keep my score unchanged."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7957/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700717755274,
                "cdate": 1700717755274,
                "tmdate": 1700717788352,
                "mdate": 1700717788352,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]