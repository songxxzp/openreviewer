[
    {
        "title": "Enhancing One-Shot Federated Learning Through Data and Ensemble Co-Boosting"
    },
    {
        "review": {
            "id": "Qm8ZQcWzh8",
            "forum": "tm8s3696Ox",
            "replyto": "tm8s3696Ox",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1615/Reviewer_rouU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1615/Reviewer_rouU"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a method for one-shot federated learning named \"co-boosting\" that aims to improve ensemble and data quality for training the server model. The main contribution lies in the utilization of adversarial samples for training the server-side model."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "On a high level, the idea of OFL and co-boosting is understandable if one wants to avoid multi-round FL then the method is useful. The evaluation is thorough across datasets, architectures, baselines and varying federated settings. From results it looks like method performs exceptionally well compare to the baselines at least on MNIST, FMNIST and SVHN."
                },
                "weaknesses": {
                    "value": "The important details of synthetic data generation and ensemble creation are vague and overly complicated, which makes the paper very hard to follow and identify the key components of the method. As a result, it is difficult to judge if the method is indeed OFL. The evaluation is thorough, but if it is not conducted when the clients' models are trained only once, then I have serious concerns about what the paper is claiming and how the experiments are conducted.\n\nSection 3.2 is extremely hard to follow and lacks structure. It is unclear whether the data is generated on the fly during the training of the server model or if it is pregenerated.\n\nAlgorithm 1 does not provide any useful information beyond what has already been described in earlier sections. Please highlight what happens on the server and what happens on the client device. If it is truly OFL, what does the epoch correspond to? Is it the server model training? This question also relates to Figure 1 (a) and (d). If there are multiple epochs of both the client and server, how is it considered OFL?"
                },
                "questions": {
                    "value": "\"Though samples synthesized using Eq.(8) are hard to fit for the current ensemble model, their difficulties for the server model are still lacking\". If the synthesized samples are hard to fit by the ensemble, why make it even more harder for the server model to learn by introducing adversarial samples? \n\nWhat kind of issues arise when one has multi-round FL? This question is related to the following statement: \"mitigating errors arising from multi-round.\" Is OFL a silver bullet? when it can fail ? Please explain in Limitation section to better contextualize when using OFL is useful. \n\nWhere does the generator G(\u00b7) come from? What is the architecture, and how is it trained? It is completely unclear how data is generated for knowledge distillation eq. 4 to train the server model. I suggest the authors write a separate section explaining it in detail since it is a main component of the proposed method. Is it pure random noise? If yes, why not use [1]?\n\nHow does Co-Boosting not require model transmission? From eq. 2, it seems like the ensemble is created with FedAvg in a single round. But as per section 3.3, the ensemble is not created by weighted averaging of client models? Is that correct? So logits are averaged on synthetic data, which acts as an ensemble for the server model, what is correct?\n\nWhat is GHM? Please provide the full form and a minimal description to improve the readability of the paper. There are already too many references/links to prior work and moving back and forth makes it hard to follow the method.  In eq. 5, does $\\bf{x}$ represent a synthetic sample?\n\nRegarding the statement \"searching for the optimal ensembling weights of each client's logits,\" it is unclear how the search is performed.\n\n[1] Baradad Jurjo, Manel, et al. \"Learning to see by looking at noise.\" Advances in Neural Information Processing Systems 34 (2021): 2556-2569."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1615/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1615/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1615/Reviewer_rouU"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1615/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698406517902,
            "cdate": 1698406517902,
            "tmdate": 1700556256304,
            "mdate": 1700556256304,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ofa7gEiobH",
                "forum": "tm8s3696Ox",
                "replyto": "Qm8ZQcWzh8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1615/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1615/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank the reviewer for taking the time to review our work. We appreciate that you find our experiments extensive and thorough. According to your valuable comments, we provide detailed feedback.\n\n**Q1**: Setting issues: \n>**a):** The important details of synthetic data generation and ensemble creation are vague and overly complicated, which makes the paper very hard to follow and identify the key components of the method. As a result, it is difficult to judge if the method is indeed OFL. The evaluation is thorough, but if it is not conducted when the clients' models are trained only once, then I have serious concerns about what the paper is claiming and how the experiments are conducted.\n>**b):** Section 3.2 is extremely hard to follow and lacks structure. It is unclear whether the data is generated on the fly during the training of the server model or if it is pregenerated.\n>**c):** Algorithm 1 does not provide any useful information beyond what has already been described in earlier sections. Please highlight what happens on the server and what happens on the client device. If it is truly OFL, what does the epoch correspond to? Is it the server model training? This question also relates to Figure 1 (a) and (d). If there are multiple epochs of both the client and server, how is it considered OFL?\n\n***Ans for Q1):***\nWe apologize for the misunderstanding. In response to your valuable comments, we have added the following explanations to our revision.\n\nReplay to **a)** and **c)**: We have added more explanations for our method in the OFL context. To clarify, OFL involves aggregating information from multiple pre-trained client models with various architectures into a unified server model. In contemporary model market scenarios, all algorithms, including ours, operate exclusively on the server side. This means that there is **only one communication round between the server and clients**. Once the server acquires the client models, no further communication is needed, and all computations are performed on the server side. We would like to clarify that the term \"epoch\" in our paper refers solely to the training epoch of the server model, and all experiments adhere to an OFL setting.\n\nWe would like to demonstrate the process of our proposed method as illustrated in Fig. 1a. In each training epoch, our algorithm starts by generating \u2018hard\u2019 samples based on the current state of the weighted ensemble and the server model (lines 4-11 in Algorithm 1). These challenging samples are then used to adjust the weights of the ensemble (lines 12-14 in Algorithm 1). The server model is subsequently updated by distilling knowledge from both the enriched data and the refined ensemble (lines 15-18 in Algorithm 1). This process represents one epoch and is not a singular enhancement but a continuous cycle where the quality of data and the ensemble are iteratively improved.\n\nReply to **a)** and **b)**: Regarding the **ensemble, it is constructed as a weighted combination of each client's logit outputs**, as defined in Eq. (2). The ensemble comprises pre-trained models, and the only modifiable parameters are the weights of each client. This can be seen as a function of client weights outputting weighted logits, with the **pre-trained models remaining unchanged**. The data generation stage is detailed and discussed in section 3.2. Previous OFL methods generate data that can be classified by the ensemble, using the loss defined in Eq. (3), which is later used in the distillation stage (Eq. (4)) to transfer knowledge from the ensemble to the server. However, we identify that this loss (Eq. (3)) is insufficient for effective knowledge transfer. To address this, we introduce a hard sample emphasizing loss (Eqs. (6) and (7)). The generator is optimized according to the loss defined Eq. (8), providing the data for subsequent use (lines 5-10 in Algorithm 1). Additionally, Eqs. (9) and (10) are employed to dynamically make the generated data harder in each epoch (lines 10-11 in Algorithm 1).\n\nReply to **b)**: The data is **generated on-the-fly** in each epoch for the server model's training, and it is not pre-generated. We would like to note that previous OFL methods focus on either improving the ensemble or improving the data quality. Once this step is achieved, knowledge is distilled from these static high-quality sources into the server model. This process is linear, where the enhancement of data or the ensemble is a prerequisite step before the server model can benefit. In contrast, our method adopts **an iterative and interconnected approach**, integrating high-quality data generation and ensemble refinement into a continuous, dynamic cycle. This novel approach to linking these two processes is a key contribution of our proposed paradigm, demonstrating its uniqueness and potential impact in the field of OFL.\n\nWe hope these clarifications assist in better understanding the process and contributions of our proposed method."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1615/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700207126547,
                "cdate": 1700207126547,
                "tmdate": 1700207126547,
                "mdate": 1700207126547,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cabYIsDfXB",
                "forum": "tm8s3696Ox",
                "replyto": "Qm8ZQcWzh8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1615/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1615/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Welcome for more discussions"
                    },
                    "comment": {
                        "value": "Dear reviewer #rouU,\n\nThanks for your valuable time in reviewing and insightful comments. Following your comments, we have tried our best to provide responses and revise our paper. Here is a **summary of our response** for your convenience:\n\n- (1) **Setting issues**: We apologize for the misunderstanding. In response to your valuable comments, we have added the following explanations to our revision. OFL means there is only one communication round between the server and clients. We would like to clarify that the term \"epoch\" in our paper refers solely to the training epoch of the server model, and all experiments adhere to an OFL setting. The ensemble is constructed as a weighted combination of each client's logit outputs with the inside pre-trained models remaining unchanged. Regarding the data, it is generated on-the-fly in each epoch for the server model's training. Detailed explanations of the generator design, learnable ensemble weights, and how our method work has been added to the revision to enhance our work.\n- (2) **Data generation issues**: Thank you for posing this insightful question. We would like to note that at each epoch, we generate examples, reweight clients based on these examples, and distill knowledge from the refined ensemble into the server model, utilizing all generated data up to that point. Due to the iterative nature of the learning process, there's a risk that the server model might overfit this static set of hard samples. Therefore, to prevent this potential overfitting, we designed our approach to dynamically diversify and increase the difficulty of samples for the server model in each epoch.\n- (3) **Multi-round FL comparisons:**: Following your valuable comments, we have added more discussions as follows in the revised paper. We would like to note that there may be communication overheads and connectivity issues in multi-round FL, which may severely impact the efficiency and reliability of the FL process. Moreover, the multi-round approach poses potential risks for man-in-the-middle attacks, alongside various other privacy and security concerns. Last but not least, in contemporary model market scenarios that involve multiple pre-trained and large-scale models, it becomes impractical to engage in repeated transmissions and retraining processes. We have added discussions in the revision to enhance our work.\n\nWe humbly hope our response has addressed your concerns. If you have any additional concerns or comments that we may have missed in our responses, we would be most grateful for any further feedback from you to help us further enhance our work.\n\nBest regards\n\nAuthors of #1615"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1615/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700471966084,
                "cdate": 1700471966084,
                "tmdate": 1700471966084,
                "mdate": 1700471966084,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OF6Xdcl9yS",
                "forum": "tm8s3696Ox",
                "replyto": "Qm8ZQcWzh8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1615/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1615/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Window for responsing and draft updating is closing"
                    },
                    "comment": {
                        "value": "Dear Reviewer #rouU,\n\nThanks very much for your time and valuable comments. We understand you're busy. But as the window for responding and paper revision is closing, would you mind checking our response ([a brief summary](https://openreview.net/forum?id=tm8s3696Ox&noteId=cabYIsDfXB), and [details](https://openreview.net/forum?id=tm8s3696Ox&noteId=ofa7gEiobH)) and confirm whether you have any further questions? We are very glad to provide answers and revisions to your further questions.\n\nBest regards and thanks,\n\nAuthors of #1615"
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1615/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700472241542,
                "cdate": 1700472241542,
                "tmdate": 1700472241542,
                "mdate": 1700472241542,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "N70z21xBB7",
                "forum": "tm8s3696Ox",
                "replyto": "Qm8ZQcWzh8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1615/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1615/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Window for responding and draft updating is closing"
                    },
                    "comment": {
                        "value": "Dear Reviewer #rouU,\n\nThanks very much for your time and valuable comments. We understand you're busy. But as the window for responding and paper revision is closing, would you mind checking our response ([a brief summary](https://openreview.net/forum?id=tm8s3696Ox&noteId=cabYIsDfXB), and [details](https://openreview.net/forum?id=tm8s3696Ox&noteId=ofa7gEiobH)) and confirm whether you have any further questions? We are very glad to provide answers and revisions to your further questions.\n\nBest regards and thanks,\n\nAuthors of #1615"
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1615/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700548903992,
                "cdate": 1700548903992,
                "tmdate": 1700548903992,
                "mdate": 1700548903992,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rdka0mDVyw",
                "forum": "tm8s3696Ox",
                "replyto": "Qm8ZQcWzh8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1615/Reviewer_rouU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1615/Reviewer_rouU"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the effort in providing explanation and making relevant changes. In summary, the method does the following: a) train locally on client data, generate logits on synthetic data, 3) server aggregates the logits to create so called ensemble (which does not sound like an ensemble in true sense), and then server applies knowledge distillation on data generated by G + optimizing on so called ensemble logits with GHM in the loop. Is it correct?"
                    }
                },
                "number": 26,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1615/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700550349897,
                "cdate": 1700550349897,
                "tmdate": 1700550477634,
                "mdate": 1700550477634,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OLKtKblxTZ",
                "forum": "tm8s3696Ox",
                "replyto": "Qm8ZQcWzh8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1615/Reviewer_rouU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1615/Reviewer_rouU"
                ],
                "content": {
                    "comment": {
                        "value": "\"Ensemble Update: (Lines 12-14 in Algorithm 1) We use the synthetic data to update the ensemble by reweighting each pre-trained model.\" -> these are logits as per your earlier explanation! I suggest to stick to one definition. \n\nThanks for the clarification. I am increasing the score but I am not very convinced that this method is proposing something entirely new in terms of methodological contributions to FL."
                    }
                },
                "number": 28,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1615/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700556238898,
                "cdate": 1700556238898,
                "tmdate": 1700556294138,
                "mdate": 1700556294138,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "58soGtUnRU",
            "forum": "tm8s3696Ox",
            "replyto": "tm8s3696Ox",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1615/Reviewer_vHVL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1615/Reviewer_vHVL"
            ],
            "content": {
                "summary": {
                    "value": "This paper developed Co-Boosting, a new framework in which synthesized data and the ensemble model mutually enhance each other progressively. Extensive experiments demonstrated the good performance of the proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Develop a new framework that can enhance synthesized data and the ensemble model mutually\n2. The generation of hard samples seems interesting and helpful.\n3. Extensive experiments have shown the proposed method outperforms the baselines."
                },
                "weaknesses": {
                    "value": "1. Fig 1 (a) needs to be elaborated to help readers better understand the 3 steps. Please elaborate on these steps in the caption.\n\n2. Missing one-shot FL baselines. The introduction discussed two methods, Heinbaugh et al. (2023), and Diao et al. (2023). However, the authors did not compare them in the experiments.\n\n3. How to evaluate whether the labels generated in the synthetic data are accurate and correct?\n\n4. The technical contribution is not very significant. It seems to combine some existing works. For instance, generating hard samples motivated by existing work like Dong et al. (2020) and Li et al. (2023)."
                },
                "questions": {
                    "value": "How to evaluate whether the labels generated in the synthetic data are accurate and correct? Whether the pseudo label affects the results a lot?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1615/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1615/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1615/Reviewer_vHVL"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1615/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698624693239,
            "cdate": 1698624693239,
            "tmdate": 1700674128647,
            "mdate": 1700674128647,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uohlmGQYuZ",
                "forum": "tm8s3696Ox",
                "replyto": "58soGtUnRU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1615/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1615/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank the reviewer for taking the time to review our work. We appreciate that you find our method instesesting and experiments are extensive. According to your valuable comments, we provide detailed feedback.\n\n**Q1**: Caption issues:\n> Fig 1 (a) needs to be elaborated to help readers better understand the 3 steps. Please elaborate on these steps in the caption.\n> \n***Ans for Q1):***\nThank you for your valuable suggestion regarding Fig. 1 (a). We have revised the figure caption to provide a more detailed explanation of the three key steps involved in each epoch of the server training process. The revised caption now reads as follows: In each epoch of server training, high-quality samples are first generated based on last epoch\u2019s ensemble and server, which are then used to adjust client weights giving a better ensemble. Based on the enriched data and refined ensemble, server model is updated by distilling knowledge from them.\n\n**Q2**: Baseline issues:\n> Missing one-shot FL baselines. The introduction discussed two methods, Heinbaugh et al. (2023), and Diao et al. (2023). However, the authors did not compare them in the experiments.\n> \n***Ans for Q2):***\nThank you for bringing attention to OFL methods proposed by Diao et al. (2023) and Heinbaugh et al. (2023). Accordingly, we have added the following discussions in the revised paper.\n\n- The primary reason for this omission is grounded in the **practical constraints of contemporary model market scenarios**, which form the basis of our research approach in OFL. In contemporary settings, the server typically has access only to pre-trained client models, without the feasibility of imposing modifications on the local training phase of these models. The methods introduced by Diao et al. and Heinbaugh et al. both necessitate alterations to the local training process of client models. Diao et al. integrate placeholders in client models, while Heinbaugh et al. convert local models into conditional variational auto-encoders. Such modifications, while innovative, are **not always practical or feasible in real-world model market scenarios** where the server's interaction with client models is limited to the use of pre-trained, unmodifiable models. \n\n- Both the methods by Diao et al. and Heinbaugh et al. primarily focus on improving the ensemble model. Once an enhanced ensemble is obtained, it is used in a fixed state for knowledge distillation with additional data. We have outlined a detailed comparison with these two mentioned and other baselines, which are discussed in Appendix A.1 and presented in Table 8. The rationale behind our baseline selection is further elaborated in Appendix B.1. Notably, these previous baselines, including those by Diao et al. and Heinbaugh et al., concentrate on improving either the ensemble or the data. In contrast, our method innovatively proposes a mechanism that allows for the periodic mutual enhancement of both the ensemble and the data.\n\nInspired by your valuable comments, it would be intriguing to investigate how our method performs when the constraint of fixed local training is relaxed, and alterations in client local training are permitted. To explore this, we have conducted preliminary experiments, presented in Appendix B.6, demonstrating that our method can **indeed benefit from advanced local training techniques**. These findings suggest the potential for further enhancing our approach by leveraging improvements in client model training."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1615/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700206592190,
                "cdate": 1700206592190,
                "tmdate": 1700206592190,
                "mdate": 1700206592190,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IxCcQNinhB",
                "forum": "tm8s3696Ox",
                "replyto": "58soGtUnRU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1615/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1615/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Welcome for more discussions"
                    },
                    "comment": {
                        "value": "Dear reviewer #vHVL,\n\nThanks for your valuable time in reviewing and insightful comments. Following your comments, we have tried our best to provide responses and revise our paper. Here is a **summary of our response** for your convenience:\n\n- (1) **Baseline issues**: Following your valuable comments, we have added the following discussions in the revised paper. The primary reason for this omission is grounded in the practical constraints of contemporary model market scenarios, which form the basis of our research approach in OFL. Detailed explanations of the baseline choosing are added to the revision. Also inspired by your valuable comments, we conduct experiments when the constraint of fixed local training is relaxed, and alterations in client local training are permitted. Results in Appendix B.6, demonstrate that our method can indeed benefit from advanced local training techniques.\n- (2) **Pseudo label correctness concern**: Thank you for your insightful inquiry. We conduct experiments on assigning the synthetic data orderly or random labels. Results in the following demonstrate that the specific assignment of labels does not significantly affect the overall performance. The assigned pseudo label is used to make the samples produced by the generator likely to be correctly predicted by the ensemble yet remains challenging for the server model to recognize.\n- (3) **Contribution concern**: We would like to note that the main contribution is the proposed co-boosting approach. It is **the first time** to make the ensemble and the synthetic data **mutually boost each other**. Existing literature focuses on either enhancing the ensemble or the data quality in isolation. Once a higher quality of data or an improved ensemble is achieved, these elements typically remain static during the server training phase. Our work diverges from this norm by proposing a method that establishes **a dynamic and mutually reinforcing relationship between the ensemble and data generation processes.** We provide a comprehensive comparison and summarize them in Table 8 in response. \n\nWe humbly hope our response has addressed your concerns. If you have any additional concerns or comments that we may have missed in our responses, we would be most grateful for any further feedback from you to help us further enhance our work.\n\nBest regards\n\nAuthors of #1615"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1615/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700471789796,
                "cdate": 1700471789796,
                "tmdate": 1700471789796,
                "mdate": 1700471789796,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZAYr1MhMJY",
                "forum": "tm8s3696Ox",
                "replyto": "58soGtUnRU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1615/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1615/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Window for responsing and draft updating is closing"
                    },
                    "comment": {
                        "value": "Dear Reviewer #vHVL,\n\nThanks very much for your time and valuable comments. We understand you're busy. But as the window for responding and paper revision is closing, would you mind checking our response ([a brief summary](https://openreview.net/forum?id=tm8s3696Ox&noteId=IxCcQNinhB), and [details](https://openreview.net/forum?id=tm8s3696Ox&noteId=uohlmGQYuZ)) and confirm whether you have any further questions? We are very glad to provide answers and revisions to your further questions.\n\nBest regards and thanks,\n\nAuthors of #1615"
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1615/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700472192478,
                "cdate": 1700472192478,
                "tmdate": 1700472192478,
                "mdate": 1700472192478,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NQennqLslj",
                "forum": "tm8s3696Ox",
                "replyto": "58soGtUnRU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1615/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1615/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Window for responding and draft updating is closing"
                    },
                    "comment": {
                        "value": "Dear Reviewer #vHVL,\n\nThanks very much for your time and valuable comments. We understand you're busy. But as the window for responding and paper revision is closing, would you mind checking our response ([a brief summary](https://openreview.net/forum?id=tm8s3696Ox&noteId=IxCcQNinhB), and [details](https://openreview.net/forum?id=tm8s3696Ox&noteId=uohlmGQYuZ)) and confirm whether you have any further questions? We are very glad to provide answers and revisions to your further questions.\n\nBest regards and thanks,\n\nAuthors of #1615"
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1615/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700548875074,
                "cdate": 1700548875074,
                "tmdate": 1700548875074,
                "mdate": 1700548875074,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oavFp0DE2R",
                "forum": "tm8s3696Ox",
                "replyto": "58soGtUnRU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1615/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1615/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Window for responding and draft updating is closing"
                    },
                    "comment": {
                        "value": "Dear Reviewer #vHVL,\n\nThanks very much for your time and valuable comments. We understand you're busy. But as the window for responding and paper revision is closing, would you mind checking our response ([a brief summary](https://openreview.net/forum?id=tm8s3696Ox&noteId=IxCcQNinhB), and [details](https://openreview.net/forum?id=tm8s3696Ox&noteId=uohlmGQYuZ)) and confirm whether you have any further questions? We are very glad to provide answers and revisions to your further questions.\n\nBest regards and thanks,\n\nAuthors of #1615"
                    }
                },
                "number": 33,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1615/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700621315640,
                "cdate": 1700621315640,
                "tmdate": 1700621315640,
                "mdate": 1700621315640,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sx2Mv7kyMN",
                "forum": "tm8s3696Ox",
                "replyto": "58soGtUnRU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1615/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1615/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Window for responding and draft updating is closing"
                    },
                    "comment": {
                        "value": "Dear Reviewer #vHVL,\n\nThanks very much for your time and valuable comments. We understand you're busy. But as the window for responding and paper revision is closing, would you mind checking our response ([a brief summary](https://openreview.net/forum?id=tm8s3696Ox&noteId=IxCcQNinhB), and [details](https://openreview.net/forum?id=tm8s3696Ox&noteId=uohlmGQYuZ)) and confirm whether you have any further questions? We are very glad to provide answers and revisions to your further questions.\n\nBest regards and thanks,\n\nAuthors of #1615"
                    }
                },
                "number": 37,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1615/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700654083152,
                "cdate": 1700654083152,
                "tmdate": 1700654083152,
                "mdate": 1700654083152,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KmEDQAoVTP",
                "forum": "tm8s3696Ox",
                "replyto": "uohlmGQYuZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1615/Reviewer_vHVL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1615/Reviewer_vHVL"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response"
                    },
                    "comment": {
                        "value": "Thanks for your detailed response! I think your answers have addressed most of my concerns. I will raise the score."
                    }
                },
                "number": 38,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1615/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674108442,
                "cdate": 1700674108442,
                "tmdate": 1700674108442,
                "mdate": 1700674108442,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QOaIdBrO9z",
            "forum": "tm8s3696Ox",
            "replyto": "tm8s3696Ox",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1615/Reviewer_qWAT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1615/Reviewer_qWAT"
            ],
            "content": {
                "summary": {
                    "value": "This work focuses on one-shot federated learning, following the common practice that uses data-free distillation with local models. Co-Boosting is proposed as an algorithm in which the synthesized data and the ensemble model collaboratively enhance one another in a progressive manner. To elaborate, the algorithm produces challenging samples by training a data generator using a loss function that is re-weighted for the samples and includes an adversarial loss component."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The idea is quite straightforward and sound.\n* Nice and clean writing\n* Experiments are complete with comparison to baselines and ablations, across common datasets and FL settings"
                },
                "weaknesses": {
                    "value": "* The related works could be improved. Specifically, \n1) FL works about distillation and ensemble with or without a server dataset: there are many follow-up works after Lin et al. (2020). More works about model ensemble or aggregation would be better. \n2) Since the idea is about co-learning, it will help if a discussion about centralized co-learning techniques can be added; such as\nDeep co-training for semi-supervised image recognition. CVPR 2018\n\n* The experiments are a bit lack of insights. The paper could be stronger if deeper analysis beyond accuracy can be provided. For example, what examples are considered hard? What are the training dynamics in terms of model weighting or sample re-weighting?  Any failure cases? \n\n=== Minor issues ===\n* Suggestions on fig 1: no label for the y-axis\n* Experiment tables: It is better to include the upper bound such as multi-round FedAvg or centralized training\n* \"round\" terminology: I understand the paper is about one-shot FL but sometimes the term \"round\" is used in the text causing me slight confusion."
                },
                "questions": {
                    "value": "Recent works in multi-round FL have suggested to use of a pre-trained model for initialization [A, B]. A pre-trained model is typically publically available on both the server side and the user side and might significantly help the convergence speed given limited rounds or distillation. Could the authors provide any insights or discussions about this case?\n\n[A] When foundation model meets federated learning: Motivations, challenges, and future directions. ArXiv 2023\n[B] On the Importance and Applicability of Pre-Training for Federated Learning. ICLR 2023"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1615/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698692162076,
            "cdate": 1698692162076,
            "tmdate": 1699636089773,
            "mdate": 1699636089773,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7DFxpCioY5",
                "forum": "tm8s3696Ox",
                "replyto": "QOaIdBrO9z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1615/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1615/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank the reviewer for taking the time to review our work. We appreciate that you find our paper well writing and experiments are complete. According to your valuable comments, we provide detailed feedback.\n\n**Q1**: Related works issues:\n> **a)** FL works about distillation and ensemble with or without a server dataset: there are many follow-up works after Lin et al. (2020). More works about model ensemble or aggregation would be better.\n> **b)** Since the idea is about co-learning, it will help if a discussion about centralized co-learning techniques can be added; such as Deep co-training for semi-supervised image recognition. CVPR 2018\n> \n***Ans for Q1):***\n- **a)** We appreciate the reference to the follow-up works post Lin et al. (2020). We have added the following discussions in the revised paper. We would like to note that predominant approaches in OFL involve distilling knowledge from an ensemble using either synthetic or real server data. However, existing literature focuses on either enhancing the ensemble or the data quality in isolation. Once a higher quality of data or an improved ensemble is achieved, these elements typically remain static during the server training phase. Our work diverges from this norm by proposing a method that **periodically and mutually enhances both the ensemble and data quality**. This dynamic approach represents a significant advancement in the field of OFL. We have elaborated on this in the revision, providing a more comprehensive discussion of our methodology.\n\n- **b)** Addressing the parallels and contrasts with centralized co-learning techniques, such as deep co-training for semi-supervised image recognition (CVPR 2018), offers insightful perspectives. We have added a discussion section to our revision. Centralized co-learning is generally predicated on the concept of learning from data that offers varying perspectives. In contrast, OFL deals with more pronounced divergences in data distributions among clients. Additionally, in OFL, the models provided to the server are pre-trained, eliminating the feasibility of retraining local models.\n\n**Q2**: Experiments issues:\n> The experiments are a bit lack of insights. The paper could be stronger if deeper analysis beyond accuracy can be provided. For example, what examples are considered hard? What are the training dynamics in terms of model weighting or sample re-weighting? Any failure cases?\n>\n***Ans for Q2):***\nThank you for your constructive comments and kind suggestions. We appreciate your attention to the depth of our analysis and agree that a more comprehensive exploration beyond accuracy could enrich the paper. \n- We have provided preliminary insights in Figure 1 and Table 7, which highlight the effectiveness of each component of our proposed method. Regarding the dynamic model weights, it is demonstrated in Figure 1d, showcasing the performance of our weighted ensemble. Additional discussions regarding model architecture, the impact of heavier models, and the integration with advanced local training techniques are detailed in Appendix B. \n- We attribute the main success factor to the mutual boosting mechanism between synthetic data generation and the reweighting ensemble strategy. Your suggestion to delve into the nature of 'hard samples', the dynamics of model weighting, and an analysis of failure cases is indeed intriguing. We plan to incorporate these in-depth studies into our revision, as they would provide a more holistic understanding of our method.\n\n**Q3**: Minor issues:\n> **a)** Suggestions on fig 1: no label for the y-axis\n> **b)** Experiment tables: It is better to include the upper bound such as multi-round FedAvg or centralized training\n> **c)** \"round\" terminology: I understand the paper is about one-shot FL but sometimes the term \"round\" is used in the text causing me slight confusion.\n> \n***Ans for Q3):***\n\n**a)** We thank the reviewer for pointing out the missing label on the y-axis in Figure 1. This axis represents the 'Test Accuracy,' which we have now clearly labeled and elaborated upon in the figure's caption.\n\n**b)** In the context of OFL, the upper bound of each method is inherently linked to their ensemble performance. We have added discussions in Tables 2 and Appendix B.2. The superiority of our proposed method is, in part, attributable to the enhanced performance of the ensemble. We have conducted a primary comparison with multi-round FedAvg in Appendix B.8. We agree that a comparison with centralized training is also crucial and plan to incorporate this in the revised manuscript.\n\n**c)** We apologize for any confusion caused by the use of the term \"round\" in the text. To clarify, in our paper, we now consistently use the term \"epoch\" to refer to the training epochs of the server model. We would like to note that all experiments and algorithms occur after the server has collected all client models. This means there is only one communication round in our setup."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1615/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700206453874,
                "cdate": 1700206453874,
                "tmdate": 1700206453874,
                "mdate": 1700206453874,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jZG4pOxvTd",
                "forum": "tm8s3696Ox",
                "replyto": "QOaIdBrO9z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1615/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1615/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Welcome for more discussions"
                    },
                    "comment": {
                        "value": "Dear reviewer #qWAT,\n\nThanks for your valuable time in reviewing and insightful comments. Following your comments, we have tried our best to provide responses and revise our paper. Here is a **summary of our response** for your convenience:\n\n- (1) **Related work issues**: Following your constructive comments, we have discussed more related works. We would like to note that existing literature focuses on either enhancing the ensemble or the data quality in isolation. Once a higher quality of data or an improved ensemble is achieved, these elements typically remain static during the server training phase. Our work diverges from this norm by proposing a method that **periodically and mutually enhances both the ensemble and data quality**. We provide a comprehensive comparison and summarize them in Table 8 in response. Discussions about centralized co-learning techniques are also added to our revision to enhance our work.\n- (2) **Experiments issues**: Thank you for your constructive comments and kind suggestions. We have provided preliminary insights in Figure 1 and Table 7, which highlight the effectiveness of each component of our proposed method. We attribute the main success factor to the mutual boosting mechanism between synthetic data generation and the reweighting ensemble strategy. We also plan to incorporate more in-depth studies into our revision\n- (3) **Discussion on pre-trained models**: We appreciate the reference to recent works advocating the use of pre-trained models in multi-round Federated Learning (FL). Viewing OFL through the lens of today's pre-trained foundation model era opens up new avenues of exploration. Our work positions itself as a pioneering effort in adapting OFL to the era of foundation models. The key strength of our method is its independence from alterations in local training and its architecture-agnostic nature. \n\nWe humbly hope our response has addressed your concerns. If you have any additional concerns or comments that we may have missed in our responses, we would be most grateful for any further feedback from you to help us further enhance our work.\n\nBest regards\n\nAuthors of #1615"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1615/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700471699532,
                "cdate": 1700471699532,
                "tmdate": 1700471699532,
                "mdate": 1700471699532,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1BZmeBfRVx",
                "forum": "tm8s3696Ox",
                "replyto": "QOaIdBrO9z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1615/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1615/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Window for responsing and draft updating is closing"
                    },
                    "comment": {
                        "value": "Dear Reviewer #qWAT,\n\nThanks very much for your time and valuable comments. We understand you're busy. But as the window for responding and paper revision is closing, would you mind checking our response ([a brief summary](https://openreview.net/forum?id=tm8s3696Ox&noteId=jZG4pOxvTd), and [details](https://openreview.net/forum?id=tm8s3696Ox&noteId=7DFxpCioY5)) and confirm whether you have any further questions? We are very glad to provide answers and revisions to your further questions.\n\nBest regards and thanks,\n\nAuthors of #1615"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1615/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700472137483,
                "cdate": 1700472137483,
                "tmdate": 1700472137483,
                "mdate": 1700472137483,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ibBN1Rdgpk",
                "forum": "tm8s3696Ox",
                "replyto": "QOaIdBrO9z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1615/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1615/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Window for responding and draft updating is closing"
                    },
                    "comment": {
                        "value": "Dear Reviewer #qWAT,\n\nThanks very much for your time and valuable comments. We understand you're busy. But as the window for responding and paper revision is closing, would you mind checking our response ([a brief summary](https://openreview.net/forum?id=tm8s3696Ox&noteId=jZG4pOxvTd), and [details](https://openreview.net/forum?id=tm8s3696Ox&noteId=7DFxpCioY5)) and confirm whether you have any further questions? We are very glad to provide answers and revisions to your further questions.\n\nBest regards and thanks,\n\nAuthors of #1615"
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1615/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700548846685,
                "cdate": 1700548846685,
                "tmdate": 1700548846685,
                "mdate": 1700548846685,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6957M7fT4V",
                "forum": "tm8s3696Ox",
                "replyto": "QOaIdBrO9z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1615/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1615/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Window for responding and draft updating is closing"
                    },
                    "comment": {
                        "value": "Dear Reviewer #qWAT,\n\nThanks very much for your time and valuable comments. We understand you're busy. But as the window for responding and paper revision is closing, would you mind checking our response ([a brief summary](https://openreview.net/forum?id=tm8s3696Ox&noteId=jZG4pOxvTd), and [details](https://openreview.net/forum?id=tm8s3696Ox&noteId=7DFxpCioY5)) and confirm whether you have any further questions? We are very glad to provide answers and revisions to your further questions.\n\nBest regards and thanks,\n\nAuthors of #1615"
                    }
                },
                "number": 32,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1615/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700621290330,
                "cdate": 1700621290330,
                "tmdate": 1700621290330,
                "mdate": 1700621290330,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZT3DpBmlSo",
                "forum": "tm8s3696Ox",
                "replyto": "QOaIdBrO9z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1615/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1615/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Window for responding and draft updating is closing"
                    },
                    "comment": {
                        "value": "Dear Reviewer #qWAT,\n\nThanks very much for your time and valuable comments. We understand you're busy. But as the window for responding and paper revision is closing, would you mind checking our response ([a brief summary](https://openreview.net/forum?id=tm8s3696Ox&noteId=jZG4pOxvTd), and [details](https://openreview.net/forum?id=tm8s3696Ox&noteId=7DFxpCioY5)) and confirm whether you have any further questions? We are very glad to provide answers and revisions to your further questions.\n\nBest regards and thanks,\n\nAuthors of #1615"
                    }
                },
                "number": 36,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1615/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700654059559,
                "cdate": 1700654059559,
                "tmdate": 1700654059559,
                "mdate": 1700654059559,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KvQnPt4zG0",
                "forum": "tm8s3696Ox",
                "replyto": "QOaIdBrO9z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1615/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1615/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Window for responding and draft updating is closing"
                    },
                    "comment": {
                        "value": "Dear Reviewer #qWAT,\n\nThanks very much for your time and valuable comments. We understand you're busy. But as the window for responding and paper revision is closing, would you mind checking our response ([a brief summary](https://openreview.net/forum?id=tm8s3696Ox&noteId=jZG4pOxvTd), and [details](https://openreview.net/forum?id=tm8s3696Ox&noteId=7DFxpCioY5)) and confirm whether you have any further questions? We are very glad to provide answers and revisions to your further questions.\n\nBest regards and thanks,\n\nAuthors of #1615"
                    }
                },
                "number": 41,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1615/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700699953814,
                "cdate": 1700699953814,
                "tmdate": 1700699953814,
                "mdate": 1700699953814,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "71Irm7kP2Q",
                "forum": "tm8s3696Ox",
                "replyto": "QOaIdBrO9z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1615/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1615/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Window for responding and draft updating is closing"
                    },
                    "comment": {
                        "value": "Dear Reviewer #qWAT,\n\nThanks very much for your time and valuable comments. We understand you're busy. But as the window for responding and paper revision is closing, would you mind checking our response ([a brief summary](https://openreview.net/forum?id=tm8s3696Ox&noteId=jZG4pOxvTd), and [details](https://openreview.net/forum?id=tm8s3696Ox&noteId=7DFxpCioY5)) and confirm whether you have any further questions? We are very glad to provide answers and revisions to your further questions.\n\nBest regards and thanks,\n\nAuthors of #1615"
                    }
                },
                "number": 43,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1615/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700720431685,
                "cdate": 1700720431685,
                "tmdate": 1700720431685,
                "mdate": 1700720431685,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "v35Nk9H9RT",
            "forum": "tm8s3696Ox",
            "replyto": "tm8s3696Ox",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1615/Reviewer_WMX4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1615/Reviewer_WMX4"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes Co-Boosting to improve the performance of one-shot federated learning. The existing one-shot federated learning algorithm requires training a synthesized-sample generator on the server using the ensemble model of client models. The core ideas of this paper are twofold: 1. Fine-tuning the weights of the ensemble model using synthesized samples to improve its ability to classify these synthesized samples. 2. When training the generator, utilizing the output of the ensemble model to calculate the difficulty of samples, assigning higher loss function weights to hard samples. These two points can be iteratively applied, mutually enhancing each other. The experiments conducted on datasets like SVHN and CIFAR10 demonstrated that Co-Boosting has a certain superiority over existing approaches."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper pays attention to the relationship between generator training and ensemble model, which is an interesting perspective.\n2. The overall paper is well organized and easy to follow."
                },
                "weaknesses": {
                    "value": "1. The novelty of the proposed method is relatively insufficient. The overall method is composed of several existing tricks, lacking a comprehensive core framework or a more in-depth analysis.\n2. The one-shot federated learning encompasses various types of methods, including training generators [DENSE, FedDF], uploading distilled datasets [1], utilizing existing pre-trained models [2], and so on. However, this paper specifically focuses on methods related to training generators. This point should be clearly discussed in the introduction and related work sections.\n3. The experiments are insufficient. The most complex model architecture discussed in this paper is a 5-layer CNN, and the most complex dataset is CIFAR100. This raises doubts about the practical value of this one-shot method.\n4. In the experimental section, this paper focuses on class distribution. However, this alone may not be sufficient to demonstrate the effectiveness of the proposed approach. This is because when there are differences in class distribution, the weights of the ensemble model will naturally play a crucial role. For instance, if a model does not support category 'a', it would be necessary to reduce the weight assigned to this model. I would recommend the authors to incorporate experiments involving feature distribution differences, for instance, by utilizing the DomainNet dataset.\n\n[1] Zhou, Yanlin, et al. \"Distilled one-shot federated learning.\" arXiv preprint arXiv:2009.07999 (2020).\n\n[2] Yang, Mingzhao, et al. \"Exploring One-shot Semi-supervised Federated Learning with A Pre-trained Diffusion Model.\" arXiv preprint arXiv:2305.04063 (2023)."
                },
                "questions": {
                    "value": "See Weaknesses.\nIn Algorithm 1 Co-Boosting, should Lines 16 to 18 be placed after Line 19?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1615/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698740685788,
            "cdate": 1698740685788,
            "tmdate": 1699636089661,
            "mdate": 1699636089661,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5hVsJeNw6H",
                "forum": "tm8s3696Ox",
                "replyto": "v35Nk9H9RT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1615/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1615/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank the reviewer for taking the time to review our work. We appreciate that you find our paper well organized and easy to follow. According to your valuable comments, we provide detailed feedback.\n\n**Q1**: Novelty concern:\n> \"The novelty of the proposed method is relatively insufficient. The overall method is composed of several existing tricks, lacking a comprehensive core framework or a more in-depth analysis.\"\n\n***Ans for Q1):***\nWe apologize for the misunderstanding. In response to your valuable comments, we have added the following explanations to our revision, aiming at highlighting the novelty of this work.\n\n- We agree that we leverage a straightforward approach to realize OFL, where server model is aggregated by distilling knowledge from the ensemble as discussed in [1][3][4][5][6][7].\n- We would like to note that the main contribution is the proposed **co-boosting** approach. It is **the first time** to make the ensemble and the synthetic data mutually boost each other.\n- We believe the reviewer agrees with the motivation of the proposed method that the server model's performance is intricately linked to both the quality of synthesized data and the ensemble. Aligning with this motivation, the proposed method ensures that **the ensemble and the synthetic data are mutually promoting each other**. This dynamic interaction is the key to achieving the new state-of-the-art results, which is evidenced in Fig. 1(b)-(d) and Table 7.\n- Thanks to the simplicity design, our method **eliminates the need** for modifications to the client's local training, requires no additional data or model transmission, and accommodates heterogeneous client model architectures.\n\nWe appreciate your feedback and will enhance the manuscript by providing a more detailed analysis of how this integration uniquely contributes to the field. Additionally, we will draw clearer distinctions from existing methods to further underscore the novelty and practicality of our approach.\n\n**Q2**: More related works:\n> \"The one-shot federated learning encompasses various types of methods, including training generators [DENSE, FedDF], uploading distilled datasets [1], utilizing existing pre-trained models [2], and so on. However, this paper specifically focuses on methods related to training generators. This point should be clearly discussed in the introduction and related work sections\"\n\n***Ans for Q2):***\nThank you for highlighting the breadth of methods encompassed in OFL and for bringing our attention to the related works. Accordingly, we have added the following discussions to our revision.\n\n- We have highlighted the discussions about the mentioned approaches. Specifically, we discussed various approaches, including uploading distilled datasets approach [1], using public dataset [3], uploading clusters [4], and modified local training [5][6]. In response to your valuable comments, we have highlighted the relation and difference between our work and previous works.\n-  We appreciate your mention of utilizing existing pre-trained models [2], which adopts the concept of using a pre-trained diffusion model, to generate high-quality data. We have added it in the according section.\n-  In response to your constructive comments, we have highlighted the difference between our work and the mentioned outstanding works. Specifically, our method is based on the intuition that the server model's performance is intricately linked to both the quality of synthesized data and the ensemble. Existing methods generally focus on either enhancing the ensemble or improving the synthetic data. In contrast, we propose to intertwine the ensemble and the synthetic data, fostering **mutual enhancement** (Summarized in Table 8). We believe this finding could catalyze advancements in OFL methods by underscoring the importance of optimizing their interaction. Furthermore, as discussed in the appendix (pls also refer to Table 8), only a few methods can adapt to contemporary model market scenarios, including our proposed method, due to its inherent design."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1615/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700204949687,
                "cdate": 1700204949687,
                "tmdate": 1700204949687,
                "mdate": 1700204949687,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "x1SAGpc1jK",
                "forum": "tm8s3696Ox",
                "replyto": "v35Nk9H9RT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1615/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1615/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Welcome for more discussions"
                    },
                    "comment": {
                        "value": "Dear reviewer #WMX4,\n\nThanks for your valuable time in reviewing and insightful comments. Following your comments, we have tried our best to provide responses and revise our paper. Here is a **summary of our response** for your convenience:\n\n- (1) **Novelty concern**: We would like to note that the main contribution is the proposed co-boosting approach. It is **the first time** to make the ensemble and the synthetic data **mutually boost each other**. Our method also **eliminates the need** for modifications to the client's local training, requires no additional data or model transmission, and accommodates heterogeneous client model architectures. We add these discussions into our revision to enhance our work.\n- (2) **Related work issues**: Following your constructive comments, we have discussed related works including uploading distilled datasets approach, using public datasets, uploading clusters, modifying local training, and utilizing existing pre-trained models to highlight our novelty. We provide a comprehensive comparison and summarize them in Table 8 in response.\n- (3) **More experiments**: Thanks for your valuable comments, we have provided the inherent **model structure-agnostic** feature in a client-heterogeneous setting, results in Table 3 and Appendix B.4. We also conduct experiments to evaluate our works in larger models. Results are shown in Appendix B.5. Regarding more datasets, we operate experiments using \"Tiny-ImageNet\". Results shown in Appendix B.3 and the following illustrate that Co-Boosting outperforms existing related works.\n- (4) **Feature shift exploration**: Thank you for your insightful suggestion regarding the exploration of feature distribution differences. We have extended our experiments to include commonly used domain generalization datasets, MNIST-M and PACS. Results in Appendix B.3 and the following demonstrate the superior performance of our proposed method, which we attribute to the mutual enhancement principle inherent in our approach.\n\nWe humbly hope our response has addressed your concerns. If you have any additional concerns or comments that we may have missed in our responses, we would be most grateful for any further feedback from you to help us further enhance our work.\n\nBest regards\n\nAuthors of #1615"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1615/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700471598937,
                "cdate": 1700471598937,
                "tmdate": 1700471598937,
                "mdate": 1700471598937,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IWsp6Dy3Oy",
                "forum": "tm8s3696Ox",
                "replyto": "v35Nk9H9RT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1615/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1615/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Window for responsing and draft updating is closing"
                    },
                    "comment": {
                        "value": "Dear Reviewer #WMX4,\n\nThanks very much for your time and valuable comments. We understand you're busy. But as the window for responding and paper revision is closing, would you mind checking our response ([a brief summary](https://openreview.net/forum?id=tm8s3696Ox&noteId=x1SAGpc1jK), and [details](https://openreview.net/forum?id=tm8s3696Ox&noteId=5hVsJeNw6H)) and confirm whether you have any further questions? We are very glad to provide answers and revisions to your further questions.\n\nBest regards and thanks,\n\nAuthors of #1615"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1615/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700472087319,
                "cdate": 1700472087319,
                "tmdate": 1700472087319,
                "mdate": 1700472087319,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "H13w5d1GQR",
                "forum": "tm8s3696Ox",
                "replyto": "v35Nk9H9RT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1615/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1615/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Window for responding and draft updating is closing"
                    },
                    "comment": {
                        "value": "Dear Reviewer #WMX4,\n\nThanks very much for your time and valuable comments. We understand you're busy. But as the window for responding and paper revision is closing, would you mind checking our response ([a brief summary](https://openreview.net/forum?id=tm8s3696Ox&noteId=x1SAGpc1jK), and [details](https://openreview.net/forum?id=tm8s3696Ox&noteId=5hVsJeNw6H)) and confirm whether you have any further questions? We are very glad to provide answers and revisions to your further questions.\n\nBest regards and thanks,\n\nAuthors of #1615"
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1615/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700548819114,
                "cdate": 1700548819114,
                "tmdate": 1700548819114,
                "mdate": 1700548819114,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "o8cEBF8qJP",
                "forum": "tm8s3696Ox",
                "replyto": "v35Nk9H9RT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1615/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1615/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Window for responding and draft updating is closing"
                    },
                    "comment": {
                        "value": "Dear Reviewer #WMX4,\n\nThanks very much for your time and valuable comments. We understand you're busy. But as the window for responding and paper revision is closing, would you mind checking our response (a brief summary and details) and confirming whether you have any further questions? \n\nWe are looking forward to your kind comments and questions.\n\nBest regards and thanks,\n\nAuthors of #1615"
                    }
                },
                "number": 30,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1615/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700621241438,
                "cdate": 1700621241438,
                "tmdate": 1700621241438,
                "mdate": 1700621241438,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Jl4v34VEiK",
                "forum": "tm8s3696Ox",
                "replyto": "v35Nk9H9RT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1615/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1615/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Window for responding and draft updating is closing"
                    },
                    "comment": {
                        "value": "Dear Reviewer #WMX4,\n\nThanks very much for your time and valuable comments. We understand you're busy. But as the window for responding and paper revision is closing, would you mind checking our response (a brief summary and details) and confirming whether you have any further questions? \n\nWe are looking forward to your kind comments and questions.\n\nBest regards and thanks,\n\nAuthors of #1615"
                    }
                },
                "number": 35,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1615/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700654035517,
                "cdate": 1700654035517,
                "tmdate": 1700654035517,
                "mdate": 1700654035517,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XWMtbqkJja",
                "forum": "tm8s3696Ox",
                "replyto": "v35Nk9H9RT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1615/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1615/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Window for responding and draft updating is closing"
                    },
                    "comment": {
                        "value": "Dear Reviewer #WMX4,\n\nThanks very much for your time and valuable comments. We understand you're busy. But as the window for responding and paper revision is closing, would you mind checking our response (a brief summary and details) and confirming whether you have any further questions? \n\nWe are looking forward to your kind comments and questions.\n\nBest regards and thanks,\n\nAuthors of #1615"
                    }
                },
                "number": 40,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1615/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700699908581,
                "cdate": 1700699908581,
                "tmdate": 1700699908581,
                "mdate": 1700699908581,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ty4BYxP04W",
                "forum": "tm8s3696Ox",
                "replyto": "v35Nk9H9RT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1615/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1615/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Window for responding and draft updating is closing"
                    },
                    "comment": {
                        "value": "Dear Reviewer #WMX4,\n\nThanks very much for your time and valuable comments. We understand you're busy. But as the window for responding and paper revision is closing, would you mind checking our response (a brief summary and details) and confirming whether you have any further questions? \n\nWe are looking forward to your kind comments and questions.\n\nBest regards and thanks,\n\nAuthors of #1615"
                    }
                },
                "number": 42,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1615/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700720405783,
                "cdate": 1700720405783,
                "tmdate": 1700720405783,
                "mdate": 1700720405783,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]