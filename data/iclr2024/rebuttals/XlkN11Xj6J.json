[
    {
        "title": "Adding 3D Geometry Control to Diffusion Models"
    },
    {
        "review": {
            "id": "HkAf7DCsjE",
            "forum": "XlkN11Xj6J",
            "replyto": "XlkN11Xj6J",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2030/Reviewer_eSpe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2030/Reviewer_eSpe"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigate how to use generated images from diffusion models to improve discrimitive tasks in both in-distribution and out-of-distribution settings. The authors propose to render 3D data to 2D edge maps, fine-tune the large-scale diffusion model via ControlNet approach with the prompt augmented form LLM. After training, the generated images naturally have all the 3D object annotations. These generated data can be subsequently used as data augmentation for downstream tasks. The paper demonstrate the effectiveness of the proposed method on image classification and 3D pose estimation tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The proposed framework is simple and straightforward to use. The main technical contribution seems to be how to better use 3D data, ControlNet and LLM for data augmentation.\n- Quantitative improvement looks promising. On the evaluated task (image classification and pose estimation), the quantitative improvement seems quite obvious, espcially for the OOD settings."
                },
                "weaknesses": {
                    "value": "- The main method aims to produce (image, 3D annotation) pairs. Then is 2D image classification a good task for evaluation? The corresponding 3D ground truth is not used anywhere in this task. And you don't need 3D data to create (image, label) pairs. Even though experiments indeed show the improvement, I doubt this could be achieved without using 3D data.\n- As the main purpose is to use the generated data for downstream tasks, I think the paper needs to carefully examine the data quality and show the necessity of the proposed approach. From this aspective, some necessary ablations are missing. \n  - One simplest baseline to use 3D data is to just use the rendered image with background (e.g., random environment map). Would this kind of synthetic data improve the evaluated tasks? I think this baseline is needed to prove the necessity of using a image generative model.\n  - For image classification, a simpler approach is to just use the imagenet images, extract the edge map and then generate new image conditioned on the LLM prompt. Would this kind of synthetic data also give big improvement? This baseline is needed to show the necessity of using the 3D data, as least for the image classification task.\n- The title is a bit misleading. It seems to suggest a method that enable 3D control of the diffusion model (e.g., changing view points), but it's not. The proposed method merely use the 3D data and diffusion model to create (image, 3D annotation) pairs."
                },
                "questions": {
                    "value": "What is the prompt to LLM for enriching the description?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2030/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698612312896,
            "cdate": 1698612312896,
            "tmdate": 1699636134575,
            "mdate": 1699636134575,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UI2i0KLAk9",
                "forum": "XlkN11Xj6J",
                "replyto": "HkAf7DCsjE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2030/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2030/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author's Response to Concerns Raised by Reviewer eSpe"
                    },
                    "comment": {
                        "value": "We thank Reviewer eSpe for the valuable feedback and constructive comments. Here are our responses to each concern.\n\n&emsp;\n\n**`W1` Purpose of 2D image classification task**:\n\nIn this work, we present a synthetic data generation approach, 3D-DST, that produces synthetic images with controllable viewpoints and automatic 3D annotations at no extra cost. To demonstrate the advantage of our approach, we provide the following experimental results:\n1. Improved 2D image classification on both in-distribution and out-of-distribution datasets (Table 1) by using our 3D-DST data for pretraining. These results demonstrate that our synthetic images possess a high level of realism (for ID classification) and diversity (for OOD classification) compared to other synthetic data, such as the 2D images in Text2Img.\n2. Improved 3D pose estimation (Table 4) and improved 3D object detection (General Response 3, Section E). These results not only show the great potential of our approach on multiple 3D tasks but also verify the quality of the automatically obtained 3D annotations in 3D-DST.\n\nWe agree with the reviewer that improved 2D image classification can be achieved with standard 2D diffusion models without 3D-DST. In Table 3 we compared our approach with Text2Img that utilizes a 2D diffusion model for synthetic image generation, along with other complex modules such as language enhancement and CLIP filtering. Results show that pretraining on our 3D-DST images outperforms pretraining on Text2Img by a wide margin for both ID and OOD settings.\n\nDespite the fact that 2D image classification doesn't rely on the automatic 3D annotations generated by 3D-DST, we argue that our approach is still a compelling choice for 2D tasks. The 3D visual prompts effectively ensure the class of the salient object, and with our 3D control module, viewpoints of objects in our 3D-DST images follow a uniform distribution, as opposed to the strong biases observed from a standard image diffusion model (Figure 6).\n\n\n&emsp;\n\n**`W2` Some necessary ablations are missing**:\n\nWe thank the reviewer for pointing out the missing ablation experiments. Please refer to **General Response 4** where we present the new ablation study results.\n\n&emsp;\n\n**`W3` The title is a bit misleading**:\n\nThanks for the suggestion. We are considering using **\u201cGenerating Images with 3D Annotations using Diffusion Models\u201d** as the new title. If you have other suggestions for the title, we are very glad to modify it accordingly.\n\n&emsp;\n\n**`Q` What is the prompt to LLM for enriching the description?**\n\nIn contrast to previous works (e.g., VisProg), which directly acquire desirable outputs from LLM-powered chatbots by feeding curated prompts describing the tasks and several example outputs for in-context learning, our approach leverages the generative power of LLMs to produce a series of diverse prompts describing the target objects. More specifically, we start from \u201cA photo of a [class]\u201d, and run LLM in an auto-regressive manner. We find that LLMs are capable of giving detailed descriptions regarding the color, texture, and shape information of the object, as well as diverse scene environments and actions. As demonstrated in Figure 5, prompts like \u201cA photo of a taxicab covered in a thick layer of snow\u201d and \u201cA photo of a park bench covered in fallen leaves\u201d give diversity to the objects\u2019 appearance. And prompts like \u201cA photo of a park bench nestled under the shade of a towering oak tree\u201d enrich the background of the generated images. We will release our codebase with the implementation of our LLM generation module to reproduce our results."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2030/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700505583089,
                "cdate": 1700505583089,
                "tmdate": 1700527778612,
                "mdate": 1700527778612,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XHIOo7q5TN",
                "forum": "XlkN11Xj6J",
                "replyto": "UI2i0KLAk9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2030/Reviewer_eSpe"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2030/Reviewer_eSpe"
                ],
                "content": {
                    "title": {
                        "value": "Reply"
                    },
                    "comment": {
                        "value": "Thanks for siginificant effort and detailed response from the authors.\n\n- The provided ablation experiments are great. I'm a bit confused when looking at Figure 7, in particular the \"w imagenet edges\" row. Why are the generated images have top and down horizontal frames, like the model is generating non-square images?\n\n- The new title is better than the old one.\n\n- After reading other reviews, I kind of agree that the technical contribution is a bit limited, i.e., the method itself seems to be a direct extension to ControlNet, even though I respect the authors' claim that the whole framework (method, datasets, experiments) provides insights to the community. I'm not a expert on image classification / pose estimation tasks, so I'm not sure how such quantitative improvement should be appreciated.\n\nI'm not fully convinced to raise my score at this moment, but I do feel more positive towards this paper."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2030/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700679138189,
                "cdate": 1700679138189,
                "tmdate": 1700679138189,
                "mdate": 1700679138189,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6pVRhuuUjQ",
                "forum": "XlkN11Xj6J",
                "replyto": "HkAf7DCsjE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2030/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2030/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official comment by authors"
                    },
                    "comment": {
                        "value": "Dear Reviewer eSpe,\n\nThank you very much for the valuable feedback and constructive comments!\n\n&nbsp;\n\n`Regarding the qualitative examples in Figure 7:`\n\nThe horizontal frames are due to the ImageNet image preprocessing. Images are first upsampled and then padded to the same shape. Images with higher resolutions improve the performance of the Stable Diffusion and running diffusion models in batch mode significantly improves the data generation speed given limited time for the rebuttal experiments. In general, we find the quality of the produced images is not impacted by the image boundaries. We will re-run the experiments again without the boundaries.\n\n&nbsp;\n\nAgain, we appreciate the reviewer\u2019s continuous efforts to provide helpful suggestions and valuable input, which make the paper better. If there is any further information or clarification you need from us, please feel free to let us know!"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2030/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700688015067,
                "cdate": 1700688015067,
                "tmdate": 1700688015067,
                "mdate": 1700688015067,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TLVcBUSG6w",
            "forum": "XlkN11Xj6J",
            "replyto": "XlkN11Xj6J",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2030/Reviewer_B6MH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2030/Reviewer_B6MH"
            ],
            "content": {
                "summary": {
                    "value": "- The author introduces a method named 3D-DST, aimed at enhancing the comprehension of 3D objects by diffusion models.\n- This method comprises two modules: the \"3D Visual Prompt\" module, which utilizes edge maps as prompts derived from rendering, and the \"Text Prompt\" module, which extends prompt words through LLM.\n- Through experiments, the author demonstrates that the images generated by the proposed method, along with paired labels, serve as an effective approach for data enhancement or pre-training. This leads to improved performance in tasks such as image classification and 3D pose estimation across multiple baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is easy to understand.\n- The paper presents an approach that incorporates edge maps as additional prompts to enhance the performance of the diffusion-based method."
                },
                "weaknesses": {
                    "value": "- The framework is mainly inherited from Controlnet, so the technical contributions are limited and incremental.\n- The idea of 3D Visual Prompt via CG rendering and LLM Prompt is more like a combination of multiple previous effective techniques.\n- The author's excessive focus on introducing background knowledge of known technologies like diffusion or cross-attention is unnecessary if the method utilized in this article relies on off-the-shelf approaches. It is not recommended to extensively discuss these technologies in the main text.\n- The second challenge, \"simple text prompts,\" seems to be less directly relevant to the paper's introduction on adding 3D geometry control to diffusion models.\n- The experimental part of the paper lacks details on training the network."
                },
                "questions": {
                    "value": "How to define camera extrinsic matrix and whether to use class-level canonical-space as the extrinsic matrix of identity. If this is the case, there are many symmetric objects whose poses are ambiguous (this issue has been extensively discussed in the work on object 6dof estimation). How to define objects with multiple symmetry axes such as round tables? In addition, how to align the definition of extrinsic coordinate systems between different classes?\nIt is counterintuitive to claim that edge maps are superior to depth maps because depth maps provide more 3D information, such as occlusion relationships, which goes beyond the 2D representation of edge maps. The conclusions presented in the author's paper are difficult to support with only a few selectively chosen qualitative examples."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics review needed."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2030/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2030/Reviewer_B6MH",
                        "ICLR.cc/2024/Conference/Submission2030/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2030/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698672376944,
            "cdate": 1698672376944,
            "tmdate": 1700983777197,
            "mdate": 1700983777197,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "waAqAunlWQ",
                "forum": "XlkN11Xj6J",
                "replyto": "TLVcBUSG6w",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2030/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2030/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author's Response to Concerns Raised by Reviewer B6MH (1/2)"
                    },
                    "comment": {
                        "value": "We thank Reviewer B6MH for the valuable feedback and constructive comments, and we address the weaknesses below.\n\n&emsp;\n\n**`W1` Limited technical contributions**:\n\nThanks to the reviewer for the comments. Please refer to **General Response 2** where we summarize our results and highlight our contributions.\n\n&emsp;\n\n**`W2&3` The idea of 3D Visual Prompt via CG rendering and LLM Prompt is more like a combination of multiple previous effective techniques. The author's excessive focus on introducing background knowledge of known technologies like diffusion or cross-attention is unnecessary if the method utilized in this article relies on off-the-shelf approaches**:\n\nWe respectfully disagree with the reviewer\u2019s biased opinions against works built on existing methods. A successful work builds on existing technologies but solves new and challenging problems [A]. In this work, we show that with the help of diverse LLM prompts and controllable visual prompts, we can obtain high-quality images with 3D annotations. We demonstrate that using 3D-DST data for pretraining can achieve improved performance for image classification, 3D pose estimation, and 3D object detection. By combining graphics-based rendering with strong generative power of diffusion models and our diverse LLM prompts, we achieve promising results on both in-distribution data and out-of-distribution data.\n\nIn Section 3.1 we discussed the background of existing generative methods (diffusion models and ControlNet) for readers lack of related context, which is a common practice. Moreover, we discussed the limitations of existing methods and presented the motivation of our work, as a way to highlight our contributions. Given the reviewer\u2019s advice, we will re-organize Section 3.1 to make the contents more succinct and highlight our key ideas.\n\n[A] T Gupta et al. Visual programming: Compositional visual reasoning without training. In CVPR, 2023.\n\n\n&emsp;\n\n**`W4` Simple text prompts seem to be less directly relevant to the paper's introduction on adding 3D geometry control to diffusion models**:\n\nWe acknowledge that the current introduction in the manuscript was not very clear about the challenges with \u201csimple text prompts\u201d and the motivation of our \u201cdiverse text generation\u201d. We have revised this in the next revision. Here we would like to make two arguments motivating the necessity of LLM prompts.\n\n1. Unlike typical 2D diffusion models such as Stable Diffusion, introducing 3D visual prompts as a condition to diffusion models largely limits the diversity of background and appearances, as well as the richness of details. This motivates us to encourage the diversity of output images from textual inputs, leading our proposed approach of generating diverse text prompts with LLMs. We qualitatively compare the generated images with and without LLM prompts in Figure 5.\n\n2. In terms of the purpose of our work, we aim to develop a synthetic data generation method with 3D annotations that demonstrates better realism and diversity compared to graphics-based rendered data. Thus diverse LLM prompt serves as a core module of our approach and is key to achieving improved performance on OOD data in downstream tasks as presented in Table 3.\n\n&emsp;\n\n**`W5` Training details of networks.**:\n\nGiven the limited space in the main text, we move some of the model training specifics from Section 4.1 \u201cImplementation details'' to Section B in the supplementary materials. In Section 4.1 we report the network architectures, model sizes, and synthetic data size. In Section B of supplementary materials, we go through the choice of hyperparameters and training settings for both image classification and 3D pose estimation. Hyperparameters not discussed in our manuscript follow the choices from the publicly released code of DeiT, ConvNeXt, etc. We are happy to improve the reproducibility of our experiments so please let us know if additional details should be covered in our revision."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2030/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700506314966,
                "cdate": 1700506314966,
                "tmdate": 1700527633034,
                "mdate": 1700527633034,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "orAO9NlxlZ",
                "forum": "XlkN11Xj6J",
                "replyto": "TLVcBUSG6w",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2030/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2030/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author's Response to Concerns Raised by Reviewer B6MH (2/2)"
                    },
                    "comment": {
                        "value": "We answer the questions raised by reviewer B6MH below.\n\n&emsp;\n\n**`Q1` How to define camera extrinsic matrix and whether to use class-level canonical-space as the extrinsic matrix of identity. How to define objects with multiple symmetry axes such as round tables?**\n\nOur 3D-DST generation pipeline saves pose annotations in terms of the 3D viewpoints (azimuth, elevation, and in-plane rotation), as well as object distance, object center, and Blender camera parameters. This follows the convention in standard pose estimation datasets, such as PASCAL3D+ and ObjectNet3D. Similar to other 3D datasets such as Omni3D, the intrinsic matrix can be obtained from the focal length and principal point of the camera, and the extrinsic matrix is simply an identity matrix as we use a canonical camera. In 3D-DST, the object rotation matrix can be computed from the viewpoint parameters.\n\nMoreover, as all CAD models share the same annotation convention, we further ensure the T-pose of all CAD models are aligned both in-category and across-category. For instance, the \u201cfront\u201d of a vehicle and the \u201cfront\u201d of a dog would face the same direction as they share similar semantic parts. This allows 3D models trained on our synthetic 3D-DST data to exploit semantic similarities across categories.\n\nRegarding how to define objects with multiple symmetry axes such as round tables, we simply follow the convention in PASCAL3D+ and ObjectNet3D and set the azimuth rotation to zero. This applies to both rigid objects commonly used for pose estimation, such as round tables, bowls, and bottles, as well as other objects less used, such as apples and golf balls. As mentioned by the reviewer, these symmetry issues have been well-studied by previous works in pose estimation. In this work, we simply follow the convention in downstream tasks and we find models pretrained on our 3D-DST data working fairly well for both 3D pose estimation (Table 4) and 3D object detection (General Response 3, Section E).\n\n&emsp;\n\n**`Q2` It is counterintuitive to claim that edge maps are superior to depth maps because depth maps provide more 3D information, such as occlusion relationships, which goes beyond the 2D representation of edge maps.**\n\nWe acknowledge that further study is necessary regarding the choice of edge maps or depth maps as the 3D visual prompts. We make the following observations:\n\n1. The current inferior results when using depth maps as 3D visual prompts are attributed to the domain gap between the MiDaS depths used for training and the depths used for inference. Running MiDaS on synthetic images or running diffusion models conditioned on synthetic depths both result in large domain gaps with the MiDaS depth maps obtained from real images during training. We believe certain finetuning is necessary to close the domain gap and to fully demonstrate the advantage of depth maps over edge maps. We leave this as an interesting direction for future work.\n2. Although edge maps provide less 3D information than depth maps, it is still a compelling choice for 2D image generation despite its simplicity. With the scale-distance ambiguity, it abstracts away the need to sample a wide range of scales and distances during rendering, and directly provides necessary shape information that has been \u201cprojected\u201d onto the 2D plane. Moreover, results on scene image generation with 3D-DST (in Section E of supplementary materials) also show that edge maps work fairly well for multiple objects with mutual occlusion."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2030/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700506435944,
                "cdate": 1700506435944,
                "tmdate": 1700527824062,
                "mdate": 1700527824062,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Tjt2WQQJIM",
                "forum": "XlkN11Xj6J",
                "replyto": "orAO9NlxlZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2030/Reviewer_B6MH"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2030/Reviewer_B6MH"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors,\n\nThank you for your response.\n\nI have read all the reviewers' comments and the authors' rebuttal and some of my concerns are addressed, e.g. the 3D pose annotations. However, the authors' responses to W4 and Q2 are not entirely convincing to me. Regarding the technical contribution and novelty of the paper, I respect the authors' perspective on 'building upon existing technology' in their response to W2 & W3. However, I want to point out that the contribution summarized from the current structure of the paper (i.e., the 3D-DST framework) seems more like an extension of ControlNet rather than a complete work.\n\nI appreciate the effort made by the authors in the rebuttal, but I believe that for the paper to be more self-consistent, further improvements and supplements are needed in the writing and experimental sections. For example, addressing the misleading aspects of the paper title mentioned by other reviewers, or their concerns about consistency.\n\nEven though I do not consider the current state of the paper to reach an acceptable standard, I will also consider the opinions of other reviewers to decide the final score."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2030/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700625913961,
                "cdate": 1700625913961,
                "tmdate": 1700625913961,
                "mdate": 1700625913961,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "aEvidPVvdB",
            "forum": "XlkN11Xj6J",
            "replyto": "XlkN11Xj6J",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2030/Reviewer_5ugh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2030/Reviewer_5ugh"
            ],
            "content": {
                "summary": {
                    "value": "A method to add 3D geometry control into the image generation process. To achieve the goal, three key techniques are leveraged, including 2D edge maps generation with 3D annotations via rendering, text prompt generation for improving the diversity, and conditional image generation from edge maps and text descriptions. The method can be utilized as a data augmentation strategy for many downstream tasks such as image classification. Experiments can show its promising application potential."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Adding 3D geometry control via 2D edge maps and text descriptions is interesting and reasonable. This way the generative model only needs to deal with controlling information represented in 2D images and texts. Then many existing powerful techniques can be leveraged for controllable generation. \n- The proposed method is reasonable. It can achieve plausible controllable and diverse generation results. Generated images are of good quality and well-related to edge prompts and text conditions. \n- The method can serve as a promising data augmentation strategy for many downstream tasks. It is a promising way to generate diverse 2D images with 3D information annotation."
                },
                "weaknesses": {
                    "value": "- The technical significance is relatively limited. The problem of generating 2D edge maps from 3D models and generating text prompts from 3D CAD models can be solved by existing techniques. Though the idea is interesting, no new techniques are proposed. The overall method is rather like an application-guided strategy. Though with promising application potential, it is hard to say what general principles that can guide the research in other domains can be distilled from the paper. \n- It is not sure whether the generated images are very faithful to the edge maps conditions. For example, there is no good guarantee that the objects in the generated images are consistent with the geometry described via the edge maps."
                },
                "questions": {
                    "value": "- Evaluations on whether the generated images are faithful to images and text conditions. \n- It would be better if more potential applications could be discussed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2030/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2030/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2030/Reviewer_5ugh"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2030/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698737109058,
            "cdate": 1698737109058,
            "tmdate": 1699636134419,
            "mdate": 1699636134419,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "66oPPiXWug",
                "forum": "XlkN11Xj6J",
                "replyto": "aEvidPVvdB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2030/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2030/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 5ugh"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the valuable feedback and constructive comments. Here are our responses to each question.\n\n&nbsp;\n\n**`[Q1]` Limited technical significance.**\n\nThanks to the reviewer for the comments. Please refer to General Response 2 where we summarize our results and highlight our contributions.\n\n&nbsp;\n\n**`[Q2]` Though with promising application potential, it is hard to say what general principles that can guide the research in other domains can be distilled from the paper.**\n\nIn general, our 3D-DST presents the following conclusions and guidelines that may benefit other 3D-from-2D tasks:\n\n1. By leveraging the strong generative power of diffusion models, our 3D-DST data generation approach is an effective way to produce images with 3D annotations that demonstrate a higher level of realism and diversity. We demonstrate improved performance on both indoor and outdoor scenes with single-object or multiple-object images. These results show that 3D-DST is a generic approach with great potential in a wide range of 3D-from-2D tasks.\n2. Despite the availability of 3D annotations from synthetic data, previous synthetic data generation methods were often hindered by the domain gap between synthetic and real, and the huge efforts to produce complex textures, realistic materials, and complex lighting conditions in graphics-based rendering. 3D-DST addresses the above problems with the powerful generative capabilities of diffusion models and allows us to quickly generate synthetic images for various 3D-from-2D tasks. In this work, we only consider existing 3D annotations from real images, but a lot of more useful 3D annotations as by-products can be exploited in future works.\n3. Using graphics-rendered images is widely exploited in previous works as an effective approach to improve models\u2019 performance on standard benchmarks. However, with 3D-DST, we argue that generating diverse synthetic images for pretraining is also an important approach to improve models\u2019 robustness on OOD data, as demonstrated by 3D pose estimation on OOD data in Table 4. This is accomplished by our proposed 3D-DST pipeline and is not possible from previous synthetic data methods.\n\n&nbsp;\n\n**`[Q3]` There is no good guarantee that the objects in the generated images are consistent with the geometry described via the edge maps. Evaluations on whether the generated images are faithful to images and text conditions.**\n\nThanks to the reviewer for raising this important question. We develop two metrics to measure the quality of the 3D annotations, namely real-to-synthetic performance (R2S) and pose consistency score (PCS). Moreover, we develop a simple yet effective approach, K-fold consistency filter (KCF) to remove images likely with wrong 3D annotations. We present some quantitative results and qualitative examples to demonstrate the quality of 3D annotations in our 3D-DST data. For future revision, we will conduct a thorough analysis and involve human comparison to quantitatively study the quality of 3D annotations in our 3D-DST data, as well as the efficacy of KCF. Please refer to General Response 1 and Section F for detailed results and discussions.\n\n&nbsp;\n\n**`[Q4]` It would be better if more potential applications could be discussed.**\n\nWe agree with the reviewer that more 3D applications should be discussed. Therefore in General Response 3 and Section E in supplementary materials, we present the qualitative and quantitative results of 3D object detection with 3D-DST. Compared to other results shown in our paper, generating synthetic data for 3D object detection (i) involves scene image generation with multiple objects from multiple categories, and (ii) requires both 2D bounding box and 3D bounding box annotations to be accurate enough. Results show that our 3D-DST can also produce good scene images with multiple objects and diverse viewpoints, and state-of-the-art models pretrained on our 3D-DST scene images achieve improved performance on downstream tasks."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2030/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700505654672,
                "cdate": 1700505654672,
                "tmdate": 1700527526365,
                "mdate": 1700527526365,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tbBkR8I9kY",
                "forum": "XlkN11Xj6J",
                "replyto": "3CGuPkreSj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2030/Reviewer_5ugh"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2030/Reviewer_5ugh"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors, \n\nThanks for your responses. While I still value the concept presented in the paper, I maintain my initial reservations regarding its limited contribution. I appreciate your dedicated efforts in addressing my concerns."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2030/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740258584,
                "cdate": 1700740258584,
                "tmdate": 1700740258584,
                "mdate": 1700740258584,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3hO5sdQdD2",
            "forum": "XlkN11Xj6J",
            "replyto": "XlkN11Xj6J",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2030/Reviewer_Xyyh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2030/Reviewer_Xyyh"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a novel method for adding 3D geometry control to diffusion models such that recognition models pre-trained on diffusion models generated synthetic data and then trained on target datasets have performance gains on classic tasks like 2D image classification and 3D pose estimation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The idea in this paper is neat, simple yet effective.\n- The idea is also very novel.\n- The empirical improvements on ImageNet classification and pose estimations are solid, significant, and surprising."
                },
                "weaknesses": {
                    "value": "- In table 4, why the baseline result NeMo w/ AugMix is missing?\n- Could you discuss or ablate using other rendering types other than canny edges? Does canny edges work the best and why?\n- There is no discussion for limitations."
                },
                "questions": {
                    "value": "see weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2030/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698821320994,
            "cdate": 1698821320994,
            "tmdate": 1699636134319,
            "mdate": 1699636134319,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BBY8gvBfNt",
                "forum": "XlkN11Xj6J",
                "replyto": "3hO5sdQdD2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2030/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2030/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Xyyh"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the valuable feedback and constructive comments. Here are our responses to each question.\n\n&nbsp;\n\n**`[Q1]` The baseline result NeMo w/ AugMix is missing.**\n\nThe baseline result of NeMo w/ AugMix is missing due to limited time and computing resources when we prepare the manuscript. We have added this baseline experiment and updated the results in our revision (Table 4).\n\n&nbsp;\n\n**`[Q2]` Could you discuss or ablate using other rendering types other than canny edges? Does canny edges work the best and why?**\n\nIn Section 4.3 and Figure 4, we visualized some examples and discussed the choice of different rendering types. In general, we find using edges as visual controls works the best. We acknowledge the fact that using depth maps could potentially provide useful 3D information for 2D image generation. However, this approach is mainly hindered by the domain gap between the synthetic depths (during inference) and MiDaS depths from real images (during training). Meanwhile, canny edges as visual prompts are a compelling choice despite its simplicity. With the scale-distance ambiguity, it abstracts away the need to sample a wide range of scales and distances during rendering, and directly provides necessary shape information that has been \u201cprojected\u201d onto the 2D plane. Moreover, results on scene image generation with 3D-DST (in Section E of supplementary materials) also show that edge maps work fairly well for multiple objects with mutual occlusion.\n\n&nbsp;\n\n**`[Q3]` Missing discussion for limitations.**\n\nThank the reviewer for pointing out this issue. We will involve a thorough analysis of limitations in our next revision. Here we briefly summarize our limitations:\n\n* **Limitations of 3D models:** Our data generation pipeline starts with the rendering of 3D object models. Despite the availability of large 3D model datasets, the abundance and diversity of 3D models can be limited for specific categories, such as gyromitra (a type of fungi) or plastic bags.\n* **Privacy and copyright issues:** As our model is built on large pretrained generative models, e.g., Stable Diffusion, the generated dataset may reflect data presented in the training set, violating privacy or copyright agreements.\n* **Biases in training data of generative model:** Although our model can generate diversified viewpoints and distances by adding 3D control to the data generation process, our pipeline may suffer from other biases inherent in the training data. As the Stable Diffusion model is trained on LAION-2B, known biases of the dataset may be observed in the 3D-DST dataset."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2030/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700505322126,
                "cdate": 1700505322126,
                "tmdate": 1700527416124,
                "mdate": 1700527416124,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]