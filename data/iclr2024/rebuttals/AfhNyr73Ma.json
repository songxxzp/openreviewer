[
    {
        "title": "General Stability Analysis for Zeroth-Order Optimization Algorithms"
    },
    {
        "review": {
            "id": "J0vZItzHur",
            "forum": "AfhNyr73Ma",
            "replyto": "AfhNyr73Ma",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5497/Reviewer_N2Y8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5497/Reviewer_N2Y8"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers the generalization bound of zero-order methods and presents a general analysis framework. Results on 2-point, 1-point, and coordinate-wise gradient estimators are established, which improves the existing work in this direction. Based on these results, the authors show that coordinate estimation leads to tighter generalization bounds for many zeroth-order methods. Experiments are also provided to verify the theoretical conclusions."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is well-presented and well-organized. The motivation, technique, and results are clearly stated.\n\n2. The framework established is a nice theoretical contribution. Consequently, the generalization bounds of many zero-order methods are developed. Moreover, the differences between these generalization bounds are captured and analyzed, making the whole theory complete and convincing.\n\n3. The technique used in the paper is solid and interesting."
                },
                "weaknesses": {
                    "value": "It would be better if the author emphasized the new technique and idea used in this paper compared with the existing work by Nikolakakis et al. (2022)."
                },
                "questions": {
                    "value": "In Figure 1 (b), why did the generalization bound of ZO-SVRG decrease at the beginning?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "Non."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5497/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697700825293,
            "cdate": 1697700825293,
            "tmdate": 1699636561995,
            "mdate": 1699636561995,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ofoxvREoRe",
                "forum": "AfhNyr73Ma",
                "replyto": "J0vZItzHur",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5497/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5497/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To N2Y8"
                    },
                    "comment": {
                        "value": "**Q1:**\nIt would be better if the author emphasized the new technique and idea used in this paper compared with the existing work by Nikolakakis et al. (2022).\n\n**A1:**\nThanks for your valuable comments. There exist  key differences between (Nikolakakis et al. 2022) and our work, which are summarized as below:\n\n1) Objective function: **ZO-SGD with 2-point gradient estimator Vs. general ZO optimization.** Nikolakakis et al. (2022) merely considered the generalization bound of ZO-SGD with 2-point gradient estimator. In this paper, we established a unified generalization analysis framework  for  ZO-GD, ZO-SGD as well as ZO-SVRG algorithms with 1-point / 2-point / coordinate-wise gradient estimators, which cover the mainstream estimators in existing zeroth-order algorithms.\n\n2) Conditions: **Gaussian distribution and non-convex condition Vs. general distribution and convex conditions**.  Nikolakakis et al. (2022) just considered the $u_k\\sim\\mathcal{N}\\left(0, I_d\\right)$ and non-convex loss function. In this paper, we derive the generalization bounds for more distributions (e.g.,uniform $u_k\\sim \\mathcal{U}\\left(\\sqrt{d+2} \\mathbb{B}^d\\right)$, $u_k\\sim \\mathcal{U}\\big(\\sqrt{d} \\mathbb{S}^{d-1}\\big)$ see **Appendix E**) and wider convexity conditions (strongly convex see **Theorem 3,6,9 and Appendix G, H, I**, convex see **Theorem 3,6,9 and Appendix G,H, I** and non-convex **Theorem 1-9**).\n\n3)  **Filling the gap on the  generalization bounds of ZO-SVRG.** ZO-SVRG is a  stage-wise algorithm with multiplication and summation from previous stages, which is totally different from ZO-SGD. To the best of our knowledge, this paper is the first endeavor to investigate the ZO-SVRG's generalization behavior. Meanwhile, our derived estimations of ZO-SGD $\\Big({O}\\big(T^{\\frac{\\beta C}{\\beta C + 1}}/n\\big)\\Big)$ are tighter than the related results $\\Big({O}\\big(T/n\\big)\\Big)$ in Nikolakakis et al. 2022.\n\nWe have polished the comparisons with the related works and highlighted our contributions (see **Appendix J**). \n\n\n\n**Q2:**\nIn Figure 1(b), why did the generalization bound of ZO-SVRG decrease at the beginning?\n\n**A2:**\nAs shown in Figure 1(b), ZO-SVRG still lies in the fitting stage at the beginning. The training loss, testing loss as well as their difference are all declining due to random initialization.\n\nTo enrich the empirical evaluations, we have incorporated new results in the revision in **Section 5 Numerical Experiments and Appendix B**. Particularly, \nwe have compared the generalization gaps of several estimators for zeroth-order algorithms (see **Figure 2**). \nExtended experiments verify our theoretical findings on generalization bounds."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5497/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700490908246,
                "cdate": 1700490908246,
                "tmdate": 1700490908246,
                "mdate": 1700490908246,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LwnAe9YlkH",
            "forum": "AfhNyr73Ma",
            "replyto": "AfhNyr73Ma",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5497/Reviewer_x656"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5497/Reviewer_x656"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, a new simple framework for analyzing the generalization error of zeroth order optimization for Lipschitz and smooth objective functions is proposed and several novel generalization bounds are provided. The framework proposed consists of a method for analyzing the stability of a given optimization algorithm, based on the notions of boundedness and expansivity of the corresponding update rule. The update rules considered all correspond to zeroth order analogues of first order optimization algorithms, where a first order oracle is approximately simulated using queries to the values of the objective function (i.e., a zeroth order oracle). The simulated oracles considered are based on either 1-point, 2-point or coordinate-wise approximation, and the latter achieves the tightest generalization bounds for each of the algorithms considered, matching, in each case, the best known bounds achievable by the corresponding first order algorithms in each of the cases where the objective is strongly convex, convex or non-convex. The algorithms considered are Gradient Descent, Stochastic Gradient Descent as well as Stochastic Variance Reduced Gradient method. The theoretical guarantees of the paper are accompanied by experimental results on real world data."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The results provided in this paper, which are obtained by leveraging a simple yet powerful framework and the idea to approximate the gradient of a smooth and Lipschitz function coordinate-wise, are strong and, to the best of my knowledge, novel. The presentation of the results is clear and detailed."
                },
                "weaknesses": {
                    "value": "One potential weakness of the paper is that the results assume that the objective function is smooth, which, in many important optimization problems (e.g., learning ReLU networks) is not true."
                },
                "questions": {
                    "value": "Could your results be extended to the case where the objective function is not smooth? Are there any generalization bounds for this setting, even for first order algorithms?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5497/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5497/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5497/Reviewer_x656"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5497/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698527338817,
            "cdate": 1698527338817,
            "tmdate": 1699636561898,
            "mdate": 1699636561898,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "opUlENm6NV",
                "forum": "AfhNyr73Ma",
                "replyto": "LwnAe9YlkH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5497/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5497/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To x656"
                    },
                    "comment": {
                        "value": "**Q1:**\nCould your results be extended to the case where the objective function is not smooth? Are there any generalization bounds for this setting, even for first order algorithms?\n\n**A1:**\nThanks for your constructive comments. Indeed, there exist several works on non-smooth settings for first-order optimization, see e.g., [Ref1][Ref2]. However, these researches are limited to the convex or weakly convex settings. \nMoreover, the absence of smooth condition posed challenges on bounding the approximation error induced by zeroth-order optimization. Our analysis framework established here can not be extended to the non-smooth setting directly.  \nWe are striving to address this issue in the future work.\n\n\nWe also added the related discussions in the revised paper, see **new Remark 3 at Appendix J**.\n\n\n\n[Ref1] Bassily R, Feldman V, Guzm\u00e1n C, et al. Stability of stochastic gradient descent on nonsmooth convex losses, NeurIPS, 2020.\n \n[Ref2]Lei Y. Stability and generalization of stochastic optimization with nonconvex and nonsmooth problems, ICML, 2023."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5497/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700490851143,
                "cdate": 1700490851143,
                "tmdate": 1700490851143,
                "mdate": 1700490851143,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "32zEjuMe5z",
                "forum": "AfhNyr73Ma",
                "replyto": "opUlENm6NV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5497/Reviewer_x656"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5497/Reviewer_x656"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. My rating remains the same."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5497/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700608785508,
                "cdate": 1700608785508,
                "tmdate": 1700608785508,
                "mdate": 1700608785508,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "oomD7YOMY8",
            "forum": "AfhNyr73Ma",
            "replyto": "AfhNyr73Ma",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5497/Reviewer_qUEd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5497/Reviewer_qUEd"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on zeroth-order optimization algorithms. While previous stability analysis results were limited to the basic 2-point zeroth-order estimate with a Gaussian distribution in stochastic gradient descent (SGD) algorithms, this paper introduces a general proof framework for stability analysis. This framework is applicable to convex, strongly convex, and non-convex conditions and provides results for various zeroth-order optimization algorithms, including SGD, gradient descent (GD), and stochastic variance-reduced gradient (SVRG) methods. It also covers different zeroth-order estimates, such as 1-point and 2-point estimates with various distributions and coordinate estimates. The general analysis reveals that coordinate estimation can lead to improved generalization bounds for SGD, GD, and SVRG versions of zeroth-order optimization algorithms by reducing the expansion in stability analysis."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper exhibits a well-organized structure, including a clear motivation, an extensive literature review, and a rigorous theoretical analysis. However, I have not verified the validity of all statements in the Appendix.\n- One intriguing and original contribution of the paper is its theoretical assertion that coordinate estimation can enhance the generalization bounds for zeroth-order optimization algorithms like SGD, GD, and SVRG."
                },
                "weaknesses": {
                    "value": "- Theoretical contributions, especially those stemming from the primary theoretical lemmas (Lemma 3 and 4), are incremental compared to prior works such as (Hardt et al., 2016) and (Nikolakakis et al., 2022).\n- The paper could benefit from more extensive numerical experiments and a more detailed implementation section. Specifically, it should include a comparison of the performance of various zeroth-order estimators, providing empirical support for the favorable theoretical results associated with coordinate estimation.\n\n(Hardt et al., 2016) Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic gradient descent. In International conference on machine learning, pp. 1225\u20131234. PMLR, 2016.\n\n(Nikolakakis et al., 2022) Konstantinos Nikolakakis, Farzin Haddadpour, Dionysis Kalogerias, and Amin Karbasi. Black-box generalization: Stability of zeroth-order learning."
                },
                "questions": {
                    "value": "Majors:\n\n1. What is the zeroth-order estimator used in the experiments?\n2. How do different zeroth-order estimators affect the optimization performance?\n\nMinors:\n- Please indicate beta and C in Table 1.\n- Figure 2(b):  Generalization error: GD - >  Generalization error: SGD?\n- More implementation details are needed.\n- Utilize parenthetical citations to enhance readability."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5497/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5497/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5497/Reviewer_qUEd"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5497/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698641321919,
            "cdate": 1698641321919,
            "tmdate": 1700554816819,
            "mdate": 1700554816819,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "z10ANfpWXX",
                "forum": "AfhNyr73Ma",
                "replyto": "oomD7YOMY8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5497/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5497/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer qUEd"
                    },
                    "comment": {
                        "value": "**Q1:**\nTheoretical contributions, especially those stemming from the primary theoretical lemmas (Lemma 3 and 4), are incremental compared to prior works such as (Hardt et al., 2016) [2] and (Nikolakakis et al., 2022) [1].\n\n**A1:**\nThanks for your constructive comments. The zeroth-order optimization often is considered as an approximation of the first-order optimization. Naturally, it enjoys some similar strategy for error analysis.  However, compared to prior works (e.g., Hardt et al., 2016 [Ref1], Nikolakakis et al., 2022 [Ref2]), there are key differences stated as below:\n\n1) **Unified generalization analysis framework for zeroth-order optimization.**\nThe existing analysis techniques (Hardt et al., 2016 [Ref1], Nikolakakis et al., 2022[Ref2]) mainly focus on ZO-SGD, and  can not be  applied to the other zeroth-order optimization approach (e.g., ZO-SVRG) directly.  And our proof framework can encompass the generalization analysis of ZO-GD, ZO-SGD, and ZO-SVRG algorithms under various convexity conditions. \n\n\n\n2) **First stability analysis for  zeroth-order SVRG algorithm.** \nIn contrast to SGD and GD algorithms, SVRG is a stagewise algorithm with new expansion factors among each stage and its stability-based generalization is unexplored for both first-order and zeroth-order settings. The update strategy of SVRG makes   the existing theoretical techniques  in [Ref1] and [Ref2] are not applicable directly. Here, we fill this theoretical gap by establishing  stability-based error bounds for zeroth-order SVRG,   where analysis technique are developed by the fine-grained  error decomposition and estimations.  \n\n\n \n3) **Diverse estimations for zeroth-order approximation.** Nikolakakis et al. 2022 [Ref2] only considered the two-point estimator under Gaussian distribution. In contrast, this paper investigates one-point, two-point, and coordinate-wise estimators under different distributions and two difference forms. Theoretical results demonstrate that the smaller approximation error of these estimators are, the better generalization performance they get. The coordinate-wise estimators usually enjoy smaller approximation error. In addition, compared to first-order algorithms, coordinate-wise estimators can achieve similar generalization results under different convexity conditions.\nIn particular,  we have empirically verified that the zeroth-order algorithms with coordinate-wise estimator usually own the smallest generalization bounds compared with those using other estimators.\n\nIn summary, this paper provides a unified framework for generalization analysis of zeroth-order optimization and fills the theoretical gap on the stability-based generalization for SVRG algorithm. Based on your important comments, we have added the above illustrations in (**Section 1: Introduction** and **Appendix J** ) to highlight our work clearly. \n\n\n [Ref1] M. Hardt et al., Train faster, generalize better: Stability of stochastic gradient descent, International conference on machine learning (ICML), 2016.\n \n[Ref2] K. Nikolakakis et al., Black-box generalization: Stability of zeroth-order learning. Advances in Neural Information Processing Systems (NeurIPS), 2022.\n\n\n\n**Q2:**\nThe paper could benefit from more extensive numerical experiments and a more detailed implementation section.\n\n**A2:**\nThanks for your constructive comments. In the revised version, we have conducted more numerical experiments to verify the theoretical results, where the implementation details are also provided (see **Section 5.1 Experimental Setups**). \n\nFirstly, in **Section 5.2**, we compare the generalization errors of different ZO algorithms associated with the same gradient estimators. Experimental results in **Figure 1** show that ZO-SGD can achieve the competitive generalization performance compared with ZO-GD and ZO-SVRG.\n\nSecondly, we compare the generalization errors of same ZO algorithms with  different gradient estimators in  **Section 5.3 and Appendix B**. Experimental results in **Figure 2** demonstrate that ZO algorithms with coordinate-wise gradient estimator usually have the lowest generalization error.\n\n**Q3:**\nWhat is the zeroth-order estimator used in the experiments?\n\n**A3:**\nIn the experiments, we employ the coordinate-wise estimator  as the zeroth-order estimator.  Implementing code can be found in  the **Supplementary Material**, lines 45-59 of the algs.py file."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5497/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700490977778,
                "cdate": 1700490977778,
                "tmdate": 1700490977778,
                "mdate": 1700490977778,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FP8ZOuj4fq",
                "forum": "AfhNyr73Ma",
                "replyto": "mspq1QbWPF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5497/Reviewer_qUEd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5497/Reviewer_qUEd"
                ],
                "content": {
                    "comment": {
                        "value": "Thank the authors for the response! The response and the revision has addressed all my concerns. As an result, I increase my rating to 6."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5497/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700554795572,
                "cdate": 1700554795572,
                "tmdate": 1700554795572,
                "mdate": 1700554795572,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9M9VpLx9Sp",
            "forum": "AfhNyr73Ma",
            "replyto": "AfhNyr73Ma",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5497/Reviewer_PNwi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5497/Reviewer_PNwi"
            ],
            "content": {
                "summary": {
                    "value": "In this submission, the authors proposed a general frame-work to prove the generalization error for the zero-order optimization methods. They proved the generalization error bounds for different zero-order optimization algorithms such as SG, GD and SVRG under different convexity conditions. They also conduct numerical experiments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This submission is very clear with simple structures and languages. The main idea is natural and understandable. The mathematical and theoretical analysis is strict and the empirical results are consistent with the theoretical analysis."
                },
                "weaknesses": {
                    "value": "There is no significant weakness for this submission. \n\nOnly one question is that for the ZO-SGD, ZO-GD and ZO-SVRG with one-point and two-point gradient estimation, the authors only presented the results for the none-convex setting. Is there any theoretical results for the general convex and strongly convex settings? Like the results for the coordinate-wise gradient estimation. If the authors could add these theoretical results, it could make the theoretical contributions much more complete."
                },
                "questions": {
                    "value": "Please check the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5497/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5497/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5497/Reviewer_PNwi"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5497/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698783421598,
            "cdate": 1698783421598,
            "tmdate": 1699636561690,
            "mdate": 1699636561690,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BxXzGn5ZKf",
                "forum": "AfhNyr73Ma",
                "replyto": "9M9VpLx9Sp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5497/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5497/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer PNwi"
                    },
                    "comment": {
                        "value": "**Q1:**\nOnly one question is that for the ZO-SGD, ZO-GD and ZO-SVRG with one-point and two-point gradient estimation, the authors only presented the results for the non-convex setting. Is there any theoretical results for the general convex and strongly convex settings? Like the results for the coordinate-wise gradient estimation. If the authors could add these theoretical results, it could make the theoretical contributions much more complete.\n\n**A1:**\nThanks for your constructive comments.convex and strongly convex conditions. \nFollowing your  valuable suggestions, we have added the corresponding generalization bounds  under convex and strongly convex conditions. For details, please refer to **Appendix (F.1, F.2, G.1, G.2, H.2)**.  We expect that the additional results can make our analysis more complete and improve its readability. \nExplicitly, the generalization bounds for ZO-SGD are  $O(T/n^{1-\\frac{c}{d^2 L C \\sqrt{n}}})$ and  $O(T/n^{1-\\frac{c}{d^2 L C \\sqrt{n}}})$  under one-point strongly convex condition and one-point convex conditions respectively. Meanwhile, generalization bounds for ZO-SGD are $O(T/n)$ under two-point strongly convex condition  and $O(T/n)$ under two-point convex condition. For ZO-GD, we present the generalization bounds $O(T^{C}/n)$ under one-point strongly convex condition, $O(T^{C}/n)$ under one-point convex condition, $O(T^{\\beta C}/n)$ under two-point strongly convex condition, and $O(T^{\\beta C}/n)$ under two-point convex condition. Similarly, ZO-SVRG exhibits generalization bounds $O(S^{3 C}/n)$ under one-point strongly convex condition, $O(S^{3 C}/n)$ under one-point convex condition, $O(S^{3 \\beta C}/n)$ under two-point strongly convex condition, and $O(S^{3\\beta C}/n)$ under two-point convex condition. Indeed, there are some preliminary discussions in Remark 1 in the original version.\n\nIn essential, the convexity property has impact  on the expansion coefficient $\\eta$ involving in the generalization bounds of first-order optimization methods (see **Appendix (Lemma 6)**) . It is easy to deduce that $\\eta_t=1-\\frac{\\alpha_t\\beta\\gamma}{\\beta+\\gamma}$ (strong convex), $\\eta = 1$ (convex), and $\\eta_t = 1 + \\alpha_t\\beta$ (non-convex). However, for the zeroth-order case, there often is additional increment induced by the procedures of one-point or two-point estimation (Ref1). We can verify  $\\eta^{\\prime}_t=1+\\alpha_t\\left(\\frac{L}{\\mu}\\mathbb{E}\\|u\\|+\\beta\\mathbb{E}\\left[\\|u\\|^2\\right]\\right)$ (one-point convex condition), $\\eta^{\\prime}_t=1+\\alpha_t\\mu\\beta\\mathbb{E}\\left[\\|u\\|^2\\right]$ (two-point convex condition),  $\\eta^{\\prime}_t=1-\\frac{\\alpha_t\\beta\\gamma}{\\beta+\\gamma}+\\alpha_t\\left(\\frac{L}{\\mu}\\mathbb{E}\\|u\\|+\\beta\\mathbb{E}\\left[\\|u\\|^2\\right]\\right)$ (one-point strongly convex condition), and $\\eta^{\\prime}=1-\\frac{\\alpha_t\\beta\\gamma}{\\beta+\\gamma}+\\alpha_t\\mu\\beta\\mathbb{E}\\left[\\|u\\|^2\\right]$ (two-point strongly convex condition). \n\nDue to the additional increments (e.g., $\\eta^{\\prime}$ may exceed $1$), multiplication process. \nthe additional results for the general convex and strongly convex settings are analogous to the existing error bounds under non-convex conditions (e.g., **Theorems 1,2,4,5,7,8**).\n\n\n[Ref1] K. Nikolakakis et al., Black-box generalization: Stability of zeroth-order learning. Advances in Neural Information Processing Systems (NeurIPS), 2022."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5497/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700490463088,
                "cdate": 1700490463088,
                "tmdate": 1700490463088,
                "mdate": 1700490463088,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]