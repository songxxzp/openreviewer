[
    {
        "title": "Fairness Feedback Loops: Training on Synthetic Data Amplifies Bias"
    },
    {
        "review": {
            "id": "zDTgNd9caJ",
            "forum": "svSWP21tdp",
            "replyto": "svSWP21tdp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6793/Reviewer_HHFw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6793/Reviewer_HHFw"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel term, \"model-induced distribution shift,\" aiming to encompass various distribution shifts within a single framework. It delves into two scenarios to highlight their effects. Furthermore, the study reveals that model-induced distribution shifts can rapidly result in suboptimal performance, skewed class distribution, and underrepresentation of marginalized demographic groups. To address this challenge, this paper showcases the potential of algorithmic reparation to reduce disparities among sensitive groups."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* This paper systematically categorizes the existing literature on model-induced distribution shift, offering a consolidated overview for the machine learning fairness community.\n* The experimental setups involving sequences of classifiers and generators are interesting, as they aptly simulate the real-world data distribution shifts induced by deployed models.\n* Numerical experiments show the effectiveness of applying algorithmic reparation for mitigating the model-induced data distribution issues."
                },
                "weaknesses": {
                    "value": "I have two main concerns about this work\n* In this study, the authors employ a synthetic process using a series of classifiers and generation models to emulate real-world data distribution shifts. How do the authors substantiate that this accurately reflects actual distribution shift behaviors in the real world?\n* Could the authors delve into a discussion regarding how current methodologies address the challenges of model-induced distribution shifts?\n* Furthermore, the numerical experiments focus solely on a comparison between scenarios with and without algorithmic repair. Could the authors also present comparisons against established baseline methods?\n\nOverall, I'm uncertain about the adequacy of the contribution presented in this work for acceptance. I would recommend that the authors elaborate more on the distinct contributions of their paper in relation to existing methodologies addressing this issue."
                },
                "questions": {
                    "value": "My questions are provided in the Weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6793/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6793/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6793/Reviewer_HHFw"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6793/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698161597587,
            "cdate": 1698161597587,
            "tmdate": 1699636785038,
            "mdate": 1699636785038,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "naRAxxqQOb",
                "forum": "svSWP21tdp",
                "replyto": "zDTgNd9caJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6793/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6793/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer HHFw"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their comments and address their questions:\n\n```\n\u201cHow do the authors substantiate that this accurately reflects actual distribution shift behaviors in the real world?\u201d\n```\n\nAll of our settings revolve around a sequence of models trained with datasets that get updated over time (whether by adding synthetic data, synthetic labels, or some mixture of synthetic and \u2018real\u2019 data). Concerns of use of synthetic data, whether intentionally or otherwise have already appeared in research and media. Our work seeks to better understand MIDS and opportunities for algorithmic reparation. We do not attempt to reflect distribution shift in general, just those induced by models. We also do not study what the impact of MIDS alongside other types of distribution shift might entail, though we do explore interactions between MIDS. \n\n```\n\u201cCould the authors delve into a discussion regarding how current methodologies address the challenges of model-induced distribution shifts?\u201d \n\n\u201cCould the authors also present comparisons against established baseline methods?\u201d\n```\nPoints 2 and 3 ask for a discussion and experimental comparison against related methods. We show one comparison in Appendix E.2, where we provide an ablation study for the amount of synthetic data present at each generation. We also anticipate that training with fairness at each generation may be insufficient to stop unfairness accumulating due to MIDS (see our response to reviewer QqzD, point 4.b). Otherwise, please refer us to works that can address MIDS."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6793/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700343699256,
                "cdate": 1700343699256,
                "tmdate": 1700343699256,
                "mdate": 1700343699256,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "t62sMF6Sf3",
            "forum": "svSWP21tdp",
            "replyto": "svSWP21tdp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6793/Reviewer_QqzD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6793/Reviewer_QqzD"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a framework called model-induced distribution shifts (MIDS) that unify several existing notions such as model collapse, unfairness feedback loops, class imbalance, label/ input drift, etc."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThe formulation and procedure to observe and evaluate MIDS is clear. The flowcharts in Section 3.1 and Section 3.2 are really helpful\n2.\tThe discussions on model collapse for generative models and the performative prediction in Section 4.2 are inspiring"
                },
                "weaknesses": {
                    "value": "1.\tThis paper attempts to encompass several issues such as model collapse, performative prediction, unfairness feedback loops, and algorithmic reparation. However, the benefit and motivation for the unifying MIDS is not clear to me. It is encouraged that the authors clearly state what addition challenge could be solved, or what existing challenges could be better solved by the MIDS framework. \n2.\tThere are existing algorithms that could solve unfairness feedback loops, class imbalance, etc. However, the MIDS framework is not compared with those existing benchmarks. \n3.\tAlgorithmic reparation (AR) should be the most important concept/baseline in this paper; however, it is not technically introduced in the paper. It is encouraged that the authors add more discussion, use case, and operational meaning of AR.\n4.\tThe methodologies proposed in Section 3 for MIDS lack theoretical analysis, performance guarantee, etc."
                },
                "questions": {
                    "value": "1.\tIn Figure 4, as the number of generations increase, the accuracy drops from 92% to 82%, and the fairness metrics like DP and EO are still large. Why a reduction of accuracy and unfairness will occur at the same time?\n2.\tWhy we need the settings of sequences of generators?\n\nFor other questions, please refer to the Weaknesses. I will consider raising the scores if the authors could adequately address my questions in the rebuttal."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6793/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698815182665,
            "cdate": 1698815182665,
            "tmdate": 1699636784893,
            "mdate": 1699636784893,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qJFGmdpctC",
                "forum": "svSWP21tdp",
                "replyto": "t62sMF6Sf3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6793/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6793/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer QqzD"
                    },
                    "comment": {
                        "value": "We thank the reviewers for their comments and address their questions below, followed by addressing the weaknesses.\n\n```\nIn Figure 4, as the number of generations increases, the accuracy drops from 92% to 82%, and the fairness metrics like DP and EO are still large. Why does a reduction of accuracy and unfairness occur at the same time?\n```\n\nFigure 4 depicts sequential classifiers undergoing performative prediction. In the non-reparative results, there are no fairness interventions and we do not expect the classifiers to achieve lower unfairness. However, each generation\u2019s mistakes continue to compound, lowering the accuracy. The algorithmic reparation is successful in eventually yielding batches with even\\ideal class and group representation (Figure 4, bottom row), benefiting the fairness metrics. However, as these even batches no longer match the original distribution\u2019s imbalances, there must be some incorrect classification when compared to the original distribution, which is why the accuracy still decreases. Please also see our response to reviewer qqNH\u2019s first question for more information.\n\n```\n\u201cWhy do we need the settings of sequences of generators?\u201d\n```\n\nThe settings with sequential generators are motivated as generated content is published online and re-scraped to form new datasets. For example, in the second paragraph of the introduction we discuss the proliferation of AI-generated baby peacock photos which (as of writing) frequently show up in the first page of Google image search results (as found by Shah and Bender (2022)). A generative model trained on these images would misrepresent the baby peacocks. Another example is from Venugopal et al. (2011), when Google researchers realized that training on previous translations published online might harm future versions of their model. Similarly, Veselovsky et al (2023) showed that AI is now being used by MTurkers, suggesting that we already may have a significant proportion of AI labels in our crowdsourced datasets.\n\nPlease see our next comment which addresses the weaknesses."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6793/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700343611789,
                "cdate": 1700343611789,
                "tmdate": 1700343611789,
                "mdate": 1700343611789,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pfh98NIbrr",
                "forum": "svSWP21tdp",
                "replyto": "Ab2lu56f4B",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6793/Reviewer_QqzD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6793/Reviewer_QqzD"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the authors for their response. I will stand my current score."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6793/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700669680030,
                "cdate": 1700669680030,
                "tmdate": 1700669680030,
                "mdate": 1700669680030,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pnn7cKxsqU",
            "forum": "svSWP21tdp",
            "replyto": "svSWP21tdp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6793/Reviewer_vnkU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6793/Reviewer_vnkU"
            ],
            "content": {
                "summary": {
                    "value": "The paper is about the feedback loop when models are continuously trained on data generated by them. The authors introduce model-induced distribution shifts (MIDS) which occur as previous model outputs pollute new model training sets over generations of models. They provide a taxonomy for MIDS and demonstrate that their fairness effects lead to a lack of representation and per-\nformance on minoritized groups within a few model iterations. The authors propose Algorithmic Reparation (AR) as another explicit MIDS\ndeployed with the goal of reducing societal inequity and correcting for historical oppression; they use AR to reduce the unfairness impacts of other MIDS by sampling for minoritized group representation, leading to better downstream fairness over time."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The feedback loop problem is important and interesting.\n* The authors propose a setup in which this problem can be studied and explore Algorithmic Reparation as a possible solution."
                },
                "weaknesses": {
                    "value": "Personally, I find the paper a bit hard to understand.\n\n* Introduction seems a bit verbose and overly lyrical in moments, making it harder to read and follow\\\n\"recent demographic information of the Black population\" - it is written that the maps are from 1939 and 1955. I am not sure that this is very recent.\n\n* I am not sure that MIDS require their own \"taxonomy\", given that there are only label and input drifts (Table 1). Impact of feedback loop in fairness has been acknolwedged as a problem for a while (Mehrabi et al. 2019)\n\n* The related work is not clearly discussed in the paper, making it a bit hard to get the overall context and contributions with respect to prior work.\n\n* A recent related work by Taori and Hashimoto (2023) is missing. How does this paper relate to them?\n\nMehrabi et al., A Survey on Bias and Fairness in Machine Learning, arXiv 2019\\\nTaori and Hashimoto , Data Feedback Loops: Model-driven Amplification of Dataset Biases, 2023"
                },
                "questions": {
                    "value": "Q1: How do you select the datasets and what are the motivations for them and the experimental setup exactly?\n\nQ2: What are the models that you employ for the classifiers and the generator?\n\nQ3: Why does the accuracy drops over time in Fig. 5 and 6?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6793/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698899685347,
            "cdate": 1698899685347,
            "tmdate": 1699636784775,
            "mdate": 1699636784775,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cSxg1VXv7w",
                "forum": "svSWP21tdp",
                "replyto": "pnn7cKxsqU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6793/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6793/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer vnkU"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their comments, and address their questions here:\n\n```\n\u201cQ1: How do you select the datasets and what are the motivations for them and the experimental setup exactly?\u201d\n```\n\nDataset choice: We follow a common experimental setting used in computer vision fairness research, adapting datasets into binary classification and binary sensitive grouping (Arjovsky et al. 2022). We use MNIST and SVHN as they both contain the same number of classes. Their difference is in difficulty (SVHN is harder, usually) and class imbalance when split at 5. These two datasets, despite their similarity, showed very different points of model collapse and opposite behavior when observing the accuracy difference between groups (Figures 14 and 15) as ColoredMNSIT reparation resulted in less accuracy difference unlike the other datasets. \nWe chose CelebA for a much more complex/real-world dataset with many sensitive features and well-documented disparities present in the dataset\u2019s base rates (see Table 2). \nAll of these datasets can either be manipulated (as in Arjovsky et al. 2022) into binary group and binary class classification problems (MNIST, SVHN) or already have these features (CelebA). This allows for ease of evaluation of our metrics and allows us to ablate both group and class imbalance to study the mechanics of MIDS. \nExperimental setup details may be found in Appendix C, we also added further discussion for the reasons we chose these datasets in Appendix C.1.\n\n```\n\u201cQ2: What are the models that you employ for the classifiers and the generator?\u201d\n```\n\nModels for classifiers are CNNs or ResNets, and generators are beta-VAEs. Full details are in Appendix C.3, or in our code hosted anonymously https://anonymous.4open.science/r/ FairFeedbackLoops-1053/README.md\n\n```\n\u201cQ3: Why does the accuracy drop over time in Fig. 5 and 6?\u201d\n```\n\nThe accuracy decreases in Figures 5 and 6 (SeqGenSeqClass) due to model collapse. The accuracy is measured at each generation by inputting a held-out set into the classifier and recording performance. However, each classifier was trained from the distribution represented by a generative model undergoing model collapse. As the generators lose the ability to accurately represent the initial distribution (see Figure 19 to see how MNIST and SVHN digits deform over time), we expect the classifiers to lose their ability to discriminate between classes, and their accuracy should decrease.\n\nPlease see our next comment where we address the weaknesses."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6793/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700343424881,
                "cdate": 1700343424881,
                "tmdate": 1700343424881,
                "mdate": 1700343424881,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nD7UOGilLR",
                "forum": "svSWP21tdp",
                "replyto": "rvbCnEWNvC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6793/Reviewer_vnkU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6793/Reviewer_vnkU"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their efforts and rebuttal.\n\nWhile the work and the presented ideas have their merit, I still believe that the technical contributions, the theoretical framework and the experimental setup are not yet clear enough (to me). Moreover, beta-VAEs for example are very crude generative models, which makes me wonder how realistic, practical and informative the evaluation is.\n\nAfter reading the rest of the reviews, I would like to keep my score."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6793/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700662324029,
                "cdate": 1700662324029,
                "tmdate": 1700662324029,
                "mdate": 1700662324029,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MYsFVaeGyj",
            "forum": "svSWP21tdp",
            "replyto": "svSWP21tdp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6793/Reviewer_qqNH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6793/Reviewer_qqNH"
            ],
            "content": {
                "summary": {
                    "value": "This paper provides a conceptual taxonomy of various ways in which repeated training of a model (either a classifier or a generator) in the same data ecosystem lead to negative effects on fairness such as degradation in fairness metrics and distorted class proportions. The overall phenomenon is terms MIDS - Model Induced Distribution Shift, and encompasses previously studied phenomena/concepts such as model collapse, performative prediction and fairness feedback loops. Using experiments on a sequence of recursively trained classifiers (and likewise generators), the paper exposes the several possible harmful effects of MIDS. The paper then introduces a simple resampling scheme for Algorithmic Reparation (AR) into the MIDS framework, and shows through experiments that the proposed AR can ameliorate some of the harmful effects of MIDS."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper addresses the important issue of studying the fairness effects of repeated training in a data ecosystem. \n- The paper presents a clear model of sequentially training classifiers and generators that can be used to simultaneously model various effects like feedback loops, performative effects and model collapse."
                },
                "weaknesses": {
                    "value": "- The paper does not propose a mathematical model for the proposed MIDS scheme. Although various effects of retraining are shown through experiments, there is no theoretical investigation on the root causes of these effects, or the efficacy of the presented Algorithmic Reparation (AR) scheme.\n- The experimental results with the proposed AR scheme based on resampling are not very convincing. For example, the decrease in the equalized odds difference with reparation in Figure 4 is not monotonic. The gap closes for a few generations and then seems to rise up again. The plots on fairness metrics in Figure 5 are even less convincing."
                },
                "questions": {
                    "value": "- What is the reason for the decrease in the equalized odds difference with reparation in Figure 4 not being monotonic?\n- The paper claims that \"Generator-side AR improves fairness and minoritized representation\", but plots on fairness metrics in Figure 5 do not corroborate this claim. Please explain.\n- I do not understand the claim, \"Performative prediction on model collapse leads to higher utility\". Please explain what you mean by this."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6793/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699109189948,
            "cdate": 1699109189948,
            "tmdate": 1699636784669,
            "mdate": 1699636784669,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WlV5CmMNU7",
                "forum": "svSWP21tdp",
                "replyto": "MYsFVaeGyj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6793/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6793/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer qqNH"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their comments, and address their questions here. \n\n```\nWhat is the reason for the decrease in the equalized odds difference with reparation in Figure 4 not being monotonic?\n```\nEqualized odds is the maximum of the difference between the true positive rate (TPR) difference or false positive rate (FPR) difference between groups, and is therefore dependent on the ground truth. In Figure 4, we show our metrics as measured with respect to the original distribution (see Table 2), where there is correlation between the sensitive attribute value and class. The first classifiers in early generations start off unfair wrt the original distribution. As AR starts to balance the training set, the classifiers achieve a minimal unfairness wrt the original distribution (generation 13). However, as AR continues to balance the dataset and reach its ideal (Figure 4 bottom right), in the view of the original distribution the minoritized group is being \u201cunfairly\u201d benefited over the majoritized group, which is why the unfairness increases. When plotted, AR causes the majoritized TPR to decrease until it is lower (by about 0.035 also the final EOdds at generation 40) than the minoritized TPR, showing the \u201cunfair\u201d benefit AR gives to the minoritized group when seen from the view of fairness in the original distribution. There is also the presence of label shift due to the sequential classifiers, but these effects taken together is why we mention that we might not want to achieve fairness in Figures 4-6.\n\n```\nThe paper claims that \"Generator-side AR improves fairness and minoritized representation\", but plots on fairness metrics in Figure 5 do not corroborate this claim.\n```\nWe find that using the generator-side AR technique does improve fairness and minoritized representation. In Figure 5, this is shown by the purple dotted line where the demographic parity and equalized odds values are lower (fairer) than (a) classifier-side reparation and (b) results without reparation. The generator-side reparation maintains the original minoritized group population at 50% of the total, as in the ColoredMNIST dataset. Because classifier-side AR and baseline results diminish this population, they result in less representation.\n\n```\nI do not understand the claim, \"Performative prediction on model collapse leads to higher utility\". Please explain what you mean by this.\n```\nThis claim means that the combined setting SeqGenSeqClass allows model collapse (MIDS on the generative models) and performative prediction (MIDS on the classifiers) to co-exist. Compared to the SeqGenNoSeqClass (i.e., just model collapse), this setting has higher utility (by this we mean accuracy). This result highlights cooperation between the model collapse and performative prediction MIDS as the SeqGenSeqClass classifiers adapt to changes due to model collapse. Please see Appendix E.3 for further discussion. \n\nIn retrospect, the wording of this claim is confusing so we amended it to \u201cPerformative prediction combined with model collapse yields higher accuracy.\u201d \n\nWe also address the weaknesses:\n* `The paper does not propose a mathematical model for the proposed MIDS scheme.` If interested in theoretical backgrounds behind MIDS, please see Shumailov et al. (2023) for a formulation of the SeqGenNoSeqClass setting in terms of a Markov chain and Taori and Hashimoto (2023) for a formulation of SeqClass in terms of calibration.\n* `the decrease in the equalized odds difference with reparation in Figure 4 is not monotonic. The gap closes for a few generations and then seems to rise up again. The plots on fairness metrics in Figure 5 are even less convincing.` Question about Figure 4 is answered above in point 1. The AR scheme as presented in Figure 5 shows that generator-side reparation, while performing with less accuracy, maintains class and group balance and achieves far lower unfairness than the results without any reparation. \n\n[1] Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and Ross Anderson. The curse of recursion: Training on generated data makes models forget, 2023. \n\n[2] Taori, Hashimoto. Data feedback loops: Model-driven amplification of dataset biases. InInternational Conference on Machine Learning 2023."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6793/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700343310346,
                "cdate": 1700343310346,
                "tmdate": 1700343310346,
                "mdate": 1700343310346,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]