[
    {
        "title": "Directed Graph Generation with Heat Kernels"
    },
    {
        "review": {
            "id": "A4MXvfqxv1",
            "forum": "xXtD9P2lvH",
            "replyto": "xXtD9P2lvH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6309/Reviewer_E6dr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6309/Reviewer_E6dr"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a generative model, similar to denoising autoencoders, for directed graph generation. The encoder adds noise based on a heat equation expression to generate a perturbed representation, which the decoder denoises to reconstructs the desired generated graph via the random walk Laplacian. The authors test their approach in empirical evaluations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Originality and Novelty: The approach that the authors propose is, to the best of my knowledge, original and novel. \n- Significance: Nowadays it is certainly an interesting and important topics to study graphs, as well as generative models on graphs. The topic of directed graph generation has indeed been underappreciated in the literature, so this is a welcome addition. \n- Quality: The technical claims are, to the best of my knowledge, sound and reasonable. Details and questions are given below in the next section. \n- Clarity: The article is written moderately clearly, with ample room for improvement in its exposition. Suggestions are given in the section below."
                },
                "weaknesses": {
                    "value": "- The main weakness is in the presentation and empirical evaluation: \n\n1. I suggest that the authors provide more background and motivation on the mathematical prerequisites to make the paper more self contained. \n\n2. The decision to set M to be a constant matrix should be further motivated and explained (to people familiar with this, this is a natural choice, but this can be unclear to the less initiated readers)\n\n3. Crucial concepts rely on very recent work such as the Set Transformer in 2019 and the work of Veerman and Lyons in 2020. This makes the article more difficult to read...I suggest that the authors try their best to make this work more self contained in the presentation. \n\n4. The empirical evaluation is limited to very simple models (ER and SBM) under the squared MMD distance for various descriptors, under hyperparameter settings of 3 blocks and p as 0.6, seemingly without much justification."
                },
                "questions": {
                    "value": "1. Is there a concrete reason/justification for why the empirical evaluation is so focused on disconnected digraphs? My impression is that many interesting applications concerns connected/strongly connected digraphs. Is there a possibility where the evaluation metrics (clustering coefficients etc) are just capturing the disjointness of the generated graph, rather than the more fine-grained properties of connectivity within a single connected/strongly connected digraph?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6309/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698218394141,
            "cdate": 1698218394141,
            "tmdate": 1699636693727,
            "mdate": 1699636693727,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DPy6l1P9lr",
                "forum": "xXtD9P2lvH",
                "replyto": "A4MXvfqxv1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6309/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6309/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q:**  The decision to set M to be a constant matrix should be further motivated and explained (to people familiar with this, this is a natural choice, but this can be unclear to the less initiated readers).\n\n   **A:**  Thank you for the suggestion. We have added in Section 2 the motivation of our formulation of $\\textbf{M}$, which is to represent a non-informative matrix when the node representation matrix is constrained to be column stochastic.  Each column of $\\textbf{M}$ is defined as the uniform probability distribution vector, which corresponds to the maximum entropy probability distribution vector. \nEach column of $\\textbf{M}$ also corresponds to the expected value of a random variable following a flat Dirichlet distribution.\n\n**Q:**  Crucial concepts rely on very recent work such as the Set Transformer in 2019 and the work of Veerman and Lyons in 2020. This makes the article more difficult to read...I suggest that the authors try their best to make this work more self contained in the presentation.\n\n**A:**  Thank you for this suggestion. We have added a high-level description of SetTransformers and our motivation to use it (i.e. using a model that is invariant to the order of the rows of our node representations) in Section 3.1. The necessary concepts from Veerman and Lyons are given in our submission. We cite their paper only when (1) we define the (negative of) the random walk Laplacian matrix, which is a standard definition in graph theory, and when (2) we say that its matrix exponential is column stochastic, which can be verified by eigendecomposition. This last concept is actually explained in our Appendix F when we study the eigendecomposition of our heat kernel matrix. We have then added a reference to Appendix F for details. \n\n**Q:**  The empirical evaluation is limited to very simple models (ER and SBM) under the squared MMD distance for various descriptors, under hyperparameter settings of 3 blocks and p as 0.6, seemingly without much justification.\n    Is there a concrete reason/justification for why the empirical evaluation is so focused on disconnected digraphs? My impression is that many interesting applications concerns connected/strongly connected digraphs. Is there a possibility where the evaluation metrics (clustering coefficients etc) are just capturing the disjointness of the generated graph, rather than the more fine-grained properties of connectivity within a single connected/strongly connected digraph?\n\n**A:**  We would like to emphasize that the blocks in the stochastic block models are disconnected only in the qualitative experiment of Section 5.3, so that the different modes are easy to visualize. In our experiments with quantitative results (i.e. Section 5.2 and Appendix H.2), the different blocks are still connected but with low transition probability between blocks. The probabilities of transition between blocks are given in Equation (8) and Equation (30). \nTherefore, our evaluation metrics capture the fine-grained properties of connected digraphs. \nMoreover, we provide additional experiments with larger graphs in Appendix H.2 with $p = 0.4$ for the Erdos-Renyi category, and 5 blocks for the stochastic block models."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6309/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700658037558,
                "cdate": 1700658037558,
                "tmdate": 1700658037558,
                "mdate": 1700658037558,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IiVV6UqiS3",
            "forum": "xXtD9P2lvH",
            "replyto": "xXtD9P2lvH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6309/Reviewer_APd2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6309/Reviewer_APd2"
            ],
            "content": {
                "summary": {
                    "value": "The focus of the manuscript is on directed graphs (digraphs) generative process. The authors propose an encoder-decoder architecture. Their encoder is based on the heat diffusion (defined by the graph Laplacian), and does not require any training. The representation is then perturbed such that it corresponds to a nonhomogeneous process. The denoiser is then trained to reverse the diffusion, i.e. to match the initial condition of the process. They then provide experiments to validate their claims."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The proposed method is well motivated from a theoretical point of view. \n- Focusing on digraphs is interesting as most methods are on undirected graphs."
                },
                "weaknesses": {
                    "value": "- The overall writing and organization could be improved. In particular, section 3, which lack continuity. \n- I found the experiments a bit limited, I think a few things are missing:\n    - The authors should report the standard deviation in the table. \n    - I think it is important to report other distances than the current MMD with RBF kernel, either with different kernels and / or with different variance parameters. \n    - In the tables, it is hard to understand the magnitude of the scores. It would be great to add a row for a random method (e.g. random adjacency matrix used in line 2 of Alg.1). \n    - The results in 5.3 could be in the main body of the text by shortening other sections (e.g. related work)."
                },
                "questions": {
                    "value": "- How do you choose the initial node representation $X(0)$ ?\n- In eq.1, you also need to specify the initial condition at $t=0$. You could also explain $X(0)$ has $d$ signals that you diffuse following the heat equation (it might give more intuition).  \n- >Finally, some denoising decoder is trained to predict the nodes and/or edges when given only X(T ) as input (see details in Section 3.1).\n    \n    I don't fully understand this sentence. Looking at $\\mathcal{L}_{node}$ it is trained to reverse the diffusion process. It is not trained to predict the node, but rather the initial $d$ signals. \n    \n- Is the decoder conditioned on the noise level (e.g. like in score matching) ?\n- Possible typo: \"our decoders are learned \" $\\to$ \"our decoders are trained \""
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6309/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6309/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6309/Reviewer_APd2"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6309/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698685238179,
            "cdate": 1698685238179,
            "tmdate": 1699636693584,
            "mdate": 1699636693584,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AnIJFuxGBt",
                "forum": "xXtD9P2lvH",
                "replyto": "IiVV6UqiS3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6309/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6309/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q:**     The overall writing and organization could be improved. In particular, section 3, which lack continuity.\n    I found the experiments a bit limited, I think a few things are missing:\n        The authors should report the standard deviation in the table.\n        I think it is important to report other distances than the current MMD with RBF kernel, either with different kernels and / or with different variance parameters.\n        In the tables, it is hard to understand the magnitude of the scores. It would be great to add a row for a random method (e.g. random adjacency matrix used in line 2 of Alg.1).\n        The results in 5.3 could be in the main body of the text by shortening other sections (e.g. related work).\n\n\n**A:**  Thank you for the suggestion. We have shortened the related work section by moving the similarities with diffusion models to the Appendix, and we have rewritten some parts of Section 3 that might be confusing. \nWe have also included different variance parameters ($\\sigma^2 = 100, 10, 1$) in Table 2 as requested. As a random method, we have added results when sampling each column of the input of the decoder directly from a flat Dirichlet distribution. We have tested the sampling method that replaces line 2 of algorithm 1 by sampling the non-diagonal elements of the adjacency matrix uniformly in the interval [0,1] (while keeping the diagonal elements equal to 1). We call that approach \"continuous DGDK\" because the sampling space is continuous as opposed to our previous sampling approach that is discrete. The former random method outperforms the baseline but is outperformed by our approach. On the other hand, the latter is competitive with our proposed sampling approach since it also exploits the learned matrix $\\textbf{N}$. Due to lack of time, we did not include standard deviation, we intend to do it in the next version.\n\n**Q:**    How do you choose the initial node representation? \n\n**A:**  The initial node representation matrix $\\textbf{N} = \\textbf{X} (0)$ is learned. Following Reviewer hJPx's suggestion, we have specified in Section 2 and Section 3.1 that the matrix $\\textbf{N}$ is learned jointly with our decoders, and we have added implementation details on how we train it in Appendix B. In short, $\\textbf{N}$ is a matrix that is made column stochastic by applying a column-wise softmax operator. It is optimized via standard gradient descent.\n    \n**Q:**  In eq.1, you also need to specify the initial condition at $t=0$. You could also explain has signals that you diffuse following the heat equation (it might give more intuition). \n\n**A:**  Thank you for the suggestion. We have included that intuition after Eq. (2). All the necessary conditions/constraints are included in Eq. (1) so that Eq. (2) is its solution (see e.g. Veermans and Lyons).\n\n\n\n**Q:**  I don't fully understand this sentence. Looking at\nit is trained to reverse the diffusion process. It is not trained to predict the node, but rather the initial signals.\n\n**A:**  Your understanding is correct. Our decoders are trained to reconstruct the initial representation that does not contain noise. We have replaced the word \"predict\" with \"reconstruct\". \n\n**Q:**  Is the decoder conditioned on the noise level (e.g. like in score matching) ?\n\n**A:** The decoder is in a sense conditioned on the \"noise level\" since we formulate: (1) our encoder so that its limit tends to $\\textbf{M}$ when $T$ tends to infinity, (2) each column of $\\textbf{M}$ corresponds to the mean/expected value of a random variable following the flat Dirichlet distribution (with all the parameters $\\alpha$ equal to 1). \nTherefore, the input of our decoder is \"close\" to the expected value of the flat Dirichlet distribution. However, this is different from the noise level conditioning in regular diffusion models trained with denoising score matching at various noise levels. In those cases, the decoder, or denoiser, is trained at multiple different noise levels simultaneously. But in our case, our decoder is only trained at one, \"the final\", noise level. Since our method is an efficient one-shot generation technique, the decoder never needs to be trained or evaluated at \"intermediate\" noise levels, as in diffusion models. Hence, an explicit noise level conditioning like in diffusion models is not necessary for our method."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6309/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700657338097,
                "cdate": 1700657338097,
                "tmdate": 1700657895639,
                "mdate": 1700657895639,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PmRCbthgHK",
            "forum": "xXtD9P2lvH",
            "replyto": "xXtD9P2lvH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6309/Reviewer_hJPx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6309/Reviewer_hJPx"
            ],
            "content": {
                "summary": {
                    "value": "The paper describes a generative approach for directed graphs. It loosely follows the idea of denoising autoencoders: a trained input function in R^d over nodes is corrupted through a heat diffusion process as to produce an almost constant function over the graph nodes. This function is then given as input to an encoder that is tasked to project the node representation into a latent space and a decoder that, given two node embeddings, predicts the presence of an edge."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper deals with an interesting problem, the one of directed graph generation, which seems to be partially neglected by the main body of literature in graph generation but that is nevertheless relevant.\nTo the best of my knowledge, the proposed methodology is original. It adapts some ideas from denoising autoecoders and the more recent denoising diffusion to the domain of directed graphs, building a principled and sound method."
                },
                "weaknesses": {
                    "value": "One of the drawbacks of the paper is that the mathematical notation is a bit intricate. Many quantities are redefined during the method description and is difficult to keep track of all of them. Some equations are also defined but I missed if or where they were used, for eq 6 or the node loss (where is the model \\phi used in the sampling process?).\nPersonally, until section 3, I was imagining some sort of denoising diffusion technique (especially after eq 5 and 6). It took me a while to change my \u201cexpectations\u201d about the following sections.\n\nThe other weakness is about the comparisons. Even if there aren\u2019t many works dealing with directed graphs, it is still worth providing a solid testbench that could possibly be used also from future works that want to compare with the proposed method. Isn\u2019t there more datasets that can be considered or comparative methods that could be adapted"
                },
                "questions": {
                    "value": "- The diffusion time is set to 1 in the experiments, but is it a dataset-dependent parameter? I guess that it might be somehow dependent on the graph radius?\n\n- It took me a while to figure out what a kind of node function N had to be. Maybe making it clear from the beginning that it is learned could help the reading. \n\n- Still regarding N, permutation invariance is not an easy thing to learn. How much could it be a problem for the training convergence?\n\n- Your method consists of diffusing an initial random graph. How much important is the starting graph family? Since your noise converges to a constant function, would it be possible to sample directly M?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6309/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6309/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6309/Reviewer_hJPx"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6309/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699131360820,
            "cdate": 1699131360820,
            "tmdate": 1699649115592,
            "mdate": 1699649115592,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "k6igazZWHg",
                "forum": "xXtD9P2lvH",
                "replyto": "PmRCbthgHK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6309/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6309/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q:**    The diffusion time is set to 1 in the experiments, but is it a dataset-dependent parameter? I guess that it might be somehow dependent on the graph radius?\n\n **A:**  Yes, both the diffusion time $T > 0$ and noise diffusivity rate $\\alpha > 0$ are hyperparameters that depend on the dataset, can be tuned, and have an impact on the eigenvalues of the heat kernel matrix as discussed in Appendix F. We perform an ablation study of the parameter $\\alpha$ in Appendix H.4, and we show that careful tuning of $\\alpha$ is required in order to generate digraphs that are similar to those in the training distribution. A similar ablation study could be performed for $T$. \n\n We agree that the optimal value of $T$ is related to the radius of the digraph. The intuition is similar to the difference between ODE solvers and their approximation via the Euler method. Our method solves a linear differiental equation (the heat equation) in closed-form by using the matrix exponential $e^{t \\Delta}$. An alternative solver could use the forward Euler method and iteratively use $t$ times the matrix $(\\Delta + \\textbf{I}) = \\textbf{S}$ instead, which would correspond to performing $t$ steps of message passing.\n\n**Q:**      It took me a while to figure out what a kind of node function N had to be. Maybe making it clear from the beginning that it is learned could help the reading.\n\n **A:**  Thank you for this suggestion, we have included in Section 2 and Section 3.1 that the matrix $\\textbf{N}$ is learned jointly with our decoders, and we have added experimental details on how we train it in Appendix B.\n\n **Q:**      Still regarding N, permutation invariance is not an easy thing to learn. How much could it be a problem for the training convergence?\n\n **A:**  In practice, this is not a problem. One reason is that the matrix $\\textbf{N} = \\textbf{X} (0)$ is learned. As explained in our revised Section 3.2, our way of  dealing with permutations can be seen as a data augmentation scheme that adds more adjacency matrices (of isomorphic digraphs) to the training set by considering different permutation matrices $\\textbf{P}$ and formulates noisy node representations that can be written as a function of $\\textbf{P}^\\top e^{T\\Delta} \\textbf{P} \\textbf{N}$ instead of only $e^{T\\Delta} \\textbf{N}$ (i.e., only $\\textbf{P} = \\textbf{I}$). The matrix $\\textbf{N}$ is jointly learned with the edge and node decoders that are robust to this kind of transformation. \n We report the plot curves of our optimization scheme with and without this data augmentation technique in Figure 6. One can see, that it doesn't have an impact on the training loss optimization. Moreover, it obtains slightly better performance in practice.\n\n  **Q:**     Your method consists of diffusing an initial random graph. How much important is the starting graph family? Since your noise converges to a constant function, would it be possible to sample directly M?\n\n**A:**  We agree that it is possible to generate the input of the decoder by sampling each column of the input from a flat Dirichlet distribution. In practice, when we tried it, many generated graphs were not as sparse as in the training set. We have included quantitative results when using that sampling strategy in the revised version of Table 2 (called \"Sampling from flat Dirichlet distribution\"), its performance is worse than with our proposed approach, but better than the baseline GRAN. This means that it is a viable strategy.\n\nWe have also tried the sampling strategy proposed by Reviewer APd2, where instead of sampling discrete adjacency matrices with elements equal to 0 or 1, the sampled adjacency matrices have their non-diagonal elements sampled uniformly in the continuous interval [0,1], and their diagonal elements are 1. \nIn this case, we obtain results that are competitive with our proposed sampling strategy. Unlike the strategy that samples directly from the flat Dirichlet distribution, this strategy exploits the learned initial node representation matrix $\\textbf{N}$ and the matrix exponential $e^{T \\Delta}$ to construct the input of the decoder. This competitive performance suggests that the starting graph family is not that important to obtain good results."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6309/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700657376300,
                "cdate": 1700657376300,
                "tmdate": 1700657376300,
                "mdate": 1700657376300,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2ORJkmWE9Z",
                "forum": "xXtD9P2lvH",
                "replyto": "k6igazZWHg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6309/Reviewer_hJPx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6309/Reviewer_hJPx"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for answering my questions. I'll go through the revised paper in the next few days."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6309/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668072520,
                "cdate": 1700668072520,
                "tmdate": 1700668072520,
                "mdate": 1700668072520,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zU76HZEuEX",
            "forum": "xXtD9P2lvH",
            "replyto": "xXtD9P2lvH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6309/Reviewer_t7Sc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6309/Reviewer_t7Sc"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents an novel approach to crate a one-shot generative model for directed graphs.\nThe approach represents the graph through their heat diffusion with a forcing term Q(t) that forces the limit distribution to be uniform over the nodes. \nTo generate the graph, they train an edge decoder that, given a noisy representation of the heat diffusion, predicts the presence of the edge.\nWith the decoder to hand, they generate a Erdos-Renyi random graph, and add some Bernoulli noise to the adjacency matrix, then obtain the directed Laplacian for the result and compute the heat kernel under the mentioned forcing term and feed it to the edge decoder."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Novel approach to cast the one-shot generation of graph\n- can handle directed as well as undirected graphs"
                },
                "weaknesses": {
                    "value": "- The experimental evaluation is a bit substandard given the relatively large recent literature on the topic. The authors should at least match SPECTRE (cited) for the evaluation protocol."
                },
                "questions": {
                    "value": "While it is clear that the link predictor tries to match what it has seen in the training set, it is not clear how their approach changes the Erdos-Renyi distribution to one more similar to the training set."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6309/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699203031918,
            "cdate": 1699203031918,
            "tmdate": 1699636693361,
            "mdate": 1699636693361,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cEOiTodYuG",
                "forum": "xXtD9P2lvH",
                "replyto": "zU76HZEuEX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6309/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6309/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q:** The experimental evaluation is a bit substandard given the relatively large recent literature on the topic. The authors should at least match SPECTRE (cited) for the evaluation protocol.\n\n**A:** We agree that there is a large literature about graph generation. However, the literature focuses mostly on undirected graphs, whereas our paper focuses on directed graphs. All our training graphs are directed, including Erdos-Renyi or stochastic block models. We explain in Section 4 how Spectre cannot be applied to directed graphs as their method assumes that their Laplacian matrix lies on a Stiefel manifold, which is not the case when the graph is directed (i.e. the adjacency matrix is not symmetric). \n\n\n\n**Q:** \"While it is clear that the link predictor tries to match what it has seen in the training set, it is not clear how their approach changes the Erdos-Renyi distribution to one more similar to the training set.\"\n\n**A:**  About the evaluation on the Erdos-Renyi distribution category specifically, we use a probability of $p = 0.6$ in Table 1, and $p=0.4$ in Table 2 to generate the training digraphs in this category. In those tables, we report the evaluation metric based on the in-degree MMD between the training and generated sets. \nEven in the directed case, the Erdos-Renyi category is defined by the overall in-degree of nodes. Our approach results in low MMD for that metric, which means that in-degrees between training and generated graphs are similar."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6309/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700657086600,
                "cdate": 1700657086600,
                "tmdate": 1700658850551,
                "mdate": 1700658850551,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]