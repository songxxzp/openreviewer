[
    {
        "title": "ResBit: Residual Bit Vector for Categorical Values"
    },
    {
        "review": {
            "id": "Teq17gfEkz",
            "forum": "RPhoFFj0jg",
            "replyto": "RPhoFFj0jg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5859/Reviewer_Zunf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5859/Reviewer_Zunf"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a new encoding technique for categorical values: residual bit vectors which are computed iteratively as bit-representations of category number (where category is treated as an actual number; see section 3.2. for a detailed explanation). The motivation for this work is in the application of one-hot encoded vectors: the authors are motivated to train table diffuison models where input/outputs are tabular data that can potentially have millions of categories. Training the diffusion model with million dimensional input outputs is a challenge, thus some lossless dimensionality reduction is needed. Why not compute bit representation once? Paper defines the main issue with simple bitwidth as an \"out of index\" problem, meaning that if total number of categories are not exact power of 2, say 9, then the bit representation of such a categories introduces extra \"sampling\" dimensions that might be an issue during diffusion training/sampling. In case of 9, its representation would require 4 bits, i.e. 9=1001; during diffusion sampling a number 1011 can be sampled, which would correspond to non-existing category. \n\nAuthors test out their proposed encoding method on several datasets in a TSTR manner (train on synthetic, test on real): they train a diffusion model to generate synthetic data, train a classifier/regressor on just generated data, and test the classifier/regressor on the actual data used for the diffusion model training. Results indicate that a proposed encoding is on par with simple log_2 encoding."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The proposed method is very simple to understand and implement."
                },
                "weaknesses": {
                    "value": "Paper has two weaknesses: results and presentation\n1. Results. On 5 tabular datasets where the such an encoding method would be of most use, the proposed is clearly better only on 2 of the tasks (CC, AR), whereas on BD and AD performance is on par, and I'm going to discount any results on IS due to the size of dataset (1338).  Similarly, when used for conditioning of GANs, visually speaking res-bit results seems to be worse (much less diverse) and have no strong edge over one-hot in classification tasks. Considering these observations, it is hard to say that residual bitwidth representation of categorical values is a good encoding in general. \n2. Presentation & Motivation. I the writing and the flow of the paper hard to follow. Initial pages are more like a catalog book of ml methods (section 2 in particular) rather than a cohesive presentation of ideas. The paper has many stylistic issues like using \"that this\", \"very widely used\", \"limit the increase in dimensionality to a logarithmic increase\" and etc. Also, I find the motivation a bit underdeveloped. Section 3.1. explains the \"out of index\" issue but does not provide evidence whether it is indeed the main cause that limits the model performance. I, generally believe, that a well trained model would learn to not sample from category bits that does not exist. It would be an important addition to the paper to show not only better results but provide evidence that improvement was due to solving out-of-index issue."
                },
                "questions": {
                    "value": "Please address weaknesses above as much as possible."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5859/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698816383871,
            "cdate": 1698816383871,
            "tmdate": 1699636620319,
            "mdate": 1699636620319,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Y2An9ekdvn",
                "forum": "RPhoFFj0jg",
                "replyto": "Teq17gfEkz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5859/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5859/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer Zunf (except reference)"
                    },
                    "comment": {
                        "value": "Thank you for your constructive feedback. Here we response to your questions as much as possible. \n\n**As for Weakness 1**\n\nResults from the remaining datasets experimented in the TabDDPM paper, which include categorical columns, are presented in the COMMON RESPONSE. According to the findings, the results are comparable for the cardio dataset and slightly inferior for the abalone dataset. However, the facebook dataset, with 157,638 training data, demonstrates significant results. If we exclude abalone and IS due to the small dataset sizes, TRBD overall equals or surpasses TabDDPM. The above, coupled with the runtime results (refer to Section 4.1.5 and Table 4), are significant in that the same or better results can be achieved in a shorter time. Concerning the conditioning of GANs, when using CIFAR10, it can be visually said that more mode collapse can be observed in the one-hot case compared to ResBit (see Appendix. C). Furthermore, ResBit is superior to one-hot in terms of FID, so it is not a result that the superiority of ResBit is not generally recognized. As for image classification, there is certainly no strong advantage. However, as mentioned **twice** in our paper (see Sections 4.2 and 4.3), the main purpose of these two experiments is not to show the superiority in terms of accuracy. **We aim to confirm that ResBit can be used as an alternative to one-hot while reducing the number of dimensions (see Section 4.4)**. Therefore, the lack of strong significance in terms of accuracy is not a concern. In addition, even if it is somewhat inferior in terms of accuracy, in this experimental setting, the runtime has an advantage (refer to the table below). Considering the trade-off between runtime and quality, there is no compelling reason to strongly negate this advantage.\n\n\nTable 1. Comparison the runtime using SNGAN and 40 selected class from the Food-101.\n|  |  | | | |\n| :------: | :--------------------: | :--------------: | :--------------: | :--------------: |\n| Method  | Label dim | #params D | #params G | Training Time (hours)\n| one-hot | 40 | 38,446,736 | 32,806,147 | 8.75 |\n| ResBit | **9** | **38,424,896** | **32,298,243** | **8.25** |\n    \n\n**As for Weakness 2**\n\n(Regarding motivation) Certainly a well-trained model may not generate data from non-existent category bits, but what makes it any different from over-fitting or copying training data? For example, large language models (LLMs) and text-to-image diffusion models (DMs) have been suggested to be able to extract training data [1, 2]. We consider tabular data to be more sensitive than images or text, since they often directly contain personal information, such as financial or physical data. In addition, the data used in our experiment is not large, 600k at most. Furthermore, the datasets used in recent table data generation using Diffusion Models [3, 4, 5] are no larger than this. In [6], it is stated that \"Typically, this type of memorization happens when the model makes many passes over a small training set\". It is thought that over-fitting is particularly likely to occur in the generation of tabular data, where small data sets are not uncommon. Therefore, it is not sufficient to train them on as much data as LLMs and DMs. Assuming it works, Machine Unlearning [7] is a way to address the privacy issue, but we believe it will only lead to loss of diversity in the generative task."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5859/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700546222811,
                "cdate": 1700546222811,
                "tmdate": 1700546222811,
                "mdate": 1700546222811,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pf7p62QfhL",
            "forum": "RPhoFFj0jg",
            "replyto": "RPhoFFj0jg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5859/Reviewer_42Kk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5859/Reviewer_42Kk"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a hierarchical bit representation called Residual Bit Vector (ResBit) to address the complexity issue of one-hot encoding of categorical data. Because the number of elements of one-hot encoding grows linearly with the number of categories, the increased dimensionality may be harmful to performance. ResBit mainly follows the idea of residual vector quantization (Juang & Gray, 1982). It finds binary representation hierarchically and is shown to avoid the so-called \u201cout-of-index\u201d problem for some cases. Several experiments in tabular data generation, image generation, and image classification are conducted to study the performance of ResBit. Mixed results are reported."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "I find it really hard to find the strengths of this paper. See the reasons below."
                },
                "weaknesses": {
                    "value": "- There are several false claims in the paper. First, ResBit may not fully address the \u201cout-of-index\u201d issue. Since $N=50=32+16+2$, the example given in the paper is free from the issue. Proof for any natural number is missing. One can find a counterexample by find the ResBit representation of $N=51$? Second, the ResBit does not really improve or at least achieve no worse results compared to their baselines. In some cases, ResBit even performs much worse than the baselines.\n\n- Some descriptions in the paper are not clear. For example, the authors claim that increasing the dimensionality can cause model learning to fail. It is not clear to me why and how it fails. For example, overparameterization can lead to better results. Providing some references could be helpful.\n\n- In Section 4.1.4, the authors state that the loss exploded or disappeared during the training phase of TabDDPM for certain datasets and argue that that is probably due to the very large number of dimensions. This seems to be a strange reason because the dimensions are not too large in these problems and usually this kind of problem can be addressed by normalizing the features or using smaller learning rates.\n\n- The runtime comparison seems unfair because the TabDDPM and TRBD use different networks with different number of layers.\nIn Section 4.3, it would make more sense to use ResBit for datasets like ImageNet. CIFAR-10 only has 10 classes so the reduction of the encoding of the categories is insignificant.\n\n- In Section 4.4, the authors argue that ResBit reduces the representation complexity of categorical data. However, this would be only meaningful when the performance of ResBit is justified."
                },
                "questions": {
                    "value": "1. Can we prove that ResBit does not have the \u201cout-of-index\u201d issue mathematically?\n\n2. Given that ResBit is proposed for reducing the representation complexity of categorical data, have you tried to run ResBit for image classification on ImageNet? Does it maintain the performance compared to one-hot encoding while achieving lower complexity?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5859/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5859/Reviewer_42Kk",
                        "ICLR.cc/2024/Conference/Submission5859/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5859/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698821544189,
            "cdate": 1698821544189,
            "tmdate": 1700641459051,
            "mdate": 1700641459051,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "B199ZV4jlC",
                "forum": "RPhoFFj0jg",
                "replyto": "pf7p62QfhL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5859/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5859/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer 42Kk (except reference)"
                    },
                    "comment": {
                        "value": "Thank you for your constructive comments. We response to your questions and concerns.\n\n> Can we prove that ResBit does not have the \u201cout-of-index\u201d issue mathematically?\n\n> First, ResBit may not fully address the \u201cout-of-index\u201d issue. Since $N=50=32+16+2$, the example given in the paper is free from the issue. Proof for any natural number is missing. One can find a counterexample by find the ResBit representation of $N=51$?\n\nSince ResBit is to be applied when the number of classes is known in advance, the reviewer seems to have a major misunderstanding. In the analog bits, \"out of index\" occurs when the length of a class increases, even if the number of classes is known in advance. The $N=50$ shown in our paper is a perfect example. If $N=2^k$, \"out of index\" does not occur. ResBit is an expression that depends on the number of classes, similar to one-hot, etc. In the example of the states of the United States, one-hot considers a 50-dimensional vector because one-hot depends on the number of classes.\u3000In the following, we formulate the ResBit. The number of classes is set to $N\\geq2$. When $N=1$, there is no problem in the actual task, so we do not consider it (e.g., if conditioning, it corresponds to no condition). Considering 0-index, ResBit can be written as follows.\n\n$$\n    N-1=\\sum_{k=0}^{\\infty}a_k(2^{k}-1)\\qquad \\mathrm{s.t.}\\min\\sum_{k=0}^{\\infty}a_kk\n$$\n\nThe $k$-th term can represent any natural number in the range $[0, a_k(2^k-1)]$. It is easy to decompose $i\\in\\mathbb{Z}$ with $0\\leq i<N$ as the sum of such natural numbers.\n\n\n> Given that ResBit is proposed for reducing the representation complexity of categorical data, have you tried to run ResBit for image classification on ImageNet? Does it maintain the performance compared to one-hot encoding while achieving lower complexity?\n\nAs described in Section 4.4, this is a future work. Note that there are two possible ways to evaluate the output of ResBit in this experiment. In both cases, the output of the model is rounded.\n\n- Check if the output of the model matches the ResBit of the correct label\n- Convert the model output to an integer value and check if it matches the correct label\n\nSince the accuracy was the same in both cases, the model is considered to correctly represent ResBit. Therefore, we expect the same degree of difficulty in experiments on datasets with numerous labels, such as ImageNet-1k, as in the one-hot case. Note that image classification in ImageNet is also possible by ensembling many classifiers like ACGAN[1].\n\n\n> the ResBit does not really improve or at least achieve no worse results compared to their baselines. In some cases, ResBit even performs much worse than the baselines.\n\nMany of our experiments show applicability. Learning with ResBit is equivalent to multi-label classification, and in some cases, devising multi-class classification alone may not be sufficient. In our experiments, the one-hot method is slightly more advantageous than the one-hot method because the one-hot classification employs top-1. However, this is not the case in most cases in the experiment. Therefore, only some bad results should not be emphasized.\n\n\n> The runtime comparison seems unfair because the TabDDPM and TRBD use different networks with different number of layers.\n\nAs for the runtime comparison, we show in the table below the results when we use same networks with the same number of layers.\n\n|  |  | | |\n| :------: | :--------------------: | :--------------: | :--------------: |\n| Dataset  | layers | train | sample |\n| CH (TabDDPM) | [512, 1024, 1024, 1024] | 516s | 42s |\n| CH (TRBD) | [512, 1024, 1024, 1024] | **264s** | **21s** |\n| IS (TabDDPM) | [1024, 512, 512, 1024] | 385s | 28s |\n| IS (TRBD) | [1024, 512, 512, 1024] | **173s** | **17s** |\n\nBecause the number of layers has been reduced, a slight speedup in training time has been achieved. On the other hand, TRBD's superiority in speed is unassailable; the difference in input dim is the difference between ResBit and one-hot.\n\n> In Section 4.1.4, the authors state that the loss exploded or disappeared during the training phase of TabDDPM for certain datasets and argue that that is probably due to the very large number of dimensions. This seems to be a strange reason because the dimensions are not too large in these problems and usually this kind of problem can be addressed by normalizing the features or using smaller learning rates.\n\nIt is unfair if the experiment is not conducted in the same learning rate search space (fairness is what you pointed out as a weakness). In the same search space, the fact that the loss exploded or disappeared for TabDDPM but not for TRBD may be due to the number of dimensions.\u3000This is because there is no other difference between TabDDPM and TRBD.\u3000In the experiments with Multinomial Diffusion [2], the number of classes is 8, 27, and 256, so compared to those, 6k and 7k are very large and not something to consider in general terms."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5859/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700546299674,
                "cdate": 1700546299674,
                "tmdate": 1700546299674,
                "mdate": 1700546299674,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RdblbDBILT",
                "forum": "RPhoFFj0jg",
                "replyto": "pf7p62QfhL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5859/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5859/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer 42Kk (reference)"
                    },
                    "comment": {
                        "value": "## Reference\n[1] Augustus Odena and Christopher Olah and Jonathon Shlens. Conditional Image Synthesis with Auxiliary Classifier GANs. Proceedings of the 34th International Conference on Machine Learning, in Proceedings of Machine Learning Research 70:2642-2651 Available from https://proceedings.mlr.press/v70/odena17a.html.\n\n[2] Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forre \u0301, and Max Welling. Argmax flows and multinomial diffusion: Learning categorical distributions. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=6nbpPqUCIi7."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5859/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700546327776,
                "cdate": 1700546327776,
                "tmdate": 1700553677500,
                "mdate": 1700553677500,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "d7iqj4YHss",
                "forum": "RPhoFFj0jg",
                "replyto": "RdblbDBILT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5859/Reviewer_42Kk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5859/Reviewer_42Kk"
                ],
                "content": {
                    "comment": {
                        "value": "I express my gratitude to the authors for their responses. Several of my minor concerns have been appropriately addressed, leading me to revise my rating upward from 1 to 3. However, it is crucial to note that my significant reservations regarding ResBit remain unchanged. The assertions made about its performance and complexity reduction necessitate further experimentation and robust justifications."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5859/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700642102245,
                "cdate": 1700642102245,
                "tmdate": 1700642102245,
                "mdate": 1700642102245,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "KigNiAp0Nk",
            "forum": "RPhoFFj0jg",
            "replyto": "RPhoFFj0jg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5859/Reviewer_Dspo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5859/Reviewer_Dspo"
            ],
            "content": {
                "summary": {
                    "value": "In the paper, the authors propose a Residual Bit Vector (ResBit), which is a hierarchical bit representation. Authors also show that such representation can be used to build a tabular data generation method called TRBD. TRBD can generate diverse and high-quality data from small-scale table data. ResBit was also used in GANs or conditioning."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper introduces the interesting extension of Analog Bits.\n2. The paper has good theoretical fundaments."
                },
                "weaknesses": {
                    "value": "1. In the abstract, the authors introduce methods in a different order than in the introduction. It is misleading. Maybe it is possible to do it consistently.\n2. The first Fig 1. in the paper refers to the reference paper. Maybe at the beginning, authors can give some illustrations describing the new proposed method. \n3. Some illustrations of the method should be added.\n4. The model proposes three elements: ResBit, TRBD, and conditioning GAN. Unfortunately, none of such components are well evaluated. Especially ResBit should be compared with Analog Bits.\n5. In TabDDPM, authors propose experiments on 15 datasets with many baselines. Authors should follow such an experimental setting. \n6. Maybe authors should introduce fewer components but add more detailed comparisons with existing methods.\n7. Maybe it is possible to run the algorithms on an image dataset."
                },
                "questions": {
                    "value": "1. How the ResBit algorithm works concerning Analog Bits.\n2. Maybe it is possible to show some practical tasks to show that ResBit works better than Analog Bits.\n3. The United States example is convincing, but the authors should present that such a problem is a real problem in practical application."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5859/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5859/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5859/Reviewer_Dspo"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5859/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698924440562,
            "cdate": 1698924440562,
            "tmdate": 1699636620057,
            "mdate": 1699636620057,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KffFUYY8u3",
                "forum": "RPhoFFj0jg",
                "replyto": "KigNiAp0Nk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5859/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5859/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer Dspo (except reference)"
                    },
                    "comment": {
                        "value": "Thank you for your many valuable feedback and suggestions. Here we address responses to your questions and weakness.\n\n> How the ResBit algorithm works concerning Analog Bits.\n\nResBit is a fusion of part of the Analog Bits concept, which represents discrete data as a sequence of bits, and the RVQ concept, which further vector quantizes the difference of vector quantization. The full application of the Analog Bits concept requires the high expressive power of Diffusion Models, and TRBD is an example of its application. On the other hand, ResBit can be considered to work the same as one-hot if it is considered as a method of expressing ResBit as  0/1. Its application to image classification and conditional image generation is a verification of this. In addition, if the number of classes is represented by $2^m$, it is truly the same as the analog bits. Representing the pixels of an image in such a way is exactly what was experimented in the analog bits paper.\n\n> Maybe it is possible to show some practical tasks to show that ResBit works better than Analog Bits\n\n> The model proposes three elements: ResBit, TRBD, and conditioning GAN. Unfortunately, none of such components are well evaluated. Especially ResBit should be compared with Analog Bits.\n\nWe cannot evaluate in the tabular data generation task due to the \"out of index\" problem described in Section 3.1. On the other hand, we can evaluate the other two applications. Let $N$ be the number of classes and consider the smallest $m\\in\\mathbb{N}$ satisfying $N<2^m$ holds. In this experiment, $m$-dimensional vectors are used to represent the classes. For example, for $N=10$, $m=4$. The table below are the experimental results when using CIFAR-10. The results for one-hot and ResBit are those in the paper.\n\n   \n|  |  | | |\n| :------: | :--------------------: | :--------------: | :--------------: |\n| Task  | Image Classification | Image Classification | Image Generation |\n| Used Model | MobileNetV3-small | MobileNetV3-large | InfoGAN | \n| Metrics | Accuracy (%) | Accuracy (%) | FID |\n| Result (one-hot) | 66.52 | **74.63** | 95.52 |\n| Result (ResBit) | **66.70** | 72.55 | 82.21 |\n| Result (analog) | 62.70 | 70.62 | **63.54** |\n\nIn the context of conditioning, \"out of index\" scenario is not deemed a significant issue; rather, it leads to a straightforward reduction in the number of dimensions, which is believed to enhance accuracy. Conversely, in the realm of image classification, predicting an \"out of index\" situation could result in diminished accuracy. Consequently, we consider that Analog Bits are perceived as less versatile than ResBit.\n\n> The United States example is convincing, but the authors should present that such a problem is a real problem in practical application.\n\nThere are much information exists in fraudulent transaction data in the financial field. In general, the ratio of anomaly data to total data is very low. In such cases, oversampling may be performed using methods such as SMOTE [1]. In recent years, deep learning-based methods have also been researched, in which the entire table data is generated once and then anomaly data is extracted after feature engineering and other processes are performed. The method of masking by special string [2] pointed out in our paper hides rare data, which may affect the accuracy of anomaly detection. Our method can generate all data without masking. If the accuracy of anomaly detection is not improved, it is easier to analyze whether the cause is the data generation method itself or the loss of diversity due to masking.\n\n\n> In TabDDPM, authors propose experiments on 15 datasets with many  baselines. Authors should follow such an experimental setting.\n\n> Maybe authors should introduce fewer components but add more detailed comparisons with existing methods.\n\nOf the 15 datasets listed in TabDDPM, the results of those not covered in the paper are shown in the COMMON RESPONSE. Note that we do not conduct the experiments for data that have only numerical datasets, since TRBD and TabDDPM are the same in such datasets. We use CatBoost in the TSTR framework. When integrated with the results in our paper, TRBD outperforms or equals TabDDPM on most datasets. This shows the superiority of the proposed method in generating tabular data. Also, since TabDDPM has been reported to significantly outperform existing deep generative models [3, 4, 5] in the TabDDPM paper, we do not believe that we need to use them as a baseline.\n\n\n> Maybe it is possible to run the algorithms on an image dataset.\n\nRepresenting the pixels of an image in ResBit is the same as representing them in Analog Bits (Since the pixels of an image are integer values of [0, 255]). An experiment to represent image pixels using Analog Bits and image generation using Diffusion Models has been conducted in [6]. There is no need for us to perform the experiments already validated in [6]."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5859/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700546034275,
                "cdate": 1700546034275,
                "tmdate": 1700628941127,
                "mdate": 1700628941127,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dat3BIJV4t",
                "forum": "RPhoFFj0jg",
                "replyto": "KigNiAp0Nk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5859/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5859/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer Dspo (reference)"
                    },
                    "comment": {
                        "value": "## Reference\n[1] Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer. Smote: synthetic minority over-sampling technique. Journal of artificial intelli- gence research, 16:321\u2013357, 2002.\n\n[2] Masane FUCHI, Amar ZANASHIR, Hiroto MINAMI, and Tomohiro TAKAGI. Generating a wide variety of categorical data using diffusion models. Proceedings of the Annual Conference of JSAI, JSAI2023:2K5GS203\u20132K5GS203, 2023. doi: 10.11517/pjsai.JSAI2023.0 2K5GS203.\n\n[3] Zilong Zhao, Aditya Kunar, Robert Birke, and Lydia Y Chen. Ctab-gan: Effective table data synthesizing. In Asian Conference on Machine Learning, pp. 97\u2013112. PMLR, 2021.\n\n[4] Lei Xu, Maria Skoularidou, Alfredo Cuesta-Infante, and Kalyan Veeramachaneni. Modeling tabular data using conditional gan. In Advances in Neural Information Processing Systems, 2019.\n\n[5]  Zilong Zhao, Aditya Kunar, Robert Birke, and Lydia Y. Chen. Ctab-gan+: Enhancing tabular data synthesis, 2022.\n\n[6] Ting Chen, Ruixiang ZHANG, and Geoffrey Hinton. Analog bits: Generating discrete data using diffusion models with self-conditioning. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=3itjR9QxFw."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5859/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700546101893,
                "cdate": 1700546101893,
                "tmdate": 1700546143695,
                "mdate": 1700546143695,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]