[
    {
        "title": "Causal Modelling Agents: Causal Graph Discovery through Synergising Metadata- and Data-driven Reasoning"
    },
    {
        "review": {
            "id": "WfhW7dcOWF",
            "forum": "pAoqRlTBtY",
            "replyto": "pAoqRlTBtY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8513/Reviewer_pSg2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8513/Reviewer_pSg2"
            ],
            "content": {
                "summary": {
                    "value": "The paper combines the meta-data driven Large Language Models (LLMs) and data-driven Deep Structural Causal Models (DSCMs) to construct a novel framework called Causal Modeling Agent for causal discovery. The framework leverages the LLMs' state-of-the-art capability to capture domain knowledge to discover the causal relationship in DSCMs. The framework is tested against a number of benchmarks on the real-world task of modeling the clinical and radiological phenotype of Alzheimer's Disease (AD), which has a ground-truth causal relationship between the vertices. The experimental results suggest that the CMA outperforms purely data-driven and metadata-driven benchmarks. New insights into the causal relationship among biomarkers of AD have also been obtained by CMA."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The idea to combine LLM and SCM is interesting and novel.\n2. The experimental results are encouraging.\n3. New insights on the causal relationship between biomarkers have been obtained."
                },
                "weaknesses": {
                    "value": "The contribution would be stronger if further evidence from experimental or observational data can be provided for the discovered causal relationships with the CMA."
                },
                "questions": {
                    "value": "Can the authors provide further evidence from experimental or observational data for the discovered causal relationships with the CMA?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8513/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8513/Reviewer_pSg2",
                        "ICLR.cc/2024/Conference/Submission8513/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8513/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698456324316,
            "cdate": 1698456324316,
            "tmdate": 1700554727071,
            "mdate": 1700554727071,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fV7gsIeiej",
                "forum": "pAoqRlTBtY",
                "replyto": "WfhW7dcOWF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8513/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8513/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their positive review of our work. We are glad they find our experiments encouraging and the combination of LLMs and (deep) structural causal modelling to be interesting and novel. To address their central query, we have updated the paper to add additional analyses based on our observational datasets in **Appendix A.3.4** and **Appendix A.7**.\n\n In addition, we have also updated the paper to include a new **Appendix A.7.1** which highlights additional experimental/observational evidence from the literature which contextualises and/or corroborates the causal relationships discovered by the CMA in the real-world AD experiment."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8513/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700187099590,
                "cdate": 1700187099590,
                "tmdate": 1700187099590,
                "mdate": 1700187099590,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xnRtgcJTE2",
                "forum": "pAoqRlTBtY",
                "replyto": "fV7gsIeiej",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8513/Reviewer_pSg2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8513/Reviewer_pSg2"
                ],
                "content": {
                    "comment": {
                        "value": "The statistical correlations presented in Appendix A.3.4 do suggest the causal relationship between LW and HFLX. An experiment that intervenes LW may help reveal such causality."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8513/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700554680446,
                "cdate": 1700554680446,
                "tmdate": 1700554680446,
                "mdate": 1700554680446,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pWHKjMKPyi",
                "forum": "pAoqRlTBtY",
                "replyto": "WfhW7dcOWF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8513/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8513/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to comment"
                    },
                    "comment": {
                        "value": ">The statistical correlations presented in Appendix A.3.4 do suggest the causal relationship between LW and HFLX. An experiment that intervenes LW may help reveal such causality.\n\n  \n\nThank you for your response. We had previously conducted an experiment which intervenes on the LW variable in **Appendix A.3.4**. To maximise clarity, we have amended the manuscript to illustrate these results in two separate figures (instead of the single **Figure 13**). For the experimental/counterfactual intervention, please also see the [anonymised figure](https://i.imgur.com/3T1eMcb.png).\nIndeed, for each of our discovered novel relationships, we have included:\n\n1.  A regression (observational) analysis, as requested, to assess for possible statistical associations between the variables.\n    \n2.  An experiment which intervenes on the parent variable (counterfactual intervention) to assess a given relationship from a causal perspective.\n    \n3.  Experimental/observational evidence from the literature to contextualise and/or corroborate our findings.\n    \n\nPlease find references for these analyses below:\n\n-   LW \u2192 HFLX:\n    1.  Observational analysis: **Figure 13, Appendix A.3.4**. \n    2.  Counterfactual intervention: **Figure 14, Appendix A.3.4**. \n    3.  Literature: **Section 4.2**, main manuscript.  \n\t    \n\n-   Sex \u2192 P-tau:\n     1.  Observational analysis: **Figure 34, Appendix A.7**\n    2.  Counterfactual intervention: **Figure 3B**, main manuscript.   \n    3.  Literature: **Section 4.3**, main manuscript.\n    4.  Additional literature: **Appendix A.7.1**\n    \n\n-   sTREM2 \u2192 P-tau:\n    1.  Observational analysis: **Figures 32 and 33, Appendix A.7**\n    2.  Counterfactual intervention: **Figure 31, Appendix A.7**\n    3.  Literature: **Section 4.3**, main manuscript.\n    4.  Additional Literature: **Appendix A.7.1**\n    \n___\n\nThank you once more for your time. Please let us know if there are any additional points you\u2019d like to discuss, and if not, we hope that the reviewer might consider an update."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8513/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700610752828,
                "cdate": 1700610752828,
                "tmdate": 1700617655366,
                "mdate": 1700617655366,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xQTpbReJb9",
                "forum": "pAoqRlTBtY",
                "replyto": "pWHKjMKPyi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8513/Reviewer_pSg2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8513/Reviewer_pSg2"
                ],
                "content": {
                    "comment": {
                        "value": "What is the respective treatment and control groups for the experiment that generates Figures 3B, 14, and 31?"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8513/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700721987608,
                "cdate": 1700721987608,
                "tmdate": 1700721987608,
                "mdate": 1700721987608,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "UHRG0YtLyO",
            "forum": "pAoqRlTBtY",
            "replyto": "pAoqRlTBtY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8513/Reviewer_CmY7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8513/Reviewer_CmY7"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors devised a causal discovery algorithm that utilizes LLM\u2019s ability on causal reasoning using meta-data. In particular, they proposed Causal Modeling Agent (CMA), which iteratively updates a causal graph through: i) asking LLM for updates on current prediction of edges with previous graph update information; and 2) fitting a model constrained over the intermediately constructed causal graph (using deep learning to model causal mechanism for each variable). Through experiments on benchmark datasets (e.g., K\u0131c\u0131man et al. (2023)) and a case study of Alzheimer's disease, they empirically demonstrated a potential of their framework outperforming some of causal discovery algorithms and LLMs."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Paper is overall written concisely due to multiple modules involved in the framework. The idea of encoding intermediate results in a JSON format and feeding them into an LLM seems clever."
                },
                "weaknesses": {
                    "value": "- The use of LLM to tweak intermediate results seems clever but it is hard to assess its technical contribution. \n- It is unknown how LLM is doing with respect to its memory. Does LLM always try to update edges in order to maximize data fitting? If the data fitting is based on the currently predicted causal graphs, how can it improve its causal graph? It does not work like an EM algorithm. Does LLM \u2018regret\u2019 its previous decision if fitting becomes worse? Considering developing a causal discovery algorithm that is based on local search (incrementally updating causal graph based on its likelihood), how would you compare their learning trajectories?\n- Use of data only to fit the intermediate graph seems not using available dataset in full. Such as conditional independence and other information is all unused.\n- LLM\u2019s stochastic nature is ignored. LLM may answer differently for the same question.\n- It is essential to thoroughly examine the behavior of LLM. How does it adjust the result based on its belief (GPT-4 etc) and intermediate results passed. There are more questions remained than answered."
                },
                "questions": {
                    "value": "The word \u201cmetadata\u201d is somewhat used in a mixed manner between domain knowledge already encoded in LLM and memory passed through JSON format. It should be more formally defined. \n\nResults\nGiven that cases with no edges outnumber those with edges, not predicting edges may lead to an increase in accuracy. Thus, a qualitative analysis is necessary since not predicting edges might lead to an increase in the score. Other metrics such as TPR or FDR can be reported.\n\nNovelty\nGiven the abundance of similar papers (Long, S., Pich\u00e9, A., Zantedeschi, V., Schuster, T., & Drouin, A. (2023). Causal discovery with language models as imperfect experts. arXiv preprint arXiv:2307.02390., Ban, T., Chen, L., Wang, X., & Chen, H. (2023). From query tools to causal architects: Harnessing large language models for advanced causal discovery from data. arXiv preprint arXiv:2306.16902.) in the field, the contribution is not clear. \n\nI noticed discrepancies between what was mentioned and the results such as the performance of gpt-4 in the table 7 in K\u0131c\u0131man et al. (2023). For example, NHD of GPT 4 in K\u0131c\u0131man et al. (2023) was reported as 0.22 but you reported 0.35 for GPT 4 in the table 2 in your paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8513/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698815111896,
            "cdate": 1698815111896,
            "tmdate": 1699637063773,
            "mdate": 1699637063773,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PssPp9mjxP",
                "forum": "pAoqRlTBtY",
                "replyto": "UHRG0YtLyO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8513/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8513/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response 1/3"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their insight and questions.\n\nWe hope to have addressed the reviewer's comments and, if so, they would consider updating their score. We\u2019d be happy to engage in further discussions.\n\n___\n\n> -   The use of LLM to tweak intermediate results seems clever but it is hard to assess its technical contribution.\n> -   Novelty Given the abundance of similar papers (Long, S., Pich\u00e9, A., Zantedeschi, V., Schuster, T., & Drouin, A. (2023). Causal discovery with language models as imperfect experts. arXiv preprint arXiv:2307.02390., Ban, T., Chen, L., Wang, X., & Chen, H. (2023). From query tools to causal architects: Harnessing large language models for advanced causal discovery from data. arXiv preprint arXiv:2306.16902.) in the field, the contribution is not clear.\n\nThank you for noting these important related works. In contrast to previous work, the CMA is the first framework to use LLMs as priors, critics during an iterative learning procedure, and as post-processors. We demonstrate the utility of this approach as per our results in **Section 4.2**, and in **Section 4.3**, we identify novel causal structures in the challenging real-world task of jointly modelling the radiological and clinical phenotype of Alzheimer\u2019s disease. An additional novel aspect of our work is that our framework is the first causal discovery approach to output a multi-modal structural causal model (that is, we generalise and extend the DSCM framework), which enables reasoning over multi-modal data (e.g., for computing counterfactuals in the imaging space, as can be seen in Figure 3, Panel C of the main manuscript).  \n  \nThe method developed by Long et al.[1] assumes knowledge of the Markov Equivalence Class (MEC) containing the true Directed Acyclic Graph (DAG). This is a strong assumption for real-world applications as it presumes optimal outputs from causal discovery algorithms. Their work shows promising results in substituting an LLM in lieu of a human expert in this setting, but their approach serves as a post-processing step, reliant on a priori knowledge of a MEC which contains the true causal graph. In contrast, the CMA does not require any prior assumptions about the graph's structure.  \n  \nBan et al.[2] use an LLM's output as a prior for causal discovery algorithms, demonstrating its effectiveness with synthetic datasets, an approach also explored in the \u2018LMPriors\u2019 paper by Choi et al.[3]. However, this method lacks a feedback mechanism for hypothesis refinement, potentially limiting novel causal link detection by ignoring signals from the data. An example of this is the 'biological sex' -> 'phosphorylated tau protein' link identified by the CMA in **Section 4.3**, but overlooked by domain experts. We additionally note that in contrast to Ban et al.[2], we consider a real-world multi-modal dataset as one of our experiments, demonstrating our framework\u2019s empirical utility for more complex and varied data scenarios beyond synthetic cases.  \n  \nWe have revised the text in the Related works section of the paper to better highlight these differences.  \n\n___\n\n> It is unknown how LLM is doing with respect to its memory. Does LLM always try to update edges in order to maximize data fitting? If the data fitting is based on the currently predicted causal graphs, how can it improve its causal graph? It does not work like an EM algorithm. Does LLM \u2018regret\u2019 its previous decision if fitting becomes worse? Considering developing a causal discovery algorithm that is based on local search (incrementally updating causal graph based on its likelihood), how would you compare their learning trajectories?\n\n As you correctly point out, the data fitting step at a given iteration is based on the currently predicted causal graph. However, the CMA uses both the current data fit, as well as the data fit from the previous iteration to produce a memory. As we show in the \u2018post-processing\u2019 paragraph of **Section 3**: $\\mu_t=\\textsf{LLM}{\\mu}(\\mathcal{G}^s_t,\\mathcal{G}^s_{t-1},F_t,F_{t-1})$, where for iteration $t$, $\\mathcal{G}_t$ is the causal graph and $F_t$ is the metric of fit.\n\n\n \n The prompting strategy and procedure for memory generation is further outlined in \u2018Listing 3: System prompt for memory generation\u2019, and \u2018Algorithm 5: Post-processing and memory generation\u2019 in **Appendix A.1.5**. Exactly as you intuited, the LLM is therefore prompt-engineered to \u2018regret\u2019 its previous decisions if fitting degrades, and to select an action or set of actions to improve fit. The LLM constrains the super-exponential search space of potential causal graphs, which a naive local-search-based algorithm would need to explore."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8513/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700186903096,
                "cdate": 1700186903096,
                "tmdate": 1700186903096,
                "mdate": 1700186903096,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "26IRK4PY3k",
                "forum": "pAoqRlTBtY",
                "replyto": "UHRG0YtLyO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8513/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8513/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response 2/3"
                    },
                    "comment": {
                        "value": ">  Use of data only to fit the intermediate graph seems not using available dataset in full. Such as conditional independence and other information is all unused.\n\nWe believe the reviewer means that there are conditional independence statements which exist within the dataset that are not being utilised. If this is the case, then in fact we do utilise conditional independence information from the data. The metadata-based modules of the CMA produce a hypothesis (DAG) which is then encoded as a Deep Structural Causal Model (DSCM). Conditional independence statements are encoded within the structure of the DSCM at the current iteration and, as shown in **Section 4.1**, this has an effect on data likelihood under the model (up to Markov Equivalence; see **Appendix A.2.3** for more details).\n\n___\n>LLM\u2019s stochastic nature is ignored. LLM may answer differently for the same question.\n\n\nThank you for this very important insight. We very much agree that LLMs do not always answer the same question deterministically, however we take account of this by querying the LLMs multiple times to mitigate for potential stochasticity in the outputs. We also attempt to account for failure modes in LLM reasoning. For example, Berglund et al. [4] identified a 'reversal curse' in LLMs, where they fail in logical deduction based on question order, not always generalising from 'A is B' to 'B is A'. To additionally address this layer of variability, queries are also made in both causal directions for local phase amendments. This process is described in more detail in **Appendix A.1.3**.\n\n\n___\n> It is essential to thoroughly examine the behavior of LLM. How does it adjust the result based on its belief (GPT-4 etc) and intermediate results passed. There are more questions remained than answered.\n\nThank you once more for this important point. We strongly agree that investigating the behavioural patterns of LLMs is important to better understand their outputs, and indeed had performed a behavioural analysis in **Appendix A.2.4**: \u2018Additional Results II - LLM Behavioural Patterns\u2019. Here, we investigate behavioural patterns for three types of relationships: 1) Relationships which might plausibly exist in either causal direction (i.e., A -> B or B -> A), 2) Relationships which should only exist in a single direction (i.e., A -> B, but not B -> A), and, 3) Variables which should have no functional causal relationship between them. Furthermore, for each of these types of relationship, we assess LLM behaviour by first assuming the non-existence of the relationship, and then its existence in the A -> B direction, and then in the B -> A direction (**Appendix A.2.4; Figure 5**). \nWe additionally investigated how these behavioural patterns change with the use of a Retrieval Augmented Generation (RAG) pattern [5], and find that in-context learning [6] strongly enforces causal relationships given the retrieved context (**Appendix A.2.4; Figure 6**). \n\nWe have amended the manuscript to more explicitly reference this analysis. \n\n\n\n___\n>The word \u201cmetadata\u201d is somewhat used in a mixed manner between domain knowledge already encoded in LLM and memory passed through JSON format. It should be more formally defined.\n\nWe appreciate this suggestion. We\u2019ve now made a number of amendments to keep the distinction between both concepts clear.\n\n___\n>Results Given that cases with no edges outnumber those with edges, not predicting edges may lead to an increase in accuracy. Thus, a qualitative analysis is necessary since not predicting edges might lead to an increase in the score. Other metrics such as TPR or FDR can be reported.\n\nThank you for this point. Exactly as you point out, the Normalised Hamming Distance (NHD) metric is contingent on the number of edges returned by a causal discovery algorithm. Indeed, predicting no edges at all may have a lower (better) NHD than predicting a number of edges which are incorrect. We therefore follow the approach taken by Kiciman et al. [7] in comparing the ratio of the NHD to a \u2018floor\u2019 baseline (Baseline Hamming Distance [BHD]) which outputs the same number of edges but all of them are incorrect. The NHD/BHD ratio is then the multiple by which the discovery algorithm is better than the worst baseline.  \nIn addition to accounting for the NHD\u2019s contingency on the number of edges predicted by using the NHD/BHD ratio, we believe that we perform sensible qualitative analyses of the results both in the main manuscript and **Appendix A.3.3, A.4.3, A.5.1, and A.5.3**. However, please let us know whether there are any specific types of additional analyses you\u2019d like to see. Finally, in the above mentioned sections, we additionally report the True Positive Rates (TPR), the precision and recall of the algorithms, as well as the F1 score."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8513/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700186966492,
                "cdate": 1700186966492,
                "tmdate": 1700186966492,
                "mdate": 1700186966492,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XbMExd7ZUq",
                "forum": "pAoqRlTBtY",
                "replyto": "UHRG0YtLyO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8513/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8513/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response 3/3"
                    },
                    "comment": {
                        "value": "**References**:  \n*1. (Long, S., Pich\u00e9, A., Zantedeschi, V., Schuster, T., & Drouin, A. (2023). Causal discovery with language models as imperfect experts. arXiv preprint arXiv:2307.02390.  \n2. Ban, T., Chen, L., Wang, X., & Chen, H. (2023). From query tools to causal architects: Harnessing large language models for advanced causal discovery from data. arXiv preprint arXiv:2306.16902.)   \n3. Choi, K., Cundy, C., Srivastava, S., & Ermon, S. (2022). LMPriors: Pre-Trained Language Models as Task-Specific Priors. arXiv preprint arXiv:2210.12530.  \n 4. Berglund, Lukas, et al. \"The Reversal Curse: LLMs trained on\" A is B\" fail to learn\" B is A\".\" arXiv preprint arXiv:2309.12288 (2023).  \n 5. Lewis, Patrick, et al. \"Retrieval-augmented generation for knowledge-intensive nlp tasks.\" Advances in Neural Information Processing Systems 33 (2020): 9459-9474.  \n6. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., ... & Amodei, D. (2020). Language models are few-shot learners. Advances in neural information processing systems, 33, 1877-1901.  \n7. K\u0131c\u0131man, E., Ness, R., Sharma, A., & Tan, C. (2023). Causal reasoning and large language models: Opening a new frontier for causality. arXiv preprint arXiv:2305.00050.**"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8513/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700187050075,
                "cdate": 1700187050075,
                "tmdate": 1700187050075,
                "mdate": 1700187050075,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ij3cZ8HmHN",
                "forum": "pAoqRlTBtY",
                "replyto": "XbMExd7ZUq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8513/Reviewer_CmY7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8513/Reviewer_CmY7"
                ],
                "content": {
                    "title": {
                        "value": "follow-up"
                    },
                    "comment": {
                        "value": "Section 4.1\nDAG 3 is Markov equivalent to DAG 1 (and ground truth) (same skeleton, the same v-structure). Please explain the statistically significant difference in model fitting results. This conflicts the authors\u2019 statements: \u201cThe model that aligns most closely with the true data-generating process produces the highest data likelihood. As expected, we find that this is only valid up to the Markov equivalence class of the ground-truth DAG\u201d. (definition in Page 25 seems correct.)\n\nSection 4.2\nWhy did you choose NOTEARS or DAG-GNN, which are relatively new causal discovery algorithms but it is unclear why classical approaches (PC, MMHC, or GES like algorithms) are not employed. For example, NOTEARS assumes linearity.\nFurther, Arctic-Sea ice dataset\u2019s ground truth is somewhat controversial given that the data does not match ground truth as demonstrated in the paper, which is based on meta-analysis. For LW \u2192 HFLX, what about other algorithms? You mentioned that it is not in the ground truth graph but it does not mean that other algorithms couldn\u2019t figure out. If it is not detectable by LLM nor data, then shouldn\u2019t it be considered hallucination, or sort of, even though the authors can argue with the results in Bates 2012? (BTW, Figure 12 seems that LLM can reason about the edge? ) Hence, this \u2018anecdote\u2019 cannot be used as an evidence that CMA is superior to LLM- or pure data-based approaches.\nIt is also awkward to me to see that fitting a linear regression line to argue about \u201cobservational analysis\u201d and \u201cThe output of the model trained by the CMA is shown in Figure 13, which illustrates that by counterfactual inference, an increase in LW leads to an increased measurement of HFLX.\u201d I am not sure how counterfactual inference (level 3 inference based on Pear\u2019s hierarchy) is done here. Typically this involves unobserved variables. Isn\u2019t this fitting a model and change the value? Shouldn\u2019t it be considered intervention? (level 2 inference)? Also how the change of HFLX represent direct cause from LW? Can\u2019t it be indirect causal relationship?\n\nSection 4.3 :\nI am wondering whether the results properly evaluate the quality of CMA. This seems more relevant to the applicability/utility of Deep SCM itself rather than CMA itself.\nOverall, theoretical justification for the method is a bit lacking and empirical evaluation seems insufficient to understand how memory, LLM, data fitting work in harmony to create the final results.\n\n\n\nAdditional comments on the benchmark results.\n\nAlthough you've mentioned following the approach by Kiciman et al, I'm struggling to find a justification for the discrepancies in comparing the baseline of Kiciman et al. with your framework.\nFirstly, if the intention is to make a direct comparison with Kiciman et al., it would be advisable to cite the results as they are. This would help avoid any perception of arbitrary selection, editing, or cherry-picking. It seems there may be a discrepancy in the reported increase in the BHD of the data-driven causal discovery algorithms compared to what was claimed in your paper. While you utilized Kiciman et al.'s result for the data-driven causal discovery algorithm, the adjustments made to GPT-4 and GPT-3.5 Turbo results were not explicitly addressed in your paper.\nSecondly, there appears to be a challenge in reproducing the results of Kiciman et al. According to their findings, there were 46 edges, and the NHD was 0.22. In contrast, you mentioned no correct edges when selecting 16 edges, indicating a significant difference in results. I suggest attempting to reproduce the NHD score as closely as possible to the original result of Kiciman et al., even if there's a variation in the number of edges.\n\n\n(I am trying to genuinly understand the method better given that other reviewers positively assess the paper. I will properly raise the score if I find out the method is novel and is properly evaluated.)"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8513/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700536598364,
                "cdate": 1700536598364,
                "tmdate": 1700536598364,
                "mdate": 1700536598364,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fBTAXtMIO6",
                "forum": "pAoqRlTBtY",
                "replyto": "UHRG0YtLyO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8513/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8513/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to follow-up 3/3"
                    },
                    "comment": {
                        "value": "Whilst the Deep SCM framework can be used to model tabular and imaging data, it is **not** a causal discovery algorithm. One must 1) know the causal graph a priori and, 2) manually program the graph\u2019s structure as a model. We have therefore generalised, extended, and subsumed this framework into a broader causal discovery algorithm which utilises data-driven and metadata-driven reasoning to produce causal graphs. We could, for example, perform exhaustive fitting of DSCMs, whereby we enumerate all possible DAGs for a given set of variables and fit each one. However, a full ablation of this nature would not be possible for anything but trivial graphs due to the super-exponential search space (Peters el a. [2], Appendix B: \u2018Causal Orderings and Adjacency Matrices\u2019). In our pilot study, we found the concept of \u2018memory\u2019 to be crucial, as there would be no possible alternative to providing a feedback signal to the metadata-based modules, and therefore each iteration would be entirely independent of previous amendments, which obviates the need for any numerical data (however as we demonstrate, signal from data leads to improved performance for causal discovery). We may be able to improve performance even further by using, e.g., alternative prompt tuning approaches to the memory structures, the user prompt, and the system prompts of the LLMs, but consider this important future work.\n\n___\n\n> Additional comments on the benchmark results. [...]\n\nThank you for this point, and well noticed. We re-ran all experiments in this work, but simply had not moved this information from our Appendix to the main manuscript for the data-driven approaches for the arctic sea ice benchmark. This has now been rectified. To replicate the metadata-based benchmarks for gpt-3.5-turbo and gpt-4, we used the same \u2018single prompt\u2019 approach as in the kiciman paper (this is described in kiciman et al. [8] in their Appendix A.1, Table 14). They augment this prompt so that it can be used for \u2018full graph discovery\u2019 (this modification is described in kiciman et al. [8]; Section 3.2.1, page 13). For convenience, we provide the prompt template in full in **Appendix A.3.2, Table 5** of our own work. We believe the slightly different results could be down to a couple of factors. First, it appears that gpt-4\u2019s performance on a multitude of tasks can change over time, as demonstrated by Chen et al. [11], which may partly explain these differences. Second, due to a lack of description as to how they handled errors, we implemented our own error-handling strategy. Namely, the model is meant to produce a single letter in {A,B,C} which reflects one of three options: \n\n\n\n\n- A: Changing $A$ causes a change in $B$\n\n- B: Changing $B$ causes a change in $A$\n\n- C: No causal relationship exists \n\n\n\n\n\nOccasionally the model might output \u2018C/A\u2019. This would be handled as an error in our work, and no edge would be proposed in these cases. A more detailed description of the experimental setup can be found in our **Appendix A.3.2**.\n\n\nWe have updated **Appendix A.3.3** to include the points above in our interpretation of the results for the arctic sea ice dataset. \n___\nThank you for your continued engagement with our work, and we hope to have addressed the majority of your concerns."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8513/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700609862616,
                "cdate": 1700609862616,
                "tmdate": 1700610132528,
                "mdate": 1700610132528,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OnptOuAMkA",
                "forum": "pAoqRlTBtY",
                "replyto": "UHRG0YtLyO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8513/Reviewer_CmY7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8513/Reviewer_CmY7"
                ],
                "content": {
                    "title": {
                        "value": "follow-up"
                    },
                    "comment": {
                        "value": "Okay, I missed the bidirectional edge in the graph. My bad.\nI was thinking 'directed acyclic graph' without an unobserved confounder since the paper keep talks about DAG. Here, chain graph is the type of graph (more like syntax) and the semantics for the bidirected edge is not clear. Do you mean it to be an unobserved confounder?\n\nGiven that all three are connected (both ground truth and DAG3), both graph encodes no conditional independence, and, hence, the same power to represent P(Age, AV45, P-tau). What am I missing here?\n\nThanks for the additional experiments and detailed explanations on my previous questions and comments. I greatly appreciate devoting your time on running additional experiments that could certainly support your claims in the paper.\n\n(I will try to discuss with other reviewers after the author/reviewer discussion period, then reflect all those comments from the authors and other reviewers in my official review and rating.)"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8513/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700611040494,
                "cdate": 1700611040494,
                "tmdate": 1700611715680,
                "mdate": 1700611715680,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "g7aZUo2QeG",
            "forum": "pAoqRlTBtY",
            "replyto": "pAoqRlTBtY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8513/Reviewer_GSYx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8513/Reviewer_GSYx"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to integrate large language models into causal discovery algorithms for multi-modal data and shows superiority of this model is shown in a number of examples."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The model architecture is convincing and the extensive numerical experiments show strong promise of the proposed method."
                },
                "weaknesses": {
                    "value": "The generalization performance/robustness of the proposed method is not completely clear. One challenge in causal discovery is the sensitiveness of the learned causal graph towards perturbation of the distributions, in the presence of weak causal link.\n\nPost-rebuttal: I thank the authors for their response and additional experiments for the case of weak causal link. I am increasing my score to 8."
                },
                "questions": {
                    "value": "It could be more convincing to analyze the sensitivity of the proposed model in accordance to perturbation of the input parameters, in particular in the presence of weak causal link."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8513/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8513/Reviewer_GSYx",
                        "ICLR.cc/2024/Conference/Submission8513/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8513/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698839850295,
            "cdate": 1698839850295,
            "tmdate": 1700482538718,
            "mdate": 1700482538718,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "M5IiMgLSdn",
                "forum": "pAoqRlTBtY",
                "replyto": "g7aZUo2QeG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8513/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8513/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their insights and are glad they found the model architecture convincing, and our experiments extensive. To address their central question, we have updated the paper to add additional experimental results (**Appendix A.5.4**) which probed the effect of perturbing the synthetic AD data on the output of the numerical causal discovery algorithms.\n\nWe provide a high-level summary of these results below.\n\n___\n \n| Algorithm | L1 | L2 | L3 |\n|-----------|--------------|------------|-----------------|\n| DAG-GNN       | 0.65        | 0.61      | 0.71           |\n| NOTEARS   | 0.71        | 0.83    | 0.83           |\n| CMA | **0.28**        | **0.30**    | **0.36**            |\n\n *There are three noise levels whereby Gaussian noise is added with mean 0  \nand standard deviations of 0.4 for L1, 0.8 for L2, and 1.2 for L3.* \n\n[As can be seen in this anonymised figure,](https://i.imgur.com/G5Pej61.png) we found that with increasingly weaker causal relationships, both NOTEARS and DAG-GNN struggle to learn the causal graph, though the DAG-GNN algorithm maintains a more consistent performance throughout. The CMA outperforms both algorithms, and we believe this is partly due to the metadata-based modules of the framework, which can still propose reasonable causal structures."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8513/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700186611751,
                "cdate": 1700186611751,
                "tmdate": 1700186611751,
                "mdate": 1700186611751,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "D9HRP9qJdW",
            "forum": "pAoqRlTBtY",
            "replyto": "pAoqRlTBtY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8513/Reviewer_Jpz4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8513/Reviewer_Jpz4"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a novel framework that synergizes the metadata-based reasoning capabilities of LLMs with the data-driven modeling of Deep Structural Causal Models for causal discovery. The authors evaluated the performance on benchmarks and real-world tasks. Real-world tasks were related to modeling the clinical and radiological phenotype of Alzheimer\u2019s Disease. The experimental results indicate that the CMA can outperform previous approaches to causal discovery and derive new insights regarding causal relationships."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- the paper proposes an original approach to causal modeling\n - the paper has a good quality: benchmark and real-world tasks are considered, showing promising results in both cases\n - the paper is well structured and written, making it easy to follow\n - the topic is a relevant topic on which much research is being invested, given the new capabilities and opportunities LLMs provide to causal modeling"
                },
                "weaknesses": {
                    "value": "- we have not found strong weaknesses in the paper"
                },
                "questions": {
                    "value": "1. While the authors do a good job regarding the related work, we consider this could be further enhanced by citing surveys that provide an overview of the relevant topics and domains. E.g., the authors may be interested on the following works: (a) for causal deep modelling: Li, Zongyu, and Zhenfeng Zhu. \"A survey of deep causal model.\" arXiv preprint arXiv:2209.08860 (2022); (b) for Alzheimer disease neuroimaging: Varghese, Tinu, et al. \"A review of neuroimaging biomarkers of Alzheimer\u2019s disease.\" Neurology Asia 18.3 (2013): 239. and M\u00e1rquez, Freddie, and Michael A. Yassa. \"Neuroimaging biomarkers for Alzheimer\u2019s disease.\" Molecular neurodegeneration 14 (2019): 1-14; and (c) Huang, Yiyi, et al. \"Benchmarking of data-driven causality discovery approaches in the interactions of arctic sea ice and atmosphere.\" Frontiers in big Data 4 (2021): 642182, and Kretschmer, Marlene, et al. \"Using causal effect networks to analyze different Arctic drivers of midlatitude winter circulation.\" Journal of climate 29.11 (2016): 4069-4081.\n2. In the related work section, the authors may consider weighting the views and findings regarding LLMs and causality expressed in the following paper: Ze\u010devi\u0107, Matej, et al. \"Causal parrots: Large language models may talk causality but are not causal.\" arXiv preprint arXiv:2308.13067 (2023).\n3. When reporting results in Section 4.1, the authors measure average data likelihood and the deviation. It would be helpful to have some reference value to understand whether the reported values are good or not and why.\n4. How is the threshold for DAG-GNN selected?\n5. Table 1: align results to the right so that differences in magnitude are quickly visualized.\n6. Table 2: add up/down arrows near the reported metrics, indicating greater/lower is better.\n7. Table 2: for some algorithms (TCDF, NOTEARS (Temporal), NOTEARS (Temporal)), the authors report results only for the Arctic sea ice dataset, but no clarification is provided as to why no results are reported for the Alzheimer\u2019s disease and Sangiovese datasets."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8513/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8513/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8513/Reviewer_Jpz4"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8513/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698869256808,
            "cdate": 1698869256808,
            "tmdate": 1699637063528,
            "mdate": 1699637063528,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zBVQByr6Wo",
                "forum": "pAoqRlTBtY",
                "replyto": "D9HRP9qJdW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8513/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8513/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response 1/2"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their questions and insights. We are glad they found our paper original, of good quality, well-structured, and relevant. To address their questions, we have made a number of amendments to the manuscript, including extending the \u2018Related works\u2019 section and incorporating several clarifications throughout.\n\n\n___\n> While the authors do a good job regarding the related work, we consider this could be further enhanced by citing surveys that provide an overview of the relevant topics and domains. E.g., the authors may be interested on the following works: [..]\n\nThank you for your kind words. We have included a reference to the causal deep modelling survey at the beginning of our discussion of LLMs and causality in the related works section.  \nThe Alzheimer\u2019s disease (AD) neuroimaging references provide a nice exposition of the 18F-AV-45 tracer in particular, which is a variable we consider in our AD models. We therefore additionally include these references in the \u2018Experiments\u2019 section.  \nFinally, Huang et al.\u2019s work on data-driven causal discovery for the arctic sea ice dataset is referenced in **Section 4.2: \u2018Benchmarking Experiments\u2019.**\n\n___\n>  In the related work section, the authors may consider weighting the views and findings regarding LLMs and causality expressed in the following paper: Ze\u010devi\u0107, Matej, et al. \"Causal parrots: Large language models may talk causality but are not causal.\" arXiv preprint arXiv:2308.13067 (2023).\n\nThank you for pointing us to this contemporary work. We have added a discussion of this paper in a new section on the use of LLMs for causal reasoning. This can be found in the \u2018Related work\u2019 section of the main manuscript.  \n\n___\n> When reporting results in Section 4.1, the authors measure average data likelihood and the deviation. It would be helpful to have some reference value to understand whether the reported values are good or not and why.\n\nIn **Section 4.1**, the most meaningful comparison lies in the relative difference in data likelihoods under each model. We have amended the manuscript to highlight that the model that matches the data-generating process acts as the reference likelihood.  \nWe then demonstrate that making incorrect modelling assumptions about the data-generating process leads to a lower data likelihood. Finally, the difference in data likelihoods is compared with a pairwise Tukey HSD test. Assessing relative model performance in this way has been employed by other related works[1].  \n \n___\n> How is the threshold for DAG-GNN selected?\n\nWe assessed two thresholds for the DAG-GNN algorithm and chose the threshold with the best performance for inclusion in the main manuscript. We began with a threshold of 0.3 as per the DAG-GNN paper by Yu et al.[2], who themselves selected this parameter based on prior work by Zheng et al.[3]. We also assessed a threshold of 0.1, which had worse performance on our benchmarks. Results of the DAG-GNN algorithm with different thresholds can be found in **Tables 6, 9, and 12** of **Appendix Sections A.3.3, A.4.3, and A.5.3**, respectively. We had also conducted a similar hyperparameter tuning experiment for the NOTEARS algorithm and reported those results in the same tables.  \n \n___      \n> 1.  Table 1: align results to the right so that differences in magnitude are quickly visualized.\n> 2.  Table 2: add up/down arrows near the reported metrics, indicating greater/lower is better.\n\nWe very much appreciate the formatting/readability suggestions. We\u2019ve amended the tables and re-ordered some of the columns in **Table 1** to improve clarity.\n\n> Table 2: for some algorithms (TCDF, NOTEARS (Temporal), NOTEARS (Temporal)), the authors report results only for the Arctic sea ice dataset, but no clarification is provided as to why no results are reported for the Alzheimer\u2019s disease and Sangiovese datasets.\n\nAs we state in **Appendix A.3.2**, the arctic sea ice dataset includes optional temporal data, which the TCDF algorithm expects by default. The Alzheimer\u2019s disease and Sangiovese benchmarks are cross-sectional datasets, which precludes the use of the TCDF algorithm on them. The \u2018temporal\u2019 versions of the NOTEARS and DAG-GNN algorithms require that we pre-process the time-series data as per Huang et al.[4] and Kiciman et al.[5]; a detailed description can also be found in Appendix A.3.2.\n\n  \nFor clarity, we have now added an explicit reference to Appendix A.3.2 in the main manuscript under **Section 4.2**."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8513/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700186459920,
                "cdate": 1700186459920,
                "tmdate": 1700186459920,
                "mdate": 1700186459920,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yZb0SxPzLG",
                "forum": "pAoqRlTBtY",
                "replyto": "D9HRP9qJdW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8513/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8513/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response 2/2"
                    },
                    "comment": {
                        "value": "**References**:  \n\n*1. Pawlowski, N., Coelho de Castro, D., & Glocker, B. (2020). Deep structural causal models for tractable counterfactual inference. Advances in Neural Information Processing Systems, 33, 857-869.     \n2. Yu, Y., Chen, J., Gao, T., & Yu, M. (2019, May). DAG-GNN: DAG structure learning with graph neural networks. In International Conference on Machine Learning (pp. 7154-7163). PMLR.   \n3. Zheng, X., Aragam, B., Ravikumar, P. K., & Xing, E. P. (2018). Dags with no tears: Continuous optimization for structure learning. Advances in neural information processing systems, 31.  \n4. Huang, Y., Kleindessner, M., Munishkin, A., Varshney, D., Guo, P., & Wang, J. (2021). Benchmarking of data-driven causality discovery approaches in the interactions of arctic sea ice and atmosphere. Frontiers in big Data, 4, 642182.   \n5. K\u0131c\u0131man, E., Ness, R., Sharma, A., & Tan, C. (2023). Causal reasoning and large language models: Opening a new frontier for causality. arXiv preprint arXiv:2305.00050.*"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8513/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700186551123,
                "cdate": 1700186551123,
                "tmdate": 1700186551123,
                "mdate": 1700186551123,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mykZ3GacTW",
                "forum": "pAoqRlTBtY",
                "replyto": "zBVQByr6Wo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8513/Reviewer_Jpz4"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8513/Reviewer_Jpz4"
                ],
                "content": {
                    "comment": {
                        "value": "We authors have tackled our comments. We have reviewed the comments from other reviewers and have no further observations."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8513/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700727697850,
                "cdate": 1700727697850,
                "tmdate": 1700727697850,
                "mdate": 1700727697850,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]