[
    {
        "title": "Gradient-free Proxy for Efficient Language Model Search"
    },
    {
        "review": {
            "id": "b4zEnHMCIQ",
            "forum": "Vc52HZwNwe",
            "replyto": "Vc52HZwNwe",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3977/Reviewer_7PxN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3977/Reviewer_7PxN"
            ],
            "content": {
                "summary": {
                    "value": "This work introduces a novel zero-shot NAS method, called as weight-weighted PCA (W-PCA), designed to efficiently explore lightweight language models within a teacher-student network for knowledge distillation. The method harnesses the parameter counts and principal component analysis (PCA) values of the feed-forward neural (FFN) layer, instead of relying on gradient metrics, to offer a comprehensive and impartial evaluation. In experimental trials, this approach demonstrates the ability to significantly reduce training time when compared to previous one-shot NAS methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This work presents a zero-shot NAS method that:\n1. Identifies significant dimensions that contribute to the performance.\n2. Reduces the need for extensive backpropagation and derivative calculations and requires only forward propagation during the evaluation of candidate architectures."
                },
                "weaknesses": {
                    "value": "1. The novelty may be limited, as the proposed method has not provided a comprehensive and robust architectural design guideline or led to the discovery of an efficient and universally applicable architecture.\n2. Tables 2 and 3 demonstrate only modest performance improvements with W-PCA in comparison to baseline methods, with exceptions on a few datasets. This might raise questions about the overall effectiveness of the proposed method."
                },
                "questions": {
                    "value": "1. \u201cBy considering the PCA values, we can identify dimensions that contribute the most to the architecture\u2019s performance, allowing for informed decision-making during architecture search.\u201d How will the dimensions benefit the architecture search?\n2. Are there any particularities of the architectures searched by your method?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3977/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3977/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3977/Reviewer_7PxN"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3977/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698455967348,
            "cdate": 1698455967348,
            "tmdate": 1699636359047,
            "mdate": 1699636359047,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hsyeHE2mtL",
                "forum": "Vc52HZwNwe",
                "replyto": "b4zEnHMCIQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3977/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3977/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your valuable feedback and constructive comments have been instrumental in shaping our research.\n\n> The novelty may be limited, as the proposed method has not provided a comprehensive and robust architectural design guideline or led to the discovery of an efficient and universally applicable architecture.\n\nWe conducted additional experiments by applying the zero-shot proxy mentioned in the related work to our search space and discussed the impact of the search space on the experimental results. The experimental results have been added to Tables 2 and 3. Additionally, relevant discussions have been included in Section 6.3.\n\n> Tables 2 and 3 demonstrate only modest performance improvements with W-PCA in comparison to baseline methods, with exceptions on a few datasets. This might raise questions about the overall effectiveness of the proposed method.\n\nAs mentioned above, we have supplemented the relevant experimental content in Tables 2 and 3.\n\n> \u201cBy considering the PCA values, we can identify dimensions that contribute the most to the architecture\u2019s performance, allowing for informed decision-making during architecture search.\u201d How will the dimensions benefit the architecture search?\n\nIn general, the larger the dimension of PCA, the more effective information is contained in this block. In this regard, we are more or less inspired by the paper [1].\n\n> Are there any particularities of the architectures searched by your method?\n\nWe have supplemented the discussion on model visualization in Appendix D.\n\n[1] Pan et al. Budgeted Training for Vision Transformer, ICLR 2023."
                    },
                    "title": {
                        "value": "Response to Reviewer 7PxN"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3977/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700739186863,
                "cdate": 1700739186863,
                "tmdate": 1700739214615,
                "mdate": 1700739214615,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "stlSluLrY2",
            "forum": "Vc52HZwNwe",
            "replyto": "Vc52HZwNwe",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3977/Reviewer_EqU6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3977/Reviewer_EqU6"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors aim to address these limitations and improve the applicability of zero-shot NAS. As prevailing approaches to NAS often confront issues such as biased evaluation metrics and computational inefficiencies, the authors proposed w-PCA (given an observed strong correlation in ranking between PCA and the # of parameters (#params), with their product demonstrating even better performance) to better and more inclusively consider both the model parameters count and PCA values. Combining these two aspects, w-PCA shows significantly smaller training time compared to 1-shot NAS, while achieving higher scores in the testing phase compared to previous SOTA training-based methods."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed method (w-PCA) achieved significantly smaller training time compared to 1-shot NAS\n2. w-PCA achieving higher scores in the testing phase compared to previous SOTA training-based methods on GLUE and SQuAD datasets\n3. Concluding experiments on two widely used NLU datasets & detailed analyses of the results are provided\n4. First work that applies 0-shot NAS to NLU tasks\n5. Details of the implementation (in the main text and appendix A), making comprehension easier, and reproducibility as well"
                },
                "weaknesses": {
                    "value": "1. The method struggled in performance on the GLUE & SQuAD datasets (e.g. BERT-base* performed the best, even if it is by a 2-point margin). Do you have any idea why?\n\n2. Does the work scale? It would have been nice to discuss the limitations of this approach. For instance, I see this approach to be useful in resource constraint settings e.g. low-resource scenarios but the datasets used cover very few to none of such languages. It would be great to have it included to see to which extent this could benefit extremely constrained environments\n\n3. What should be the trade-off between training time, the weight of the obtained model, and the performance? i.e. how much decrease (worse case) in performance could we \"sacrifice\" for how many # of parameters and how much time should/could?\n\n4. Not related to the PCA necessarily but to the idea of reduction - have you tried or explored working in latent space?"
                },
                "questions": {
                    "value": "See Weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3977/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698799689150,
            "cdate": 1698799689150,
            "tmdate": 1699636358964,
            "mdate": 1699636358964,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lOzBcVMl28",
                "forum": "Vc52HZwNwe",
                "replyto": "stlSluLrY2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3977/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3977/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer EqU6"
                    },
                    "comment": {
                        "value": "Thank you for your valuable feedback and constructive comments have been instrumental in shaping our research.\n\n> The method struggled in performance on the GLUE & SQuAD datasets (e.g. BERT-base* performed the best, even if it is by a 2-point margin). Do you have any idea why?\n\nIn fact, BERT-base is the teacher model, while our student model is based on knowledge distillation training, so the results generally do not surpass the teacher model.\n\n> Does the work scale? It would have been nice to discuss the limitations of this approach. For instance, I see this approach to be useful in resource constraint settings e.g. low-resource scenarios but the datasets used cover very few to none of such languages. It would be great to have it included to see to which extent this could benefit extremely constrained environments\n\nWe have applied the previously used zero-shot proxy, which was originally applied to the Transformer block, in the search space we designed. The results have been added to Table 2 and Table 3, and relevant discussions have been included in Section 6.2. We have also added a discussion on parameter initialization in Appendix B.\n\n> What should be the trade-off between training time, the weight of the obtained model, and the performance? i.e. how much decrease (worse case) in performance could we \"sacrifice\" for how many # of parameters and how much time should/could?\n\nWe compared our method with one-shot NAS, and the results are shown in Appendix E.2. In fact, spending more search time does not necessarily lead to significantly better results. Additionally, more constraints can be set as search conditions for future work.\n\n> Not related to the PCA necessarily but to the idea of reduction - have you tried or explored working in latent space?\n\nIn fact, the discussion on KD loss in our current Appendix C should involve relevant categories, as the output of each layer can also be understood as a representation of a latent space."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3977/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700739826977,
                "cdate": 1700739826977,
                "tmdate": 1700739826977,
                "mdate": 1700739826977,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "hnAq8WGsOU",
            "forum": "Vc52HZwNwe",
            "replyto": "Vc52HZwNwe",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3977/Reviewer_pvZH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3977/Reviewer_pvZH"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a zero-shot neural architecture search method for selecting lightweight language models. The method involves leveraging two evaluation proxies - parameter dimension and eigen/spectral values. This methodology enables faster throughput of evaluating and selecting lightweight language models via gradient free computations. Evaluation on the GLUE benchmark shows higher scores as compared to previous state-of-the-art methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Zero-shot NAS enables faster searching of the models 0.5d as compared to > 50d using other approaches.\n    \n2. Competitive results against multiple baselines."
                },
                "weaknesses": {
                    "value": "1. The paper is convoluted and difficult to understand. Multiple sections need to be written again. Kindly see Questions/Comments.\n    \n2. Table 5 results are close to each other. Thereby showing that the weight-weighted version might not improve the results too much as compared to Vanilla. A deeper qualitative analysis is required to establish the usage which is missing in the current version of this work."
                },
                "questions": {
                    "value": "1. The first two lines of the abstract are not well connected. Kindly reframe the abstract to highlight the motivation and contribution better\n    \n2. Abstract: Unbiased assessment of what?\n    \n3. Section 1. 3rd last paragraph - \u201cpotentiallly overlooking important characteristics\u201d. What are these important characteristics?\n    \n4. Section 2.2 mentions primitive operators - What are these?\n    \n5. Section 2.3: Include year with the citations.\n    \n6. Section 2.3: Inconsistent notations throughout this section - N is parameters, as well as batch size in this section. This section needs to be written better to make the reader understand the motivation for using each of the metrics, instead of just writing the formulation.\n    \n7. Section 3: Kindly mention how PCA_dim is calculated using equations, and how the threshold serves as the lower/upper limit for deciding the principal components. Explicitly call out hidden_dim = dimension of the hidden layer after before applying FFN.\n    \n8. Why is the scaling of hidden_dim needed ?\n    \n9. Equation 4: Notation writeup should be improved currently it looks like PCA(X) subtracted from W.\n    \n10. Section 5.1: Kindly include the details of the appendix section of Serianni & Kalita. Each paper should be a standalone read.\n    \n11. Section 6.2.1: Details of the genetic algorithm being used is missing here. Kindly include it here or the appendix.\n    \n12. Section 6.2.3: KD Loss - $\\mathcal{L}^i_{attn} = \\text{MSE}(\\mathbb{A}^{S}*i\\mathbb{W}a, \\mathbb{A}^T*j).\n    \n13. Section 6.2.3: KD Loss - \u201cjth teacher model layer corresponds to ith student model layer\u201d - How do we get this correspondence?\n    \n14. Conclusion: Kindly mention some quantitative metrics here. In the current version it looks like a paraphrase of the abstract."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3977/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698834788351,
            "cdate": 1698834788351,
            "tmdate": 1699636358850,
            "mdate": 1699636358850,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NwSJMfg20S",
                "forum": "Vc52HZwNwe",
                "replyto": "hnAq8WGsOU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3977/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3977/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer pvZH (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your valuable feedback and constructive comments have been instrumental in shaping our research.\n\n> The first two lines of the abstract are not well connected. Kindly reframe the abstract to highlight the motivation and contribution better\n\nThank you very much for your correction.  We have reframed the abstract to better highlight the motivation and contribution of our work.\n\n>  Abstract: Unbiased assessment of what?\n\nThe term \"unbiased assessment\" refers to the comprehensive evaluation of the language model's performance using the parameter count and PCA value. We have clarified this in the modified abstract.\n\n> Section 1. 3rd last paragraph - \u201cpotentiallly overlooking important characteristics\u201d. What are these important characteristics?\n\nThese characteristics include some features of neural networks extracted in previous zero-shot NAS, such as gradients, parameter sizes, and parameter quantities discussed in Section 2.3.\n\n> Section 2.2 mentions primitive operators - What are these?\n\nThese include unary (such as negation, softmax) and binary (such as addition, multiplication) mathematical operations, which we have explained in the original text.\n\n> Section 2.3: Include year with the citations.\n\nThank you for the correction. We have made the necessary amendments to the relevant citations.\n\n> Section 2.3: Inconsistent notations throughout this section - N is parameters, as well as batch size in this section. This section needs to be written better to make the reader understand the motivation for using each of the metrics, instead of just writing the formulation\n\nThank you for your correction. We have replaced \"n\" with \"N\" as the parameter and added a description of the relevant motivation in the corresponding zero-shot proxy.\n\n> Section 3: Kindly mention how PCA_dim is calculated using equations, and how the threshold serves as the lower/upper limit for deciding the principal components. Explicitly call out hidden_dim = dimension of the hidden layer after before applying FFN.\n\nPCA_dim refers to the dimensions with PCA values exceeding a threshold $\\eta$. We have added relevant explanations below Equation (1). In Section 6.2.1, we mentioned that the value of \n$\\eta$ is set to 0.99 in the experiments.\n\n> Why is the scaling of hidden_dim needed ?\n\nThank you for helping us reconsider the role of hidden_dim. After repeated experiments, we found that removing this scaling would lead to better correlation in the dataset rankings. We have updated the relevant experimental results.\n\n> Equation 4: Notation writeup should be improved currently it looks like PCA(X) subtracted from W.\n\nWe have updated the expression of the relevant formula, and now the \"-\" in the formula no longer looks like a minus sign.\n\n> Section 5.1: Kindly include the details of the appendix section of Serianni & Kalita. Each paper should be a standalone read.\n\nThank you for your correction. We have added the relevant content to Appendix A.\n\n> Section 6.2.1: Details of the genetic algorithm being used is missing here. Kindly include it here or the appendix.\n\nWe employ a genetic algorithm with a population size of 50 and a generation count of 40 to identify the combination of blocks that yields the highest PCA value. The crossover probability is set to 1, the mutation probability to 0.1, and the upper limit for the model parameters to 15.7M in order to obtain the W-PCA-Small model. By further reducing the upper limit for the model parameters to 10M and halving the number of layers ($m$), we obtain the W-PCA-Tiny model.\n\nThis part of the content has been written in Section 6.2.1.\n\n> Section 6.2.3: KD Loss - $\\mathcal{L}^i_{attn} = \\text{MSE}(\\mathbb{A}^{S}i\\mathbb{W}a, \\mathbb{A}^Tj).\n\nThank you for your correction. We have made the necessary revisions. We have moved all the sections related to KD loss to Appendix C.\n\n> Section 6.2.3: KD Loss - \u201cjth teacher model layer corresponds to ith student model layer\u201d - How do we get this correspondence?\n\nFor our fixed teacher model, BERT-base, which comprises 12 layers, a one-to-one sequential correspondence exists between the layers of the student and teacher models when both models have 12 layers. However, in the case of a student model with only 6 layers, the correspondence remains one-to-one, but with a 2-layer interval. This implies that the first layer of the student model corresponds to the second layer of the teacher model, and so forth, until the sixth layer of the student model aligns with the twelfth layer of the teacher model.\n\nWe have incorporated this part of the content into the explanation of the formula.\n\n> Conclusion: Kindly mention some quantitative metrics here. In the current version it looks like a paraphrase of the abstract.\n\nWe have revised the Conclusion section according to your suggestions."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3977/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738172886,
                "cdate": 1700738172886,
                "tmdate": 1700738172886,
                "mdate": 1700738172886,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NZWaRQ5D8w",
            "forum": "Vc52HZwNwe",
            "replyto": "Vc52HZwNwe",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3977/Reviewer_8KoX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3977/Reviewer_8KoX"
            ],
            "content": {
                "summary": {
                    "value": "As the model size of large language models continues to increase, the development of lightweight language models becomes increasingly significant. While neural architecture search (NAS) is commonly used for this purpose, it often encounters biased metrics and inefficiencies. This paper introduces two evaluation proxies, specifically parameter count and principal component analysis (PCA) value, which eliminate the need for gradients and enhance efficiency. Experiments conducted on GLUE and SQuAD demonstrate the effectiveness of this approach."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed algorithm significantly enhances the search efficiency of NAS.\n2. The models discovered through this method outperform other baseline methods in GLUE and SQuAD benchmarks.\n3. The visualization of the correlation between principal component analysis (PCA) and the number of parameters (#params) rankings helps interpret the effectiveness of the paper."
                },
                "weaknesses": {
                    "value": "1. The paper primarily focuses on experiments with the BERT model, and it would be beneficial to conduct more experiments on other types of language models, such as generative models or larger-sized models, to ascertain the method's applicability.\n2. A more in-depth discussion of the proposed method is needed. It would be beneficial to conduct ablation studies to assess the impact of each loss term, considering that multiple loss terms are included in the fine-tuning process.\n3. The differences in training datasets and objectives compared to previous works make it unclear whether the improvement stems from the searched architecture or other factors.\n4. The improvement observed in GLUE and SQuAD is described as marginal.\n5. This article requires significant improvement in writing and formatting. The misplaced tables contribute to a lack of clarity in the paper's presentation, such as in section 6.3."
                },
                "questions": {
                    "value": "1. Could you provide information regarding the zero-shot performance of the searched model? Additionally, I'm interested in learning about the performance of architectures discovered using W-PCA with different parameters.\n2. Have you conducted experiments with multiple shots and employed an iterative searching strategy for multiple shots searching?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3977/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698892629301,
            "cdate": 1698892629301,
            "tmdate": 1699636358751,
            "mdate": 1699636358751,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FFYvNo9i9C",
                "forum": "Vc52HZwNwe",
                "replyto": "NZWaRQ5D8w",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3977/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3977/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 8KoX (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your valuable feedback and constructive comments have been instrumental in shaping our research.\n\n>The paper primarily focuses on experiments with the BERT model, and it would be beneficial to conduct more experiments on other types of language models, such as generative models or larger-sized models, to ascertain the method's applicability.\n\nWe conducted experiments using a larger-sized model by increasing the size of the search space, as described in Section 6.2.1. Specifically, we doubled the hidden\\_size and the hidden\\_dimension of $n$ candidate dimensions. Additionally, we raised the parameter limit in the genetic algorithm to 67M, resulting in our W-PCA-Large model. As shown in the Table, despite having a slightly lower parameter count, our model outperforms TinyBERT-6 [1] and EfficientBERT [2] models of similar scale in terms of average GLUE score. This indicates that our proposed zero-shot proxy also demonstrates good adaptability in larger search spaces.\n\n|  Model   | #Params  |  QNLI   | MRPC  |  SST-2   | CoLA |  STS-B   | MNLI-m/mm  | RTE  |  QQP   | AVG  |\n|  ----  | ----  |  ----  | ----  |  ----  | ----  |  ----  | ----  | ----  |  ----  | ----  |\n| TinyBERT-6 [1]  |  67.0M  | 89.8 | 89.0 | 92.0 | 38.8 | 83.1 | 83.8/83.2 | 65.8 | 71.4 |77.4 |\n| EfficientBERT [2]  | 70.1M  | 90.4 | 89.0 | 92.6 | 46.2 | 83.7 | 84.1/83.2 | 67.7 | 74.4 | 78.7 |\n| W-PCA-Large  | 66.9M  | 90.9 | 88.7 | 93.0 | 40.0 | 87.5 | 84.6/83.3 | 75.6 | 71.5 | 79.5 |\n\nWe have included this part of the content in Appendix E.1 of the paper (Page 13).\n\n> A more in-depth discussion of the proposed method is needed. It would be beneficial to conduct ablation studies to assess the impact of each loss term, considering that multiple loss terms are included in the fine-tuning process.\n\nWe utilized the components of W-PCA, as described in Equation (4) where the first component is the number of parameters (\\#Params) and the second component is the V-PCA value (defined in Equation (2)), as fitness values for the genetic algorithm to explore the optimal network structure in Section 6.2.1. We then compared the performance of the discovered network structures with W-PCA.\n\nThe results, presented in Table 5, demonstrate that by multiplying the number of parameters with the V-PCA value and using W-PCA as the zero-shot evaluation metric, the performance of the searched networks significantly improves compared to using either \\#Params or V-PCA alone as the evaluation metric.\n\nWe have added this explanation to Section 6.5 of the original text. \n\n> The differences in training datasets and objectives compared to previous works make it unclear whether the improvement stems from the searched architecture or other factors.\n\nWe have applied the zero-shot proxy from related work to the search space we designed (Figure 3), and the results from GLUE test set are as follows.\n\n|  Model   |   Type | #Params   |  Time  |  QNLI   | MRPC  |  SST-2   | CoLA |  STS-B   | MNLI-m/mm  | RTE  |  QQP   | AVG  |\n|  ----  | ----  | ----  | ----  | ----  | ----  |  ----  | ----  |  ----  | ----  | ----  |  ----  | ----  |\n| Synaptic Saliency [3] |zero-shot |15.7M | 58ms | 89.4 | 88.1 | 91.0| 33.6 | 83.1 | 82.6/81.1 | 70.6|  70.3 | 76.6 |\n| Activation Distance [4] | zero-shot |15.6M | 60ms | 88.9 | 87.6 | 91.2 | 30.7 | 82.9 | 81.1/80.4 | 70.4| 70.1 |75.9 |\n| Synaptic Diversity [5] | zero-shot |15.6M | 57ms | 88.3 | 88.1 | 91.5 | 25.8 | 84.7 | 81.3/80.2 | 70.6| 70.3 |75.6  |\n| Head Confidence [6] |  zero-shot |15.6M | 63ms | 89.5 | 88.3 | 92.4 | 31.7 | 85.7 | 82.8/81.9 | 74.0 | 70.9 |77.5  |\n| Softmax Confidence[6] |  zero-shot |15.6M | 61ms | 88.4 | 87.5 | 90.8 | 32.5 | 83.5 | 81.2/80.5 | 70.3| 69.9 |76.1  |\n\nThe results from GLUE dev set are as follows.\n\n|  Model   |   #Params  |  Time  |  QNLI   | MRPC  |  SST-2   | CoLA |  STS-B   | MNLI-m/mm  | RTE  |  QQP   | AVG  |\n|  ----  | ----  | ----  | ----  | ----  |  ----  | ----  |  ----  | ----  | ----  |  ----  | ----  |\n| Synaptic Diversity [5] | 15.6M | 0.7 d | 88.9 | 87.6 | 91.4 | 32.0 | 84.1 | 81.0 | 73.4| 88.2 |78.3  |\n| Head Confidence [6] |  15.6M | 0.5 d | 90.1 | 89.7 | 92.4 | 37.5 | 84.1 | 82.5 | 75.9| 89.1 |80.2  |\n| Softmax Confidence[6] |  15.6M | 0.5 d | 89.4 | 88.3 | 92.0 | 32.6 | 84.7 | 81.6 | 73.9| 88.9 |78.9  |\n\nWe have added the comparative results of this part in the same search space to Table 2 and Table 3, and discussed the relevant content in the main text.\n\n>  The improvement observed in GLUE and SQuAD is described as marginal.\n\nWe have expanded the relevant discussions in Sections 6.3 and 6.4. \n\n> This article requires significant improvement in writing and formatting. The misplaced tables contribute to a lack of clarity in the paper's presentation, such as in section 6.3.\n\nWe have reorganized the relevant tables and improved multiple expressions in the original text. You can also refer to the revisions made based on the guidance of Reviewer pvZH."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3977/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700737808776,
                "cdate": 1700737808776,
                "tmdate": 1700738293499,
                "mdate": 1700738293499,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bCut3wABgE",
                "forum": "Vc52HZwNwe",
                "replyto": "NZWaRQ5D8w",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3977/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3977/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 8KoX (2/2)"
                    },
                    "comment": {
                        "value": "> Could you provide information regarding the zero-shot performance of the searched model? Additionally, I'm interested in learning about the performance of architectures discovered using W-PCA with different parameters.\n\nWe conducted supplementary experiments on different initialization parameters, which can be seen in the newly added Appendix B. Additionally, the discussion on model visualization has been included in Appendix D.\n\n> Have you conducted experiments with multiple shots and employed an iterative searching strategy for multiple shots searching?\n\nThere is limited research on n-shots (n > 1) NAS in NLP tasks. The only one we are aware of is [7], as it requires significant computational resources. We have conducted supplementary experiments for the n = 1 case, and the results are as follows:\n\n|  Model   |   Type | #Params   |  Time  |  QNLI   | MRPC  |  SST-2   | CoLA |  STS-B   | MNLI-m/mm  | RTE  |  QQP   | AVG  |\n|  ----  | ----  | ----  | ----  | ----  | ----  |  ----  | ----  |  ----  | ----  | ----  |  ----  | ----  |\n| W-PCA-Tiny |zero-shot | 9.6M | 0.4 d | 88.7 | 87.6 | 91.9 | 27.4 | 84.8 | 81.1/79.8 | 71.1| 70.3 |75.9 |\n|  | one-shot |9.7M | 24 d | 89.2 | 87.5 | 92.3 | 28.9 | 83.7 | 81.4/80.5 | 71.4 | 70.5 |76.2 | \n| W-PCA-Small | zero-shot |15.6M | 0.5 d | 90.3 | 88.7 | 91.5 | 38.4 | 86.4 | 82.8/82.2 | 73.8 | 70.8 |78.3  |\n| |  one-shot |15.6M | 28 d | 90.3 | 88.9 | 92.5 | 36.1 | 86.7 | 83.7/82.5 | 74.4 | 70.6 |78.4 |\n\nDespite investing a significant number of GPU days in the one-shot NAS search, the performance improvement on various-sized models of W-PCA is not significant. Zero-shot NAS remains the most cost-effective search solution.\n\nThe relevant content has been included in Appendix E.2 \uff08Page 14\uff09\n\n[1] Jiao et al.Tinybert: Distilling bert for natural language understanding.  EMNLP 2020.\n\n[2] Dong et al. Efficientbert: Progressively searching multilayer perceptron via warm-up knowledge distillation.  EMNLP 2021.\n\n[3] Abdelfattah et al. Zero-costproxies for lightweight nas. ICLR 2020.\n\n[4] Mellor et al. Neural architecture search withouttraining. ICML 2021.\n\n[5] Zhou et al.  Training-free transformer architecture search. CVPR 2022.\n\n[6] Aaron Serianni and Jugal Kalita. Training-free neural architecture search for RNNs and transform-\ners. ACL 2023\n\n[7] So D, Le Q, Liang C. The evolved transformer. ICML 2019."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3977/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700737863456,
                "cdate": 1700737863456,
                "tmdate": 1700738084244,
                "mdate": 1700738084244,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]