[
    {
        "title": "AttEXplore: Attribution for Explanation with model parameters eXploration"
    },
    {
        "review": {
            "id": "LIbpxCRivh",
            "forum": "FsVxd9CIlb",
            "replyto": "FsVxd9CIlb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5047/Reviewer_UxR1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5047/Reviewer_UxR1"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes AttEXplore, an attribution framework based on transferable adversarial attacks. The authors claim that there are shared principles between the decision boundary exploration approaches of attribution and transferable attacks. Based on such observation, the framework performs attribution analysis based on the design of a nonlinear integration path. The frequency-based input feature alterations make the framework transferable across different decision boundaries."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well-written with a clear storyline that motivates the work.\n2. The experiments on multiple attribution baselines are comprehensive.\n3. The idea of bridging attribution methods and adversarial attacks is novel."
                },
                "weaknesses": {
                    "value": "1. Given the transferable claim of this new attribution method, it will be necessary to show sufficient visualizations of how the attribution generalizes to different types of task models other than image classification.\n\n2. The post-hoc explanation methods often suffer from false-positive responses, which highlight the semantic parts that are actually not relevant to the true grounding objects. The paper should elaborate more on how the framework overcomes such challenges. Similarly, the paper shall have more quantitative evaluations on whether the attribution is correctly aligning with the ground truth.\n\n3. Recent years have witnessed the development of white-box transformers (e.g., [1]), whose self-attentions naturally emerge as attributions for the model decision. It remains a question as how will AttEXplore outperform these interpretable-by-design approaches.\n\n[1] Yu et al., Emergence of Segmentation with Minimalistic White-Box Transformers."
                },
                "questions": {
                    "value": "Please address my concerns listed in the weakness section. I look forward to the authors' response and I will possibly consider revising the rating based on the response."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5047/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5047/Reviewer_UxR1",
                        "ICLR.cc/2024/Conference/Submission5047/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5047/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698708918968,
            "cdate": 1698708918968,
            "tmdate": 1700616699966,
            "mdate": 1700616699966,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MG3yqvsgzk",
                "forum": "FsVxd9CIlb",
                "replyto": "LIbpxCRivh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5047/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5047/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official reply to Reviewer UxR1"
                    },
                    "comment": {
                        "value": "**Weaknesses:**\n\n1. Thank you for the thoughtful consideration of our work. It is necessary to generalize our method to models for different tasks beyond image classification. Working on the interpretability of NLP models is a good direction. For example, verifying the interpretability of medical reports generated by applying NLP technology. \n     At present, our work is aimed at attribution interpretation of image related tasks, which is consistent with the current SOTA research works including AGI [1], BIG [2], GIG [3], and SG [4].\n \n2. Given that our method satisfies the sensitivity and implementation invariance axioms in IG [5], this ensures a one-to-one correspondence between the model's input and output during attribution, guaranteeing the accuracy of the attribution process. We have provided detailed proofs of our method's adherence to these axioms in the appendix.\n \n3. We appreciate this suggestion. However, attribution methods are model agnostic interpretability methods, which is different from the interpretable-by-design approaches for white-box transformers. While Vision Transformers (ViTs) represent a typical application of the Transformer model in computer vision tasks, we have additionally included the experiment result of our method on the ViT model. We anticipate the result in the global comments Table *4 may be helpful to further extend the comparison in the future to evaluate its interpretability performance in this domain. We will include the discussion with this specific white-box transformer [6] work in our revision.\n \nReference:\n\n[1] Pan, D., Li, X., & Zhu, D. (2021, August). Explaining deep neural network models with adversarial gradient integration. In Thirtieth International Joint Conference on Artificial Intelligence (IJCAI).\n\n[2] Wang, Z., Fredrikson, M., & Datta, A. (2022). Robust models are more interpretable because attributions look normal. Proceedings of the 39th International Conference on Machine Learning.\n\n[3] Kapishnikov, A., Venugopalan, S., Avci, B., Wedin, B., Terry, M., & Bolukbasi, T. (2021). Guided integrated gradients: An adaptive path method for removing noise. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 5050-5058).\n\n[4] Smilkov, D., Thorat, N., Kim, B., Vi\u00e9gas, F., & Wattenberg, M. (2017). Smoothgrad: removing noise by adding noise. the 34th International Conference on Machine Learning.\n\n[5] Sundararajan, M., Taly, A., & Yan, Q. (2017, July). Axiomatic attribution for deep networks. In International conference on machine learning (pp. 3319-3328). PMLR.\n\n[6] Yu, Y., Chu, T., Tong, S., Wu, Z., Pai, D., Buchanan, S. and Ma, Y., 2023. Emergence of segmentation with minimalistic white-box transformers. arXiv preprint arXiv:2308.16271."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5047/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700044056577,
                "cdate": 1700044056577,
                "tmdate": 1700044056577,
                "mdate": 1700044056577,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eARBfa2UxU",
                "forum": "FsVxd9CIlb",
                "replyto": "MG3yqvsgzk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5047/Reviewer_UxR1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5047/Reviewer_UxR1"
                ],
                "content": {
                    "comment": {
                        "value": "The response well addresses my concerns. I will rate this paper a borderline accept."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5047/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700616677879,
                "cdate": 1700616677879,
                "tmdate": 1700616677879,
                "mdate": 1700616677879,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WR4ihVexes",
            "forum": "FsVxd9CIlb",
            "replyto": "FsVxd9CIlb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5047/Reviewer_r7kS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5047/Reviewer_r7kS"
            ],
            "content": {
                "summary": {
                    "value": "The authors present AttEXplore, a novel feature attribution method for explaining Deep Neural Networks (DNNs), designed to address the growing need for transparent and interpretable AI models. AttEXplore is inspired by recent advances in the domain of transferable adversarial attacks, and it combines two essential components: \n\n(1) Feature attributions based on adversarial gradient integration, and \n\n(2) Frequency-based data augmentation techniques to enhance the robustness of explanations.\n\nIn their paper, the authors conduct an extensive evaluation of AttEXplore on the ImageNet dataset, employing three different DNN architectures (Inception-v3, ResNet-50, VGG-16). They compare AttEXplore's faithfulness and time complexity against other attribution methods, providing valuable insights into its performance. Furthermore, the authors perform an ablation study to better understand the impact of the key components within AttEXplore.\n\nTo facilitate the adoption of their method, the authors also provide the associated code, making it accessible for other researchers and practitioners seeking to utilize AttEXplore for their own applications."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper offers several notable strengths:\n\n**Leveraging of Insights from Transferable Adversarial Attack Works:** The authors introduce a unique approach that draws inspiration from the field of transferable adversarial attacks. Particularly, their integration of frequency-based data augmentation techniques, aimed at reducing noise in heatmaps (usually caused by gradient shattering and/or high non-linearity of DNNs), is an innovative contribution. This demonstrates a thoughtful incorporation of knowledge from a related domain into the field of local XAI.\n\n**Comprehensive Model Evaluation:** The authors carry out an extensive evaluation, involving multiple deep neural network models and the ImageNet dataset. This thorough examination of AttEXplore's performance across different DNN architectures enhances the credibility of their findings and demonstrates its versatility and applicability.\n\n**Related Work Section:** The authors effectively contextualize their work within the existing literature, highlighting its relevance and significance in the broader research landscape.\n\n**Open-Source Code Availability:** A notable strength of this paper is the provision of the code associated with AttEXplore. This openness enables other researchers and practitioners to readily adopt and build upon the proposed method, promoting further research and development in the area of local XAI.\""
                },
                "weaknesses": {
                    "value": "The paper exhibits, however, several weaknesses:\n\n**Clarity:** \nSome sections of the paper lack clarity, making it challenging for readers to grasp the key concepts. The abstract and introduction suffer from being overly specific without providing sufficient context. Additionally, important elements of the proposed method are not adequately introduced, leaving readers without a clear understanding of the approach.\n\nFigure 1, a critical element in conveying the method, could benefit from improved clarity. Enhancements to this figure would help readers better comprehend their motivation for AttEXplore.\n\nFurther, the paper frequently references the appendix but fails to provide specific locations. It would be highly beneficial if the authors included clear references to specific sections within the appendix, making it easier for readers to locate relevant supplementary information.\n\nTo me, some sentences in the paper do not make sense, such as \"DeepLIFT and LIME generate explanations for generic models which limits the understanding of model\u2019s global behaviors.\u201c Why would it limit the understanding, and what do you mean with global behvior?\n\nIn the Method section, Equation (4) includes index $i$ without a clear definition. It is unclear whether $I$ refers to a random draw of sample $x$ or a class index. This ambiguity needs to be resolved to enhance the precision of the method description.\n\n**Evaluation Shortcomings:**\nThe evaluation has certain limitations. The absence of faithfulness curves is one deficiency as it hinders a comprehensive understanding of the method's performance. Moreover, the paper does not compare AttEXplore against state-of-the-art approaches designed to address noise in heatmaps, such as LRP (composite rules), SHAP, or LIME. Additionally, the time complexity comparison is limited to only a subset of methods, which restricts the scope of the evaluation and limits the ability to assess AttEXplore's competitive performance in terms of speed.\""
                },
                "questions": {
                    "value": "Have you compared your method against other local XAI techniques that attempt to generate more robust explanations by reducing noisy attributions?\n\nIn the Method section, Equation (4) includes index $i$ without a clear definition. It is unclear whether $I$ refers to a random draw of sample $x$ or a class index. What does $i$ stand for?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5047/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5047/Reviewer_r7kS",
                        "ICLR.cc/2024/Conference/Submission5047/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5047/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698831183827,
            "cdate": 1698831183827,
            "tmdate": 1700723703492,
            "mdate": 1700723703492,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7pp5NUiZDh",
                "forum": "FsVxd9CIlb",
                "replyto": "WR4ihVexes",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5047/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5047/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official reply to Reviewer r7kS (Part 1)"
                    },
                    "comment": {
                        "value": "**Clarity:**\n\nThanks for the constructive comment. We will rewrite the relevant sections of abstract and introduction to facilitate a more seamless understanding for readers. Additionally, we will improve Figure 1, enhancing image clarity and the representation of key elements to provide a clearer understanding of our method. We will also refine the structure of the appendix and provide more precise and specific references to its content within the main text for improved citation.\n \nRegarding the statement \"DeepLIFT and LIME generate explanations for generic models which limits the understanding of the model\u2019s global behaviors,\" our intention is to convey that DeepLIFT [1] and LIME [2], as local interpretability methods, primarily focus on explaining the local neighborhood behaviors of models. These methods typically employ simpler, interpretable models to approximate the behavior of complex models near specific data instances or attempt to perturb inputs to identify the inputs most responsible for a prediction in the neighborhood. Therefore, these two methods do not satisfy the sensitivity and implementation invariance axioms proposed by IG [3], resulting in limited global interpretability of model behaviors.  Global interpretability refers to understanding the entire behavior of the model, not just explanations for specific instances or local regions. Global interpretability focuses on properties such as features, weights, structure, and behavior patterns of the model across the entire input space.\n \nIn Equation (4), the variable $i$ represents the number of frequency domain explorations, i.e., $i=1,2,...,N$. Ultimately, we generate $N$ adversarial samples through frequency domain exploration. We will review the main context to avoid similar ambiguities."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5047/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700043936475,
                "cdate": 1700043936475,
                "tmdate": 1700043936475,
                "mdate": 1700043936475,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pSEEE2XpjF",
                "forum": "FsVxd9CIlb",
                "replyto": "WR4ihVexes",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5047/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5047/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official reply to Reviewer r7kS (Part 2)"
                    },
                    "comment": {
                        "value": "**Evaluation Shortcomings:**\n\nThanks for the reviewer's comments. Our method satisfies the Completeness axiom, as proven in Section 3 of IG [3], serving as a sanity check. We have also conducted INFD score tests [4] in the global comments in Table *2, and the results demonstrate that our method exhibits superior robustness and fidelity.\n \nConcerning LIME [2], LRP [5], and SHAP [6], we would like to note that these methods have different requirements compared to current state-of-the-art attribution methods. \nFirstly, LIME attempts to perturb inputs to identify the inputs most responsible for a prediction in the neighborhood. In addition, LIME\u2019s interpretable behavior requires cluster segmentation of images, so its interpretation model is not point-to-point. And if it performs point-to-point calculations, the computational complexity of LIME will be extremely high. LRP relies on model structure and is challenging to implement. DeepLIFT is a more general case of LRP, and our method surpasses DeepLIFT. SHAP, when applied to samples with high dimensions, uses approximate results for inference and involves a significant amount of interaction operations, resulting in extremely high computational complexity. \n\nSecondly, IG [3] extensively discusses that these methods do not necessarily satisfy the sensitivity and implementation invariance axioms. For example, LRP uses discrete gradients to compute feature importance, which does not satisfy the sensitivity axiom and may lead to instances where attributions change, but the attribution result is 0, rendering interpretability completely ineffective. \n\nLastly, current state-of-the-art methods including AGI [7], BIG [8] and GIG [9] don\u2019t include LIME, LRP, and SHAP as the comparison in their experiments. It is noteworthy that some of the methods are noise reduction-based methods which were published much earlier. In our baseline, GIG represents the state-of-the-art noise reduction-based method as proposed in 2021. We have achieved significant improvement over the GIG method.\n \nRegarding the subset selection for time complexity comparison, we selected the five methods that are closest in performance to our method for efficiency comparison. Other methods, such as SaliencyMap, DeepLIFT and EG have relatively poor attribution accuracy in the baseline, making them unnecessary for comparison. Although Fast-IG achieves faster attribution speeds, its accuracy is unfortunately the lowest.\n \nReference:\n\n[1] Shrikumar, A., Greenside, P., & Kundaje, A. (2017, July). Learning important features through propagating activation differences. In International conference on machine learning (pp. 3145-3153). PMLR.\n\n[2] Ribeiro, M. T., Singh, S., & Guestrin, C. (2016, August). \" Why should i trust you?\" Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining (pp. 1135-1144).\n\n[3] Sundararajan, M., Taly, A., & Yan, Q. (2017, July). Axiomatic attribution for deep networks. In International conference on machine learning (pp. 3319-3328). PMLR.\n\n[4] Yeh, C. K., Hsieh, C. Y., Suggala, A., Inouye, D. I., & Ravikumar, P. K. (2019). On the (in) fidelity and sensitivity of explanations. Advances in Neural Information Processing Systems, 32.\n[5] Bach, S., Binder, A., Montavon, G., Klauschen, F., M\u00fcller, K. R., & Samek, W. (2015). On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. PloS one, 10(7), e0130140.\n\n[6] Lundberg, S. M., & Lee, S. I. (2017). A unified approach to interpreting model predictions. Advances in neural information processing systems, 30.\n\n[7] Pan, D., Li, X., & Zhu, D. (2021, August). Explaining deep neural network models with adversarial gradient integration. In Thirtieth International Joint Conference on Artificial Intelligence (IJCAI).\n\n[8] Wang, Z., Fredrikson, M., & Datta, A. (2022). Robust models are more interpretable because attributions look normal. Proceedings of the 39th International Conference on Machine Learning.\n\n[9] Kapishnikov, A., Venugopalan, S., Avci, B., Wedin, B., Terry, M., & Bolukbasi, T. (2021). Guided integrated gradients: An adaptive path method for removing noise. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 5050-5058)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5047/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700043980958,
                "cdate": 1700043980958,
                "tmdate": 1700648606175,
                "mdate": 1700648606175,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YtZQApg7Pg",
                "forum": "FsVxd9CIlb",
                "replyto": "7pp5NUiZDh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5047/Reviewer_r7kS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5047/Reviewer_r7kS"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for the clarifications and your willingness to improve certain sections of your work.\n\nUnfortunately, I still can not follow your remark fully about DeepLIFT and LIME. First, because DeepLIFT is based on the (modified) gradient and thus does not require a simpler interpretable model or perturbs inputs. Also according to [3] (your reference), DeepLIFT fulfills the sensitivity axiom. Further, one could also apply LIME / or perturbation-based methods to weights and structures (however, might be computationally very demanding, as you also point out in your reply)."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5047/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700638936838,
                "cdate": 1700638936838,
                "tmdate": 1700638936838,
                "mdate": 1700638936838,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TSk0aeSlm2",
                "forum": "FsVxd9CIlb",
                "replyto": "pSEEE2XpjF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5047/Reviewer_r7kS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5047/Reviewer_r7kS"
                ],
                "content": {
                    "comment": {
                        "value": "I agree that LIME and SHAP are infeasible to apply on a pixel-level level. However, still, my point remains that it would be of interest to compare your method against explanation methods that result in less noisy heatmaps. This also includes GradCAM, which [1] includes in the evaluation.\n\nRegarding the complexity comparison: is SaliencyMap [2] not simply the gradient (i.e., derivative of output of softmax class node with respect to the input image), thus computationally very efficient? I still think it will be valuable to include all evaluated methods to understand the trade-off between attribution faithfulness/accuracy and efficiency.\n\nReferences:\n[1] Andrei Kapishnikov, Subhashini Venugopalan, Besim Avci, Ben Wedin, Michael Terry, Tolga Bolukbasi:\nGuided Integrated Gradients: An Adaptive Path Method for Removing Noise. CVPR 2021: 5050-5058\n\n[2] Karen Simonyan, Andrea Vedaldi, Andrew Zisserman: Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps. ICLR (Workshop Poster) 2014"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5047/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700640167110,
                "cdate": 1700640167110,
                "tmdate": 1700640167110,
                "mdate": 1700640167110,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mMq4er3GQw",
                "forum": "FsVxd9CIlb",
                "replyto": "WR4ihVexes",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5047/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5047/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Our clarification (Part 1)"
                    },
                    "comment": {
                        "value": "Thanks to the reviewer for the response. We hope that the following detailed discussion will clear up your confusion:\n\nFirst of all, we would like to correct our point of view. In fact, DeepLIFT [1] does not all satisfy the two axioms proposed by IG [2]. Although DeepLIFT does not require a simpler interpretable model or perturbs inputs, and satisfies the sensitivity axiom, DeepLIFT breaks Implementation Invariance axiom. The corresponding original text in IG is at the beginning of Section 2: **We now discuss two axioms (desirable characteristics) for attribution methods. We find that other feature attribution methods in literature break at least one of the two axioms. These methods include DeepLift ( Shrikumar et al., 2016; 2017), Layer-wise relevance propagation (LRP) (Binder et al., 2016), Deconvolutional networks (Zeiler & Fergus, 2014), and Guided back-propagation (Springenberg et al., 2014) .**\n\nA more direct explanation can be found in the second paragraph of Section 2.2: **We now discuss intuition for why DeepLift and LRP break Implementation Invariance.**\n\nOur attribution method satisfies the two axioms of sensitivity and implementation invariance, which enables one-to-one correspondence between model input and output to obtain more accurate attribution results.\n\nSecondly, gradient-based and perturbation-based methods are currently two important directions in interpretability methods. LIME [3] or other perturbation-based methods obtain the contribution of the model input to the output through perturbation. But this is not consistent in principle with our gradient-based attribution method, where we leverage the backward pass of a neural network to assess the influence and relevance of an input feature on the model decision. In addition, as you said, LIME requires a large amount of calculation, which is one of its disadvantages.\n\n\n[1] Shrikumar, A., Greenside, P., & Kundaje, A. (2017, July). Learning important features through propagating activation differences. In International conference on machine learning (pp. 3145-3153). PMLR.\n\n[2] Sundararajan, M., Taly, A., & Yan, Q. (2017, July). Axiomatic attribution for deep networks. In International conference on machine learning (pp. 3319-3328). PMLR.\n\n[3] Ribeiro, M. T., Singh, S., & Guestrin, C. (2016, August). \" Why should i trust you?\" Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining (pp. 1135-1144)."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5047/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700648357327,
                "cdate": 1700648357327,
                "tmdate": 1700649072533,
                "mdate": 1700649072533,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FqMv1sKlrZ",
                "forum": "FsVxd9CIlb",
                "replyto": "WR4ihVexes",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5047/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5047/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Our clarification (Part 2)"
                    },
                    "comment": {
                        "value": "Thank you for your constructive comments. GradCAM [1] needs to select a layer in the middle (the dimension is lower than the original image dimension and has multiple channels). Feature channels are combined based on the gradient sum of each channel. Because of the low dimensionality, pixel-level attribution cannot be achieved. In addition, since it only uses the gradient of a single state, similar to Saliency Map [2], it does not satisfy Sensitivity. Although we believe that GradCAM is not an attribution algorithm in the strict sense (does not satisfy the definition and axioms of attribution) and it cannot be on the same track as our method, we are happy to provide the comparison results of GradCAM. While we believe this is not necessarily fair, our method still perform better than GradCAM. Because our method is better than GradCAM in both insertion score and deletion score.\n\nIn order to prove the effect of our method on other models, we also conducted additional experiments on the currently widely used Vision Transformer (ViT) models. The experimental results can be seen in Table 4 in the global comment. Our method can still achieve the best results on the ViT models.\n\n**We will update all details of this experiment to the anonymous link of the code.**\n\nWe apologize for our honest fault in the rebuttal reply in Official reply to Reviewer r7kS (Part 2), which is a typo for 'Other methods, such as SaliencyMap, involve extensive computations, making them slower'. In fact, as you point out, Saliency Map is more efficient. We would like to clarify that **Other methods, such as SaliencyMap, DeepLIFT [3] and EG [4] have relatively poor attribution accuracy in the baseline, making them unnecessary for comparison.** We will correct this typo in the original reply.\n\nWe have listed the speed analysis table of the algorithm in Section 5.3 of the article. It can be seen that the efficiency of our algorithm has achieved the optimal effect in the IG-based attribution method. On the other hand, we believe that the faithfulness of attribution is more important than the efficiency of the interpretability.\n\n| \u3000         | Inception-v3 |          | ResNet-50 |          | VGG-16    |          |\n|------------|--------------|----------|-----------|----------|-----------|----------|\n| Method     | Insertion    | Deletion | Insertion | Deletion | Insertion | Deletion |\n| GradCAM    | 0.4496       | 0.1084   | 0.2541    | 0.0942   | 0.3169    | 0.0841   |\n| AttEXplore | 0.4732       | 0.0297   | 0.4197    | 0.0293   | 0.3186    | 0.0226   |\n\n[1] Selvaraju, R. R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., & Batra, D. (2017). Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE international conference on computer vision (pp. 618-626).\n\n[2] Karen Simonyan, Andrea Vedaldi, Andrew Zisserman: Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps. ICLR (Workshop Poster) 2014\n\n[3] Shrikumar, A., Greenside, P., & Kundaje, A. (2017, July). Learning important features through propagating activation differences. In International conference on machine learning (pp. 3145-3153). PMLR.\n\n[4] Erion, G., Janizek, J. D., Sturmfels, P., Lundberg, S. M., & Lee, S. I. (2021). Improving performance of deep learning models with axiomatic attribution priors and expected gradients. Nature machine intelligence, 3(7), 620-631."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5047/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700648531797,
                "cdate": 1700648531797,
                "tmdate": 1700649824185,
                "mdate": 1700649824185,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nPXLmJh65c",
                "forum": "FsVxd9CIlb",
                "replyto": "FqMv1sKlrZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5047/Reviewer_r7kS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5047/Reviewer_r7kS"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much again for your additional results and detailed clarifications.\nWith these additional results for the evaluation section and a rewriting to improve clarity of some sections in the main manuscript, I am increasing my score from 5 to 6 (marginally above the acceptance threshold)."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5047/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700723683860,
                "cdate": 1700723683860,
                "tmdate": 1700723683860,
                "mdate": 1700723683860,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "sgZQBxRdCL",
            "forum": "FsVxd9CIlb",
            "replyto": "FsVxd9CIlb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5047/Reviewer_M36V"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5047/Reviewer_M36V"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel attribution method called AttEXplore, designed for explaining DNN models through the exploration of model parameters. AttEXplore leverages the concept of transferable attacks as there's consistency between the decision boundary exploration approaches of attributionand the process for transferable adversarial attacks. By manipulating input features according to their frequency information, AttEXplore enhances the interpretability of DNN models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper is well-written and easy to follow.\n2. AttEXplore outperforms existing methods on the ImageNet dataset and models such as Inception-v3, ResNet-50, and VGG-16, achieving higher insertion scores and lower deletion scores, which underscores its effectiveness in model evaluation.\n3. AttEXplore exhibits superior computational efficiency in terms of the number of frames processed per second when compared to existing methods."
                },
                "weaknesses": {
                    "value": "1. This paper appears to focus exclusively on evaluating AttEXplore using image data, it lacks experiments on other data modalities, such as text data on NLP models. Expanding the evaluation to different data modalities could provide a more comprehensive assessment of AttEXplore's applicability.\n2. For image classification models, the authors did experiments on CNN model groups including Inception-v3, ResNet-50, and VGG-16. It is unclear that if the superior explainability of AttEXplore stands still on other model groups, like vision transformers and MLP model groups. Extending the evaluation to diverse model architectures would further validate its effectiveness across different model types."
                },
                "questions": {
                    "value": "It's shown in Table 1 in the appendix that compared with N = 10, the drop of insertion score  for N = 1is not huge (~0.007 for inception-v3, still better than existing methods ). The perturbation rate was set as 16, while the model performance is better when perturbation rate is 48. Would the trend of model performance under different N be more clear when the perturbation rate is set as a larger value?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5047/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5047/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5047/Reviewer_M36V"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5047/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698838710466,
            "cdate": 1698838710466,
            "tmdate": 1699636494399,
            "mdate": 1699636494399,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JveIXMIr9c",
                "forum": "FsVxd9CIlb",
                "replyto": "sgZQBxRdCL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5047/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5047/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official reply to Reviewer M36V"
                    },
                    "comment": {
                        "value": "**Weaknesses:**\n\n1. Thanks for pointing out the valuable comment for attribution method research. Actually, exploring interpretability in the context of NLP models is undoubtedly a valuable direction, especially in domains such as medical report generation where interpretability is crucial. \nCurrently, our work focuses on attribution interpretation for images, aligning with the current state-of-the-art approaches like AGI [1], BIG [2], GIG [3], SG [4], and other baselines. We would like to provide a discussion of potential work following the comment for NLP works.\n2. We appreciate the reviewer's constructive suggestions. We have conducted the additional experiment on the model of Vision Transformers (ViTs). ViTs represent an excellent vision model based on the Transformer architecture. Different from traditional DNN models, ViTs process images as sequences of patches, leading to a patch-like interpretation of features [5]. The experimental results on ViTs, included in the global comments Table *4, still demonstrate that our method achieves the best performance.\n \n**Questions:**\n\nWe are afraid the answer is negative for the trend of model performance under different $N$, even with a larger perturbation rate. \n\nOn one hand, the ablation experiments in Tables 3, 4 and 5, and the appendix file, indicate that increasing the number of generated features or perturbation rates, among other hyperparameters, can enhance attribution accuracy to some extent. This is rational as the increase in the number of generated features ($N$) implies the exploration of more frequency domain information, while an increase in $\\epsilon$ signifies the generation of higher-quality adversarial samples. Consequently, the performance at a perturbation rate of 48 may surpass the result with a perturbation rate of 16.\n \nOn the other hand, the impact of altering hyperparameters on performance, as observed in ablation experiments, is relatively small. This suggests that the improvement of our method does not heavily rely on the specific choice of hyperparameters. However, when the perturbation rate is set to a larger value such as 48, the trend in model performance across different $N$ values does not become more clear. This might be attributed to the fact that a larger perturbation rate represents a larger search space. Although the number of explored samples increases, it doesn\u2019t mean that all these samples are necessarily effective for the attribution result. We have validated this observation in the additional experiments provided in the global comments Table *5.\n \nReference:\n\n[1] Pan, D., Li, X., & Zhu, D. (2021, August). Explaining deep neural network models with adversarial gradient integration. In Thirtieth International Joint Conference on Artificial Intelligence (IJCAI).\n\n[2] Wang, Z., Fredrikson, M., & Datta, A. (2022). Robust models are more interpretable because attributions look normal. Proceedings of the 39th International Conference on Machine Learning.\n\n[3] Kapishnikov, A., Venugopalan, S., Avci, B., Wedin, B., Terry, M., & Bolukbasi, T. (2021). Guided integrated gradients: An adaptive path method for removing noise. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 5050-5058).\n\n[4] Smilkov, D., Thorat, N., Kim, B., Vi\u00e9gas, F., & Wattenberg, M. (2017). Smoothgrad: removing noise by adding noise. arXiv preprint arXiv:1706.03825.\n\n[5] Ma, W., Li, Y., Jia, X., & Xu, W. (2023). Transferable Adversarial Attack for Both Vision Transformers and Convolutional Networks via Momentum Integrated Gradients. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 4630-4639)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5047/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700043842391,
                "cdate": 1700043842391,
                "tmdate": 1700043842391,
                "mdate": 1700043842391,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OAo7zzHz9k",
            "forum": "FsVxd9CIlb",
            "replyto": "FsVxd9CIlb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5047/Reviewer_H27T"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5047/Reviewer_H27T"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a new attribution technology AttExplore that improves over three existing gradient integral methods, IG, BIG and AGI. The main contributions are that 1) the proposed method finds a baseline in the integral that serves as an adversarial input for the current model and some other variations; 2) their empirical results show improved insertion and deleting scores compared to baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Authors have made it clear how AttExplore computes the attribution scores. Their experiments include 3 networks and a rich amount of baselines. Table 1 shows a good amount of improvement over the best baseline."
                },
                "weaknesses": {
                    "value": "My main concerns include the motivation and the evaluation of this paper. \n\n### Motivation\nI am not entirely sure I understand the motivation of the paper. It looks like this paper accuses BIG and AGI of using the exact boundary of the underlying model, which fails to be generic to similar models with variations in the decision boundary? This is pretty counterintuitive to me as feature attributions are often perceived as local explanations at the given model and the given input. Why do we need to consider if the explanation generalizes other models? With that being said, generalizing to other decision boundaries may trade in the faithfulness of the underlying model (this would relate to my second question about faithfulness and I will explain later). \n\n### Generalization of Frequency-based Methods?\nThe motivation to alter features in the frequency domain is because there are works showing high-frequency features help the model to generalize better [1]. I am pretty worried the authors explain this conclusion as altering some features in the frequency domain helps to create examples that transfer better, especially for methods just proposed in this paper (Eq. 4 - 5).  Have you verified your adversarial examples actually transfer better? Can you provide some analytical results convincing me this helps better explore more decision boundaries? So far all descriptions are pretty hand-wavy.  \n\n### Evaluations\nIs the method faithful to the underlying model? It looks like to realize \u201ca more general adversarial input\u201d the proposed method manages to find an adversarial example that is much further to the decision boundary compared to the previous methods. I think authors may want to compare how much farther you go. It is pretty concerning to me that no matter how you adjust the feature generations in Table 3 and steps in Table 4, the proposed method has almost identical results on your metrics. Similar, no matter how you decrease $\\epsilon$ in Table 5, the results do not change at all. What should we make out of these results? Are they showing the proposed method may be actually unfaithful to something? BTW, using $\\epsilon=8$ is pretty huge and I think decision boundaries are usually much much closer than $8/255$ for models without adversarial training. \n\nI recommend testing INFD score [2] and run sanity check [3].\n\n### Ending\n\nUnlike doing detection or being more robust, I really think research works in explaining feature importance are not result-driven. Namely, it is not about being state-of-the-art on some scores. Not every attribution method uses deletion and inserting scores in baseline papers cited. There are a lot of discussions for the unreliability of feature attributions that are not cited here. It is fine to say \u201cwe do not care about those critiques but just want to improve the current method\u201d but it is important to point out *important flaws* in the existing methods and fix those. \n\n[1] Haohan Wang, Xindi Wu, Zeyi Huang, and Eric P Xing. High-frequency component helps explain the generalization of convolutional neural networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 8684\u20138694, 2020b.\n\n[2] Yeh, C. K., Hsieh, C. Y., Suggala, A., Inouye, D. I., & Ravikumar, P. K. (2019). On the (in) fidelity and sensitivity of explanations. Advances in Neural Information Processing Systems, 32.\n\n[3] Adebayo, J., Gilmer, J., Muelly, M., Goodfellow, I., Hardt, M., & Kim, B. (2018). Sanity checks for saliency maps. Advances in neural information processing systems, 31."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5047/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698949424473,
            "cdate": 1698949424473,
            "tmdate": 1699636494324,
            "mdate": 1699636494324,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wSfGf4TjbK",
                "forum": "FsVxd9CIlb",
                "replyto": "OAo7zzHz9k",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5047/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5047/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official reply to Reviewer H27T (Part 1)"
                    },
                    "comment": {
                        "value": "**Motivation:**\n\nWe appreciate the comments about the motivation and are sorry about the confusion.\n \nWe would like to clarify a misunderstanding about the comparison with BIG and AGI (which are the SOTA methods), by highlighting the novel contribution which is also noted by other reviewers. The technical contribution is actually to leverage the insight from transferable adversarial attacks.\n \nThe motivation is to enhance the attribution performance by considering different decision boundaries with the insights from transferable adversarial attacks. We consider the features to be crucial for the model output if the altered sample will cross the decision boundary. The summarization of the contributions of these features is considered as the attribution results. However, the training process from literature methods is limited to its capability of finding the accurate decision boundary (most variations in the decision boundary are OOD samples). How to effectively identify and leverage the decision boundary is challenging. \n\nThus, inspired by the exploration of decision boundaries in transferable attacks, we observe the consistency of such adversarial sample generation with the samples for attribution. The reason is that a transferable adversarial sample on a given model also needs multiple operations of exploring the decision boundaries for a robust result. In this way, we consider the obtained sample will be able to cross more decision boundaries without the parameters from the target model, which means a more accurate attribution result. \nMore discussion is available in Section 4.1, where we provide the reasons about the possibility of utilizing the samples to explore the decision boundary during the training process. \n \nMoreover, we argue that the performance of current attribution methods is generally influenced by the choice of different baselines. When the underlying task is complex, selecting an appropriate baseline is often difficult and ad-hoc. Therefore, for the decision boundaries of specific models corresponding to different underlying tasks, the local explanation of IG may not be optimal. For BIG, it uses a linear attribution path based on adversarial samples, but it cannot effectively solve this problem because linear attribution paths are unstable when crossing the decision boundaries of nonlinear complex models. For AGI, although it incorporates adversarial samples and nonlinear attribution paths other than the subjective selection of baselines, it aims at targeted adversarial attack, which may cover some unnecessary decision boundaries before successfully exploring the decision boundary corresponding to the target label, potentially leading to overfitting issues.\n \nIn conclusion, to obtain a decision boundary that allows samples to cross without overfitting concern, we consider directly generating multiple decision boundaries with slight offsets through parameter adjustment is not practical and effective. Therefore, we conduct input exploration with transferable adversarial attack to simulate the process of sample generation by crossing the decision boundaries, thereby obtaining more accurate attribution.\n \n**Generalization of Frequency-based Methods?**\n\nPlease see the analytical results provided in the global official comment in Table *1-1 and *1-2, which shows the result of attack success rate with different transferable adversarial attack methods. The generated adversarial samples are tested for the attack success rate on other victim models, in which a higher attack success rate represents a higher potential for crossing other general decision boundaries [1]. We extend our discussion of the motivation to leverage the insights from transferable attacks for decision boundaries exploration to further enhance the attribution method performance. In practice, it means that, if the adversarial samples can effectively cross various decision boundaries, it implies a higher transferability for transferable attacks and, for attribution, a better capability to obtain more accurate identification of critical features. Additionally, we emphasize that our findings extend beyond frequency domain transformations. We have tested the impact of different input transformations for interpretability."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5047/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700040279928,
                "cdate": 1700040279928,
                "tmdate": 1700112059986,
                "mdate": 1700112059986,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "l4AMQeWxDK",
                "forum": "FsVxd9CIlb",
                "replyto": "OAo7zzHz9k",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5047/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5047/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official reply to Reviewer H27T (Part 2)"
                    },
                    "comment": {
                        "value": "**Evaluations:**\n\nRegarding Tables 3, 4 and 5, these results are for the ablation study, which is to evaluate the impact of different hyperparameters. The conclusion is that, by increasing the number of generated features or perturbation rates, the attribution accuracy may be slightly increased. This is also apparent since an increase in $N$ implies the exploration of more frequency domain information, and an increase in $\\epsilon$ signifies higher-quality adversarial samples.\nHowever, we also acknowledge that the improvement is not substantial, indicating that our method's effectiveness does not heavily rely on hyperparameter selection, showcasing higher generalization and fidelity. We also provide the performance results utilizing different transferable attack methods in the appendix, which proves the effectiveness of our method.\n\nConcerning the $\\epsilon$ value, the value of 8 in ablation experiments is not large as we have followed the literature method from [2]. Typically, only when $\\epsilon<=16$, we consider the samples can reliably cross more general decision boundaries for a satisfactory attack outcome. We have now conducted more additional experiments to demonstrate that our method doesn\u2019t need particular hyperparameter selection.\n \nFor the INFD score and sanity check, our method is faithful to the underlying model, as demonstrated in Section 3 of IG [3], which has proven the Completeness (Sensitivity) axiom as a sanity check. We did not utilize Faithfulness [4] as a fidelity metric because its calculation involves random selection of a subset, and the subset's size and randomness in selection will introduce significant bias to the results. To mitigate this bias, a more extensive range of sampling is required, leading to substantial computational costs. \nThus, we have been mindful when employing such a stochastic evaluation metric, and we would like to note that for AGI, BIG, GIG and similar methods, they also do not adopt this metric in their experiment. \n\nIn the global official comment, we provide INFD score results [5] in Table *2, and conduct ablation experiments with lower $\\epsilon$ values in Table *3, demonstrating that our method exhibits the best performance of robustness and fidelity.\n \nReference:\n\n[1] Zhu, Z., Chen, H., Zhang, J., Wang, X., Jin, Z., Lu, Q., ... & Choo, K. K. R. (2023, October). Improving Adversarial Transferability via Frequency-based Stationary Point Search. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management (pp. 3626-3635).\n\n[2] Zhang, J., Wu, W., Huang, J. T., Huang, Y., Wang, W., Su, Y., & Lyu, M. R. (2022). Improving adversarial transferability via neuron attribution-based attacks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 14993-15002).\n\n[3] Sundararajan, M., Taly, A., & Yan, Q. (2017, July). Axiomatic attribution for deep networks. In International conference on machine learning (pp. 3319-3328). PMLR.\n\n[4] Bhatt, U., Weller, A., & Moura, J. M. (2021, January). Evaluating and aggregating feature-based model explanations. In Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence (pp. 3016-3022).\n\n [5] Yeh, C. K., Hsieh, C. Y., Suggala, A., Inouye, D. I., & Ravikumar, P. K. (2019). On the (in) fidelity and sensitivity of explanations. Advances in Neural Information Processing Systems, 32."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5047/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700043678655,
                "cdate": 1700043678655,
                "tmdate": 1700043678655,
                "mdate": 1700043678655,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cpjyOQ477v",
                "forum": "FsVxd9CIlb",
                "replyto": "l4AMQeWxDK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5047/Reviewer_H27T"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5047/Reviewer_H27T"
                ],
                "content": {
                    "title": {
                        "value": "My score has to remain"
                    },
                    "comment": {
                        "value": "Thanks for the exhaustive responses provided by the authors. After reading the rebuttal, I have a mixed feeling about this paper.\n\nFirst of all, I acknowledge the authors' efforts in providing many more tables in the global comment. However, after I read the rebuttal and the table more carefully, I do not think they managed to address my concerns. \n\n**Motivation.** \n> The motivation is to enhance the attribution performance by considering different decision boundaries with the insights from transferable adversarial attacks. We consider the features to be crucial for the model output if the altered sample will cross the decision boundary. The summarization of the contributions of these features is considered as the attribution results. ...\n\nI am still unable to clearly see \"why one wants to cross multiple decision boundary\". What benefit does crossing multiple decision boundary bring to our explanation. This is too hand-wavy. I really hope there is any rigorous justification.\n\n> In this way, we consider the obtained sample will be able to cross more decision boundaries without the parameters from the target model, which means a more accurate attribution result.\n\nI don't think transferable adversarial examples play any role in motivating this paper. Transferable attack is a **means** to find baseline points that on the other sides of many decision boundaries. That is, it is not a motivation. The technique itself does not justify any purpose. \n\n**Generalization**\n\nThanks for showing FDE actually transfer to many other models. However, one weird thing to me is that FDE is so much better than your baselines. To be honest, the purpose of this table is not to show FDE works better than other methods (otherwise I have to cite more recent papers and show that you are not using reasonable baselines), which is not the reason I asked for this experiment. The authors can add the results of FDE to the paper but you should not try to demonstrate how well it is compared to your baselines. Namely, the purpose of this experiment is to show you use some method that transfer reasonably good and it is not necessary to be the state-of-the-art attack to be useful to your proposed explanation. \n\n**Evaluations**\n\nFaithfulness is not equal to completeness. But INFD results show that your proposed method is reasonably faithful within some ball park compared to BIG and Saliency Map. I appreciate for this result.\n\nHowever, on the other hand, Table 3 strengthens my concerns about whether \"using FDE points\" is actually important to your final results. No matter how you change $\\epsilon$, your results do not change. I do not interpret $\\epsilon$ as a hyper-parameter. It is a really important argument. It determines how far the point you choose is away from the input of interest. And both results show that that distance does not matter at all, which is counter-intuitive. \n\nIn summary, my score is mostly based on motivations and take-aways of the current paper. I appreciate for running more experiments during this period. However, I find too many things are not clearly stated, discussed, justified and presented. As a result, I cannot increase my score."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5047/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700727778844,
                "cdate": 1700727778844,
                "tmdate": 1700727778844,
                "mdate": 1700727778844,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NdfcJ8zDqN",
                "forum": "FsVxd9CIlb",
                "replyto": "OAo7zzHz9k",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5047/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5047/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the reviewer for the response.\n\n**Clearer Motivation clarification:**\n\nIt is obvious that if an input feature can cross the decision boundary of the model and change the decision-making of the model, then this input feature can be attributed as a high-contribution feature, that is, an important feature. However, we need to consider the impact of inaccurate decision boundaries during model training, because usually, the confidence of the training data is high and far away from the decision boundaries. That is to say, those samples that are very close to the decision boundary are likely to be OOD samples and are prone to overfitting.\n\nWe believe that since the transferable attacks can more generally cross the decision boundary of the target model, the decision boundary obtained by the transferable attacks is likely to be less overfitting than the original decision boundary, in other words, more accurate. Therefore, it is clear whether we can use the characteristics of transferable attacks as our motivation for attribution. Because the relationship between attribution and decision boundaries is very obvious. We also illustrate the equivalence between decision boundaries and input exploration in the paper.\n\n**Clearer Generalization clarification:**\n\nRegarding your **original comment**, *'I am pretty worried the authors explain this conclusion as altering some features in the frequency domain helps to create examples that transfer better, especially for methods just proposed in this paper (Eq. 4 - 5). Have you verified your adversarial examples actually transfer better? Can you provide some analytical results convincing me this helps better explore more decision boundaries?'*\n\nOur purpose in adding FDE experiments is to **implement the proposed requirements**, *which also shows that our method can achieve better transferable attack results (meaning more decision boundaries are crossed)*. We do not attempt to prove that the transferability of FDE is better, since our goal is focusing on attribution. The cited baselines are from recent years to illustrate that there is indeed an impetus to crossing more general baselines. Similarly, comparing all transferable attack methods is not realistic. For us, we have selected the **most representative and SOTA works** to prove the effectiveness of FDE. Our goal is not to create a better transferable attack but to illustrate the effectiveness of frequency exploration for attribution. Always, there will be better ways to move the attacks, but that's not the focus of this paper, not either the job for attribution.\n\nFor the comparison process, we used the same open-source code and the same experimental design as the baselines in most recent years. And we have added all the details in the anonymous code.\n\n**Clearer clarification of Evaluations:**\n\nOur experiments show that the larger $epsilon$ does have an impact on the results, and it shows an increasing trend. Together with Table 5 (in our submission), Table *5, and the Table *3, the impact of $epsilon$ cannot be ignored. From the literature, the general setting for $epsilon$ is 16, which is actually larger than 8. In addition, our attribution process also requires multiple gradient ascents, so even a very low $epsilon$ will have an effect after multiple gradient ascents. This is not counter-intuitive."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5047/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700734750055,
                "cdate": 1700734750055,
                "tmdate": 1700736390474,
                "mdate": 1700736390474,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]