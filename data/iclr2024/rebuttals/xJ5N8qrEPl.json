[
    {
        "title": "Constrained Bi-Level Optimization: Proximal Lagrangian Value function Approach and Hessian-free Algorithm"
    },
    {
        "review": {
            "id": "xWzHZGnqTy",
            "forum": "xJ5N8qrEPl",
            "replyto": "xJ5N8qrEPl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3552/Reviewer_4Vih"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3552/Reviewer_4Vih"
            ],
            "content": {
                "summary": {
                    "value": "This paper concerns bi-level optimization (BLO) problems with coupled inner constraints. By introducing the Moreau envelope of the Lagrange function, the lower-level problem can be reformulated into a smooth function value constraint. Then, a single loop algorithm is proposed based on the formulation. Non-asymptotic rate is established for the proposed method using a newly defined stationarity measure."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The smoothing technique based on the Moreau envelope is good. This provides a new reformation for the BLO.\n\n2. The theoretical rate and numerical experiments are sufficient."
                },
                "weaknesses": {
                    "value": "1.Assumption 3.1(i) seems too strong for me. Usually, the involved functions in BLO are assumed convex only w.r.t. $y$. \n\n2. The stationarity measure needs further clarification. The proposed measure is based on (16). However, it is not clear how the stationary points of (16) are related to the original BLO.\n\n3. The techniques in the theoretical proofs lack novelty, which directly takes advantage of analysis for penalty methods."
                },
                "questions": {
                    "value": "What is the central benefit of using the Moreau envelope instead of the optimal value function, especially in the technical parts?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Research integrity issues (e.g., plagiarism, dual submission)"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "This submission is rather similar to submission 3145 of ICLR 2024. The similarities between submission 3552 (this paper) and 3145 are summarized as follows.\n\n(a)Submissions 3552 and 3145 both consider BLO with constrained LL problems. \n\n(b)Submission 3552 (denoted by [A]) considers the proximal functions in dealing with the LL problem and submission 3145 (denoted by [B]) uses the Moreau envelope. These two techniques are basically the same in terms of the inner sights. Please compare (7) (8) in [A] and (7) (8) in [B].\n\n(c)The main equations in the two submissions are highly similar. Please compare (9)-(16) in [A] and (9)-(16) in [B]. \n\n(d)The theoretical results are basically the same. Please compare Theorem 3.1 in [A] and Theorem 3.1 in [B]. \n\nI hope that the authors can address my concern. Thanks."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3552/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3552/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3552/Reviewer_4Vih"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3552/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697997500004,
            "cdate": 1697997500004,
            "tmdate": 1700715126399,
            "mdate": 1700715126399,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3ILpBITQrg",
                "forum": "xJ5N8qrEPl",
                "replyto": "xWzHZGnqTy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3552/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3552/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 4Vih"
                    },
                    "comment": {
                        "value": "We sincerely appreciate the time you dedicated to reviewing our paper. Your feedback is valuable to us, and we would like to address some specific points for clarification.\n\n(1) Assumption 3.1(i) seems too strong for me. Usually, the involved functions in BLO are assumed convex only w.r.t. $y$.\n\n$\\textbf{Reply:}$\nWe appreciate your comment and would like to clarify. First, Assumption 3.1 is about the $L$-smoothness and the lower boundedness of $F(x,y)$, without imposing any convexity condition on $F$.\n\nIf you are referring to the convexity in Assumption 3.3(i), it is a common and standard assumption in constrained bilevel optimization. Recent research, such as Gao et al. (2023), also employs this assumption.\n\nIt is important to note that in all experiments conducted by Khanduri et al. (2023), Xiao et al. (2023b), and Gao et al. (2023), as well as in the application models used in our numerical experiments, the lower-level constraints are either linear or linearly coupled and thus consistently satisfy Assumption 3.3(i).\n\n(2) The stationarity measure needs further clarification. The proposed measure is based on (16). However, it is not clear how the stationary points of (16) are related to the original BLO.\n\n$\\textbf{Reply:}$\nTo provide some clarifications:\nFirstly, the equivalence between the constrained problem (6) and the original BLO problem (1) is rigorously discussed in Theorems A.1 and A.2. Additionally, in our study, we utilize the residual function $R_k$, as defined in (15), as the measure of stationarity for the constrained problem (6). This approach is justified as $R_k$ acts as a stationarity measure for the approximate KKT condition of problem (6), as discussed in Andreani et al. (2010). Moreover, $R_k$ is also applicable as a stationarity measure for the penalized problem (16) of problem (6).\n\n(3) The techniques in the theoretical proofs lack novelty, which directly takes advantage of analysis for penalty methods.\n\n$\\textbf{Reply:}$ \nOur work introduces significant technical innovations for several reasons:\n\n(i)  We diverge from traditional methods by proposing a novel single-level equivalent reformulation for constrained BLO problems. This reformulation is based on a newly introduced proximal Lagrangian value function associated with the constrained LL problem.\n\n(ii)  The newly introduced proximal Lagrangian value function is defined as the value function of a strongly-convex-strongly-concave proximal min-max problem. Our study of its continuous differentiability, along with the derivation of its derivative as presented in Lemma A.1, constitutes a novel theoretical contribution.\n\n(iii)  Additionally, our findings regarding the Lipschitz continuity of the saddle point solution to the strongly-convex-strongly-concave proximal min-max problem defining the proximal Lagrangian value function, as detailed in Lemma A.3, represent a significant theoretical advancement.\n\n(iv)  Furthermore, the formulation of the descent lemma for the proximal Lagrangian value function, as elaborated in Lemmas A.2 and A.4, also signifies a theoretical progression.\n\n(v)  The newly established properties of the proximal Lagrangian value function, as mentioned above, play a crucial role in underpinning the non-asymptotic convergence of our proposed single-loop algorithm.\n\n(4) What is the central benefit of using the Moreau envelope instead of the optimal value function, especially in the technical parts?\n\n$\\textbf{Reply:}$\nFirstly, we would like to clarify that our approach is not the Moreau envelope of the lower-level problem as described in Gao et al. (2023). Our new proposed algorithm is based on the concept of the proximal Lagrangian value function.\n\nIn constrained BLO, methods based on value functions face challenges due to the non-differentiability of constraints arising from such value function-based reformulations. This issue arises even when the lower-level problem's data is smooth, as the value function typically lacks differentiability if the solutions or the Lagrangian multipliers of the lower-level problem are not unique.\n\nTo address this challenge, we introduce a new proximal Lagrangian value function tailored for handling constrained LL problem. Utilizing this function, we propose a new single-level reformulation for constrained BLOs, transforming them into equivalent single-level optimization problems with smooth constraints. The proximal Lagrangian value function, defined as the value function of a strongly-convex-strongly-concave min-max problem, exhibits the advantageous property of continuous differentiability.\n\nBuilding on this reformulation, our work develops a Hessian-free gradient-based algorithm for constrained BLOs and provides a non-asymptotic convergence analysis."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3552/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700210251179,
                "cdate": 1700210251179,
                "tmdate": 1700210251179,
                "mdate": 1700210251179,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lUUQ6z6ZCE",
                "forum": "xJ5N8qrEPl",
                "replyto": "xWzHZGnqTy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3552/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3552/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 4Vih"
                    },
                    "comment": {
                        "value": "We respectfully disagree with Reviewer 4Vih's comments regarding the Ethics Concerns, which did not appear in the original version of this reviewer\u2019s Official Review. It should be noted that, as indicated by the system time record, these newly added Ethics Concerns were included in the reviewer\u2019s Official Review only after we had submitted our rebuttal to their initial review. Moreover, the reviewer did not respond to our rebuttal. Consequently, we did not receive any notifications from the system and only became aware of these newly added Ethics Concerns a few hours ago.\n\nBelow, we provide detailed clarifications to address this biased comments.\n\n\n\n(1)   $\\textbf{Differences in Problem Settings and Challenges:}$ \n\nIt's important to note that submissions 3145 and 3552 are tackling two distinct challenges that arise in the development of single-loop Hessian-free algorithms for bi-level optimization (BLO) problems. Specifically, the configurations of the BLO problems, and more specifically, the settings of the lower-level problems examined in submissions 3145 and 3552, are significantly different.\n\nTo be more specific, submission 3145 focuses on BLO with an $\\textbf{unconstrained}$ lower-level problem with potentially $\\textbf{nonconvex}$ objective and a potentially additional $\\textbf{nonsmooth}$ component depending on both upper- and lower-level variables and it is for addressing the nonconvexity and nonsmothness challenges appeared in the lower-level problem of BLO.\n\nIn contrast, submission3552 addressed BLO problems with  $\\textbf{constrained}$ convex lower-level problems.To provide more detail, the lower-level problem is with coupled lower-level constraints, and both the objective and constraint functions of lower-level problem is smooth and convex with respect to the lower-level variable. And submission3552 primarily addressed challenges arising from coupled lower-level constraints, i.e., lower-level $x$-dependent constraints.\n\n\nTo summarize, the differences in problem settings and challenges can be outlined as follows.\n|                  | Submission3552 | Submission3145  |\n| :--------: | :--------: | :-------------: | \n|   $\\textbf{Lower-level problem}$ | $\\min_{y\\in Y} f(x,y) \\ \\mathrm{s.t.}\\  g(x,y)\\leq 0$      |       $\\min_{y\\in Y} \\varphi(x,y):=f(x,y)+g(x,y) $       | \n|   $\\textbf{Annotation}$ | Here $g(x,y)$ is a coupled lower-level constraint      |       Here $g(x,y) $ is the nonsmooth part of the lower-level objective       | \n|   $\\textbf{Challenges}$ | Lower-level $x$-dependent constraints, and non-differentiability in both value function and Moreau envelope-based reformulations      |       nonconvexity and nonsmoothness in LL objective      |"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3552/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740676143,
                "cdate": 1700740676143,
                "tmdate": 1700740676143,
                "mdate": 1700740676143,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "XIVlPNw1Tr",
            "forum": "xJ5N8qrEPl",
            "replyto": "xJ5N8qrEPl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3552/Reviewer_W8M5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3552/Reviewer_W8M5"
            ],
            "content": {
                "summary": {
                    "value": "This paper studied a class of new constrained bilevel optimization, where the lower-level problem involves constraints coupling both upper-level and lower-level variables. Based on the Moreau envelope value function, this paper proposed an efficient single-loop Hessian-free gradient-based algorithm. Moreover, it studied the non-asymptotic convergence analysis for the proposed algorithm. Extensive experimental results verify the efficiency of the proposed algorithm. In summary, the contributions of this paper are significant on the design algorithm and solid theoretical analysis."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "This paper studied a class of new constrained bilevel optimization, where the lower-level problem involves constraints coupling both upper-level and lower-level variables. Based on the Moreau envelope value function, this paper proposed an efficient single-loop Hessian-free gradient-based algorithm. Moreover, it studied the non-asymptotic convergence analysis for the proposed algorithm. Extensive experimental results verify the efficiency of the proposed algorithm. In summary, the contributions of this paper are significant on the design algorithm and solid theoretical analysis."
                },
                "weaknesses": {
                    "value": "It is better to list some bilevel optimization examples in machine learning that have non-smooth and weakly convex lower level functions, which will strength the motivation of this work."
                },
                "questions": {
                    "value": "Some questions:\n\n1)\tIn the proposed LV-HBA algorithm, there exist many hyper parameters such as $\\alpha_k,\\beta_k,\\eta_k,\\gamma,c_k$. Although the authors provided the range of these parameters in the convergence analysis, I still think that the choice of these hyper parameters is not easy in practice. \n\n2)\tIn the experiments, how to choose these hyper parameters including the parameter $r$ in set $Z$ ?\n\n3)\tFrom the Theorem 3.1, the best convergence rata is $O(1/K^{1/4})$ when $p=1/4$ ?\n\n4)\tIt is better to list some bilevel optimization examples in machine learning that have nonsmooth and weakly convex lower level functions, which will strength the motivation of this work."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3552/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698496379160,
            "cdate": 1698496379160,
            "tmdate": 1699636309518,
            "mdate": 1699636309518,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rsMecOSxB7",
                "forum": "xJ5N8qrEPl",
                "replyto": "XIVlPNw1Tr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3552/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3552/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer W8M5. Part 1"
                    },
                    "comment": {
                        "value": "We sincerely appreciate the time you dedicated to reviewing our paper and are grateful for the valuable insights you provided. In the following, we will address each of your questions with thorough consideration.\n\n(1) In the proposed LV-HBA algorithm, there exist many hyper parameters such as $\\alpha_k$, $\\beta_k$, $\\eta_k$, $\\gamma$, $c_k$. Although the authors provided the range of these parameters in the convergence analysis, I still think that the choice of these hyper parameters is not easy in practice.\n\n$\\textbf{Reply:}$\nIn addition to the step size parameters, our method involves three hyperparameters: $\\gamma$, $r$, and $c_k$. The parameters $\\gamma$ and $r$ define the truncated proximal Lagrangian value function. Theoretically, the requirements for $\\gamma$ and $r$ are modest; $\\gamma$ is positive, and $r$ should be large. The penalty parameter, $c_k$, follows a specific selection strategy outlined in Theorem 3.1, namely $c_k = \\underline{c}(k+1)^p$ where $p \\in (0,1/2)$ and $\\underline{c}>0$. Consequently, in practice, typically only the step size parameters require careful tuning.\n\nIn our numerical experiments, we analyzed the sensitivity of the parameter $c_k$, with its convergence curve displayed in Figure 2.\n\nAdditionally, to assess the sensitivity of the remaining parameters, further numerical experiments were conducted on the LL merely convex synthetic model. We record the time when $\\|x^k - x^*\\| / \\|x^*\\| \\leq 10^{-2}$ is met by the iterates generated by LV-HBA. The results are as follows:\n\n|    Strategy     | &nbsp;&emsp;$\u03b1$ |   $\u03b2$   |   $\u03b7$   | $\u03b3=\u03b3_1=\u03b3_2$ | $\\underline{c}$ |  $p$   | $r$    | Time(s) |\n| :-------------: | :-----------: | :---: | :---: | :-----: | :-------------: | :--: | ---- | ------- |\n|        $\u03b1$        |     0.001     | 0.02  | 0.01  |   10    |      0.025      | 0.3  | 1000 | 0.23    |\n|        $\u03b1$        |     0.005     | 0.02  | 0.01  |   10    |      0.025      | 0.3  | 1000 | 0.064   |\n|        $\u03b1$        |     0.001     | 0.02  | 0.01  |   10    |      0.025      | 0.3  | 1000 | 0.052   |\n|        $\u03b2$        |     0.005     | 0.005 | 0.01  |   10    |      0.025      | 0.3  | 1000 | 0.068   |\n|        $\u03b2$        |     0.005     | 0.02  | 0.01  |   10    |      0.025      | 0.3  | 1000 | 0.064   |\n|        $\u03b2$        |     0.005     |  0.1  | 0.01  |   10    |      0.025      | 0.3  | 1000 | 0.063   |\n|        $\u03b7$        |     0.005     | 0.02  | 0.005 |   10    |      0.025      | 0.3  | 1000 | 0.098   |\n|        $\u03b7$        |     0.005     | 0.02  | 0.01  |   10    |      0.025      | 0.3  | 1000 | 0.064   |\n|        $\u03b7$        |     0.005     | 0.02  | 0.05  |   10    |      0.025      | 0.3  | 1000 | 0.025   |\n|        $\u03b3=\u03b3_1=\u03b3_2$        |     0.005     | 0.02  | 0.01  |    5    |      0.025      | 0.3  | 1000 | 0.069   |\n|        $\u03b3=\u03b3_1=\u03b3_2$        |     0.005     | 0.02  | 0.01  |   10    |      0.025      | 0.3  | 1000 | 0.064   |\n|        $\u03b3=\u03b3_1=\u03b3_2$        |     0.005     | 0.02  | 0.01  |   500   |      0.005      | 0.3  | 1000 | 0.064   |\n| $\\underline{c}$ |     0.005     | 0.02  | 0.01  |   10    |      0.005      | 0.3  | 1000 | 0.026   |\n| $\\underline{c}$ |     0.005     | 0.02  | 0.01  |   10    |      0.05       | 0.3  | 1000 | 0.064   |\n| $\\underline{c}$ |     0.005     | 0.02  | 0.01  |   10    |      0.025      | 0.3  | 1000 | 0.086   |\n|        $r$        |     0.005     | 0.02  | 0.01  |   10    |      0.025      | 0.3  | 200  | 0.066   |\n|        $r$        |     0.005     | 0.02  | 0.01  |   10    |      0.025      | 0.3  | 1000 | 0.064   |\n|        $r$        |     0.005     | 0.02  | 0.01  |   10    |      0.025      | 0.3  | 2000 | 0.065   |"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3552/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700210089941,
                "cdate": 1700210089941,
                "tmdate": 1700210089941,
                "mdate": 1700210089941,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Qwy3E355VP",
                "forum": "xJ5N8qrEPl",
                "replyto": "XIVlPNw1Tr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3552/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3552/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer W8M5. Part 2"
                    },
                    "comment": {
                        "value": "(2) In the experiments, how to choose these hyper parameters including the parameter $r$ in set $Z$?\n\n$\\textbf{Reply:}$  \nThank you for your question. The table below outlines the parameter settings for the experiments involving the parameter $r$:\n|      Expriment       | &nbsp;&emsp; $\u03b1$ |   $\u03b2$   |  $\u03b7$   |  $\u03b3=\u03b3_1=\u03b3_2$   |    $r$   | $\\underline{c}$ |  $p$   |\n| :------------------: | :-----------: | :---: | :--: | :--: | :--: | :-------------: | :--: |\n|         LL merely convex          |     0.002     | 0.002 | 0.03 |  10  |  1   |        1        | 0.3  |\n|         LL strongly convex         |     0.01      | 0.01  | 0.05 |  10  | 1000 |        1        | 0.3  |\n|         SVM          |     0.01      |  0.1  | 0.01 |  10  | 200  |       10        | 0.3  |\n| Data Hyper-Cleaning |     0.01      |  0.1  | 0.01 |  10  | 200  |       10        | 0.3  |\n\n\n\n(3) From the Theorem 3.1, the best convergence rata is $O(1/K^{1/4})$ when $p=1/4$?\n\n$\\textbf{Reply:}$ \nYes, you are correct.\n\n(4) It is better to list some bilevel optimization examples in machine learning that have nonsmooth and weakly convex lower level functions, which will strength the motivation of this work.\n\n$\\textbf{Reply:}$\nThank you for your suggestions. Since the $L$-smoothness of $f$ over $X \\times Y$ ensures its weak convexity of $f$ on the same domain, the setting of a weakly convex lower-level objective encompasses all sufficiently smooth applications within bounded $X \\times Y$ in the realm of machine learning."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3552/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700210154811,
                "cdate": 1700210154811,
                "tmdate": 1700210154811,
                "mdate": 1700210154811,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mmtu20cX9L",
                "forum": "xJ5N8qrEPl",
                "replyto": "Qwy3E355VP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3552/Reviewer_W8M5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3552/Reviewer_W8M5"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Authors"
                    },
                    "comment": {
                        "value": "Thanks for your responses. My concerns have been well-addressed, so I keep my score."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3552/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700712082854,
                "cdate": 1700712082854,
                "tmdate": 1700712082854,
                "mdate": 1700712082854,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "EQLckrPBw4",
            "forum": "xJ5N8qrEPl",
            "replyto": "xJ5N8qrEPl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3552/Reviewer_hK2o"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3552/Reviewer_hK2o"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel gradient-based algorithm designed for a class of constrained Bi-Level Optimization (BLO) problems that have coupled lower-level (LL) constraints. In this problem, the lower-level problem takes the form:\n$$\n\\min_y f(x,y), s.t. g(x,y) \\leq 0.\n$$\nIn contrast to existing methods relying on implicit differentiation techniques, this paper introduces a smooth proximal Lagrangian value function for the lower-level problem. This new value function enables the formulation of a single-level reformulation for the constrained BLO. Building upon this reformulation, a single-loop alternating gradient descent method is derived to efficiently tackle the constrained BLO. The paper establishes a non-asymptotic convergence result for the proposed algorithm. Additionally, the paper includes numerical experiments conducted on both illustrative toy examples and practical applications, demonstrating the algorithm's superior practical performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tThis paper is well-organized and in general easy to follow. The assumptions concerning the functions and problem settings are clearly elucidated, facilitating a clear grasp of both the primary concept and the technical intricacies of the proposed approach.\n\n2.\tA noteworthy contribution to the bi-level optimization community is the introduction of the proximal Lagrangian value function and the resulting single-level reformulation for constrained BLOs. This addition holds significant promise for advancing the development of other efficient methods to the constraints BLOs.\n\n3.\tNotably, the proposed method distinguishes itself by its Hessian-free nature, obviating the need for any computations involving the Hessian matrix of the lower-level problem data. Furthermore, its single-loop structure renders it highly implementable and computationally efficient. An appealing aspect of this method is its ability to work without imposing the stringent requirement of strong convexity on the lower-level problem. Moreover, it exhibits versatility by accommodating scenarios in which the lower-level problem possesses multiple solutions, expanding its applicability to a broad spectrum of practical applications.\n\n5.\tThe paper extensively explores the properties of the newly introduced proximal Lagrangian value function and the associated reformulation. Additionally, a non-asymptotic convergence analysis is included, further substantiating the contribution and the validity of the proposed approach."
                },
                "weaknesses": {
                    "value": "1.\tIn Table 1, to the best of my knowledge, BVFSM does not require LL objective to be convex.\n\n2.\tThe authors have not included some recent works that investigate algorithms for addressing constrained Bi-Level Optimization (BLO) problems through the value function approach, such as:\n\nFliege, J., Tin, A., & Zemkoho, A. (2021). Gauss\u2013Newton-type methods for bilevel optimization. Computational Optimization and Applications, 78(3), 793-824.\n\nFischer, A., Zemkoho, A. B., & Zhou, S. (2022). Semismooth Newton-type method for bilevel optimization: global convergence and extensive numerical experiments. Optimization Methods and Software, 37(5), 1770-1804.\n\n3.\tOn page 8,  $\\|y^k-y^*(x)\\|$ should be revised to `` $\\left\\|y^k-y^*(x^k)\\right\\| $ \"?\n\n4.\tWhen the constraints of the LL problem are absent, i.e., when $g = 0$, does the introduced proximal Lagrangian value function revert to the classical Moreau envelope function of the LL objective? Furthermore, is there any relationship between the proximal Lagrangian value function and the augmented Lagrangian function of the LL problem?"
                },
                "questions": {
                    "value": "I think this work provides some good contributions to constrained bilevel optimization, and hence I tend to accept it. See my questions in the weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3552/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3552/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3552/Reviewer_hK2o"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3552/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698692431548,
            "cdate": 1698692431548,
            "tmdate": 1699636309440,
            "mdate": 1699636309440,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rfBHptfEny",
                "forum": "xJ5N8qrEPl",
                "replyto": "EQLckrPBw4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3552/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3552/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer hK2o"
                    },
                    "comment": {
                        "value": "We sincerely appreciate the time and effort you dedicated to reviewing our paper and are grateful for your constructive comments. Below, we will respond to each of the questions you raised.\n\n(1) In Table 1, to the best of my knowledge, BVFSM does not require LL objective to be convex.\n\n$\\textbf{Reply:}$ \nThank you for your observation. You are correct that BVFSM does not require the LL objective to be convex. We appreciate your comment and will clarify this point in a future revision of the paper.\n\n\n(2) The authors have not included some recent works that investigate algorithms for addressing constrained Bi-Level Optimization (BLO) problems through the value function approach, such as:\n\nFliege, J., Tin, A., & Zemkoho, A. (2021). Gauss\u2013Newton-type methods for bilevel optimization. Computational Optimization and Applications, 78(3), 793-824.\n\nFischer, A., Zemkoho, A. B., & Zhou, S. (2022). Semismooth Newton-type method for bilevel optimization: global convergence and extensive numerical experiments. Optimization Methods and Software, 37(5), 1770-1804.\n\n$\\textbf{Reply:}$ \nThank you for sharing these recent works with us. We'll take them into account when revising our manuscript.\n\n\n(3) On page 8, $|y^k - y^*(x)|$ should be revised to $|y^k - y^*(x^k)|$?\n\n$\\textbf{Reply:}$ \nThank you for pointing out the typo. We'll correct it in the revised version.\n\n(4) When the constraints of the LL problem are absent, i.e., when $g=0$, does the introduced proximal Lagrangian value function revert to the classical Moreau envelope function of the LL objective? Furthermore, is there any relationship between the proximal Lagrangian value function and the augmented Lagrangian function of the LL problem?\n\n$\\textbf{Reply:}$\nThank you for your interesting questions. \n\nFirstly, your observation is accurate: the proximal Lagrangian value function introduced in our study simplifies to the classical Moreau envelope function of the lower-level objective in the absence of LL problem constraints. Regarding the connection between this proximal Lagrangian value function and the augmented Lagrangian function of the LL problem, this indeed presents an intriguing avenue for exploration. We intend to investigate this relationship in our future research endeavors."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3552/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700210040941,
                "cdate": 1700210040941,
                "tmdate": 1700210040941,
                "mdate": 1700210040941,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zvhcqmflCs",
            "forum": "xJ5N8qrEPl",
            "replyto": "xJ5N8qrEPl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3552/Reviewer_ttDx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3552/Reviewer_ttDx"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a new single-loop Hessian-free algorithm for the solving Bi-Level Optimization (BLO) problems. \n\nIt first creates a smooth proximal Lagrangian value function, effectively addressing the constrained lower-level problem. Subsequently, the authors present a single-level reformulation for constrained BLOs, converting the original BLO problem into an equivalent optimization problem with smooth constraints.\n\nThe paper includes a non-asymptotic convergence analysis for the proposed algorithm. Some experiments have been conducted to show the superior practical performance of the algorithm"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "S1. This paper proposes the first single-loop Hessian-free algorithm for solving the BLO problem.\nS2. The authors introduce a new potential function, which is associated with the monotonically decreasing step size. Furthermore, they demonstrate how to select these step sizes to ensure the potential function exhibits the sufficient descent property.\nS3. The authors have conducted experiments on five machine learning tasks to validate the performance of their proposed methods."
                },
                "weaknesses": {
                    "value": "W1. There are an excessive number of stepsize parameters in use, and it remains uncertain whether the algorithm's performance is significantly affected by the choice of these parameters.\n\nW2. While other bilevel optimization algorithms employ techniques such as Nesterov's momentum or utilize high-order information of the objective function, the plain and simple gradient descent/ascent algorithm may be slow in practical applications.\n\nW3. Assumption 3.3 (ii) can be quite stringent since the global Lipschitz constant for both $L_{g_1}$ and $L_{g_2}$ have not been determined in advance for many applications.\n\nW4. The choice for the parameter $r$ for the truncated proximal Lagrangian value function is not mentioned."
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3552/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3552/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3552/Reviewer_ttDx"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3552/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698730305112,
            "cdate": 1698730305112,
            "tmdate": 1699636309351,
            "mdate": 1699636309351,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rdDqB0n1pZ",
                "forum": "xJ5N8qrEPl",
                "replyto": "zvhcqmflCs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3552/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3552/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ttDx. Part 1"
                    },
                    "comment": {
                        "value": "We sincerely appreciate the time and effort you devoted to reviewing our paper. Your valuable feedback is greatly appreciated, and we would like to clarify some specific points in response.\n\n(1) There are an excessive number of stepsize parameters in use, and it remains uncertain whether the algorithm's performance is significantly affected by the choice of these parameters.\n\n$\\textbf{Reply:}$\nIn addition to the step size parameters, our method involves three hyperparameters: $\\gamma$, $r$, and $c_k$. The parameters $\\gamma$ and $r$ define the truncated proximal Lagrangian value function. Theoretically, the requirements for $\\gamma$ and $r$ are modest; $\\gamma$ is positive, and $r$ should be large. The penalty parameter, $c_k$, follows a specific selection strategy outlined in Theorem 3.1, namely $c_k = \\underline{c}(k+1)^p$ where $p \\in (0,1/2)$ and $\\underline{c}>0$. Consequently, in practice, typically only the step size parameters require careful tuning.\n\nIn our numerical experiments, we analyzed the sensitivity of the parameter $c_k$, with its convergence curve displayed in Figure 2.\n\nAdditionally, to assess the sensitivity of the remaining parameters, further numerical experiments were conducted on the LL merely convex synthetic model. We record the time when $\\|x^k - x^*\\| / \\|x^*\\| \\leq 10^{-2}$ is met by the iterates generated by LV-HBA. The results are as follows:\n\n|    Strategy     | &nbsp;&emsp;$\u03b1$ |   $\u03b2$   |   $\u03b7$   | $\u03b3=\u03b3_1=\u03b3_2$ | $\\underline{c}$ |  $p$   | $r$    | Time(s) |\n| :-------------: | :-----------: | :---: | :---: | :-----: | :-------------: | :--: | ---- | ------- |\n|        $\u03b1$        |     0.001     | 0.02  | 0.01  |   10    |      0.025      | 0.3  | 1000 | 0.23    |\n|        $\u03b1$        |     0.005     | 0.02  | 0.01  |   10    |      0.025      | 0.3  | 1000 | 0.064   |\n|        $\u03b1$        |     0.001     | 0.02  | 0.01  |   10    |      0.025      | 0.3  | 1000 | 0.052   |\n|        $\u03b2$        |     0.005     | 0.005 | 0.01  |   10    |      0.025      | 0.3  | 1000 | 0.068   |\n|        $\u03b2$        |     0.005     | 0.02  | 0.01  |   10    |      0.025      | 0.3  | 1000 | 0.064   |\n|        $\u03b2$        |     0.005     |  0.1  | 0.01  |   10    |      0.025      | 0.3  | 1000 | 0.063   |\n|        $\u03b7$        |     0.005     | 0.02  | 0.005 |   10    |      0.025      | 0.3  | 1000 | 0.098   |\n|        $\u03b7$        |     0.005     | 0.02  | 0.01  |   10    |      0.025      | 0.3  | 1000 | 0.064   |\n|        $\u03b7$        |     0.005     | 0.02  | 0.05  |   10    |      0.025      | 0.3  | 1000 | 0.025   |\n|        $\u03b3=\u03b3_1=\u03b3_2$        |     0.005     | 0.02  | 0.01  |    5    |      0.025      | 0.3  | 1000 | 0.069   |\n|        $\u03b3=\u03b3_1=\u03b3_2$        |     0.005     | 0.02  | 0.01  |   10    |      0.025      | 0.3  | 1000 | 0.064   |\n|        $\u03b3=\u03b3_1=\u03b3_2$        |     0.005     | 0.02  | 0.01  |   500   |      0.005      | 0.3  | 1000 | 0.064   |\n| $\\underline{c}$ |     0.005     | 0.02  | 0.01  |   10    |      0.005      | 0.3  | 1000 | 0.026   |\n| $\\underline{c}$ |     0.005     | 0.02  | 0.01  |   10    |      0.05       | 0.3  | 1000 | 0.064   |\n| $\\underline{c}$ |     0.005     | 0.02  | 0.01  |   10    |      0.025      | 0.3  | 1000 | 0.086   |\n|        $r$        |     0.005     | 0.02  | 0.01  |   10    |      0.025      | 0.3  | 200  | 0.066   |\n|        $r$        |     0.005     | 0.02  | 0.01  |   10    |      0.025      | 0.3  | 1000 | 0.064   |\n|        $r$        |     0.005     | 0.02  | 0.01  |   10    |      0.025      | 0.3  | 2000 | 0.065   |"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3552/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700209641781,
                "cdate": 1700209641781,
                "tmdate": 1700209845165,
                "mdate": 1700209845165,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PAvG56t9Wi",
                "forum": "xJ5N8qrEPl",
                "replyto": "zvhcqmflCs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3552/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3552/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ttDx. Part 2"
                    },
                    "comment": {
                        "value": "(2) While other bilevel optimization algorithms employ techniques such as Nesterov's momentum or utilize high-order information of the objective function, the plain and simple gradient descent/ascent algorithm may be slow in practical applications.\n\n$\\textbf{Reply:}$\nIn the numerical experiments section, we compare our method against two competitors: AiPOD and E-AiPOD (Xiao et al., 2023b), as well as GAM (Xu & Zhu, 2023). Both of these are bilevel optimization algorithms that leverage the Hessian information from the lower-level objective. Our results demonstrate a significant advantage of our method: it relies solely on gradient information of the lower-level problem. This approach not only simplifies the computational process but also notably enhances the speed, rendering our method considerably faster than the competing algorithms.\n\n(3) Assumption 3.3 (ii) can be quite stringent since the global Lipschitz constant for both\u00a0$L_{g_1}$\u00a0and\u00a0$L_{g_2}$\u00a0have not been determined in advance for many applications.\n\n$\\textbf{Reply:}$\nWe appreciate your feedback and understand your concern about Assumption 3.3 (ii). Here are some clarifications:\n\n(i) The Lipschitz continuity of the gradient of the lower-level constraints in Assumption 3.3 (ii) is a common and standard assumption in the context of constrained bi-level optimization. This assumption is similarly employed in recent studies, such as those by Khanduri et al. (2023) and Xiao et al. (2023b).\n\n(ii) Importantly, in practical applications considered in the experiments conducted in Khanduri et al. (2023) and Xiao et al. (2023b), including scenarios like Adversarially Robust Learning and Federated Bilevel Learning, the lower-level constraints are either linear or linearly coupled. This observation holds true for all models assessed in our numerical experiments. In such cases, the global Lipschitz constants for both $L_{g_1}$ and $L_{g_2}$ are equal to zero.\n\n(4) The choice for the parameter $r$ for the truncated proximal Lagrangian value function is not mentioned.\n\n$\\textbf{Reply:}$  \nAs demonstrated in Theorem A.2, that given a solution $(x^*, y^*, z^*)$ to the reformulation (4), selecting the parameter $r$ to be sufficiently large, specifically $r \\ge \\|z^*\\|_{\\infty}$, ensures that $(x^*, y^*, z^*)$ also solves variant (6), Therefore, it is standard to choose a large value for the parameter $r$ in practice."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3552/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700209982876,
                "cdate": 1700209982876,
                "tmdate": 1700209982876,
                "mdate": 1700209982876,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4WI5PMrPzv",
            "forum": "xJ5N8qrEPl",
            "replyto": "xJ5N8qrEPl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3552/Reviewer_7k2H"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3552/Reviewer_7k2H"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a single loop proximal Lagrangian Value function-based Hessian-free Bi-level Algorithm (LV-HBA) for solving constrained bilevel optimization where the lower-level problem involves constraints coupling both upper-level and lower-level variables.\nThe authors relax the strongly convex assumption on lower-level problem to the general convex case, and provide non-asymptotic convergence analysis for LV-HBA. They also provide numerical experiments on the synthetic problem, hyperparameter selection problem and data hypercleaning task."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is well-written. The authors propose a single loop Hessian free method that we refer to as Lagrangian Value function-based Hessian-free Bi-level Algorithm (LV-HBA). The authors provide non-asymptotic convergence analysis for LV-HBA, relaxing the underlying assumptions for lower level problem from strongly convexity to only convexity."
                },
                "weaknesses": {
                    "value": "Please see questions."
                },
                "questions": {
                    "value": "1. Can the authors elaborate why there is no need to assume the Lipschitz continuity of the upper level function $F(x,y)$, which is typically necessary in bilevel optimization even the lower function is strongly convex, e.g. Xiao et al. (2023b).\n2. On page 6, the sentences after Assumption 3.2 say $f$ is $\\rho_f$-weakly convex on $X\\times Y$ with $\\rho_f\\ge 0$, which is potentially being smaller than $L_f$. Can you provide the exact form of $\\rho_f$ or show the case of $\\rho_f$ being smaller for some specific application? Does that mean we can only suppose $\\rho_f = L_f$ in general?\n3. Theorem A.1 says reformulation (4) is equivalent to constrained BLO problem (1). In the proof of Theorem A.1, it only illustrates that the feasible points of (4) and (1) are equivalent. It is unclear why this means the formulations (4) and (1) and are also equivalent. Can we conclude the optimal solution of (4) and (1) are equivalent?\n4. In reformulation (5), how to choose the parameter $r$?\n5. The proposed methods includes many hyperparameters, making difficult to implement in practice."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3552/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3552/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3552/Reviewer_7k2H"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3552/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699697247123,
            "cdate": 1699697247123,
            "tmdate": 1699697247123,
            "mdate": 1699697247123,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "roPaKyepCn",
                "forum": "xJ5N8qrEPl",
                "replyto": "4WI5PMrPzv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3552/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3552/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 7k2H. Part 1"
                    },
                    "comment": {
                        "value": "We appreciate your time and effort in reviewing our work and providing constructive feedback. Below, we will address each of your questions with careful consideration.\n\n(1) Can the authors elaborate why there is no need to assume the Lipschitz continuity of the upper level function $F(x,y)$, which is typically necessary in bilevel optimization even the lower function is strongly convex, e.g. Xiao et al. (2023b).\n\n$\\textbf{Reply:}$\nOur approach diverges fundamentally from previous algorithms, such as the one proposed by Xiao et al. (2023b). Specifically, Xiao et al. assume the strong convexity of the lower-level objective with respect to $y$, leading to a unique solution for the lower-level problem, denoted as $y^*(x)$. Their algorithm is designed based on the hyper-gradient of the hyper-objective $F(x, y^*(x))$. The Lipschitz continuity of the upper-level objective $F(x, y)$ is utilized to establish the $L$-smoothness of the hyper-objective.\n\nIn contrast, our work does not presume strong convexity of the lower-level objective with respect to $y$. This allows for the possibility of multiple lower-level solutions, and consequently, the hyper-objective framework used by Xiao et al. is inapplicable. Instead of relying on the hyper-objective, our paper introduces an innovative single-level reformulation for constrained bilevel optimization. This reformulation translates the problem into an equivalent single-level optimization problem with a smooth constraint. We then develop a gradient-type method to address this reformulation. Consequently, our theoretical analysis requires only that $F(x,y)$ be $L_F$-smooth, as detailed in Assumption 3.1, without necessitating the Lipschitz continuity of $F(x, y)$.\n\n(2) On page 6, the sentences after Assumption 3.2 say $f$ is $\\rho_f$-weakly convex on $X\\times Y$ with $\\rho_f\\geq0$, which is potentially being smaller than $L_f$. Can you provide the exact form of  or show the case of $\\rho_f$ being smaller for some specific application? Does that mean we can only suppose $\\rho_f=L_f$ in general?\n\n$\\textbf{Reply:}$ \nA typical instance occurs when $f$ is convex over the domain $X \\times Y$. In such scenarios, $\\rho_f$ equals $0$, which is less than $L_f$. This property appears in bilevel optimization problems related to hyperparameter selection, where $f$ often exhibits convexity. For detailed examples, refer to the following works:\n\nLucy L Gao, Jane J. Ye, Haian Yin, Shangzhi Zeng, and Jin Zhang. Value function based difference-of-convex algorithm for bilevel hyperparameter selection problems. In ICML, 2022.\n\nJane J. Ye, Xiaoming Yuan, Shangzhi Zeng, and Jin Zhang. Difference of convex algorithms for bilevel programs with applications in hyperparameter selection. Mathematical Programming, 198(2):1583\u20131616, 2023.\n\n\n(3) Theorem A.1 says reformulation (4) is equivalent to constrained BLO problem (1). In the proof of Theorem A.1, it only illustrates that the feasible points of (4) and (1) are equivalent. It is unclear why this means the formulations (4) and (1) and are also equivalent. Can we conclude the optimal solution of (4) and (1) are equivalent?\n\n$\\textbf{Reply:}$ \nIn addition to the equivalence of feasible points between formulation (4) and (1), they share identical objectives, denoted as $F(x,y)$. Consequently, both (4) and (1) have identical optimal solutions, applicable in both global and local contexts.\n\n(4) In reformulation (5), how to choose the parameter $r$?\n\n$\\textbf{Reply:}$ \nAs demonstrated in Theorem A.2, that given a solution $(x^*, y^*, z^*)$ to the reformulation (4), selecting the parameter $r$ to be sufficiently large, specifically $r \\ge \\|z^*\\|_{\\infty}$, ensures that $(x^*, y^*, z^*)$ also solves variant (6). Therefore, it is standard to choose a large value for the parameter $r$ in practice."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3552/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700209380924,
                "cdate": 1700209380924,
                "tmdate": 1700209800052,
                "mdate": 1700209800052,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kJgs5OF3D7",
                "forum": "xJ5N8qrEPl",
                "replyto": "4WI5PMrPzv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3552/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3552/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 7k2H. Part 2"
                    },
                    "comment": {
                        "value": "(5) The proposed methods includes many hyperparameters, making difficult to implement in practice.\n\n$\\textbf{Reply:}$ \nWe appreciate your feedback and would like to clarify a few points:\n\nIn addition to the step size parameters, our method involves three hyperparameters: $\\gamma$, $r$, and $c_k$. The parameters $\\gamma$ and $r$ define the truncated proximal Lagrangian value function. Theoretically, the requirements for $\\gamma$ and $r$ are modest; $\\gamma$ is positive, and $r$ should be large. The penalty parameter, $c_k$, follows a specific selection strategy outlined in Theorem 3.1, namely $c_k = \\underline{c}(k+1)^p$ where $p \\in (0,1/2)$ and $\\underline{c}>0$. Consequently, in practice, typically only the step size parameters require careful tuning.\n\nIn our numerical experiments, we analyzed the sensitivity of the parameter $c_k$, with its convergence curve displayed in Figure 2.\n\nAdditionally, to assess the sensitivity of the remaining parameters, further numerical experiments were conducted on the LL merely convex synthetic model. We record the time when $\\|x^k - x^*\\| / \\|x^*\\| \\leq 10^{-2}$ is met by the iterates generated by LV-HBA. The results are as follows:\n\n|    Strategy     | &nbsp;&emsp;$\u03b1$ |   $\u03b2$   |   $\u03b7$   | $\u03b3=\u03b3_1=\u03b3_2$ | $\\underline{c}$ |  $p$   | $r$    | Time(s) |\n| :-------------: | :-----------: | :---: | :---: | :-----: | :-------------: | :--: | ---- | ------- |\n|        $\u03b1$        |     0.001     | 0.02  | 0.01  |   10    |      0.025      | 0.3  | 1000 | 0.23    |\n|        $\u03b1$        |     0.005     | 0.02  | 0.01  |   10    |      0.025      | 0.3  | 1000 | 0.064   |\n|        $\u03b1$        |     0.001     | 0.02  | 0.01  |   10    |      0.025      | 0.3  | 1000 | 0.052   |\n|        $\u03b2$        |     0.005     | 0.005 | 0.01  |   10    |      0.025      | 0.3  | 1000 | 0.068   |\n|        $\u03b2$        |     0.005     | 0.02  | 0.01  |   10    |      0.025      | 0.3  | 1000 | 0.064   |\n|        $\u03b2$        |     0.005     |  0.1  | 0.01  |   10    |      0.025      | 0.3  | 1000 | 0.063   |\n|        $\u03b7$        |     0.005     | 0.02  | 0.005 |   10    |      0.025      | 0.3  | 1000 | 0.098   |\n|        $\u03b7$        |     0.005     | 0.02  | 0.01  |   10    |      0.025      | 0.3  | 1000 | 0.064   |\n|        $\u03b7$        |     0.005     | 0.02  | 0.05  |   10    |      0.025      | 0.3  | 1000 | 0.025   |\n|        $\u03b3=\u03b3_1=\u03b3_2$        |     0.005     | 0.02  | 0.01  |    5    |      0.025      | 0.3  | 1000 | 0.069   |\n|        $\u03b3=\u03b3_1=\u03b3_2$        |     0.005     | 0.02  | 0.01  |   10    |      0.025      | 0.3  | 1000 | 0.064   |\n|        $\u03b3=\u03b3_1=\u03b3_2$        |     0.005     | 0.02  | 0.01  |   500   |      0.005      | 0.3  | 1000 | 0.064   |\n| $\\underline{c}$ |     0.005     | 0.02  | 0.01  |   10    |      0.005      | 0.3  | 1000 | 0.026   |\n| $\\underline{c}$ |     0.005     | 0.02  | 0.01  |   10    |      0.05       | 0.3  | 1000 | 0.064   |\n| $\\underline{c}$ |     0.005     | 0.02  | 0.01  |   10    |      0.025      | 0.3  | 1000 | 0.086   |\n|        $r$        |     0.005     | 0.02  | 0.01  |   10    |      0.025      | 0.3  | 200  | 0.066   |\n|        $r$        |     0.005     | 0.02  | 0.01  |   10    |      0.025      | 0.3  | 1000 | 0.064   |\n|        $r$        |     0.005     | 0.02  | 0.01  |   10    |      0.025      | 0.3  | 2000 | 0.065   |"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3552/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700209495770,
                "cdate": 1700209495770,
                "tmdate": 1700209811482,
                "mdate": 1700209811482,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]