[
    {
        "title": "Actions-to-Action: Inductive Attention for Egocentric Video Action Anticipation"
    },
    {
        "review": {
            "id": "0OfNlU5JG1",
            "forum": "dl34rOnbqJ",
            "replyto": "dl34rOnbqJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6336/Reviewer_Uc4h"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6336/Reviewer_Uc4h"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a model for action anticipation based on the integration of attention-based (such as transformers) and auto recurrent (such as LSTM) mechanisms. Differently from previous works, the proposed method takes into account a history of previous hidden states rather than the last one when making predictions, thus relaxing the first-order Markovian assumption usually introduced in recurrent models. The proposed approach is evaluated on the main benchmarks for egocentric action anticipation. Results suggest that the method outperforms competitors when a single RGB modality is considered."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "While the model proposes a few modifications to the attention mechanism, this ends up in a novel approach which outperforms previous works.\n\nIt is interesting to see that the proposed method works well with context length of up to 30s. This is not common in previous approaches and seems to be a promising direction for better exploiting past history."
                },
                "weaknesses": {
                    "value": "1) PRESENTATION QUALITY\nThe quality of paper writing is not always up to standard. Some sentences are a bit overselling, unclear, or not adequate for scientific writing. Some examples:\n- \u201cby integrating gaze information within the observed frames\u201d: is the paper referring to a specific work here? As far I know gaze analysis for intention prediction has not been systematically investigated in egocentric vision.\n- \u201cUnlike action recognition, which primarily relies on patter recognition\u201d: this statement seems to imply that action anticipation does not rely on pattern recognition, which I don\u2019t think is an accurate statement (neural networks are anyway patter recognition machines)\n- \u201cour model can infer causation from abstract concepts\u201d: this statement is a bit overselling and does not seem to be shown/proved in any of the experiments. \n- \u201cour innovative model sets new performance benchmarks\u201d: I do not think this is accurate. It seems that the proposed model outperforms competitors by small margins. I would highlight instead that it may point out to a promising direction for future models.\n- in section 3, the set X is later referred to $X_{T-t_s:t}$, which is a bit confusing\n- It is not clear how equation (1) is an accurate description of existing methods, whether recurrent or transformer-based. In the equation, it seems that models following this formulation explicitly plug in the last observed action for predicting the future one. However, methods that directly predict future action do not do that (e.g., vanilla LSTMs)\n- In eq (2), is $\\hat y_i$ a probability or a predicted label?\n- Section 5.3.1 \u201cIAM demonstrated notable e enhancement over several formidable baselines\u201d.I would suggest to revise the use of \u201cformidable\u201d in this scientific context.\n- Throughout the paper, I could not find a clear motivation for the use of the term \u201cinductive\u201d in \u201cinductive attention\u201d. I think this could be clarified.\n\nThis are some examples. Overall, I would suggest a thorough review of the paper to improve presentation.\n\nLITTLE INSIGHT ON WHY THE MODEL WORKS\nWhile I appreciate the description of the model architecture in Eq (11)-(14), there is no discussion as to why the introduced modifications are adequate and what kind of processing they could be intuitively bring to the model. After reading the description, I felt there is little insight into why the attention mechanism is tweaked the way it is. Also, while ablation studies detail the weigh of each macro-component to performance (Table 6), it would have been interesting to see a more detailed ablation into the various modification introduced by each of the aforementioned equations with respect to a baseline attention architecture.\n\nMULTI-MODALITY\nThe proposed algorithm outperforms competitors when a single modality is considered, while some approaches outperform the proposed method in the presence of multiple modalities. It would have been interesting to see how the proposed approach does when multiple modalities are considered, even with a simple late fusion. This would shed some light into the generalizability of the approach to the use of signals other than RGB images."
                },
                "questions": {
                    "value": "I found the paper overall interesting, but I think the quality of presentation is somewhat limited. Also, there is little insight into why the proposed modifications work.\n\nThe authors could clarify this latter aspect in the rebuttal, while the quality improvements can only be done in the camera ready, if the paper is accepted."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6336/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698560301266,
            "cdate": 1698560301266,
            "tmdate": 1699636697729,
            "mdate": 1699636697729,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ckdWtekqoY",
                "forum": "dl34rOnbqJ",
                "replyto": "0OfNlU5JG1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6336/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6336/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> PRESENTATION QUALITY The quality of paper writing is not always up to standard. Some sentences are a bit overselling, unclear, or not adequate for scientific writing. Some examples:\n\n> \u201cby integrating gaze information within the observed frames\u201d: is the paper referring to a specific work here? As far I know gaze analysis for intention prediction has not been systematically investigated in egocentric vision.\n\nWe have revised the following sentences: \u201cFurthermore, video action anticipation frequently employs egocentric videos, which not only harmonize the perspectives from diverse subjects but also implicitly unveil their intentions by integrating gaze information within the observed frames\u201d to \u201cFurthermore, video action anticipation frequently employs egocentric videos, which harmonize perspectives from diverse subjects and implicitly unveil their intentions. This is achieved by integrating elements such as coarse-grained visual attention, indicated by the camera\u2019s heading direction, within the observed frames.\u201c\n\n> \u201cUnlike action recognition, which primarily relies on patter recognition\u201d: this statement seems to imply that action anticipation does not rely on pattern recognition, which I don\u2019t think is an accurate statement (neural networks are anyway patter recognition machines)\n\nWe agree reviewer\u2019s insightful comment. We revised the original sentences \u201cUnlike action recognition, which primarily relies on pattern recognition, video action anticipation explores the complex nature of potential future actions, each with multiple possibilities.\u201d to \u201cWhile action recognition focuses on classifying current actions through pattern recognition, video action anticipation uses these patterns to predict the complex nature of potential future actions, each with multiple possibilities.\u201d\n\n\n> \u201cour model can infer causation from abstract concepts\u201d: this statement is a bit overselling and does not seem to be shown/proved in any of the experiments.\n\nWe have down-tone our sentences and rewritten \u201cMimicking human reasoning, our model can infer causation from abstract concepts without relying on finer details like pixels.\u201d to \u201cOur model detects video patterns to predict future actions, using past conditions to enhance understanding of complex interactions, focusing more on overall patterns than on specific details like pixels.\u201d\n\n\n> \u201cour innovative model sets new performance benchmarks\u201d: I do not think this is accurate. It seems that the proposed model outperforms competitors by small margins. I would highlight instead that it may point out to a promising direction for future models.\n\nWe revised the original sentences from \u201cExperimental results and in-depth analyses confirm that our innovative model sets new performance benchmarks, while also competing effectively in both model size and computational efficiency.\u201d to \u201cExperimental results and in-depth analyses indicate that our model achieves competitive performance gains and suggests a promising direction for future models in the field. Additionally, it maintains an effective balance in model size and computational efficiency.\u201c\n\n\n> in section 3, the set $X$ is later referred to $X_{T-t_s:t}, which is a bit confusing\n\nTo improve the readability, we revised the sentence to \u201cat every timestep $t$, the model uses the observed input up to time $t$, denoted as $X_{*:t}$, to predict the subsequent action\u201d\n\n> It is not clear how equation (1) is an accurate description of existing methods, whether recurrent or transformer-based. In the equation, it seems that models following this formulation explicitly plug in the last observed action for predicting the future one. However, methods that directly predict future action do not do that (e.g., vanilla LSTMs)\n\nWe revise the equation (1) to be more general and should be much clearer to highlight our design is to add the explicit feedback from the classifier of past timestep and utilize it in the attention mechanism.\n\n\n> In eq (2), is $\\hat{y_i} a probability or a predicted label?\n\nWe apologize for the confusion. Both $y_\\*$ and $\\hat{y_{\\*}}$ represent probabilities. However, $y_*$ is the ground-truth label, where only the top-1 action is annotated, thus the probability distribution can be considered a Dirac distribution. We have revised all the sentences which mentioned $y_*$ or $\\hat{y}_*$ to a probability rather than described as a label.\n\n\n> Section 5.3.1 \u201cIAM demonstrated notable e enhancement over several formidable baselines\u201d.I would suggest to revise the use of \u201cformidable\u201d in this scientific context.\n\nWe thank the reviewer's suggestion. We have revised by using \u201cwell-established\u201d rather than \u201cformidable\u201d."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6336/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738875241,
                "cdate": 1700738875241,
                "tmdate": 1700738875241,
                "mdate": 1700738875241,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Q2extkr4fY",
                "forum": "dl34rOnbqJ",
                "replyto": "0OfNlU5JG1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6336/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6336/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Throughout the paper, I could not find a clear motivation for the use of the term \u201cinductive\u201d in \u201cinductive attention\u201d. I think this could be clarified.\n\nWe appreciate your observation regarding the term \"inductive\" used in the context of our \"inductive attention\" mechanism. The choice of this term was intended to reflect the specific way in which our model processes and learns from video data.\n\nIn our model, \"inductive\" refers to the process of making generalized predictions about future actions based on specific observed patterns and sequences within the video data. This is akin to the inductive reasoning process in human cognition, where conclusions or generalizations are drawn from specific instances or observations.\n\nOur model learns to anticipate future actions by identifying and generalizing patterns observed in the training data. It extrapolates from these specific instances to make broader predictions, a process that mirrors the inductive reasoning used in human learning and decision-making. In addition, the term \"inductive\" in the context of attention mechanism signifies that the model selectively focuses on certain aspects of the input data (observed actions and interactions) to inductively predict future actions. This involves not just recognizing patterns but also using these patterns to infer what actions are likely to occur next.\n\nBy using the term \"inductive,\" we aim to emphasize the model's ability to generalize from observed data to predict future actions, a key aspect that differentiates it from other attention-based models which may focus more on direct pattern matching or deductive reasoning processes.\n\n---\n\n> LITTLE INSIGHT ON WHY THE MODEL WORKS While I appreciate the description of the model architecture in Eq (11)-(14), there is no discussion as to why the introduced modifications are adequate and what kind of processing they could be intuitively bring to the model. After reading the description, I felt there is little insight into why the attention mechanism is tweaked the way it is. Also, while ablation studies detail the weigh of each macro-component to performance (Table 6), it would have been interesting to see a more detailed ablation into the various modification introduced by each of the aforementioned equations with respect to a baseline attention architecture.\n\nWe thank the reviewers for their interest in our inductive attention model. We have revised Section 3 of our manuscript to provide a clearer overview of our proposal. The inductive attention is designed to utilize previous predictions as prior input for the next timestep. By doing so, we first (1) extend our recurrent model to a higher order, explicitly forming a context window during each recurrent update; (2) utilize attention to aggregate the higher order, assigning the query (Q) with the previous prediction. Correspondingly, the keys (K) are also in the action space (obtained from the prediction of that time) to represent each past recurrent state. All these steps are mentioned in Tables 6 and 7. In the revised version, we separated parts of the original Table 6 into Table 7 for a more comprehensive view.\n\n--- \n\n> MULTI-MODALITY The proposed algorithm outperforms competitors when a single modality is considered, while some approaches outperform the proposed method in the presence of multiple modalities. It would have been interesting to see how the proposed approach does when multiple modalities are considered, even with a simple late fusion. This would shed some light into the generalizability of the approach to the use of signals other than RGB images.\n\nWe thank the reviewer for their interest in the multi-modality aspect of our research. Multi-modality is indeed a promising direction for our future work and warrants further exploration. Our proposed method is flexible enough to consider multi-modality in each recurrent update. For instance, the inductive attention can utilize different action structures (such as verbs or nouns) to select various parts of the recurrent state and modality.\n\nIn this work, we primarily focus on RGB and demonstrate that our model, relying solely on RGB video, can outperform most current methods with more efficient modeling. Additionally, Tables 1 and 2 include single-modal variants of the multi-modal baseline. For example, AVT+/AVT++ and AFFT/AFFT+ are compared, and our method surpasses their single-modal configurations."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6336/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738980814,
                "cdate": 1700738980814,
                "tmdate": 1700738980814,
                "mdate": 1700738980814,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cKvz1sdow3",
            "forum": "dl34rOnbqJ",
            "replyto": "dl34rOnbqJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6336/Reviewer_nTno"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6336/Reviewer_nTno"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces the \"Inductive Attention\" mechanism, an approach to video action anticipation. Unique in its design, this method employs the class prediction from the prior step as the query for attention. The authors argue that this design allows the model to recognize many-to-many relationships more effectively. Experimental evidence demonstrates that the Inductive Attention model achieves state-of-the-art results on several large-scale datasets, highlighting its efficacy in predicting human actions within video content."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The paper reports commendable performance on benchmark datasets.\n2. The idea of utilizing the prediction from the previous step as the attention query offers a fresh perspective and holds intrinsic interest."
                },
                "weaknesses": {
                    "value": "The core contribution of the paper revolves around leveraging the prior prediction as an attention query. It is natural to use the input frame feature, or hidden state as a query, but if using the previous prediction leads to significantly better performance, it would be noteworthy and might have wider applications in related fields. However, the current version of the paper lacks depth in discussing the implications and rationale behind this choice. The proposed method, as presented, may seem like an incremental architectural tweak that provides some improvement in a particular task, significantly limiting its impact. For the authors' assertion that \"class probability is a superior choice for attention\" to be compelling, it requires a more rigorous justification than what is currently provided. Merely pointing out performance gains does not substantiate this claim sufficiently."
                },
                "questions": {
                    "value": "Please see the weaknesses section. Is there any experimental evidence that can show the potential expandability of the proposed method to other related CV tasks?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6336/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6336/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6336/Reviewer_nTno"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6336/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698653852095,
            "cdate": 1698653852095,
            "tmdate": 1699636697619,
            "mdate": 1699636697619,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3iUTlQxuAH",
                "forum": "dl34rOnbqJ",
                "replyto": "cKvz1sdow3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6336/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6336/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your insightful comment. Our choice to leverage prior predictions as an attention query stems from the hypothesis that recent action outcomes are indicative of imminent actions in a sequential context. This approach aligns with cognitive theories suggesting that humans use recent experiences to anticipate future actions. By incorporating class probabilities from prior predictions, our model mimics this aspect of human cognition, leading to a more generalizability (see Table 2 for the testing results) and contextually aware anticipation mechanism.\n\nWe conducted experiments to validate our approach. Specifically, we compared the performance of models using hidden states and prior predictions, as now shown in Table 7 in the supplementary material (which separated from the Table 6) of revised manuscript, demonstrate a consistent improvement when using prior predictions, affirming our hypothesis.\n\nWhile the architectural modification might seem incremental, its implications are substantial. The improved accuracy and contextual awareness achieved through this method can significantly enhance the performance of models in various domains, marking a notable advancement in the field of predictive analytics.\n\nWe hope these additional details and justifications address your concerns and illustrate the depth and impact of our contribution more convincingly.\n\n> Please see the weaknesses section. Is there any experimental evidence that can show the potential expandability of the proposed method to other related CV tasks?\n\nThank you for your inquiry regarding the expandability of our proposed method to other computer vision tasks. We understand the importance of demonstrating the versatility and applicability of our approach beyond the specific scope of egocentric video action anticipation.\n\nIn our current study, the primary focus has been on advancing the state of action anticipation in egocentric videos. While we have not conducted experiments directly applying our method to other CV tasks, the underlying principles of our approach hold significant promise for broader applications.\n\nOur method's ability to effectively integrate temporal context and leverage inductive attention mechanisms is not limited to egocentric videos and can be beneficial in other areas where understanding and predicting sequential patterns is crucial. For example, the principles underlying our method could enhance the accuracy of predicting object trajectories and interactions over time in dynamic environments. \n\nOur approach could also potentially be adapted to improve scene understanding tasks, especially in scenarios that require anticipation of future states or changes.\n\nIn addition, the temporal modeling capabilities of our method make it suitable for applications in gesture and activity recognition, where predicting the progression of actions is key.\n\nWe acknowledge the value of experimental evidence in substantiating the adaptability of our method. \nTherefore, as part of our future work, we plan to explore and conduct experiments applying our approach to these and other related computer vision tasks.\n\nThese future experiments will aim to demonstrate the method's efficacy in different contexts, further validating its potential as a versatile tool in the field of computer vision.\n\nIn summary, while direct experimental evidence for the expandability of our method to other CV tasks is currently limited, the foundational aspects of our approach are conducive to such applications. We are committed to exploring this potential in our future research, aiming to provide empirical evidence for the method's broader applicability."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6336/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738542334,
                "cdate": 1700738542334,
                "tmdate": 1700738542334,
                "mdate": 1700738542334,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "V6Tzel4mev",
            "forum": "dl34rOnbqJ",
            "replyto": "dl34rOnbqJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6336/Reviewer_omT8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6336/Reviewer_omT8"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents an approach for egocentric action anticipation by introducing an inductive attention module. This module is helpful in capturing longer, temporal, history information and resolves the forgetting nature of recurrent neural data by using a higher order information. For the inductive attention, the last/previous prediction is compressed using a learnable compression function and used as query. The before/history predictions are also compressed and used as key and the history recurrent states are used as value. The frame feature and the inductive attention value are aggregated together to form the current recurrent state. The method is evaluated on three datasets - Epic-kitchens-100, Epic-kitchens-55, EGTEA Gaze+"
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The quantitative results are quite exhaustive and the results are shown on three egocentric datasets for action anticipation."
                },
                "weaknesses": {
                    "value": "It seems that the technical contribution of the work is weak. While the motivation of adding longer, temporal history is appreciated, the inductive attention module itself does not yield much improvement for the task of anticipation and does not provide a strong signal to the model for modelling the long-term, temporal history context. There seems to be less significant improvement in the performance with the inductive attention mechanism module. For example, in Table 1, when comparing Swin-IAM with MeMViT 32x3 there is only a performance improvement of 0.4% on actions, almost none on verbs, and 0.2% on nouns."
                },
                "questions": {
                    "value": "Suggestion: \n1. There can be a grammar check run on the paper text. For example, the first line of section 3, problem statement can be edited. Additionally, multi-modal and multi-model terms have been used interchangeably in abstract and results table. The last line of the abstract can also be checked - 'multi-modality models using only RGB visual inputs' whereas multi-modal approaches have more modalities than visual input. \n\n2. The association with object tracking in the introduction and Figure 1 also seems a bit misplaced as it seems for the reader that object tracking would be used for the task of action anticipation, which is not actually used."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6336/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6336/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6336/Reviewer_omT8"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6336/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698723624197,
            "cdate": 1698723624197,
            "tmdate": 1699636697488,
            "mdate": 1699636697488,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fsu4KPZuvY",
                "forum": "dl34rOnbqJ",
                "replyto": "V6Tzel4mev",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6336/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6336/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your observations regarding the technical contributions of our work and the impact of the inductive attention module. In action anticipation, particularly with complex video datasets, even modest improvements in performance metrics can be significant. The field's challenges stem from the need to process and predict actions based on dynamic and often ambiguous visual data. In this context, each fraction of a percentage increase in accuracy can substantially impact the model's overall effectiveness.\n\nThe inductive attention mechanism, a central innovation of our work, demonstrates a nuanced approach to incorporating action histories. It not only achieved better test scores (as seen in Table 2) but also required fewer model parameters, highlighting its efficiency. Additionally, it provides the most influential priors for each model prediction, enhancing the model's decision-making transparency.\n\nWhile the quantitative improvements shown in Table 1 might seem modest (0.4% on actions, almost none on verbs, and 0.2% on nouns), these gains are significant in this task's context. In a highly competitive field where models already perform at high levels, these incremental improvements underscore our approach's effectiveness in refining the model's predictive capabilities.\n\n\n> 1. There can be a grammar check run on the paper text. For example, the first line of section 3, problem statement can be edited. Additionally, multi-modal and multi-model terms have been used interchangeably in abstract and results table. The last line of the abstract can also be checked - 'multi-modality models using only RGB visual inputs' whereas multi-modal approaches have more modalities than visual input.\n\nThank you for pointing out the grammatical issues and the inconsistency in the use of terms in our manuscript. We have carefully revised the first sentence of Section 3 to ensure it clearly and accurately conveys our intended message. The revised sentence now reads \u201cVideo action anticipation is defined as a task where an arbitrary length, $t_s$, of video inputs $X = \\{x_i \\in \\R^{C \\times H \\times W} | i = T-t_s, \\dots, T\\}$, starting from timestep $T-t_s$ to $T$, is used to predict the target action that happens in the future, $\\tau_a$ seconds later.\u201d\n\nRegarding the last line of the abstract, we have revised it to accurately reflect the nature of our models. The revised line now reads: \u201c...our proposed model surpasses most multi-modal models by only using RGB visual inputs\u2026\u201d. This change clarifies that our models, while primarily utilizing RGB visual inputs, result in competitive performance to the models using more modalities.\n\n---\n\n> 2. The association with object tracking in the introduction and Figure 1 also seems a bit misplaced as it seems for the reader that object tracking would be used for the task of action anticipation, which is not actually used.\n\nWe apologize for any confusion caused. Figure 1 is intended to illustrate the underlying similarity between action anticipation and object tracking. In object tracking, the previous bounding box location is used as a prior to predict the next bounding box. Similarly, action anticipation can benefit from feeding back past action predictions and conditioning them to the target action. This concept forms the main motivation for our inductive attention mechanism."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6336/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738448255,
                "cdate": 1700738448255,
                "tmdate": 1700738448255,
                "mdate": 1700738448255,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FGEkeoitOK",
            "forum": "dl34rOnbqJ",
            "replyto": "dl34rOnbqJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6336/Reviewer_gdBN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6336/Reviewer_gdBN"
            ],
            "content": {
                "summary": {
                    "value": "Paper proposes a novel method which utilizes multiple S past actions anticipation results to improve the next action anticipation. Extensive experiments on 3 datasets and several analytic experiments on proposed model performance were conducted. Empirical results show competitive results against other State of The Arts approaches."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Paper's motivations are clear and the proposed method is explained in sufficient details.\n2. Quality of experiment designs and analysis are excellent.\n3. Novelty/originality is good within the context of action anticipation."
                },
                "weaknesses": {
                    "value": "1. Originality is somewhat limited as using prior predictions to condition the target prediction has been applied to other problems. See reference\n2. The choice of egocentric action anticipation problem for the proposed method is not well motivated. There is no inherent advantage of the proposed method for egocentric action anticipation, compared to other video prediction problems, e.g. physical interaction/dynamics, 3PV action anticipation, gaze anticipation/prediction etc.\n3. Significance is average. While the problem of action anticipation is interesting, it is not clear how the proposed method can be applied to other related problems.\n\nReferences\nDuan, J., Yu, S., Poria, S., Wen, B., & Tan, C. (2022, October). PIP: Physical Interaction Prediction via Mental Simulation with Span Selection. In European Conference on Computer Vision (pp. 405-421). Cham: Springer Nature Switzerland.\n\nVincent Le Guen and Nicolas Thome, \u201cDisentangling physical dynamics from unknown factors for unsupervised video prediction,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 11474\u201311484.\n\nCarl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Anticipating visual representations from unlabeled video. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 98\u2013106, 2016.\n\nZhang, M., Teck Ma, K., Hwee Lim, J., Zhao, Q., & Feng, J. (2017). Deep future gaze: Gaze anticipation on egocentric videos using adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 4372-4381)."
                },
                "questions": {
                    "value": "1. Please explain the motivation for applying the proposed technique to egocentric videos only. There is no clear reason why the proposed method cannot be applied to other video prediction tasks, e.g. 3rd person videos, physics interactions, gaze prediction etc.\n\n2. I will be interested to see the results comparison for S=1."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No concerns."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6336/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698765161861,
            "cdate": 1698765161861,
            "tmdate": 1699636697350,
            "mdate": 1699636697350,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "veNteOyFEg",
                "forum": "dl34rOnbqJ",
                "replyto": "FGEkeoitOK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6336/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6336/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> 1. Originality is somewhat limited as using prior predictions to condition the target prediction has been applied to other problems. See reference\n\nThank you for pointing out the existing applications where prior predictions are used to condition target predictions. We acknowledge that this concept is not entirely new in the broader field of machine learning and has indeed been explored in various contexts. However, our work distinguishes itself in several key aspects, particularly in the domain of egocentric video anticipation.\n\nFirst Application in Egocentric Video Anticipation: To the best of our knowledge, ours is the first study to apply the concept of using prior predictions to condition target predictions specifically to the egocentric video anticipation problem. This application is novel in this context and addresses unique challenges inherent to egocentric video analysis.\n\nInnovative Model Design: Our paper introduces a novel model that synergistically combines higher-order recurrent and attention mechanisms. This integration is specifically tailored to leverage the dynamics of egocentric videos, a domain where temporal context and rapid shifts in the visual field present unique challenges.\n\nImproved Generalization and Efficiency: The results from our experiments demonstrate that utilizing prior predictions in our specific model architecture not only enhances generalization across different scenarios but also leads to a more efficient model design. This efficiency is quantifiable in terms of the reduced number of model parameters required, which is a significant advancement in resource-constrained environments.\n\nEmpirical Evidence (see Figure 3 and new Figure 7 and 8 in the supplementary material): We provide empirical evidence to support our claims, for example the topmost positive and negative prior learned by the model, showcasing how our approach outperforms existing methods in terms of accuracy and efficiency, particularly in the context of egocentric video anticipation.\n\nIn conclusion, while the foundational concept of using prior predictions may have precedents in other domains, our application of this idea to egocentric video anticipation, coupled with our novel model design, represents a significant original contribution to the field. We believe our approach sets a new precedent in this specific area and opens up avenues for further innovative research."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6336/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738248098,
                "cdate": 1700738248098,
                "tmdate": 1700738248098,
                "mdate": 1700738248098,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qAGV0uNOvE",
                "forum": "dl34rOnbqJ",
                "replyto": "FGEkeoitOK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6336/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6336/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> 2. The choice of egocentric action anticipation problem for the proposed method is not well motivated. There is no inherent advantage of the proposed method for egocentric action anticipation, compared to other video prediction problems, e.g. physical interaction/dynamics, 3PV action anticipation, gaze anticipation/prediction etc.\n\nThank you for your query regarding the specific focus on the egocentric action anticipation problem in our study. The choice of this domain was driven by both the unique challenges it presents and the specific advantages our proposed method offers in addressing these challenges.\nEgocentric action anticipation involves predicting future actions from a first-person perspective, which is fundamentally different from third-person views (3PV) or other video prediction tasks. This perspective is characterized by rapid, unpredictable motion, a high degree of camera wearer's interaction with the environment, and a need for understanding subtle cues from the wearer's viewpoint.\n\n1. Advantages of Our Method in Egocentric Context:\n\n- Handling Perspective Shifts: Our model's integration of higher-order recurrent and attention mechanisms is particularly effective in managing the perspective shifts typical in egocentric videos. This perspective shift can be unintentional (i.e., distraction) or intentional (relevant to the target action), and learned form the data.\n\n- Leveraging Temporal Context: The egocentric video stream provides a rich temporal context that our model exploits, using prior predictions to inform future anticipations. This is particularly beneficial in first-person videos where sequential actions are closely linked.\nEfficient Processing of Dense Interactions: Egocentric videos often contain dense interactions with objects and environments. Our model's efficient design allows it to process these interactions without the need for extensive computational resources.\n\n2. Comparative Advantage over Other Domains:\n\n- Physical Interaction/Dynamics: While our model could potentially be applied to physical interaction dynamics, the first-person perspective offers a more direct and immediate view of interactions, making it a more challenging and thus insightful testbed for our method.\n3PV Action Anticipation: Third-person views present different challenges and often lack the immediacy and intimacy of interaction present in egocentric videos, which our method is specifically designed to handle.\n- Gaze Anticipation/Prediction: Although gaze anticipation is related, it often focuses on predicting the focal point of attention rather than the broader spectrum of actions, which is the focus of our model.\nIn summary, our decision to focus on the egocentric action anticipation problem was motivated by the unique challenges it presents and the suitability of our proposed method in addressing these challenges. We believe that the insights gained from this domain can be instrumental in advancing the broader field of video action anticipation.\n\n--- \n\n> 3. Significance is average. While the problem of action anticipation is interesting, it is not clear how the proposed method can be applied to other related problems.\n\nThe field of video action anticipation involves predicting target actions with limited observed information. Our method demonstrates efficiency in addressing this challenge. The core idea is to utilize historical predictions of previous actions as a prior, conditioning the input. While our proposed model can be adapted to other related problems sharing similar properties, some tailored modifications may be necessary. For instance, gaze prediction might lean more towards regression than classification. Additionally, third-person view (3PV) action anticipation could overlook critical features such as the subject's intention."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6336/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738304290,
                "cdate": 1700738304290,
                "tmdate": 1700738304290,
                "mdate": 1700738304290,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1gxixxrCAI",
                "forum": "dl34rOnbqJ",
                "replyto": "FGEkeoitOK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6336/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6336/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> 1. Please explain the motivation for applying the proposed technique to egocentric videos only. There is no clear reason why the proposed method cannot be applied to other video prediction tasks, e.g. 3rd person videos, physics interactions, gaze prediction etc.\n\nThe field of video action anticipation involves predicting target actions with limited observed information. Our method demonstrates efficiency in addressing this challenge. The core idea is to utilize historical predictions of previous actions as a prior, conditioning the input. While our proposed model can be adapted to other related problems sharing similar properties, some tailored modifications may be necessary. For instance, gaze prediction might lean more towards regression than classification. Additionally, third-person view (3PV) action anticipation could overlook critical features such as the subject's intention.\n\n--- \n\n> 2. I will be interested to see the results comparison for S=1.\n\nWe have updated Table 7 (originally is part of table 6) in our manuscript to include these results, thereby providing a more comprehensive view of the performance impact when the inductive attention component is not utilized.\n\nIn the absence of inductive attention, which is a combination of higher-order processing and the use of predictions as priors, we observed the overall action score drop of 1.3%/1.0%/1.1% decreased in the different backbone configuration in terms of $S=1$.\n\nThese results highlight the significant contribution of the inductive attention mechanism to our model's performance. The reduction in action scores when inductive attention is removed underscores its effectiveness, particularly in enhancing the model's ability to accurately anticipate actions in the S=1 setting.\n\nThis detailed analysis strengthens our assertion that the inductive attention mechanism, with its integration of higher-order processing and predictive priors, is a crucial component of our model, contributing substantially to its overall efficacy in action anticipation tasks."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6336/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738360527,
                "cdate": 1700738360527,
                "tmdate": 1700738360527,
                "mdate": 1700738360527,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "A7QPUqPDLS",
            "forum": "dl34rOnbqJ",
            "replyto": "dl34rOnbqJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6336/Reviewer_2chi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6336/Reviewer_2chi"
            ],
            "content": {
                "summary": {
                    "value": "The paper produces an Inductive Attention Model (IAM) for egocentric video action anticipation. The model melds recurrent and attention\nmechanisms to explicitly employ prior anticipation results to refine subsequent action predictions. This design allows the model to form higher-order recurrent states and make current predictions based on extended historical data. Experiment results on several action anticipation datasets show that the proposed model surpasses most multi-modality models using only RGB visual inputs, showing the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed IAM architecture utilizes prior predictions as part of the attention mechanism. This allows for the aggregation of higher-order recurrent states, which is an advancement over traditional first-order recurrent models.\n2. IAM achieves competitive performance on several datasets with relatively fewer parameters.\n3. IAM achieves better performance on unseen classes on EK100, indicating good generalizability."
                },
                "weaknesses": {
                    "value": "1. In Table 6, the ablation study shows that one of the major designs of this paper (predictions as prior) doesn't play a major role in the final performance improvement. The performance improvement is highly attributed to some design choices like a better backbone and class weighting.\n2.  Lack of analysis and visualization of the proposed mechanism. For example, how do previous predictions affect future action anticipation? \n3. Is the proposed model able to also handle the long-term action anticipation tasks (i.e. predicting multiple future actions) defined in Ego4D (grauman2022ego4d) and EgoTOPO (nagarajan2020ego)."
                },
                "questions": {
                    "value": "See weakness section"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6336/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699224554625,
            "cdate": 1699224554625,
            "tmdate": 1699636697234,
            "mdate": 1699636697234,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Uo6TfhzW0h",
                "forum": "dl34rOnbqJ",
                "replyto": "A7QPUqPDLS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6336/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6336/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> 1. In Table 6, the ablation study shows that one of the major designs of this paper (predictions as prior) doesn't play a major role in the final performance improvement. The performance improvement is highly attributed to some design choices like a better backbone and class weighting.\n\nThank you for your observation regarding the ablation study in Table 6 (in the revised manuscript table 6 and 7). Our paper introduces an innovative inductive attention model, fundamentally integrating higher-order recurrent and attention mechanisms with a focus on using predictions as priors (serve as attention query and keys). It's important to recognize these elements as integral to our algorithm's design and performance.\n\nClass weighting is indeed widely adopted for the long-tailed distributed dataset such as EPIC-Kitchens, as carried out in MeMViT (Wu et al. 2022). While it falls outside the primary scope of our research, it undeniably enhances our method by ensuring a more equitable exposure of different categories within the training set.\n\nTo provide a clearer picture of our method's effectiveness, we have revised Table 6 with the following enhancements: (1) Inclusion of Unseen and Tail Scores: Alongside overall scores, this allows for a more comprehensive evaluation (2) Distinct Presentation of Different Backbones: Ensuring that the impact of backbone choice is transparent and understandable. (3) Aggregation of Components: We've combined \u201cPrior predictions serve as attention query (Q) and keys (K) \u201d and \u201cExtend to higher-order (S=1 to S=30)\u201d of the table 7 into a single \u201cInductive Attention\u201d category in the table 6. This change reflects the interdependence of these features, as inductive attention cannot function in isolation.\n\nIt's important to note that all improvements in our model build upon the results achieved through class weighting. Thus, directly comparing the incremental improvements of our method with those attributed to class weighting is not straightforward, as the enhancements are not linearly additive.\n\nFinally, we emphasize that our model not only shows significant improvements over previous works in terms of overall, unseen, and tail action scores but also achieves this with considerably fewer model parameters. This aspect underscores the efficiency and innovation of our approach.\n\n---\n\n> 2. Lack of analysis and visualization of the proposed mechanism. For example, how do previous predictions affect future action anticipation?\n\nThank you for highlighting the need for a deeper analysis and visualization of our proposed inductive attention mechanism, especially regarding the influence of previous predictions on future action anticipation. We have included new evidence in Figure 3, and Figure 7 and 8 in the supplementary material, showing the topmost positive and negative previous prediction entry inductive attention referenced related to the final prediction. It is important to mention that without the previous predictions as prior, the model can only base on the recurrent hidden states $h_t$ and as shown in the Table 6, resulting in -1.3%, -1.0%, -1.1% absolute overall action scores drop.\n\nWe believe these additions will greatly enhance the understanding of our proposed mechanism and its efficacy in improving action anticipation in egocentric video analysis.\n\n---\n> 3. Is the proposed model able to also handle the long-term action anticipation tasks (i.e. predicting multiple future actions) defined in Ego4D (grauman2022ego4d) and EgoTOPO (nagarajan2020ego).\n\nThank you for your question regarding the applicability of our proposed model to long-term action anticipation tasks, as exemplified in datasets like Ego4D (Grauman et al., 2022) and EgoTOPO (Nagarajan et al., 2020).\n\nIn our current work, the focus has been predominantly on short-term action anticipation with a temporal anticipation window $\\tau_a$ set to 1s/0.5s. This decision was based on a well-established evaluation protocol of the dataset design of EPIC-Kitchens/EGTEA Gaze+ evaluation protocols from previous works.\n\nLong-term action anticipation, especially in datasets like Ego4D, introduces additional challenges. These include dealing with more extended temporal dependencies and the need for a broader contextual understanding to accurately predict a sequence of future actions.\nWe recognize the importance and potential of extending our model to these long-term tasks. As such, we have earmarked this as a key area for future research. Our planned extensions will involve adapting and enhancing the current model to handle longer temporal sequences and more complex action interdependencies characteristic of datasets like Ego4D and EgoTOPO."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6336/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738101953,
                "cdate": 1700738101953,
                "tmdate": 1700738101953,
                "mdate": 1700738101953,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]