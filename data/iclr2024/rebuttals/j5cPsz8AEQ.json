[
    {
        "title": "Physics-informed neural networks with unknown measurement noise"
    },
    {
        "review": {
            "id": "PhlaqXJeLV",
            "forum": "j5cPsz8AEQ",
            "replyto": "j5cPsz8AEQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2522/Reviewer_99Tt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2522/Reviewer_99Tt"
            ],
            "content": {
                "summary": {
                    "value": "The authors introduce a new training procedure for PINNs which are adapted to unknown measurement noise, i.e., a training procedure which works for any noise model. This is done via EBMs, which are trained jointly with the PINN. Here the EBMs estimate a 1d noise model based on the estimation of the PINN (conditional to the point $t_i$). Since they only estimate a 1d distribution, the (usually intractable) normalization constant can be estimated via numerical integration. The approach is tested on several (partial) differential equations and benchmarked against the standard PINN."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is easy to follow (except a few minor points). The idea is interesting and well-executed. The approach outperforms the standard PINN and offset PINN baseline. The experiments are well described, so that I think reproduction should be easy."
                },
                "weaknesses": {
                    "value": "1) While the idea is heuristically clear, it would be interesting whether one can obtain theoretical guarantees. I have got the hunch that it should be possible to cast the framework into one of expectation maximization (EM) algorithms (maybe one slightly needs to change the loss and train alternating instead of jointly). Did the authors give this some thought? This would greatly strengthen the paper in my opinion. For this see e.g. [1]\n\n2) The discussion in 4.1 and 4.2 is a bit confusing. While I think I got the gist of it, please make clear what variables the functions $\\mu_{\\varepsilon}$ and $\\theta_0$ depend on. \n\n3) The metric logL is not clearly defined. How is that calculated in the case of a standard PINN, just Gaussian likelihood?\n\n4) The non-Gaussian noise is a GMM. I would like to see physically more realistic noise models. One thing that could be interesting is whether this approach is able to learn mixed Gaussian noise, i.e., $y = f(t) + \\eta_1 + f(t)\\ \\eta_2$ for normal $\\eta_1,\\eta_2$ with some variances. While this is still Gaussian, this is a noise model used in practice. \n\n5) Please make the relation to model errors [2] and [3] more clear. Although the model error framework tries to solve a different problem (Bayesian inversion) the ideas are somewhat similar.\n\n6) A very similar is to train a surrogate on the data only (no PINN loss), then estimate the noise via an appropriate model, such as an EBM and then to train the surrogate on a combined loss. Please comment on this. \n\n[1] DeepGEM: Generalized Expectation-Maximization for Blind Inversion, Gao et al\n\n[2] Iterative Updating of Model Error for Bayesian Inversion, Calvetti et al\n\n[3] Noise-aware physics-informed machine learning\nfor robust PDE discovery, Thanasutives et al"
                },
                "questions": {
                    "value": "See weaknesses. I overall like the idea and think it has a lot of merit. A consideration of more realistic noise models and some theoretical guarantees would strenghten the article imo."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2522/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2522/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2522/Reviewer_99Tt"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2522/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697714851680,
            "cdate": 1697714851680,
            "tmdate": 1699636188807,
            "mdate": 1699636188807,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "maFespFvSo",
                "forum": "j5cPsz8AEQ",
                "replyto": "PhlaqXJeLV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2522/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2522/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the careful evaluation of our paper and the suggested ways of improving the paper.\n\nTo comment on Questions/Weaknesses:\n\n1) This is an interesting suggestion and we will look into it.\n\n2) The variable mu_epsilon denotes the mean of the noise distribution. theta_0 denotes a learnable parameter which is supposed to learn mu_epsilon. In the revised version, we will clarify this.\n\n3) Indeed, in the case of the standard PINN a Gaussian likelihood is employed to estimate the log-likelihood; the parameters of this Gaussian are estimated from the residuals. We will also clarify this in the revised version.\n\n4) While our method would not allow for this kind of noise in its current form  due to the f(t)*eta_2 term, it should be possible to extend it in a suitable way.\n\n5) We thank the reviewer for bringing these works to our attention. We will consider them in the revised version.\n\n6) We would expect the proposed approach to perform worse than PINN-EBM (although probably better than the standard PINN). The reason for this is that a standard neural network (without  the additional regularization provided by the PINN loss) will likely overfit to the noise and in turn yield a worse noise estimate. Furthermore, the offset would still need to be learned jointly with the PINN in the last step, since we have no way of knowing the offset of the learned noise distribution without the PINN-loss."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2522/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699970479371,
                "cdate": 1699970479371,
                "tmdate": 1699970479371,
                "mdate": 1699970479371,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pGDGwplkYb",
                "forum": "j5cPsz8AEQ",
                "replyto": "maFespFvSo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2522/Reviewer_99Tt"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2522/Reviewer_99Tt"
                ],
                "content": {
                    "title": {
                        "value": "Quick Question"
                    },
                    "comment": {
                        "value": "Thanks for the response.\n\nRegarding 4) can you explain why your method is not applicable to this setting? I thought the EBM should also be able to learn these kinds of noise models and the EBM is allowed to depend on t?"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2522/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699972692216,
                "cdate": 1699972692216,
                "tmdate": 1699972692216,
                "mdate": 1699972692216,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aeYJaDYyLN",
                "forum": "j5cPsz8AEQ",
                "replyto": "5KPp88bimt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2522/Reviewer_99Tt"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2522/Reviewer_99Tt"
                ],
                "content": {
                    "title": {
                        "value": "Post rebuttal"
                    },
                    "comment": {
                        "value": "Overall my opinion of the paper stays the same. I found it to be a good read with room for improvements on the theoretical side as well as more realistic examples."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2522/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700651177084,
                "cdate": 1700651177084,
                "tmdate": 1700651177084,
                "mdate": 1700651177084,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WZTwW4Qmbw",
            "forum": "j5cPsz8AEQ",
            "replyto": "j5cPsz8AEQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2522/Reviewer_4nHF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2522/Reviewer_4nHF"
            ],
            "content": {
                "summary": {
                    "value": "This article proposes a method for training physics informed neural networks (PINNs) when the distribution of measurement noise is unknown. The key idea is to learn noise distribution using an energy-based model on top of training of PINNs. A few numerical experiments show the usefulness of the proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The usefulness of the method is shown by numerical experiments for a few example problems."
                },
                "weaknesses": {
                    "value": "There is little theoretical backing. Extension to high-dimensional and/or non-iid noises would require much heavier computation. Experiments are limited only to synthetic problems."
                },
                "questions": {
                    "value": "Are there any practical problems that could be resolved by the proposed method?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2522/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2522/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2522/Reviewer_4nHF"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2522/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698459150455,
            "cdate": 1698459150455,
            "tmdate": 1699636188703,
            "mdate": 1699636188703,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wysImHiJ5i",
                "forum": "j5cPsz8AEQ",
                "replyto": "WZTwW4Qmbw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2522/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2522/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the evaluation of our paper.\n\nTo comment on the stated weaknesses:\n\n*) We would like to point out that we provide theoretical justification for our choice of loss function.\n\n*) While it is correct that we evaluate our method on snythetic data, this is still the case for most PINN research.\n\nTo address the question:\n\n*) In the last paragraph of Section 1, we speculate where practical applications of our work may be found."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2522/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699970456304,
                "cdate": 1699970456304,
                "tmdate": 1699970456304,
                "mdate": 1699970456304,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "i3pHJhdYdE",
                "forum": "j5cPsz8AEQ",
                "replyto": "wysImHiJ5i",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2522/Reviewer_4nHF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2522/Reviewer_4nHF"
                ],
                "content": {
                    "comment": {
                        "value": "> *) We would like to point out that we provide theoretical justification for our choice of loss function.\nSorry, I could not find \"theoretical justification\". For instance, is there any theoretical guarantee for convergence by the choice of loss function?"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2522/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700546313854,
                "cdate": 1700546313854,
                "tmdate": 1700546313854,
                "mdate": 1700546313854,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4QVsY67WcE",
                "forum": "j5cPsz8AEQ",
                "replyto": "WZTwW4Qmbw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2522/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2522/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We were referring to the discussion in Sections 4.1-4.3, in which we provide justification for our choice of loss function. The main argument is that for our choice of loss function, both terms have the same minimizer in the limit of infinite data. We do not have theoretical guarantees for the convergence."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2522/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700609251891,
                "cdate": 1700609251891,
                "tmdate": 1700609306827,
                "mdate": 1700609306827,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iZBa5ndTth",
                "forum": "j5cPsz8AEQ",
                "replyto": "4QVsY67WcE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2522/Reviewer_4nHF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2522/Reviewer_4nHF"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the explanation. Then, I keep my current score."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2522/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634778380,
                "cdate": 1700634778380,
                "tmdate": 1700634778380,
                "mdate": 1700634778380,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QtKCcuiIf9",
            "forum": "j5cPsz8AEQ",
            "replyto": "j5cPsz8AEQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2522/Reviewer_PpGy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2522/Reviewer_PpGy"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes the integration of an energy-based model (EBM) to learn the distribution of the noise that is added to samples in a dataset to be modeled by a physics-informed neural network (PINN). A joint loss function is used to train the EBM and the PINN, whereas the EBM can be trained at the same time or with a delayed start with respect to the PINN. Numerical experiments use synthetic data governed by several well-known PDEs from physics, polluted with a variety of noise distributions, to test the performance of the proposed approach."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The approach is principled, the description is clear, the results are convincing."
                },
                "weaknesses": {
                    "value": "The proposed approach integrates two well-known models from the literature; the approach is straightforward and the results are not surprising. EBMs have been used before in classification, generative modeling, and regression problems; the authors state that the novelty is in the leveraging of physical knowledge within PINNs. In addition, all the results are focused on synthetic data. Thus the impact of the proposed approach appears limited to the current combination of tools for the usual applications of PINNs.\n\nMinor comments:\n\nIn Algorithm 1, within the training loop, i should be updated."
                },
                "questions": {
                    "value": "To better evaluate the impact of the proposed approach, it would be good to discuss the following questions:\n\n(1) How is the formulation of the proposed approach different from the integration of EBM to a regular neural network?\n\n(2) Is there real-world data that would usually be modeled by a PINN where non-Gaussian additive noise is present and for which the proposed approach can be shown to provide better solutions than the baseline PINN?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2522/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698612383691,
            "cdate": 1698612383691,
            "tmdate": 1699636188620,
            "mdate": 1699636188620,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CAVxOXCcKK",
                "forum": "j5cPsz8AEQ",
                "replyto": "QtKCcuiIf9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2522/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2522/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the evaluation of our paper.\n\nTo answer the questions:\n(1)\nThe main differences of combining PINN vs a normal neural network with EBM are the following:\n\nfirstly, the PINN loss acts as and additional means of regularization and suppresses overfitting to the noise. The regular neural network, on the other hand, has no knowledge of the PDE and may overfit strongly to noise in the data. The EBM would in turn also learn an incorrect noise distribution that assigns too much weight to smaller noise values.\n\nSecondly, the PINN loss allows for the identification of the noise offset, which would not be possible without the requirement that the solution also adhere to the PDE.\n\n(2)\nIn the last paragraph of Section 1, we speculate where practical applications of our work may be found. At the moment we are not aware of a good open source dataset to evaluate our method on."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2522/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699970433236,
                "cdate": 1699970433236,
                "tmdate": 1699970433236,
                "mdate": 1699970433236,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ChZIvtHMjU",
                "forum": "j5cPsz8AEQ",
                "replyto": "CAVxOXCcKK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2522/Reviewer_PpGy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2522/Reviewer_PpGy"
                ],
                "content": {
                    "title": {
                        "value": "Regarding question 1"
                    },
                    "comment": {
                        "value": "I appreciate the answers. For question 1, you are describing the differences between the PINN and a normal neural network. I would like to know more about differences in the integration of the PINN or NN with the EBM - what is the added contribution for  the integration you are proposing? Does the noise offset identification from PINN+EBM not occur from NN+EBM? Can you illustrate?"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2522/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700582423489,
                "cdate": 1700582423489,
                "tmdate": 1700582423489,
                "mdate": 1700582423489,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "EEXkz6RbSW",
            "forum": "j5cPsz8AEQ",
            "replyto": "j5cPsz8AEQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2522/Reviewer_pToC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2522/Reviewer_pToC"
            ],
            "content": {
                "summary": {
                    "value": "the paper propose a method to handle measurement noise that has non-zero bias (Eq.7) and algorithm 1. the paper is indeed very hard to read. I would suggest the authors rewrite the paper to allow readers to understand and therefore use this paper for the progress of science. then resubmit the paper in the next conference. I will explain why the paper is hard to read in the next section."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "A learning method to handle more sophisticated measurement noise."
                },
                "weaknesses": {
                    "value": "I try to help the authors by explaining why the paper is hard to read to me. I hope these feedback can help improve the writing for a future paper.\n\n1. math symbols are not defined when they are first used. examples:\n\n1a. page3, line 3, D_d = {d_d, y_d}. these symbols are not explained and define. y_d was explained only towards end of page 3.\n\n1b. page3, line 3, what is \"d\"? is this the index of the data point? furthermore D_d is just a set with two elements. how to learn from a set of two elements?\n\n1c. what is the math object of y_d? is it \\mathbb{R}^m or \\mathbb{R}? t_d \\in \\mathbb{R}? what is \\lambda and what dimension is it?\n\n1d. Eq2. t_c, how to get the colocation points?\n\n1e. algorithm 1, \"if i<i_ebm then\", what is i?\n\n2. page3 second paragraph. I read this paragraph many times, I still cannot understand it. this paragraph needs to be expanded and writing needs to be clear.\n\noverall the math formulation needs to be improved a lot.\n\nassessment on the results and experiment section becomes invalid if the methods section of the paper is not clear and people cannot reproduce this work."
                },
                "questions": {
                    "value": "see above 'weakness' section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2522/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699070702647,
            "cdate": 1699070702647,
            "tmdate": 1699636188531,
            "mdate": 1699636188531,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "O8AcxbWAKU",
                "forum": "j5cPsz8AEQ",
                "replyto": "EEXkz6RbSW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2522/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2522/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We regret that the reviewer found the paper hard to read. \n\nTo address the stated weaknesses/questions:\n\n1a) In the sentence where these quantities are first used, it is stated that this denotes a dataset of N_d noisy measurements of the PINN solution. We will clarify this with the notation \\{t_d^i, y_d^i}}_{i=1}^{N_d} and similar for the other sets in the paper.\n\n1b) The subscript d allows for the distinction between the datasets D_d and D_c, where D_d contains data points (-> d) and D_c contains collocation points (-> c). In the text it is also stated that D_d contains N_d elements and not only two.\n\n1c) y_d denotes measurements of the PINN solution, which is stated in line 4 on page 3. As such, it has the same dimensionality as x. As is mentioned in the first paragraph of 3.1, x can be multidimensional. Ultimately, the dimension depends on the PDE under consideration. lambda denotes parameters of the differential operator F, which is stated in line 5 on page 3.\n\n1d) As is stated in the last 2 sentences of paragraph 1 on page 3, collocation points can be chosen arbitrarily in the domain of interest. In section 5 we have explained how it is done for  the different experiments.\n\n1e) i denotes the loop index. We apologize that we forgot to define it and will clarify this in the revised version."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2522/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699970369990,
                "cdate": 1699970369990,
                "tmdate": 1699970385576,
                "mdate": 1699970385576,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "864yhHHqJ7",
                "forum": "j5cPsz8AEQ",
                "replyto": "O8AcxbWAKU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2522/Reviewer_pToC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2522/Reviewer_pToC"
                ],
                "content": {
                    "title": {
                        "value": "keep scores"
                    },
                    "comment": {
                        "value": "I like to keep my scores. encourage the authors to keep improving their paper."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2522/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700620506012,
                "cdate": 1700620506012,
                "tmdate": 1700620506012,
                "mdate": 1700620506012,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]