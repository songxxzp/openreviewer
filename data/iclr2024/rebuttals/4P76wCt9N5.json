[
    {
        "title": "DAG-based Generative Regression"
    },
    {
        "review": {
            "id": "h2M9H8Ip3G",
            "forum": "4P76wCt9N5",
            "replyto": "4P76wCt9N5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5528/Reviewer_ay5v"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5528/Reviewer_ay5v"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a generative model which assumes that the true data distribution is induced by a structural equation model (SEM). The proposed inference method uncovers both the underlying DAG structure of the SEM, as well as the generating functions in each of the DAG nodes, thus enabling the generation of synthetic data from the model. A number of numerical experiments show that the proposed model is competitive against the baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ An interesting approach combining DAG learning and generative modelling (e.g. Wasserstein loss)\n+ Theoretical results providing the motivation for the proposed learning objective\n+ Multiple numerical experiments showing the competitive performance of the proposed method"
                },
                "weaknesses": {
                    "value": "I think the presentation could be improved, I struggled to understand some of the details of the proposed method (see the questions below). Also some notation is confusing and there are a few typos, here are some examples:\n\n- What is X (without subscript) in Eq. (1)?\n- What is W in Eq. (2)?\n- Typo in Definition 3.2. (G_A \\neq G_A)\n- What does X = A^T X + Z mean in Section 5.1.? Is it a recurrence relation?\n- What exactly is \"all observations\" in Corollary 3.2.1.?"
                },
                "questions": {
                    "value": "- Eq. (1) defines the SEM through the expectations. Does it mean that you consider all SEMs with the same expectations (but potentially different joint distributions) as the same SEM?\n- Could you please provide more details on the network architecture in Sec. 4? As I understood the first layer (parameterised by W^1) learns the DAG structure, while the subsequent L-1 layers learn the functions f_i in the SEM. Is it correct? What is the architecture of these layers? How to you estimate the Wasserstein distance?\n- What modifications of the baseline methods did you do when you say \"To make a fair comparison, we add noise to each competitor\u2019s architecture\"? (in Sec. 5)\n- Why do you think the proposed method outperforms Zheng et al. (2020) on structural discovery while using their architecture?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5528/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698235792765,
            "cdate": 1698235792765,
            "tmdate": 1699636566990,
            "mdate": 1699636566990,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "uFySuEDvx8",
            "forum": "4P76wCt9N5",
            "replyto": "4P76wCt9N5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5528/Reviewer_XSCR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5528/Reviewer_XSCR"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the DAG learning problem. Different from the previous method, the paper utilizes both MSE and MMD loss to measure the distance between generated and truth data distribution. Extensive experiments on both synthetic and real-world datasets demonstrate the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The DAG learning problem is important.\n2. The proposed method achieves better results than the baseline."
                },
                "weaknesses": {
                    "value": "1. The problem setting is unclear. It is unclear the major difference between the traditional DAG learning task with the studied DAG-based generative regression task.\n2. The novelty of the paper is very limited. The major difference between the proposed method with NOTEARS-MLP is the additional MMD loss. The contribution is marginal.\n3. It lacks an analysis of the identifiability of used loss on the causal structure."
                },
                "questions": {
                    "value": "1. Does the model can be used for regression prediction task?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5528/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5528/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5528/Reviewer_XSCR"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5528/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698650247007,
            "cdate": 1698650247007,
            "tmdate": 1699636566883,
            "mdate": 1699636566883,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "9z0YbMOjpY",
            "forum": "4P76wCt9N5",
            "replyto": "4P76wCt9N5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5528/Reviewer_F2tX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5528/Reviewer_F2tX"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a generative regression through the DAG model, that is, learning the Directed Acyclic Graph (DAG) through actual data, and then modeling the regression function from parent variables to child variables based on neural networks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper is written clear, and the idea of construct generative regression based on DAG models is valuable for applications in machine learning fields."
                },
                "weaknesses": {
                    "value": "In this article, I did not identify any notable advancements in the areas of causal structure learning, regression modeling, or the use of generative regression grounded in causal models."
                },
                "questions": {
                    "value": "In Section 4 of the paper, the primary methodology is presented with limited detail and without a proper analysis of its validity. Additionally, I have reservations about the efficacy of solely using the MSE criterion to learn the underlying causal structure."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5528/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5528/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5528/Reviewer_F2tX"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5528/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698673414311,
            "cdate": 1698673414311,
            "tmdate": 1699636566756,
            "mdate": 1699636566756,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "9XzBBNIQtF",
            "forum": "4P76wCt9N5",
            "replyto": "4P76wCt9N5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5528/Reviewer_K8HK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5528/Reviewer_K8HK"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an approach to perform causal discovery. \n\n### Review summary:\nThis work is below the bar for NeurIPS, since it lacks clarity, has poor motivation, is not well situated in the literature, and presents many conceptual inaccuracies. For these reasons, I cannot recommend acceptance. That being said, the empirical results do seem encouraging, which might mean that the authors are onto something. With a significant effort to understand the literature better and how their approach fits in it, this work could be of practical interest."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- Many experiments with apparently very strong performance."
                },
                "weaknesses": {
                    "value": "### **Confusing motivation/framing**\nThe introduction makes an analogy between regression and DAG learning, but the rest of the paper is only about DAG learning. Also, I couldn\u2019t understand the contribution from the introduction.\n\n### **I can\u2019t understand the proposed approach**\nThe writing is in general very confusing. At first, $\\tilde{X}$ was described as the \u201cfake sample\u201d generated from the learned model. But (2) and (3) suggest these are sampled conditionally on actual data samples X. Right? I cannot make sense  of the proposed approach. The lack of clarity casts a shadow over the apparently very strong performance of the approach in the experimental section. \n\n### **Novelty is unclear**\nThe paper does not contrast clearly their approach with the rest of the literature. From what I understand of the method, the novelty is very low. The empirical results are impressive, but there is no compelling story as to why there is such an improvement. \n\n### **Poor sentence formulations**\n- In abstract: \u201cWe learn DAG by reconstructing the model to\u2026\u201d Should add \u201cthe\u201d between \u201clearn\u201d and \u201cDAG\u201d.\n- In intro: \u201c... to identify independent variables (exogenous) that are directly associated with the dependent variable.\u201d What does \u201cdirectly associated\u201d mean?\n- In intro: \u201cDAG was first introduced as part of the formal theories for causal graphs in epidemiology a very long time ago (Greenland et al., 2007), \u2026\u201d The phrase \u201ca very long time ago\u201d feels weird in the context of an academic paper.\n- In intro: \u201cCausality learning is more about discovering\u2026\u201d It\u2019s the first time I read the term \u201ccausality learning\u201d. It should be \u201ccausal learning\u201d, no?\n\n### **Inaccuracies/Imprecisions, sometimes major**\n- In intro: \u201cThe goal of DAG-learning is to holistically capture interplays between the variables in an entire dataset, which extends standard regression analysis that hypothesizes a fixed many-to-one DAG structure between the independent and dependent variables (see more details in Section 3).\u201d This seems inaccurate, since most regression methods allow for dependencies between the covariates.\n- In intro: \u201cUnder the causal identifiability assumption (Neal, 2020), a causality model is capable of replicating the distribution of real data if and only if the model is correct.\u201d What does it mean for a model to be \u201ccorrect\u201d here?\n- In Section 3: you refer to || P(X) - P(\\tilde X)|| as a \u201cdistance function\u201d. But which one is it? The total variation distance? The Wasserstein metric? Unclear.\n- Definition 3.1: What you call \u201cDAG-based generative regression\u201d is a very standard problem in the literature. Does it really warrant a definition?\n- Lemma 3.1 is not really a lemma. It is simply a  vague claim that $\\mathbb{E}||X - \\tilde{X}||_2$ is a good quantitative measure of how far two distributions are. However it\u2019s not, since it\u2019s not even a metric. For instance, if X and \\tilde X are identically distributed (I assume independence) then in general the expectation won\u2019t be zero. Also, the \u201cproof\u201d doesn\u2019t make much sense.\n-  Section 3: \u201cIn addition to the Markov and faithfulness assumptions, the identifiability assumption assumes an identifiable causal model, which suggests that there is only one unique DAG structure to generate P(X).\u201d I don\u2019t understand this sentence.\n- Definition 3.2 is unclear. Same for Lemma 3.2.\n- I\u2019m confused by Equation (2) and (3). Why are \\tilde{X} and X related? I thought X was the data and \\tilde{X} the generated data?\n\n### **Inaccurate related work**\n- In related work: \u201cRecently, DAG-learning with continuous optimization framework NOTEARS (Zheng et al., 2018) has drawn much attention evidenced by a series of NOTEARS-based DAG learning methods (Yu et al., 2019); (Lachapelle et al., 2020); (Zheng et al., 2020), which have fundamentally transformed DAG-learning from combinatorial search into a solvable continuous optimization problem through the use of acyclicity constraints.\u201d The phrasing here makes it sound like these continuous-constrained approaches have transformed the field. This is an overstatement. In a sense these methods are indeed fundamentally different from classical combinatorial approaches, but we cannot say that they have overthrown the older approaches.\n- \u201cMost of the existing methods involve only reconstruction (i.e. mean squared) loss, which is limited to the generative process with additive noise models.\u201d This is not true, many methods do not make this assumption, like the classical PC algorithm and its variants, which are based on conditional independence testing. Another example would be DCDI [1], which uses normalizing flow (more expressive than additive noise models).   \n\n[1] Philippe Brouillard, S\u00e9bastien Lachapelle, Alexandre Lacoste, Simon Lacoste-Julien, and Alexandre Drouin. Differentiable causal discovery from interventional data. In Advances in Neural Information Processing Systems 33, 2020."
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5528/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698714938219,
            "cdate": 1698714938219,
            "tmdate": 1699636566663,
            "mdate": 1699636566663,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "YI8sS2pDuW",
            "forum": "4P76wCt9N5",
            "replyto": "4P76wCt9N5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5528/Reviewer_sCo4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5528/Reviewer_sCo4"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes DAG-based generative regression learning the data generation mechanism from real data by explicitly involving DAG in the generative process. Experiments are condcted showing that their algorithm can outperform state-of-the-art methods in replicating the real data distribution."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper introduces a new approach, DAG-based generative regression for regression analysis, showing the potential to advance regression analysis by capturing the causal relations among variables.\n2. Experiments are conducted on various datasets and results are provided in the form of metrics and visualization compared to SOTA methods ."
                },
                "weaknesses": {
                    "value": "1. The motivation of the model design is not clear, i.e, there is no evident theoretical basis to ensure that the model architecture can effectively capture the causal relationships among data.\n2. The paper is not organized well enough to provide clear analysis."
                },
                "questions": {
                    "value": "1. Can you provide more details about the experiments, such as the choice of evaluation metrics and the results on larger datasets?\n2. How to prove that the algorithms proposed can capture the DAG structure in the data?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5528/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699084499741,
            "cdate": 1699084499741,
            "tmdate": 1699636566537,
            "mdate": 1699636566537,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]