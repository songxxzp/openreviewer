[
    {
        "title": "SparseDFF: Sparse-View Feature Distillation for One-Shot Dexterous Manipulation"
    },
    {
        "review": {
            "id": "zJEZ4hapaN",
            "forum": "HHWlwxDeRn",
            "replyto": "HHWlwxDeRn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3409/Reviewer_hwb9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3409/Reviewer_hwb9"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents an innovative method to obtain view-consistent 3D DFFs from sparse RGBD data, enabling one-shot learning of complex manipulations that can be adapted to unfamiliar settings. The key contribution of SparseDFF comprises a lightweight feature refinement network, optimized using a contrastive loss applied to pairs of views after projecting image features onto the 3D point cloud. Furthermore, by establishing consistent feature fields in both the source and target scenes, they design an energy function that simplifies the process of minimizing feature differences with respect to the end-effector parameters between the demonstration and the target manipulation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper presents a captivating approach to 3D feature learning, involving the creation of a point-cloud-based 3D representation and the utilization of the DINO feature extractor. This 3D representation based on point clouds can enable one-shot dexterous manipulation. As demonstrated by the experimental results, the proposed method exhibits robust performance across various settings."
                },
                "weaknesses": {
                    "value": "In light of the experimental findings presented in this paper, it is respectfully suggested that the method described may not be particularly captivating. For a more comprehensive critique, kindly refer to the Questions section."
                },
                "questions": {
                    "value": "### Question 1:\nIn the contrast learning process, a distance of 1cm is set as the threshold to distinguish between similar and dissimilar parts. Can the authors provide clarification on how they precisely define this distance?\n### Question 2:\nIs distance truly an effective criterion for distinguishing between similarity and dissimilarity?\n### Question 3:\nIs there a typographical error in Equation (3)? Why does it contain both an equation symbol and an inequality symbol?\n### Question 4:\nAdditionally, is the pruning process defined in Equation (3) considered reasonable?\n### Question 5:\nCould this pruning process potentially lead to the removal of critical edge information?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3409/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3409/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3409/Reviewer_hwb9"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3409/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698576555175,
            "cdate": 1698576555175,
            "tmdate": 1699636292143,
            "mdate": 1699636292143,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6msQ3wLnjz",
                "forum": "HHWlwxDeRn",
                "replyto": "zJEZ4hapaN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3409/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3409/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your feedback! Here are our responses to your questions and comments and we hope that they could address your concerns:\n\n**Threshold distance choice for the contrastive loss.**\n\nFollowing the PointContrast paper, let $X_1, X_2$ be two partial point clouds from two views. We sample point pairs from them $(x_{1i}, x_{2i})$. We say that it is a positive pair if $||x_{1i} - x_{2i}||$ < 1cm and a negative pair elsewise. The positive and negative pairs are then plugged into the PointNCE loss defined in Equation 2.\n\nHere we note that, although this threshold is hard, the contrastive loss itself is a soft loss for two reasons: (a) the weighting in the soft PointNCE loss, and (b) the balanced sampling rate between positive and negative pairs.\n\nThe specific threshold distance is decided based on the object scale and the overall framework is not too sensitive to this hyperparameter within a reasonable range.  We will add more clarifications to the paper.\n\n**Is distance truly an effective criterion for distinguishing between similarity and dissimilarity?**\n\nFor the feature similarities, we majorly follow prior works on their choices of metrics. Specifically:\n- We choose the cosine similarity for contrastive learning, as the InfoNCE loss with theoretical foundations (ref point contrast)\n- We choose L1-distance for computing the energy function during optimization, the same as in prior works such as NDF. Here the dominant factor for us to make the choice is not the criteria for similarity, but the optimization curve/landscape. We tried L1-distance, L2-distance, and cosine similarity here and observed that optimizing the hand pose with L1-distance gives the best quality and stability.\n\n**Symbols in Equation (3).**\n\nThe # means the cardinality of the set defined in the brackets {...}, and the inequality $||f\u2019_i - f\u2019_j|| < \\delta$ defines the region of the set. There are multiple notation systems for sets and we picked the one we\u2019re used to. We apologize for the confusion and will add further clarifications to the paper.\n\n**Is the pruning process defined in Equation (3) considered reasonable?**\n\nThe overall intuition of keeping 80% percent of points is that: (a) we want to discard the points with the worst features (here we recognize \u201cbad\u201d as lower feature consistency within local regions), but (b) we don\u2019t want to discard way too many points to maintain the overall shape of the object/scene.\n\n**Could this pruning process potentially lead to the removal of critical edge information?**\n\nThis is indeed a very good question. We think this should not be a big concern, as the pruning is majorly based on the features instead of point cloud geometry, and it is after the feature refinement. Sharp structures such as edges and corners usually easier for DINO to extract stable and coherent features. In fact, based on our observation, if we gradually decrease the percentage of points remaining (from 80% to 5%), the points leaving till the last are mostly the edge points which are more identifiable by the feature extractor. For 80% of points remaining, both edge and face points are kept and their distribution is fairly even across the object. We will add these discussions to the paper."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3409/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700088741098,
                "cdate": 1700088741098,
                "tmdate": 1700088741098,
                "mdate": 1700088741098,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nievfpER1t",
                "forum": "HHWlwxDeRn",
                "replyto": "zJEZ4hapaN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3409/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3409/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you once again for your valuable comments on our work. We have already provided our response to your suggestions. As the deadline for discussion is approaching, we kindly request you to promptly provide any further questions or concerns you may have."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3409/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700541736348,
                "cdate": 1700541736348,
                "tmdate": 1700541736348,
                "mdate": 1700541736348,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "r9mUlIISS0",
                "forum": "HHWlwxDeRn",
                "replyto": "nievfpER1t",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3409/Reviewer_hwb9"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3409/Reviewer_hwb9"
                ],
                "content": {
                    "title": {
                        "value": "Official Comments"
                    },
                    "comment": {
                        "value": "Thank you very much for your reply, which effectively addresses my issue."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3409/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700553683383,
                "cdate": 1700553683383,
                "tmdate": 1700553683383,
                "mdate": 1700553683383,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IqHMrHnH7g",
            "forum": "HHWlwxDeRn",
            "replyto": "HHWlwxDeRn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3409/Reviewer_sPkV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3409/Reviewer_sPkV"
            ],
            "content": {
                "summary": {
                    "value": "This work applies 3D feature fields for manipulation. The key contribution lies in introducing a sparse view setting, where unlike previous works that use dense RGB views, this work uses sparse RGB-D views. A point-based sparse 3D feature field construction method is introduced to improve the 3D information aggregation quality and the grasping task performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The introduction of sparse RGBD camera setting.\nThe method of sparse DFF to reconstruct 3D feature fields from sparse RGBD inputs.\nReasonable experiment design and analysis."
                },
                "weaknesses": {
                    "value": "There are existing sparse-view NeRF methods (e.g., [1,2]) that applies similar ideas as the sparse DFF, some of which are not extremely hard to apply to the normal DFF (e.g,, [1]). It is fairer to allow baselines to also utilize the depth information introduced in this work (e.g., introducing depth supervision similar as [1] in DFF). \n\n[1] Depth-supervised NeRF: Fewer Views and Faster Training for Free\n[2] MVSNeRF: Fast Generalizable Radiance Field Reconstruction from Multi-View Stereo"
                },
                "questions": {
                    "value": "N.A."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3409/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698604942916,
            "cdate": 1698604942916,
            "tmdate": 1699636292058,
            "mdate": 1699636292058,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KArQkZsDnZ",
                "forum": "HHWlwxDeRn",
                "replyto": "IqHMrHnH7g",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3409/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3409/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your feedback! Here are our responses to your questions and comments and we hope that they could address your concerns:\n\n**Sparse-view or depth-guided NeRF methods.**\n\nIn our baseline comparison, we also used the depth for DFF (we apologize for not making it clear and will add clarifications to the paper). Therefore, we remove the difficulty of reconstructing the 3D geometry in both our method and the baselines and majorly focus on extracting the latent features for manipulation.\n\nSpecifically, we want to address that, 3D consistency itself also exists in DFF/NeRF which directly averages the features from different views (because only the mean is considered and the variance is omitted). However, such view averaging with inconsistent 2D features can result in low-quality 3D features \u2013 which is the message we want to convey in this paper.\n\n\n**Fairer to utilize depth in the baseline.**\n\n- As mentioned above, for the DFF baseline, we are also using the scanned depth same as in our method for a fair comparison. We apologize for not clarifying this in the paper.\n- For UniDexGrasp++, we are using their state-based model which takes the ground-truth 3D geometry as input.\n\nWe will add further clarifications to the paper."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3409/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700088529593,
                "cdate": 1700088529593,
                "tmdate": 1700088529593,
                "mdate": 1700088529593,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aGJfaTRHU5",
                "forum": "HHWlwxDeRn",
                "replyto": "IqHMrHnH7g",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3409/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3409/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you once again for your valuable comments on our work. We have already provided our response to your suggestions. As the deadline for discussion is approaching, we kindly request you to promptly provide any further questions or concerns you may have."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3409/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700541720528,
                "cdate": 1700541720528,
                "tmdate": 1700541720528,
                "mdate": 1700541720528,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "e0wmHWlHbw",
                "forum": "HHWlwxDeRn",
                "replyto": "KArQkZsDnZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3409/Reviewer_sPkV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3409/Reviewer_sPkV"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response, though I am not sure whether providing the depth information to the baselines without proper incorporation of the techniques in [1] or [2] is the best practice, I understand that conducting such extra experiments would be impractical during rebuttal. I would keep the score the same (slightly above the acceptance threshold) and suggest the authors to  consider this point further in the camera ready, i.e., either explain clearly why techniques like [1] or [2] do not apply to the baseline, or conduct extra experiments."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3409/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631176545,
                "cdate": 1700631176545,
                "tmdate": 1700631176545,
                "mdate": 1700631176545,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1Il4S22OY5",
            "forum": "HHWlwxDeRn",
            "replyto": "HHWlwxDeRn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3409/Reviewer_dYVL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3409/Reviewer_dYVL"
            ],
            "content": {
                "summary": {
                    "value": "To mitigate the dense view requirement in the distilled feature field (DFF) for the application of one-shot dexterous manipulation, the authors introduce sparseDFF, which utilizes a sparse collection of RGB-D scans of a scene. Feature points are reprojected using depth, followed by a feature refinement process on these reprojections. Contrastive loss and point pruning are employed to enhance feature consistency within each local neighborhood, and an energy function is formulated to aid in reducing feature discrepancies. Performance on grasping benchmarks demonstrate the proposed methods surpasses DFF while significantly outperforms UniDexGrasp++."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The necessity of as few as 4 views for transferring manipulation skills is noteworthy, as this method can be readily generalized to novel scenes.\n- Point feature refinement, the minimization of feature discrepancies using an energy function, and point pruning are specifically designed for applications with RGB-D scans as input.\n- Utilizing DINO feature distillation for diverse downstream grasping tasks markedly outperforms the previous baseline (DFF) and UniDexGrasp++."
                },
                "weaknesses": {
                    "value": "**[Clearance]** \n- The authors are encouraged to establish connections regarding why distilling DINO features is advantageous and elucidate its applications in downstream tasks.\n- The definition of **one-shot** should be explained in the manuscript (abstract or introduction).\n- The input should accurately be described as \"multi-view RGB-D scans\" rather than \"Given a 3D point cloud X\", in the method section. And the dimension of the variable should be added.\n- Regarding the motivation of using DINO, while the authors have highlighted, \"This field offers semantic understandings for inter-scene correspondences that transcend geometric descriptors\", how does it contribute specifically to image matching deep models like LOFTR?\n\n**[Method]** \nThe authors propose \"discard the 20% of points that accumulate the fewest votes.\".How was this hyper-parameter for the pruning ratio determined? Were multiple-stage or iterative pruning strategies considered?\n\n**[Experiments]** \n- How did the inference performance compare with baseline methods?\n- Was depth information also utilized by DFF?"
                },
                "questions": {
                    "value": "See the raised questions in Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3409/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698820727744,
            "cdate": 1698820727744,
            "tmdate": 1699636291990,
            "mdate": 1699636291990,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3zCX3ueBlD",
                "forum": "HHWlwxDeRn",
                "replyto": "1Il4S22OY5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3409/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3409/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your feedback! Here are our responses to your questions and comments and we hope that they could address your concerns:\n\n**Elucidating the advantages of distilling DINO features in the downstream tasks.**\n\nThe features from large vision models such as DINO provide semantics-aware correspondences across different instances/categories, allowing one-shot/few-shot transfer of manipulation policies. However, the 2D image features are only sufficient for simple end-effectors such as parallel grippers operating on single points in the space. For dexterous hands with complex geometry, we need a 3D feature field to decide the parameters of all the finger joints, and that\u2019s why we need such 2D-to-3D distillation. We will add more explanations and discussions to the paper.\n\n**The definition of one-shot should be explained in the manuscript (abstract or introduction).**\n\nHere the \u201cone-shot\u201d means that given one demonstration on the source object, we seamlessly transfer learned manipulations to novel scenes, accommodating variations in object poses, deformations, scene contexts, and even object categories. We abridge the wording a bit to avoid making the title too long. We will add more clarifications to the paper.\n\n**The input format and the dimension of the variables.**\n\nThe 3D point cloud is obtained by merging the depth scans from the 4 views. We will add clarifications to the paper.\n\n**Motivation for using DINO and other possible choices of image-matching deep models like LOFTR.**\n\nOur framework is not specifically curated for DINO features and would be potentially compatible with any image feature extractors. We adopt DINOv2 here as they\u2019ve shown strong cross-instance and cross-category correspondences. LOFTR seems to work mostly on matching two identical objects in different images. We will add the discussions of other image-matching frameworks like LOFTR to the paper.\n\n**Hyper-parameter for point pruning pruning. Multiple-stage or iterative pruning strategies.**\n\nWe have tried different hyperparameters for the point pruning and within a certain range (70%-80% points left) it offers consistently good results. The overall intuition of choosing the hyperparameters is that: (a) we want to discard the points with the worst features, but (b) we don\u2019t want to discard way too many points to maintain the overall shape of the object/scene.\n\nMuti-stage or iterative pruning would be an interesting future direction to study and thanks for pointing it to us.\n\n**How did the inference performance compare with baseline methods?**\n\nOur inference time is roughly the same as DFF, and both DFF and UniDexGrasp++ require iterative inference (RL or optimization) on the hand pose. This is because, for hand-like end-effectors with complex shapes, a variety of geometric and physical factors (e.g. contacts, penetrations, force closures, etc) need to be carefully decided for a successful manipulation (grasp). On the other hand, one-step forward prediction methods are prevalent for parallel grippers because they operate on single 3D points and can apply infinite forces.\n\n**Was depth information also utilized by DFF?**\n\nYes, depth information is also used in DFF for a fair comparison. We apologize for not clarifying this in the paper.\n\nWhat we want to address in the paper is that, even with the scanned 3D geometry guiding the feature aggregation, the feature incoherency between different views, if not properly tackled, will lead to a lower-quality 3D feature field, causing a great drop in the success rate."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3409/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700088455275,
                "cdate": 1700088455275,
                "tmdate": 1700088455275,
                "mdate": 1700088455275,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "q0OTpATc9T",
            "forum": "HHWlwxDeRn",
            "replyto": "HHWlwxDeRn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3409/Reviewer_Txdk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3409/Reviewer_Txdk"
            ],
            "content": {
                "summary": {
                    "value": "The broader goal of the paper is to enable the transfer of robotic dexterous grasps from one object to a similar object (which can happen with understanding the inherent similarities of the 3D shape instances despite the variations in appearances, poses, or categories). More precisely, given a source scene, source hand-object grasp and a target scene, what could be the target grasps.\nTo do so, they leverage 2D vision models (namely DINO) to learn features in 2D space and then distill or back-project those features in 3D. The similarly of features in 3D space allows transfer of grasps from one object to another.\nCompared to prior works, which leverage dense multi-view images to infer dense 2D features for each three 3D point and simply average the multi-view features, the paper operates in sparse view setting. Rather than simply out the point features from multiple view, the paper paper learns a feature refinement network on each point features and defines a contrastive learning approach to bring points closer to each in 3D other more closer in feature space and points farther from each other in 3D space more farther in feature space.\nFinally, leveraging the projected and refined 3D features from sparse views, the paper performs the task mentioned in first bullet, mapping source grasp (on sourc e scene) to target scene."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Writing is good, and the paper is easy to follow.\nThe motivation of creating a generalized robotic manipulator and leveraging 3D priors for that, seems exciting and promising."
                },
                "weaknesses": {
                    "value": "[Novelty Issue] Lack of any exciting factor or in some sense novelty: The difference from prior work in DFF (distill feature field) is that the paper operates in sparse view setting, where simple fusion of features from multiple views doesn\u2019t perform best. To overcome this loss of views from prior work, they refine the point features baed on the insight that points close in 3D, should have similar features. This seems a very natural and obvious technique of pruning or refining points which have incorrect feature consistency w.r.t 3D consistency.\n\n[Assumptions on GT depth and camera pose] Secondly, following up on the novelty part, the paper makes the assumption of having GT depth maps and also GT camera poses. Errors in either of them will make the above refinement step tricky.\n\n[Additional experimental comparison] Experimental Comparison against baselines like (Neural descriptor fields and follow-up) where the goal is similar but rather than using the large vision model, an object category specific feature descriptor is learned.\n\n[Less Relevant] Newer papers like Lseg, Conceptfusion have projected LLM/ vision-LLM features on the 3D scenes, comments on using them as feature backbones will be appreciated.\n\n[Less Relevant] Papers learning joint hand object poses, leaning visual affordances from images, also seem to be relevant related works, comparison against them in related work would be appreciated (Affordance Diffusion: Synthesizing Hand-Object Interactions, papers on hand-object interaction: HOI etc)."
                },
                "questions": {
                    "value": "I would like authors to address the points raised in weaknesses section. \n1. What happens when there is noise in pose and/or depth? How will that impact refinement module?\n2. How does the method compare with baselines like neural descriptor fields and follow up works?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3409/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3409/Reviewer_Txdk",
                        "ICLR.cc/2024/Conference/Submission3409/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3409/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698893923492,
            "cdate": 1698893923492,
            "tmdate": 1700602651596,
            "mdate": 1700602651596,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qB933afDeT",
                "forum": "HHWlwxDeRn",
                "replyto": "q0OTpATc9T",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3409/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3409/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your feedback! Here are our responses to your questions and comments and we hope that they could address your concerns:\n\n**Novelty. It is natural that feature refinement induces 3D consistency.**\n\nSpecifically, we want to address that, 3D consistency itself also exists in vanilla DFF/NeRF without any refinement that directly averages the features from different views (because only the mean is considered and the variance is omitted). However, such view averaging with inconsistent 2D features can result in low-quality 3D features \u2013 which is the message we want to convey in this paper.\n\nWe agree that feature refinement w.r.t. 3D consistency gives better 3D consistency is not surprising. But what is not obvious is that feature refinement w.r.t. 3D consistency gives better cross-instance correspondences.\n\nMoreover, most contrastive feature learning (such as PointContrast) show their generalization after training on a large dataset of pairwise samples, but here we show that even only overfitted to the 4 views of the training scene with a few minutes of optimization, our refinement network naturally generalizes to novel scenes.\n\nApplication-wise, compared to parallel grippers which operate on one single 3D point and can apply infinitely large force, dexterous hands require more accuracy in the end-effector pose to, contact the object surface, wrap the object, and get the forced closure. This makes a lot of methods for parallel grippers not directly applicable to dexterous hands. In such a scenario, fine-grained properties of the feature field become more important.\n\n**Noise in depth and camera pose.**\n\nAll experiments for our method and the DFF baseline are on real robots, and the depth and camera poses are all from real data with scan errors and calibration errors. Only the UniDexGrasp++ baseline is run in the simulation environment with ground-truth 3D geometry (as we\u2019re running their state-based model).\n\nThe sim2real gap is a long-standing problem in robotics, and it is particularly severe for dexterous hands compared to simpler end-effectors such as parallel grippers. In this work, we show our results on real robots to demonstrate our robustness towards this sim2real gap.\n\n**Additional experimental comparison with NDF.**\n\nHere we provide additional comparisons with NDF. As we temporarily don\u2019t have access to the hardware, we conduct the experiments in simulation environments (IsaacGym) with the object scans from the experiments in the paper.\n\nThe success rates are:\n\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n\nSource: Mug2, Target: Mug2 (same object)\n- NDF: 8/9, Ours: 9/9\n\nSource: Mug2, Target: BeerBarrel (same category)\n- NDF: 6/10, Ours: 9/10\n\nSource: Mug2, Target: CatBowl (cross category)\n- NDF: 0/10, Ours: 7/10\n\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n\n(*There are only 9 test samples for Mug2 because one Mug2 scan is used for providing the demonstration. We will update the results as well as conduct real-robot comparisons once we get access to the hardware again.)\n\nIn addition, we also address the following limitations of NDF:\nNDF needs category-specific training and it takes a lot of time.\nNDF is specifically designed for category-level rigid object manipulation. It is not applicable to other scenarios such as deformable objects or cross-category generalizations.\nAs a pure geometry-based method, NDF is highly sensitive to geometric variations (such as the BeerBarrel and CatBowl in the experiment here).\nFollowing the previous question on scan noises, our method actually shows much better noise stability than NDF in real-world experiments.\n\n**Other LLM/VLM backbones for feature extractions.**\n\nWe choose the latest DINOv2 here as they demonstrated strong cross-instance/category correspondences on images, while some other VLMs are more focused on language queries (such as open-vocab query/segmentation). But we agree that exploring different VLM backbones, especially studying their view consistency and 3D awareness, would be an interesting future work. We will also add more discussions to the paper.\n\n**Other related works on learning joint hand-object poses or learning visual affordances from images.**.\n\nThanks for providing these references. We will add the discussions to our paper."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3409/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700088315617,
                "cdate": 1700088315617,
                "tmdate": 1700088315617,
                "mdate": 1700088315617,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yiboV1A4ZS",
                "forum": "HHWlwxDeRn",
                "replyto": "q0OTpATc9T",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3409/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3409/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you once again for your valuable comments on our work. We have already provided our response to your suggestions. As the deadline for discussion is approaching, we kindly request you to promptly provide any further questions or concerns you may have."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3409/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700541799894,
                "cdate": 1700541799894,
                "tmdate": 1700541799894,
                "mdate": 1700541799894,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lfLIFhGlOt",
                "forum": "HHWlwxDeRn",
                "replyto": "yiboV1A4ZS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3409/Reviewer_Txdk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3409/Reviewer_Txdk"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for addressing my concerns. I have updated my rating from 5 to 6."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3409/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700602704769,
                "cdate": 1700602704769,
                "tmdate": 1700602704769,
                "mdate": 1700602704769,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]