[
    {
        "title": "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting"
    },
    {
        "review": {
            "id": "YZySQzD9Xu",
            "forum": "RIu5lyNXjT",
            "replyto": "RIu5lyNXjT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6435/Reviewer_c9z9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6435/Reviewer_c9z9"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores the sensitivity of large language models (LLMs) to prompt formatting choices and investigates the impact of prompt design on LLM performance in few-shot (especially few-shot classification) settings. The authors find that even subtle changes in prompt formatting can have a significant effect on model behavior, with performance differences of up to 76 accuracy points. This sensitivity remains even when the model size or the number of few-shot examples is increased. \nThe authors argue that evaluating LLMs with a single prompt format is inadequate and propose reporting a range of performance across plausible formats. \nThey also demonstrate that format performance weakly correlates between models, questioning the validity of comparing models using a fixed prompt format. To facilitate systematic analysis, the authors introduce an algorithm called FORMATSPREAD, which quickly evaluates a sampled set of prompt formats for a given task. The paper emphasizes the importance of reporting and considering prompt format variations when comparing models and highlights the impact of formatting choices on model behavior by extensive experiments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper investigates a critical issue for Large Language Models (LLMs), specifically the impact of formatting on the few-shot examples used in prompts.\n\n2. The assertion that \"The performance of Large Language Models (LLMs) is highly sensitive to prompt formatting choices, especially in few-shot settings,\" is substantiated by numerous experiments on few-shot classification tasks (e.g., Super-Natural Instructions) and short text generation tasks, such as identifying the second letter of a word, performing arithmetic, or responding with a synonym for a given word.\n\n3. In Section 3, the authors formally define the \"grammar\" of plausible prompt formats, thereby making the problem formulation more rigorous."
                },
                "weaknesses": {
                    "value": "1. The paper primarily substantiates its core claim that \"Performance of large language models (LLMs) is highly sensitive to prompt formatting choices, particularly in few-shot settings,\" through experiments in classification tasks. However, the scope of the experiments does not extend to the frequently utilized capability of LLMs for long text generation. While short text generation tasks (such as identifying the second letter of a word, adding numbers, or responding with a synonym) are discussed in the appendix, these do not fully capture the important aspect of long text generation. Hence, I suggest that the authors either explicitly state that these findings are specifically under the context of classification tasks or conduct additional experiments on long-text generation to avoid any potential overclaim or misleading interpretation.\n\n2. For Figure 2, I recommend that the authors include the Pearson correlation coefficient directly on the figure for a more comprehensive representation of the data.\n\nOverall, I believe this paper studies an important question. If the authors can address my concerns, I would consider increasing my score."
                },
                "questions": {
                    "value": "1. In Section 3.2, the performance spread is quantified as the difference between the maximum and minimum values. I suggest that a more comprehensive approach would be to report both the range (max - min) and the standard deviation, providing a fuller understanding of the data distribution.\n\n2. In the experimental section, the authors explain that classification tasks were selected for their ease of automatic evaluation. I am curious about the challenges associated with measuring performance for text generation tasks, especially considering the benchmarks that have been proposed recently."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6435/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6435/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6435/Reviewer_c9z9"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6435/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698743737300,
            "cdate": 1698743737300,
            "tmdate": 1700637113197,
            "mdate": 1700637113197,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dZWau7TzKa",
                "forum": "RIu5lyNXjT",
                "replyto": "YZySQzD9Xu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6435/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6435/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer c9z9"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their thoughtful review! We are pleased that they pointed out our paper studies \u201ca critical issue for LLMs\u201d, with a rigorous problem formulation and substantiated with numerous experiments on few-shot classification tasks (e.g., Super-Natural Instructions) and short text generation tasks. \n\nWe address all comments below, and **we believe that the two new, additional experiments** *(an experiment on performance variance on 17 new sentence-level open text generation tasks; and an analysis of 30 classification tasks quantifying the dispersion of the accuracy distribution using standard deviation on 500 formats evaluated)* **resolve all the reviewers\u2019 concerns**!\n\n- ***\u201cA more comprehensive approach would be to report both the range plus the standard deviation\u201d*** We agree that including standard deviation gives a more comprehensive understanding of the distribution, and we have added analysis to Appendix B.2 (see \u201cCharacterizing a model\u2019s accuracy distribution beyond spread\u201d and Figure 20) showing the distribution of accuracy across 500 formats for 30 different tasks. Plots also include the standard deviation, which varies from stdev=0.31 to 0.02 across the tasks considered (median stdev=0.04). We would like to emphasize that reporting the spread (i.e. the distribution range) is already a much more comprehensive report than the single-format estimation that is currently the norm, and that efficiently estimating the standard deviation of a distribution is a priori a much more computationally expensive problem. This would require precisely computing the accuracy of possibly hundreds of formats, and would render our current bandit formulation unsuitable. We nonetheless believe that efficiently computing the standard deviation of such a distribution of accuracies is an exciting question for future work!\n\n- ***\u201cWhy classification tasks? What are the challenges associated with measuring performance in long generation tasks?\u201d*** We focused primarily on classification tasks since they unambiguously showcase clearly that performance spread can be a serious concern, given that they are generally much more straightforward to evaluate than text generation metrics. Text generation evaluation is notoriously more difficult, noisy, and subjective than simple classification metrics. Text generation metrics remain an active area of research, and metrics are often task-specific (e.g. a summarization metric) which would hinder us from using the same metric across tasks. Moreover, evaluating text generation often requires a multifaceted approach (e.g. fluency, helpfulness, similarity with the recorded human response) and we specifically wanted to avoid conflating the establishment of the spread discussion with the open problem of text generation evaluation. We nonetheless conducted additional experiments on sentence-length text generation problems; see next question for details!\n\n- ***\u201cI suggest that the authors either explicitly state that these findings are specifically under the context of classification tasks or conduct additional experiments on long-text generation\u201d*** We agree with the reviewer that longer text generation is an important setting to evaluate for LLM usage, and thus we are included additional experiments for text generation using BertScore and ROUGE-L as metrics in App. B.2 (\u201cExperiments with continuous metrics in open-ended text generation tasks\u201d, also Fig. 19), showing that the issue of performance variance across formats still holds: we computed spread on 10 randomly sampled formats on 17 open text generation tasks with LLaMA-2-7B, and found that performance variance remains regardless of the metric and the number of n-shots considered, with LLaMA-2-7B 5-shot having 25% of tasks with a ROUGE-L spread \u2265 0.098, and a BERTScore spread \u2265 0.09. ROUGE-L is the metric used in the original Super-NaturalInstructions tasks, but is known to have a preference for lexical similarity; therefore, we complement results with BertScore, a model-based metric (which, like all model-based open generation metrics, is more robust to changes in vocabulary but that will express Bert\u2019sinductive bias). We focus our new experiments on evaluating sentence-long text generation tasks, since longer text generation experiments are significantly more computationally intensive, given that they require processing a significantly longer input including the solved few-shot examples, plus generating a long text. We will carefully qualify the extent of our experiments across the manuscript: we also added a Limitations section (Appendix C) in this same vein.\n\n- ***\u201cPearson correlation coefficient in Figure 2?\u201d*** We have now included the Pearson correlation coefficient in all scatter plots, including appendix.\n\nWe believe this resolves the concerns the reviewer flagged and we thank them again for their thorough review!"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6435/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700460997990,
                "cdate": 1700460997990,
                "tmdate": 1700460997990,
                "mdate": 1700460997990,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MnPQFnTzgb",
                "forum": "RIu5lyNXjT",
                "replyto": "dZWau7TzKa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6435/Reviewer_c9z9"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6435/Reviewer_c9z9"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Authors,\n\nThank you all for your responses.  There are no other serious problems on my side. Consequently, I have adjusted my rating from 5 to 6.\n\nBest,"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6435/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700637095660,
                "cdate": 1700637095660,
                "tmdate": 1700637095660,
                "mdate": 1700637095660,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BpQRKA1iUZ",
            "forum": "RIu5lyNXjT",
            "replyto": "RIu5lyNXjT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6435/Reviewer_fgsZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6435/Reviewer_fgsZ"
            ],
            "content": {
                "summary": {
                    "value": "Authors claim that in recent trends, LLMs are the inevitable choice for language technologies; their sensitivity to prompt formatting results in critical issues. The sensitivity remains critical even after increasing model size, usage of in-context learning, and instruction-tuning. Given this situation, authors suggest evaluating the performance of LLMs with various prompt formatting, not with a single prompt format. The authors also report that format performance across LLMs has a low correlation, which supports the multi-prompt format evaluation of LLMs. To facilitate the multi-prompt format, the authors propose an analysis method, FORMATSPREAD, that evaluates a set of plausible prompt formats for a given task. The proposed method induces the concept of semantically equivalent formats and measures the performance gap among LLMs queried with the different formats but in a semantically equivalent set, procured by the help of Bayesian Optimization."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- Focus on the subject that has not taken much attention from the community but should be addressed for robust application of LLMs\n- Evaluating LLMs over the prompt distribution provides a more informative understanding of the model's performance and robustness than evaluating only with a single prompt.\n- The proposed method can be utilized on API-gated models, with no need for access to model weights."
                },
                "weaknesses": {
                    "value": "- The authors cast the problem of searching prompt space as a bandit problem. However, many current important applications of LLM assume multi-turn conversation between the user and LLM, which is highly dependent on the conversation history.\n- In Algorithm 1, the formulation assumes the reward of each arm is success or failure, but in the NLP field, there are many important tasks where the output of LM cannot be determined between success or failure (for example, a task needs human alignment). Does the formulation still hold for the tasks that cannot be evaluated in discrete value?"
                },
                "questions": {
                    "value": "- As the search space of the prompt is intractable, a good initial point (in this case, a prompt) would be crucial for successful entire optimization (as authors assume non-adversarial user). How can we be convinced that we start from a good initial point?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6435/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6435/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6435/Reviewer_fgsZ"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6435/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698833359492,
            "cdate": 1698833359492,
            "tmdate": 1699636718065,
            "mdate": 1699636718065,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "e4SUkkaiYr",
                "forum": "RIu5lyNXjT",
                "replyto": "BpQRKA1iUZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6435/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6435/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer fgsZ"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their insightful and encouraging feedback! We are glad they pointed out our work covers a \u201csubject that has not taken much attention from the community but should be addressed for robust application of LLMs\u201d, with a method that \u201ccan be utilized on API-gated models\u201d and gives a \u201cmore informative understanding of the model's performance and robustness\u201d. We address all questions below.\n\n- ***\u201cDoes the formulation still hold for the tasks that cannot be evaluated in discrete value?\u201d*** It does! Thompson Sampling still works for continuous rewards (see Algorithm 1 of Chappelle & Li, 2011; Bernoulli bandits are shown in Algorithm 2). We focused our explanation on the exact setting we used throughout the paper for ease of reading, but the only requirement to extend it to continuous rewards would be to provide a more suitable reward distribution to sample from instead of sampling from a Beta (e.g. a Gaussian).\n\n- ***\u201cA good initial point (a prompt) would be crucial for a successful entire optimization. How can we be convinced that we start from a good point?\u201d*** We are not using a local search algorithm, where an initial point would strongly influence the trajectory of the search; instead, our approach samples random formats by assigning different values to all the constants defined in the grammar. The only influence that the initial prompt has on the search is that if such a prompt is available for a task in our evaluation data, we use it to induce the prompt format grammar for that task. \n\n- ***\u201ccurrent important applications of LLM assume multi-turn conversation between the user and LLM, which is highly dependent on the conversation history\u201d*** We fully agree! Formatting brittleness issues may still stand in these settings, as one can consider each partial conversation as an individual test point to be analyzed in the context of FormatSpread. Conducting a sensitivity experiment with humans in the loop would be difficult because the user behavior might also change in response to system behavior. However, we hypothesize that this sensitivity may be even more detrimental in these multi-turn settings, as small differences in behavior might compound turn after turn. This would be an exciting future direction to test LLM robustness beyond the few-shot setting we focus on!"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6435/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700460850777,
                "cdate": 1700460850777,
                "tmdate": 1700460850777,
                "mdate": 1700460850777,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qnuHYOzvjF",
            "forum": "RIu5lyNXjT",
            "replyto": "RIu5lyNXjT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6435/Reviewer_RFEF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6435/Reviewer_RFEF"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores a very interesting question, which is the impact of different prompt formats of LLMs on the accuracy of downstream tasks. The authors found that this impact is significant to some extent and present a new algorithm called FORMATSPREAD, which can estimate the performance spread of different prompt formatting choices. FORMATSPREAD efficiently searches the space of plausible prompt formats within a specified computational budget."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The issue discussed in this paper, i.e., LLM evaluation should not be limited to a single specific prompt, has good informativeness for the community.\n- This paper is well written and easy to understand. \n- The proposed construction of grammar rules for different prompt formats and the design of sensitivity evaluation are quite ingenious."
                },
                "weaknesses": {
                    "value": "As the authors claim that LLMs are extremely sensitive to subtle changes in prompt formatting, with performance differences of up to 76 accuracy points, which is quite surprising. It is necessary to conduct a more in-depth analysis of these somewhat counterintuitive conclusions. For example, \n1. Is the difference in  prompt formats the only influencing factor, or do other confounders exist, such as the content length of in-context, different tokenize methods?\n2. It is difficult to predict the impact on specific task sensitivities. How can we analyze which types of tasks are more susceptible to prompt format influences, rather than just conducting sensitivity evaluations? This requires further explanation."
                },
                "questions": {
                    "value": "1. Add more analysis about cofounders, such as content length in the examplars, different tokenize methods, etc.\n2. Add further explanation of which specific tasks are more susceptible of changes in prompt formatting."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6435/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6435/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6435/Reviewer_RFEF"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6435/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699448533367,
            "cdate": 1699448533367,
            "tmdate": 1699636717957,
            "mdate": 1699636717957,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QfkUzZ7myS",
                "forum": "RIu5lyNXjT",
                "replyto": "qnuHYOzvjF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6435/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6435/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer RFEF"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their detailed feedback! We are glad they mentioned that we explore \u201ca very interesting question\u201d and that our \u201cproposed construction of grammar rules [...] and the design of sensitivity evaluation are quite ingenious\u201d. We address all questions below.\n\n\n- ***\u201cIs the difference in prompt formats the only influencing factor, or do other confounders exist, such as the content length of in-context, different tokenize methods?\u201d*** This is a great suggestion. While our work focuses on measuring the sensitivity to prompt formatting in particular, our analysis in Section 4.2 could definitely be adapted to features like the length of in-context examples. We don\u2019t claim prompt formatting to be the sole influencing factor on performance variation, but rather a compelling example of spurious features that can seriously affect final performance. Nonetheless, **this question inspired us to add an analysis on the impact of in-context length in Appendix B.2** (\u201cOn factors influencing performance variance besides prompt formatting\u201d), where we found a negligible correlation between the spread and both the average and the standard deviation of prompt length for a task: correlation was $r=0.228$ ($p=1.4\\times 10^{-7}$) for mean prompt length when using exact matching prefix as accuracy metric, and $r=-0.022$ ($p=0.615$) for option ranking metric, see App B.2 for all details. Analyzing the impact of a specific tokenizer exceeds the scope of this work, since decoupling the influence of a model\u2019s tokenizer vs. other factors like the data it was trained on would require training several models with identical training procedures but different tokenizers.\n\n- ***\u201cHow can we analyze which types of tasks are more susceptible to prompt format influences, rather than just conducting sensitivity evaluations?\u201d*** This is an interesting question, and one we focused on as part of our preliminary explorations. We did not find specific factors that can faithfully predict a priori the sensitivity of a model on a given task. For example, we did not find that multiple choice questions had any significant difference from regular tasks with two fields (e.g. \u201cInput: <text> Output: <text>\u201d) or more. However, digging more into why certain tasks result in more sensitivity is an exciting direction for future work!\n\n- ***On the counterintuitiveness of our sensitivity results.*** Sensitivity to spurious features in prompt design is understudied and often not taken into account; much existing research and practice uses LLMs with the assumption they are robust to these features, so we believe that quantifying this sensitivity is crucial to give it its deserved visibility! That being said, we do not believe that our results are necessarily counterintuitive to what we know about LLMs. For example, few-shot examples\u2019 ordering (which doesn\u2019t drastically change the semantics of a prompt) may affect final results (Lu et al., 2022).  As models with multiple billions of parameters, it is not unreasonable to expect their distribution to have sharp changes and discontinuities: we do however hope that FormatSpread sparks discussions on new training objectives that provide robustness guarantees to spurious prompt changes!"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6435/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700460752975,
                "cdate": 1700460752975,
                "tmdate": 1700460778251,
                "mdate": 1700460778251,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]