[
    {
        "title": "Interpretable Diffusion via Information Decomposition"
    },
    {
        "review": {
            "id": "XRFmhe0zG5",
            "forum": "X6tNkN6ate",
            "replyto": "X6tNkN6ate",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5031/Reviewer_U1vn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5031/Reviewer_U1vn"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to investigate the fine-grained relationships learned by diffusion models by noticing a precise relationship between diffusion and information decomposition.  It exploits these new relations to measure the compositional understanding of diffusion models, to do unsupervised localization of objects in images, and to measure effects of prompt interventions."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. It is interesting to view diffusion models from the information theory prespective. For examle, exact expressions for mutual information and conditional mutual information can be expressed in terms of the denoising model.\n2. Compared with attention, the proposed mutual information seems more exact in identifying the relationship between the text and image. Massive visualization in the supplement demonstrate the effectiveness of this method."
                },
                "weaknesses": {
                    "value": "1. It is still unclear of the concrete implementation. Without access to the network, but just with API, how to get the mutual information and corresponding visualization.\n2. What is the application field. Attention-based methods are widely applied to image editing and show impressive results. What is the advantage of this decomposition method, compared to attetion.\n3. The paper is poor structured and hard to read."
                },
                "questions": {
                    "value": "Refer to the weakness and questions above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5031/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5031/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5031/Reviewer_U1vn"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5031/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698419259714,
            "cdate": 1698419259714,
            "tmdate": 1699636492463,
            "mdate": 1699636492463,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "o5oeuwYDG7",
                "forum": "X6tNkN6ate",
                "replyto": "XRFmhe0zG5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5031/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5031/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your thorough review and thoughtful questions! We answer your questions below. Let us know how to further improve the paper!\n\n\n- **Q1: It is still unclear of the concrete implementation. Without access to the network, but just with API, how to get the mutual information and corresponding visualization.**  \n\nThanks for your question! We provide our demo code detailing our implementation for image-level and pixel-level MI/CMI in the updated supplementary material. Our information maps don't require the details of the denoiser (the U-Net for Stable Diffusion) but just use the output of the denoiser directly, e.g. predicted noise $\\hat \\epsilon_\\alpha$, to calculate MI/CMI as an integral of MMSE under a bunch of logSNR $\\alpha$. Thus, our MI/CMI will benefit from a more powerful denoiser in the future or a mixture of experts. However, attention maps require extracting features from self-attention or cross-attention layers and delicately designing a heuristic way to calculate heatmaps, which is hard to generalize. \n\n- **Q2: What is the application field? Attention-based methods are widely applied to image editing and show impressive results. What is the advantage of this decomposition method, compared to attention.**  \n\nWe believe a key strength of our approach is faithful evaluation. On compositional understanding benchmarks, our estimator is significantly better aligned with ground truth compared to other estimators (c.f. the Table in our Scientific Finding section of general response). An application could be image retrieval based on text, for example, along with interpretable maps about why/which parts of an image matched.\n\n- **Q3: The paper is poorly structured and hard to read.**  \n\nThank you for your concern! We will update our presentation in our updated version. Meanwhile, we hope that our added information theory preliminaries and contributions can address some of your concerns."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5031/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700214514710,
                "cdate": 1700214514710,
                "tmdate": 1700214514710,
                "mdate": 1700214514710,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "aBRMMY2XEB",
            "forum": "X6tNkN6ate",
            "replyto": "X6tNkN6ate",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5031/Reviewer_gEHf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5031/Reviewer_gEHf"
            ],
            "content": {
                "summary": {
                    "value": "This work discusses the interpretability of denoising diffusion models, which are used for conditional generation and density modeling of complex relationships like images and text. The authors aim to understand the relationships learned by these models by establishing a connection between diffusion and information decomposition. They provide exact expressions for mutual information and conditional mutual information in terms of the denoising model. The paper also introduces methods to quantify informative relationships between words and pixels in an image. The authors utilize these relationships to measure the compositional understanding of diffusion models, perform unsupervised localization of objects in images, and measure effects when editing images through prompt interventions."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper provides an approach to illuminate the fine-grained relationships learned by diffusion models.\n- The authors introduce a natural non-negative decomposition of mutual information, which can be beneficial for understanding high-dimensional data.\n- The paper offers practical applications, such as unsupervised localization of objects in images.\n- The research uses information theory to provide a black-box method for understanding relationships in complex spaces like text and images."
                },
                "weaknesses": {
                    "value": "- The method is relatively trivial, replacing word in the prompt and comparing the difference in pixel level\n- lack of comprehensive comparison with attention based method"
                },
                "questions": {
                    "value": "- The example in first row of Fig. 1 seems to have difference other than the bear and elephant, why does the heat map only show difference in salient objects? Other examples have similar questions, but Fig. 1 is most obvious. \n- What's the advantage compared to attention based method?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5031/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5031/Reviewer_gEHf",
                        "ICLR.cc/2024/Conference/Submission5031/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5031/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698807530538,
            "cdate": 1698807530538,
            "tmdate": 1700581674534,
            "mdate": 1700581674534,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qrpkrr8Fqg",
                "forum": "X6tNkN6ate",
                "replyto": "aBRMMY2XEB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5031/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5031/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your critique! We hope to resolve your concerns with additional clarifications on our method and experimental design.\n\n\n- **W1: The method is relatively trivial, replacing word in the prompt and comparing the difference in pixel level**\n\nWe are glad that the reviewer found the method so simple that it appears trivial. Mutual information estimation is an extremely challenging problem, with many mathematically complex approaches proposed in the literature. As far as we are aware, ours is the first proposal to estimate MI using diffusion models. However, another diffusion-based method has appeared simultaneously in [1]. We will include this reference in our final version. But we would like to call the reviewer's attention to their final estimator, Eq. 21, which is not nearly as elegant/trivial to understand as ours and is valid only for (average) mutual information but does not generalize to a pointwise version as ours does.\n\n- **W2: Lack of comprehensive comparison with attention based method**    \n\nWe included attention baselines on all tasks that involved word/pixel level localization (Sec. 3.2, 3.3). The main goal of our method, as discussed in the top-level response, is measuring the total strength of relationships between image and text. We show SOTA results for this task in Sec. 3.1. Attention has not, to our knowledge, ever been used as a measure of total dependence between images and prompts because the attention distribution is a normalized distribution over tokens which must always sum to 1.\n\n- **Q1: The example in the first row of Fig. 1 seems to have differences other than the bear and elephant, why does the heat map only show differences in salient objects? Other examples have similar questions, but Fig. 1 is most obvious.**  \n\nThanks for the question! A simple reason is just that we threshold the color scale, in addition to your points that these shouldn't exactly correspond.\n\n- **Q2: What's the advantage compared to attention based methods?**  \n\nWe clarify our contributions in the general response. In summary, we propose a mutual information estimator that can be easily obtained from denoising diffusion models. With Stable Diffusion, we find our estimator can provide faithful evaluation for compositional understanding, quantify effect sizes of image edits, and localizing phrases in images. \n\n[1] Franzese et al. 2023. MINDE: Mutual Information Neural Diffusion Estimation. ArXiv"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5031/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700214778632,
                "cdate": 1700214778632,
                "tmdate": 1700215284844,
                "mdate": 1700215284844,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bPvtur5SAZ",
                "forum": "X6tNkN6ate",
                "replyto": "aBRMMY2XEB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5031/Reviewer_gEHf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5031/Reviewer_gEHf"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for response"
                    },
                    "comment": {
                        "value": "Thank you for detailed response to all my concerns and questions. \n\nAfter reviewing the rebuttal and other reviews feedback, I decide to increase my score for this paper. \n\nGood work."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5031/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700581803599,
                "cdate": 1700581803599,
                "tmdate": 1700581817270,
                "mdate": 1700581817270,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "EJ9WnYNaZB",
            "forum": "X6tNkN6ate",
            "replyto": "X6tNkN6ate",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5031/Reviewer_S4G8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5031/Reviewer_S4G8"
            ],
            "content": {
                "summary": {
                    "value": "Denoising diffusion models are used for complex data relationships like images and text, and this paper has revealed a link between these models and information decomposition, enabling precise characterization of relationships, including mutual information, which can be used to understand and measure the relationships between specific elements in high-dimensional data like words and image pixels."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper provides a new perspective to explore the relationship between image and text in the Generation field, and quantify compositional understanding capabilities of diffusion models at a certain level."
                },
                "weaknesses": {
                    "value": "However, the primary drawback lies in the paper's somewhat inadequate presentation, making comprehension challenging. Additionally, a limitation is the insufficient experimentation, which fails to demonstrate how this method could enhance the generation process. To conclude, I strongly advise the authors to undergo a significant revision."
                },
                "questions": {
                    "value": "1. Inadequate preliminary information on related information theory makes Section 2 difficult to comprehend.\n2. Clicking on citations does not redirect to the bibliography, making it challenging to locate the correct reference.\n3. Apart from the formations, understanding the implementation of this method is challenging, necessitating more detailed explanations from the authors.\n4. Improved demonstration of how this method enhances the generation process in experiments would be beneficial.\n5. Does Section 6 exceed the limitation of 9 pages?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5031/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5031/Reviewer_S4G8",
                        "ICLR.cc/2024/Conference/Submission5031/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5031/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698843305117,
            "cdate": 1698843305117,
            "tmdate": 1700565539895,
            "mdate": 1700565539895,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1oS9aiJmPO",
                "forum": "X6tNkN6ate",
                "replyto": "EJ9WnYNaZB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5031/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5031/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your critique and detailed list of questions! We hope to resolve your concerns and questions below.\n\n- **W1: However, the primary drawback lies in the paper's somewhat inadequate presentation, making comprehension challenging.**  \n\nWe appreciate your constructive feedback on the presentation. We have updated Section 2 to include more background on information theory, and will aim to improve presentation throughout.\n\n- **W2: Additionally, a limitation is the insufficient experimentation, which fails to demonstrate how this method could enhance the generation process.**\n\nWe apologize for causing confusion! We would like to invite you to read our Paper objectives and Contributions sections in the general response to clarify that enhancing generation is not the goal. In summary, we believe our core contribution is an information-theoretic estimator that can be obtained directly from a denoising diffusion model. While our approach is not geared towards generation, we nevertheless believe this is an important topic: measuring text-image relationships is a challenging task, and existing approaches are often model- or task-specific. Please refer to our general response for additional discussions and results.\n\n- **Q1: Inadequate preliminary information on related information theory makes Section 2 difficult to comprehend.**\n\nWe have updated our manuscript with additional background in information theory in Section 2.1 and in Appendix B.\n\n- **Q2: Clicking on citations does not redirect to the bibliography, making it challenging to locate the correct reference.**\n\nThank you for bringing this to our attention! Indeed, the reference hyperlinks ceased to function when we split the appendix from the main body of our manuscript. We leave this unchanged in our manuscript to prevent excessive triggers of pdfdiff, but will address this issue in subsequent updates.\n\n- **Q3: Apart from the formations, understanding the implementation of this method is challenging, necessitating more detailed explanations from the authors.**\n\nThank you for your interest in understanding our implementation. We have provided code in our supplementary material, and we will release our code on GitHub after the anonymity period.\n\n- **Q4: Improved demonstration of how this method enhances the generation process in experiments would be beneficial.**\n\nPlease see our discussion in the second quote block of this response.\n\n- **Q5: Does Section 6 exceed the limitation of 9 pages?**\n\nAccording to the ICLR 2024 Author Guide (https://iclr.cc/Conferences/2024/AuthorGuide): \u201cThe optional ethic statement will not count toward the page limit, but should not be more than 1 page.\u201d"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5031/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700215248258,
                "cdate": 1700215248258,
                "tmdate": 1700215248258,
                "mdate": 1700215248258,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2GBQPFoP4O",
                "forum": "X6tNkN6ate",
                "replyto": "1oS9aiJmPO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5031/Reviewer_S4G8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5031/Reviewer_S4G8"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the authors' further explanation. After carefully reading the rebuttal and revised paper, all my concerns have been addressed. Especially, sorry for the misunderstanding about the ethics statement. I decided to increase my score, and hope this paper can be published at ICLR.  \n\nOn a side note, in the initial stages, I hesitated to respond to the rebuttal due to a lack of feedback from the reviewers of my own paper, which was a source of frustration. However, I have chosen to adhere to my principles and act as a responsible reviewer, regardless of those guys. Meanwhile, I admire your responsible AC who who has emphasized the importance of the reviewing process."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5031/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700565369440,
                "cdate": 1700565369440,
                "tmdate": 1700565369440,
                "mdate": 1700565369440,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9YDsVttBil",
            "forum": "X6tNkN6ate",
            "replyto": "X6tNkN6ate",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5031/Reviewer_qKVB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5031/Reviewer_qKVB"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces the application of information theory to interpret text-to-image diffusion models. The authors present a way to measure and utilize the mutual information between the image and the text conditioning to visualize \"where\" a specific prompt maps to. They also demonstrate how this mapping from text to pixels can be used to relate images with text and localize words as well as help in interpretability by visualizing the effects of the text prompt in the generation process."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper introduces a novel approach to measuring the relationship between the text conditioning and the image pixels in text-to-image diffusion models. Previous attempts that focused on interpreting the cross-attention operations within the model were architecture-specific and usually required additional processing or fine-tuning to translate attention into meaningful signals. In contrast, the authors show that their information-theoretic approach can be easily interpreted without additional overhead.\n\n- The experiments section clearly demonstrates both the advantages and disadvantages of the proposed approach. The paper paints a clear picture of when the mutual information can be useful and where attention can be used instead.\n\n- The method is well-grounded, with Section 2 presenting the approach and necessary background thoroughly. The paper is overall well-written."
                },
                "weaknesses": {
                    "value": "- The authors present this paper as an approach to interpretability for text-to-image diffusion models but there is no clear demonstration of where this could be applied other than better visualizations of text-image relations. It would be informative to discuss (or even demonstrate) whether the mutual information can be exploited to better align text-to-image models with human preferences during inference time or fine-tuning. The broader scope of interpreting the generation process by relating pixels to words is missing from the paper and could greatly benefit it."
                },
                "questions": {
                    "value": "- How is the attention extracted in Figure 2? The proposed per-pixel mutual information averages the result over all timesteps of the diffusion process. However, the attention seems to be extracted from a single forward pass of the model. That could be disadvantageous as the attention mechanism highlights different scales of features at different timesteps. In section 3.2.2 the authors acknowledge the existence of a  \"feature pyramid\" in the attention but is this again for a single time step of the diffusion process?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5031/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5031/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5031/Reviewer_qKVB"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5031/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698848537225,
            "cdate": 1698848537225,
            "tmdate": 1699636492184,
            "mdate": 1699636492184,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "s3VIhvBvY4",
                "forum": "X6tNkN6ate",
                "replyto": "9YDsVttBil",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5031/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5031/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your insightful comments on interpretability! We agree these are valid concerns, and wish to use this opportunity to discuss them.\n\n- **W1: The authors present this paper as an approach to interpretability for text-to-image diffusion models but there is no clear demonstration of where this could be applied other than better visualizations of text-image relations.**  \n\nThe main value of our approach is for estimating mutual information for relationship testing, which has many applications and for which we get SOTA results in an image-language similarity matching task in Sec. 3.1. A surprising benefit of this particular mutual information method, in stark contrast to other MI estimators, is that it is possible to interpret the discovered relationships at a word and pixel level.\n\n- **W2: It would be informative to discuss (or even demonstrate) whether the mutual information can be exploited to better align text-to-image models with human preferences during inference time or fine-tuning**  \n\nWe invite you to review the table in the Scientific Finding section of our general response. Our INFO metric is more faithful in reflecting the compositional understanding capabilities of diffusion models; it reveals that contemporaneous works underestimate DMs in this department. \n\nWe may presumably adapt our more aligned metric as an unnormalized reward and/or a procedure that filters text-to-image generation results. These filtered results can serve as AI feedback to subsequently improve DMs in compositional understanding via supervised fine-tuning. \n\n- **W3: The broader scope of interpreting the generation process by relating pixels to words is missing from the paper and could greatly benefit it.**  \n\nWe appreciate your perspective! While the focus of this work is on evaluating text-image relationships with our INFO metric, we believe INFO may contribute to the mechanistic interpretability of the generation process as well. A large body of work in mechanistic interpretability of language models identifies circuits by attributing a performance metric to certain network components. For LMs, these metrics are often implemented as total probabilities of desired token predictions [1][2]; yet they remain elusive for diffusion models. We hope the INFO metric can serve as a faithful and theoretically-grounded metric for similar works of mechanistic interpretability for diffusion models, a future direction which we identify in the Conclusion section of our manuscript. \n\n- **Q1: How is the attention extracted in Figure 2? The proposed per-pixel mutual information averages the result over all timesteps of the diffusion process. However, the attention seems to be extracted from a single forward pass of the model. That could be disadvantageous as the attention mechanism highlights different scales of features at different timesteps. In section 3.2.2 the authors acknowledge the existence of a \"feature pyramid\" in the attention but is this again for a single time step of the diffusion process?** \n\nThank you for the question. We use a real image as input, add noise to it, and then denoise it step by step until we get the reconstructed image. The attention heatmaps are generated by the DAAM method [3], which is a wrapper applied on the UNet that tracks the features of the attention layer in the whole denoising process. DAAM extracts features from multi-scale attention layers (feature pyramid) in the UNet at a series of time steps. It upscales all intermediate attention feature arrays to the original image size using bicubic interpolation, then sums them over the heads, layers, and time steps. For a fair comparison, in our experiments, we set the steps as 100.\n\n[1] Wang et al. 2023. Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 Small. ICLR  \n\n[2] Hanna et al. 2023. How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. NeurIPS  \n\n[3] Tang et al. 2022. What the DAAM: Interpreting Stable Diffusion Using Cross Attention. ACL."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5031/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700215546064,
                "cdate": 1700215546064,
                "tmdate": 1700215546064,
                "mdate": 1700215546064,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "X5myY8FJhd",
                "forum": "X6tNkN6ate",
                "replyto": "s3VIhvBvY4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5031/Reviewer_qKVB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5031/Reviewer_qKVB"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed responses to my comments. Given that my concerns were mostly about demonstrating applicability, which I believe you had already done your best to address in the main text and that my score was already positive, I will be keeping my rating as is."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5031/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700578151492,
                "cdate": 1700578151492,
                "tmdate": 1700578151492,
                "mdate": 1700578151492,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7ScLqdfOwT",
            "forum": "X6tNkN6ate",
            "replyto": "X6tNkN6ate",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5031/Reviewer_ajkd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5031/Reviewer_ajkd"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method to visualize the relationship between image and text caption based on the mutual information (MI) estimated by a pre-trained diffusion model. The core observation is that the log probability density can be estimated up to constant by integrating the diffusion-model loss over timesteps (McAllester, 2023), which enables the MI between image and text-caption to be computed by the difference of the estimated log density from the conditional and the unconditional inference result of a text-to-image diffusion model. While the original MI is the average over the samples, the authors consider per-sample (point-wise) MI and further decompose it to per-pixel MI, which gives rise to a visualization of which region in the image is related with a certain word. In experiments, the proposed visualization method showed superior interpretation over the attention-based method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper considers an important topic in text-to-image diffusion models. Analyzing the influence of each word to the generated image has been attracting many researchers, since it serves as an essential tool to come up with ideas for improving or debugging the semantic alignment of the generated image and for developing image-editing methods.\n\nWhile a large body of literature exploits the cross-attention maps for the problem, this paper considers a new approach acts on pixels. The approach itself is quite similar to the classifier-free guidance that contrasts the conditional and unconditional inference results, which has been used in most of the diffusion models, but have strength by grounding on information theory. \n\nAlso, experimental results demonstrate the proposed method gives better interpretability compared to the attention-based methods."
                },
                "weaknesses": {
                    "value": "A weak point of this paper is that the main manuscript seems to be not self-contained. While each of the following can be rather minor, they together drops readability.\n\n- References:\n    - The proposed method is heavily relying on the log-density estimator of McCallester, 2023, but lacks details. Further explanation on the practical behavior of this estimator and how this estimator can be derived, at least in abstract sense, would help the understanding.\n- Rather important figures and tables are in appendix.\n    - There is a fair amount of discussion in the main manuscript with Table 9 and Figure 10, but these are on the appendix thus making difficult to read.\n- Dataset information\n    - CoCo-IT is the main dataset used for evaluation, but how it has been filtered from CoCo is not described at least briefly."
                },
                "questions": {
                    "value": "- Are the \u201cstandard definition\u201d and the \u201corthogonality principle\u201d the full list of the decomposition possibilities? If not, we might need some justification that why these two are better suited as pointwise information.\n    - The main manuscript mentions that both $i^s$ and $i^o$ will be explored. Can you locate where the $i^s$ results are?\n- Is there a notable relationship between $i^o$ and the classifier-free guidance, considering their similarity in computation? For example, if the guidance scale gets larger, does it make a enhanced visualization or a larger MI between the text and image?\n- The Eq. (5) and (6) contains expectation over the noise, $\\epsilon$. But different noises will make dramatically different images. How are the visualizations in Fig. 2 are generated, well aligned with the image?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5031/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698853511424,
            "cdate": 1698853511424,
            "tmdate": 1699636492080,
            "mdate": 1699636492080,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "c5r8wQmk1J",
                "forum": "X6tNkN6ate",
                "replyto": "7ScLqdfOwT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5031/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5031/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for considering our research an important topic and for your insightful comments contrasting our estimators against attention-based methods! Please see our response below.\n\n- **W1: The proposed method is heavily relying on the log-density estimator of McCallester, 2023, but lacks details. Further explanation on the practical behavior of this estimator and how this estimator can be derived, at least in an abstract sense, would help the understanding.**  \n\nThank you for raising this concern. Our information decomposition builds on ITDiffusion [1] rather than McCallester\u2019s work (clarified in the revision). Unlike the approaches of Song, McAllester, and others, our approach does not require complex differential equation solvers for connecting diffusion and density estimation tasks. We added an appendix on the detailed derivation of the density estimator from [1] in an appendix, to make the paper more self-contained.\n\n- **W2: There is a fair amount of discussion in the main manuscript with Table 9 and Figure 10, but these are on the appendix thus making it difficult to read.**\n\nWe appreciate your critique! We have moved part of the results from Table 9 and referred example in Figure 10 to our main body.\n\n\n- **W3: CoCo-IT is the main dataset used for evaluation, but how it has been filtered from CoCo is not described at least briefly.**\n\nWe described the details of the preprocessing steps for the COCO-IT, COCO100-IT, and COCO-WL datasets in Appendix E.\n\n- **Q1: Are the \u201cstandard definition\u201d and the \u201corthogonality principle\u201d the full list of the decomposition possibilities? If not, we might need some justification as to why these two are better suited as pointwise information.**\n\nThis is a good point, it is possible to define a whole continuum of different pointwise estimators by simply adding any zero-mean random variable to the estimate. The \u201cstandard\u201d definition is special by convention, as it is the difference of two density estimates. The \u201corthogonal\u201d definition is special because it is the minimum variance unbiased estimator. \n\n- **Q1.1: The main manuscript mentions that both $i^s$ and $i^o$ will be explored. Can you locate where the $i^s$ results are?**\n\nYes, we have explored $i^s$ and $i^o$ at image-level (Fig. 5) as well as pixel-level (Fig. 8) in the Appendix C.2. \n\n\n- **Q2: Is there a notable relationship between i^o and the classifier-free guidance, considering their similarity in computation? For example, if the guidance scale gets larger, does it make a enhanced visualization or a larger MI between the text and image?**  \n\nThis is a great question. The form of the MI and classifier-free guidance formula both include the difference between a conditional estimator and an unconditional one. In this sense, we can interpret the density model with the modified score function using guidance as being one that artificially boosts the mutual information of the original density function. It would be interesting to explore these types of score-based information manipulations further.\n\n- **Q3: The Eq. (5) and (6) contains expectation over the noise, $\\epsilon$, But different noises will make dramatically different images. How are the visualizations in Fig. 2 are generated, well aligned with the image?**\n\nThanks for your question. The calculation of CMI/MI is just an integral of MMSE differences, and MMSE is an expectation over MSE between gaussian noise and the predicted one, not the noise itself. Additionally, we repeat the denoising on multiple sampled noises.  \n\n[1] Kong et al. 2023 Information-Theoretic Diffusion ICLR"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5031/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700216077247,
                "cdate": 1700216077247,
                "tmdate": 1700216077247,
                "mdate": 1700216077247,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9tqpR4KZ9h",
            "forum": "X6tNkN6ate",
            "replyto": "X6tNkN6ate",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5031/Reviewer_EMcD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5031/Reviewer_EMcD"
            ],
            "content": {
                "summary": {
                    "value": "This work presents a new method for information decomposition at both pointwise and pixel-wise level via well-trained diffusion models, utilizing the results such that there is an exact connection between mutual information and MMSE denoising estimators under Gaussian noise, as well as the exact correspondence between denoising and density modeling. The work demonstrates the ability of such information decomposition to measure the compositional understanding of diffusion models, to perform unsupervised localization of objects in images, and to measure effects of selective image editing via prompt interventions. The proposed metric, *Conditional Mutual Information (CMI) estimators*, is displayed as an alternative to attention in capturing the effect of text interventions in image generation."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The finding that attention to a word doesn\u2019t necessarily imply this word would affect the generated outcome is interesting, and Figure 1 shows how CMI can be helpful in interpreting the model for the cases where attention cannot.\n2. The authors present a standalone section to discuss ethical implications of the models they are working on, as well as how their method could help deal with the potential risks."
                },
                "weaknesses": {
                    "value": "1. The walkthrough of some key results that help construct the proposed information decomposition approach is too brief: it would be nice to have a more in-depth review of these results, for example, the derivation of Eq. (2), the part of diffusion model parameterization that\u2019s related to MMSE, etc. Meanwhile, the experiment section is relatively lengthy: some parts could be moved to the appendix, leaving the contents most closely connected to information decomposition in the main paper.\n2. Error bars in Table 3 for the comparison between CMI and attention are not reported.\n3. Source code is not shared to facilitate reproducibility."
                },
                "questions": {
                    "value": "1. Could the authors give a quick example of how $i^o(\\mathbf{x};\\mathbf{y})$ from Eq. (5) is computed? Under the DDPM parameterization, the noise network predicts the forward noise, thus it makes sense to start with a real image $\\mathbf{x}$, sample $\\mathbf{\\epsilon}$ to obtain $\\mathbf{x}_{\\alpha}$, and compute the L2 term: this utilizes the forward diffusion. Meanwhile, the goal is to decompose the *generated* samples, which uses the reverse diffusion. Therefore, it\u2019s a bit unclear on how these mutual information estimators are calculated.\n2. The authors mentioned that Eq. (2) would hold for an \u201coptimal denoiser\u201d: what does it mean for a denoiser to be optimal?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5031/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5031/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5031/Reviewer_EMcD"
                    ]
                }
            },
            "number": 6,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5031/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699436058627,
            "cdate": 1699436058627,
            "tmdate": 1699636492006,
            "mdate": 1699636492006,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "v88VmXjCOg",
                "forum": "X6tNkN6ate",
                "replyto": "9tqpR4KZ9h",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5031/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5031/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for noting a key difference between our INFO estimators and attention-based methods. We\u2019re encouraged by your kind words on our ethics statement! Please see our response below.\n\n- **W1: The walkthrough of some key results that help construct the proposed information decomposition approach is too brief: it would be nice to have a more in-depth review of these results, for example, the derivation of Eq. (2), the part of diffusion model parameterization that\u2019s related to MMSE, etc. Meanwhile, the experiment section is relatively lengthy: some parts could be moved to the appendix, leaving the contents most closely connected to information decomposition in the main paper.**\n\nThank you for your interest in our theoretical results. We have updated Section 2 to provide additional background on information theory and our derivation.\n\n- **W2:  Error bars in Table 3 for the comparison between CMI and attention are not reported.**\n\nWe have updated our Table 3 (now Table 4) in our manuscript with (standard errors) for pixel-level experiments. We observe INFO and attention-based methods to be relatively stable. We will update image-level results in our subsequent updates.\n\n |           | $i(x; y \\| c)$ |  Attention |\n |:-------------------|:--------:|:--------:|\n |     Image-level     |   **0.50**   |   0.31   |\n |      Pixel-level     |   **0.31** ($\\pm 0.017$)   |   0.27 ($\\pm 0.019$)   |\n\n- **W3: Source code is not shared to facilitate reproducibility.** \n\nWe provide an example implementation of our INFO estimators (MI and CMI) in the supplementary for your reference. We will provide open-source implementations after the anonymity period.\n\n- **Q1: Could the authors give a quick example of how $i^o(x;y)$  from Eq. (5) is computed? Under the DDPM parameterization, the noise network predicts the forward noise, thus it makes sense to start with a real image $x$,  sample $\\epsilon$  to obtain $x_\\alpha$  and compute the L2 term: this utilizes the forward diffusion. Meanwhile, the goal is to decompose the generated samples, which uses the reverse diffusion. Therefore, it\u2019s a bit unclear on how these mutual information estimators are calculated.**\n\nYou say that \"the goal is to decompose the generated samples, which uses the reverse diffusion\", but we want to strongly emphasize that this is **not** the goal. We want to decompose the information in **any image, real or generated**. Therefore, our measure always uses the \"forward diffusion\". That is, we always start with an image, add noise, and then measure the error of the denoiser to recover the signal. This doesn't require access to a reverse diffusion process that generates the image.\n\n- **Q2: The authors mentioned that Eq. (2) would hold for an \u201coptimal denoiser\u201d: what does it mean for a denoiser to be optimal?**\n\nThe 'optimal denoiser' means the denoiser minimizes the MMSE (Eq.1) for recovering $x$ in this noisy channel. We clarified this in the revised text. \n\n[1] Kong et al. 2023 Information-Theoretic Diffusion ICLR"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5031/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700216361015,
                "cdate": 1700216361015,
                "tmdate": 1700216361015,
                "mdate": 1700216361015,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5n49piD3Gy",
                "forum": "X6tNkN6ate",
                "replyto": "v88VmXjCOg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5031/Reviewer_EMcD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5031/Reviewer_EMcD"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for addressing my questions and comments. I decide to keep my current score."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5031/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700595502226,
                "cdate": 1700595502226,
                "tmdate": 1700595502226,
                "mdate": 1700595502226,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]