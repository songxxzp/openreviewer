[
    {
        "title": "You Only Query Once: An Efficient Label-Only Membership Inference Attack"
    },
    {
        "review": {
            "id": "TeoM8R72U4",
            "forum": "7WsivwyHrS",
            "replyto": "7WsivwyHrS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2986/Reviewer_1W2d"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2986/Reviewer_1W2d"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors propose a novel label-only membership inference attack named YOQO. Before querying the target model, the attacker first perturbs the target image in such a way that it produces differing predictions between shadow models trained with and without the target image. Subsequently, the attacker queries the target model with the perturbed image and examines the predicted label. Remarkably, YOQO can achieve state-of-the-art performance with just a single query."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is well-written, making it easy to follow.\n- The results are promising. The method requires only one query yet achieves results comparable to state-of-the-art methods.\n- The extensive ablation studies presented in the paper are commendable, particularly the experiments under multiple defenses.\n- The offline attack demonstrates impressive performance without necessitating the retraining of any shadow models, which is advantageous for practical applications."
                },
                "weaknesses": {
                    "value": "While I'm impressed with the paper, a primary concern is the apparent similarity between the proposed method and the method in [1] from ICLR 2023. The latter utilizes a similar loss to craft queries, enhancing membership inference attacks. Their emphasis is on the threat model that extracts logits from the target model. However, [1] is absent from the paper's references.\n\n[1] Wen, Y., Bansal, A., Kazemi, H., Borgnia, E., Goldblum, M., Geiping, J., & Goldstein, T. (2022). Canary in a Coalmine: Better Membership Inference with Ensembled Adversarial Queries. arXiv preprint arXiv:2210.10750."
                },
                "questions": {
                    "value": "- As mentioned in the weaknesses section, what would be the main difference between the proposed method and the approach outlined in [1]?\n- It's encouraging that the crafted query is transferable. However, if the training algorithm of the target model differs from that of the shadow model, such as having different learning rates or optimizers, would the attack still retain its potency?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2986/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2986/Reviewer_1W2d",
                        "ICLR.cc/2024/Conference/Submission2986/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2986/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698003433026,
            "cdate": 1698003433026,
            "tmdate": 1700400253019,
            "mdate": 1700400253019,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NX1BW89qhy",
                "forum": "7WsivwyHrS",
                "replyto": "TeoM8R72U4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2986/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2986/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "# To Reviewer 1W2d\n## Q1: What would be the main difference between the proposed method and the approach outlined in [1]?\n\nWe apologise for missing this important work in our paper. We will take your advice to cite and discuss this paper. Our attacks are highly different from this paper from the following perspectives.\n\n(1) **Difference in the Threat Models.**\nThe method proposed in [1] is to work in the circumstances where the attacker is able to obtain the ***logits of the model***, whereas our method is exclusively designed for the ***label-only*** situations.\n\n(2) **Difference in the loss function.** \nThe difference in the threat model also leads to the difference in the loss function design. In [1], since the attacker can get the predicted logits, the main purpose of the loss function is to \u2018optimise $x$ so that IN shadow models have low losses on $x$ and OUT models have high losses on $x$\u2019. It directly minimises the ***logits*** of the out models by using the loss term $-\\log(1-f_\\theta(x)_y)$ where $f_\\theta(x)_y$ stands for the ***logits*** corresponding to the ground truth $y$, which will result in a smaller logits, i.e., smaller $f_\\theta(x)_y$.\n\nIn contrast, our attacker can only get the ***hard labels***. Therefore, the goal of our loss function is to optimise $x$ so that it can be correctly classified by IN models while being mis-classified by the OUT models. In this case, it is problematic to just enlarge the variance in the logits, confidence scores or the loss values, as it may not be enough to cause the differences in the predicted labels of the query samples between the in and out models. To solve this problem, we propose to use $CE(f_{out}^i(x),l_i')$, where $l_i'$ is the 'nearest' class of the target sample $x$ other than its ground truth. Unlike $-\\log(1-f_\\theta(x)_y)$, the $CE(f_{out}^i(x),l_i')$ term tends to ensure the ***label difference***. When minimising the cross-entropy, it enlarges the logits of target label l_i\u2019 at the mean time minimising the other logits to guarantee the predicted-label-level differences. Moreover, to carefully choose the target label $l_i\u2019$ helps us to push the query sample into the \u2018improvement area\u2019, i.e. the most sensitive part of the decision boundary towards the membership difference between the in and out models, therefore further enhancing the attack. (We\u2019ve demonstrated the effectiveness of choosing target labels in the ablation study of the manual.)\n\n(3) **Difference in the attack effectiveness**\nTo further see the differences, we migrate the method in [1] to the hard label scenario, and measure the attack effectiveness. We adopt the method in [1] for the query-sample-generation (Algorithm 1 in our paper), while the rest parts are the same. We perform evaluations on CIFAR10 (2,500 samples) using CNN7. The rest of the settings are the same as the experiments in the paper. The results are shown in the table below. It is clear that our methods are much more effective than [1] in terms of label-only MIA. \n\nLoss term|Inference Accuracy\n---|---\nYOQO Online| 81.65%\nCanary[1] Online| 73.27%\nYOQO Offline| 79.19%\nCanary[1] Offline| 71.54%\n\n# Q2: It's encouraging that the crafted query is transferable. However, if the training algorithm of the target model differs from that of the shadow model, such as having different learning rates or optimizers, would the attack still retain its potency?\n\nWe conduct experiments using victim models trained with various settings that are different from the shadow models. Specifically, we test different optimizers, batch sizes, and learning rate. All the experiments are conducted using the same setting in the ablation studies. The results are shown below. \n\nOptimizer of shadow models|Optimizer of target models|Batch Size|Learning Rate|Gap Attack|Online Attack|Offline Attack\n---|---|---|---|---|---|---\nAdam|SGD|128|0.01|68.70%| 80.97% | 78.35%\nAdam|SGD |64 |0.005|69.05%|79.23%|77.59%\nAdam|AdamW|128|0.01 |67.50%| 80.01% |77.43%\nAdam|AdamW |64 |0.005|68.38%|79.31%|76.92%\n\nWe observe that the different training details such as batch size and optimizer have slight impacts on the effectiveness of our methods. ***The crafted query samples from YOQO have high transferability.***\n\n## Reference:\n[1] Wen, Y., Bansal, A., Kazemi, H., Borgnia, E., Goldblum, M., Geiping, J., & Goldstein, T. (2022). Canary in a Coalmine: Better Membership Inference with Ensembled Adversarial Queries. arXiv preprint arXiv:2210.10750."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2986/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700389805748,
                "cdate": 1700389805748,
                "tmdate": 1700389805748,
                "mdate": 1700389805748,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IQk3nu7BZz",
                "forum": "7WsivwyHrS",
                "replyto": "NX1BW89qhy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2986/Reviewer_1W2d"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2986/Reviewer_1W2d"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response."
                    },
                    "comment": {
                        "value": "I greatly appreciate the authors' detailed explanations and additional experiments. The proposed method is clearly superior to [1], though they share a similar concept. Moreover, its robustness to various training algorithms further solidifies the work. Therefore, I have raised my rating score accordingly."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2986/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700400238909,
                "cdate": 1700400238909,
                "tmdate": 1700400238909,
                "mdate": 1700400238909,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3Ma6AHWhWQ",
            "forum": "7WsivwyHrS",
            "replyto": "7WsivwyHrS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2986/Reviewer_1cwK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2986/Reviewer_1cwK"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces YOQO, a novel label-only membership inference attack designed to address privacy threats in machine learning models. YOQO's key innovation lies in its identification of an \"improvement area\" around a target sample, enabling the crafting of query samples with hard labels that effectively determine the target sample's membership, significantly reducing the query budget required from over 1,000 queries to just one query. The study demonstrates that YOQO exhibits effectiveness comparable to state-of-the-art Membership Inference Attacks (MIA) while demonstrating greater resilience against various defense mechanisms. This underscores its significance in the context of privacy attacks on machine learning models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- YOQO introduces a label-only membership inference attack, reducing the query budget from over 1,000 queries to just one query.\n- The attack demonstrates effectiveness on par with state-of-the-art MIA methods, indicating its practical relevance.\n- YOQO exhibits greater robustness against various defense mechanisms, underlining its potential for real-world applications."
                },
                "weaknesses": {
                    "value": "- The use of accuracy as the primary evaluation metric is questioned, as it may not reflect worst-case performance in MIA.\n- The potential for overfitting in the experimental setting due to a small training dataset (2,500 samples) raises concerns about the generalizability of the results.\n- While the transformation of Equation 1 into a minimization problem is justified, the exploration of alternative techniques, such as introducing a weight term, could enhance the paper's depth and robustness."
                },
                "questions": {
                    "value": "Two noteworthy concerns arise in the evaluation of the paper.\n\nFirst, in terms of the chosen evaluation metric, the authors have opted for accuracy to assess the performance of their attack. However, there are concerns about the appropriateness of this choice. While I agree that using a log-scale ROC curve may not be practical for this case, the fundamental issue at hand pertains to the need for evaluating the worst-case scenario for MIA. Based on my understanding, samples closer to the decision boundary should have a higher probability of being classified as members, while those significantly distant from this boundary may exhibit a lower likelihood of membership. Consequently, assessing accuracy across all samples may not effectively unveil the worst-case performance. To address this, a suggestion is made to focus on accuracy calculations for samples located farthest from the decision boundary, even though the attacker may lack knowledge of the specific location of such samples. The presence of ground truth information during the evaluation phase supports the feasibility of this approach.\n\nSecond, the experimental setting raises concerns regarding overfitting. The target model is trained with a relatively small dataset of 2,500 samples, which could potentially result in severe overfitting. Although the paper does not explicitly state the training and testing accuracy gap for the target model, it can be inferred from the GAP attack that the overfitting level exceeds 40%, potentially impacting the reliability of the conclusions. I would like to see the authors conduct experiments on well-generalized models, involving a larger training dataset to mitigate the risk of overfitting.\n\nIn Section 3.2, when the authors optimize the x to get the improvement area, they believe that Equation 3 won\u2019t work since scalars in both in models and out models are in [0,1]. I agree, and to address this constraint, the authors adapt Equation 1 into a minimization problem, which is a common practice in finding adversarial examples. Just out of curiosity, I wonder whether adding a weight term $\\lambda$ to Equation 2 could achieve the same result, that is, turn the output of in models to [0,$\\lambda$]."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2986/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2986/Reviewer_1cwK",
                        "ICLR.cc/2024/Conference/Submission2986/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2986/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698328654788,
            "cdate": 1698328654788,
            "tmdate": 1700588184327,
            "mdate": 1700588184327,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "q9jFFtf6v5",
                "forum": "7WsivwyHrS",
                "replyto": "3Ma6AHWhWQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2986/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2986/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "# To Reviewer 1cwK\n## Q1: The use of accuracy as the primary evaluation metric is questioned, as it may not reflect worst-case performance in MIA.\n\nWe appreciate your suggestions on how to show the worst-case performance in MIA, which is really inspiring. Following your suggestions, we use the distance provided by boundary attack [1] as the metric to pick the worst-case samples, which directly measures the distance (L0, L1, L2, L $\\infty$) in the space of the target samples. W e calculate the distance on 5 victim models and take the averages as the distance. Then we select the top 10% samples (200 samples) with the largest distance. The results are shown below:\n\nModel Type|L0|L1|L2|L $\\infty$\n---|---|---|---|---\nGap Attack |50.0%|50.0%|50.0%|50.0%\nBoundary Accuracy| 52.7% | 54.0% | 54.1% | 53.3%\nOnline Accuracy | 65.3% | 61.0% | 61.22% | 61.5%\nOffline Accuracy| 60.0% | 57.7% | 56.5% | 59.5%\n\nWe believe this result can reflect the worst-case situation to some extent. We observe that ***our methods tend to have better performance than the boundary attack. This indicates that YOQO can be more effective to infer the membership of the worst-case samples which the boundary attack is hard to deal with.*** To strengthen the solicity of our work, we will add the discussions in the revision. \n\n## Q2:  The target model is trained with a relatively small dataset of 2,500 samples, which could potentially result in severe overfitting. I would like to see the authors conduct experiments on well-generalised models, involving a larger training dataset to mitigate the risk of overfitting.\n\n(1)We would like to clarify that ***we varied the size of the training dataset from 1,500 to 10,000 in the paper to show how the overfitting would affect the attack performance.*** The small dataset of 2,500 samples is only used for defence evaluation. As shown in Fig.2, the smallest gap between the test and train set is around 24% (training set accuracy = 98%, test set accuracy = 74%)\n\n(2)Following your suggestions, we further do the experiments using bigger datasets (25,000 samples of CIFAR10 and 75,000 samples of Tiny-ImageNet) to reduce the overfitting. To make the target model more well-generalised, we also conduct experiments with models pretrained on Imagenet. The results are shown in the table below. ***In all cases, YOQO retains its comparable performance as the boundary attack.***\n\nDataset|Datasize|Model Type|Gap Attack|Boundary Attack|YOQO Online|YOQO Offline\n---|---|---|---|---|---|---\nCIFAR-10|25,000|CNN7 from Scratch|61.62%| 74.34% | 75.03%|73.88%\nCIFAR-10|25,000|ResNet34 Pretrained on      Imagenet|56.2%|63.01%|62.27%|60.34%\nTiny-Imagenet|75,000|ResNet34 Pretrained on Imagenet|65.2%|73.21%|73.15%|70.47%\nTiny-Imagenet|75,000|ResNet34 From Scratch|75.7%|81.03%|80.94%|77.65%\n\n(3)Additionally, to explore the impact of overfitting on attack effectiveness, we conduct the experiments using L2 regulation to further reduce the overfitting. The results are shown below. ***Our attacks are still comparable with SOTA attacks but with much more efficiency (query only once).***\n\nRegulation Param $\\lambda$|Gap Attack|YOQO Online|YOQO Offline|Boundary Attack\n---|---|---|---|---\n0.01| 60.10% | 72.32% | 69.91% | 71.75% \n0.02| 54.13% | 68.70% | 66.47% | 70.03%\n0.04| 52.32% | 57.71% | 55.70% | 55.65%\n\nWe will add all the discussion and evaluation results of overfitting in the revised paper. \n\n## Q3: Whether adding a weight term to Equation 2 could achieve the same result, that is, turn the output of in models to [0, $\\lambda$]. \n\nIndeed, having a weight term $\\lambda$ can scale the output of in models to [0, $\\lambda$], which may ameliorate the imbalance between the two terms. However, the real problem lies in the range of $CE(\\frac{\\lambda \\cdot f_{in}}{f_{out}})$. For any given $f_{in}$, the loss $CE(\\frac{\\lambda \\cdot f_{in}}{f_{out}}, l)$ ranges in $(-\\infty, CE(\\lambda \\cdot f_{in}, l)]$, which means for any threshold $t$, we can theoretically find a $f_{out}$, so that $CE(\\frac{\\lambda \\cdot f_{in}}{f_{out}})<t$, indicating the algorithm is not convergent. This means we cannot simply set a threshold on the total loss to decide when the algorithm should stop. In this case, we need to carefully choose the total iterations of the optimization as well as the $\\lambda$ to ensure the acceptable inference accuracy, which is not that practical in the real word setting, and also means it is a sub-optimal choice (for the best $\\lambda$ and iteration numbers can vary from samples to samples). Below we do the experiments to demonstrate the our opinion. We varies the $\\lambda$ from 1 to 16. The results indicate that the performance in this case is more dependent on the $\\lambda$, and is less effective in comparison with the original loss as in Fig.6 shows in the paper, which used the same settings of this experiment, i.e., 2,500 samples on CIFAR10.\n\n$\\lambda$|1|2|4|8|16\n---|---|---|---|---|---\nInference Accuracy|72.13%|74.15%|75.25%|75.21%|75.07%"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2986/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700389700166,
                "cdate": 1700389700166,
                "tmdate": 1700391335218,
                "mdate": 1700391335218,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eyj8X4n3UH",
                "forum": "7WsivwyHrS",
                "replyto": "q9jFFtf6v5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2986/Reviewer_1cwK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2986/Reviewer_1cwK"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response! My concerns are adequately addressed, therefore I raise my score from 6 to 8."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2986/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700588162190,
                "cdate": 1700588162190,
                "tmdate": 1700588162190,
                "mdate": 1700588162190,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qtudLaIHhj",
                "forum": "7WsivwyHrS",
                "replyto": "3Ma6AHWhWQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2986/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2986/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the suggestions to enhance the robustness and rigor of our work. Please feel free to reach out if any further concerns arise."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2986/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700627532210,
                "cdate": 1700627532210,
                "tmdate": 1700628161108,
                "mdate": 1700628161108,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DNFSmv7RMy",
            "forum": "7WsivwyHrS",
            "replyto": "7WsivwyHrS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2986/Reviewer_bo89"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2986/Reviewer_bo89"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes YOQO a label-only MIA which, unlike prior SOTA label-only MIAs, queries the target model only once using a specific sample x\u2019 derived from the target sample (x, l) with the goal of determining the membership of (x, l). YOQO finds the query sample x\u2019 in the improvement region which is the difference in the decision boundaries due to insertion of (x, l) in the training data. YOQO proposes an optimization to find x\u2019 that maximizes the error of OUT models on x\u2019 and minimize the error of IN models on x\u2019, and solves it using gradient decent. Evaluations show that YOQO is as effective or better than the existing label only MIAs on multiple datasets/model architectures."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- YOQO is a novel idea that reduces query budget required for MI\n- Idea is well explained and paper is easy to read.\n- Experiments are well designed to demonstrate the efficacy of the attack"
                },
                "weaknesses": {
                    "value": "- YOQO evaluations consider very small datasets and training data sizes\n- Models might be overfitted on training data\n- Unclear how this attack will work in practical settings"
                },
                "questions": {
                    "value": "This paper proposes a very novel YOQO attack and I think the paper also does a good job of explaining and evaluating the attack. The evaluations clearly show that YOQO is the new SOTA label only MIA. I have the following few concerns about the paper:\n\n- Although the experimental setup considered is the common setup in most prior works, it contains mostly small datasets with very small training dataset sizes. Can authors perform experiments on larger datasets, e.g., Imagenet? \n- I could not figure out the training recipe from the main paper. In particular does the training ensure that the models don\u2019t overfit to the training data, e.g., using L2 regularization or any other common regularization techniques? I feel the criterion that training goes on until a model has >98% accuracy on training data may lead to overfitting that will lead to unnecessarily stronger MIAs.\n- On the same lines as the above comment, can authors add a few more details of various dataset sizes used in training with/without defenses, e.g., adversarial regularization? Also can they add details of training procedure? These are important given that MIA efficacy is greatly affected by these factors.\n- Utility of the attack: I could not understand from the current evaluations how the attack will perform in the real world. The dataset sizes are quite small which is seldom the case now a days, unless if model is fine-tuned using a small dataset. Can authors provide results for some real-world settings? Some suggestions: 1) use larger datasets 2) fine-tune a model pre-trained on large datasets?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2986/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2986/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2986/Reviewer_bo89"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2986/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698635563222,
            "cdate": 1698635563222,
            "tmdate": 1699636242939,
            "mdate": 1699636242939,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3RFwdzsUDe",
                "forum": "7WsivwyHrS",
                "replyto": "DNFSmv7RMy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2986/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2986/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "# To Reviewer bo89: (1/2)\n## Q1: Although the experimental setup considered is the common setup in most prior works, it contains mostly small datasets with very small training dataset sizes. Can authors perform experiments on larger datasets, e.g., Imagenet?\n\nResponse: \n\nThanks for the great suggestion. ***The size of the training dataset used in the current manuscript ranges from 1,500 to 10,000, with various data types (e.g., images, tables). These are indeed common setups in existing MIA works.*** Following your suggestion, we'd like to extend the evaluation to larger datasets. Performing evaluations on Imagenet requires quite a long time, especially the online attack, which will exceed the response deadline. Alternatively, we conduct extra experiments on Tiny-Imagenet, a subset of Imagenet which contains over 100k images of 64 $\\times$ 64 size belonging to 200 categories. The evaluation results can be found in our response to your Q4. Our method remains effective in larger datasets. The results can be found in the second part of the response.\n\n## Q2: I could not figure out the training recipe from the main paper. In particular, does the training ensure that the models don\u2019t overfit to the training data, e.g., using L2 regulation or any other common regulation techniques? I feel the criterion that training goes on until a model has >98% accuracy on training data may lead to overfitting that will lead to unnecessarily stronger MIAs.\n\nResponse:\n(1)When training the models, we indeed used the L2 regulations and set the regulation parameter $\\lambda$ to be 5e-04. The influence of L2 regulation may not be strong enough to entirely prevent overfitting. Yet it is a common setting for training models on the datasets we use, and we believe the overfitting is reasonable, the same as other MIA works. \n\nTo demonstrate the effectiveness of YOQO on less overfitting models, we further perform evaluations on models trained with stronger L2 regulations. Specifically, we use CNN7 as both the shadow and victim model, over 2,500 CIFAR-10 images as the training set. The results are shown below. As the weight decay $\\lambda$ increases, the effect of the L2 regulations get stronger, resulting in  performance degradations in all of the attacks. Nevertheless, ***YOQO still keeps comparable performance as the Boundary attack.*** Additionally we also notice that strong L2 regulation makes the training very unstable, and also causes performance degradations on the test accuracy.\n\nRegulation Param $\\lambda$|Gap Attack|YOQO Online|YOQO Offline|Boundary Attack\n---|---|---|---|---\n0.01| 60.10% | 72.32% | 69.91% | 71.75% \n0.02| 54.13% | 68.70% | 66.47% | 70.03%\n0.04| 52.32% | 57.71% | 55.70% | 55.65%\n\n(2) For the condition of training termination, we would like to clarify that ***we did not take the training set accuracy as the criterion to stop training.*** Instead, for each task and dataset, ***we stop the training when the model achieves the best performance on the validation set.*** Such models will also get >98% accuracy on the training set. Sorry for the confusion, and we will add all the training details in the revised paper. \n\n(3)Additionally, the models evaluated in our paper are actually less overfitting in comparison with the Boundary attack paper [1]. For instance, using 2,500 samples from CIFAR10 for training, the ASR of gap attack in our work is around 70%, while that of [1] is around 75% (Fig.1 of [1]). As the gap attack is correlated to the gap between the performances on the train and test sets, a lower ASR in gap attack means our models are less overfitting in comparison with that being tested in the Boundary attack paper [1].\n\n## Q3: On the same lines as the above comment, can authors add a few more details of various dataset sizes used in training with/without defences, e.g., adversarial regulation? Also can they add details of training procedure? These are important given that MIA efficacy is greatly affected by these factors.\n\nResponse:\nThank you for pointing this out. We are sorry for missing the training details due to the page limit. In our paper, ***for all the defence evaluations, we use 2,500 CIFAR-10 samples as the training dataset. For the attack effectiveness evaluations, we use different datasets with different numbers of samples from 1,500 to 10,000.***  These details can be found at the beginning of Sections 4.2 and 4.3. Our training settings are shown below:\n\nDataset|Model|Learning Rate|Optimizer|Batch Size|Training Epoch\n---|---|---|---|---|---\nCIFAR10/CIFAR100/gtsrb/svhn|CNN7|0.001|Adam|128|30\nCIFAR10/CIFAR100/gtsrb/svhn|VGG16|0.001|Adam|64|50\nCIFAR10/CIFAR100/gtsrb/svhn|ResNet18|0.001|Adam|64|20\nCIFAR10/CIFAR100/gtsrb/svhn|DenseNet121|0.001|Adam|128|30\nCIFAR10/CIFAR100/gtsrb/svhn|InceptionV3|0.001|Adam|128|25\nCIFAR10/CIFAR100/gtsrb/svhn|SeResNet18|0.001|Adam|128|20\nPurchase100/location|ColumnFC|0.001|Adam|32|30\n\nWe will clarify those details in the revised manuscript."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2986/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700388349406,
                "cdate": 1700388349406,
                "tmdate": 1700388349406,
                "mdate": 1700388349406,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0Oe1dw5m9w",
                "forum": "7WsivwyHrS",
                "replyto": "DNFSmv7RMy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2986/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2986/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "# To Reviewer bo89: (2/2)\n\n## Q4: Utility of the attack: I could not understand from the current evaluations how the attack will perform in the real world. The dataset sizes are quite small which is seldom the case nowadays, unless the model is fine-tuned using a small dataset. Can authors provide results for some real-world settings? Some suggestions: 1) use larger datasets 2) fine-tune a model pre-trained on large datasets?\n\n\nThank you for the suggestions. We follow them to evaluate our attacks on these new settings. Particularly, 1) we use larger training datasets with higher resolutions and more samples (75k Tiny-ImageNet, 25k CIFAR-10); 2) we consider both training from scratch and fine-tuning a public model pre-trained on ImageNet. The inference accuracy of different attacks are shown in the following table. We can observe that our attacks maintain comparable accuracy as SOTA attacks, but much more efficient (only query once). This conclusion is consistent with the evaluations in the paper, demonstrating the practicality and utility of our attacks in a wider set of configurations. \n\nDataset|Datasize|Model Type|Gap Attack|Boundary Attack|YOQO Online|YOQO Offline\n---|---|---|---|---|---|---\nCIFAR-10|25,000|CNN7 from Scratch|61.62%| 74.34% | 75.03%|73.88%\nCIFAR-10|25,000|ResNet34 Pretrained on      Imagenet|56.2%|63.01%|62.27%|60.34%\nTiny-Imagenet|75,000|ResNet34 Pretrained on Imagenet|65.2%|73.21%|73.15%|70.47%\nTiny-Imagenet|75,000|ResNet34 From Scratch|75.7%|81.03%|80.94%|77.65%\n\n\n\n## References:\n\n[1] Christopher A Choquette-Choo, Florian Tramer, Nicholas Carlini, and Nicolas Papernot. Label-only membership inference attacks. In International conference on machine learning, pp. 1964\u20131974.\nPMLR, 2021."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2986/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700388402294,
                "cdate": 1700388402294,
                "tmdate": 1700388402294,
                "mdate": 1700388402294,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3tN4X1fdAQ",
                "forum": "7WsivwyHrS",
                "replyto": "0Oe1dw5m9w",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2986/Reviewer_bo89"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2986/Reviewer_bo89"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response!"
                    },
                    "comment": {
                        "value": "Thanks for the detailed response and new results. I have a few more questions:\n\n- in [1], Figure 2-c, I see that for cifar10 trained on 2500 training samples, boundary distance attack with random noise based augmented samples achieves close to 80% accuracy with just 10 queries. Why are your results so different?\n\n- current threat model assumes that the attacker has data from the exact same distribution as the training data of the target model. I feel this is quite a strong assumption that even some of the previous works make. Can you comment on how practical this is and how would the attack perform if the attacker does not have such access? I think it is important to understand how these attacks will work in practice. \n\n- in the 'Hyper-parameter in loss functions' section: it says that with increasing alpha, there are fewer FPs. If i understand correctly, this means you can keep increasing \\alpha to get low FPRs and measure TPRs at low FPRs?\n\n- I guess the idea here is to query the target model only once to infer membership and looks like the attack does close to boundary attack. But, if the query budget is higher, e.g., 10 queries, will this attack be able to surpass boundary attack, e.g., in case of Purchase100? How would attacker design those queries?\n\n- i see that offline attack many times outperforms the online attack; why is this the case?\n\nComments:\n- Add model archs in Table 3 and training data size in caption of table 2."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2986/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700594261128,
                "cdate": 1700594261128,
                "tmdate": 1700594261128,
                "mdate": 1700594261128,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]