[
    {
        "title": "Measuring Information in Text Explanations"
    },
    {
        "review": {
            "id": "O0fbdBuewq",
            "forum": "bpheRCxzb4",
            "replyto": "bpheRCxzb4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4543/Reviewer_acAy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4543/Reviewer_acAy"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a framework for evaluating textual explanations (rationale and free-text rationale).\n\nBut I still have several questions about this paper.\n\n1. The authors use the method of mutual information to assess the information of two text explanations, which is a common and intuitive practice. My confusion lies in how the authors estimate this mutual information. As far as I understand, using InfoNCE to estimate mutual information should require training. What is the training data? What is the training process?\n\n\n2. I notice that the authors have compared several mutual information estimators in the appendix, such as club, smile, InfoNCE. Here I still have questions about the club method. According to the paper of club, the value estimated by club should be higher than the real mutual information, and thus it should be higher than the value estimated by InfoNCE, how do the authors judge that the value estimated by InfoNCE is more accurate?\n\n\n3. Why use V-information instead of the traditional mutual information method?\n\n4. Does this method depend on the accuracy of the mutual information estimator? What are its advantages over other evaluation methods (e.g., traditional metrics for evaluating NLE (rouge))?\n\n5. In summary, I think this paper has limited innovation and usefulness.\n\nFinally, for ICLR submissions, the appendix could be placed after the main text, not separately in the supplementary material."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "See Summary for details."
                },
                "weaknesses": {
                    "value": "See Summary for details."
                },
                "questions": {
                    "value": "See Summary for details."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4543/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4543/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4543/Reviewer_acAy"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4543/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698068149968,
            "cdate": 1698068149968,
            "tmdate": 1700125276424,
            "mdate": 1700125276424,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Vzn1b9GpUG",
                "forum": "bpheRCxzb4",
                "replyto": "O0fbdBuewq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4543/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4543/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response by authors"
                    },
                    "comment": {
                        "value": "Thank you for reading this paper and providing comments. To the points you mentioned:\n\n- **What is the training data and procedure for InfoNCE?** We have train - validation - test splits in the dataset (Appendix A.1 elaborates more on the details). The InfoNCE, as well as other estimators, are trained on the train set.\n- **About the CLUB method, and why preferring InfoNCE over CLUB?** Appendix A.2 shows the procedure of finding one estimator among the collections. On correlated Gaussian, the mean squared errors of any estimators except DoE were acceptable. However, on e-SNLI, other estimators have much larger variance on the validation set than InfoNCE. This motivates us to choose InfoNCE rather than CLUB on the experiments (which uses e-SNLI problems).\n- **Why use V-info vs MI estimation method?** V-info has different applicability from the MI estimation problems. V-info is suitable for \u201cvector-to-scalar\u201d data, whereas MI estimators (CLUB, DoE, InfoNCE, MINE, NWJ, SMILE, etc.) are for \u201cvector-to-vector\u201d data. Sections 3.4 and 3.5 contains more formal elaborations.\n- **Does this method depend on the accuracy of MI estimator?** Yes. This method also depends on how well the embeddings encode the related information.\n- **Proposed information-theoretic scores vs other metrics?** Each metric provides a unique perspective. Our scores take root in communication channels and information theory, and reveal mechanisms about the NLEs that we consider to be quite interesting. One benefit of our score is the flexibility and the potential to generalize across multiple modalities (given suitable embedding).\n- **About the innovation and usefulness.** We consider the innovation and usefulness mainly in two aspects: (1) A step towards unifying the evaluations of two different types of text explanations. (2) Some insights about the mechanisms of the explanations, e.g., by the informativeness-relevance trade-off."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4543/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699912540566,
                "cdate": 1699912540566,
                "tmdate": 1699912540566,
                "mdate": 1699912540566,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jQZ8cAN80F",
                "forum": "bpheRCxzb4",
                "replyto": "Vzn1b9GpUG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4543/Reviewer_acAy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4543/Reviewer_acAy"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the reply, authors solved my problem mostly. \n\nBut I am still confused about the question \"About the CLUB method, and why preferring InfoNCE over CLUB\". The authors explained that CLUB has much larger variance on the validation set than InfoNCE from the perspective of the experiment.\nDifferently, I think this may be because CLUB itself is used to estimate the upper bound of MI, while InfoNCE is used to estimate the upper bound of MI. Besides, as the batch increases, the value of MI_InfoNCE tends to log(N) ( N is the size of the batch), which may be the reason why InfoNCE has lower variance. Can the authors theoretically analyze why CLUB has much larger variance on the validation set than InfoNCE from a similar perspective?"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4543/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699942585293,
                "cdate": 1699942585293,
                "tmdate": 1699942585293,
                "mdate": 1699942585293,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2BmvxXnLKP",
                "forum": "bpheRCxzb4",
                "replyto": "O0fbdBuewq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4543/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4543/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Some analysis following this line of thought"
                    },
                    "comment": {
                        "value": "Thanks for the recommendation. Here is an attempt following this line of thought. (Note that we do not have as strong background to carry out the theory analysis to the same level as the authors of the CLUB paper and many other papers that propose information estimator algorithms.)\n\nMany other estimators have larger variances than InfoNCE, echoing the results shown in e.g., figure 1 of the CLUB paper. McAllester and Stratos (2020) showed that lower bound MI estimation from N samples can\u2019t be larger than O(log N), and InfoNCE is one of the lower bounds. We used batch sizes of 64 (except for NLE-roberta where we used 32), so the upper bound on the estimated amount is around a constant times 4 nats. This explains why even the most informative explanations have I(X;E) of around 4 nats (in Figures 4 and 5). Also, selecting the estimator by the least variance doesn't seem to be the optimal criterion (while it's the most convenient criterion in the absence of ground truth). Comparing the behavior of CLUB to InfoNCE, and other estimators on the explanation data has a lot of research opportunities left in the future works. We will add these discussions to the papers."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4543/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700084979395,
                "cdate": 1700084979395,
                "tmdate": 1700085046162,
                "mdate": 1700085046162,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LWoMyDD5ZS",
                "forum": "bpheRCxzb4",
                "replyto": "2BmvxXnLKP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4543/Reviewer_acAy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4543/Reviewer_acAy"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response. I hope that in the revised version, the authors can combine the theory of mutual information estimation and experiments to show why preferring InfoNCE over CLUB. It will be beneficial for the reliability of the proposed metrics.\n\nFinally, I will improve my score."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4543/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700125253893,
                "cdate": 1700125253893,
                "tmdate": 1700125253893,
                "mdate": 1700125253893,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cLqz3Zlbzq",
            "forum": "bpheRCxzb4",
            "replyto": "bpheRCxzb4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4543/Reviewer_7iHa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4543/Reviewer_7iHa"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to view text explanations in an information theory based framework. This paper focuses on the rationale and NLE types of text explanations for text classification (specifically the e-SNLI dataset). After formulating the investigated cases within the information theory based framework, the authors propose to measure the mutual information (1) between the input and the explanan (called relevance score) and (2) between the explanan and the target (called informativeness score), and analyze the correlation among the proposed relevance score, informativeness score, and multiple proposed silver labels. The goal of this paper, to my understanding, is to propose the relevance and informativeness scores for future explanation evaluation."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "* The way of forming the text explanation within the information theory framework is interesting.\n* The experiments have included three embedding models: RoBERTa, OpenAI, Cohere, to test the generalizability of the proposed relevance and informativeness scores, which highly depends on the used embeddings.\n* The experiments have considered two often-seen types: the rationale, which is defined to include the tokenwise explanation, and the NLE, which is defined to be explaining the input-target relationship via natural language description."
                },
                "weaknesses": {
                    "value": "* The writing can be improved: The goal of this paper can be more clear. I was lost in the middle of the paper and wondered (1) which parts are used for evaluation and (2) what are they truly evaluating for? I could only realize the goal after reading through the whole paper and gave it a guess.\n* This paper claims to unify the evaluation of rationale and natural language explanations. Unification would make me expect the proposed evaluation method can evaluate them in a good standing. However, the results turn out the proposed metrics do not have a consistent meaning for human interpretation and meanwhile there is no user study to validate this.\n* From the results, the most consistent part is that the proposed relevance score I(X;E) is correlated with the type overlap ratio and embedding similarity. Nonetheless, since explanan often includes (similar) words in the input, the result is not giving new insights."
                },
                "questions": {
                    "value": "* Is there a reason for choosing embeddings for computing the entropy? What do the authors think about using the probability distribution to compute the entropy?\n* Please add proper reference to the statistics in the paper:\n  *In section 4.3: Where are the statistics for the statement \u201cOn the explanations in natural language, most inter-score correlations are low, indicating that these scores measure a diverse collection of aspects.\u201d? The authors have pointed to Appendix A.5, but it could be better to point to Figure7. Also, I would suggest this statistics be moved to the main content.\n  * In section 5.1: How do the authors compute the statistics \u201cThe reasoning category scores can explain 16% and 21% of the variance in the estimated I(Y ; E) for OpenAI and RoBERTa on NLE (18% and 19% for rationale), and no more than 15% for any other categories.\u201d?\n  * In section 5.2: The authors do not put any reference to statistics in this section. I guess the authors are referring to Figure 4, 5, and 6. I would also suggest Figures 5 and 6 be moved to the main content. To comprise the paper space, I would think that prompt templates and discussion about multimodal can be moved to appendix.\n* I would suggest changing the naming of \u201cinformativeness\u201d in the GPTScore evaluation items. This term is the same as the proposed \u201cinformativeness score I(Y;E)\u201d, so can cause confusion.\n* Why is there no \u201cinformativeness\u201d from Table1 in Figure3\u2019s x-axis?\n* I would suggest changing the order of x-axis in Figure3 to match the order of them in Table1."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4543/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4543/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4543/Reviewer_7iHa"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4543/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698779496088,
            "cdate": 1698779496088,
            "tmdate": 1700681013187,
            "mdate": 1700681013187,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pXmSkA6N77",
                "forum": "bpheRCxzb4",
                "replyto": "cLqz3Zlbzq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4543/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4543/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response by authors"
                    },
                    "comment": {
                        "value": "Thank you for considering this paper interesting, and for commending on the empirical solidity of this paper! To the points mentioned in weaknesses and questions:\n\n- **The evaluation plan and goals.** We added the experiment protocol and goals to the first paragraphs in 5.1 to 5.3. We also updated the pdf to make it more organized.\n- **Only automatic and no human evaluations?** Yes \u2014 human eval of explanation is a huge topic. Each format of explanation format (NLE or rationale), and each topic of the explanation contents can have a unique impact to the audience [1]. Also the situation of the audience matters [2]. The human evaluations in a careful enough manner would lead to the scales of experiments that significantly go beyond the scope of this paper, so we can only focus on the automatic evaluation perspective.\n- **New insights beyond the correlations to some scores?** There are at least three insights beyond correlations: (1) With ANOVA, we quantify how much each aspect (lexical-semantic, reasoning, clarity, relevance) can explain the variation in the measured information scores. (2) We show a tradeoff between relevance-informativeness phenomenon. (3) We illustrate that the openai and cohere embeddings preserve the distributions, whereas the roberta embeddings focus more on the decision boundary.\n- **Why choose embeddings to compute the entropy?** Embedding is the crucial component that allows multiple types of explanations (NLE, rationale, and potentially other types) to be comparable on the same ground. The pipelines involving conditional probabilities usually need another module that computes the one-dimensional probability value from the embeddings.\n- ******************************************************Comments about the formats:******************************************************\n    - Reference for \u201cInter-score correlations are low\u201d: Indeed. They are from Figure 7. We updated the references in the pdf to make this clearer.\n    - References for the ANOVA results: We updated the references to the ANOVA details to make this clearer.\n    - Figures 4-6, and moving some statistics to the main content. We moved around some parts. Figures 5-6 takes more space than the prompt templates (which we also consider quite important, since LLM\u2019s behavior is heavily affected by the actual wording of the prompts) so we are keeping them at the current version.\n    - Naming confusion for \u201cinformativeness\u201d (GPTScore vs I(Y;E)). Good catch. We changed to informational support (`info_support` in the tables to save space) to avoid confusion.\n    - Table 1 in Figure 3 x-axis: The \u201cinformativeness\u201d (now info_support) is added now. The past version didn\u2019t contain this due to a confusion with I(Y;E).\n    - Changing the order of x-axis in Figure 3 to match the previous table: Done.\n\nReferences:\n\n[1] For example: Chen, Chacha, et al. \"Machine explanations and human understanding.\"\u00a0*arXiv preprint arXiv:2202.04092*\u00a0(2022). The \u201cHow are explanations useful to humans?\u201d section in this reading list compiles some papers on this topic: [https://ziningzhu.notion.site/Explanations-reading-list-56cd203b1d1c4fd79e8fcf319b1560a8?pvs=4](https://www.notion.so/Explanations-reading-list-56cd203b1d1c4fd79e8fcf319b1560a8?pvs=21)\n\n[2] Zhu, Zining, et al. \"Situated Natural Language Explanations.\"\u00a0*arXiv preprint arXiv:2308.14115*\u00a0(2023)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4543/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699912572045,
                "cdate": 1699912572045,
                "tmdate": 1699912572045,
                "mdate": 1699912572045,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "a2rTayc8dB",
                "forum": "bpheRCxzb4",
                "replyto": "cLqz3Zlbzq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4543/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4543/Authors"
                ],
                "content": {
                    "title": {
                        "value": "More about the meaning of the two proposed metrics"
                    },
                    "comment": {
                        "value": "Dear Reviewer 7iHa,\n\nFollowing the previous response, we would like to further discuss the meaning of the two information-theoretic scores and put them into context. \n\nThe two scores take root in the communication channels: one unit of information (bit) quantifies the amount of information that helps us remove the uncertainty of a fair coin toss. (Machine learning uses ln instead of log, which we follow. It's just differing by a constant.) Intuitively, there should be a score that describes how much information is \"transmitted\" through each channel in the communication. We apply information estimation tools and showed it's possible to do so. \n\nWe call the I(X;E) score relevance, since this score describes more of how the explanan relates to the input. The experiments verify that the I(X;E) is correlated to many lexical-semantic features, especially the cosine similarity. Additionally, the features in the lexical-semantic category can explain a high portion of the variance in I(X;E), illustrating that I(X;E) describes more about the relatedness of the input.   \n\nWe call the I(Y;E) score predictive informativeness, because (1) the computation approach uses predictive V-info, and (2) this score describes more of how the explanan predicts the output label. To predict the label (i.e., do the task specified by the dataset), a wide range of mechanisms might happen under the hood, including reasoning, inferring, pattern matching based on the embedding spaces, etc. These are illustrated from our correlation experiments and the analysis of variance experiments. Overall, it's hard to claim that this score is about e.g., reasoning, so we do not draw such a conclusion.  \n\nWe hope the above elaboration clarifies a bit. If there are further questions, please do not hesitate to follow up on the comments."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4543/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700248964282,
                "cdate": 1700248964282,
                "tmdate": 1700248964282,
                "mdate": 1700248964282,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iE5iCObiWx",
                "forum": "bpheRCxzb4",
                "replyto": "pXmSkA6N77",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4543/Reviewer_7iHa"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4543/Reviewer_7iHa"
                ],
                "content": {
                    "title": {
                        "value": "Thanks to authors' response"
                    },
                    "comment": {
                        "value": "Thanks to the authors for their responses and paper revision. Since the paper writing (one of my concerned weaknesses) has improved and most of the confusion in my original questions has been tackled, I may raise my score from 3 to 5. However, I am not sure to further raise the score since my other two concerned weaknesses haven\u2019t been addressed.\n\nFrom my observation, a major part of the paper\u2019s contribution is claimed to be the introduction of the relevance and informativeness scores. From the paper, the two scores are compared to the defined silver labels, which are also a type of automatic evaluation metrics. Both the relevance and informativeness scores do not have apparent correlation with certain types of silver labels, other than the type overlap ratio.\n\nTherefore, (1) based on the results, I\u2019m not sure why using the two scores instead of silver labels themselves as the evaluation metrics for natural language explanations, (2) I\u2019m not sure if the two scores can have other meanings if possibly compared to other defined human evaluations.\n\nFrom the paper and authors\u2019 responses, I still conclude my considering strengths for this paper is their introduced framework and the diverse experiments. Nonetheless, I\u2019m not giving a higher rating since a major part of the paper\u2019s claim is the introduced scores, which I found are not well validated by experiments yet."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4543/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700681099595,
                "cdate": 1700681099595,
                "tmdate": 1700681099595,
                "mdate": 1700681099595,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gubuq7qQ8m",
            "forum": "bpheRCxzb4",
            "replyto": "bpheRCxzb4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4543/Reviewer_gJuB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4543/Reviewer_gJuB"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a framework that considers the post-hoc text pipeline as a series of communication channels. They quantify the information flow through these channels and facilitate the assessment of explanation characteristics, quantifying two information scores: relevance and informativeness. They illustrated the proposed information score measures by comparing them against traditional evaluation metrics."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- the paper is original - we found only a paper with some similar concepts used in a different context\n - the authors tackle a relevant problem and propose a framework to evaluate the informativeness of text explanations. Furthermore, the framework contemplates a high degree of automation, making it feasible to deploy in production settings.\n - the authors propose measuring two key aspects of the explanations: relevance and predictive informativeness.\n - we consider the paper to be of good quality: they performed a good overview of related work, all of the claims are supported with experimental results, acknowledged limitations, and ensured the presentation is clear."
                },
                "weaknesses": {
                    "value": "- the structure of how experiments and results are reported can be improved. In particular, it would be helpful if the authors list the experiments performed, listing rationale behind the experiment, the procedure, aims, metrics, and other aspects of relevance."
                },
                "questions": {
                    "value": "We consider the paper interesting and relevant. Nevertheless, we would like to point to the following improvement opportunities:\nData and Experiments\n   \t- We encourage the authors to restructure Section 4 and Section 5 to describe better (a) the original data they have and (b) the experimental design (rationale behind the experiment, procedure, aims, metrics, etc.). In the current manuscript, it seems most of the experimental design is described in the \"Data and Materials\" section, while the \"Experiments\" section resembles more to \"Results and Evaluation\".\n   \t- Why do the authors report Spearman and not Kendall correlation? Did they check for the Spearman correlation assumptions?Figures: \n  - Figure 4: the authors provide two plots with identical descriptions, but from the caption, they seem to refer to different concepts (one should reflect informativeness, while the second reflects rationale)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4543/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4543/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4543/Reviewer_gJuB"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4543/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698838656124,
            "cdate": 1698838656124,
            "tmdate": 1699636431658,
            "mdate": 1699636431658,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iWuuj1rrO8",
                "forum": "bpheRCxzb4",
                "replyto": "gubuq7qQ8m",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4543/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4543/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response by authors"
                    },
                    "comment": {
                        "value": "Thank you for commending on the novelty, feasibility, and the quality of the paper! To the points in weaknesses and questions:\n\n- **Improve the structure of experiments and results.** Thanks \u2014 we updated the pdf. The current version lists the sections in a more organized way.\n- **Figure 4 name confusion**: Yes this confusion is avoided in this version. We changed the \u201cinformativeness\u201d in the silver labels into \u201cinformational support\u201d."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4543/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699912736126,
                "cdate": 1699912736126,
                "tmdate": 1699912736126,
                "mdate": 1699912736126,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "R6vNiuNPSg",
                "forum": "bpheRCxzb4",
                "replyto": "iWuuj1rrO8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4543/Reviewer_gJuB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4543/Reviewer_gJuB"
                ],
                "content": {
                    "comment": {
                        "value": "We authors have tackled our comments. We have reviewed the comments from other reviewers and have no further observations."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4543/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700727554179,
                "cdate": 1700727554179,
                "tmdate": 1700727554179,
                "mdate": 1700727554179,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "O4vL3i4zDk",
            "forum": "bpheRCxzb4",
            "replyto": "bpheRCxzb4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4543/Reviewer_21zw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4543/Reviewer_21zw"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores a novel, information theoretic approach for evaluating natural language explanations. Both rationale-based and natural language-based explanations are considered.  The paper speaks of an \"explanan\" - the \"product\" (as opposed to the process) that is the explanation.  The inputs and outputs of the any explanan can be reduced to a fixed dimension representation by conventional text embedding tools, to which mutual information approximations can be applied.  These are two - one input based, the \"relevance\"; and the other output-based, the \"informativeness.\" \n\nIn summary by considering these two measures as information channels, the paper finds that relevance is related to traditional measures of relevance, and informativeness is related to the explanans' reasoning."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "Text-based explanation is a nascent field for which this paper offers a novel attempt.   The paper shows the feasibility of applying mutual information measures, noting that currently it is \"still unknown whether these tools can be used to examine information scores.\" The paper's demonstration of the feasibility of using various approximations to mutual information in this case is novel. The use in practice of these measures for evaluation is a worthwhile contribution. The entire field of natural language model evaluation is a developing area, unlike the mature methods used in supervised machine learning that have propelled that field forward."
                },
                "weaknesses": {
                    "value": "The conclusions are modest, and give limited insight.  Despite this the approach has promise, as it plows new ground in an area where there is limited success today."
                },
                "questions": {
                    "value": "It is not clear how the dataset - benchmark, the \"silver labels\" and the language models come together in the experiments. The experiments section does not describe the process  - how is evaluation applied?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4543/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698863031860,
            "cdate": 1698863031860,
            "tmdate": 1699636431587,
            "mdate": 1699636431587,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bjlU39Nxh6",
                "forum": "bpheRCxzb4",
                "replyto": "O4vL3i4zDk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4543/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4543/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response by authors"
                    },
                    "comment": {
                        "value": "Thank you for commending on the contribution! We believe that model evaluation can benefit from information-theoretic approaches. While the area of information estimator is not as popular as e.g., prompting LLMs, or LLM agents, there are still some meaningful progress in the past a few years, and we still believe there will be improvement potential.\n\nTo your question about the experiment procedures: we updated the pdf. Each subsection there now starts with a paragraph describing the experiment setup. The procedure is now clearer and reflects how each experiment is set up."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4543/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699912901606,
                "cdate": 1699912901606,
                "tmdate": 1699912901606,
                "mdate": 1699912901606,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]