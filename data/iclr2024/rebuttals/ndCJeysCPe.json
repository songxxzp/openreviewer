[
    {
        "title": "Analysis of Learning a Flow-based Generative Model from Limited Sample Complexity"
    },
    {
        "review": {
            "id": "IdQsE5Crfr",
            "forum": "ndCJeysCPe",
            "replyto": "ndCJeysCPe",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1747/Reviewer_AsnX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1747/Reviewer_AsnX"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies a certain asymptotic limit of a flow-based generative model that uses weight-tied fully connected network with skip connection to approximate the velocity field. In such scenario, under the isotropic gaussian mixture assumption the authors provide a characterization of training dynamics in the finite sample complexity regime. Notably, the resulting asymptotic behaviour of the learned cluster means enjoys the Bayes optimal rate of $O(1/n)$."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- a complete characterization of training dynamics of shallow generative flow under the isotropic mixture of gaussians assumption\n- optimality of the resulting sample asymptotics\n- a neat symmetric ansatz"
                },
                "weaknesses": {
                    "value": "- lack of the correlation structure in the input data, which draws the conclusions to be less practical\n- minor: the approach is still an ansatz"
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1747/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1747/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1747/Reviewer_AsnX"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1747/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698150060810,
            "cdate": 1698150060810,
            "tmdate": 1699636103746,
            "mdate": 1699636103746,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hKIXwjg7Oq",
                "forum": "ndCJeysCPe",
                "replyto": "IdQsE5Crfr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1747/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1747/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their comments and questions, which we address below.\n\n>lack of the correlation structure in the input data, which draws the conclusions to be less practical\n\nWe believe that ideas similar to the ones used in the present work could be leveraged to address mixtures with structured clusters, see also the answer to reviewer wFJz. Such an extension would allow to analyze more realistic data distributions, and potentially quantitatively capture the learning curves of some simple real datasets, like in (Cui & Zdeborov\u00e1, 23). This constitutes an interesting research direction which we will further pursue in future work."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1747/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700229128328,
                "cdate": 1700229128328,
                "tmdate": 1700229128328,
                "mdate": 1700229128328,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2Ph2Q0iIpn",
            "forum": "ndCJeysCPe",
            "replyto": "ndCJeysCPe",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1747/Reviewer_wFJz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1747/Reviewer_wFJz"
            ],
            "content": {
                "summary": {
                    "value": "The authors analyze a generative flow-based model to sample from a mixture of Gaussians, where at each timestep the vector field is parameterized by a two-layer neural network.\nThey consider a high dimensional finite sample regime with the number of samples scaling linearly in the dimension. \nUsing tools from statistical physics they derive a precise characterization of the optimal performance.\nTheir experiments corroborate the theoretical findings."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The strongest point is the exact characterization of all important quantities. \n- Clear and concise presentation of the results\n- Solid experiments demonstrating the validity of the theoretical results"
                },
                "weaknesses": {
                    "value": "The approach seems to heavily rely on the gaussianity of the target distribution"
                },
                "questions": {
                    "value": "- Do you think this the same method could be used for more complex distributions, if yes what would be a concrete example?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1747/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1747/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1747/Reviewer_wFJz"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1747/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698237649880,
            "cdate": 1698237649880,
            "tmdate": 1699636103654,
            "mdate": 1699636103654,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9OeFgavLHg",
                "forum": "ndCJeysCPe",
                "replyto": "2Ph2Q0iIpn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1747/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1747/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their appreciation of our work, and address below their concerns.\n\n>Do you think this the same method could be used for more complex distributions, if yes what would be a concrete example?\n\nWhile the method indeed makes use of the fact that the target density is a Gaussian mixture, we believe that very similar ideas can be employed to address Gaussian mixture densities with structured covariances. This would provide a gateway to modeling more realistic setups, since as shown in (Cui & Zdeborov\u00e1, 23), at the level of the learning process, the performance of the DAE on MNIST is quantitatively captured when modeling the latter by the matching Gaussian mixture density. Another natural extension which could be addressed with similar methods is the case of more clusters, and using an autoencoder with more hidden units. We leave the exploration of these exciting questions to future work."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1747/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700229218268,
                "cdate": 1700229218268,
                "tmdate": 1700229218268,
                "mdate": 1700229218268,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5CqsOrwQLW",
            "forum": "ndCJeysCPe",
            "replyto": "ndCJeysCPe",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1747/Reviewer_v6CB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1747/Reviewer_v6CB"
            ],
            "content": {
                "summary": {
                    "value": "The paper in detail analyses the case of training a stochastic interpolation model on few samples from a bimodal Gaussian mixture model. All theoretic predictions are accompanied by experiments.\n\nThe first result 2.1 characterizes the solution that is obtained when training on a bimodal Gaussian mixture. It shows that the weight vector is contained in the span of $\\mu$ (the displacement vector for the mixture components), $\\xi$ (the mean of all latent points), and $\\eta$ (the average offset of the data from the corresponding mean) and thus no other directions are relevant.\n\nThe second result 3.1 derives the resulting generative ODE and summarizes it in terms of the relevant space from result 2.1.\n\nThe third result 3.2 spells out the Euler integration of result 3.1.\n\nThe final result 3.3 shows that the distance and angle between the true $\\mu$ and the estimated $\\hat\\mu$ reduce by $\\Theta(1/n)$. This is the same as the Bayes optimal rate."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper considers an interesting question, how generative models generalize as a function of the number of training samples.\n\nThe solution for the bimodal Gaussian Mixture model appears sound and plausible.\n\nAll theoretical results are accompanied by experiments closely matching the prediction, increasing the credibility of the theoretical results."
                },
                "weaknesses": {
                    "value": "I find the main technical result presented in a misleading way: Results 2.1 and 3.1 show that the weight vector and the ODE dynamic are not orthogonal to $\\mu$. I think a better way to describe the behavior of the system would be to span the relevant space via $\\eta$ and $\\hat\\mu_+$ and $\\hat\\mu_-$, corresponding to the empirical mean of the samples at $\\pm\\mu$. I assume that this is also sufficient to span the weight vector (please correct me if I\u2019m wrong), and it would make clear that the model only has access to the empirical means. The remaining results could then be adapted to show how the sampling process is able to reproduce the empirical means of the two modes. Then, the Bayes-optimal baseline could be inserted to show that the empirical estimates go down with Theta(1/n), and that the flow is able to achieve the same rate.\n\nI also find that the research question formulated in the beginning is not really addressed in the main text, whether the network architecture determines whether a generative model memorizes training data. If my above understanding is correct, then the model actually does learn the training data by heart (i.e. it predicts the empirical means), and the rate to the true solution is essentially given how fast empirical means converge to the mean of the generative distribution. Also, the rate $\\Theta(1/n)$ is not affected by the regularization $\\lambda$, and the network architecture is not varied so as to judge wether this particular setup has a particular convergence rate.\n\nMinor points:\n- Formatting of contributions via itemize\n- Sentence? \u201eNote that (9) is a special case of the architecture studied in Cui & Zdeborov\u00e1 (2023), and differs from the very similar network considered in Shah et al. (2023) in its slightly more general activation (Shah et al. (2023) address the case \u03c6 = tanh)\u201c\n- Result 3.1 Le X_t -> Let X_t\n- finding that that the DAE on p. 9"
                },
                "questions": {
                    "value": "1. How many dimensions are needed to span $\\hat w_t$ for all $t$? From a simple drawing of $\\eta, \\mu_+, \\mu_-$ I conjecture that two dimensions are enough (similar to first weakness).\n\n2. Why is $\\mu \\propto d$ a reasonable scaling? This seems like an unrealistic choice to me. In practice, data is often normalized say to a fixed range $[-1, 1]$ per dimension, so in order to obtain the scaling behavior the means have to be $\\mu = (\\pm 1, \u2026, \\pm 1)$, i.e. both mixture components are at the corners of the hypercube. Alternative question: Does the sample complexity also transfer to $\\|\\mu\\|^2 < O(d)$, e.g. $O(1)$? I would guess that the other directions start playing a role then.\n\n3. What is the solution to this simple setup intuitively? Can you provide a simple drawing of $0, \\eta, \\mu_+, \\mu_-, \\mu, \\eta$ and a learnt trajectory? If the required dimension is indeed two, this should be easy.\n\n4. What is the shape of the learnt distribution within the two clusters, i.e. what is the local density in each cluster?\n\n5. Fig. 2: Why are the learnt means biased in one direction? Is more training data added sequentially?\n\n\nGiven the substantial weaknesses and the above questions, my *preliminary* vote for this paper is therefore not to accept it. I am happy to be corrected in any of the criticisms I raised and look forward to the authors' rebuttal."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1747/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1747/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1747/Reviewer_v6CB"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1747/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698330628709,
            "cdate": 1698330628709,
            "tmdate": 1700750746226,
            "mdate": 1700750746226,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5zsfazCZxY",
                "forum": "ndCJeysCPe",
                "replyto": "5CqsOrwQLW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1747/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1747/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their careful reading of our manuscript and the constructive comments. We address below their questions.\n\n> How many dimensions are needed to span $\\hat{w}_t$ for all $t$\n? [...] I conjecture that two dimensions are enough[...]it would make clear that the model only has access to the empirical means\n\nWe thank the reviewer for this suggestion. The weight vectors can indeed be completely equivalently rewritten in terms of $\\xi,\\mu_{emp.} $ where $\\mu_{emp.}$ is the empirical means\n$$\n\\mu_{emp.}=\\frac{1}{n}\\sum\\limits_{\\mu=1}^n s^\\mu x_1^\\mu=\\mu+\\frac{1}{n}\\eta.\n$$\nNote that this is just an equivalent way to present the equation, and that the underlying results remain totally unchanged. We chose the initial presentation in terms of $\\xi, \\eta, \\mu$ to put further emphasis on the parameters of the target distribution.\nWe however agree with the reviewer that using $\\{\\eta, \\mu_{emp.}\\}$, rather than $\\{\\eta,\\mu,\\xi\\}$, constitutes a more concise rephrasing of the results, and have rewritten the manuscript in this way. We have updated the pdf of our paper to reflect this.\n\n\n\n\n> then the model actually does learn the training data by heart (i.e. it predicts the empirical means), \n\nThe reviewer is correct that the model overfits, as the trained weights bear explicit dependence on the training samples, and the Gaussian noises $x_0$ employed during training. By \"not memorizing\", we rather mean that the generated density is not the discrete empirical distribution supported on the training data, but instead a Gaussian mixture. In other words, the generative model allows to generate novel samples, distinct from the training data. We emphasized this distinction in updated manuscript.\n\nOn the other hand, it is not entirely correct that the generated density has the empirical mean of the training data as cluster mean. It is already not the case for the Bayes-optimal estimate, which is the empirical means rescaled by a $1/(\\sigma^2+n)$ factor. For the generative model, the cluster mean is a linear combination of $\\xi$ (subsuming the effect of the noises used during training) and the empirical means, rescaled by multiplicative factors whose expression depend non-trivially on the model parameters - notably the schedule functions $\\alpha(t),\\beta(t)$, and the regularization $\\lambda$. We believe that this precise characterization of the cluster mean of the generated density as a function of the parameters of the learning model is a strength of our analysis.\n\n>Also, the rate $\\Theta(1/n)$\n is not affected by the regularization \n, and the network architecture is not varied so as to judge whether this particular setup has a particular convergence rate. [...]  \n\nThe main focus of the paper is not to study the dependence of the convergence rate as a function of the model architecture and parameters, but rather show, in a particular setting, how a learning model can manage to learn the target density with a reasonably fast rate only _from a few samples_. This is _allowed_ by the network architecture : indeed, had the minimization (5) been carried out over the space of denoising functions, instead of the parameter space of the considered learning model, the resulting generative model would memorize the training samples, and only allow to generate samples already present in the\ntrain set, instead of sampling the Gaussian mixture. Furthermore, a discussion for another architecture can already be found in Appendix D of the supplementary material, where we detail the case of an auto-encoder without skip connection. We completely agree with the reviewer that the study of a greater number of architectures is an important research direction to be explored in future works.\n\n>the rate to the true solution is essentially given how fast empirical means converge to the mean of the generative distribution.\n\nThis is indeed the correct intuition. We would however like to stress that the mean of the generated density is not equal to the empirical means, and that it is quite non-trivial that an auto-encoder can learn to generate a Gaussian mixture with such means just from a finite number of samples when trained on a denoising loss, and that the learning and transport processes can be sharply theoretically characterized."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1747/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700229955247,
                "cdate": 1700229955247,
                "tmdate": 1700231499802,
                "mdate": 1700231499802,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GkpNnsXORP",
                "forum": "ndCJeysCPe",
                "replyto": "5CqsOrwQLW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1747/Reviewer_v6CB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1747/Reviewer_v6CB"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for the extensive answers and update of the manuscript.\n\nI appreciate that the authors undertook the effort to show that the main behavior of the flow can be described by the span of $\\xi, \\mu_\\text{emp.}$. This makes clear that the model does not have implicit knowledge of the underlying distributions, but really the convergence is successfully borrowed from the convergence of the empiric mean (and not worse).\n\n**However, my concern remains that the research question is not really addressed.**\n\nLet us maybe approach this starting from my Q4: The characterization of the learned density in Corollary 3.3 describes the unspanned directions as Gaussian, and the distribution in the relevant subspace $\\operatorname{span}(\\xi, \\mu_\\text{emp.})$ is specified as time-recursive formulas of the components in $\\xi$ and $\\mu_\\text{emp.}$ directions, influenced by the corresponding components of the weights. What does this tell me about the learned distribution at each of the mixtures? The authors say that the DAE with skip connections is superior over the DAE without skip connections. However, Appendix D shows the same scaling behavior with $n$ is observed in the metric of Corollary 3.3. Even the other end of the spectrum, a fully flexible function memorizing the training data, has this convergence behavior.\n\nSo under what metric should architectures be compared, how do the architectures differ in this metric, and what should we conclude from this on the inductive bias & sample complexity of models? In my view, this metric is a strong metric of convergence such as KL divergence or total variation, and the authors are able to show convergence as $\\Theta(1/n)$ if this is indeed the dominant scaling behavior.\n\nI am sceptical that the theoretical insight of this work is not large enough for being interesting to a broader audience. Due to the granularity of the scoring system, I think that the improvements are currently not enough to improve my rating. However, I am still happy to be convinced otherwise."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1747/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700509511664,
                "cdate": 1700509511664,
                "tmdate": 1700509542647,
                "mdate": 1700509542647,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GmTVLTOPU2",
                "forum": "ndCJeysCPe",
                "replyto": "ctgLZWSXNq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1747/Reviewer_v6CB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1747/Reviewer_v6CB"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for the additional information that provides additional insight.\n\nThe additional and explicit characterization of the learned density is quite insightful, as it explicitly describes what the distribution is. I think that this discussion is very useful: For a paper targeting a sharp analysis of the learned distribution, I would expect an easily interpretable functional form, which seems easy to derive according to the authors.\n\nHowever, I am not convinced about their derivation: To obtain the shape of the learned mixture components, the authors derive the first order correction term of a sample from x_0=0 if I understand correctly. Thus, the derivation only computes the second moment of a mixture component. This does not show that the learned density is actually Gaussian. It is encouraging to see that the standard deviations seem to converge to the correct value.\n\nIn this light, I am surprised to see that the DAE without skip connections learns a $\\delta$ distribution. What happens if you apply the same argument as for the DAE with skip connections, i.e. what standard deviation is learned?\n\nRegarding the provided mixture Wasserstein distance, it does not seem to be a generalization of the Wasserstein distance, but rather a related concept (\"Wasserstein-type distance\" as per the referenced paper). In particular, it only applies to Gaussian mixtures. In light of the above argument, it seems that the learned density does not obtain Gaussian shape and therefore the mixture Wasserstein distance does not apply.\n\nAre the above points valid?"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1747/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700656655771,
                "cdate": 1700656655771,
                "tmdate": 1700656655771,
                "mdate": 1700656655771,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]