[
    {
        "title": "CNN Kernels Can Be the Best Shapelets"
    },
    {
        "review": {
            "id": "4Zff89qcAa",
            "forum": "O8ouVV8PjF",
            "replyto": "O8ouVV8PjF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8442/Reviewer_Lmj7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8442/Reviewer_Lmj7"
            ],
            "content": {
                "summary": {
                    "value": "The authors introduce ShapeConv, an interpretable CNN layer whose kernels function as shapelets, designed for time series modeling in both supervised and unsupervised settings. They demonstrate that using the square norm in convolution, coupled with max pooling, is equivalent to computing the distance between a shapelet and a time series. Within this framework, a convolutional kernel essentially serves as a tunable shapelet. The authors also incorporate regularization to enforce similarity and diversity among shapelets, depending on whether the task is supervised (classification) or unsupervised (clustering). The methodology is validated through experiments on time series classification and clustering, using several competitor models and alternative implementations of ShapeConv for comparison. XAI is assessed via author-selected examples."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is generally well-structured and straightforward to follow.\n- It establishes an interesting link between convolutional operations and the shapelet transform.\n- The proposed methodology is versatile, applicable to both supervised and unsupervised tasks."
                },
                "weaknesses": {
                    "value": "- The paper lacks a comprehensive review of related work, and the selection of competitor approaches for comparison is odd.\n- Parts of the experimental section are unclear and require further clarification. The XAI evaluation is restricted to examples selected by the authors.\n- There is no discussion or citation concerning code implementation."
                },
                "questions": {
                    "value": "1. **Lack of Comprehensive Review of Related Work**:\n   - The authors focus exclusively on optimization-based shapelet approaches. While space is limited, notable methods like Random Shapelet Forest and standard shapelet transform should not be omitted. Dictionary-based and interval-based approaches are also relevant and have achieved state-of-the-art performance in time series classification, yet they are not mentioned. Furthermore, the competitor models used in the experimental section are largely transformer-based or rely on embeddings, making for an unusual selection. I recommend that the authors thoroughly review relevant literature on time series classification, such as the paper by Ruiz et al. (2021) and models like ROCKET by Dempster et al. (2020).\n\n2. **Ambiguities in the Experimental Section**:\n    - Is the \"Initialization\" phase's cost included in the runtime?\n    - In Table 1, why do the methods differ with respect to the cd plots?\n    - Why is the evaluation limited to 25 UCR datasets, and what was the criteria for selection?\n    - Several state-of-the-art methods like Rocket, CIF, ShapeletTransform, and MUSE are absent from the comparison.\n\n3. **Limitations in XAI Evaluation**:\n    - While the author-selected examples support the paper's claims, they do not suffice to demonstrate the superiority of ShapeConv in terms of shapelet quality. Additionally, pairing ShapeConv with MLP or SVM models does not provide sufficient interpretability. I suggest testing the approach with tree-based or linear models, or employing explainers such as SHAP to determine the importance of shapelets, especially in supervised tasks."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8442/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698303691696,
            "cdate": 1698303691696,
            "tmdate": 1699637053233,
            "mdate": 1699637053233,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kXkI3PEAgk",
                "forum": "O8ouVV8PjF",
                "replyto": "4Zff89qcAa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8442/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8442/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Lmj7 (Part 1)"
                    },
                    "comment": {
                        "value": "Thanks for your valuable comments. Below we have addressed your questions and concerns point-by-point, and we have updated and added more clarifications to our manuscript accordingly.\n\n### Weaknesses\n\n> The paper lacks a comprehensive review of related work, and the selection of competitor approaches for comparison is odd.\n>\n> Parts of the experimental section are unclear and require further clarification. The XAI evaluation is restricted to examples selected by the authors.\n\nWe have addressed them in the Questions section.\n\n> There is no discussion or citation concerning code implementation.\n\nThe code implementation is included in the supplementary material.\n\n### Questions\n\n> Lack of Comprehensive Review of Related Work:\n> \n> - The authors focus exclusively on optimization-based shapelet approaches. While space is limited, notable methods like Random Shapelet Forest and standard shapelet transform should not be omitted. Dictionary-based and interval-based approaches are also relevant and have achieved state-of-the-art performance in time series classification, yet they are not mentioned. Furthermore, the competitor models used in the experimental section are largely transformer-based or rely on embeddings, making for an unusual selection. I recommend that the authors thoroughly review relevant literature on time series classification, such as the paper by Ruiz et al. (2021) and models like ROCKET by Dempster et al. (2020).\n\nThank you for the suggestions. \n\nFirst, one kind of baseline models we have included in the Experiment section are *shapelet related methods, which includes not only optimization-based shapelet approaches*, but also the selection and generative based methods. The standard shapelet transform is denoted IGSVM in the table (Table F.1) and CD plot (Figure 3) according to the name in its paper. Besides, standard shapelet transform and its variants have also been mentioned in our related work like that: *Traditional practice is to search the raw datasets with some speed-up strategies, like paralleling computing (Chang et al., 2012), SAX transformation(Rakthanmanon & Keogh, 2013) and procedure simplification through newly designed measurements (Lines et al., 2012; Guillaume et al., 2022; Zakaria et al., 2012).*\n\nSecond, other two kinds of baselines are common deep learning methods (MLP, CNN, RNN) and state-of-the-art methods (TS2Vec, TST, DTW, etc), which covers various types of neural networks.\n\nThird, we have added ROCKET as a baseline in the UCR dataset comparison upon your request. However, we also want to mention that ROCKET is already compared with some of our baselines, like TST and OS-CNN, in their papers. Since we are comparing with the state-of-the-art methods in recent two years, and many previous methods like HIVE-COTE and BOSS are already compared in our baselines' papers, we believe that superior performance over these state-of-the-art methods can well demonstrate the effectiveness of our model.\n\nFinally, we have also updated our related work section by introducing non-deep-learning approaches like the dictionary-based method MUSE and the ensemble method HIVE-COTE.\n\n> Is the \"Initialization\" phase's cost included in the runtime?\n\nYes, it is included. We have clarified this in Table 1.\n\n> In Table 1, why do the methods differ with respect to the cd plots?\n>\n> Why is the evaluation limited to 25 UCR datasets, and what was the criteria for selection?\n\nIn the CD plot (Figure 3), ShapeConv is compared with shapelet-based methods on 25 UCR datasets, and full results here are shown in Table F.1 in Appendix. In Table 1, ShapeConv is compared with state-of-the-art time-series classification models on 125 UCR and 29 UEA datasets, and the full results are shown in Table F.3 and F.4. Methods are different since they are two different experiments.\n\nWe use the subsets (25 UCR datasets) in the CD plot because the baseline (ADSN) only reported results on 25 UCR dataset, and the results of baselines are taken directly from the respective papers. Full results of ShapeConv and state-of-the-art classification methods on all 128 UCR datasets are shown in Table F.3 in Appendix."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8442/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700593788361,
                "cdate": 1700593788361,
                "tmdate": 1700593788361,
                "mdate": 1700593788361,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dwBj0B1jEv",
            "forum": "O8ouVV8PjF",
            "replyto": "O8ouVV8PjF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8442/Reviewer_JpzU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8442/Reviewer_JpzU"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a modified convolutional layer for time series analysis inspired by the Shapelet distance that is widely used in the domain.\nThis new layer is then used at the core of neural networks for both supervised and unsupervised tasks.\nA regularization term for the task-specific losses is designed that enforces learned kernels to (i) look like actual subseries from the training set and (ii) form a diverse set."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is very well written and the motivation for the method is clear.\nThe experimental validation is quite thorough and it is nice to showcase that the method can be used for both supervised and unsupervised learning.\nIn terms of the method, while the idea of having a Shapelet layer included in a neural network is not novel, both the initialization scheme and the regularization terms included in the loss lead to improvement on the performance of the resulting models."
                },
                "weaknesses": {
                    "value": "In the abstract (it is also said in the introduction in other words), it is stated that:\n\n>  In this paper, we demonstrate that shapelets are essentially equivalent to a specific type of CNN kernel with a squared norm and pooling\n\nIn fact, this demonstration is not novel, it is for example stated in (Lods et al. 2017) (that is cited in the paper).\nHowever, it seems that here, the proof aims at more rigor, but Theorem 3.1 is not successful in this regard since it completely disregards the fact that the bias term in convolution is independent of the input, which is not the case in the $-\\mathcal{N}(s_i, X_{j:j+l_s-1})$ term. (Also, as a side note, in Theorem 3.1, Squared Euclidean distance is used, not Euclidean distance as stated.)\n\nMoreover, the review of the Related Work is very succinct and a more thorough presentation of competing interpretable Shapelet-based methods would have been a plus. \nSimilarly, a more detailed comparison of the interpretability of the ShapeConv model with those baselines is required to fully assess interpretability:\n* Only toy examples are presented (eg. Fig 4: 2 shapelets), what does it give when training with a large amount of shapelets?\n* Also, providing visualization for a large number of datasets instead of only GunPoint+Herring+ECG200 would be a real plus"
                },
                "questions": {
                    "value": "Apart from the questions/suggestions related to the evaluation of interpretability, I have a few remarks/questions that are listed below:\n\n* If you took your your ShapeConv model (with exact same initialization, regularization terms, etc.) and changed the ShapeConv layer with a convolutional one, what would you get in terms of performance? This experiment is required to fully assess if the norm terms a really helpful\n* In terms of evaluation:\n    * How are baseline model hyperparameters tuned (and which parameters are tuned)?\n    * How do you pick the datasets for the subsets (25 datasets for supervised learning and 36 datasets for unsupervised learning)?\n    * If the goal is to compare to state-of-the-art methods, other competitors should be included in the comparison (eg. ROCKET, COTE & variants, ...)\n\nBelow are some minor remarks/questions:\n* In Section 1, you write:\n    >  they are more likely to overfit when the signal-to-noise ratio is relatively large\n    * Don't you mean \"is relatively low\"?\n* Initialization\n    * Have you assessed how important it was to use supervised information at initialization?\n    * Have you tried simpler approaches (eg. kmeans++ on randomly selected subsequences of adequate length)?\n* If ShapeConv is faster than LTS, it is probably more an artifact of the implementation since the overall complexity of ShapeConv is probably higher than that of LTS (similar local representation extracted, but ShapeConv have additional loss terms that induce more computations)\n* Presentation\n    * Unsupervised learning: it is unclear from the presentation in Section 3.4 which clustering method is used on top of the features extracted from ShapeConv. This is detailed in Section 4.2, but should be explained in Section 3.4 imho"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8442/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698661476203,
            "cdate": 1698661476203,
            "tmdate": 1699637053115,
            "mdate": 1699637053115,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pt9VeE4NKF",
                "forum": "O8ouVV8PjF",
                "replyto": "dwBj0B1jEv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8442/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8442/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer JpzU (Part 1)"
                    },
                    "comment": {
                        "value": "Thanks for your valuable comments. Below we have addressed your questions and concerns point-by-point, and we have updated and added more clarifications to our manuscript accordingly.\n\n### Weaknesses\n\n>In the abstract (it is also said in the introduction in other words), it is stated that:\n>\n>> In this paper, we demonstrate that shapelets are essentially equivalent to a specific type of CNN kernel with a squared norm and pooling\n>\n> In fact, this demonstration is not novel, it is for example stated in (Lods et al. 2017) (that is cited in the paper). However, it seems that here, the proof aims at more rigor, but Theorem 3.1 is not successful in this regard since it completely disregards the fact that the bias term in convolution is independent of the input, which is not the case in the $-\\mathcal{N}(\\mathbf{s_i}, \\mathbf{X}_{j:j+l_s-1})$ term.\n\nWe would like to emphasize that the bias term in convolution has no relation with the $-\\mathcal{N}(\\mathbf{s\\_i}, \\mathbf{X}\\_{j:j+l\\_s-1})$ term. The bias term, as we have explained in the end of Sec. 3.1, can be optionally added after the features are extrated with the shapelets (or CNN kernels). And the $-\\mathcal{N}(\\mathbf{s_i}, \\mathbf{X}_{j:j+l_s-1})$ term is an additional squared L2 norm term added in ShapeConv besides the cross-correlation operator in CNN. The equivalence cannot be realized without this additional squared L2 norm. Therefore, the bias term do not influence the proof of equivalence, so that Theorem 3.1 is successful in the rigor equivalence between CNNs and shapelets.\n\n> Also, as a side note, in Theorem 3.1, Squared Euclidean distance is used, not Euclidean distance as stated.\n\nThanks for pointing out. We have revised our manuscript.\n\n\n> Moreover, the review of the Related Work is very succinct and a more thorough presentation of competing interpretable Shapelet-based methods would have been a plus. Similarly, a more detailed comparison of the interpretability of the ShapeConv model with those baselines is required to fully assess interpretability:\n> - Only toy examples are presented (eg. Fig 4: 2 shapelets), what does it give when training with a large amount of shapelets?\n> - Also, providing visualization for a large number of datasets instead of only GunPoint+Herring+ECG200 would be a real plus\n\nFor the visualizations, we did not choose \"toy examples\" - the ShapeConv in Fig 4 could achieve 0.75 accuracy on the Herring dataset with 2 shapelets. When the number of shapelets are large, the model tends to overfit for these simple datasets. As for examples with large number of shapelets, we provided a real-world application of ShapeConv in EEG (Appendix D) where 2,688 shapelets are trained. Two of them are visualized in Figure D.1, and they matched nicely with the EEG examples from textbook.\n\nWe also provided more visualizations in the appendix E, showing learnt shapelets in 4 additional datasets from UCR: DodgerLoopWeekend, SonyAIBORobotSurface1, ItalyPowerDemand, BME. ShapeConv captures the determinative regions in all of them.\n\nIn terms of interpretability, ShapeConv is generally shapelet-based method, so its interpretability inherits from shapelet. We have compared the interpretation results of ShapeConv with the other shapelet-based method (i.e., LTS) via visualizations, and results shows that ShapeConv is better at capturing discriminative sub-sequences, thus having better interpretability.\n\n\n### Questions\n\n> If you took your your ShapeConv model (with exact same initialization, regularization terms, etc.) and changed the ShapeConv layer with a convolutional one, what would you get in terms of performance? This experiment is required to fully assess if the norm terms a really helpful\n\nWe anticipate your proposed ablation is to drop the squared $L_2$ norm term $\\mathcal{N}(\\mathbf{s_i}, \\mathbf{X'})$ from Equation (4) in the loss. However, then the shape regularizer in the loss would become \n$$\n\\mathcal{R}\\_{shape}=\\frac{1}{n_{out}}\\sum_{i=1}^{n_{out}}  \\min_{\\mathbf{x}\\in\\hat{\\mathbf{X}}_i} -2Y(\\mathbf{s_i},\\mathbf{x})\n$$\nwhere $Y(\\mathbf{s_i},\\mathbf{x})$ denotes the cross-correlation between kernel $\\mathbf{s_i}$ and subsequence $\\mathbf{x}$. Minimizing this regularizer would make the loss diverge and goes to negative infinity. This is because without the norm terms, the regularizer could not be interpreted as a distance and is not always nonnegative. \n\nIf we do not use the shape regularizer, the formulation would become similar with the CNN baseline. We tested the ShapeConv without Shape Loss (by setting the $\\lambda_{shape}$ in Equation (8) to zero). The results for the GunPoint dataset is illustrated in appendix E.2. We found that its kernel is not interpretable and it performs worse than ShapeConv (it constantly overfits the training set). The result performance is similar with the CNN baseline. We did not include the full results in the paper due to space constraints."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8442/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700592945548,
                "cdate": 1700592945548,
                "tmdate": 1700592945548,
                "mdate": 1700592945548,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "a8Uugvr71C",
                "forum": "O8ouVV8PjF",
                "replyto": "kIY7ZZoRmG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8442/Reviewer_JpzU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8442/Reviewer_JpzU"
                ],
                "content": {
                    "title": {
                        "value": "Answer to part 2"
                    },
                    "comment": {
                        "value": "Thank you for this feedback (part 2), it makes things clearer for me."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8442/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700641145702,
                "cdate": 1700641145702,
                "tmdate": 1700641145702,
                "mdate": 1700641145702,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tbFLM2RMsd",
                "forum": "O8ouVV8PjF",
                "replyto": "pt9VeE4NKF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8442/Reviewer_JpzU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8442/Reviewer_JpzU"
                ],
                "content": {
                    "title": {
                        "value": "Answer to part 1"
                    },
                    "comment": {
                        "value": "I still don't understand what Theorem 3.1 means by \"equivalence\".\n\nIn order to prove equivalence between CNNs and Shapelets, you need to exhibit that for a given set of shapelets, the same transformation can be achieved using a given kernel and vice versa, which is not done. Once again, I do not see how the $\\||X\\||^2_2$ term can be obtained at the output of a convolution operation.\n\nWhat you prove is that Shapelet transform is identical to your ShapeConv (which is not a standard convolution).\nIn other words, ShapeConv is just a re-writing of the shapelet transform, and is not a convolutional layer.\n\nThis relates to the question on complexity: as stated in your answer, the only difference between your ShapeConv and tslearn's implementation of LTS is that you run yours on GPU, but `tslearn` uses the same tricks as you do to compute the shapelet transform it seems: https://github.com/tslearn-team/tslearn/blob/09441abdeb3056a47615a299cbd41499def4dd46/tslearn/shapelets/shapelets.py#L141 hence could probably be run on GPU with the same time complexity."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8442/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700641983540,
                "cdate": 1700641983540,
                "tmdate": 1700641983540,
                "mdate": 1700641983540,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wCkfgHfJM8",
            "forum": "O8ouVV8PjF",
            "replyto": "O8ouVV8PjF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8442/Reviewer_nftB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8442/Reviewer_nftB"
            ],
            "content": {
                "summary": {
                    "value": "The paper combines Shapelets and CNNs"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Good empirical results.\nTests on many datasets (although, just download and test, no new datasets)"
                },
                "weaknesses": {
                    "value": "I appreciate the accuracy improvements, while small, are probably real.\n\nHowever, I have no confidence in any of the claims of interpretability and explainability.\nYou made no effort to obtain the original herring images or gun-point videos. You explanations here are \"just-so\" stories. [v].\n\nIf you wanted to make convincing claims here, you could obtain the herring images, modify them to add / remove effects, then see how this affects the shapelets. Or reenact the gun-point video, and  modify the protocol to add / remove effects, or...\n\nI do understand that most people in this space are too lazy to go beyond downloading the UCR datasets. But if that is all you do, it seems like you should temper your claims about interpretability and explainability.\n\n\n\n\n\n\u201cIn the realm of machine learning, interpretable time-series modeling stands as a pivotal endeavor, striving to encode sequences and forecast in a manner that resonates with human comprehension\u201d\nThis (and the rest of the paper) read like flowery language [a].\n\n\nIn fig 1, can you move the legend away from the data?\n\n\n\u201cis evaluated on the 25 UCR\u201d Did you mean \u201c125\u201d or \u201c25\u201d?\n\n\n\u201cFigure 5: Shaplets learned\u201d typo (Shapelets)\n\n\n\u201cIt is evident that the shapelet learned by ShapeConv captures the distinguishing features of the class effectively\u201d  Evident to whom? You should argue that the blue shapelets correctly represents the actors hand having to hover over the gun holster, then reach down to the gun, then draw the gun.\n\n\n\u201cclustering task using 36 UCR univariate\u201d\nWhy 36? Why this particular 36?\n\n\n\n\u201cIn response to the first RQ, we observe that ShapeConv\u2019s shapelets (Figure 4 (a)) cover all turning points in the time series, where the two classes differ the most, while LTS\u2019s shapelets (Figure 4 (b)) do not cover the targeted regions.\u201d\nThis evaluation is tautological. If  \u201cturning points\u201d are the best places for shapelets, then we don\u2019t need any search for shapelets at all.\n\n\n\n\u201cIn contrast, when using human initialization,..\u201d\nHmm, it is a bit tricky to claim results based on human initialization. Which humans, how trained are they in the system, how are they briefed. In my mind, that is a separate \u201chuman in the loop\u201d paper.\n\n\nHowever, despite ingenious, the performance (missing a word?)\nHowever, despite ingenious suggestions, the performance\n\n\ngun out of the gun pocket (holster) \n\n\n\u201cwhile data from the \u201cfinger\u201d class don\u2019t.\u201d\n\u201cwhile data from the \u201cfinger\u201d class do not.\u201d  (avoid contractions in scientific writing) \n\n\nThis illustrate how\nThis illustrates how\n\nIn table E.5, why four significant digits? This is spurious  accuracy.\n\nIn table E.5 and elsewhere, you report the average accuracy.  This is meaningless for datasets of different sizes, class skews, number of classes, default rates etc. To be clear, it is not a flawed metric, it is just meaningless. \n\n\n[a] https://www.latimes.com/socal/daily-pilot/opinion/story/2022-05-03/a-word-please-flowery-writing-can-turn-off-readers\n\n[v] https://en.wikipedia.org/wiki/Just-so_story"
                },
                "questions": {
                    "value": "NA"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8442/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698771209827,
            "cdate": 1698771209827,
            "tmdate": 1699637052992,
            "mdate": 1699637052992,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "APkQhSSMIl",
                "forum": "O8ouVV8PjF",
                "replyto": "wCkfgHfJM8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8442/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8442/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer nftB"
                    },
                    "comment": {
                        "value": "Thanks for your valuable comments. Below we have addressed your questions and concerns point-by-point, and we have updated and added more clarifications to our manuscript accordingly.\n\n### Weaknesses\n\n> However, I have no confidence in any of the claims of interpretability and explainability. You made no effort to obtain ... go beyond downloading the UCR datasets. But if that is all you do, it seems like you should temper your claims about interpretability and explainability.\n\nThank you for the suggestions. We tried to access the original data of the GunPoint and Herring dataset, but it is not available. Nevertheless, to further support our claims about interpretability, we have added a real-world example usage of ShapeConv in EEG when stacked with deep models. Breifly speaking, we have excitedly discovered that a few shapelets are highly similar to waveforms in clinical textbooks. By aligning shapelets with data and conduct shapelet transform, we see clear separatable clusters. The overall experiments in Appendix D demonstrate the possibility of ShapeConv, when stacking with deep models, can benefit medical practitioners in practice by not only offering an accurate judgement, but also pointing out the area of interests with respect to their expert knowledge. **Please kindly refer to the revised version of our manuscript, Appendix D, for figures and more details.**\n\n> \u201cIn the realm of machine learning, interpretable time-series modeling stands as a pivotal endeavor, striving to encode sequences and forecast in a manner that resonates with human comprehension\u201d This (and the rest of the paper) read like flowery language [a].\n\nThanks for the suggestion. We will go through the paper and try to make the sstatements simplified and less confusing in the final version.\n\n> In fig 1, can you move the legend away from the data?\n\nThank you for pointing out. We have moved the legend away from the data.\n\n> \u201cis evaluated on the 25 UCR\u201d Did you mean \u201c125\u201d or \u201c25\u201d?\n\nHere the 25 UCR dataset is the subset of the 128 UCR dataset. We use this subset because the baseline method (ADSN) only reported this subset. We have clarified this in the paper.\n\n> \u201cFigure 5: Shaplets learned\u201d typo (Shapelets)\n\nThank you for pointing out. We have fixed the typo.\n\n> \u201cIt is evident that the shapelet learned by ShapeConv captures the distinguishing features of the class effectively\u201d Evident to whom? You should argue that the blue shapelets correctly represents the actors hand having to hover over the gun holster, then reach down to the gun, then draw the gun.\n\nThank you for the suggestions. We have added more details in the paper.\n\n> \u201cclustering task using 36 UCR univariate\u201d Why 36? Why this particular 36?\n\nWe use the 36 UCR dataset because the baseline method (AutoShape) only reported this subset. We have clarified this in the paper.\n\n> \u201cIn response to the first RQ, we observe that ShapeConv\u2019s shapelets (Figure 4 (a)) cover all turning points in the time series, where the two classes differ the most, while LTS\u2019s shapelets (Figure 4 (b)) do not cover the targeted regions.\u201d This evaluation is tautological. If \u201cturning points\u201d are the best places for shapelets, then we don\u2019t need any search for shapelets at all.\n\nIn this example, the turning points are the best places for shapelets. However, we agree that this is not always the case. We have clarified this in the paper.\n\n> \u201cIn contrast, when using human initialization,..\u201d Hmm, it is a bit tricky to claim results based on human initialization. Which humans, how trained are they in the system, how are they briefed. In my mind, that is a separate \u201chuman in the loop\u201d paper.\n\nWe agree that this is tricky to claim. Our goal is to show that ShapeConv has the ability to use human knowledge as prior, which is not possible in other shapelet-based models. \n\n> However, despite ingenious, the performance (missing a word?) However, despite ingenious suggestions, the performance\n>\n> gun out of the gun pocket (holster)\n>\n> \u201cwhile data from the \u201cfinger\u201d class don\u2019t.\u201d \u201cwhile data from the \u201cfinger\u201d class do not.\u201d (avoid contractions in scientific writing)\n>\n> This illustrate how This illustrates how\n\nThank you for carefully reading the paper and pointing out these typos. We have fixed the typos.\n\n> In table E.5, why four significant digits? This is spurious accuracy.\n\nWe use four significant digits because the results are taken directly from the respective papers. We have revised this in the paper to make it consistent.\n\n> In table E.5 and elsewhere, you report the average accuracy. This is meaningless for datasets of different sizes, class skews, number of classes, default rates etc. To be clear, it is not a flawed metric, it is just meaningless.\n\nWe agree that the average accuracy is meaningless for different datasets. However, the community has been using this metric for a long time. We have to report this metric to compare with the previous methods."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8442/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700592642723,
                "cdate": 1700592642723,
                "tmdate": 1700592642723,
                "mdate": 1700592642723,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aPvCDJMzUC",
                "forum": "O8ouVV8PjF",
                "replyto": "APkQhSSMIl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8442/Reviewer_nftB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8442/Reviewer_nftB"
                ],
                "content": {
                    "comment": {
                        "value": "You say \"We tried to access the original data of the GunPoint and Herring dataset, but it is not available\".\n\nHow did you try to \"access the original data of the GunPoint\"?\n\nTwo years ago my student asked the original auhours for \"the original data of the GunPoint\", They not only gave her the original data, they gave her the orginal video and photographs from the recording session, and they answered all her questions about it etc.\nIt is very surprising that they were so helpful to one researcher, but not to another."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8442/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700593826631,
                "cdate": 1700593826631,
                "tmdate": 1700593826631,
                "mdate": 1700593826631,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eRP56qhgyb",
                "forum": "O8ouVV8PjF",
                "replyto": "DNEHcP5BDc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8442/Reviewer_nftB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8442/Reviewer_nftB"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate your efforts in obtaining the original data for gun. And your general responsiveness.\n--\n\"morphology of waveform in EEG describes its overall shape\"\nThis is tautological, the definition of waveform morphology is overall shape.\nIf you meant to define the term, then \"The definition of waveform morphology is its overall shape\"\" \n--\n\"To this end, we visualized a few obtained shapelets out of 2,688 shapelets (128 shapelets per variate, 21 variates in total) and\nexcitedly found that some of them accord with some textbook waveform\"\nSorry to be skeptical, but if you searched thru  2,688 random examples of EEG, you would have found that some of them accord with some textbook waveform.\n--\nI noted\n\"In table E.5 and elsewhere, you report the average accuracy. This is meaningless for datasets of different sizes, class skews, number of classes, default rates etc. To be clear, it is not a flawed metric, it is just meaningless.\"\n\nAnd you say \"We agree that the average accuracy is meaningless for different datasets. However, the community has been using this metric for a long time. We have to report this metric to compare with the previous methods.\"\n\nYou agree that some results are meaningless, but you include them?  Why not just add some text to remind the reader that  average accuracy is meaningless?  If you continue the precedent, then the next author will have to report meaningless numbers too. Surely at some point this nonsense must stop.  \nIt is almost insulting to assume that \"Yeah, WE know this is meaningless stuff, but the reviewers are so dumb they expect it, so we will put it in\"\n\n--\nUltimately, I remain on the fence for this paper.\nIt is nicely written, and the idea of combining the CNN kernels with shapelets is nice.\nBut I was never really convinced by any claims of interpretability."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8442/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700678061419,
                "cdate": 1700678061419,
                "tmdate": 1700678061419,
                "mdate": 1700678061419,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ptQrS5aYKg",
            "forum": "O8ouVV8PjF",
            "replyto": "O8ouVV8PjF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8442/Reviewer_JCHN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8442/Reviewer_JCHN"
            ],
            "content": {
                "summary": {
                    "value": "The paper bridges the divide between traditional shapelets and modern deep learning methods in time-series modeling. Shapelets, while interpretable, face efficiency issues; deep learning models offer performance but lack interpretability. The proposed ShapeConv melds these approaches, using a CNN layer with its kernel functioning as shapelets. This layer is both interpretable and efficient, achieving state-of-the-art results in experiments. The introduction of shaping regularization and human knowledge further enhances its performance and interpretability."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper theoretically establishes an equivalence between traditional Shapelets and using a convolutional layer to derive similar features. It\u2019s a fresh perspective in utilizing shapelets in combination with deep learning methods and structure.\n2. The comprehensive experiments empirically demonstrate the superior performance of ShapeConv, in both classification as well as clustering tasks.\n3. The paper is well-written and easy to understand."
                },
                "weaknesses": {
                    "value": "1. An analysis of the computational complexity and resource requirements of ShapeConv could make the paper more comprehensive.\n2. Though the model's performance is promising, concerns may arise regarding the complexity of implementing ShapeConv compared to other traditional or deep learning models."
                },
                "questions": {
                    "value": "This study opted for a combination of CNN and Shapelets to enhance interpretability while also boosting performance. For time series classification tasks, why not choose the stronger baseline models for research, such as RNN or Transformer?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8442/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8442/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8442/Reviewer_JCHN"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8442/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698775734631,
            "cdate": 1698775734631,
            "tmdate": 1699637052832,
            "mdate": 1699637052832,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mmNOV2JSgU",
                "forum": "O8ouVV8PjF",
                "replyto": "ptQrS5aYKg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8442/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8442/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer JCHN"
                    },
                    "comment": {
                        "value": "Thanks for your valuable comments. Below we have addressed your questions and concerns point-by-point, and we have updated and added more clarifications to our manuscript accordingly.\n\n### Weaknesses\n\n> An analysis of the computational complexity and resource requirements of ShapeConv could make the paper more comprehensive.\n\nThe ShapeConv has the same computational complexity and resource requirements as the 1D convolution. The only difference is that ShapeConv has an additional regularization term and additional losses. These terms are negligible compared to the convolution operation. The total computational complexity of ShapeConv for a univariate time series is $O(l_x \\times l_s \\times n_{out})$, where $l_x$ is the length of the input time series; $l_s$ is the length of the shapelet, and $n_{out}$ is the number of shapelets. The advantage of ShapeConv is that is could be paralleled in GPU, making is much faster than traditional methods on CPU (2000x faster than LTS).\n\n> Though the model's performance is promising, concerns may arise regarding the complexity of implementing ShapeConv compared to other traditional or deep learning models.\n\nShapeConv is implemented as a Module in PyTorch similar to a common CNN layer, which can be easily integrated as a layer in any deep learning model. The implementation of ShapeConv is available at the supplementary material. Besides, the implementation of ShapeConv itself is simple and intuitive, which contains the convolution layer and some additional regularizers, aligning with Eq. (4) in Theorem 3.1.\n\n\n### Questions\n\n> This study opted for a combination of CNN and Shapelets to enhance interpretability while also boosting performance. For time series classification tasks, why not choose the stronger baseline models for research, such as RNN or Transformer?\n\nRNN and Transformer-based methods are already included in our baselines. Table F.2 shows results of ShapeConv and various RNN-based methods, and we also compared with Transformer-based model, TST [1], in Table 1 and appendix Table F.3. ShapeConv outperforms these methods on most of the tasks. These methods are not listed in the main paper since we only include most related and competitive methods due to the limited spaces.\n\n[1] George Z., et al. A transformer-based framework for multivariate time series representation learning. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pp. 2114\u20132124, 2021."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8442/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700592353305,
                "cdate": 1700592353305,
                "tmdate": 1700592353305,
                "mdate": 1700592353305,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8S8eBMgH3q",
            "forum": "O8ouVV8PjF",
            "replyto": "O8ouVV8PjF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8442/Reviewer_VPQr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8442/Reviewer_VPQr"
            ],
            "content": {
                "summary": {
                    "value": "This article deals with the classification of time series. The authors describe the equivalence between a particular approach, shapelets, and convolutional layers. They provide several losses to enforce the diversity of learned shapelets and closeness to original data, as well as intuitive initialization methods. The proposed approach is compared to several algorithms in a thorough experimental study."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The article describes the methodology well, and, to the best of my knowledge, the proposed initializations and losses are novel in the context of shapelets. The experiment study is extensive (with one caveat, see below) and convincing."
                },
                "weaknesses": {
                    "value": "- The main contribution is based on Theorem 3.1, which shows that the shapelet transform is somewhat equivalent to a convolution layer followed by a max pooling operation. However, this fact has been observed previously to provide accelerated shapelet transform: the authors of [1] show that computing the distance profile ($dist(\\mathbf{s}, \\mathbf{x})$ for a given sequence $\\mathbf{s}$ and all subsequences $\\mathbf{x}$ of $\\mathbf{X}$) is equivalent to a convolution. \n\n- An extensive review of time series classification algorithms exists on the same data sets, see [2] and more recently but unpublished [3]. None of the algorithms referenced in [2, 3] are compared to ShapeConv. The authors should at least compare themselves to the best-performing algorithms of the state-of-the-art.\n\n- (Minor comment.) It is considered bad practice to start sentences with mathematical symbols.\n\n[1] Yeh, C. C. M., Zhu, Y., Ulanova, L., Begum, N., Ding, Y., Dau, H. A., Zimmerman, Z., Silva, D. F., Mueen, A., & Keogh, E. (2016). Matrix Profile I: All Pairs Similarity Joins for Time Series: A Unifying View that Includes Motifs, Discords and Shapelets. Proceedings of the IEEE International Conference on Data Mining (ICDM), 1317\u20131322. https://doi.org/10.1007/s10618-017-0519-9\n\n[2] Bagnall, A., Lines, J., Bostrom, A., Large, J., & Keogh, E. (2017). The great time series classification bake off: a review and experimental evaluation of recent algorithmic advances. Data Mining and Knowledge Discovery, 31(3). https://doi.org/10.1007/s10618-016-0483-9\n\n[3] Middlehurst, M., Sch\u00e4fer, P., & Bagnall, A. (2023). Bake off redux: a review and experimental evaluation of recent time series classification algorithms. ArXiv. http://arxiv.org/abs/2304.13029"
                },
                "questions": {
                    "value": "In addition to addressing my comments about Theorem 3.1 and the comparison to the state-of-the-art, I have one question:\n- Convolutional layers are meant to be stacked. Unless I am mistaken, in the experiments, there is only one ShapeConv layer. Would the interpretability of ShapeConv remain if there are several layers?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8442/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8442/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8442/Reviewer_VPQr"
                    ]
                }
            },
            "number": 6,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8442/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699350643955,
            "cdate": 1699350643955,
            "tmdate": 1700726925271,
            "mdate": 1700726925271,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PRQ0w8Rbv1",
                "forum": "O8ouVV8PjF",
                "replyto": "8S8eBMgH3q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8442/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8442/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer VPQr (Part 1)"
                    },
                    "comment": {
                        "value": "Thanks for your valuable comments. Below we have addressed your questions and concerns point-by-point, and we have updated and added more clarifications to our manuscript accordingly.\n\n### Weaknesses\n\n> The main contribution is based on Theorem 3.1, ... is equivalent to a convolution.\n\nWe argue that our work is definitely different from the work in [1] despite some faint connections. The authors of [1] discuss the connection between z-normalized Euclidean distance and dot products so as to utilize convolution techniques such as the fast Fourier transform (FFT) to achieve speed-ups in searching similar subsequences. The method are mentioned to \"*suggest*\" candidate shapelets as *indicators*, and barely limited to sub-sequences of the original time series which cannot meet the needs of deep learning. We, to be noticed, rigorously prove the equivalence between a certain neural convolutional layer (not just convolution) and the originally defined shapelet in order to propose a newly interpretable and learnable neural kernel for deep learning. \n\nIn fact, we have properly cited the papers of [2] and [3] (also discuss the relationship between convolution and shapelet) which are more relevant and compared them with our work to highlight our contribution of demonstrating the strict equivalence between shapelet transform and a certain learnable neural operator. With GPUs\u2019 built-in parallel computation of for deep learning, our ShapeConv is naturally high-speed and does not need extra algorithms specifically designed for acceleration. Besides, we also put forward the diversity regularization and incorporate human-knowledge-based initialization to enhance the interpretability of our proposed method as non-negligible contributions.\n\n[1] Yeh, C. C. M., et al. (2016). Matrix Profile I: All Pairs Similarity Joins for Time Series: A Unifying View that Includes Motifs, Discords and Shapelets. Proceedings of the IEEE International Conference on Data Mining (ICDM), 1317\u20131322.\n\n[2] Arnaud L., et al. Learning dtw-preserving shapelets. In Advances in Intelligent Data Analysis XVI: 16th International Symposium, IDA 2017, London, UK, October 26\u201328, 2017, Proceedings 16, pp. 198\u2013209. Springer, 2017.\n\n[3] Yichang W., et al. Learning interpretable shapelets for time series classification through adversarial regularization. arXiv preprint arXiv:1906.00917, 2019.\n\n> An extensive review ... the best-performing algorithms of the state-of-the-art.\n\nSince our proposed ShapeConv is a deep-learning based new method and some of our baseline have outperformed the state-of-the-art algorithms in [8,9] (e.g. OS-CNN[4] outranking HIVE-COTE[5], WEASEL+MUSE[6] and InceptionTime[7] on both univaraite and multivariate datasets), we only conduct most relevant comparison with shapelet-based and deep-learning based approaches to avoid redundancy. Besides, we add comparison with ROCKET[10] in our revised version.\n\n[4] Wensi T., et al. Omni-scale cnns: a simple and effective kernel size configuration for time series classification. In International Conference on Learning Representations, 2021b.\n\n[5] Jason L., et al. Hive-cote: The hierarchical vote collective of transformation-based ensembles for time series classification. In 2016 IEEE 16th international conference on data mining (ICDM), pp. 1041\u20131046. IEEE, 2016.\n\n[6] Patrick S., Ulf L.. Multivariate time series classification with weasel+ muse. arXiv preprint arXiv:1711.11343, 2017.\n\n[7] Hassan I. F., et. al. InceptionTime: Finding AlexNet for Time Series Classification. arXiv preprints, arXiv:1909.04939, 2019.\n\n[8] Bagnall, A., et. al. (2017). The great time series classification bake off: a review and experimental evaluation of recent algorithmic advances. Data Mining and Knowledge Discovery, 31(3). \n\n[9] Middlehurst, M., et al. (2023). Bake off redux: a review and experimental evaluation of recent time series classification algorithms. arXiv: 2304.13029\n\n[10] Angus Dempster, et. al. Rocket: Exceptionally fast and accurate time series classification using random convolutional kernels. Data Mining and Knowledge Discovery, pp. 1\u201342, 2020.\n\n> (Minor comment.) It is considered bad practice to start sentences with mathematical symbols.\n\nThanks for pointing out. We have revised the paper accordingly."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8442/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700592043398,
                "cdate": 1700592043398,
                "tmdate": 1700622968559,
                "mdate": 1700622968559,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RsXVIQ3xKI",
                "forum": "O8ouVV8PjF",
                "replyto": "klUTdy2uU1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8442/Reviewer_VPQr"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8442/Reviewer_VPQr"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their detailed answers. I still find that the methodological contribution is limited but the amount of experimental validation is very convincing. I have upgraded my rating accordingly."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8442/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700726957522,
                "cdate": 1700726957522,
                "tmdate": 1700726957522,
                "mdate": 1700726957522,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]