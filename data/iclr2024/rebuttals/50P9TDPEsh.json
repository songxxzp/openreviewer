[
    {
        "title": "Critique Ability of Large Language Models"
    },
    {
        "review": {
            "id": "NW5KsbCn3m",
            "forum": "50P9TDPEsh",
            "replyto": "50P9TDPEsh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2045/Reviewer_qpMo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2045/Reviewer_qpMo"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a benchmark to evaluate the critique ability of LLMs. This benchmark consists of 3K high-quality natural language queries and their corresponding model responses. They also introduce a baseline for self-check, to improve the performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- To explore the critique ability of LLMs is interesting, and timely at this point. \n- This paper provides a standardized way to evaluate the critique ability of LLMs on diverse tasks, \n- The paper offers several noteworthy insights, such as the challenges associated with self-critique in LLMs. These findings can guide future research and model development."
                },
                "weaknesses": {
                    "value": "- The evaluation is not comprehensive. While it claims to evaluate the critique ability, it only evaluates this across three tasks: math, code, and commonsense. A broader range of tasks should be tested.\n- The paper does not discuss potential biases. Without discussing these biases, it's unclear how they might influence the evaluation results, which could affect the validity of the findings.\n- Authors could offer a more in-depth analysis of the utility of self-critique. Understanding why self-critique could be better and its influence on critique capabilities would strengthen the paper's arguments.\n- The paper's presentation appears disjointed. The content seems pieced together without careful review. Consistency in terminology is essential for clarity.\n- The paper does not define key terms like the policy model and critic model. \n- Lack of related work.\n- Despite introducing a benchmark, the authors do not release it, limiting its utility and reproducibility for the research community."
                },
                "questions": {
                    "value": "- What is the rationale behind choosing different values of k, specifically k = 64 for GSM8K and TruthfulQA, and k = 100 for HumanEval? \n- In Section 5, the phrase \"Assume with appropriate prompting\" is mentioned. Could you provide a detailed explanation of how the prompting was conducted in this step? There are certain aspects that remain ambiguous. Could you clarify these points to ensure a comprehensive understanding for the readers?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2045/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2045/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2045/Reviewer_qpMo"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2045/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698569537354,
            "cdate": 1698569537354,
            "tmdate": 1699636136079,
            "mdate": 1699636136079,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SCxCi3IMER",
                "forum": "50P9TDPEsh",
                "replyto": "NW5KsbCn3m",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2045/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2045/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your thorough and detailed feedback, reviewer qpMo. We are grateful for the time you've invested in reviewing our paper. We regret that certain aspects of our paper may not have been as clear as intended, which seems to have led to some misunderstandings. Rest assured, we will endeavor to clarify these points as follows.\n\n1. We've addressed this in our \"common response to all\" and invite you to review it for more details. Regarding your use of the word \"claim\", which means *\u201cstate or assert that something is the case, typically without providing evidence or proof\u201d*[1], we understand it implies we state to evaluate critique ability while actually not. We welcome specific insights on why you believe we fail to do so. We've included tasks including QA&Classification, reasoning, coding in the paper, and additional preliminary numbers for generation tasks in the rebuttal. However, we're open to suggestions on other NLP tasks or domains we might have missed and are eager to incorporate them in future work. Lastly, there is no dataset regarding common-sense in our paper.\n2. We understand the workload of reviewing tasks can be heavy, leading to skipping some details in the paper. We are more than willing to guide you through the critical sections of our paper. Specifically, in Section 3.2, spanning pages 3 to 6, we explore various factors that could potentially *\u201cinfluence evaluation results\u201d*. This includes the quality of queries and responses, the impact of model sizes, and model certainty. Moreover, for your convenience, we draw your attention to Sections 4.2 and 4.3, on pages 7 to 9, where we delve into how model size and certainty play a pivotal role in evaluation results. Those discussions and empirical observations provide valuable insights for us on how to construct a robust evaluation benchmark \u2014 which is the main focus of this paper.\n3. We made every effort to comprehend the question but failed. What exactly does *\"why self-critique could be better\"* mean? Are you asking whether self-critique should be superior to normal critique, or if it should be improved beyond what we presented in the paper? We never state anything close to *\u201cself-critique should be better (than something)\u201d*. Assuming we understand the basis of such a question, then what is the meaning of *\"its (the reason's) influence on critique capabilities\u201d*? By definition, self-critique is a special form of critique. Thus, the question *\"(a reason) why self-critique could be better (than something unspecified) and its influence on critique\"*, remains unclear to us. We recommend that the reviewer refine their question to make its intention and meaning clearer to the audience.\n4. We regret that the paper seemed challenging to comprehend. We acknowledge that if certain sections, particularly those between pages 3 and 9 based on your Question 2, were skipped, the flow of content might indeed appear disjointed. Our sincere apologies for any bad experience this might have caused. We are open to constructive feedback and would greatly appreciate any specific suggestions you might have to enhance the clarity and coherence of our work.\n5. In the context of LLM, where RLHF [2] being a very common and basic approach for LLM tuning, \u201cpolicy model\u201d typically means the LLM that takes inputs (states) and makes actions (outputs) for the base task. \u201cCritic\u201d means *\u201ca person who expresses an unfavorable opinion of something\u201d*[1], we simply use its literal English meaning to indicate a model providing critiques.\n6. Due to the extensive content and details in the paper, we have chosen to include only a brief discussion of related work in Section 2. We thank the reviewer for highlighting this issue and will consider adding more detailed literature in the appendix.\n7. This is incorrect. (1) The dataset is ready but currently under legal and compliance review to ensure safety and prevent the leakage of sensitive information. (2) Our institution has very strict rules regarding data publication, requiring all data releases to go through official channels. To maintain the anonymity of the submission, it is impossible to show the data during the review period. This is a common practice in large and responsible institutions, and we do not anticipate criticism for this decision.\n\nAnswers to questions:\n\n1. PaLM-2 tech report (Google et al., 2023) demonstrates that such choices of k could guarantee an acceptable rate of correct answers for questions in those datasets.\n2. Due to the length limitations imposed by ICLR, we cannot include all details in the main body of the paper, particularly for the prompt template, which is quite extensive. As mentioned at the beginning of Section 4, all detailed settings, including the construction of the prompt, are described in Appendix F. Specifically for Critic-GSM8K, all the details are available in Appendix F.1.\n\n[1] Oxford Languages. https://languages.oup.com/google-dictionary-en/\n\n[2] https://arxiv.org/pdf/2009.01325.pdf"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2045/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700719253320,
                "cdate": 1700719253320,
                "tmdate": 1700719253320,
                "mdate": 1700719253320,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8EcckOIEUs",
            "forum": "50P9TDPEsh",
            "replyto": "50P9TDPEsh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2045/Reviewer_WsbC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2045/Reviewer_WsbC"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents an investigation into the critique abilities of Large Language Models (LLMs) across various tasks. The authors introduce a new benchmark, CRITICBENCH, which consists of 3K high-quality natural language queries and corresponding model responses annotated for correctness. The benchmark covers tasks such as math problem-solving, code completion, and question answering. The study evaluates multiple LLMs on the dataset and introduces a simple yet effective baseline method named self-check, which leverages self-critique to improve task performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper addresses an important and under-explored aspect of LLMs, which is their ability to critique their own outputs. This is a valuable contribution as it moves beyond traditional evaluation metrics and looks at a model's ability to self-improve.\n\n2. The paper presents a clear definition of critique ability and distinguishes between critique and self-critique, which helps in setting the scope and understanding the objectives of the study."
                },
                "weaknesses": {
                    "value": "1. The paper could benefit from a more detailed discussion on the limitations of the current approach, particularly regarding the scalability of the self-check method and its applicability to real-world scenarios [1,2,3].\n\n2. The study is limited to a few tasks and datasets. Expanding the benchmark to include more diverse tasks and domains would make the findings more generalizable.\n\n3. The evaluation of self-critique abilities shows that models struggle with certain tasks, but the paper does not delve deeply into why this is the case or propose potential solutions to improve self-critique performance.\n\n4. The paper does not address the potential ethical implications of models that can self-critique and self-improve, especially in terms of reduced human oversight.\n\nReferences \n\n[1] Madaan, Aman, et al. \"Self-refine: Iterative refinement with self-feedback.\" arXiv preprint arXiv:2303.17651 (2023).\n\n[2] Krishna, Satyapriya. \u201cOn the Intersection of Self-Correction and Trust in Language Models.\u201d (2023).\n\n[3] Huang, Jie, et al. \"Large language models cannot self-correct reasoning yet.\" arXiv preprint arXiv:2310.01798 (2023)."
                },
                "questions": {
                    "value": "'None"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2045/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699421879170,
            "cdate": 1699421879170,
            "tmdate": 1699636136015,
            "mdate": 1699636136015,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QB0FAfA4b5",
                "forum": "50P9TDPEsh",
                "replyto": "8EcckOIEUs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2045/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2045/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "- In the three references the reviewer mentioned, one [3] became publicly available **after** our ICLR submission, and another [2] even **did not exist on the public internet** when we received the review. We are open to including further discussion on these studies in future revisions. However, for this ICLR submission, with the deadline being **September 28**, we believe it would be **UNFAIR** to be criticized for not discussing papers submitted to arXiv in **October** or **November**.\nAdditionally, the main focus of our paper is the comprehensive evaluation of the critique ability of LLMs, as detailed in Sections 3 and 4 including extensive discussions on scalability, generalizability, and quality. The self-check baseline, as emphasized in the paper, serves merely as a simple example demonstrating the potential use of critique ability. Although this method surpasses some prior approaches like [1], we actually do not consider it a major contribution to our work.\n- We've addressed this in our \"common response to all\" and invite the reviewer to check it for more details.\n- The purpose of this paper is to introduce a standard framework for assessing the critique ability of LLMs. The questions raised here fall outside the scope of this paper. Nevertheless, we do discuss why models underperform on certain tasks and propose potential solutions.\n  - In Section 4.1, regarding Critic-HumanEval, we note: *\"This is somewhat anticipated, as evaluating the correctness of a code snippet without execution is often challenging even for expert software engineers. It is likely to gain a notable improvement when augmented by a code interpreter tool.\"* \n  - Furthermore, in Section 4.2 on Critic-TruthfulQA, we note: *\"We hypothesize the disparity is due to the underlying reasons of a model answering incorrectly to queries. For TruthfulQA, the wrong answers largely stem from false beliefs or misconceptions in models, which would also lead to critique failures.\"*\n- This question is far beyond the scope of this paper. None of the references mentioned in the first question of this comment address issues like human oversight. In fact, we did discuss this topic in the last paragraph of Section 2. Please refer to the paper for more details."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2045/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700718635954,
                "cdate": 1700718635954,
                "tmdate": 1700718635954,
                "mdate": 1700718635954,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "86snFxKYr8",
            "forum": "50P9TDPEsh",
            "replyto": "50P9TDPEsh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2045/Reviewer_qunD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2045/Reviewer_qunD"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a new dataset to evaluate language model's capability of identifying flaws in language model outputs, referred to as the critique ability. The dataset is constructed fully automatically based on language model outputs on three datasets. The authors use various filtering strategies to ensure that the data is of high quality and can effectively differentiate models. The whole process is fully automated, so theoretically it can be extended to other task as well. The authors then use the dataset to evaluate a series of pretrained language models of various sizes to examine their critique abilities as well as the scaling laws."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is well-written and easy to follow. The authors are very clear about all details in the data collection process and provided good motivation for the various design choices. The evaluation is thorough and covers a wide range of models. The proposed new heuristic is not particularly novel, but achieves solid improvement on the new benchmark."
                },
                "weaknesses": {
                    "value": "A critique in this paper is defined as a language model assessment of another language model output on some underlying task. A good critique model should be effective at identifying flaws in language model outputs. The challenging examples to the task of critique are nuanced flaws, which would also require a detailed explanation by the critique model. But the benchmark proposed by this paper use a simplistic quantitative metric that reduces the quality of a critique to a binary decision, which assumes that it\u2019s appropriate to use a binary metric for the underlying task as well. The benchmark offers very limited granularity.\n\nUsing a granular quantitative measure means that the qualitative questions that the benchmark can answer are also limited. Outside of developing and evaluating self-refinement heuristics like the one proposed by the authors, the benchmark provides limited information for other uses of model-generated critique, such as informing human oversight. Since the benchmark requires tasks with well-defined, fully-automated metrics for the underlying task, the problem of developing self-refinement critiques does not in fact depend on such a benchmark: even if the model critique doesn\u2019t make sense to a human, as long as it improves subsequent prediction accuracy, it\u2019s a good critique."
                },
                "questions": {
                    "value": "The larger models seem much better at critiquing outputs from large models than smaller models. Looking at figure 4, large models have much smaller advantage on critiquing small model outputs than large model outputs. Does this mean the critique ability measurements are inflated by improvement in accuracy on the underlying task?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2045/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699499088334,
            "cdate": 1699499088334,
            "tmdate": 1699636135953,
            "mdate": 1699636135953,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Oktl5FC61X",
                "forum": "50P9TDPEsh",
                "replyto": "86snFxKYr8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2045/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2045/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the time and effort! Please find our responses below.\n\n## Binary metric only offers high-level granularity\n\nWe are aware of the limitations of a binary (correct/incorrect) metric regarding granularity. However, this is actually an intentional choice.\n\n1. Firstly, regarding motivation: This approach is a **trade-off** for better **generalizability**. Although we could design more fine-grained metrics for evaluation, such as distinguishing between calculation errors and reasoning errors in math tasks, these metrics are not as generalizable to other tasks. For example, in coding tasks, people may care more about whether there are syntax errors, compile errors, or runtime errors. Considering the limited studies on the critique ability of LLMs in the community, we prioritize broader generalizability across various domains and task types rather than delving deeper into one specific task.\n2. Secondly, in terms of practice: Despite its limitations, a binary metric can still provide insights into how well a model critiques. In the second paragraph of Section 4, we note, *\"In evaluation, we focus solely on the accuracy of this final judgment, disregarding the correctness of the intermediate analysis, as empirical evidence has shown a strong correlation between the accuracy of intermediate chain-of-thought and the final answer (Wei et al., 2022b; Lewkowycz et al., 2022; Fu et al., 2023a).\"* \n\n## \u201ceven if the model critique doesn\u2019t make sense to a human, as long as it improves subsequent prediction accuracy, it\u2019s a good critique.\u201d\n\nWe **strongly disagree** with this statement due to its potentially **dangerous** implications. For instance, consider the task of determining patients' depression levels based on their therapy session transcripts. A model might incorrectly assess a depressed or anxious minority patient as having a low level of depression. If a critique model then suggests, \u201cThe patient mentioned they are a minority. All minorities are weak and vulnerable, so they should be classified as having high-level depression,\u201d this could make the downstream task to correct its prediction. Based on the reviewer\u2019s statement, the subsequent prediction accuracy increased so it\u2019s a good critique. However, the critique here is not only illogical to humans but also **biased** and **harmful**.\n\nThis is a realistic task and it's a fact that minority groups often exhibit higher levels of depression in psychological studies, typically due to exposure to discrimination and unfair treatment, especially during childhood [1]. Therefore, the critique mentioned above is not only inaccurate but also detrimental, despite its potential to increase downstream accuracy.\n\nEvaluating the critique capabilities of LLMs is crucial. Downstream accuracy is not the sole indicator of effectiveness. An accurate, precise, and reasonable critique itself is equally important. Our current benchmark represents an initial step in thoroughly assessing the critique abilities of LLMs. While it is not yet perfect, we recognize the significance of this task.\n\n[1] https://www.nature.com/articles/s44159-023-00241-5"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2045/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700717773288,
                "cdate": 1700717773288,
                "tmdate": 1700717773288,
                "mdate": 1700717773288,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]