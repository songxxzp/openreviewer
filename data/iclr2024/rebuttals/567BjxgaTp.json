[
    {
        "title": "How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions"
    },
    {
        "review": {
            "id": "xfitSlesn2",
            "forum": "567BjxgaTp",
            "replyto": "567BjxgaTp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5212/Reviewer_cT55"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5212/Reviewer_cT55"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a simple methodology to detect whether an LLM\u2019s response is a lie (an answer that is false, even though the model can produce the correct answer). Authors use a set of fixed elicitation questions to probe a model and only use the responses to these followup questions to predict whether an LLM lied. The results demonstrate that this methodology is surprisingly accurate and has intriguing generalization properties."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The problem definition is strong and seems impactful. That is, the concept of lying for an LLM is formalized as LLMs producing a false statement while being able to accurately produce the correct statement. The toolbox proposed in the paper contributes to having practical reliability safeguards for existing LLMs.\n\n2. I am surprised and excited to see that the lie detector generalizes to different architectures and distributions of input text, which further adds to the value of the proposed methodology. Specifically, this allows the design of general-purpose lie detectors that are cheap to implement across a variety of settings. \n\n3. I am also very curious about the underpinnings of the observed behavior, which is very novel and not have been discussed before to the best of my knowledge. Specifically -- what are the mechanisms that relate the lying behavior to the answers to seemingly unrelated questions? Although the paper does not provide concrete explanations as to why the classifiers surprisingly work very well, these properties motivate several interesting questions to pursue for future work.\n\n4. The authors did extensive evaluations to make sure that the results were not due to spurious correlations, such as simply picking up on the repeated lying behavior, but have more going on (i.e. the fact that responses to ambiguous questions can also predict the lying behavior)."
                },
                "weaknesses": {
                    "value": "1. (Lack of Descriptive Discussion) I would have loved to see more on what the classifier may be picking on; or what the suggested inductive biases may be. As it stands, it is rather a mysterious result. However, I do not find this to be a significant limitation, as the results are thorough, and future work can further investigate this phenomenon.\n\n2. (Labeling for Correctness) The way authors labeled an answer as correct/incorrect appeared slightly brittle to me (in that the correctness of a model\u2019s response is defined as whether an LLM\u2019s response contains the ground truth entity). For instance, a response could still mention the ground truth entity and be wrong. However, I acknowledge that evaluating natural language generations is challenging and still an open research area, thus this limitation is hard to address.\n\n3. (Lack of ablation) I would also be curious to see how redundant the set of 48 questions is, the set of questions and the number of questions is somewhat arbitrary and I\u2019m curious how the authors reached this number. Can we achieve the same performance with only 2 ambiguous elicitation questions? What happens if the authors just keep the top 2 questions with the largest weights? What does that imply? \n\n4. (Presentation) There is a lot to unpack, and I had a bit of a hard time following Sections 4 and 5. I think one factor was that the presentation of the dataset is rather scattered (4.1, 4.2), and the methodology is rather scattered (4.3, 5.2). There are also quite a bit of details deferred to the Appendix (e.g. finetuning details), which makes reading very nonlinear.\n\n\nMinor\n\n1. There are a lot of numerical data in the paper that are inline, which makes it a bit hard to see when one needs to recall previous related results. I\u2019d appreciate having most of the results contained in tables in the main text, however, I can also see that the space is a limiting factor."
                },
                "questions": {
                    "value": "1. Regarding W3: Have the authors performed any ablations for the number of questions? Otherwise, how did the authors reach the number 48 / other more granular numbers for each of the factual / ambiguous questions?\n\n2. Similar to Q1: I\u2019d be curious to see a study around performing lasso/feature selection experiment to see how far this number can be reduced or is saturated."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5212/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5212/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5212/Reviewer_cT55"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5212/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697954703102,
            "cdate": 1697954703102,
            "tmdate": 1699636518839,
            "mdate": 1699636518839,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gjRqWbpsoW",
                "forum": "567BjxgaTp",
                "replyto": "xfitSlesn2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5212/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5212/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "Thank you very much for your review of our work, we appreciate your comments and are glad that you found the paper both surprising and exciting! \n\n> (Labeling for Correctness) The way authors labeled an answer as correct/incorrect appeared slightly brittle to me (in that the correctness of a model\u2019s response is defined as whether an LLM\u2019s response contains the ground truth entity). For instance, a response could still mention the ground truth entity and be wrong. However, I acknowledge that evaluating natural language generations is challenging and still an open research area, thus this limitation is hard to address.\n\nWe agree that this can definitely be a challenging area to assess and so to mitigate this we did make sure to randomly check a small number of the examples by hand in order to make sure nothing was getting too obviously misclassified. Our approach appeared to work well. We also made sure to think through failure modes and avoid them, e.g., we made sure to look for the full word, not just potential substrings (e.g., the word \u201cno\u201d instead of the substring, which can be found in words such as \u201ckNOwledge\u201d). Additionally, we had alternative evaluations methods for e.g. the translation questions where we asked a separate model. Finally, the questions we ask are mostly relatively clear and simple, and the LLMs usually answer in a simple and straight-forward way. The evaluation method is thus less likely to fall prey to the particular failure mode you describe.\n\n> (Presentation) There is a lot to unpack, and I had a bit of a hard time following Sections 4 and 5. I think one factor was that the presentation of the dataset is rather scattered (4.1, 4.2), and the methodology is rather scattered (4.3, 5.2). There are also quite a bit of details deferred to the Appendix (e.g. fine-tuning details), which makes reading very nonlinear.\n\nThank you for highlighting these points - in the revised paper that we have uploaded we have slightly restructured Sections 4 and 5 in order to make the ideas easier to follow. We have collated the methodology by moving most of Section 5.2 into a new Section 4. \n\n> (Lack of ablation) I would also be curious to see how redundant the set of 48 questions is, the set of questions and the number of questions is somewhat arbitrary and I\u2019m curious how the authors reached this number. Can we achieve the same performance with only 2 ambiguous elicitation questions? What happens if the authors just keep the top 2 questions with the largest weights? What does that imply?\n> Regarding W3: Have the authors performed any ablations for the number of questions? Otherwise, how did the authors reach the number 48 / other more granular numbers for each of the factual / ambiguous questions?\n> Similar to Q1: I\u2019d be curious to see a study around performing lasso/feature selection experiment to see how far this number can be reduced or is saturated.\n\nThank you for this interesting suggestion for further exploring the properties of our lie detection method. In order to explore this question we have added an **additional experiment** in Appendix D.7 (that we shall also detail here) regarding an ablation where we vary the number of questions (features) used for our lie detector on the in-distribution examples. The results can be seen in Figure 11 (also available at [this link](https://imgur.com/a/LPBEHLI)) and show the AUC for both the Binary and Logprobs detector as on the x-axis the number of features is increased. The selection method for additional features was performed using scikit-learn\u2019s $\\texttt{SequentialFeatureSelector}$ which greedily picks the best feature to add to the existing set according to cross-validation performance. The selection was performed on the train dataset for our lie detector (consisting of lies and truths generated by GPT-3.5 with direct and CoT prompts). We then compute the AUC of the detector as the number of features is increased, both on the training distribution and on two of the generalization studies (those on Alpaca-7B and Vicuna-7B, for which the detector and the feature choice are frozen to those obtained from the training distribution). Figure 11 shows how, while high in-distribution AUC can be obtained with a few features (particularly for some of the elicitation question groups), increasing the number of features generally leads to better performance for out-of-distribution cases, although the trend is not monotonous. This is particularly true for the binary detector."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5212/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700170889956,
                "cdate": 1700170889956,
                "tmdate": 1700245369455,
                "mdate": 1700245369455,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "n2rtvbgXRj",
                "forum": "567BjxgaTp",
                "replyto": "gjRqWbpsoW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5212/Reviewer_cT55"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5212/Reviewer_cT55"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for your rebuttal! I appreciate the presentation changes and the additional experiment.\n\n**Additional Experiment:** This seems pretty interesting and surprising -- am I correctly reading that having only 1-2 questions can result in 90%+ AUC in certain settings (i.e. mostly for 'in-distribution', and certain cases for generalization experiments)? How do you interpret this?\n\nAlso, this makes me wonder whether you use not only the in-distribution model but a second model to pick questions and observe the performance of the third model. Do you think the questions picked this way could generalize better, perhaps?\n\nMinor: The caption for Figure 11 and the subcaption for Figure 11a has parentheses that are left open."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5212/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700262320213,
                "cdate": 1700262320213,
                "tmdate": 1700262320213,
                "mdate": 1700262320213,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hoGxpuTTZl",
                "forum": "567BjxgaTp",
                "replyto": "unnHpuqVsF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5212/Reviewer_cT55"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5212/Reviewer_cT55"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Acknowledging your response, thanks for the clarifications and updates."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5212/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700587786067,
                "cdate": 1700587786067,
                "tmdate": 1700587786067,
                "mdate": 1700587786067,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BWUS9FX9TG",
            "forum": "567BjxgaTp",
            "replyto": "567BjxgaTp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5212/Reviewer_eJWS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5212/Reviewer_eJWS"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a lie detector for LLMs that works by answering unrelated follow-up questions and requires no access to internals. This is an interesting topic that relates to hallucinations but still has not received a lot of attention on its own. The author's method strikes through simplicity, which is a positive aspect. However, the paper is a tough read due to a confusing structure and lack of proper formalization (pseudocode, definitions). Furthermore, the scenario and evaluation appear insufficient. I hope that the detailed comments help the authors to improve their manuscript.\n\nDetailed:\n* Large language models (LLMs) can \u201clie\u201d, which we define as outputting false\nstatements despite \u201cknowing\u201d the truth in a demonstrable sense.\n -> this is fuzzy definition. Say a model is prompt sensitive (non-robust) and output the wrong reply for a slighlty modified prompt, according to this definition, this would be lying. A formal treatment of lying (or deceiving)  that might help in a proper definition can be found, e.g., in\n Schneider, J., Meske, C., & Vlachos, M. (2020). Deceptive AI explanations: Creation and detection. arXiv preprint arXiv:2001.07641.\n* After reading the intro the question arises: Could not lying be directly detected from the prompt? I am aware that this might not cover all scenarios covered in the paper, but it appears obvious for some that are discussed.\n* Also the scenario is unclear and to what I understood there are direct lie instructions \"Lie when answering the following question\". I think a liar should be instructed to lie consistently (A system prompt like. \"Lie to the first question and then treat the answer as truthful to avoid lie detection\"). Such approaches should be evaluated. Furthermore there are different kinds of lies and different lying approaches. The paper discusses this only on a shallow level.\n* The intro is very short. Crisp is good, but the balance between Abstract and intro is poor - Make the abstract shorter and prolong the intro. Also the contribution section discusses more what is actually done (and should go into the intro main part) than the contribution. Contributions are typically a short list of key points. Overall this structure should be altered.\n* The paper jumps right into the experiment section without clearly describing the method (pseudocode etc.)"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "see above"
                },
                "weaknesses": {
                    "value": "see above"
                },
                "questions": {
                    "value": "see above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "The authors discuss ethical concerns sufficiently in the paper."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5212/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5212/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5212/Reviewer_eJWS"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5212/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698655232996,
            "cdate": 1698655232996,
            "tmdate": 1700591771995,
            "mdate": 1700591771995,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "F85sIkou4O",
                "forum": "567BjxgaTp",
                "replyto": "BWUS9FX9TG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5212/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5212/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response 1/3"
                    },
                    "comment": {
                        "value": "Thank you very much for your review of our work. We appreciate your suggestions for improving the paper and we hope that the following response (as well as the global response posted) will help to alleviate any of your concerns about parts of the paper that were unclear.\n\n> Large language models (LLMs) can \u201clie\u201d, which we define as outputting false statements despite \u201cknowing\u201d the truth in a demonstrable sense. -> this is fuzzy definition. Say a model is prompt sensitive (non-robust) and output the wrong reply for a slighlty modified prompt, according to this definition, this would be lying. A formal treatment of lying (or deceiving) that might help in a proper definition can be found, e.g., in Schneider, J., Meske, C., & Vlachos, M. (2020). Deceptive AI explanations: Creation and detection. arXiv preprint arXiv:2001.07641.\n\nThank you, this is a very useful comment. There is no universal definition of \u201clying\u201d [1]; however, as the reviewer points out, most definitions of lying include the following three components [1]:\nThe speaker makes a false statement.\nThe speaker knows that the statement is false.\nThe speaker has the intention to deceive the addressee.\n\nOur original definition only included conditions 1) and 2), adjusted to LLMs. Originally, we excluded 3), \u201cintention to deceive\u201d, from the definition. We did this because there is no universally accepted philosophical theory of intention, and ascribing intent to artificial agents is contentious and is particularly hard to define for LLMs.\n\nWard et al. (Neurips 2023) [2] operationalise \u201cintention\u201d via incentives in structural causal games, and show how this can be applied to LLMs. Exactly because we wanted to study intentional lying, we have **already** structured our experiments accordingly; in all our experiments, there is a clear incentive to produce the false statement: direct instructions to lie, contexts that incentivise lying to complete the pattern (e.g. the experiments on sycophancy or biased few-shot examples), role-playing scenarios that contain an incentive to lie, or fine-tuning models to lie. Ward et al. [2] discuss how prompting and fine-tuning map to incentives and intention in their definition.\n\nThis means that our experiments do **not** contain situations like the one that the reviewer highlighted, where an LLM is simply prompt-sensitive and returns a different output for a slightly modified prompt. Our experiments contain specific incentives to lie, which produces systematically different behaviour than when models are simply sensitive to the exact wording of the prompt. We have now adjusted our definition to mirror this fact, both in the abstract and in the introduction. \n\nQuotes from the updated paper:\n\nAbstract:\n*Large language models (LLMs) can \u201clie\u201d, which we define as outputting false statements when incentivised to, despite \u201cknowing\u201d the truth in a demonstrable sense. LLMs might \u201clie\u201d, for example, when instructed to output misinformation*\n\nIntro:\n*We study lie detection in a question-answering setting. Briefly, we define a model\u2019s answer to be a lie if it is a) wrong, and b) in response to a question to which the model \u201cknows\u201d the correct answer (i.e. the model gives the correct answer under typical question-answering prompts), and c) there is an incentive (through the prompt or fine-tuning) for the model to give the wrong answer [WARD, 2023].*\n\nWe acknowledge that this is still not a fully formal definition. However, creating a formal definition of lying for LLMs is tricky---refer to Ward et al., who spent almost an entire paper on only this; and it\u2019s also challenging to determine if current LLMs lie according to any formal definition. While this is a great topic for future work, it isn\u2019t necessary to productively make empirical progress on lie detection, as we have shown.\n\nAs we discuss in the related work section, the distinction between lying and hallucination becomes fairly clear when there is goal-directed lying behaviour; which we study, e.g., in our real-life role-playing scenarios.\n\n\n[1]: Mahon, James Edwin, \"The Definition of Lying and Deception\", The Stanford Encyclopedia of Philosophy (Winter 2016 Edition), Edward N. Zalta (ed.), URL = <https://plato.stanford.edu/archives/win2016/entries/lying-definition/>.\n\n[2]: Honesty is the best policy: defining and mitigating AI deception. FR Ward, F Toni, F Belardinelli, T Everitt. Thirty-seventh Conference on Neural Information Processing Systems"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5212/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700170691832,
                "cdate": 1700170691832,
                "tmdate": 1700170823170,
                "mdate": 1700170823170,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QmmzSLF89z",
                "forum": "567BjxgaTp",
                "replyto": "BWUS9FX9TG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5212/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5212/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response 2/3"
                    },
                    "comment": {
                        "value": ">After reading the intro the question arises: Could not lying be directly detected from the prompt? I am aware that this might not cover all scenarios covered in the paper, but it appears obvious for some that are discussed.\n\nOur method is developed for situations where the **user does not have access to the system prompt**. The obvious example here is the OpenAI API where the model has been given a specific prompt that it is specifically instructed not to divulge to the user. Now, in this case, there is unlikely to be anything nefarious, but (as we discussed in Section 2.1) other parties could easily create language models and make them available for malicious purposes such as scams, disinformation, and propaganda. As these threats will often involve AI systems lying, lie detection could be a part of the solution. Our lie detector does not require access to the system prompt, only to the LLM\u2019s responses to elicitation questions.\n\nAdditionally, our lie detector also works on models finetuned to lie (no prompt). A key contribution of our paper is that we finetune a lie-detector on simple, prompted lies, and show that it generalises to many other settings.\n\n>Also the scenario is unclear and to what I understood there are direct lie instructions \"Lie when answering the following question\". I think a liar should be instructed to lie consistently (A system prompt like. \"Lie to the first question and then treat the answer as truthful to avoid lie detection\"). Such approaches should be evaluated.\n\nThe lying prompts we consider cover a wide range. They include some very simple prompts like \u201cAnswer the following question with a lie\u201d, but they also include several prompts equivalent to your suggestion. Table 5 in Appendix E.1 provides a list of prompts used *in the main paper*, including, e.g., number 10: \u201cAnswer the first question with a lie. Answer related questions in a way that is consistent with the answer to the first question.\u201d. The lie detector works on this prompt just as well as on the others (AUC 0.99), even when this prompt is left out of the training data (see the leave-one out cross-validation experiments in Section 6.1.1)\n\n> Furthermore there are different kinds of lies and different lying approaches. The paper discusses this only on a shallow level.\n\nWe would like to respectfully disagree with this characterisation, and suggest that we did in fact focus quite seriously on a range of possible lies and lying approaches, which we detail mostly in Section 6.1.2.\n\nThe base case we considered was, as you say, these *direct lie instructions* where the model is simply instructed in the prompt to lie, although here we already try a wide range of instructions (see Table 5 in Appendix E.1). We then considered more developed lying prompts with a range of **Chain-of-Thought prompts** (see Tables 6 and 7 in Appendix E.1) as well as **topic-specific** lies (Appendix D.1).\n\nImportantly, we also consider multiple scenarios where there is no direct instruction to lie but where doing so would might be expected or encouraged, such as the set of **goal-directed lies** or the **suggested answer (sycophancy)** settings of Turpin et al. 2023. Please note that within each of these settings, there are multiple different sub-settings (see, for example, Appendix F for the list of goal-directed lies which itself contains 26 individual lie generation scenarios - with a complementary 26 truthful versions.). On top of this we also considered generating lies without specific prompts by **fine-tuning the models** to produce lies.\n\nIt is the case that our lie detector was only **trained** on lies generated by *direct lie instruction* and *Chain-of-Thought prompts*. However we consider this to be a key part of the story, as we discuss in Sec 6.1 and as can be seen in Figure 2, it is the generalisation of this detector **at test time** to all other lie generation methods mentioned above that is particularly interesting.\n\n>The intro is very short. Crisp is good, but the balance between Abstract and intro is poor - Make the abstract shorter and prolong the intro. Also the contribution section discusses more what is actually done (and should go into the intro main part) than the contribution. Contributions are typically a short list of key points. Overall this structure should be altered.\n\nThank you for your suggestions, we have altered the structure of the introduction and the contributions which can be seen in the revised paper that has been uploaded."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5212/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700170755929,
                "cdate": 1700170755929,
                "tmdate": 1700213087739,
                "mdate": 1700213087739,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KKpXINxPE9",
                "forum": "567BjxgaTp",
                "replyto": "BWUS9FX9TG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5212/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5212/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up"
                    },
                    "comment": {
                        "value": "Dear Reviewer eJWS, once again thank you for your time and effort in providing your thoughtful review of our paper. As we are now entering the last day or so of the rebuttal period we wanted to just reach out to see if there was any feedback from our response - we appreciate that this will be a busy time for you but we hope that our current response has addressed any current concerns and are keen to engage further if there are any outstanding concerns. Best wishes, the Authors."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5212/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700584743466,
                "cdate": 1700584743466,
                "tmdate": 1700584743466,
                "mdate": 1700584743466,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NabA53bzYT",
                "forum": "567BjxgaTp",
                "replyto": "BWUS9FX9TG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5212/Reviewer_eJWS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5212/Reviewer_eJWS"
                ],
                "content": {
                    "title": {
                        "value": "Author responses read"
                    },
                    "comment": {
                        "value": "I read the author responses and updated the rating as the authors pointed out a few facts that are indeed in their favor. But it should be noted that the author responses are partially verbose and diverge from the topic."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5212/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700592059272,
                "cdate": 1700592059272,
                "tmdate": 1700592059272,
                "mdate": 1700592059272,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4coMx5ZHth",
            "forum": "567BjxgaTp",
            "replyto": "567BjxgaTp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5212/Reviewer_fhKw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5212/Reviewer_fhKw"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a lie detector designed to determine whether a Language Model (LLM) is providing deceptive answers to questions. First, it provides an overview of the latest advancements and studies in the field, focusing on two main areas: 'Distinguishing Lies from Other Falsehoods' and 'Building Lie Detectors'. The authors developed a simple yet effective black-box lie detector, which operates by posing a predefined set of unrelated follow-up questions after a suspected lie, and then feeding the LLM\u2019s yes/no answers into a logistic regression classifier. The experimental setup of the paper is meticulously explained, detailing how to generate lies with the LLM and how to conduct black-box lie detection using elicitation questions. Finally, an analysis of the lie detection results is presented."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "In the case of a black box scenario, the lie detector proposed in this paper is capable of rendering a verdict solely through a series of yes/no questions answered by the LLM, with the responses then fed into a binary classifier. This approach represents a substantial innovation and carries two significant implications.\n\n- This approach eliminates the need for access to the LLM\u2019s activations, allowing researchers to investigate lie detection in advanced models while having only limited API access.\n\n- Some approaches depend on assistant LLMs to validate claims made by potentially deceptive LLMs. This method might fail if the assistant possesses less knowledge than the deceptive LLM. In contrast, the lie detector proposed here requires no ground truth knowledge about the statement under evaluation. In addition, the paper has developed a dataset of 'synthetic facts'\u2014questions for which the model has not encountered the answers during pre-training, but to which it provides the correct answers in the prompt.\n\nThe appendix of this paper is highly detailed, encompassing the dataset, specifics of the experimental setup and lie generation, elicitation questions for the lie detector, additional lie detection results, details on specific generalization experiments, prompts, and real-life role-playing scenarios for goal-directed lying. This comprehensive coverage makes it easy for readers to follow and understand the experiment."
                },
                "weaknesses": {
                    "value": "This paper uncovers a number of interesting properties. However, providing a more detailed and intuitive explanation of the thought process that inspired the methods proposed in this article, as well as a deeper analysis and insight into the underlying mechanisms of the discovered properties, would greatly aid in enhancing the reader's understanding of this intriguing issue.\n\nThere is room for improvement in the organization and presentation of the article. For instance, the caption in Figure 4 is placed below, while the caption in Figure 5 is placed above."
                },
                "questions": {
                    "value": "If feasible, it would be beneficial to include comparative experiments in order to demonstrate that the method proposed in this paper offers more advantages than alternative approaches."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5212/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698799488295,
            "cdate": 1698799488295,
            "tmdate": 1699636518650,
            "mdate": 1699636518650,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LyZB5lFFYq",
                "forum": "567BjxgaTp",
                "replyto": "4coMx5ZHth",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5212/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5212/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "Thank you very much for your review of our work, we appreciate your suggestions for improving the paper.\n\n> This paper uncovers a number of interesting properties. However, providing a more detailed and intuitive explanation of the thought process that inspired the methods proposed in this article, as well as a deeper analysis and insight into the underlying mechanisms of the discovered properties, would greatly aid in enhancing the reader's understanding of this intriguing issue.\n\nIn terms of our thought process behind the work, we believe there is reason that this sort of detection should be possible as long as there is some kind of difference in a model\u2019s activations or outputs when (or after) it is lying. To lie convincingly, the model must be willing to answer follow-up questions in a way that is consistent with the lie and thus, for every follow-up question, the model must be attending to the original lie and considering whether the follow-up question is in some way related to the lie. On the other hand, if you are being truthful, you do not need to closely attend to your previous answers in order to continue being truthful.\n\nThis means that a skilled lying model will likely give different answers (and more subtly, logprobs) to follow-up questions related to the lie than a truthful model. It is also plausible that lying models would also have different logprobs for questions unrelated to the lie. Why? Because a rational agent would assign at least a small probability that any random question is somehow related to the lie in a way they might not have anticipated.\n\nWe wanted to try a first, simple version of such an \u201cinterrogation\u201d, using fixed questions that are unrelated to the specific lie, and using a logistic regression to classify the answers. For current models, already this simple method appears to work well. We think it is especially exciting that the **ambiguous** questions are enough to elicit a signal, and that this opens up a lot of interesting research directions.\n\nWe have included more of this intuition in the main paper (please see the updated introduction of the revised paper) in order to better motivate the work.\n\n> There is room for improvement in the organization and presentation of the article. For instance, the caption in Figure 4 is placed below, while the caption in Figure 5 is placed above.\n\nThank you for highlighting this inconsistency, this has now been amended in the revised uploaded version of the paper. We have also restructured the introduction and contribution section and the description of experiments (Sections 4-6) to make them more easily readable.\n\n> If feasible, it would be beneficial to include comparative experiments in order to demonstrate that the method proposed in this paper offers more advantages than alternative approaches.\n\nWe agree that comparison is a useful tool in understanding the method. Unfortunately, as we discussed in Section 3.2, existing methods for lie detection are white-box, hence having a much more reduced applicability. Indeed, in our study we trained the lie detector on GPT-3.5 (text-davinci-003), for which inner activations are unavailable; as such, an extensive comparison with white-box approaches would be impossible. \n\nAnother set of methods (e.g. Cohen et al. 2023) rely on checking the model-produced answer with an external LLM assistant - this would also be inapplicable in our setting, where the assistant has less knowledge than the lying model (see Section 2.1 Scenario 1). For example, if the lying model says \"the capital of France is Berlin\", then it is easy for an external LLM to recognise this as incorrect based on its prior knowledge. However, we would not be able to test whether the external LLM could detect lies in any case where it does not know ground-truth.\n\nWe hope then that our method serves as a solid baseline and starting point for future work."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5212/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700170533474,
                "cdate": 1700170533474,
                "tmdate": 1700227528265,
                "mdate": 1700227528265,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cZcSJ0wSIj",
                "forum": "567BjxgaTp",
                "replyto": "4coMx5ZHth",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5212/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5212/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up"
                    },
                    "comment": {
                        "value": "Dear Reviewer fhKw, once again thank you for your time and effort in providing your thoughtful review of our paper. As we are now entering the last day or so of the rebuttal period we wanted to just reach out to see if there was any feedback from our response - we appreciate that this will be a busy time for you but we hope that our current response has addressed any current concerns and are keen to engage further if there are any outstanding concerns. Best wishes, the Authors"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5212/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700584775957,
                "cdate": 1700584775957,
                "tmdate": 1700584775957,
                "mdate": 1700584775957,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8A0K3KjFm3",
                "forum": "567BjxgaTp",
                "replyto": "cZcSJ0wSIj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5212/Reviewer_fhKw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5212/Reviewer_fhKw"
                ],
                "content": {
                    "title": {
                        "value": "Acknowledgement"
                    },
                    "comment": {
                        "value": "Thanks for your reply."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5212/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700694978796,
                "cdate": 1700694978796,
                "tmdate": 1700694978796,
                "mdate": 1700694978796,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LoK7o2YQ9v",
            "forum": "567BjxgaTp",
            "replyto": "567BjxgaTp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5212/Reviewer_Y6rc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5212/Reviewer_Y6rc"
            ],
            "content": {
                "summary": {
                    "value": "This paper provide a definition for a language model lie based on consistency: an LM lies if it gives an incorrect answer to a question, but would answer correctly if the question is given in a standard question-answering format. The authors propose to detect this kind of lies by observing how the LM behaves on a fixed, specifically curated set of unrelated question. The authors train a logistic regression classifier to detect lies using outputs from a GPT 3.5 *that\u2019s prompted specifically to lie*, which manages to generalize very well to new models, new tasks, and several other aspects. The results seem to suggest that lying\u2014a type of inconsistency as defined by the paper\u2014is surprisingly consistent across many language models."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The findings of this paper are somewhat surprising and definitely interesting. The authors also did a great job on ablations, e.g., spurious correlations that might exist in the experiments but not in general. The idea of using unrelated questions to establish a behavioral baseline is smart. And since the set of elicitation questions are particularly optimized for lie detection, this opens up a lot of follow-up work."
                },
                "weaknesses": {
                    "value": "Nothing in particular. This paper seems well executed and the results are worth sharing with the ICLR audience."
                },
                "questions": {
                    "value": "The lying behavior studied in this paper can be seen as a special case of sycophantic behavior, where the LM model does something like \"there is a lie in the context (that I can detect), so I should be consistent and keep lying\". Would it be interesting to study whether this effect will be diluted by longer context, e.g., with a long and truthful prefix before the lie?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5212/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699502549326,
            "cdate": 1699502549326,
            "tmdate": 1699636518571,
            "mdate": 1699636518571,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4kfYv5ByXt",
                "forum": "567BjxgaTp",
                "replyto": "LoK7o2YQ9v",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5212/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5212/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "Thank you very much for your review of our work, we appreciate your comments and are glad that you found the paper both surprising and interesting! \n\n> The lying behavior studied in this paper can be seen as a special case of sycophantic behavior, where the LM model does something like \"there is a lie in the context (that I can detect), so I should be consistent and keep lying\". Would it be interesting to study whether this effect will be diluted by longer context, e.g., with a long and truthful prefix before the lie?\n\nWe agree with this point on similarities to a sycophantic setting, and even more generally, LLMs can have a tendency to just double down on previous statements made (mostly it would seem due to the fact that the training data will very rarely make a statement followed by a direct retraction). We did consider a setting like the one you suggest (although it is buried in Appendix B2.3) where once the lie has been generated, before follow up questions, we add a long \u201cdistraction\u201d of text and unrelated questions. Indeed with our non-CoT lying prompts, models are somewhat easily distracted and sometimes give a truthful answer when the initial question is repeated. However, given our CoT lying prompts, models remain undistracted and continue lying in almost every case, probably because the CoT refocusses the model and causes it to attend more to the previous lie.\n\nOn top of this, as per your suggestion, we have now added a **new experiment** in Appendix D.8 (that we shall also detail here) to consider how the effect of adding multiple truthful examples to the prompt might dissuade the model from lying.\n\nIn the experiment, we give the model a standard lie prompt followed by a number $n$ of correct (truthful) question-answer pairs, followed by the question of interest and then measure the lying rate of the model. We sweep over $n$ and see the results plotted in **Figure 12** (also available at [this link](https://imgur.com/a/CrSE7Er)) for 10 questions from the Question1000 dataset with the plot averaged across lie instructions. We can see a very clear effect where the more truthful pairs added to the prompt the more the effect of the lie prompt is diluted until the model eventually returns to a base truthful state."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5212/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700170407337,
                "cdate": 1700170407337,
                "tmdate": 1700245449805,
                "mdate": 1700245449805,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]