[
    {
        "title": "Policy Disentangled Variational Autoencoder"
    },
    {
        "review": {
            "id": "LSEcq2rvAy",
            "forum": "NhUinwpVSQ",
            "replyto": "NhUinwpVSQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2317/Reviewer_FTr9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2317/Reviewer_FTr9"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a discrete latent code into the TD-VAE framework to better model discrete actions in videos and disentangle the actions from states. Experiential results on MNIST, KTH, and Vizdoom show that their model, policy-disentangled VAE, can recognize the discrete action in the input videos and generate the futures conditioned on other actions."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- The paper is easy to understand."
                },
                "weaknesses": {
                    "value": "- The method evaluation is not convincing. As a 2024 submission on video generation, it only tests on three easy benchmarks: Moving MNIST, KTH with only one type of video (walk/run), and Vizdoom. In addition, the baseline method it compared with is MoCoGAN in 2018. This evaluation setting is definitely not at the bar of 2023. Even papers from three years ago like Dreamer [1] (ICLR 2020) have a more difficult setting.\n- The contribution is limited. Although the method is named policy-disentangled VAE, it is not conditioned on \u201cpolicy\u201d, but discrete actions like move left or move right. This can be achieved in papers 5 years ago like World Models [2] (NIPS 2018).\n\n- In addition, the idea of introducing discrete modes in VAE for better future modeling is not new and has been explored in other fields at least 3 years ago. (e.g., Trajectron++ [3] (ECCV 2020) in motion forecasting)\n\n\n[1] Hafner, Danijar, et al. \"Dream to control: Learning behaviors by latent imagination.\" arXiv preprint arXiv:1912.01603 (2019).\n\n[2] Ha, David, and J\u00fcrgen Schmidhuber. \"World models.\" arXiv preprint arXiv:1803.10122 (2018).\n\n[3] Salzmann, Tim, et al. \"Trajectron++: Dynamically-feasible trajectory forecasting with heterogeneous data.\" ECCV 2020"
                },
                "questions": {
                    "value": "In general, I think the evaluation setting is too easy for 2023. Showing performance that is compatible with Dreamer v3 [4] (2023) or even Dreamer (2020) on tasks they evaluate will be a plus.\n\n[4] Hafner, Danijar, et al. \"Mastering diverse domains through world models.\" arXiv preprint arXiv:2301.04104 (2023)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2317/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2317/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2317/Reviewer_FTr9"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2317/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698420232664,
            "cdate": 1698420232664,
            "tmdate": 1699636164130,
            "mdate": 1699636164130,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "x0gdmc81sW",
                "forum": "NhUinwpVSQ",
                "replyto": "LSEcq2rvAy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2317/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2317/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for taking the time to read and organize the review of our works. We appreciate your concerns about our proposed method and experiments. Following are the statements on the weakness of our paper. Please refer to the response to all reviewers for our response on contribution, baseline, and evaluation tasks.\n\nThe objective of our paper is not to generate high-quality video. Our argument and contribution are not to generate high-quality video while controlling the policy of an agent in the video. The contribution of the paper is that PDVAE categorizes the unlabeled video by the policy and generates video according to the policy. The experimental results with three datasets demonstrate the capability of PDVAE to learn the policy representation and controllability.\n\n$\\textbf{World model}$ utilizes the VAE architecture to generate images and video. However, the model does not have model architecture nor loss to learn and generate policy. The model learns the impact of action on the dynamics of the environment, overlooking the intention behind the action. We have raised the problem of overlooking in the introduction. $\\textbf{Dreamer\u2019s}$ objective is to learn the agent\u2019s model dynamics. The model intends to explicitly learn the model dynamics. According to the objective of PDVAE, the model does not need to learn the dynamics explicitly. PDVAE learns the dynamics implicitly with the generative model. $\\textbf{Trajectron++}$ learns the discrete latent feature $z_{mode}$ for better future modeling where PDVAE learns the discrete latent feature $\\pi$ from the video and uses it to simulate diverse scenarios conditioned to the $\\pi$. Trajectron++ does not control the generation with the latent mode. Considering the objective and methods used, we believe each model and PDVAE in different research criteria.\n\nThank you."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2317/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700546052572,
                "cdate": 1700546052572,
                "tmdate": 1700546341309,
                "mdate": 1700546341309,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "auz3egSyny",
                "forum": "NhUinwpVSQ",
                "replyto": "x0gdmc81sW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2317/Reviewer_FTr9"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2317/Reviewer_FTr9"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the answers. The latent code z in Trajectron++ in 2020 is learned from input sequence and used to simulate diverse future possibilities. PDVAE does the same thing but with simple videos. As we still don't have more recent baselines to compare with and more convincing experiments, and there is no obvious difference between PDVAE and methods like Trajectron++. I decide to keep my score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2317/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700655480611,
                "cdate": 1700655480611,
                "tmdate": 1700655480611,
                "mdate": 1700655480611,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qjxmLihrDO",
            "forum": "NhUinwpVSQ",
            "replyto": "NhUinwpVSQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2317/Reviewer_bJsN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2317/Reviewer_bJsN"
            ],
            "content": {
                "summary": {
                    "value": "### Problem Statement\n\nThe paper addresses the problem of overlooking the underlying intentions or policies driving the actions in videos while utilizing deep generative models for video generation. Traditional models primarily treat videos as visual representations of actions performed by agents without delving into the motivations or intentions behind those actions. This lack of attention towards the intention behind actions can limit the understanding and representation of behaviors exhibited in videos.\n\n### Main Contributions\n\nThe main contribution of the paper is the introduction of a novel model called Policy Disentangled Variational Autoencoder (PDVAE). PDVAE aims to learn the representation of the policy, akin to the underlying intention guiding the actions, without supervision, alongside learning the dynamics of the environment conditioned to the policy. Unlike previous models, PDVAE can generate diverse videos aligned with a specified policy, and even allows for altering the policy during the generation to produce varied behavioral outcomes in videos. The model differentiates videos based on the policy of an agent and can generate videos where each agent adheres to the specified policy.\n\n### Methodology and experiments\n\nThe PDVAE extends beyond Temporal-difference Variational Autoencoder (TD-VAE) by disentangling the state and policy, thereby providing a more nuanced understanding and representation of agents' behaviors in videos based on their underlying intentions or policies. Through qualitative and quantitative experimental validations on three video datasets (Moving MNIST, KTH action dataset, and VizDoom), the paper demonstrates the effectiveness of PDVAE in capturing and generating videos based on policy representations, opening up new avenues for more intention- and context-aware video generation."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This work is essentially an extension to the referred TD-VAE work, introducing so-called \"policy\" latent variables to condition the generative decoder. The derivation introducing the latent variable by decomposing the ELBO is neat. The resulted architecture is sophisticated with carefully designed loss.\n\nThe smooth transition on the change of the policy latent is intriguing in that the generated video respects the dynamics of its content while being controllable."
                },
                "weaknesses": {
                    "value": "### Limited evaluation tasks and baselines\n\nThe video generation mothod is evaluated on three datasets and compared to one baseline, which is not persuasive with respect to the versatility and superiority of the method.\n\n### Lack of ablation study\n\nDespite the sophisticated architecture, there is little ablation study analyzing the importance of its various components.\n\n### Writing\n\nAlthough the article conveys the ideas successfully, I find it sometimes repetitive, not very well organized, and not precise enough. Grammar mistakes also slightly hinder my comprehension.\n\n### Minor\n- The \"policy\" in this work is really a set of learnable embeddings disentangled from the latent space, while the authors' description tends to confuse it with the concept of policy in reinforcement learning, which is a function / distribution over actions. I understand that the learnable embeddings are intended to capture the \"policy\" which hypothetically controls the \"style\" or \"mode\" of the video generation, but it would help readers understand if the authors could further clarify this and differentiate the distinct concepts.\n- It would be beneficial if the authors could include the generated videos in supplement materials for better demonstration.\n- Please fix the missing space between the caption of Figure 8 and the main text below it."
                },
                "questions": {
                    "value": "- How is the policy space visualized as a 2D scatter in Figure 6?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2317/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2317/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2317/Reviewer_bJsN"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2317/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698647299040,
            "cdate": 1698647299040,
            "tmdate": 1699636164041,
            "mdate": 1699636164041,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QFe1X4xN1I",
                "forum": "NhUinwpVSQ",
                "replyto": "qjxmLihrDO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2317/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2317/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank you for your time and effort in your review. We appreciate your recognition of the model architecture and the mathematical support. Below is the response to the concerned weakness of the paper and the question. Please refer to the response to all reviewers for the response over the Limited evaluation tasks and baseline.\n\n1. Lack of ablation study \n\nThe model architecture of PDVAE might seem sophisticated, but essentially the model is composed of the policy-conditioned generative model (TD-VAE with policy), the policy extraction module, and the Siamese architecture for the regularization. We have done the ablation study on the impact of the policy-conditioned generative model on the policy extraction module with Moving MNIST and posted the qualitative results on the paper (Figures 6, 11, and 12). As we have illustrated in the second paragraph of Section 3.3, the Siamese architecture is essential to regulate the state. Without the structure, the conditioned policy does not affect the generation as the state contains complete information on the policy and environment.\n\n2. Minor: Policy \n\nPDVAE uses the latent variable $\\pi$ to encapsulate information on the agent\u2019s intention or behavioral goal. Given samples of videos where each video contains an agent with its own strategy or behavioral goal, the videos can be categorized into discrete numbers by the strategies. From the perspective of the video generative model, such latent embedding can be interpreted as the \u201cstyle\u201d or \u201cmode\u201d. PDVAE implicitly learns the dynamics of the environment with TD-VAE-based architecture, so interpreting the latent variable as the policy is more appropriate. In reinforcement learning, the policy is a function to represent the agent\u2019s behavioral intention. In PDVAE, we use the coded representation for the policy.\n\nThank you."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2317/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700545908588,
                "cdate": 1700545908588,
                "tmdate": 1700545908588,
                "mdate": 1700545908588,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CCHictd752",
                "forum": "NhUinwpVSQ",
                "replyto": "QFe1X4xN1I",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2317/Reviewer_bJsN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2317/Reviewer_bJsN"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for the reply. After considering the reviews of other reviewers and the responses, I'm not convinced that the work makes a contribution of enough significance. Introducing structures and therefore interpretability and disentanglement to the latent space through a hierarchical probabilistic model is a long existing approach [1] [2], and with the limited evaluation and experiments, it's hard to argue the superiority of the proposed method. I am also skeptical about the validity of the concept of \"policy\" that the authors have been emphasizing: Both the agents' policy / behavior and the world dynamics contribute to the formation of observed trajectories, and it seems to me rather narrow to attribute the variations of the trajectories to \"policy\". For example, for observations of a person playing with a bouncing ball, both the person's policy or behavior and the ball's mass and elasticity collectively decide the ball's movement, and I do not think calling the ball's properties \"policy\" is proper.\n\nI would keep my rating unchanged.\n\n[1] Tomczak, J. M., & Welling, M. (2017). VAE with a VampPrior. https://arxiv.org/abs/1705.07120v5\n[2] Lu, Q., Zhang, Y., Lu, M., & Roychowdhury, V. (2022). Action-conditioned On-demand Motion Generation. Proceedings of the 30th ACM International Conference on Multimedia, 2249\u20132257. https://doi.org/10.1145/3503161.3548287"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2317/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700716799318,
                "cdate": 1700716799318,
                "tmdate": 1700716799318,
                "mdate": 1700716799318,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nbqMZHdeHV",
            "forum": "NhUinwpVSQ",
            "replyto": "NhUinwpVSQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2317/Reviewer_y3xx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2317/Reviewer_y3xx"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a policy disentangled VAE that can generate diverse videos with user-specified policy during generation. It extends TDVAE by adding policy to the posterior distribution in ELBO. In order to learn the policy from observations, it proposes a lossy compression module and policy module to map observations to a fixed set of latent codes and train it with reconstruction loss.\n\nIt experiments in three environments: Moving MNIST, KTH Action and VIZDOOM. The metrics used are video/image quality metrics, e.g. FID, FVD, and policy accuracy. It shows better video quality and policy accuracy compared to MoCoGAN."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "It studies an interesting problem with a lot of potential applications. For example, it could be used in gaming and movie production and many robotic tasks.\n\nThe proposed method PDVAE is consistently better than MoCoGAN in terms of policy accuracy and video quality.\n\nThe writing and math in the paper are clear and easy to follow."
                },
                "weaknesses": {
                    "value": "The paper builds on top of TDVAE. Why not compare with TDVAE in terms of video quality regardless that TDVAE cannot generate policy-conditioned rollouts? Can we prompt TDVAE with a demonstration (e.g. digits moving left) to achieve similar results as PDVAE?\n\nThe environments and tasks used in the evaluation are very toy cases where the action is always discrete. I wonder if the policy extraction module and the policy-conditioned dynamics model can deal with continuous actions. Does it work with real videos, e.g. driving videos, instead of synthetic ones?\n\nHow do you compare with other recent text2video approaches, e.g. Control-A-Video? Will a prompt like \u201cA person running in the style of KTH dataset\u201d already solve the problem?"
                },
                "questions": {
                    "value": "How does it compare with TDVAE, which the method is built on?\n\nHow does PDVAE work on real videos and continuous action space?\n\nHow do you compare with modern text2video diffusion models? Could they solve the task in zero-shot?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2317/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2317/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2317/Reviewer_y3xx"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2317/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698686167065,
            "cdate": 1698686167065,
            "tmdate": 1699636163956,
            "mdate": 1699636163956,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yd0bUE43tW",
                "forum": "NhUinwpVSQ",
                "replyto": "nbqMZHdeHV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2317/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2317/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate your effort and time for the review. Thank you for recognizing the potential applications of our work and the mathematical support of our model. We have written the response for all reviewers for the common concerns. Following is the response to your questions.\n\n1. How does it compare with TDVAE, which the method is built on?\n\nWe have included the additional quantitative evaluations of TD-VAE with the same hyperparameters as PDVAE in Appendix F and below is the table with PDVAE. You can easily think of PDVAE without the policy module and Siam architecture in Figure 3 as the TD-VAE for the comparison.\n\nAs you have pointed out, we have thought that TD-VAE is not a valid baseline for the comparison because the contribution of PDVAE is to learn the representation of policy without supervision and the policy-conditioned generative model to control the generation. \n\n2. How does PDVAE work on real videos and continuous action space?\n\n$\\textbf{PDVAE does not explicitly learn the action space as the ELBO in Equation (6) indicates.}$ Our model is agnostic to the action space as it implicitly learns the action space. The common model architecture is used for all three datasets where Moving MNIST and VizDoom have discrete action space, and KTH has the continuous action space. In each transition, the agent moves a certain degree of arms and legs, not a discrete amount. Moreover, KTH is the real videos with low dimensionality. We have elaborated why we have chosen three dataset on the section 3 of common response.\n\n3. How do you compare with modern text2video diffusion models? Could they solve the task in zero-shot?\n\nWe believe text2video diffusion model is not an appropriate comparison to our model. PDVAE and text2video diffusion models can serve similar functionality, but the size of the models and the objective of the model are different. We believe these are two different criteria. It would be great if you provide some more detailed comparisons between models.\n\n4. About policy conditioned dynamics model\n\nPDVAE is the generative model that implicitly learns the dynamics of the environment for the generation. Our model does not directly learn the dynamics, hence we believe the term dynamics model does not fit. Policy conditioned generative model should be more appropriate.\n\nThank you."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2317/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700545849103,
                "cdate": 1700545849103,
                "tmdate": 1700546084900,
                "mdate": 1700546084900,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ChS6X2PqAB",
                "forum": "NhUinwpVSQ",
                "replyto": "yd0bUE43tW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2317/Reviewer_y3xx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2317/Reviewer_y3xx"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response! First, I agree with the other reviewers that the evaluation environments are too simple to prove the superiority of PDVAE. Second, I understand that KTH has continuous action space, but your metric (e.g. pi-ACC) doesn't measure the closeness in continuous space as it relies on discrete labels."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2317/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700593629071,
                "cdate": 1700593629071,
                "tmdate": 1700593629071,
                "mdate": 1700593629071,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "i8Uz0O11vE",
                "forum": "NhUinwpVSQ",
                "replyto": "nbqMZHdeHV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2317/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2317/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer y3xx,\n\nWe sincerely appreciate your feedback on our response.\n\nWe have wanted to raise the problem of the overlooked intention/behavioral goal behind the action in the field of video generative models. We have hypothesized that the policy from reinforcement learning can be the key to learning such representation. We have derived the ELBO of PDVAE based on TD-VAE. The purpose of the experiments is to validate whether PDVAE can learn the representation and conditionally generate videos by the policy. As you have noticed, PDVAE relies heavily on TD-VAE with the model architecture. We would like to validate our model on high-quality video, but the scalability of TD-VAE (only using simple neural architecture) prohibits us from performing such an experiment. We would appreciate it if you could reconsider our paper with the perspective of learning the representation of policy from unlabeled video and the utilization of policy on the generation. \n\nSecondly, the action space is implicitly modeled with the policy-conditioned generative model. Measuring the performance over the implicit space is hardly possible. Moreover, as we have the action space is not the area of research of this paper, so we have reported the metric (pi-Acc) to evaluate whether PDVAE generates video according to the policy.\n\nThank you!"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2317/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700627044752,
                "cdate": 1700627044752,
                "tmdate": 1700628120612,
                "mdate": 1700628120612,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "K93IVKw9tp",
            "forum": "NhUinwpVSQ",
            "replyto": "NhUinwpVSQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2317/Reviewer_6csw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2317/Reviewer_6csw"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a method (PDVAE) to generate visual representations (video / image sequences) of agents based on policies (and previous states) which are meant to capture the motivation of the agents. This method uses unsupervised learning to learn representations of policies and dynamics of the partially-observable environment without labels. The core ideas are (1) learning a disentangled representation of the policy and state, and (2) adding the policy to the posterior distribution over latent states to derive the ELBO (building on TD-VAE)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper introduces a novel architecture using VAE that builds on TD-VAE, but separates the representation of the policy from the state. There is a compelling argument that this disentanglement must be done to generate visual representations of agents that exhibit behavior with the appearance of intentionality in dynamic environments.\n\nThe paper is not a big improvement over the primary benchmark MoCoGAN, but MoCoGAN has access to the labels that PDVAE does not have. Compared to MoCoGAN- without labels, PDVAE performs much better.\n\nThe methodology appears sound, though I have not checked it too closely."
                },
                "weaknesses": {
                    "value": "The writing could be improved, but overall it is fairly clear.\n\nThe results overall are not too strong, but they are reasonable and well-motivated, and may be an influence on future work on video generation. Unfortunately, only a single benchmark is used.\n\nMinor Errors:\n\nThe authors should add a sentence early on to explain what is meant by \"codebook\" in the VAE and how this relates to the action space.\n\nPage 4 (Section 3.1): \"..given as the observations do...\" doesn't quite make sense. Not entirely sure what this is supposed to say.\n\nPage 8 (Section 5.2): \"...capable of generating videos generates a video that is not...\" --> \"...capable of generating videos that are not...\""
                },
                "questions": {
                    "value": "Are there no other benchmarks that would be appropriate to compare against other than MoCoGAN?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2317/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2317/Reviewer_6csw",
                        "ICLR.cc/2024/Conference/Submission2317/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2317/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698724065170,
            "cdate": 1698724065170,
            "tmdate": 1700688665022,
            "mdate": 1700688665022,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EjUHyEgHqG",
                "forum": "NhUinwpVSQ",
                "replyto": "K93IVKw9tp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2317/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2317/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank you for your time and effort in the review. We appreciate your recognition of our model architecture and argument over the need for policy representation. We have corrected the minor errors that you have indicated on the paper. Please refer to section 2 of the common response for the answer to the question.\n\nThank you."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2317/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700545564140,
                "cdate": 1700545564140,
                "tmdate": 1700545564140,
                "mdate": 1700545564140,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UHmfoVA72O",
                "forum": "NhUinwpVSQ",
                "replyto": "EjUHyEgHqG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2317/Reviewer_6csw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2317/Reviewer_6csw"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors"
                    },
                    "comment": {
                        "value": "After considering the reviews of other reviewers and the responses, I have to agree that the experiments are insufficient to make strong claims about the efficacy of the methdology presented.\n\nHowever, I want to reiterate what I wrote in my original review which is that I see value in the approach and it is well-motivated. The core ideas is of general interest. Unfortunately, since the paper does not offer deep theoretical contributions, whether it is ready for publication must be determined by the strength of the experiments and not the general idea."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2317/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700688640273,
                "cdate": 1700688640273,
                "tmdate": 1700688640273,
                "mdate": 1700688640273,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]