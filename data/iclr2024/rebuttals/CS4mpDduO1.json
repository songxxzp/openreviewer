[
    {
        "title": "Test Error Guarantees for Batch-normalized two-layer ReLU Networks Trained with Gradient Descent"
    },
    {
        "review": {
            "id": "gh7wJ7E5DJ",
            "forum": "CS4mpDduO1",
            "replyto": "CS4mpDduO1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2741/Reviewer_ryx4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2741/Reviewer_ryx4"
            ],
            "content": {
                "summary": {
                    "value": "This paper proves the low training (with logistic loss) and test error guarantees of GD/SGD for two-layer ReLU neural network with batch normalization. It mainly uses the techniques developed in Telgarsky(2022) and Ji & Telgarsky(2020) to show that the weights will move closer to the scaled gradient of the initial predictor ($df/dw_0$) then the margin can be lower bounded then the training and test error can be bounded."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. It is a novel theoretical result.  It keeps the $\\epsilon$ in the variance term in batch norm in contrast to Arora et al. (2018).  Hence the analysis is closer to the batch norm used in practice. \n\n2. I like the idea of rescaled Euclidean potential which allows separate control over different parameters. I think it can be potentially applied to other analyses."
                },
                "weaknesses": {
                    "value": "1. My major concern is about the size of the bound in Theorem 2.1. The bound on $m$ is too large as it has constant $2^{29}$. And in Eq.(2) there is a term $2^{15}$. Even for a theory paper, I think the constants are excessively high.  I would suggest a re-evaluation of these terms to achieve more reasonable bounds.\n\n2. I believe the intuition for Assumption 1.1 should be further clarified, e.g., how does it affect the proof?\n\n3. In Proposition 1.1, the chosen parameters are too special ($C = 0$) since $f(x) = 0$ for any input x.  I don't think GD/SGD will converge to such parameters."
                },
                "questions": {
                    "value": "1. Why do the bounds on smoothness can be as large as $O(M^L)$? At least in the NTK setting, the bounds shouldn\u2019t grow with the width and the number of layers. I believe in other practical settings, the bounds shouldn't be that large as well. Why do you think such an assumption is problematic?\n\n2. Is there an error in the proof of Theorem 2.1 (F.4)?  Before Eq.(66), you used the fact that $||\\overline{U}||\\leq 1$. I didn't find the support for this fact. \n\n3. In the proof of Lemma B.1., it seems like $\\epsilon$ is assumed to be positive. Then I am confused about the sentence after Assumption 1.1. Could you clarify?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2741/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2741/Reviewer_ryx4",
                        "ICLR.cc/2024/Conference/Submission2741/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2741/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698788948479,
            "cdate": 1698788948479,
            "tmdate": 1700083048381,
            "mdate": 1700083048381,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "33iAaSyYgw",
            "forum": "CS4mpDduO1",
            "replyto": "CS4mpDduO1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2741/Reviewer_J2f7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2741/Reviewer_J2f7"
            ],
            "content": {
                "summary": {
                    "value": "Authors consider gradient descent (GD) and stochastic gradient descent (SGD) on two-layer ReLU networks with\nBatch Norm. Consider the test error rate with respect to a parameter $gamma$ similar to a parameter used by Telgarsky (2022) previously. They show that the test error decreases at a rate of $O(\\frac{m^{1/3}}{\\gamma^{1/3}t})$ and $ O(\\frac{1}{\\gamma^2 t})$ for networks with width $\\Omega(1/\\gamma^2)$."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "-Combines two original ideas to understand and control (two layer ReLU) neural networks, batch normalized networks and the margin introduced by Ji & Telgarsky (2020).\n\n-mathematical proofs are provided as well as experiments (though I did not check everything for correctness)"
                },
                "weaknesses": {
                    "value": "-write up could use some improvements (some definitions are missing,for example what is sigma what is U, there are lots of definitions and it is hard to follow the paper, sometimes the order is confusing and not everything is explained well, for example Assumption 1.1 could follow immeadialtely after the definiton of N(v) and both could be explained in more detail why they are used)\n\n-very similar to previous papers"
                },
                "questions": {
                    "value": "-"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2741/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2741/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2741/Reviewer_J2f7"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2741/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698848463568,
            "cdate": 1698848463568,
            "tmdate": 1699636216720,
            "mdate": 1699636216720,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "SPMk0DYlsr",
            "forum": "CS4mpDduO1",
            "replyto": "CS4mpDduO1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2741/Reviewer_4dGx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2741/Reviewer_4dGx"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers the two-layer ReLU networks with batch normalization (BN). In particular, this paper proposes a new margin definition for the two-layer ReLU network with BN, and develops the test error bound of SGD and GD using the proposed margin definitions.  The results can be utilized to better understand the performance of BN and provide a new method for evaluating the generalization performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* A new definition of margin that can be useful to better evaluate the generalization performance.\n* A generalization bound for SGD/GD on batch-normalized 2-layer networks."
                },
                "weaknesses": {
                    "value": "1. The paper's organization is not clear. For instance, in Section 1.4, I just quite not understand that you first say $\\bar W = W_0 + r\\overset{\\rightarrow}{W}$ is difficult to use and then mention that we again consider this $\\bar W$.\n2. I am not sure whether only considering the optimization around the initialization is good as this is different from the practice. \n3. After reading the paper, I am not sure what role the normalization layer plays in the analysis since the authors only consider the margin with respect to the scaling parameters $a$ and $c$. While the hidden layer weights $v$ could be more important.\n4. I also cannot quite understand why you need to ensure that $\\|v_i/N(v_i)-v_j/N(v_j)\\|$ is small.\n5. The new margin definition is also weird as it's written as an assumption rather than a definition. So would it be possible that the positive $\\gamma$ does not exist?\n6. In proposition 1.1, I do not think that the comparison of different margins is fair as they are defined in different ways. On the other hand, can you also give the test error bound for the model without a normalization layer?\n7. The SGD case is also confusing. To my understanding, SGD with BN only estimates the data covariance using a random mini-batch, so that the expectation of stochastic gradient will not be the full gradient. Then I do not see why can you use a similar analysis to get the theoretical results for both SGD and GD? \n8. What if we only consider linear separable data, then what's the relationship between the margin defined in this paper and the margin of the data? When using your definition of margin, what's the new margin of the original max-margin classifier? Besides, a very recent paper [1] shows that the BN could lead to a ``uniform margin'' solution (defined using the conventional margin definition), would it be also possible to translate such a margin using your definition?\n\n[1 ] Cao et al., The Implicit Bias of Batch Normalization in Linear Models and Two-layer Linear Convolutional Neural Networks, COLT 2023"
                },
                "questions": {
                    "value": "See the weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2741/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699118160644,
            "cdate": 1699118160644,
            "tmdate": 1699636216586,
            "mdate": 1699636216586,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "tog4XtuGcx",
            "forum": "CS4mpDduO1",
            "replyto": "CS4mpDduO1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2741/Reviewer_jFN6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2741/Reviewer_jFN6"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to show that online SGD (with fresh sample at each step) or full-batch GD (on finite dataset) provably decrease test errors of two-layer ReLU net with Batch Normalization to 0 as t -> inf."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper is well-written. Theoretical setups and assumptions are presented very clearly.\n2. This paper looks into the training dynamics of neural nets with normalization layers, which is an interesting and challenging case to analyze."
                },
                "weaknesses": {
                    "value": "**Main Concern:** This paper would be interesting if it really presents a new analysis that goes beyond the previous works mentioned in the introduction, but it is unclear to me whether the proof is indeed correct.\n1. For both the SGD and GD cases, it is shown that the parameters U will be trapped in a neighborhood of their initial values, and it is implied from the proof that the training loss will go to 0 as t -> inf (since the sum of $|\\ell'|$ over time is finite). However, these two cannot happen at the same time: if U is trapped in a finite region, then the output of the neural net is also finite, and thus, the logistic loss must be lower bounded by a positive constant. To make it 0, the output as well as the parameters in U has to go to infinity.\n2. The result of GD implies that 0 test error can be obtained even if there is only 1 data point. To see this, just take t -> inf in equation (6). Obviously, having only 1 data point is not information-theoretically possible to learn a perfect classifier.\n\n**A minor issue:** Page 7, $\\Gamma$ is just a set of points, not a manifold."
                },
                "questions": {
                    "value": "I urge the authors to check the correctness of the proof."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2741/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699425336412,
            "cdate": 1699425336412,
            "tmdate": 1699636216526,
            "mdate": 1699636216526,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]