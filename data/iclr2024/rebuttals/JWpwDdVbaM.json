[
    {
        "title": "ARM: Refining Multivariate Forecasting with Adaptive Temporal-Contextual Learning"
    },
    {
        "review": {
            "id": "30oLyWFlYu",
            "forum": "JWpwDdVbaM",
            "replyto": "JWpwDdVbaM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2891/Reviewer_X2pk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2891/Reviewer_X2pk"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces ARM : a multivariate temporal-contextual adaptive learning method for long-term time series forecasting, which is an enhanced architecture specifically designed for multivariate LTSF modelling. ARM consists 3 modules: Adaptive Univariate Effect Learning (AUEL), Random Dropping (RD) training strategy, and Multi-kernel Local Smoothing (MKLS).  AUEL is for adaptively estimate mean and variance and capture temporal pattern for each series; RD is for robust learning and avoid overfitting when series are interdependent; Multi-kernel Local Smoothing (MKLS) for capturing various temporal dependency among series.  ARM demonstrates superior performance on multiple benchmarks without significantly increasing computational costs compared to vanilla Transformer."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "Originality: this paper is fairly novel. It attempts to address previous inferior performance of transformer models compared to a simple feed forward neural network DLinear(Zeng et al. 2022) in long term forecasting task. It proposes unique insights on temporal pattern learning of the output sequence distribution and interdependency among univariate series.\n\nQuality: The quality of the paper is high. The proposal of the three core modules are well motivated. AUEL is for adaptively estimate mean and variance and capture temporal pattern for each series; RD is for robust learning and avoid overfitting when series are similar; Multi-kernel Local Smoothing (MKLS) for capturing dependency among series. The authors have performed experiments on 10 benchmarks which are diverse enough to contain different aspect of multivariate time series. The authors also incorporate different ablations by incorporate one or two modules of ARM. \n\nClarity: the overall clarity of presentation is good. It is motivated by the drawbacks of current sota models and address issues one by one by proposing the modules of ARM.\n\nSignificance: the paper can be important to time series learning community."
                },
                "weaknesses": {
                    "value": "I think quality and clarity can be further improved. \n\nQuality: while the empirical results (Table 1 and 2) show superior performance of the proposed model ARM, it will be better to also demonstrate how significant these results are. The authors claim the proposed approach will not significantly increase computational costs. But such discussion/analysis is not included in the paper. \n\nClarity: I found MKLS block in section 3.3 difficult to follow due to notations. The authors introduced many (subscripts and subscripts of X for example); it would be nice to refer to somewhere. \nTable 2 contains large amount of results which are replicate of Table 1. Maybe the authors can eliminate some and find a way to better present them? \nFigure 3 & 4. Due to large amount of information in Figures 3&4, when introducing the components such as MKLS block, I think it will be great to introduce pointers to these components in the figures.   For example \\tilde{X}^{*i} need pointers to point to in Figure 3."
                },
                "questions": {
                    "value": "1.\tHow does MoE better capture temporal patterns compared to autoregressive and exponential smoothing models? \n2.\tWhat is inverse processing stage? \n3.\tThe paper mentions causal dependency/ relationship among different series. Could the authors explain in what sense the notion of causality is embedded? \n4.\tI am confused about the how Multi-kernel local smoothing helps capture temporal dependencies among different series due to notations. For example, What is X_j. Could the authors elaborate more? \n5.\tThe paper demonstrates superior empirical performance for forecasting task.  It is hard to see how significant the improvement is compared to other recent models such as PatchTST and DLinear. Do the authors have some insights on the sensitivity of hyperparameters in ARM module in the tuning/training process since many learnable hyperparameters are introduced. \n6.\tWhat are the complexity/runtime compute/memory to incorporate these modules in the vanilla model? How does that compare to PatchTST and DLinear?  \n7.\tA few discrepancies regarding Table 1. I found in the original PatchTST paper on traffic datasets Table 3: for MAE,  0.249 for L_P = 96, 0.256 for L_P = 192, different from 0.239 and 0.246 in this paper. Could the authors double check if they are consistent?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2891/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2891/Reviewer_X2pk",
                        "ICLR.cc/2024/Conference/Submission2891/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2891/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698174017280,
            "cdate": 1698174017280,
            "tmdate": 1700512988708,
            "mdate": 1700512988708,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RBIfvkC8Lr",
                "forum": "JWpwDdVbaM",
                "replyto": "30oLyWFlYu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2891/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2891/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer X2pk (Part 1)"
                    },
                    "comment": {
                        "value": "Thank you very much for your detailed comments on our paper! Your suggestions significantly contribute to enhancing the quality and readability of our research. We appreciate your recognition of the potential value our paper could bring to the community, and we are eager to see our results applied in broader research to create greater value.\n\nBefore answering the specific questions, we would like to briefly recap the main ideas of our paper:\n\n- **Overall Concept of ARM**\n\n  i. Utilize AUEL to extract/control each time series' own forecasting contribution.\n\n  ii. Employ Main Predictor + MKLS to model the inter-series relationships beyond univariate forecasting contributions.\n\n  iii. Implement Random Dropping to prevent overfitting when learning inter-series relationships.\n\n### Q1. About the Presentation Quality of Tables and Figures\nWe greatly value your suggestions regarding the presentation of our paper and have made the following adjustments in our revision:\n- We revised the layout of Table 2 to reduce redundancy and added a comparison of average percentage performance improvements with the original model for each sub-experiment to demonstrate the specific enhancements brought by the A/R/M modules.\n- In line with the notations in our paper, we added relevant markings to the Figures of the overall architecture and MKLS block (Figures 2 and 4) to aid in understanding the computational details.\n- We bolded the reference in Section 4 that leads to the discussion on computational costs in Appendix A.7, making it more prominent.\n\n### Q2. How does MoE Better Capture Temporal Patterns Compared to Autoregressive (AR) and Exponential Smoothing (ES) Models?\n\n- Firstly, the AR and ES methods mentioned in our paper, such as LSTNet and ES-RNN, do not incorporate the AR and ES parameters in the neural network training. In this context, the pre-fixed AR and ES model structures may not suit time series with significant characteristic differences. We added a brief predictability analysis in Appendix A.8 to show the substantial performance variation of these basic learners when dealing with different time series data. In our AUEL, we use an adaptive EMA for each series to adjust the lookback view and then employ MoE to assign the most suitable univariate MLP predictor to each input series, achieving the best adaptation to characteristic differences.\n- Secondly, AR and ES align with the individual univariate model with no parameter sharing described in Figure 3 (a). We consider both this model and the full parameter sharing model in Figure 3 (b) to be suboptimal. If two series in the data have strong relationship (like in the Multi dataset, where one series is a shift of another), parameter sharing between these two series is a good choice. However, if a series in the dataset has no relationship with any other series, sharing its parameters with other series is not a good option. Therefore, we apply the MoE shown in Figure 3 (c) to address this issue, dynamically deciding which predictor from the predictor pool to use for univariate forecasting based on series characteristics."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2891/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699939305399,
                "cdate": 1699939305399,
                "tmdate": 1699939305399,
                "mdate": 1699939305399,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1tk30hlrri",
                "forum": "JWpwDdVbaM",
                "replyto": "30oLyWFlYu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2891/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2891/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer X2pk (Part 2)"
                    },
                    "comment": {
                        "value": "#### Q3. What is the Inverse Processing Stage?\n- In previous methods addressing the input-output distribution shift problem, such as RevIN and NLinear, they estimate each series' output mean and standard deviation based on the input. These values are used to normalize the input, and then applied during the inverse processing stage after the main model processing to ensure that the final output conforms to this distribution. Our AUEL of distribution, positioned between RevIN (full-length lookback) and NLinear (last-step lookback), adaptively adjusts the lookback view for the output distribution estimation, thus following the same computation process above.\n- In the AUEL of temporal patterns, the inverse processing stage helps the model balance between learning intra-series and inter-series dependencies. Note the preprocessing MoE in equation (2) as \\\\( \\\\mathtt{MoE}\\\\left( \\\\left[ \\\\widetilde{X}^{\\*i}\\_I  \\\\ \\\\ \\\\ \\\\ \\\\widetilde{X}^{\\*i}\\_P \\\\right] \\\\right) \\\\) and the inverse processing MoE in equation (3) as \\\\( \\\\mathtt{MoE}\\\\left( \\\\left[ \\\\widetilde{X}^{\\*i}\\_I  \\\\ \\\\ \\\\ \\\\ \\\\widehat{X}\\_{ED}^{i} \\\\right] \\\\right) \\\\), which only differ in the \\\\(L\\_P\\\\) forecasting horizon, changing the preprocessed values to the main predictor output \\\\(\\\\widehat{X}\\_{ED}^{i}\\\\). If a series \\\\(i\\\\) is strongly related to other series and the relationship is effectively captured by the main predictor output, we hope the inverse processing \\\\( \\\\mathtt{MoE}\\\\left( \\\\left[ \\\\widetilde{X}^{\\*i}\\_I  \\\\ \\\\ \\\\ \\\\ \\\\widehat{X}\\_{ED}^{i} \\\\right] \\\\right) \\\\) maintains this output value or makes minor corrections. However, if a series \\\\(i\\\\) has no relation to other series, the inter-series dependencies learned by the main predictor in \\\\(\\\\widehat{X}\\_{ED}^{i}\\\\) are likely incorrect. In this case, we expect the inverse processing \\\\( \\\\mathtt{MoE}\\\\left( \\\\left[ \\\\widetilde{X}^{\\*i}\\_I  \\\\ \\\\ \\\\ \\\\ \\\\widehat{X}\\_{ED}^{i} \\\\right] \\\\right) \\\\) to make significant corrections to the \\\\(\\\\widehat{X}\\_{ED}^{i}\\\\) portion, favoring univariate modelling contribution based on \\\\(\\\\widetilde{X}^{\\*i}\\_I\\\\). On datasets with a large number of series or series from different sources, this correction significantly improves model performance.\n\n#### Q4. About the Causality Mentioned in the Paper\n- Thank you for this valuable question. To avoid any further misunderstanding, we have deleted all the words related to \"causality\" in the revision. In our previous paper version, we tried to use the term 'causality' to convey the notion that data with strong causal relationships, like those generated through simple shifting operations in the \"Multi\" dataset, are not effectively modeled by existing models.\n- Indeed, in earlier versions of our paper, we discussed about whether to include a section in the appendix about the connection between ARM and other time series causality studies. You might have noticed, ARM shares some similarities with previous studies like Granger Causality and Vector Autoregressive: firstly control each time series' own forecasting contribution (A) and then explore/test whether other time series provide additional forecasting contributions (Main Predictor + RM). However, since ARM does not perform any statistical tests and differs significantly in practice from these statistical methods, we chose not to include these discussions to avoid misleading.\n\n#### Q5. About the Computational Details in MKLS\nWe apologize for the confusion caused. MKLS employs multiple 1D CNNs with different kernel sizes to obtain various local views $\\\\widetilde{X}_j$ of the input \\\\(X\\\\), and uses channel attention to calculate the weight of these local views on each output channel for a weighted average, resulting in a smoothed local view. Therefore, the output of the \\\\(j\\\\)th CNN should actually be computed as \\\\(\\\\widetilde{X}_j = \\\\mathtt{Conv1d}\\_j \\\\left(X \\\\right)\\\\). Our previous notation of \\\\(X\\\\) as \\\\(X_j\\\\) was incorrect and caused misunderstanding. This has been corrected in the revision. Please refer to the updated Figure 4 for more details on MKLS computations."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2891/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699939547341,
                "cdate": 1699939547341,
                "tmdate": 1699939547341,
                "mdate": 1699939547341,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "A9XlUKhyov",
                "forum": "JWpwDdVbaM",
                "replyto": "30oLyWFlYu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2891/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2891/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer X2pk (Part 3)"
                    },
                    "comment": {
                        "value": "### Q6. Insights on the Sensitivity of Hyperparameters in ARM Module\n\nThank you for this question. Indeed, the ARM effectively enhances the robustness of LTSF models towards hyperparameter selection. You can refer to section A.5 for our hyperparameter settings. We kept most of the hyperparameters fixed. The only adjustment was that we categorized datasets into small datasets (number of series \\\\(C < 20\\\\)) and large datasets (number of series \\\\(C \\geq 20\\\\)). For small datasets, we used a model dimension of the main predictor \\\\(d=16\\\\) and 2 experts in MoE; for large datasets, we used \\\\(d=64\\\\) and 4 experts in MoE. Below, we discuss the improvements in hyperparameter selection brought by ARM.\n\n**a. Lookback Length \\\\(L\\_I\\\\)**\nPrevious LTSF models attempted multiple \\\\(L\\_I \\\\in \\\\{96, 192, 336, 720\\\\}\\\\) during the training phase and chose the optimal one, significantly increasing training costs and making it difficult to select the best \\\\(L\\_I\\\\) in practical applications. Due to the presence of adaptive EMA, MoE in AUEL, and MKLS, the model can effectively adjust the lookback and local views, making ARM less sensitive to the choice of \\\\(L_I\\\\). In all our experiments, we used only \\\\(L\\_I=720\\\\) and consistently achieved results surpassing previous SOTA models.\n\n**b. Model Dimension \\\\(d\\\\) of Main Predictor**\nWith ARM, the optimal dimension \\\\(d\\\\) required for the main predictor is reduced (b.1), and it can adapt to different datasets with substantial variations in inter-series relationship intensity without adjusting \\\\(d\\\\) (b.2).\n\n- **b.1 Reduction of Optimal \\\\(d\\\\)**\n   After AUEL controls for univariate forecasting effects, the parameters of the main predictor are primarily used for modeling inter-series dependencies. As mentioned in the response to Q3, MoE distinguishes which series rely mainly on self-forecasting contribution. Therefore, the MKLS-enhanced main predictor only needs to model the remaining inter-series effects, effectively reducing the required \\\\(d\\\\). The following table shows the impact of using AUEL of temporal patterns on the optimal \\\\(d\\\\) in the Electricity dataset (\\\\(C=321 > 20\\\\)) and ETTm1 (\\\\(C=7 < 20\\\\)). For detailed analysis, please refer to section A.4.5 and Table 5.\n\n\n\n| Model        | MoE (d=16)      | MoE (d=64)        | MoE (d=256)     | w/o MoE (d=16)   | w/o MoE (d=64)   | w/o MoE (d=256)  |\n|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n| Metric       | MSE \\| MAE      | MSE \\| MAE        | MSE \\| MAE      | MSE \\| MAE      | MSE \\| MAE      | MSE \\| MAE      |\n| Electricity (96) | 0.128 \\| 0.228 | **0.126** \\| **0.225** | 0.129 \\| 0.229 | 0.313 \\| 0.392  | 0.261 \\| 0.348  | **0.256** \\| **0.341** |\n| Electricity (192) | 0.146 \\| 0.242 | **0.142** \\| **0.239** | 0.145 \\| 0.245 | 0.299 \\| 0.378  | 0.255 \\| 0.349  | **0.251** \\| **0.352** |\n| ETTm1 (96)    | **0.287** \\| **0.340** | 0.295 \\| 0.348   | 0.300 \\| 0.352 | 0.488 \\| 0.460  | **0.467** \\| **0.446** | 0.503 \\| 0.468 |\n| ETTm1 (192)   | **0.325** \\| **0.371** | 0.342 \\| 0.376   | 0.346 \\| 0.385 | 0.531 \\| 0.499  | **0.498** \\| **0.482** | 0.574 \\| 0.512 |"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2891/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699939742644,
                "cdate": 1699939742644,
                "tmdate": 1699939742644,
                "mdate": 1699939742644,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dHnlSXg57i",
                "forum": "JWpwDdVbaM",
                "replyto": "30oLyWFlYu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2891/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2891/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer X2pk (Part 4)"
                    },
                    "comment": {
                        "value": "**b. Model Dimension \\\\(d\\\\) of Main Predictor** (Continue)\n\n- **b.2 Improved Robustness in \\\\(d\\\\) Selection**.\nDifferent datasets, even with the same number of series \\\\(C\\\\), can have vastly varying strengths of inter-series relationships. For example, the Exchange and Multi datasets both have \\\\(C\\\\) equal to 8, and the series within them exhibit properties similar to random walk processes. However, the former has weak inter-series relationships, while the latter has strong ones.\nAfter the extraction of univariate effect by AUEL, the model parameters of other parts, whose size mainly controlled by model dimension $d$, will focus on modelling the inter-series dependencies. $d$ typically needs to be tuned to accommodate different inter-series relationship strength of datasets. A smaller $d$ helps avoid overfitting in datasets with weak inter-series relationships, whereas a larger $d$ is necessary for datasets with stronger inter-series dependencies. Incorporating Random Dropping mitigates the need for tuning $d$ across various datasets. Random Dropping can be regarded as an \"adaptive scaler\" for $d$, allowing the model to automatically adjust to the strength of these relationships. Consequently, $d$ can be set solely based on the number of input series $C$ (we use $d=16$ for $C < 20$ and $d=64$ for $C \\geq 20$ for all the experiments), with Random Dropping modulating the learning of inter-series relationships. \nTable below demonstrate the effectiveness of \"R\": using a **consistent $d=16$** that **potentially fits stronger inter-series dependencies (like Multi)**, models with Random Dropping (**+ARM**) **perform well on dataset with weak inter-series dependencies (Exchange)**, without the **overfitting observed on Exchange** dataset in models without Random Dropping (**+AM**). \n\n| Model            | Autoformer+AM    | Autoformer+ARM  | Informer+AM     | Informer+ARM    | Vanilla+AM      | Vanilla+ARM    |\n|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n| Metrics          | MSE \\| MAE       | MSE \\| MAE      | MSE \\| MAE      | MSE \\| MAE      | MSE \\| MAE      | MSE \\| MAE     |\n| Exchange (96)    | 0.089 \\| 0.210    | 0.081 \\| 0.201  | 0.090 \\| 0.212   | 0.086 \\| 0.207  | 0.085 \\| 0.206  | 0.078 \\| 0.197 |\n| Exchange (336)   | 0.332 \\| 0.417   | 0.298 \\| 0.395  | 0.338 \\| 0.420   | 0.312 \\| 0.404  | 0.304 \\| 0.398  | 0.252 \\| 0.367 |\n| Average %        | 100% \\| 100% | 90.0% \\| 95.1%  | 100% \\| 100%| 93.0% \\| 96.7%  | 100% \\| 100%| 84.8% \\| 93.4% |\n| Multi (96)       | 0.037 \\| 0.130    | 0.038 \\| 0.130   | 0.051 \\| 0.164  | 0.051 \\| 0.166  | 0.034 \\| 0.129  | 0.032 \\| 0.125 |\n| Multi (336)      | 0.181 \\| 0.302   | 0.175 \\| 0.300    | 0.182 \\| 0.305  | 0.183 \\| 0.304  | 0.172 \\| 0.291  | 0.164 \\| 0.286 |\n| Average %        | 100% \\| 100% | 97.7% \\| 99.5%  | 100% \\| 100%| 100.4% \\| 100.2%| 100% \\| 100%| 95.1% \\| 97.9% |"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2891/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699939917387,
                "cdate": 1699939917387,
                "tmdate": 1699939917387,
                "mdate": 1699939917387,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "p66Z2RRmJS",
                "forum": "JWpwDdVbaM",
                "replyto": "30oLyWFlYu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2891/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2891/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer X2pk (Part 5)"
                    },
                    "comment": {
                        "value": "### Q7. About the Complexity/Runtime Compute/Memory of ARM\n\nWe presented a comparison of the computational costs of Vanilla+ARM with previous models in **section A.7** and **Table 7** of the original paper. In the revision, we have added a comparison with DLinear. The results are also presented as follows. It can be observed that under the optimal hyperparameter settings of each model on the ETTm1 dataset, Vanilla+ARM has competetive computational costs compared to previous models.\n\n\n|              | Vanilla+ARM (FLOPs) | Vanilla+ARM (Params) | Vanilla (FLOPs) | Vanilla (Params) | Autoformer (FLOPs) | Autoformer (Params) |\n|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\n| \\\\(X_P=96\\\\)   | 426M                | 7.89M                | 244M            | 14.9M            | 10.9G               | 15.5M               |\n| \\\\(X_P=192\\\\)  | 515M                | 10.4M                | 273M            | 17.5M            | 11.6G               | 15.5M               |\n| \\\\(X_P=336\\\\)  | 664M                | 14.9M                | 316M            | 21.9M            | 12.7G               | 15.5M               |\n| \\\\(X_P=720\\\\)  | 1.15G               | 30.6M                | 431M            | 37.8M            | 15.5G               | 15.5M               |\n\n|              | Informer (FLOPs) | Informer (Params) | PatchTST (FLOPs) | PatchTST (Params) | DLinear (FLOPs) | DLinear (Params) |\n|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\n| \\\\(X_P=96\\\\)   | 9.41G            | 11.3M             | 5.26G            | 4.87M             | 3.06M           | 138K             |\n| \\\\(X_P=192\\\\)  | 10.1G            | 11.3M             | 5.31G            | 7.08M             | 6.10M           | 277K             |\n| \\\\(X_P=336\\\\)  | 11.2G            | 11.3M             | 5.38G            | 10.4M             | 10.7M           | 485K             |\n| \\\\(X_P=720\\\\)  | 14.0G            | 11.3M             | 5.58G            | 19.2M             | 22.8M           | 1.04M            |\n\n\n### Q8. Discrepancies in Table 1\n\nWe are very grateful for your observation regarding the data entry errors in our paper. We have now corrected these two data points, which also makes our model appear more competitive. We have rechecked the data in Table 1 to ensure its correctness.\n\n---\nThank you again for these valuable questions, which greatly contributes to improving the quality of our paper. If you have any further questions, we are eager to engage in more discussions."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2891/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699940030327,
                "cdate": 1699940030327,
                "tmdate": 1699940030327,
                "mdate": 1699940030327,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yypK8MzAFn",
                "forum": "JWpwDdVbaM",
                "replyto": "p66Z2RRmJS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2891/Reviewer_X2pk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2891/Reviewer_X2pk"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for responding."
                    },
                    "comment": {
                        "value": "Thank you for addressing my questions and concerns. I have read through your responses and I would like to increase my score from 5 to 6."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2891/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700512963434,
                "cdate": 1700512963434,
                "tmdate": 1700512963434,
                "mdate": 1700512963434,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "a97sQu9wVu",
            "forum": "JWpwDdVbaM",
            "replyto": "JWpwDdVbaM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2891/Reviewer_Eni6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2891/Reviewer_Eni6"
            ],
            "content": {
                "summary": {
                    "value": "The manuscript tackles the multivariate time series forecasting problem, and proposes a solution called ARM.\nThe proposed method consists of 3 modules, and can be employed to many existing Transformer based time series forecasting models.\nEmpirical evaluation show that ARM, as well as its 3 modules individually, can improve the forecasting accuracy of the base Transformer model."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The idea of applying the 3 modules to existing Transformer models for time series forecasting is novel, to the best of my knowledge. \n\nThe text of the manuscript is clear and not difficult to understand.\n\nThe proposed model has the potential to contribute to the time series modelling community."
                },
                "weaknesses": {
                    "value": "As mentioned in the Strength part, the text of the manuscript is clear and easy to follow.\nHowever, the figures and tables in the manuscript are very hard to read.\n - Font size of the figures and tables are very small\n - The captions are super long and have a very small font size, I assume this violates the submission guideline.\n - The figures are arranged in an order which is different from how the text refers. Readers have to jump back and forth to find the corresponding (sub-)figure."
                },
                "questions": {
                    "value": "It is not clear to me how MoE is operated from the manuscript.\n\nWhy Vanilla+ARM performs better than applying ARM to other Transformer based models?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2891/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2891/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2891/Reviewer_Eni6"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2891/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698356912062,
            "cdate": 1698356912062,
            "tmdate": 1699636232534,
            "mdate": 1699636232534,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TGU5M66kop",
                "forum": "JWpwDdVbaM",
                "replyto": "a97sQu9wVu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2891/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2891/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Eni6 (Part 1)"
                    },
                    "comment": {
                        "value": "We are immensely grateful for your recognition and valuable suggestions regarding our paper. Your insights greatly contribute to enhancing the presentation quality of our work. We are thankful for your acknowledgment of the potential value our paper could bring to the community, and we are enthusiastic about the prospect of our findings being applied in broader research to create greater impact.\n\nBefore answering the specific questions, we would like to briefly recap the main ideas of our paper:\n\n- **Overall Concept of ARM**\n\n  i. Utilize AUEL to extract/control each time series' own forecasting contribution.\n\n  ii. Employ Main Predictor + MKLS to model the inter-series dependencies beyond univariate forecasting contributions.\n\n  iii. Implement Random Dropping to prevent overfitting when learning inter-series dependencies.\n\n\n### Q1. About the Formatting Problems in the Paper\nWe sincerely appreciate your suggestions regarding the formatting of our paper. Following your advice, we have made modifications to significantly enhance its readability.\n\na. We have increased the font size of Tables 1 and 2 and adjusted the layout of Table 2 to make it more readable.\n\nb. All captions have been resized to a standard font size and have been succinctly revised for clarity.\n\nc. The positions of Figures 2 and 3 have been swapped for a more appropriate presentation."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2891/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699938291745,
                "cdate": 1699938291745,
                "tmdate": 1699938291745,
                "mdate": 1699938291745,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "49vIMf9EPs",
                "forum": "JWpwDdVbaM",
                "replyto": "a97sQu9wVu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2891/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2891/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Eni6 (Part 2)"
                    },
                    "comment": {
                        "value": "### Q2. About the Operation Details of MoE\n\nIn AUEL, the MoE is responsible for building univariate temporal patterns. It adaptively allocates the most suitable univariate MLP predictor for each series. Here are the benefits of using MoE:\n- Between the individual univariate models in Figure 3 (a) and the fully-shared-parameter models in Figure 3 (b), MoE in Figure 3 (c) can adapt to datasets with both similar and independent series. \n- The presence of MoE allows the main predictor to focus solely on the contributions of inter-series dependencies, thereby reducing the required number of parameters for the main predictor. \n- In inverse processing, MoE can modify the output of the main predictor, choosing whether to retain the forecasting with inter-series information modeled by the main predictor or to correct it to rely solely on the univariate forecasting of the current series.\n\nWe illustrate the input and output of MoE in Equations (2) and (3). In the preprocessing stage, for each series $i$, MoE builds temporal patterns for the $L_P$ part containing only constant values, based on the processed $\\\\widetilde{X}^{\\*i}$ from the AUEL of distribution. This preprocessing stage of AUEL of temporal patterns is computed as $ \\\\widetilde{X}^{i} = \\\\left\\[ \\\\widetilde{X}^{\\*i}\\_I  \\\\ \\\\ \\\\ \\\\ \\\\mathtt{MoE}\\\\left( \\\\left[ \\\\widetilde{X}^{\\*i}\\_I  \\\\ \\\\ \\\\ \\\\ \\\\widetilde{X}^{\\*i}\\_P \\\\right] \\\\right) \\\\right\\] $. In the inverse processing stage, MoE determines whether to modify the output based on the input $ \\\\widetilde{X}^{i}\\_I = \\\\widetilde{X}^{\\*i}\\_I $ of the main predictor and its output $ \\\\widehat{X}\\_{ED}^{i} $, to balance learning intra-series and inter-series dependencies. This inverse processing stage of AUEL of temporal patterns is calculated as $\\\\widehat{X}\\_M^{i} = \\\\mathtt{MoE}\\\\left( \\\\left[ \\\\widetilde{X}^{\\*i}\\_I  \\\\ \\\\ \\\\ \\\\ \\\\widehat{X}\\_{ED}^{i} \\\\right] \\\\right)$.\n\nFor the specific computations within the MoE block, we employed the model structure from the Switch Transformer (Fedus et al., 2022), which uses routing operations to match each input series to the corresponding MLP predictor for computation. We also used residual connections on $L_P$ to ensure stable training. Details about these computation methods are described in section **A.6.1** of our paper, which we have also emphasized in the revision by bolding the reference in section 3.1. The description is presented as follows:\n\n> For adaptive learning of temporal patterns, we utilize the MoE predictor structure as described in Switch Transformer (Fedus et al., 2022). We adjusted the output dimension of its final layer to cater to the input length of $L_I+L_P$, an output length of $L_P$, with a hidden dimension of $4(L_I+L_P)$. Given the discrepancy in input and output lengths, the MoE's residual shortcut needs specific modifications. We establish a residual connection using the \\\\(L_P\\\\) section of the input before the AUEL preprocessing and before the AUEL inverse processing. Specifically, in the calculation of MoE in AUEL preprocessing, where $\\\\widetilde{X}^{i} = \\\\left[ \\\\widetilde{X}^{\\*i}\\_I \\\\ \\\\ \\\\mathtt{MoE}\\\\left( \\\\left[ \\\\widetilde{X}^{*i}\\_I \\\\ \\\\ \\\\widetilde{X}^{\\*i}\\_P \\\\right] \\\\right) \\\\right]$, we actually compute \\\\(\\\\mathtt{MoE}\\\\left( \\\\left[ \\\\widetilde{X}^{\\*i}\\_I \\\\ \\\\ \\\\widetilde{X}^{\\*i}\\_P \\\\right] \\\\right)=\\\\mathtt{MLP}\\_{\\\\mathtt{selected}}\\\\left( \\\\left[ \\\\widetilde{X}^{\\*i}\\_I \\\\ \\\\ \\\\widetilde{X}^{\\*i}\\_P \\\\right] \\\\right) + \\\\mathbf{0}^i\\_P\\\\). Here, \\\\(\\\\mathtt{MLP}\\_{\\\\mathtt{selected}}\\\\) denotes the MLP predictor selected by the routing part of MoE based on the input, and \\\\(\\\\mathbf{0}^i\\_P\\\\) is the \\\\(L\\_P\\\\) part before AUEL preprocessing, i.e., the default 0 values before AUEL. During inverse processing, we compute \\\\(\\\\widehat{X}\\_M^{i}=\\\\mathtt{MoE}\\\\left( \\\\left[ \\\\widetilde{X}^{\\*i}\\_I \\\\ \\\\ \\\\widehat{X}\\_{ED}^{i} \\\\right] \\\\right)=\\\\mathtt{MLP}\\_{\\\\mathtt{selected}}\\\\left( \\\\left[ \\\\widetilde{X}^{\\*i}\\_I \\\\ \\\\ \\\\widehat{X}\\_{ED}^{i} \\\\right] \\\\right)+\\\\widehat{X}\\_{ED}^{i}\\\\), where the added \\\\(\\\\widehat{X}\\_{ED}^{i}\\\\) is the \\\\(L_P\\\\) part before the AUEL block."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2891/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699939152871,
                "cdate": 1699939152871,
                "tmdate": 1699939152871,
                "mdate": 1699939152871,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "S1uMxBncPg",
                "forum": "JWpwDdVbaM",
                "replyto": "a97sQu9wVu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2891/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2891/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Eni6 (Part 3)"
                    },
                    "comment": {
                        "value": "### Q3. Why Vanilla+ARM Performs Better than Applying ARM to Other Transformer-Based Models\n\nThank you for your insightful question. Indeed, this is a scenario we anticipated during our model design process. Once ARM is applied, the primary role of the main predictor shifts to modeling inter-series dependencies. With the assistance of MKLS, a structurally simple yet effective predictor is required to process the reasonable multivariate representation constructed by MKLS. In cases where token representation is well-designed, the vanilla Transformer has been proven adaptable to a wide range of tasks. Autoformer, on the other hand, introduces an auto-correlation mechanism in the temporal dimension, an improvement that does not align with our goal of modeling inter-series relationships. Informer introduces sparse attention to increase computational efficiency, but this actually compromises performance compared to full attention. Nevertheless, it is worthy to note that, as seen in Table 2, Autoformer+ARM and Informer+ARM still demonstrate stable improvements over past SOTA models like PatchTST and DLinear without ARM.\n\n---\nWe thank you again for your valuable question, which greatly contributes to improving the quality of our paper. If you have any further questions, we are keen to engage in more discussions."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2891/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699939194518,
                "cdate": 1699939194518,
                "tmdate": 1699939194518,
                "mdate": 1699939194518,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rEMQYeOclI",
                "forum": "JWpwDdVbaM",
                "replyto": "a97sQu9wVu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2891/Reviewer_Eni6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2891/Reviewer_Eni6"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the authors for updating the manuscript and for answering my questions. I will keep my positive rating.\n\nSome of the references from arXiv have been published already, e.g. Zhanghao Wu et al. 2020 is published at ICLR, and Aurko Roy (don't know what the star means) et al. 2020 at *Transactions of the Association for Computational Linguistics*."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2891/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699953252648,
                "cdate": 1699953252648,
                "tmdate": 1699953252648,
                "mdate": 1699953252648,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "oI2jYf0hEu",
            "forum": "JWpwDdVbaM",
            "replyto": "JWpwDdVbaM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2891/Reviewer_PUmd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2891/Reviewer_PUmd"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes an enhanced architecture for multivariate time series forecasting using Transformers. The proposed ARM approach incorporates three innovations - Adaptive Univariate Effect Learning (AUEL), Random Dropping (RD), and Multi-kernel Local Smoothing (MKLS). AUEL component introduces learnable exponential moving average instead of classic autoregressive and exponential smoothing for initiating prediction part into the encoder. Random Dropping is almost like an ensemble model which models selected subsets of time series, with aim to reduce spurious patterns among the timeseries. And MKLS which uses one-dimensional convolutional kernels and a channel-wise attention to encapsulate local information. The approach is evaluated on multiple datasets against several competitor approaches."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Consistently overperforms multiple strong competitor approaches on several datasets.\nAblation study is assessing effects of each of the three presented components."
                },
                "weaknesses": {
                    "value": "Some of the performances in the Table one are reported from their respective papers, so I am wondering if it is likely to assure the same experimental setup. \nCertain statements are presented without appropriate evidence to support the claim. For example 'we introduce ARM, a methodology designed for correctly training multivariate LTSF models.', which is strong claim, and moreover implies that other approaches are 'incorrectly training'."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2891/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2891/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2891/Reviewer_PUmd"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2891/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698826699939,
            "cdate": 1698826699939,
            "tmdate": 1700513757994,
            "mdate": 1700513757994,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EFtpG5d3YW",
                "forum": "JWpwDdVbaM",
                "replyto": "oI2jYf0hEu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2891/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2891/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We greatly appreciate your questions and comments on our paper. Your insights are invaluable to us.\n\nBefore answering the specific questions, we would like to briefly recap the main ideas of our paper:\n\n- **Overall Concept of ARM**\n\n  i. Utilize AUEL to extract/control each time series' own forecasting contribution.\n\n  ii. Employ Main Predictor + MKLS to model the inter-series dependencies beyond univariate forecasting contributions.\n\n  iii. Implement Random Dropping to prevent overfitting when learning inter-series dependencies.\n\n### Q1. About the Consistency of Experimental Setup\nThe experimental setup for previous baseline models, including PatchTST, DLinear, FedFormer, Autoformer [1-4], shared the same data processing methods and experimental environment. We built our model code on this same codebase to maintain consistency in the environment. Therefore, in Table 1, we could opt to quote previously reported results from SOTA models, avoiding the potential random errors and fairness concerns that might arise from rerunning experiments. It\u2019s important to note that previous SOTA models reported optimal results for experimental settings with input length $L_I \\in \\\\{ 96, 192, 336, 720 \\\\} $. However, models with our ARM is less sensitive to the choice of \\\\( L_I \\\\), allowing us to consistently use \\\\( L_I=720 \\\\) for all experiments. Even under these comparatively challenging conditions, our results still surpassed previous SOTAs.\n\nConversely, for Table 2, we reran all baseline models at \\\\( L_I=720 \\\\) to demonstrate the improvements brought by integrating ARM. We imported their model structures directly from the baseline models' official codes listed below (see `ARM.py` in supplementary materials) and reran these experiments using hyperparameter settings obtained from their provided scripts. The effective performance improvements from ARM under the condition of merely altering the addition of A/R/M modules provide a fair comparison of results through this ablation method. \n\nIn summary, in both quoted version (Table 1) and reproduced version (Table 2), ARM stably surpuss the previous SOTA results.\n\n[1] https://github.com/yuqinie98/PatchTST/tree/main/PatchTST_supervised/data_provider\n\n[2] https://github.com/cure-lab/LTSF-Linear/tree/main/data_provider\n\n[3] https://github.com/MAZiqing/FEDformer/tree/master/data_provider\n\n[4] https://github.com/thuml/Autoformer/tree/main/data_provider\n\n\n### Q2. About the Inappropriate Strong Claim\nThank you for your attention to the rigor of our paper. In the revision, we have changed \"correctly\" to \"effectively\" in response to your concern. It's worth noting that our initial use of \"correctly\" was based on our considerations. As illustrated in the paper, our idea builds upon the analysis of past observations:\n- Univariate models like PatchTST and DLinear significantly outperformed previous multivariate LTSF models, possibly indicating:\n    - (1) Previous multivariate models failed to properly handle multivariate time series input, resulting in inferior performance in modeling intra-series temporal dependencies compared to univariate models.\n    - (2) Weak or hard-to-model inter-series dependencies in existing benchmark datasets, possibly leading to better performance of univariate models, while multivariate models might learn wrong patterns or tend to overfit.\n- Based on these analyses, we designed ARM to help LTSF models address these issues. Furthermore, we synthesized a Multi dataset with clear inter-series dependencies, generated based on simple shifting of series, as an example. This illustrates that even in scenarios with obvious dependencies, previous multivariate models still failed to learn basic \"copy-paste\" operations (see Figure 1, section A.2, and Figure 6). Since existing LTSF models struggle with such simple series-wise dependency learning, we believe they \"incorrectly process the multivariate time series input\".\n\nConsidering these analyses was based on these observations and to avoid potential misunderstandings and overgeneralizations, we decided to replace \"correctly\" with \"effectively\" in the revision.\n\n--- \nThank you again for your questions and suggestions! You might have additional queries about our paper, and we are more than happy to discuss and clarify them. Your feedback helps a lot in enhancing the quality of our research."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2891/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699937991409,
                "cdate": 1699937991409,
                "tmdate": 1699937991409,
                "mdate": 1699937991409,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KOHPq4IO1m",
                "forum": "JWpwDdVbaM",
                "replyto": "EFtpG5d3YW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2891/Reviewer_PUmd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2891/Reviewer_PUmd"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to authors for detailed responses, after reading the overall comments and feedback, I have increased the rating from 5 to 6."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2891/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700513726100,
                "cdate": 1700513726100,
                "tmdate": 1700513726100,
                "mdate": 1700513726100,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]