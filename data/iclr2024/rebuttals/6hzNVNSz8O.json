[
    {
        "title": "No learning rates needed: Introducing SaLSa - Stable Armijo Line Search Adaptation"
    },
    {
        "review": {
            "id": "bQm4buXyna",
            "forum": "6hzNVNSz8O",
            "replyto": "6hzNVNSz8O",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1753/Reviewer_9vmP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1753/Reviewer_9vmP"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an exponentially smoothed Armijo condition to improve the stability of line-search algorithms for finding the step size of Adam. Specifically, it builds upon the preconditioned gradient search condition proposed by Vaswani et al. [2], and applies exponential moving averages to both sides of the condition to improve the stability and robustness of the found step size to mini-batch noises and random parameter initializations. Experiments are performed on both image and NLP tasks to verify the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper addresses an important problem, i.e., the sensitivity of the found step size to mini-batch noises, in stochastic line-search algorithms[1, 2]. The experiments seem to be comprehensive.\n\n- I have tested the algorithm. The algorithm seems to fix the problem of learning rate drop observed in both SLS [1] and its variants for adaptive step sizes [2]. The resulting performance of the proposed algorithm indeed improves upon previous works. \n\n[1] Vaswani et al. Painless Stochastic Gradient: Interpolation, Line-Search, and Convergence Rates \n\n[2] Vaswani et al. Adaptive Gradient Methods Converge Faster with Over-Parameterization (but you should do a line-search)"
                },
                "weaknesses": {
                    "value": "- The theoretical guarantee is very weak. The assumption that the loss is monotonically decreasing is not realistic given the actual update direction includes momentum. This is further complicated by the fact that the step size is found by the modified (smoothed) Armijo condition rather than the preconditioned gradient. \n\n- I am a little bit suspicious that the step size drop in Figure 3 is caused by numerical issues. In fact, the sudden drop may happen across several iterations caused by mini-batch noises when the gradient norm becomes small. The authors may want to comment more on this."
                },
                "questions": {
                    "value": "In a more recent work [3], the (stochastic) Armijo condition has been relaxed to allow the found step size not guarantee descent. Such non-monotone condition seems to fix the learning rate drop problem. The authors may want to consider adding this work to their baselines. Despite the theoretical weakness, I think this work is significant and would like to accept it. \n\n[3] Galli et al. Don\u2019t be so Monotone: RelaxingStochastic Li neSearch in Over-Parameterized Models"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1753/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698731682817,
            "cdate": 1698731682817,
            "tmdate": 1699636104564,
            "mdate": 1699636104564,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SRTnb2c4BI",
                "forum": "6hzNVNSz8O",
                "replyto": "bQm4buXyna",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1753/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1753/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the very in depth review and your dedication in testing the algorithm.\n\n\nWeakness:\n\n> The theoretical guarantee is very weak. The assumption that the loss is monotonically decreasing is not realistic given the actual update direction includes momentum. This is further complicated by the fact that the step size is found by the modified (smoothed) Armijo condition rather than the preconditioned gradient.\n\n\nA: You are correct in pointing out that this is not a particularly strong theoretical result, however we feel that it is still important to show that we can at least provide some theoretical guarantees. This is not possible for a general case even for the ADAM optimizer alone as momentum terms in general do not allow for convergence guarantees for arbitrary loss functions.\n\n\n\n\n> I am a little bit suspicious that the step size drop in Figure 3 is caused by numerical issues. In fact, the sudden drop may happen across several iterations caused by mini-batch noises when the gradient norm becomes small. The authors may want to comment more on this.\n\n\nA: We are confident that some of the sudden drop stems from numerical issues since the gradient norm drops to true zero, which is not mathematically possible for a non-zero gradient, which we did not observe. We leave a more thorough analysis of this phenomenon for future work. \n\n\nQuestions:\n\nQ: In a more recent work [3], the (stochastic) Armijo condition has been relaxed to allow the found step size not guarantee descent. Such non-monotone condition seems to fix the learning rate drop problem. The authors may want to consider adding this work to their baselines.\n\nA: This is definitely an interesting work. We will consider adding this work as another comparison point in future versions of this paper and added it to the related work."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700559069411,
                "cdate": 1700559069411,
                "tmdate": 1700561554951,
                "mdate": 1700561554951,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "i4ZASaAxFQ",
            "forum": "6hzNVNSz8O",
            "replyto": "6hzNVNSz8O",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1753/Reviewer_XiSh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1753/Reviewer_XiSh"
            ],
            "content": {
                "summary": {
                    "value": "The paper modifies the Armijo line search method incorporating a momentum term into the Armijo criterion."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. the paper start with interesting practical observations of existing method, and improve it via simple approach.\n2. the paper is well written and easy to follow."
                },
                "weaknesses": {
                    "value": "I do not see much benefit of the proposed method over the current lr schedule. As a practitioner, I do not have a strong motivation to use the proposed method after reading the paper. The reasons are as follows:\n\n1. **Fine-tuning tasks are too simple, not convincing.**  Most NLP experiments are based on small fine-tuning datasets and small models.  These tasks are simple for optimization and insensitive to LR choices.  I am hoping to see more challenging tasks such as pretraining tasks for larger models on larger datasets, which rely much heavier on a good lr choice than the fine-tuning tasks shown in the script. \n2.  **Unsatisfactory performance.**   The performance on imagenet tasks is unsatisfactory. Unclear performance on pretraining tasks on larger datasets and larger models.\n3. **Did not save the number of hyperparameters.**  The proposed method has many hyperparameters. For instance, there are at least three hyperparameters in section 2.1 (including initial lr,  c, delta, b ), beta3 in section 3.1, and two betas in section 3.4.  As such, the proposed method does not save much trouble in the current lr schedule. The title \"No learning rate needed\" is overclaimed, as well.\n4. **More runing time**.  Backtracking methods require additional forward and backward passing of neural nets, which could be the major computational bottleneck when the model size grows. Though the authors claim they only require 3% additional running time over standard training,  there is no real evidence to support this claim.  I would suggest the authors report the wall-clock running time and compare it to that of standard training. Further, as the model size grows, the additional forward & backward passing would require much more running time as claimed 3%. \n\n\n\nIn summary, I don't see the motivation to replace the current lr schedule with the proposed method."
                },
                "questions": {
                    "value": "1. In Eq.2: Is v a vector or a scaler?\n2. Wrong template? The current script is using ICLR 2023 template, instead of ICLR 2024"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1753/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698763649083,
            "cdate": 1698763649083,
            "tmdate": 1699636104486,
            "mdate": 1699636104486,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KUNYzkEEyt",
                "forum": "6hzNVNSz8O",
                "replyto": "i4ZASaAxFQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1753/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1753/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your valuable feedback. We answer point by point.\n\nWeakness:\n\n\n> Fine-tuning tasks are too simple, not convincing. Most NLP experiments are based on small fine-tuning datasets and small models. These tasks are simple for optimization and insensitive to LR choices. I am hoping to see more challenging tasks such as pretraining tasks for larger models on larger datasets, which rely much heavier on a good lr choice than the fine-tuning tasks shown in the script.\n\n\n\nA: To address the suitability of SalSa for pretraining tasks we present full pre-training on Image tasks in which our algorithm outperforms the existing baselines. Furthermore, we argue that fine tuning tasks are the most common use case for a practitioner, since pre-training is often infeasible due to resource limitations. Further, it can empirically be observed that these tasks are also sensitive to the learning rate, especially larger sized fine tuning tasks on datasets like MNLI.\n\n\n> Unsatisfactory performance. The performance on imagenet tasks is unsatisfactory. Unclear performance on pretraining tasks on larger datasets and larger models.\n\n\nA: The original ResNet paper on ImageNet trained for 12 epochs instead of our 5, used additional data augmentation and performed 10 crop testing to obtain their final score. We omit additional data augmentation and 10 crop testing, since we are not interested in peak performance but rather the comparative performance between different optimizers. To be more comparable to the original ResNet paper we have increased our training time to 12 epochs, which resulted in the same relative results of the training schemes.\n\n\n\n> Did not save the number of hyperparameters. The proposed method has many hyperparameters. For instance, there are at least three hyperparameters in section 2.1 (including initial lr, c, delta, b ), beta3 in section 3.1, and two betas in section 3.4. As such, the proposed method does not save much trouble in the current lr schedule. The title \"No learning rate needed\" is overclaimed, as well.\n\n\nA: While we do not technically save on the number of hyperparameters, a default choice of these hyperparameters for all training tasks is sufficient. In the ablation studies we have shown extensively that SaLSa is very robust to these hyperparameters. In practice a user does not need to adapt these parameters to accommodate their task. \n\n\n> More running time. Backtracking methods require additional forward and backward passing of neural nets, which could be the major computational bottleneck when the model size grows. Though the authors claim they only require 3% additional running time over standard training, there is no real evidence to support this claim. I would suggest the authors report the wall-clock running time and compare it to that of standard training. Further, as the model size grows, the additional forward & backward passing would require much more running time as claimed 3%.\n\nA: To provide further evidence for our estimated compute we added a wall-clock running time table in the appendix, see Figure 13. There we can see that especially for larger datasets (for example ImageNet, MNLI) 3% is a valid estimate. For smaller datasets (QNLI, SST2) initialization and other overhead results in a slightly higher percentage of extra compute.\nIn the paper we explain that our line search takes on average 30% extra compute per step, since it needs on average about 1 extra forward pass, no extra backward pass is ever needed. We only have to perform our line search on average every 10 steps.\n\nQuestions:\n\n\nQ: In Eq.2: Is v a vector or a scaler?\n\nA: The \u201cv\u201d in Equation 3 is a vector.\n\n\nQ: Wrong template? The current script is using ICLR 2023 template, instead of ICLR 2024\n\nA: Thanks for pointing this out, we changed it to the ICLR 2024 template."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700558953069,
                "cdate": 1700558953069,
                "tmdate": 1700561540606,
                "mdate": 1700561540606,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CEvZliWD4p",
                "forum": "6hzNVNSz8O",
                "replyto": "KUNYzkEEyt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1753/Reviewer_XiSh"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1753/Reviewer_XiSh"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response, I will keep my score"
                    },
                    "comment": {
                        "value": "Thanks for the response, I will keep my score for the following reasons:\n\n1. Fine-tuning tasks on small or medium-sized model (e.g., Bert) is too simple. Shakespeare dataset is too toy. There is no sufficient evidence to support the efficacy of the proposed algorithm. \n\n2. The proposed method did not save the amount of hyperparameters. The title \"No learning rate needed\" is overclaimed. The authors argue that \"SaLSa is very robust to these hyperparameters\", but this might be due to the reason that the experimental settings are intrinsically simple and insensitive to the hyperparameters."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700716414770,
                "cdate": 1700716414770,
                "tmdate": 1700716414770,
                "mdate": 1700716414770,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cVhZ6ZZoxE",
            "forum": "6hzNVNSz8O",
            "replyto": "6hzNVNSz8O",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1753/Reviewer_F4Yh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1753/Reviewer_F4Yh"
            ],
            "content": {
                "summary": {
                    "value": "This work presents a modification to Stochastic Line Search for online learning rate adaptation, which 1) introduce momentum to the line search parameters and 2) gates the frequency of the execution of the line search. The authors motivate these additions with a thorough explication of relevant background, including theoretical analysis, intuitive justification, and concrete demonstration of the limitations of the original method. The authors present thorough experiments across image and language tasks, with significant variance in model architecture and dataset size. The results show impressive average performance over baselines of ADAM/SGD with and without line search. The authors release the code for public use."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The paper is clearly written. The motivation and theoretical analysis are clear. The thoroughness of the experimental setup is excellent. The contribution is very significant and I think likely to be very useful to machine learning practitioners in general."
                },
                "weaknesses": {
                    "value": "While the presented results are very compelling, it would be even stronger to show experiments on Transformer models in a pretaining setting as well. Additionally, it would be beneficial to compare different sizes of Transformer (and CNN) to show the persistence of the benefit of the method across the scaling of the model. These experiments may plausibly be left as future work (the presented ablations are more important and very thorough) but they represent practical questions that ML practitioners will have, so would strengthen the current work if included."
                },
                "questions": {
                    "value": "Nits:\nFigs 4 and 5: it is a little difficult to read these plots, as you have to go back and forth visually between the plot and the legend to see which curve is which. Consider making Salsa lines dashed / SLS lines dotted / baselines solid (or something of that nature) to help improve readability.\nFig 5: loss label missing at top of (a), accuracy label missing at top of (d)\nFig 1: fix occluded label text for y axis"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "10: strong accept, should be highlighted at the conference"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1753/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1753/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1753/Reviewer_F4Yh"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1753/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698817233059,
            "cdate": 1698817233059,
            "tmdate": 1699636104409,
            "mdate": 1699636104409,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HMD9Y4AJTS",
                "forum": "6hzNVNSz8O",
                "replyto": "cVhZ6ZZoxE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1753/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1753/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for this excellent review\n\n\nWeaknesses:\n\n\nYes this would be great, we leave this to future work.\n\n\nQuestions:\n\n\nWe incorporated all the mentioned suggestions for readability, with the exception of dashed/dotted lines. Thanks for pointing this out."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700558843090,
                "cdate": 1700558843090,
                "tmdate": 1700559221773,
                "mdate": 1700559221773,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BHiiVEc5f3",
            "forum": "6hzNVNSz8O",
            "replyto": "6hzNVNSz8O",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1753/Reviewer_QdFA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1753/Reviewer_QdFA"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents SaLSa, an approach to improving Armijo line search by incorporating a momentum term to better handle stochastic mini-batches. This is shown to outperform results without use of the buffer on a variety of different datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The proposed approach of applying smoothing to deal with noise from sampling mini-batches is intuitively and theoretically justified. \n- The authors present results demonstrating competitive performance on a couple of different domains."
                },
                "weaknesses": {
                    "value": "- I'm not too familiar with the previous SLS literature, but it does not seem like a particularly surprising result that a smoothed version of the algorithm, together with a guarantee that every learning rate yields an improved loss results in convergence.\n- I'm a bit concerned by the quality of the baselines. The accuracies achieved on ImageNet with vanilla SGD seem abnormally low; more care should be used in making sure the baselines (\"For the image tasks we compare to a flat learning rate\" seems like an unfair comparison to the baseline since line search is able to adapt learning rates. It would be better to compare to the commonly used waterfall schedule or linear decay schedule)\n- \"Peak classification accuracy\" does not seem to be a standard metric -- it would be preferable to include the accuracy at the end of training or the test accuracy at the checkpoint with the highest validation accuracy.\n- The writing is in general a bit loose and motivation for design choices could be strengthened. There are some issues with spelling and grammar."
                },
                "questions": {
                    "value": "- In general, need to be more careful in using \\citep or \\citet\n- It would be helpful to show that the problem depicted in Figures 1 and 3 is alleviated by using SaLSa.\n- It's a bit strange to report the average or log average final loss as an aggregate metric, given how different the models and datasets are. Perhaps it would be more informative to report the relative ranks of each method.\n- There are cases where SLS outperforms SaLSa -- it would be helpful to investigate further to understand why.\n\nMinor\n- full-filed -> fulfilled\n- having an average -> have an average"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1753/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698860476780,
            "cdate": 1698860476780,
            "tmdate": 1699636104348,
            "mdate": 1699636104348,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mv1joPoZ64",
                "forum": "6hzNVNSz8O",
                "replyto": "BHiiVEc5f3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1753/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1753/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your valuable feedback. We answer point by point. \n\nQuestions:\n\nQ: In general, need to be more careful in using \\citep or \\citet\n\nA: Thanks for pointing this out we fixed it according to the ICLR 2024 guidelines\n\n\nQ: It would be helpful to show that the problem depicted in Figures 1 and 3 is alleviated by using SaLSa.\n\nA: We added SaLSa step sizes to Figures 1 and 3 to show how SaLSa does fix the problem.\n\n\nQ: It's a bit strange to report the average or log average final loss as an aggregate metric, given how different the models and datasets are. Perhaps it would be more informative to report the relative ranks of each method.\n\nA: We added the relative ranks in Tables 4 and 5 in the appendix and added the average rank at the bottom of Table 1 and 2\n\n\nQ: There are cases where SLS outperforms SaLSa -- it would be helpful to investigate further to understand why.\n\nA: This is definitely interesting, but we leave it for future work.\n\nWeakness:\n> I'm not too familiar with the previous SLS literature, but it does not seem like a particularly surprising result that a smoothed version of the algorithm, together with a guarantee that every learning rate yields an improved loss results in convergence.\n\n\nA: Note that an analogous convergence result is not possible for the general case (even for the unaugmented ADAM optimizer) as momentum terms in general do not allow for convergence guarantees for arbitrary loss functions. Hence we find it important to specify relevant restrictions in which we can provide theoretical guarantees.\n\n\n> I'm a bit concerned by the quality of the baselines. The accuracies achieved on ImageNet with vanilla SGD seem abnormally low; more care should be used in making sure the baselines (\"For the image tasks we compare to a flat learning rate\" seems like an unfair comparison to the baseline since line search is able to adapt learning rates. It would be better to compare to the commonly used waterfall schedule or linear decay schedule)\n\n\nA:\nThe original ResNet paper on ImageNet trained for 12 epochs instead of 5, used additional data augmentation and performed 10 crop testing to obtain their final score. We omit additional data augmentation and 10 crop testing, since we are not interested in peak performance but rather the comparative performance between different optimizers. For better comparison to the original ResNet paper we have increase training time to 12 epochs, resulting in better results - yet the relative performance of the optimizers did not change. \n\nWith regards to more complex learning rate schedules: We showed a more complex learning rate schedule as a stronger baseline, specifically the cosine decay schedule for the NLP experiments. For the image datasets we used a flat baseline to allow comparability to the experiments from previous SLS papers [1,2].\n\n\n> \"Peak classification accuracy\" does not seem to be a standard metric -- it would be preferable to include the accuracy at the end of training or the test accuracy at the checkpoint with the highest validation accuracy.\n\nA: I think this is a misunderstanding. We did not perform any hyperparameter tuning on a single dataset for our methods and thus did not use a train, validation and test split. Consequently, our validation and test sets are the same and \u201cpeak classification accuracy\u201d is the accuracy at the checkpoint with the highest validation accuracy. \n\n\n\n\n[1] Sharan Vaswani, Aaron Mishkin, Issam Laradji, Mark Schmidt, Gauthier Gidel, and Simon Lacoste-Julien. Painless stochastic gradient: Interpolation, line-search, and convergence rates.NIPS\u201919:Proceedings of the 33rd International Conference on Neural Information Processing Systems,2019.\n\n\n[2] Sharan Vaswani, Issam H. Laradji, Frederik Kunstner, Si Yi Meng, Mark Schmidt, and Simon Lacoste-Julien. Adaptive gradient methods converge faster with over-parameterization (and you can do aline-search). 2021."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700558712917,
                "cdate": 1700558712917,
                "tmdate": 1700565033259,
                "mdate": 1700565033259,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "f97K7pOpsX",
                "forum": "6hzNVNSz8O",
                "replyto": "BHiiVEc5f3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1753/Reviewer_QdFA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1753/Reviewer_QdFA"
                ],
                "content": {
                    "title": {
                        "value": "Response/ Some additional questions"
                    },
                    "comment": {
                        "value": "Thanks for the response! Some additional thoughts/response:\n\n> For the image datasets we used a flat baseline to allow comparability to the experiments from previous SLS papers [1,2].\n\nI believe it is still valuable to use a decay, as a practitioner would not use a constant learning rate schedule (and this would be a shortcoming of the prior work as well). Additionally, training for a shorter horizon can result in different algorithm performance that may be suboptimal for longer horizons e.g. \"Understanding Short-Horizon Bias\" from Yuhuai Wu et al. \n\n> We did not perform any hyperparameter tuning on a single dataset\n\nTo clarify, does that mean you performed hyperparameter tuning across all datasets? e.g. the paragraph says \"In our experiments we found good values for c to be in the range c \u2208 [0.3, 0.5]. For all our experiments we used c = 0.3\n(compared to c = 0.1 for the original Armijo line search criterion...Furthermore, we tuned the hyperparameter \u03b23 \u2208 [0.9, 0.999] from Eq. 7 on a variety of datasets.\" \n\nDoes this lead to an unfair comparison if you spent more compute on tuning your approach?\n\nMy original point is peak classification accuracy is generally a poor metric because we do not in general have access to the true test set and is thus not possible to measure. It would also reward a method that is high variance and occasionally finds good solutions."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700686701422,
                "cdate": 1700686701422,
                "tmdate": 1700686747429,
                "mdate": 1700686747429,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HRWFmYu1u0",
                "forum": "6hzNVNSz8O",
                "replyto": "BHiiVEc5f3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1753/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1753/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">  Additionally, training for a shorter horizon can result in different algorithm performance that may be suboptimal for longer horizons e.g. \"Understanding Short-Horizon Bias\" from Yuhuai Wu et al.\n\nWe agree with this sentiment which is why we increased the training length from 5 to 12 epochs after your feedback to be the same as in the original ResNet Paper. The relative performance however did not change.\n\n> Does this lead to an unfair comparison if you spent more compute on tuning your approach?\n\nWe performed a grid search for the optimal learning rate for ADAM and SGD for the image datasets, which used a comparatively large amount of compute. In contrast for our method we performed the hyperparameter tuning only once on the datasets indicated in the Appendix but not for all datasets. We also found that in contrast to the learning rate our hyperparameters are relatively insensitive to change, especially $\\beta_3$. \n\n> My original point is peak classification accuracy is generally a poor metric because we do not in general have access to the true test set and is thus not possible to measure. It would also reward a method that is high variance and occasionally finds good solutions.\n\nWe observe that for most datasets we tested, the peak and final accuracy are the same. We also show in Figures 4 and 5 that our methods do not exhibit very high variance, but rather consistently find good solutions."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740079768,
                "cdate": 1700740079768,
                "tmdate": 1700740516143,
                "mdate": 1700740516143,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]