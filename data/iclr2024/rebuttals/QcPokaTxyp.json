[
    {
        "title": "Visual Grounding Helps Learn Word Meanings in Low-Data Regimes"
    },
    {
        "review": {
            "id": "BWqHgQTvwp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1625/Reviewer_ooAw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1625/Reviewer_ooAw"
            ],
            "forum": "QcPokaTxyp",
            "replyto": "QcPokaTxyp",
            "content": {
                "summary": {
                    "value": "The study investigates the effect of visual grounding upon language understanding of language models. For this objective, the authors perform the training of vision-language models in three settings, i.e. language-only, visual-language, and visual-word. Experiments show that visual grounding can aid language learning but mostly in the low-data regime."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper has the following strengths:\n\n- The motivation is clearly stated and makes intuitive sense.\n\n- The experiments are conducted thoroughly with various benchmarks, datasets, and prototype models.\n\n- The obtained insights are strongly proven by the experiment results."
                },
                "weaknesses": {
                    "value": "There are some details which can be raised from the paper:\n\n- Even though the authors provide various evidence to substantiate their claim about the visual grounding ability to help language models, such claim is opposite from the findings attained by previous research of visual grounding [1,2,3] to certain extent. The paper lacks a discussion towards these research.\n\n- Because language models haven proven their effectiveness in multiple applications nowadays, the contribution would become more appealing if the paper discusses what is the impact of its observation upon training language models.\n\n[1] Retrieve, Caption, Generate: Visual Grounding for Enhancing Commonsense in Text Generation Models, AAAI 2022.\n\n[2] Language Adaptive Weight Generation for Multi-task Visual Grounding, CVPR 2023. \n\n[3] Visual Grounding in Video for Unsupervised Word Translation, CVPR 2020.\n\n[4] Vokenization: Improving Language Understanding with Contextualized, Visual-Grounded Supervision, EMNLP 2020."
                },
                "questions": {
                    "value": "- Why is the capacity of visual grounding for language understanding differently found from previous works?\n\n- What benefit does the insight that visual grounding is beneficial for language understanding in low-data settings could provide for training language models?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1625/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1625/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1625/Reviewer_ooAw"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1625/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697276188119,
            "cdate": 1697276188119,
            "tmdate": 1699636091114,
            "mdate": 1699636091114,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cjQhjMrkVT",
                "forum": "QcPokaTxyp",
                "replyto": "BWqHgQTvwp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1625/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1625/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you for your feedback. We appreciate the opportunity to address the concerns and questions you have raised.\n\n**How do our findings relate to previous work?**\n\nThe question in this paper is whether visual information can improve the learning of linguistic representations. This is to eventually help reduce the amount of data that is needed to train the language models. The papers provided by the reviewer are all very different from the topic this paper investigates. All of them aim to improve the performance of certain tasks with the help of visual information. Specifically, [1] explores how visual grounding can help the task of generating common sense text from concepts. [2] focus on multi-modality tasks, exploring how language information can help referring expression comprehension and segmentation on images. [3] uses millions of videos and possibly billions of words to examine how word translation can be improved using video-caption joint training. [4] proposes a new basic architecture for vision-language joint training.\n\nSignificantly, the studies cited in references [1,2,4] either rely on existing models pretrained on billions of words or, as in [3], require large multi-modal training datasets. In contrast, our paper assesses the performance of models trained on smaller datasets, using word learning benchmarks. Our goal is twofold: firstly, to investigate various factors affecting the learning efficiency of language models, and secondly, to deepen our understanding of human language acquisition processes through these models. These objectives distinctly set our work apart from the studies mentioned by the reviewer. Furthermore, our approach contributes to the ongoing efforts in training language models by demonstrating how reducing data requirements, inspired by language acquisition in children, can be effective.\n\n**What do our findings tell us?**\n\nFinally, our findings complement the methods proposed in these papers. To be more specific, we find that the visual information and the linguistic distributional information may compete with each other during learning. This finding suggests that carefully merging these two sources of information may lead to a better model, which can be added to the methods proposed in these papers. Moreover, these methods also provide new candidates to be tested on our evaluation benchmarks. Although they typically start from pretrained language models, we can adapt their methods to start from the Language-Only models trained only on the image captions and integrate the visual information afterward. We will explore this new class of methods in our future work."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1625/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699930200388,
                "cdate": 1699930200388,
                "tmdate": 1699930200388,
                "mdate": 1699930200388,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WgXaKiO7Nl",
                "forum": "QcPokaTxyp",
                "replyto": "mkUDWiKFVC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1625/Reviewer_ooAw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1625/Reviewer_ooAw"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for providing additional details of related work, which has clarified these problems for me.\n\nI have also read your rebuttal and the comments of other reviewers. I am quite aligned with Reviewer QLPJ's perspective. It appears that the current findings may not make significant contribution to the community. Moreover, the absence of practical applications discussed in the paper is a crucial concern. As a result, I preserve my initial evaluation score."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1625/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631015392,
                "cdate": 1700631015392,
                "tmdate": 1700631015392,
                "mdate": 1700631015392,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qqoxmqlvLx",
            "forum": "QcPokaTxyp",
            "replyto": "QcPokaTxyp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1625/Reviewer_XApH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1625/Reviewer_XApH"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates the influence of visual information on word meaning learning of language models. \n\nThe authors train CLIP, GIT, and GPT-2 models in a controlled way and examine the word learning results on various benchmarks such as word similarity and PoS tagging. \n\nThe experimental findings suggest that visual signals bring slight improvements to word learning, especially under low resource regimes. Interestingly, CLIP + Word predictions and language model predictions correlate poorly, indicating that contrastive learning captures a different pattern for word distribution. GIT performs similarly with pure language model, suggesting that it mainly learns from the cross-word distribution, struggling to balance between vision and text.\n\nFurther analysis shows that grounded learning relates to concrete words more like humans than abstract words. Additional results on vision encoder variants, Flamingo architecture, and sentence processing tasks further validate the main results of previous experiments."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Investigating the effect of vision signals on language acquisition is an exciting topic, and the findings of this paper deepen our understanding of vision-aided language modeling. \n\n- The experimental setup and results are comprehensive, with different types of models trained using the controlled network architecture and dataset, various benchmarks are adopted for evaluation and different angles are analyzed."
                },
                "weaknesses": {
                    "value": "- The findings are weak in terms of practicality, as they cannot be directly translated into improvements for existing models. \n- (Minor) Some experimental details are missing. See my questions (Q2 and Q3) below. \n- (Minor) The results bullets are somewhat difficult to follow. I suggest the authors organize these findings into a more natural story to improve the reading experience."
                },
                "questions": {
                    "value": "Q1: What are the implications of these findings for future research? I am not sure if the better concrete word modeling ability in low-resource regimes would be a bonus when we have abundant image-text pairs or single modality data to train models.  I think some previous explorations such as Vokenization [1], Initialization and plug-in fine-tuning [2] and Distillation [3] can be investigated (or discussed accordingly) with the findings in this paper.\n\nQ2: How was the Flamingo model trained? Did you use the same 6-layer Transformer on the CC12M dataset? This seems not possible as it requires a special network for integrating vision information with cross-attention and an interleaved image-text dataset. \n\nQ3: What representation of Visual + language models (CLIP) was used for word-based tasks? Did you extract the corresponding word representation from the full sentence to perform the downstream tasks?\n\n[1] Vokenization: Improving language understanding with contextualized, visual-grounded supervision, Tan et al,  https://arxiv.org/abs/2010.06775\n\n[2] How much can clip benefit vision-and-language tasks?, Shen et al,  https://arxiv.org/abs/2107.06383\n\n[3] VIDLANKD: Improving Language Understanding via Video-Distilled Knowledge Transfer, Tang et al,  https://arxiv.org/abs/2107.02681\n\n[4] Can Language Models Understand Physical Concepts? Li et al, https://arxiv.org/abs/2305.14057"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1625/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1625/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1625/Reviewer_XApH"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1625/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698670669667,
            "cdate": 1698670669667,
            "tmdate": 1699636091031,
            "mdate": 1699636091031,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "eeBMV2BXoS",
                "forum": "QcPokaTxyp",
                "replyto": "qqoxmqlvLx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1625/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1625/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the feedback and suggestions.\n\n**What is the practicality of our findings?**\n\nOur work explores how to improve the learning efficiency of models on small-sized datasets. This aims to reduce the amount of training data needed by the current large-scale language models and to better understand the human language acquisition process in children. Humans can learn languages from only millions of words. This indicates that inspiration from this acquisition process should facilitate the learning process of neural language models, which are currently trained with billions of words. Through exploring the influence of different factors, like visual grounding, on the learning efficiency of such models, our paper is also helping to reverse-engineer the language acquisition and cognitive development process in children.\n\nThe reviewer also mentioned multiple previous explorations [1,2,3,4] Our findings can complement the methods proposed in these papers. To be more specific, we find that the visual information and the linguistic distributional information may compete with each other during learning. This finding suggests that carefully merging these two sources of information may lead to a better model, which can be added to the methods proposed in these papers. Moreover, these methods also provide new candidates to be tested on our evaluation benchmarks. Although they typically start from pretrained language models, we can adapt their methods to start from the Language-Only models trained only on the image captions and integrate the visual information afterward. We will explore this new class of methods in our future work.\n\n**More details about Flamingo training and CLIP evaluation**\n\nLike the other models, our Flamingo model used a 6-layer transformer as its language backbone. The Flamingo architecture worked by having additional cross-attention layers modulating the outputs of text transformers, and these cross-attention layers were inserted at equal intervals of text transformer layers. In our paper, we tried inserting the cross-attention layer after every text transformer layer or after every two text transformer layers. We found that the results of these two methods were similar (see Appendix Fig. 15). We have added more details in the paper (see A.3 in the Appendix). About the CLIP representations, we sent only the word to the model and extracted its hidden representations to be evaluated in our benchmark. We also tried to average the representations of the word across multiple sentences containing that word and found that the results were similar to the single-word representation result."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1625/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699930115706,
                "cdate": 1699930115706,
                "tmdate": 1699930115706,
                "mdate": 1699930115706,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "g05fsZTLFL",
                "forum": "QcPokaTxyp",
                "replyto": "eQOH7jT5jl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1625/Reviewer_XApH"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1625/Reviewer_XApH"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for providing additional details of the evaluation setup, which has clarified these problems for me.\n\nUpon careful consideration of both your rebuttal and the comments from other reviewers, I find myself aligned with Reviewer QLPJ's perspective. It appears that the current findings presented in this paper may not significantly contribute to the community, particularly in light of previous related studies. Furthermore, the absence of practical applications stemming from these findings is a notable concern. Accordingly, I uphold my initial evaluation score."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1625/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700573198496,
                "cdate": 1700573198496,
                "tmdate": 1700573198496,
                "mdate": 1700573198496,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "u32ATzU8uy",
            "forum": "QcPokaTxyp",
            "replyto": "QcPokaTxyp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1625/Reviewer_ZHy9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1625/Reviewer_ZHy9"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents an analysis of the word learning / understanding capabilities of visually grounded models. The paper is motivated by human word/language learning which generally requires much smaller magnitudes of text data than language models, and thus investigates what differences lie in the word-learning dynamics/capabilities of models under different regimes of exposure to text and visual data (in the form of images) under a variety of evaluations pertaining to word learning. \n\nModels evaluated have the following key properties: \n* For language-only models a variant of GPT-2 is used. \n* For visually grounded models, the two main families of models evaluated are CLIP and GIT, however Flamingo is also evaluated in one experiment. \n* Models are also evaluated under varying sizes of text input windows, i.e. at the single-word level, at small contexts around target words, as well as at the full sentence level.\n\nTo assess word learning capability , a variety of evaluations are used: \n* Word similarity -- how model similarity between words correlates with human similarity judgements. \n* Lexical relation prediction -- training linear probes over model features to predict lexical relations such as synonymy or antonymy. \n* Semantic feature prediction -- linear regression over model features to predict strengths of different features of words. \n*  Part of speech prediction -- predicting part of speech tags from SVMs trained on top of model features. \n* Context-based word-understanding benchmark -- a new benchmark presented with the paper, in which models are tasked with ascribing higher probability to correct contexts over perturbed distractors. \n* Brain-response prediction -- linear regression over model features to predict brain response features to input text. \n\nThe paper finds that for a majority of experimental conditions, the multimodal models are generally either worse or comparable to the language-only model. Other findings include: \n* A more fine-grained analysis in human ranking correlation controlled by word features (e.g. prevalence) finds that multimodal models perform better than language models on concrete words, which appears intuitive. \n* Incorporating greater amounts of language context negatively impacts some multimodal model performance. \n* Fine-tuning visual encoders improves performance. \n* Models trained with smaller amounts of data benefit more from multimodality."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The paper is compellingly motivated, and the research direction of understanding the word-learning capabilities of multimodal models is an important area that may help inform the development of future models. \n\n* The paper presents an extensive set of experiments across a wide variety of experimental conditions for analyzing word-learning capabilities of multimodal models. \n\n* The paper does a good job of summarizing its findings/conclusions gleaned from the experimental results, I found the summaries in the subsection titles and conclusions at the end of each subsection helpful in digesting the results."
                },
                "weaknesses": {
                    "value": "* I believe this is a minor weakness, but I was left wondering what experimental results would be like across a wider variety of model families /variants. I was surprised to see the relatively lower performance of Flamingo, and wonder how models such as InstructBlip, LLaVA, and CM3 would perform."
                },
                "questions": {
                    "value": "* For experiments evaluating similarities I'm wondering if any metrics other than the cosine similarity was used? I ask because as I understand it CLIP was trained explicitly to maximize the cosine similarity between matching image/text pairs, so it seems intuitive to me that its similarity judgement capability would be well evaluated under that metric. However, I'm not sure if this is necessarily true of other models, and I wonder if the observed trends might be any different under other metrics such as L2."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1625/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1625/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1625/Reviewer_ZHy9"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1625/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698813854513,
            "cdate": 1698813854513,
            "tmdate": 1700721604755,
            "mdate": 1700721604755,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "67gHUoIl4C",
                "forum": "QcPokaTxyp",
                "replyto": "u32ATzU8uy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1625/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1625/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the positive feedback. \n\n**What experimental results would be for other models?**\n\nThe reviewer is concerned about what results would be across a wider variety of model families or variants. Our selection of the diverse model architectures, including CLIP, GIT, and Flamingo, is supposed to improve the generalizability of our findings to other model families. That being said, the InstructBlip and LLaVa mentioned by the reviewer may belong to a different class of methods, and by the definition of their methods, large training datasets are necessary. This is because they start from pretrained large-scale language models already trained on billions of tokens, then finetune them on multi-modality tasks, and continue to finetune the resulting multi-modality networks using carefully selected visual-language instructions. The improvement of these methods over earlier ones is mainly on multi-modality tasks such as visual question answering and continuous chat with images, while we focus on evaluating the learning processes of linguistic information. It might be possible to replace the pretrained large-scale language models used in these methods with the Language-Only models in our paper and similarly do the multi-modality task finetuning and the instruction tuning, which could be a topic for future work. CM3 represents an even more different class of methods than InstructBlip and LLaVa, where retrieval on a large database is combined with generative models to do both text and image generation.\n\n\n**Why did Flamingo show worse performance?**\n\nAs for why Flamingo worked worse than other methods, we have several hypotheses. The original Flamingo algorithm was designed to start from pretrained large-scale language models with the weights frozen and only train the cross-attention modules. The Flamingo authors even reported lower performance when the weights of language transformer layers were also fine-tuned. Therefore, one hypothesis is that this algorithm cannot efficiently train a neural language model from scratch with the cross-attention modules. Another hypothesis is that this algorithm may work less well with the image-caption pair data used in our work than the large mix of image-caption pairs, website texts embedded with multiple images, and captioned videos used in the original paper.\n\n\n**Metrics other than cosine similarity on the word-relatedness benchmark**\n\nThe reviewer also asked whether metrics other than cosine similarity were used in the word-relatedness benchmark and mentioned that this metric could be easier for CLIP models to do well than other metrics. After testing the L2 distance metric suggested by the reviewer, we indeed find that CLIP models perform worse under this metric in the word-relatedness benchmark than the same models tested using cosine metric, while the performance of Language-Only and GIT models is not influenced by this change of metric. Since cosine similarity is closely related to the L2 distance metric with both vectors L2-normalized, this result suggests metrics influenced by the L2-norms are under-leveraging the capacities of CLIP models. This fits the reviewer\u2019s expectation and is likely because the L2-norms of the hidden representations in CLIP are not optimized by the cosine-similarity objective function during learning and, therefore, do not carry much meaningful information. However, we believe this new finding should be interpreted as support for using the cosine similarity metric in the word-relatedness benchmark since this metric is more robust to model classes and yields results that are no worse than other metrics.\n\nMoreover, the results in the word-relatedness benchmark are not the only results supporting our claim. The semantic-feature prediction benchmark, for example, also shows evidence of benefits from visual information in low-data regimes, and this benchmark is evaluated through training regressors directly on the hidden representations."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1625/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699930014708,
                "cdate": 1699930014708,
                "tmdate": 1699930014708,
                "mdate": 1699930014708,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qWJNCBW03q",
                "forum": "QcPokaTxyp",
                "replyto": "67gHUoIl4C",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1625/Reviewer_ZHy9"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1625/Reviewer_ZHy9"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you to the authors for a thoughtful response and for clearing up my questions. \n\nAfter consideration of the other reviews and discussion I echo other reviewers in agreeing with reviewer QLPJ's assessments. Although I believe the discussion around human language learning is a good motivator for the experiments presented in the paper, I agree with reviewer QLPJ and am of the opinion that asserting any statements/claims about human language learning itself as a conclusion from these experiments is too strong.\n\nWith that said, I retain the belief that the learning efficiency findings may be of interest to members of the community working with the types of models evaluated in the paper. \n\nTherefore, I have updated my score to 6."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1625/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700722085949,
                "cdate": 1700722085949,
                "tmdate": 1700722085949,
                "mdate": 1700722085949,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "x8bK7zfckU",
            "forum": "QcPokaTxyp",
            "replyto": "QcPokaTxyp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1625/Reviewer_QLPJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1625/Reviewer_QLPJ"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates the problem of visually grounded word learning, and arrives at the main conclusion that visual grounding mainly help acquire word meanings in low-data regimes. The performance of word acquisition are measured by calculating the similarity between model prediction and human annotations. Other side findings are also presented in the paper."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Comprehensive evaluation on a wide range of tasks.\n- While some figures are hard to read, the paper is generally well-written."
                },
                "weaknesses": {
                    "value": "- Motivation of the work: the main metric is the similarity between model prediction and human judgments, but there are at least two steps that may result in information loss, and I'm not sure how reliable the conclusions are with the presented approaches:\n    1. The authors used the cosine similarity between word representations to measure how the models acquire words, but other useful information that affects model preference may be encoded in the deeper architectures.\n    2. There may be disagreement between human annotators. For example, Brysbaert et al. (2014) have the word *can* labelled highly concrete, which can cause quite high disagreement among people.\n- Plausibility in terms of data exposure: while [*these (text-only) models are profoundly implausible as models of human cognitive development*] (Page 1), isn't the finetuning CLIP approach similarly implausible? Humans do not pre-train their visual and textual understanding systems on large parallel data; instead, human acquisition of words arguably happens in an incremental way.\n- Arbitrary definition of *human-likeliness* (Appendix A1.2). The evaluation metrics, while reflecting the model acquisition of word meanings to some extent and from certain perspectives, do not necessarily reflect the model's ability to learn words. \n- The line plots with multiple lines (Figs. 1B, 2B) are largely imperceptible. It would be better to use different line styles."
                },
                "questions": {
                    "value": "- For CLIP-based settings, did you use a pretrained CLIP model or use the model architecture/objective with random initialization to train from scratch? If the former, isn't it exposed to many more image-caption pairs than your training data? If the latter, I'd be surprised that 4.3K pairs can lead to a decent performance and would meanwhile suggest the authors rename their models---CLIP is usually used to refer to the pretrained CLIP model.\n- Why did you specifically pick CLIP, Flamingo and GIT as the model? There are several models, such as [Kiros et al. (2014)](https://arxiv.org/abs/1411.2539) and its variants, working on learning visually grounded word and sentence representations. In terms of performance on image-caption retrieval or generation, they might not be competitive with recent work, but the task of this paper is to investigate the acquisition of word meanings, and there's no reason to stick to the recent popular models.\n- (minor) In the first sentence, you probably wish to use *NLM* instead of *LLM* as the abbreviation of *neural language models*?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1625/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699210191100,
            "cdate": 1699210191100,
            "tmdate": 1699636090879,
            "mdate": 1699636090879,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YDpj3CglXS",
                "forum": "QcPokaTxyp",
                "replyto": "x8bK7zfckU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1625/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1625/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you for your feedback. We appreciate the opportunity to address the concerns and questions you have raised.\n\n**Why these evaluation metrics?**\n\nFirst, we emphasize that our study does not solely rely on the cosine similarity between word representations. This metric is only used in the word-relatedness benchmark. The other three benchmarks shown in Fig. 1 do not look at similarity, but instead encodings of syntactic and semantic information in embeddings, as well as their usefulness in language modeling tasks. These additional benchmarks are designed to capture various aspects of word acquisition and understanding, thereby providing a more holistic view of the models' capabilities. \n\nWe also wish to emphasize that many of our evaluations use deep representations, not representations from the word embedding matrix. As discussed in Section 3.1, we optimize the layer from which representations are drawn separately in each model. For CLIP models, later layers are better than earlier layers in general. For GIT models, earlier layers are typically the best. Moreover, the two benchmarks shown in Fig 3 test context representations of models, including surprisals and hidden representations of whole sentences. Both of these benchmarks evaluate more than just word representations.\n\nThe reviewer also mentioned that there could be disagreement between human annotators. This is a good point, but we don\u2019t think it influences the conclusions presented in this paper. The inconsistency between human annotators leads to a ceiling of the performance on the benchmarks. However, when two models are compared on the same benchmark, a higher number still indicates that the word representation is more human-like. Since our conclusions in this paper are drawn from comparisons of models, they should still hold even if human annotators disagree to some extent.\n\nFinally, the specific definition of the \u201chuman-likeness\u201d metric used in A1.2 relates only to correlation with human judgments on the word-relatedness benchmark. Our paper\u2019s broader claims about the human-likeness of learned representations are based on the full suite of evaluations described above and in the paper.\n\n**How was the CLIP model trained?**\n\nWe did not use the pretrained weights of the original CLIP models. The visual features were precomputed from a vision transformer pretrained by unsupervised visual learning algorithms (DINO) on unlabeled ImageNet images. This is discussed in the middle of page 5, beginning at \u201cVisual encoders and image features...\u201d This selection of vision encoder is also explored in Section 4.6 and Figure 5. Thus, in this paper, we use \u201cCLIP\u201d to refer to the language and image contrastive training objective, and call models we trained using this objective function the \u201cCLIP models.\u201d As suggested by the reviewer, we have updated our paper to make this reference clearer (see page 5, we\u2019re currently considering labeling them \u201cCLI\u201d models for \u201cContrastive Language/Image\u201d).\n\n**Why these model architectures?**\n\nThe CLIP, GIT, and Flamingo models represent very different multi-modality learning objective functions. As we draw conclusions from these different models, our results are more likely to generalize to other learning algorithms. For instance, the architecture proposed by Kiros et al. in 2014 utilized a structure-content neural language model module. This approach, similar to Flamingo, also implemented multi-modal interaction through a cross-modality-modulation technique. While we cannot test every possible architecture in the literature, we might expect similar findings based on structural similarity to Flamingo.\n\nFinally, we thank the reviewer for pointing out the typo (LM instead of LLM ) in our paper. We have fixed it. We also appreciate the reviewer\u2019s suggestion about the line style, which we will actively think about to have a better solution."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1625/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699929838095,
                "cdate": 1699929838095,
                "tmdate": 1699929838095,
                "mdate": 1699929838095,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "boXLfNtpHk",
                "forum": "QcPokaTxyp",
                "replyto": "YDpj3CglXS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1625/Reviewer_QLPJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1625/Reviewer_QLPJ"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the quick response! I'm now comfortable with your cognitive plausibility argument, and the point is still not perfectly clear to me in the paper---I noticed that you added \"it is important to note that they are distinct from the pretrained CLIP\nmodels developed by Radford et al. (2021)\", and it'd be good (if I understood correctly) to note that you are using the same model architecture, adapting pretrained visual features, and completely training the language-module weights from scratch. \n\nOn the other hand, I'm still unsure about the conclusion and, therefore, the contribution of this paper. While I agree with the authors that the models decently represent three vision-language modeling approaches, they are just a few among infinite models---it's hard to tell to what extent the conclusion holds for all vision-language models. Similar conclusions (i.e., visual grounding doesn't help much on pure-linguistics tasks) can also be found in various papers (e.g., Tan and Bansal (2020) mentioned by other reviewers), although this is not their major focus."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1625/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699945582746,
                "cdate": 1699945582746,
                "tmdate": 1699945582746,
                "mdate": 1699945582746,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "obfiyrAtcY",
                "forum": "QcPokaTxyp",
                "replyto": "8WwfVrNjio",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1625/Reviewer_QLPJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1625/Reviewer_QLPJ"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the justification of your model selection and further explanation! I really appreciate the active response from the authors. \n\n> Our goal is twofold: firstly, to investigate various factors affecting the learning efficiency of language models, and secondly, to deepen our understanding of human language acquisition processes through these models. \n\nI can't fully agree on either of the claimed contributions, although (I think) I got the ideas from the authors -- I apologize in advance if I misunderstood anything, and I'm happy to discuss further. \n- First contribution: the primary goal of these language models is not learning word meanings, and it's probably more appropriate to say that these models are designed for learning visually grounded meanings for sentences. While word meanings can indeed be argued as the foundation, I would expect more analysis on the original evaluation metrics (e.g., sentence generation/retrieval) if model learning efficiency is claimed as a main contribution.\n- Second contribution: I'm not sure if I see how this could advance our understanding of human language acquisition. Shouldn't we study human language acquisition through human experiments? Or are we assuming that these models are good enough to represent humans so that studying these models would necessarily lead to a better understanding of human brains? \n\n**Re. model selection**: [1-4] indeed shows positive evidence for the selected models on predicting human brain responses. However, I'm still not fully convinced about the model section: the main claims of [1-4] are showing positive results on certain models; in contrast, this paper aims to draw general conclusions for vision-language models, and the goal naturally necessitates a more comprehensive set of models. \n\n**Re. vision and language competence leads to inefficiency**: I'm sorry, but I didn't view this point as a major contribution of this paper as I thought this was quite intuitive from a machine learning perspective. Your evaluation is mostly on content words, and function words, or less visually groundable ones, could be viewed as noise. For Visual+Word model training, function words (noise) from the sentences are removed, and the model can focus more on the content words. It is, of course, nice to verify this, but I wouldn't necessarily be surprised by the results that Visual+Word outperforms Visual+Lang. in many settings, especially in low-data regimes where Visual+Lang. is not exposed to enough data to learn reasonably well. \n\nStill, I find the topic of this paper quite interesting, but the approach and conclusions need more justification."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1625/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700117407105,
                "cdate": 1700117407105,
                "tmdate": 1700117407105,
                "mdate": 1700117407105,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]