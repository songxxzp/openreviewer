[
    {
        "title": "Graph Neural Networks on Symmetric Positive Definite Manifold"
    },
    {
        "review": {
            "id": "cF1zED0zHh",
            "forum": "HAMBmtKLc8",
            "replyto": "HAMBmtKLc8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7342/Reviewer_Jw38"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7342/Reviewer_Jw38"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a convolutional GNN whose features are represented as a symmetric positive definite matrix and measured by the log-Choleski metric. The Log-Choleski metric allows a closed-form expression of the Frechet mean, which is necessary for the aggregation step of the chosen GNN framework. Prior to this step, the feature transformation is performed by a double product with a learned parameter in the Stiefel manifold. Finally, the nonlinearity step is implemented as a rectification of the diagonal of the lower traingular part of a matrix, somehow modifying the eigenvalues of the matrix. The proposed SPDGNN architecture is compared with other geometric proposals for GNN, showing the performance of the model especially in the case of the airport database and disease."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The architecture is novel and most of the choices of parametrization of each step are discussed, especially the choice of metrics.\n\n- The ablation study and experiments on classical databases.\n\n- the paper is written with particular attention to the notations necessary to capture the types of representation (SPD, lower triangular, ...)"
                },
                "weaknesses": {
                    "value": "- Missing at least one recent paper on the same topic with equivalent results on the same experiments (Hyperbolic Representation Learning : Revisiting and Advancing, M. Yang et al. ICML'23). Moreover, the numerical results for the methods presented in both papers are consistent, but not identical, with the globally higher performance reported in Yang et al.\n\n- In the same vein, it would be good to discuss why geometric learning seems to underperform on PubMed and Cora and gives scores for other types of GNN on the same experiments (best competitor). A quick search gives scores on PubMed (91.4 points method from 2021) and Cora (90.16 points method from 2020) that are largely superior, with a caveat on the experimental setting.\n\n- The case of edge-features is not addressed"
                },
                "questions": {
                    "value": "- Could a scaling factor be used in conjunction with the Stiefel parameter to give more freedom to the model?\n\n- Is the initial mapping limited to a linear map, what is its influence?\n\n- What is the expressive power of SPD-GNN? \n\n- Any number of parameters gain compared to other hyperbolic and Riemannian GNNs?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7342/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7342/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7342/Reviewer_Jw38"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7342/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698744886204,
            "cdate": 1698744886204,
            "tmdate": 1699636878533,
            "mdate": 1699636878533,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Tjk5iR7u4J",
                "forum": "HAMBmtKLc8",
                "replyto": "cF1zED0zHh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7342/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7342/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Jw38 (Part 1 of 2)"
                    },
                    "comment": {
                        "value": "Thank you for the thorough evaluation and constructive feedback on our paper. We appreciate the careful consideration of our proposed SPDGNN architecture. We have taken note of your comments and suggestions, and we are committed to addressing them in the revision. Below are our responses.\n\n\n> Q1: \"**Missing at least one recent paper on the same topic....**\"\n\n\nThanks for sharing with us this up-to-date work. There are three main reasons for the slight discrepancies between its results and the ones we reported: 1) Variations in experimental settings, as it employs higher-dimensional representations (8, 64, 256 dimensions), whereas we chose matrix features of 5 and 10 dimensions, corresponding to node representations of 15 and 55 dimensions, respectively. 2) Differences in the reported metrics, as it presents accuracy for Cora, whereas our four datasets are uniformly reported in terms of F1-score. 3) Differences in datasets like Citeseer and PubMed . Furthermore, to our knowledge, its source code is currently not publicly available, making it challenging for us to consider it as our comparative method. Nonetheless, we will cite it and add relevant discussion in the revised version.\n\n\n> Q2: \"**..., it would be good to discuss why geometric learning seems to underperform on PubMed and Cora and gives scores for other types of GNN on the same experiments...**\"\n\nThank you for your thoughtful comments. We understand your concern regarding the seemingly lower performance of geometric learning on PubMed and Cora. Allow us to provide a more detailed explanation:\n- **Problem Setting Clarification:**\nIt's crucial to emphasize that the default development of GNNs is rooted in Euclidean space assumptions, and successful methods within this paradigm often presume settings in Euclidean space. However, these assumptions may not universally hold, prompting our exploration of alternative geometric frameworks, specifically hyperbolic and SPD geometries. Our endeavor surpasses the constraints of Euclidean geometry, delving into more intricate geometric attributes.\n\n- **Addressing Fundamental Problems:**\nOur approach is distinct in that it tackles a more fundamental problem within geometric learning. While state-of-the-art GNN methods developed in Euclidean space can be extended to hyperbolic or SPD manifolds, our work focuses on exploring the foundational geometric embedding space upon which GNNs rely. This emphasis on fundamental geometric properties allows us to understand the limitations and potentials of these spaces in the context of graph representation learning.\n\n- **Representative Baselines:**\nWe exclusively compare our approach against representative methods endowed with geometric properties, specifically Euclidean and hyperbolic geometries. The comparison is intentionally limited to methods that **share a similar architectural foundation**, allowing for a more focused assessment of the inherent advantages and challenges associated with our proposed SPD geometry.\n\nIn essence, our work extends beyond the conventional Euclidean assumptions of GNNs, exploring alternative geometric spaces to address fundamental challenges in graph representation learning. We acknowledge the potential superiority of state-of-the-art methods in Euclidean space, and we appreciate your suggestion to include additional context in our discussion. We will ensure that the revised manuscript provides a more comprehensive explanation of the motivation behind our geometric choices and their implications on the reported performance.\n\n\n> Q3: \"**The case of edge-features is not addressed**\"\n\nThank you for your nice suggestion. Indeed, edge features constitute a pivotal concern. However, this manuscript concentrates on the exploration of latent geometric spaces, with further investigations into its higher echelons earmarked for our subsequent endeavors.\n\n> Q4: \"**Could a scaling factor be used in conjunction with the Stiefel parameter to give more freedom to the model?**\"\n\nThank you for your suggestion, but preliminary analysis suggests that the inclusion of a scaling factor is unavailing for degrees of freedom. Stiefel parameters are acquired through Riemannian gradient descent, ensuring the positive definiteness of the transformed matrix. However, the utilizing of a scaling factor may potentially disrupt this assurance.\n\n>Q5: \"**Is the initial mapping limited to a linear map, what is its influence?**\"\n\nThere is no absolute constraint. In truth, the objective of the initial mapping is to alter the dimensionality $n$ of input features to accommodate the matrix dimension $m$ (ensuring $n = m(m+1)/2$). The utilization of a linear mapping is the most direct and efficient means for this purpose."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7342/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700494458890,
                "cdate": 1700494458890,
                "tmdate": 1700494458890,
                "mdate": 1700494458890,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Zm4fIj1e86",
                "forum": "HAMBmtKLc8",
                "replyto": "Y2gBiLAxpx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7342/Reviewer_Jw38"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7342/Reviewer_Jw38"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the rebuttal.\nI still think the paper needs to be revised for acceptance, especially in terms of experimental settings and comparisons."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7342/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700737988991,
                "cdate": 1700737988991,
                "tmdate": 1700737988991,
                "mdate": 1700737988991,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Aag61W4MSX",
            "forum": "HAMBmtKLc8",
            "replyto": "HAMBmtKLc8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7342/Reviewer_xhi5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7342/Reviewer_xhi5"
            ],
            "content": {
                "summary": {
                    "value": "This article introduces a new GNN architecture based on the idea that nodes are represented not as vectors but as symmetric positive-definite (SPD) matrices. The authors propose an architecture based on the Riemannian manifold associated with the Log-Cholesky metric. They apply this framework to semi-supervised node classification by defining logistic regression on the SPD manifold."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The idea of extending embeddings to SPD matrices seems interesting.\n- Utilizing the Riemannian framework with Log-Cholesky and logistic regressions on the SPD manifold appears to be a promising approach."
                },
                "weaknesses": {
                    "value": "- Many of the contributions in this work are, in fact, already present in the literature, but the discussion regarding prior work is insufficient. The feature transformation with Stiefel matrices and the non-linear activation with the \"ReLU-like\" layer have already been proposed in [3]. However, at the point of defining these layers, the article does not clearly cite [3] as a reference but rather kind of presents them as original contributions. Additionally, the Riemannian/Cholesky approach is directly borrowed from [2], again without clear referencing when introduced. Moreover, there are significant interactions between the approach proposed here and that of [1], which is only discussed in the appendix. It is crucial to compare with [1] in the experiments and position this method in relation to it, as both seek GNNs where nodes are embedded in the space of SPD matrices. Consequently, it becomes challenging to precisely distinguish what constitutes contributions or ideas borrowed from other articles.\n\n- One crucial point that doesn't seem to be discussed is the algorithmic complexity compared to standard approaches. From a memory perspective, the approach seems already highly expensive since each node is represented as a matrix with roughly $n^2$ parameters. Furthermore, in terms of algorithmic complexity, just the aggregation layer requires performing approximately $m$ Cholesky decompositions and computing an outer product with each of the representations (equation 12). So I doubt that this architecture is really applicable. \n\n- I find that the article is generally quite confusing and not very clear. There are numerous and often ambiguous notations (for example, the notations for the Cholesky map and its inverse are almost identical), making it challenging to read. Additionally, the writing is quite heavy, with many vague and non-rigorous statements that don't convey a clear meaning. Here are a few examples:\n  - \"many complex graph data exhibit a profound non-Euclidean potential for analysis\" (what is a \"profound non-Euclidean potential\"?)\n  - \"the SDP manifold [...] captures the hierarchical structure of datasets in hyperbolic subspaces while retaining Euclidean characteristics.\"\n  - \"In typical GNNs, a fundamental assumption is implicitly linked to linear classifiers, as they heavily rely on the Euclidean geometry of $\\mathbb{R}^n$\" (this is not true; you can have Euclidean geometry but use non-linear classifiers).\n\n- The experimental setting in the article appears to lack clarity. The reported results include variances, but it's unclear on how many train/test splits these are based, neither if there is any cross-validation. Additionally, it's not specified whether the competing models were retrained for these experiments. If they were retrained, there is no information on how the retraining was conducted. Alternatively, it's unclear whether the performances were directly taken from the original articles. More transparency and details regarding the experimental setup and data handling would be beneficial for readers trying to understand and reproduce the results.\n\n- References:\n\n[1] Modeling Graphs Beyond Hyperbolic: Graph Neural Networks in Symmetric Positive Definite Matrices. Wei Zhao, Federico Lopez, J. Maxwell Riestenberg, Michael Strube, Diaaeldin Taha, Steve Trettel. ECML 2023.\n\n[2] Geomnet: A neural network based on riemannian geometries of spd matrix space and cholesky space for 3d skeleton-based interaction recognition. Xuan Son Nguyen.\n\n[3] A Riemannian Network for SPD Matrix Learning. Zhiwu Huang, Luc Van Gool. AAAI 2017."
                },
                "questions": {
                    "value": "The Figure 2 is not so clear; what is $V$ on the Figure ? I assume that the manifold is in green while the hyperplane is in darkgrey, but it is very hard to see this \u2018\u2018hyperplane'' and to interpret it."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7342/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698767593035,
            "cdate": 1698767593035,
            "tmdate": 1699636878395,
            "mdate": 1699636878395,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6dfQJ44eqV",
                "forum": "HAMBmtKLc8",
                "replyto": "Aag61W4MSX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7342/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7342/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xhi5 (Part 1 of 3)"
                    },
                    "comment": {
                        "value": "Thank you for your constructive feedback and valuable suggestions. In response, we have endeavored to address your concerns and provide clarifications.\n\n> Q1: \"**Many of the contributions in this work are, in fact, already present in the literature...**\"\n\nThank you for providing insightful comments and pointing out areas for improvement in our manuscript. While it is true that some aspects of our work share common ground with prior research, we contend that our study brings a unique contribution through generalizing the fundamental components of GNNs onto SPD manifold for learning node embeddings. Nevertheless, have carefully addressed each of your concerns in the revised manuscript.\n- **Feature Transformation**:\nThe feature transformation utilizing Stiefiel bilinear mapping [3] is indeed a classical methodology, as described in our related work. However, it is crucial to note that, within the domain of graph neural networks and graph embedding [1,4], Isometry maps for SPD matrices have been the prevailing methodology. Our contribution lies in introducing dimensionality transformations atop Isometry maps for crafting our feature transformation layer. We have conducted a comprehensive ablation study, including relevant comparisons, to substantiate the superiority of our approach over IsometryQR. We appreciate your suggestion to enhance references in the subsection on feature transformation, particularly by incorporating [3].\n- **Non-linearity**:\nRegarding non-linearity and the choice of elevating eigenvalues, our contribution lies in propounding a non-linear methodology grounded in the log-Cholesky metric. In contrast to methods based on eigenvalue decomposition [3], our approach demonstrates efficiency by seamlessly integrating into the neighborhood aggregation layer (Eq.17) within the SPDGNN framework. This integration avoids the computational overhead associated with standalone eigenvalue decomposition.\n\n- **Riemannian/Cholesky Approach**:\nWe respectfully disagree with the interpretation that our Riemannian/Cholesky approach is borrowed from [2]. The Log-Cholesky metric, as a Riemannian metric on the SPD manifold, has found widespread application in various domains. [2] focuses on a distinct task related to Skeleton-Based Interaction Recognition and employs Cholesky space differently. In contrast, our methodology extends the operations of GNN onto the SPD manifold based on the log-Cholesky metric. Additionally, we have duly cited [2] in the related work pertaining to SPD neural networks.\n\n- **Comparison with [1]**:\nWe have addressed your suggestion to compare our method with [1] in the experiments. In the revised version, we have included SPD4GCN [1] as a baseline and reported the results of performance and efficiency. \n\n|          | **Method** | **Disease** | **Airport** | **PubMed** | **Cora**   |\n|----------|-------------|-------------|-------------|------------|------------|\n| **Euclidean** | GCN   | 69.7$\\pm$0.4 | 81.4$\\pm$0.6 | 78.1$\\pm$0.2 | 81.3$\\pm$0.3 |\n|              | GAT   | 70.4$\\pm$0.4 | 81.5$\\pm$0.3 | 79.0$\\pm$0.3 | 83.0$\\pm$0.7 |\n|              | SAGE  | 69.1$\\pm$0.6 | 82.1$\\pm$0.5 | 77.4$\\pm$2.2 | 77.9$\\pm$2.4 |\n|              | SGC   | 69.5$\\pm$0.2 | 80.6$\\pm$0.1 | 78.9$\\pm$0.0 | 81.0$\\pm$0.1 |\n| **Hyperbolic** | HGCN  | 82.8$\\pm$0.8 | 90.6$\\pm$0.2 | 78.4$\\pm$0.4 | 81.3$\\pm$0.6 |\n|               | HAT   | 83.6$\\pm$0.9 |      --      | 78.6$\\pm$0.5 | 83.1$\\pm$0.6 |\n|               | LGCN  | 84.4$\\pm$0.8 | 90.9$\\pm$1.7 | 78.6$\\pm$0.7 | **83.3**$\\pm$**0.7** |\n|               | HYPONET | 96.0$\\pm$1.0 | 90.9$\\pm$1.4 | 78.0$\\pm$1.0 | 80.2$\\pm$1.3 |\n|   **SPD**            | SPD4GCN | 91.1$\\pm$3.5 | 65.8$\\pm$3.4 | 78.1$\\pm$0.6 | 80.2$\\pm$1.4 |\n|               | SPDGNN | **96.9**$\\pm$**0.9** | **94.9**$\\pm$**1.3** | **79.3**$\\pm$**0.7** | 80.5$\\pm$3.2 |\n\nWe can observe that the performance of our SPDGNN surpasses that of [1] significantly, owing to our more comprehensive architecture.\n\n|**Method** | **Disease**| | **Airport**| | **PubMed** | |**Cora**  | |\n|----------------|-------|-------|-------|-------|-------|-------|-------|-------|\n|                | TR(s) | IN(s) | TR(s) | IN(s) | TR(s) | IN(s) | TR(s) | IN(s) |\n| **HGCN**       | 0.011 | 0.010 | 0.014 | 0.005 | 0.018 | 0.006 | 0.011 | 0.007 |\n| **SPD4GCN**    | 1.068 | 1.144 | 3.115 | 3.265 | 18.002| 18.479| 2.779 | 2.702 |\n| **SPDGNN**     | 0.091 | 0.061 | 0.086 | 0.035 | 0.163 | 0.073 | 0.074 | 0.026 |\n\nWe can observe a significant enhancement in the efficiency of SPDGNN compared to [1], attributed to our metric selection and component optimizations as discussed in Section 2. While current SPD methods incur a certain increase in time overhead compared to hyperbolic methods due to various factors, including Cholesky decomposition, Riemannian gradient descent, and hyperplane classifier, the further exploration of its efficiency is warranted by the rich geometric properties."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7342/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700494316872,
                "cdate": 1700494316872,
                "tmdate": 1700494316872,
                "mdate": 1700494316872,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ba72vNG0rp",
            "forum": "HAMBmtKLc8",
            "replyto": "HAMBmtKLc8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7342/Reviewer_KteU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7342/Reviewer_KteU"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a novel approach for defining graph neural networks that learn representations on the SPD manifold. The authors motivate their choice of the SPD manifold by the fact it exhibits both Euclidean and Hyperbolic geometry.\n\nStarting from the Log-Cholesky metric, the authors derive closed-form expressions for weight updates, neighborhood aggregation (by computing the Frechet mean), and most notably for MLR on the SPD manifold (similar to the very important contribution of Ganea, 2018 that showed how to implement MLR for the Poincare Ball model of hyperbolic geometry).\n\nThe authors also propose a new non-linearity for SPD neural networks.\n\nFinally, the authors conduct experiments on common node classification datasets, showing large improvements over previous methods."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The paper presents strong theoretical results, in particular a closed form expression for MLR on the SPD manifold can have impact on other SPD neural network architectures. The exposition is easy to follow, and the mathematics appear sound. Experimental performance shows a significant improvement in some benchmarks."
                },
                "weaknesses": {
                    "value": "Although the paper reads well, there are some areas of lower clarity, I recommended proofreading to improve the writing a bit.\n\nThe paper does not cite previous work on SPD neural networks, e.g., SPDNet, SymNet, Chakraborty et al., etc. although they bear resemblance in the choice, e.g. of bilinear layers or of a rectifying function that amplifies small eigenvalues.\n\nThe experimental evaluation could be improved: Cora and Pubmed are saturated and unchallenging benchmarks, a better choice would be to use some of the more recent OGB benchmarks that come with a standardized evaluation procedure."
                },
                "questions": {
                    "value": "I might be missing something but why does the choice of n >= p ensure positive-definiteness of the transformed matrices? (page 5)\n\nCan the constraint of orthogonal matrices be relaxed?\n\nWith different formulations of the feature transformations and rectifying units compared to previous work, it is unclear whether part of the improved performance comes from these design choices. In the ablation study, can the authors clarify what alternatives were used when \"removing\" the Stiefel linear layers and the non-linearities? Could the authors compare against existing formulations from the SPD learning literature?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7342/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7342/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7342/Reviewer_KteU"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7342/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698779897028,
            "cdate": 1698779897028,
            "tmdate": 1699636878277,
            "mdate": 1699636878277,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zFTp4CjLgn",
                "forum": "HAMBmtKLc8",
                "replyto": "ba72vNG0rp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7342/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7342/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer KteU (Part 1 of 2)"
                    },
                    "comment": {
                        "value": "Thank you for taking time to review our paper and for providing valuable feedback. We appreciate your acknowledgement of our robust theoretical results and the improvements in experimental performance. We have thoroughly considered your suggestions and have addressed the raised points.\n\n> Q1: \"**Although the paper reads well, there are some areas of lower clarity, I recommended proofreading to improve the writing a bit.**\"\n\nThanks for your suggestions. We have conducted a thorough proofreading to enhance the overall writing quality of the paper. \n\n> Q2: \"**The paper does not cite previous work on SPD neural networks,...**\"\n\nThank you for sharing with us these related works. [1] devises a densely connected feed-forward network explicitly tailored for the SPD manifold, incorporating a bi-linear mapping layer and a non-linear activation function. Meanwhile, [2] applies a bi-linear mapping layer and a non-linear activation function for image set classification. [3] focuses on coping with manifold-value images and presents the analogue of convolution operations for manifold-value data. \nWhile our work is related, the key differences are three-fold: 1) We concentrate on graph neural networks for non-structural graph data.  2) We investigate the generalization of GNN components to SPD manifold with the Log-Cholesky metric and propose a framework named SPDGNN, encompassing linear, non-linear, neighborhood aggregation, and MLR layers. 3) We conduct experiments on four real-world graphs and validate our method against various GNN models under both Euclidean and hyperbolic geometries. \nNonetheless, we will cite these works and provide relevant discussion in the revised version.\n\n\n\n> Q3: \"**..., a better choice would be to use some of the more recent OGB benchmarks...**\"\n\nThank you for your valuable suggestions. Currently, the benchmarks commonly used in hyperbolic graph neural networks are still Disease, Airport, PubMed, and Cora [4,5]. Moreover, addressing OGB benchmarks poses additional resource demands for hyperbolic models due to the intricate mapping between the Riemannian manifold and the tangent space. We may consider expanding our experiments to OGB benchmarks after enhancing the efficiency of hyperbolic and other Riemannian manifold models.\n\n> Q4: \"**I might be missing something but why does the choice of n >= p ensure positive-definiteness of the transformed matrices? (page 5)**\"\n\nThere might be a misunderstanding. It is not the case that $n \\geq p$ ensures the positive definiteness of the transformed matrix, but rather it is guaranteed by a full-rank row matrix. The terms $p$ and $n$ merely denote the numbers of rows and columns, respectively.\n\n> Q5: \"**Can the constraint of orthogonal matrices be relaxed?**\"\n\nCertainly. Given that our transformation matrix $\\mathbf{M}$ has $p$ rows and $n$ columns, where $p \\leq n$, the orthogonality constraint solely limits that the rows to be orthogonal vectors. Hence, in essence, it is a semi-orthogonal constraint.\n\n> Q6: \"**... In the ablation study, can the authors clarify what alternatives were used...**\"\n\nThank you for your insightful feedback. In response to your suggestions, we have adjusted the settings for the ablation study, conducting new experiments on variants involving linear layer ablation to provide a more comprehensive understanding of our components. Specifically, we replaced the Stiefel Linear layer with IsometryQR, an isometric mapping approach [5,6]. Our Stiefel linear layer, built upon this method, incorporates functionality for dimension transformation, allowing us to obtain more compact node embeddings.The ablation results also demonstrate the efficacy of our method. Furthermore, in the context of non-linear ablation, we chose to directly omit the non-linear module. This decision stems from the fact that the non-linear module proposed in our approach, based on the Log-Cholesky metric, can be seamlessly integrated into the neighborhood aggregation module. This integration eliminates the separate overhead incurred by Cholesky decomposition, which is a fundamental reason for not directly introducing non-linear methods based on eigenvalue decomposition [1,2].\n\n| **Model**                | **Disease** | **Airport** | **PubMed** | **Cora** |\n|--------------------------|----------------------|----------------------|---------------------|-------------------|\n| **w/o Stiefel Linear**   | 93.1$\\pm$2.6         | 90.3$\\pm$1.2         | 77.5$\\pm$1.4        | 76.3$\\pm$2.3      |\n| **w/o Non-Linear**        | 89.2$\\pm$2.2         | 90.8$\\pm$1.4         | 77.1$\\pm$0.7        | 75.3$\\pm$3.1      |\n| **w/o SPD MLR**           | 93.8$\\pm$3.8         | 90.8$\\pm$1.9         | 76.7$\\pm$0.3        | 73.6$\\pm$3.6      |\n| **SPDGNN**               | 96.9$\\pm$0.9         | 94.9$\\pm$1.3         | 79.3$\\pm$0.7        | 80.5$\\pm$3.2      |"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7342/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700494135915,
                "cdate": 1700494135915,
                "tmdate": 1700494135915,
                "mdate": 1700494135915,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "eknYprEibO",
            "forum": "HAMBmtKLc8",
            "replyto": "HAMBmtKLc8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7342/Reviewer_KpWj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7342/Reviewer_KpWj"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to build Graph Neural Networks using the underlying geometry of symmetric positive definite matrices. The main motivation is that instead of working on Euclidean embeddings of graph features, embedding them in more geometric spaces like the Manifold of Symmetric Positive Definite Matrices (SPD) yields improved performance and richer representations.  \n\nThe authors build on the framework of Log-Cholesky metrics that allow for mapping between the space of lower triangular matrices (with positive diagonal elements) and the SPD manifold. This unique decomposition allows for deriving the main components of GNNs like Feature Transformation, Neighbourhood aggregation, and Non-linear activation using specific formulas. The most significant amongst these is neighbourhood aggregation which can be done using a computationally attractive Frechet mean on the manifold. \n\nVarious experiments are reported to demonstrate that they improve upon standard GNN baselines. An ablation study is also reported to show the efficacy of individual components."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- I found the main idea of this paper: using Log-cholesky metric to map between SPD and Positive lower triangular matrix manifold to be interesting. To that aid, the various components (especially the frechet mean reformulation of neighbourhood aggregation) looks reasonable and interesting \n-  The baseline experiments show decent proof of concept."
                },
                "weaknesses": {
                    "value": "- The exposition of this paper can be significantly improved. I feel a significant lack of overall quality in the structure and messaging of this paper. The abstract is particularly too verbose and unclear.  To this aid, Figures 2 and 3 could be annotated and captioned more clearly to convey the message of the experiment.   \n- Some important baselines appear to be lacking like Zhao 2023 and Lopez et al 2021. I am especially critical of the lack of comparison with Zhao et.al 2023 which seems to propose an identical formulation (i.e. GNNs using SPD manifold - but the specific components are different to this paper).\n- I miss any comparisons on runtime or complexity with previous methods. Again, Zhao et al 2023 seems significant baseline to compare with and report."
                },
                "questions": {
                    "value": "Overall I feel this paper is not yet in the form that can be accepted at ICLR. Despite some similar recent works, the main idea is interesting. However, a lack of comparison with these baselines, and below-par overall writing quality makes it hard to promote acceptance at this point."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7342/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699218196990,
            "cdate": 1699218196990,
            "tmdate": 1699636878153,
            "mdate": 1699636878153,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sfpfek7AEt",
                "forum": "HAMBmtKLc8",
                "replyto": "eknYprEibO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7342/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7342/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer KpWj (Part 1 of 2)"
                    },
                    "comment": {
                        "value": "Thank you for the time and valuable comments. We are glad that you liked our methodology and experimental endeavors. We notice that your concerns lie in two major points: the evaluation compared to (Zhao et.al 2023) and the clarity of exposition. Below, we offer detailed responses to these points and have made corresponding modifications to enhance the revised version in accordance with your concerns.\n\n>Q1: \"**The exposition of this paper can be significantly improved. ... Figures 2 and 3 could be annotated and captioned more clearly...**\" \n\nWe regret any inconvenience caused to the readers and have meticulously refined our manuscript, particularly focusing on the logic and expression in the abstract, as outlined in our General Response. Additionally, we appreciate your suggestions regarding the clarity of Figures 2 and 3. \n- In Figure 2, we present a 3D illustration of the SPD hyperplane, parameterized by the normal vector $\\mathbf{V}$ and a bias point $\\mathbf{P}$. The gray convex conical point cloud outlines the SPD space, while the green points, sampled within SPD space, delineate the SPD hyperplane. \n- In Figure 3, we present a comparative analysis of 3D illustrations, depicting hyperplanes and node embeddings learned through Euclidean and SPD MLR, respectively. Unlike the flat Euclidean hyperplanes, the SPD hyperplanes encompass both flat and curved hyperplanes, corresponding to the geometric structures of Euclidean and hyperbolic spaces, respectively. These comprehensive hyperplanes effectively discriminate node embeddings from distinct catalogs (colors), demonstrating the efficacy of SPD MLR in concurrently incorporating both Euclidean and hyperbolic geometries for classification.\n\n>Q2: \"**Some important baselines appear to be lacking... & I miss any comparisons on runtime...**\"\n \nThank you for your valuable suggestions. Considering the scope of [1] does not align with the domain of graph neural networks, while [2] represents a direct extension for it, we exclusively choose SPD4GCN [2] as our comparative method. Noteworthy distinctions between our approach and [2] include: \n\n- **Metric**: Our proposed operations are grounded in the Log-Cholesky metric, whereas [2] relies on the Gyrocalculus from [1]. \n\n- **Architecture**: In addition to the fundamental neighborhood aggregation layer in GNNs, we introduce dimension-varying linear transformation and SPD MLR layers, setting our approach apart from [2]. \n\nWe have meticulously reproduced the experimental results of [2] using their source code, adhering to consistent experimental settings and data partitioning akin to other baselines. Due to variations in experimental settings, there are certain discrepancies compared to the original results; however, the overall trends remain consistent, particularly in the case of suboptimal results observed on the Airport dataset. These results have been integrated into the revised version of the manuscript. \n\n|          | **Method** | **Disease** | **Airport** | **PubMed** | **Cora**   |\n|----------|-------------|-------------|-------------|------------|------------|\n| **Euclidean** | GCN   | 69.7$\\pm$0.4 | 81.4$\\pm$0.6 | 78.1$\\pm$0.2 | 81.3$\\pm$0.3 |\n|              | GAT   | 70.4$\\pm$0.4 | 81.5$\\pm$0.3 | 79.0$\\pm$0.3 | 83.0$\\pm$0.7 |\n|              | SAGE  | 69.1$\\pm$0.6 | 82.1$\\pm$0.5 | 77.4$\\pm$2.2 | 77.9$\\pm$2.4 |\n|              | SGC   | 69.5$\\pm$0.2 | 80.6$\\pm$0.1 | 78.9$\\pm$0.0 | 81.0$\\pm$0.1 |\n| **Hyperbolic** | HGCN  | 82.8$\\pm$0.8 | 90.6$\\pm$0.2 | 78.4$\\pm$0.4 | 81.3$\\pm$0.6 |\n|               | HAT   | 83.6$\\pm$0.9 |      --      | 78.6$\\pm$0.5 | 83.1$\\pm$0.6 |\n|               | LGCN  | 84.4$\\pm$0.8 | 90.9$\\pm$1.7 | 78.6$\\pm$0.7 | **83.3**$\\pm$**0.7** |\n|               | HYPONET | 96.0$\\pm$1.0 | 90.9$\\pm$1.4 | 78.0$\\pm$1.0 | 80.2$\\pm$1.3 |\n|   **SPD**            | SPD4GCN | 91.1$\\pm$3.5 | 65.8$\\pm$3.4 | 78.1$\\pm$0.6 | 80.2$\\pm$1.4 |\n|               | SPDGNN | **96.9**$\\pm$**0.9** | **94.9**$\\pm$**1.3** | **79.3**$\\pm$**0.7** | 80.5$\\pm$3.2 |\n\nWe can observe that the performance of our SPDGNN surpasses that of [2] significantly, owing to our more comprehensive architecture.\n\n|**Method** | **Disease**| | **Airport**| | **PubMed** | |**Cora**  | |\n|----------------|-------|-------|-------|-------|-------|-------|-------|-------|\n|                | TR(s) | IN(s) | TR(s) | IN(s) | TR(s) | IN(s) | TR(s) | IN(s) |\n| **HGCN**       | 0.011 | 0.010 | 0.014 | 0.005 | 0.018 | 0.006 | 0.011 | 0.007 |\n| **SPD4GCN**    | 1.068 | 1.144 | 3.115 | 3.265 | 18.002| 18.479| 2.779 | 2.702 |\n| **SPDGNN**     | 0.091 | 0.061 | 0.086 | 0.035 | 0.163 | 0.073 | 0.074 | 0.026 |"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7342/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700493955319,
                "cdate": 1700493955319,
                "tmdate": 1700493955319,
                "mdate": 1700493955319,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]