[
    {
        "title": "SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis"
    },
    {
        "review": {
            "id": "Ek8PcFB3Wy",
            "forum": "di52zR8xgf",
            "replyto": "di52zR8xgf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3626/Reviewer_rcq7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3626/Reviewer_rcq7"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces the following techniques to improve Stable Diffusion and it achieves much better performance according to user study. The trained model is also promised to be released publicly:\n1. A more scalable model architecture: modifications on transformer blocks, more powerful text encoders.\n2. Enable conditioning on image size and cropping.\n3. Training with multiple aspect ratio.\n4. Better Autoencoder.\n5. Additional refinement stage enables more high-resolution details.\n6. Additional finetuning that conditions on pooled text embedding enables multi-modal control during inference."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. The paper is well-written with clear high-level intuitions/ideas, low-level implementation details, as well as visualizations that justify the improvements. It is easy to follow and understand even for the audience that only have very basic knowledge or practice about diffusion models.\n\n2. The problems the authors are trying to tackle are very realistic in practice and the their proposed solutions are simple and effective: \n- in order to fully utilize all the training data with varying image resolutions, they propose to add conditioning on image size during training. \n- in order to solve the commonly-seen cut-out artifacts during generation (as in Fig. 4), they propose to add conditioning on cropping parameters during training.\n- in order to improve high-resolution details, they propose to train another refinement model and use it in a SDEdit way during inference.\n- in order to enable multi-modal control during inference, they propose to finetune only the embedding layer of a text-to-image SDXL such that it can condition on pooled CLIP text embedding as well, which could be naively replaced with image CLIP embedding during inference.\n\n3. The authors promised to open-source the model (last paragraph of Sec. 1), plus all the implementation details in both the main text and supplementary materials including the pseudo code in Fig. 17. I believe this work could be a huge add-on to the image generation community and enable more future works that build upon it, just like Stable Diffusion."
                },
                "weaknesses": {
                    "value": "1. Multiple techniques are proposed in this work, therefore a more detailed ablation study would help the audience better understand what\u2019s the influence of each individual component, especially for the two additional conditioning (size and crop). \n2. Particularly, I\u2019m very curious on which method would be more critical to address the aspect ratio issue, the proposed cropping conditioning or data bucketing from NovelAI? According to the description in Sec. 2.3, these two techniques are combined together and there\u2019s no separate evaluation on each of them.\n3. The rightmost examples in Fig. 5 look confusing to me: does (512, 512) mean the whole images are cropped out and there should be nothing inside (the caption says images are from a 512^2 model)? In this case, why does the top example still show basically the same image as the leftmost one while the bottom example shows mainly the background?\n4. The training data of SDXL is not mentioned in the paper. Is it the same as Stable Diffusion, i.e., LAION? If not, would the comparison still be able to justify the effectiveness of proposed techniques since the training data is different?"
                },
                "questions": {
                    "value": "It would be great if the authors could respond to the weakness points mentioned above. Thanks!"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3626/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3626/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3626/Reviewer_rcq7"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3626/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698702032089,
            "cdate": 1698702032089,
            "tmdate": 1699636317996,
            "mdate": 1699636317996,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aJ7ulCoPka",
                "forum": "di52zR8xgf",
                "replyto": "Ek8PcFB3Wy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3626/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3626/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer rcq7,\n\nThank you for your careful review of our manuscript and your appreciation for our efforts. To address your questions:\n\nIn general, crop-conditioning can be applied as a finetuning technique and address issues observed in previous versions of SD when random cropping expansion from training carried over into the synthesized samples. However, as described in Section 2.3, crop-conditioning is particularly beneficial when we train at a fixed resolution, e.g. 512x512 pixels. For the final multi-aspect model, crop-conditioning does not need to be applied. The effectiveness of size conditioning is shown in Tab. 2. In Fig 5 the model is sampled with different inputs for the crop conditoning, but no actual cropping is happening. The rightmost examples in Fig. 5 are OOD, and the behavior here is therefore less predictable."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3626/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738682612,
                "cdate": 1700738682612,
                "tmdate": 1700738682612,
                "mdate": 1700738682612,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iExMMBJ8ul",
            "forum": "di52zR8xgf",
            "replyto": "di52zR8xgf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3626/Reviewer_gmj5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3626/Reviewer_gmj5"
            ],
            "content": {
                "summary": {
                    "value": "This method proposes SDXL,  a latent diffusion model for text-to-image synthesis. Compared to previous versions of Stable Diffusion, SDXL leverages a three times larger UNet backbone, achieved by significantly increasing the number of attention blocks and including a second text encoder. Also, the authors propose multiple novel conditioning schemes and train SDXL on multiple aspect ratios. Competitive performance achieved with SoTA models such as Midjourney."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Very sound elaboration and experiments. The proposed improvement is reasonable and effective.\n2. The method is open sourced."
                },
                "weaknesses": {
                    "value": "1. Lacks some comparison with SoTA pixel-space diffusion model such as Simple Diffusion from Google.\n2.  Also, SDXL still relied on two-stage training where a high-quality encoder is required. I wonder whether the two stage can be combined and lead to further performance boost?"
                },
                "questions": {
                    "value": "1. As discussed in your future work section, can SD be combined into single-stage training and further boost the performance?\n2. Can similar training pipeline introduced in this paper be applied to 3D diffusion training, and what are the challenges?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3626/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3626/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3626/Reviewer_gmj5"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3626/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698796730798,
            "cdate": 1698796730798,
            "tmdate": 1699636317915,
            "mdate": 1699636317915,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yaLLXsQjJC",
                "forum": "di52zR8xgf",
                "replyto": "iExMMBJ8ul",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3626/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3626/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer gmj5,\nThank you for your careful review of our manuscript and your appreciation of our efforts.\n\nIn response to your questions:\n\n- Single-Stage Training: We agree that single-stage training is an exciting avenue for future research, and both [1] and [2] present promising results in this direction. Currently, two-stage, latent diffusion approaches give the best results [5] for text-to-image synthesis and we follow this paradigm.\n- For 3D, the main issue with directly scaling diffusion-based approaches is data scarcity [3]. However, we hypothesize that multimodal approaches that combine explicit 3D training with 2D and video training can eventually help to overcome this issue. The micro-conditioning techniques that we introduced in our paper can be adapted and applied to other modalities, see e.g. [4]\n\nReferences: \n\n[1]: Emiel Hoogeboom and Jonathan Heek and Tim Salimans: *simple diffusion: End-to-end diffusion for high resolution images*\n\n[2]: Gu et al: *Matryoshka Diffusion Models*\n\n[3]: Deitke el al: *Objaverse-XL: A Universe of 10M+ 3D Objects*\n\n[4]: Blattmann et al: *Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets*\n\n[5]: Betker et al: *Improving Image Generation with Better Captions*"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3626/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738643592,
                "cdate": 1700738643592,
                "tmdate": 1700738643592,
                "mdate": 1700738643592,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ObwicDKojq",
            "forum": "di52zR8xgf",
            "replyto": "di52zR8xgf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3626/Reviewer_n7kc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3626/Reviewer_n7kc"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents SDXL, an extended version of the existing StableDiffusion model, showcasing its training and capabilities. The authors detail the methodology used in image training, including the resolution, crop, top, and left condition techniques, as well as the innovative multi-aspect resolution training approach. Furthermore, the paper sheds light on the advancements in AutoEncoder technology and the implementation of a Refinement model. The overall generation process is structured in two stages, allowing for high quality images. Also authors introduce diverse multimodal controls. This comprehensive explanation highlights the significant enhancements and versatility that SDXL brings to image generation, setting a new standard in the field."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "This paper not only introduces a variety of innovative training methods but also provides clear intuition and thorough analysis to support them. The novel conditioning techniques and multi-aspect resolution training approaches presented in the manuscript are both groundbreaking and well-substantiated with comprehensive evaluations. The authors have done an excellent job of providing numerical metrics and a wide array of visual comparisons to showcase the effectiveness of their proposed methods. The manuscript is well-written, with a coherent structure that facilitates easy understanding, making it a significant contribution to the field."
                },
                "weaknesses": {
                    "value": "The manuscript appears to lack detailed explanations on how the autoencoder (AE) has been improved or advanced. There is also no clear description of the user study conducted, raising questions about its methodology and implementation. Is it conducted in a manner similar to what is described in the supplementary materials? Providing more information and context on these aspects would greatly enhance the comprehensibility and robustness of the paper."
                },
                "questions": {
                    "value": "- Refinement training : Could you kindly explain about details of refinement training? This training is only for  200 (discrete) noise scales which is [200, 0] ? \n\n- Could you give me some intuition and the effect of using a combination of language models (CLIP ViT-L & OpenCLIP ViT-bigG)? Is it crucial to use all of them?\n\n- I wonder the author's opinion about using TransFormer architecture to train the DMs. Could you share more information on \"no immediate benefit\"?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No concerns"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3626/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3626/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3626/Reviewer_n7kc"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3626/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698839937853,
            "cdate": 1698839937853,
            "tmdate": 1699636317817,
            "mdate": 1699636317817,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bH9tDuxx6j",
                "forum": "di52zR8xgf",
                "replyto": "ObwicDKojq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3626/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3626/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer n7kc\n\nWe appreciate your careful review of our manuscript and your recognition of our efforts. \n\nAs we describe in Sec 2.4, the autoencoder has been improved by adding EMA-based weight tracking and increasing the batch size used during training. Since reviewer pUdr also asked about the autoencoder, we answer that in our general response. TL;DR: EMA consistently gives better reconstruction metrics. Larger batch sizes provide better gradient updates. \n\nIn response to your further questions:\n\n1. Yes.\n2. We empirically found that these CLIP text encoders contain complementary semantic knowledge, for example about visual styles.\n3. We briefly experimented with the DiT[1] and UViT[2] architectures, but found that the images produced were not of higher quality. However, this could be due to the suboptimal choice of hyperparameters during the short experimentation phase. we remain optimistic that these architectures are good candidates for scaling diffusion models to much larger sizes, and that the simplified architectures may ease engineering efforts.\n\nReferences: \n\n[1]: William Peebles and Saining Xie: *Scalable Diffusion Models with Transformers*\n\n[2]: Emiel Hoogeboom and Jonathan Heek and Tim Salimans: *simple diffusion: End-to-end diffusion for high resolution images*"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3626/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738590962,
                "cdate": 1700738590962,
                "tmdate": 1700738590962,
                "mdate": 1700738590962,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TUdTXVGvGW",
            "forum": "di52zR8xgf",
            "replyto": "di52zR8xgf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3626/Reviewer_pUdr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3626/Reviewer_pUdr"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents SDXL, a latent diffusion model for text-to-image synthesis. SDXL uses a larger U-Net compared to previous Stable Diffusion models, and adds a refiner module to improve visual quality of image samples. During SDXL training, the U-Net is conditioned on image size, image cropping information, and receives training data in multiple aspect ratios. A new VAE is trained, with improved reconstruction performance compared to earlier SD versions."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. SDXL demonstrates impressive text-to-image generation results. It can serve as a strong base model for a broad range of research and applications in downstream image synthesis tasks.\n2. Some training and architectural choices are backed with convincing ablation experiments (e.g. Table 2). This can offer some valuable insights for future training of image and video generation models.\n3. The refinement model could be an additional contribution, to refine local details of real / generated images."
                },
                "weaknesses": {
                    "value": "1. For \"conditioning on cropping\" and \"multi-aspect training\", this paper lacks adequate quantitative experiments to demonstrate their effectiveness.\n2. Minor mistakes:\n(1) In Figure 3, the image size should be (256, 256) instead of (256, 236).\n(2) In Figure 3 and Figure 4, the quotation mark for text prompt is incorrect."
                },
                "questions": {
                    "value": "1. There could more explanations on why your current autoencoder provides better reconstruction performance: which factor contributes more, the EMA choice, or larger batch size? \n2. How well does the refinement model work when refining other real images, or images generated by previous SD versions? That is, instead of directly refining latents in VAE latent space, but refining the VAE encoded latents from other real / generated images. \n3. I wonder the relative impact, or the priority of various choices, that is, which plays a bigger role: larger UNet, or each of the conditioning mechanism. I believe these insights are what the research community is insterested in."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3626/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3626/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3626/Reviewer_pUdr"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3626/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699105924301,
            "cdate": 1699105924301,
            "tmdate": 1699636317752,
            "mdate": 1699636317752,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xcdltZMHhk",
                "forum": "di52zR8xgf",
                "replyto": "TUdTXVGvGW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3626/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3626/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer pUdr\n\nThank you for taking the time to review our manuscript and for appreciating our work. We fixed the typos you pointed out. \n\nTo answer your questions:\n\n1. Since reviewer n7kc also asked about the autoencoder, we answer that in our general response. TL;DR: EMA consistently gives better reconstruction metrics. Larger batch sizes provide better gradient updates. \n2. The refiner can be applied both on real images (in the SDEdit [1] fashion), or to \u201cfinish\u201d the denoising process from the base model. Both work equally well, note that the latter option is cheaper when synthesizing from a text prompt. We find that the refiner is also able to add high frequency details when utilizing the SDEdit approach.\n3. Crop-conditioning in particular can be applied as a finetuning technique and fix issues observed in early versions of SD, where the random cropping augmentation from training leaked into the synthesized samples. However, as we describe in Sec. 2.3, crop-conditioning is mostly beneficial when we train on a fixed resolution, e.g., 512x512 pixels. For the final multi-aspect model, crop-conditioning does not need to be applied. Further, we ablate the effectiveness of size-conditioning in Tab. 2.\n\nReferences: \n\n[1]: Meng et al: *SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations*"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3626/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738560263,
                "cdate": 1700738560263,
                "tmdate": 1700738560263,
                "mdate": 1700738560263,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]