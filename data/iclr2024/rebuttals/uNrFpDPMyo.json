[
    {
        "title": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs"
    },
    {
        "review": {
            "id": "YN66lg0xUR",
            "forum": "uNrFpDPMyo",
            "replyto": "uNrFpDPMyo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6547/Reviewer_XtqU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6547/Reviewer_XtqU"
            ],
            "content": {
                "summary": {
                    "value": "Key-value cache takes the majority of GPU memory in LLM serving, and its extent is continuously growing along with the model size and context length. Therefore, if we can reduce the key-value cache memory while maintaining the generation quality, we can accelerate the LLM inference. \bThe authors propose FastGen, a framework for efficient generative inference by applying on-the-fly key-value compression. They analyzed the structural patterns of each attention head of layers and then categorized four policies. By adopting the optimal policy based on profiling, FastGen achieves a comparable generation quality to the full-cache (non-compression) inference."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The authors provide abundant experiments with varying model sizes and tasks\n- The authors provide informative ablation studies\n- Their work will motivate various related work, for example,\n    - efficient kernel which aware of compression\n    - as the model size grows, more redundant key-values exist where we have more room to optimize"
                },
                "weaknesses": {
                    "value": "- Since the inference time matters in practical serving, it would be helpful to understand more if the authors can provide corresponding results\n    - For example, how long does inference take compared to the full-cache strategy? I think it might become slower because the existing attention kernels may not efficiently deal with the sparsity\n    - How long does profiling take? Is it feasible for practical inference scenarios?\n- It seems that the StreamingLLM paper [1] is similar to this work. It sets sink tokens and performs local attention, where the sink tokens may correspond to $C_{special}$ (and maybe $C_{punct}$. Since the StreamingLLM paper has also recently been uploaded, it is unlikely to compare this paper with it. But it would be better if the differences in this paper were clarified.\n\n[1] Xiao, Guangxuan, et al. \"Efficient Streaming Language Models with Attention Sinks.\" arXiv preprint arXiv:2309.17453 (2023)."
                },
                "questions": {
                    "value": "- What are the additional challenges for the models that use the grouped query attention technique?\n- In Figure 4, attention scores of special tokens always take more than half. Are there attention heads whose special token score is lower than half?\n- In Figure 5, compressing sometimes wins the full-cache strategy. How can we interpret such results?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6547/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6547/Reviewer_XtqU",
                        "ICLR.cc/2024/Conference/Submission6547/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6547/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698292994618,
            "cdate": 1698292994618,
            "tmdate": 1700800787469,
            "mdate": 1700800787469,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vGjhPvQNdc",
                "forum": "uNrFpDPMyo",
                "replyto": "YN66lg0xUR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6547/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6547/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**W1.1: How long does inference take compared to the full-cache strategy? I think it might become slower because the existing attention kernels may not efficiently deal with the sparsity.**\n\nR1.1: To address reviewers\u2019 concerns on the end-to-end speedup of FastGen, we implement a sparsity kernel for KV-cache pruning and present the end-to-end latency improvement in Table 1. Please refer to general response #1 for detailed settings and analysis.\n\nAs shown in Table 1, we can observe that FastGen achieves significant end-to-end speed-up across all the generation settings. For the least significant case, Fastgen can have a decent 16.04% latency improvement over the full-cache baseline on a short generation length of 512. In the best cases, we can achieve up to 55.0% latency reduction with Fastgen at a generation length of 16k. \nWe can also observe a clear and consistent tendency of larger relative speed-up as generation length becomes longer. For example, given batch_size = 1, FastGen\u2019s relative speed-up rises from 16.04% to 55.0%, as the generation length grows from 512 to 16384. The phenomenon can also be observed in other batch settings. \n\nThis analysis confirms that FastGen can achieve major speed-up in real development, especially in long-generation settings. Meanwhile, the efficiency of the customized kernels can be further improved. We leave this unique research and engineering challenge to future works.\n\n**W1.2: How long does profiling take? Is it feasible for practical inference scenarios?**\n\nR1.2: To better understand the overhead of the profiling step, we compare the profiling time with the total generation time across different generation lengths. We present the result in Table 2. Please refer to general response #2 for detailed settings and analysis. \n\nTable 2 shows the profiling time of the LLaMA65b in different generation length settings. We can observe that the profiling time only accounts for a very small percentage of the total generation duration, up to 0.35% in our tested cases. Also, the overhead decreases as the generation length increases, dropping to 0.07% when the generation length comes to 1024.\n\nIn terms of extra memory usage, it\u2019s mainly introduced by one of the compression strategies, C_frequent, which needs to store an extra cumulative sum of attention scores for each attention head.  To provide a detailed analysis, for each layer, the dimension of the KV cache is (batch_size, num_of_head, sequence_len, hidden_dimension), while the dimension of extra memory for the cumulative attention scores is (batch_size, num_of_head, sequence_len). Considering hidden_dimension=128 for all model sizes, the memory overhead is 1/128=0.78% compared to storing KV cache only, which is a negligible cost.\n\nIn conclusion, the overhead introduced by the profiling step is nearly negligible in both time and memory, which confirms FastGen\u2019s potential for real-world deployment.\n\n**W2: It seems that the StreamingLLM paper [1] is similar to this work.**\n\nA2: Thanks for mentioning this concurrent work! We were not aware of it at the time of submission given it was posted after the ICLR deadline. After reading the paper, we think it is a great parallel work on long-context LLM, and we are happy to provide a comparison between it and FastGen! Generally, the two differ in two aspects:\n1) Goal and Setting: StreamingLLM aims to extend the context length of LLM, while FastGen aims to improve the efficiency of general LLM inference (normal+long contexts). As a result, FastGen focuses specifically on generation tasks, which is different from the general task settings in StreamingLLM.\n2) Method: StreamingLLM uses a fixed attention pruning strategy for all attention heads, while FastGen focuses on adaptively choosing different compression strategies according to the attention structure of each head."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6547/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700721989802,
                "cdate": 1700721989802,
                "tmdate": 1700721989802,
                "mdate": 1700721989802,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gc87DBhLPk",
            "forum": "uNrFpDPMyo",
            "replyto": "uNrFpDPMyo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6547/Reviewer_rrTE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6547/Reviewer_rrTE"
            ],
            "content": {
                "summary": {
                    "value": "This paper propose an adaptive KV cache compression technique to reduce the memory footprint of generative inferences of LLMs. The authors fist perform targeted profiling to indentify the intrinsic structure of attention modules, and then build an adaptive KV cache by evicting long-range contexts on attention heads emphasizing local contexts, removing non-special tokens on attention heads centered on special tokens, and using only the standard KV cache. The experimental results show the adaptive KV cache achieves large reduction on GPU memory consumption with trivial geneation quality loss."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper works on an important topic, i.e., reducing the memory footprint of GPU during generative inferneces of LLMs.\n2. The paper flows well."
                },
                "weaknesses": {
                    "value": "1. The model profiling part is not clear. Did the authors do a profiling for each model on all datasets, or each model on a single dataset. \n2. The model profiling results have a huge impact on the final KV cache compression results. Although the authors show empirical data supporting the structure of the attention map is stable at different positions for all attention heads, the authors still need to discuss what if the structure of the attention map is not stable."
                },
                "questions": {
                    "value": "Please comment the two points in the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6547/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698621581818,
            "cdate": 1698621581818,
            "tmdate": 1699636738413,
            "mdate": 1699636738413,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MoMRiEpm1E",
                "forum": "uNrFpDPMyo",
                "replyto": "gc87DBhLPk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6547/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6547/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer rrTE"
                    },
                    "comment": {
                        "value": "**W1: The model profiling part is not clear. Did the authors do a profiling for each model on all datasets, or each model on a single dataset.**\n\nA1: The profiling is run on every data instance (each sentence). For the same model, the profiling would be different if the input is different. So, we do the profiling on the fly during deployment. Such fine-grained flexible adaptation allows FastGen to reduce more memory footprint while preserving the model quality. We also provide more analysis on the overhead of profiling in Table 2 of **General Response**, which shows the cost is nearly negligible. \n\n**W2: the authors still need to discuss what if the structure of the attention map is not stable.**\n\nA2 We are not quite clear what the reviewers mean by \"the structure of the attention map is not stable\". We hypothesize that the reviewer was referring to the results in Figure 4, which shows the accumulated attention scores remain consistent/stable during the entire generation phase. The reviewer might be wondering if this observation can be generalized to all situations (e.g., different datasets and models). If that's the case, we think the reviewer raises a valid concern about FastGen, as some strategies such as static eviction policy (e.g., punctuation tokens) may no longer provide an accurate prediction, and more adaptive policies are needed.  To remedy this, we propose to repeat the diagnosis step multiple times across the generation process of one sentence. To be more specific, we choose a new pruning policy once the number of decoded tokens reaches $k$, where $k \\in$[1,sentence_len] is a predefined hyperparameter. For each sentence, the diagnosis is repeated for sentence_len//$k$ times. From the analysis in the second part of General Response, we could infer that the profiling overhead is relatively small even if it is repeated several times. On the other hand, it would be interesting to study the theoretic explanation of our observation, which we would like to explore in the future."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6547/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700724016631,
                "cdate": 1700724016631,
                "tmdate": 1700724016631,
                "mdate": 1700724016631,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "aNqNdIZp5T",
            "forum": "uNrFpDPMyo",
            "replyto": "uNrFpDPMyo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6547/Reviewer_9wxD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6547/Reviewer_9wxD"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes FastGen, an adaptive key-value (KV) cache compression method to reduce the memory footprint and accelerate inference for large language models (LLMs). The key ideas are: 1) Profiling attention modules to discern their intrinsic structures, such as primarily attending to local contexts or special tokens. 2) Constructing the KV cache adaptively based on the recognized structure to compress less useful contexts. 3) The lightweight attention profiling guides the KV cache compression without expensive fine-tuning.\n\nThe experiments are conducted on LLaMa models with sizes from 7B to 65B parameters on diverse generative tasks. Results show FastGen effectively compresses the KV cache to 40-50% smaller with negligible quality loss. It also outperforms non-adaptive baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Strengths:\n-----------------\n+ Adaptively compressing KV cache better aligns with model-specific attributes without retraining.\n+ Comprehensive experiments verify FastGen works for diverse models and tasks. Up to 50% compression on 65B LLaMa with little quality loss is remarkable.\n+ Ablation studies provide good insight into the design choices. The profiling method and compression policies are well motivated."
                },
                "weaknesses": {
                    "value": "Weaknesses:\n-----------------\n- The compression policies are combined in a naive way. More advanced adaptive selection could be explored (see detailed in C1).\n- No experiment on encoder-decoder models. The efficacy on them is unclear (see detailed in C2). \n- More analysis on the overhead of profiling could be provided (see detailed in C3)."
                },
                "questions": {
                    "value": "Comments:\n-----------------\nC1:\tThe compression policies are combined in a simple naive way in FastGen, by just taking the union of multiple policies such as Cspecial + Cpunct + Cfrequent. This straightforward combination approach has several potential issues. First, is it possible that the union combination may introduce redundancy, as different policies could select overlapping important content, leading to suboptimal compression ratios? More intelligent strategies should consider the complementarity between modules to avoid duplicating the key contexts. Second, is it possible that existing policies may not be fully compatible? Some combinations could introduce conflicts and hurt generation quality. More systematic analysis should examine the compatibility between policies.\n\nC2:\tThe experiments in the paper are all conducted on the decoder-only LLaMa models, without validation on encoder-decoder models like BART and T5. These models are also widely used for generative tasks, so the efficacy of FastGen on them remains unclear. This is worth further investigation. \n\nC3:\tThe paper lacks sufficient analysis on the overhead and time cost of conducting attention profiling, which is important to judge the efficiency of FastGen in real deployment. Specifically, the time complexity of attention profiling needs analysis, and concrete profiler time under different model sizes should be provided or disscussed. Moreover, analyzing the extra memory or GPU memory required for the profiler and assessing its impact on deployment is necessary. In summary, quantitatively analyzing the resource overhead for profiling and demonstrating effective solutions to reduce it could strengthen the practicality of FastGen in real-world usage. Further experiments on optimized profiling and its cost-benefit trade-off with compression performance could provide more comprehensive insights into the efficacy of the approach."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6547/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698662301479,
            "cdate": 1698662301479,
            "tmdate": 1699636738293,
            "mdate": 1699636738293,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4h3OQgOuza",
                "forum": "uNrFpDPMyo",
                "replyto": "aNqNdIZp5T",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6547/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6547/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**W1: The compression policies are combined in a simple naive way in FastGen. This straightforward combination approach has several potential issues. First, is it possible that the union combination may introduce redundancy, as different policies could select overlapping important content? Second, is it possible that existing policies may not be fully compatible? Some combinations could introduce conflicts and hurt generation quality.**\n\nA1: We agree that different compression strategies have overlapping tokens. For example, C_frequent is sometimes overlapped with C_special and C_local, as special tokens and local contexts usually accumulate higher attention scores. However, FastGen is designed to progressively evict tokens. When there is an overlapped policy, we only consider the complementary tokens it contains. It is guaranteed that unioning the policy monotonously will only bring in newly evicted tokens.\nWe further confirm that different policies are compatible in the ablation study (section 5.3). We study (1) how removing one strategy and (2) how changing relative policy order affects the overall performance.\n\nWe can draw an empirical conclusion that little-to-no conflicts exist between different policies. In fact, we observe complementary effects between existing strategies. That is to say, the performance of standalone strategies can usually be boosted significantly by introducing other strategies when necessary.\n\nSome combinations can introduce inferior/superior performance than others. In our experiment, over all datasets, we find that following the order of  C_special, C_punct, C_frequen, C_local consistently gets the best performance. We leave investigating the intrinsic mechanism to future work.\n\n**W2: The experiments in the paper are all conducted on the decoder-only LLaMa models, without validation on encoder-decoder models like BART and T5.**\n\nA2: Thanks for the suggestion, FastGen could be easily adapted to encoder-decoder models by pruning the KV cache in their decoder. We will elaborate on this in the introduction and add several related works in the future version. In encoder-decoder models, KV cache is still used in the decoder to save computation. During generation, it is a standard practice to fix the encoder inputs as existing prompts or instructions, e.g., as in BART [1] and FlanT5 [2]. In such scenarios, the decoder part still works autoregressively to output newly generated tokens, and it is flexible to directly apply FastGen to their KV cache. In this paper, we focus on decoder-only models as most of the prevailing LLMs are decoder-only models, e.g., LLaMa, OPT, and GPT. We agree that additional discussions and experiments should be added, and we will revise accordingly.\n\n**W3: The paper lacks sufficient analysis on the overhead and time cost of conducting attention profiling, which is important to judge the efficiency of FastGen in real deployment.**\n\nA3: Thanks for the valuable advice. We provide extra experimental results on book-keeping overhead in Table 2. Please refer to the general response #2 for a detailed time and memory analysis of profiling cost. \n\nIn short, Table 2 shows the profiling time of the LLaMA65b in different generation length settings. We can observe that the profiling time only accounts for a very small percentage of the total generation duration, up to 0.35% in our tested cases. Also, the overhead decreases as the generation length increases, dropping to 0.07% when the generation length comes to 1024.\n\nIn terms of extra memory usage, it\u2019s mainly introduced by one of the compression strategies, C_frequent, which needs to store an extra cumulative sum of attention scores for each attention head.  To provide a detailed analysis, for each layer, the dimension of the KV cache is (batch_size, num_of_head, sequence_len, hidden_dimension), while the dimension of extra memory for the cumulative attention scores is (batch_size, num_of_head, sequence_len). Considering hidden_dimension=128 for all model sizes, the memory overhead is 1/128=0.78% compared to storing KV cache only, which is a negligible cost.\n\nIn conclusion, the overhead introduced by the profiling step is nearly negligible in both time and memory, which confirms Fastgen\u2019s potential for real-world deployment. We additionally provide end-to-end system latency improvement in Table 1. It shows that FastGen can achieve major speed-up in various generation settings. Please refer to General Response #1 for more analysis.\n\n\n\n**Reference:**\n\n[1] Lewis, Mike, et al. \"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension.\" Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 2020.\n\n[2] Chung, HyungWon et al. \"Scaling Instruction-Finetuned Language Models.\" arXiv preprint arXiv:2210.11416(2022)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6547/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700720442307,
                "cdate": 1700720442307,
                "tmdate": 1700722257166,
                "mdate": 1700722257166,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TKcKski2aK",
            "forum": "uNrFpDPMyo",
            "replyto": "uNrFpDPMyo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6547/Reviewer_BiHE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6547/Reviewer_BiHE"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses the memory footprint reduction of LLMs during inference, in which the recent problem is the KV cache eviction/compression policies. The paper proposes an adaptive KV cache compression technique that operates in two stages, i) diagnose through profiling based on the attention heads and ii) applying an eviction strategy per each layer."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- Having an adaptive KV cache for each of the attention module type is a really interesting and exciting idea.\n- No fine-tuning costs of the proposed method is commendable. \n- The paper clearly positions within the body of existing literature, by distinguishing the proposed method as an adaptive and a diverse set of eviction strategies.\n- The paper is clearly written, the presentation is great, easy to follow along and digest the concepts."
                },
                "weaknesses": {
                    "value": "- Although, the idea of adaptive KV cache compression sounds interesting, what is the overhead of book-keeping to support this adaptive and diverse ability based on the type of the attention? This is not discussed anywhere in the paper?\n  - That is, each layer id will be mapped to a eviction policy and is deployed with the model at hand. \n  - Next, what is the added computational complexity both asymptotically as well experimentally.\n- Table 3 shows an ablation on the policy order, why is this needed? Is the policy fixed per layer and the order will be dictated by the layer that needs a certain policy determined by the diagnosis step. Is it not true, clarify on this please.\n- Another interesting exploration/ablation to see is to experiment with long context tasks. What if the downstream task requires a long context window then what can be the best set of eviction strategies and the corresponding expected win rates?\n### Minor comments:\n- \"The resulting distribution is visualized in Figure As in Figure 3.\" can be rewritten as \" Figure 3 shows the resulting distribution\"\n- A minor nit, the paper has too much forward referencing, which disturbs the flow of reading and attention, general recommendation in research papers is to avoid such referencing..!\n- Better to define the new terms such as win-rate, KV cache budget, etc. when they were introduced for the first time. Similar applies to abbreviations when they are introduced first time, expand them, for the sack of saving readers time to search internet."
                },
                "questions": {
                    "value": "Please refer to weaknesses section for questions.\n\n## Post rebuttal comments\n\nThe responses and the detailed analysis in the Tables1,2 address my concerns.\n\nHowever the authors seem to reserve one of the suggestions to the future works. Overall, very satisfied with the impressive work in the paper and raising the score to clear accept."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6547/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6547/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6547/Reviewer_BiHE"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6547/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698705269922,
            "cdate": 1698705269922,
            "tmdate": 1700724190291,
            "mdate": 1700724190291,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WzKkjfkpgP",
                "forum": "uNrFpDPMyo",
                "replyto": "TKcKski2aK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6547/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6547/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer BiHE"
                    },
                    "comment": {
                        "value": "**W1: What is the overhead of book-keeping to support this adaptive and diverse ability based on the type of the attention? What is the added computational complexity both asymptotically as well experimentally?**\n\nA1: Thanks for the valuable advice. We provide extra experimental results on book-keeping overhead in **Table 2 of General Response**. Please refer to the second part of General Response for a detailed time and memory analysis of profiling cost. \n\nIn short, Table 2 shows the profiling time of the LLaMA65b in different generation length settings. We can observe that the profiling time only accounts for a very small percentage of the total generation duration, up to 0.35% in our tested cases. Also, the overhead decreases as the generation length increases, dropping to 0.07% when the generation length comes to 1024.\n\nIn terms of extra memory usage, it\u2019s mainly introduced by one of the compression strategies, C_frequent, which needs to store an extra cumulative sum of attention scores for each attention head.  To provide a detailed analysis, for each layer, the dimension of the KV cache is (batch_size, num_of_head, sequence_len, hidden_dimension), while the dimension of extra memory for the cumulative attention scores is (batch_size, num_of_head, sequence_len). Considering hidden_dimension=128 for all model sizes, the memory overhead is 1/128=0.78% compared to storing KV cache only, which is a negligible cost.\n\nIn conclusion, the overhead introduced by the profiling step is nearly negligible in both time and memory, which confirms Fastgen\u2019s potential for real-world deployment. We additionally provide end-to-end system latency improvement in Table 1. It shows that FastGen can achieve major speed-up in various generation settings. Please refer to the first part of General Response for more analysis.\n\n**W2: Table 3 shows an ablation on the policy order, why is this needed?  Is the policy fixed per layer and determined by the diagnosis step?**\n\nA2: The policy is determined in the diagnosis step, and it is fixed per head in each layer. As introduced in section 3.4 \u201cHybrid Policies\u201d, we search for the optimal hybrid policy according to a predefined order. The order is greedily designed to prioritize cache policy with smaller memory costs, e.g., C_special. Once the optimal policy is determined, it will stay fixed in the generation process.\n\nIn Table 3, the order ablation study aims to show that FastGen is agnostic to small changes in searching order. By shuffling the relative order of C_punct and C_local, we observe a different trade-off between KV cache compression and generation quality. Overall, our current order (as in Equation 2) achieves the highest win-rates.\n\n**W3: Another interesting exploration is long context tasks. In long context tasks, what can be the best set of eviction strategies and the corresponding expected win rates?**\n\nA3: Thanks for the suggestion, it points out a very promising area of extension for FastGen. Recent work such as StreamingLLM [1] and LM-Infinite [2] show that on long context tasks, preserving KV cache for special token and local contexts (C_special+C_local in our setting) could extend LLaMa-2 to 32k context length without significantly sacrificing performances. Although there is no clear experimental evidence on the optimal eviction ratio and corresponding win rates, their findings indicate that KV cache pruning could extend an LLM context length on-the-fly. It would be interesting to try out FastGen-style adaptive pruning on long context scenarios and measure performance and pruning ratio tradeoffs.\n\n[1] Xiao, Guangxuan, et al. \"Efficient streaming language models with attention sinks.\" arXiv preprint arXiv:2309.17453 (2023)\n.\n[2] Han, Chi, et al. \"Lm-infinite: Simple on-the-fly length generalization for large language models.\" arXiv preprint arXiv:2308.16137 (2023).\n\nWe really appreciate the reviewer\u2019s suggestions on our writing. We will reduce the forward referencing and improve the term definition in the future version."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6547/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700722568983,
                "cdate": 1700722568983,
                "tmdate": 1700722568983,
                "mdate": 1700722568983,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0sTjpbUMAn",
                "forum": "uNrFpDPMyo",
                "replyto": "WzKkjfkpgP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6547/Reviewer_BiHE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6547/Reviewer_BiHE"
                ],
                "content": {
                    "comment": {
                        "value": "Acknowledge the response from authors."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6547/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700724277713,
                "cdate": 1700724277713,
                "tmdate": 1700724277713,
                "mdate": 1700724277713,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lfT0rmabIS",
            "forum": "uNrFpDPMyo",
            "replyto": "uNrFpDPMyo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6547/Reviewer_XU71"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6547/Reviewer_XU71"
            ],
            "content": {
                "summary": {
                    "value": "This paper discussed how to apply adaptive KV cache compression to improve the system efficiency, which conducts profiling to discern the intrinsic structure of attention modules. The proposed method can be deployed without resource-intensive fine-tuning or re-training. Solid empirical study was conducted to verify the efficiency and effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper solves a critical research problem about efficient LLM inference with advanced algorithm design. The designed algorithm is straightforward and effective. \n\n- The presentation of the technical discussion is accurate and well-organized.\n\n- The organization of the evaluation sections is clear, and the presented results show the advance and efficiency of the proposed method."
                },
                "weaknesses": {
                    "value": "- Based on my understanding, the proposed algorithm specializes in the most classic softmax-based attention. Is it possible to include a small section discussing the limitations of the proposed algorithm for more complicated attention mechanisms and some preliminary ideas about supporting those mechanisms in the future?\n\n- Given the scale of the benchmarked model (llama-70B fp16 on A100-80G), I guess there is a missing detail about the parallel strategies applied in the experiments."
                },
                "questions": {
                    "value": "Would it be possible to address the minor issues I listed in the weakness section?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6547/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698806014029,
            "cdate": 1698806014029,
            "tmdate": 1699636738031,
            "mdate": 1699636738031,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mesQ9Z9IxJ",
                "forum": "uNrFpDPMyo",
                "replyto": "lfT0rmabIS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6547/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6547/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "W1: The proposed algorithm specializes in classic softmax-based attention. Is it possible to include a discussion on more complicated attention mechanisms and some preliminary ideas about supporting those mechanisms?\n\nR1: Thanks for the suggestion. We are working on extending FastGen to other attention variations. One direct application is grouped query attention (GQA). In GQA, heads within each group share the same KV vectors. Instead of head-wise pruning, we could modify FastGen to perform group-wise pruning. Specifically, we could individually evaluate each query by calculating the recovery ratio of its attention map (Q*K), and then average all ratios within the same group, using the averaged ratio as the criteria to find the optimal strategy.\nWe are not quite sure what specific \"more complicated attention mechanisms\" the reviewer has in mind, so it would be great if the reviewer could provide more references to them. We would be happy to include a discussion of these in the future version.\n\nW2: Given the scale of the benchmarked model (llama-70B fp16 on A100-80G), I guess there is a missing detail about the parallel strategies applied in the experiments.\n\nR2: We will add more details about our implementation in the future version. During inference, we perform model parallel by equally sharding the model weights to different GPUs within the same node. Attention heads are evenly distributed across GPU for parallel attention computation. During inference, the model_parallel_size is 8 for 70B, 4 for 30B, 2 for 13B, and 1 for 7B. During finetuning, we use 32 A-100 80G GPUs with model_parallel_size=4, data_parallel_size=8,  and batch_size_per_GPU=4 for all model sizes."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6547/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700719961763,
                "cdate": 1700719961763,
                "tmdate": 1700719961763,
                "mdate": 1700719961763,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "w8AOju6vTN",
            "forum": "uNrFpDPMyo",
            "replyto": "uNrFpDPMyo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6547/Reviewer_XPHq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6547/Reviewer_XPHq"
            ],
            "content": {
                "summary": {
                    "value": "This study introduces a lossy adaptive KV cache compression technique aimed at reducing the memory footprint of LLMs. The paper is guided by two key insights:\n\nDifferent attention heads typically exhibit distinct structures.\nThese attention head structures remain relatively consistent during inference.\nThe paper profiles the prompt encoding phase to identify the intrinsic structures of various attention heads and uses these structures to determine the optimal compression policy. This policy, determined during the prompt encoding phase, is then applied uniformly throughout all token generation iterations. The compression policies are combinations of the following four basic ones: special, punct, local, and frequent.\n\nThe results demonstrate that this approach yields improved model quality compared to fixed KV compression methods, with KV cache budgets ranging from 30% to 100%. The ablation study further reveals that frequency- and special-token-based compression policies have the most significant impact on compression ratio and win rate."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper introduces valuable insights drawn from LLMs: 1. Different structure in different attention 2. The same head structures persist. These insights are well-supported with empirical data and references to existing literature. \n\n- The authors leverage these insights to come up with an effective compression method that adapts to the structure of each attention head. The results show consistent compression rate and model quality improvement over prior SoTA fixed compression mechanisms."
                },
                "weaknesses": {
                    "value": "- The paper could benefit from presenting actual GPU inference performance results using FastGen and comparing them with other compression methods. Additionally, providing a runtime breakdown would offer more insights into the overhead caused by the profiling, compression, and decompression processes.\n- It would be nice to look into the structure of KV in the multi-query attention design."
                },
                "questions": {
                    "value": "-"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6547/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6547/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6547/Reviewer_XPHq"
                    ]
                }
            },
            "number": 6,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6547/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698832793525,
            "cdate": 1698832793525,
            "tmdate": 1699636737901,
            "mdate": 1699636737901,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CyFshXOXz5",
                "forum": "uNrFpDPMyo",
                "replyto": "w8AOju6vTN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6547/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6547/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer XPHq"
                    },
                    "comment": {
                        "value": "**W1: The paper could benefit from presenting actual GPU inference performance and comparing them with other compression methods.**\n\nR1: Thanks for the valuable advice. We provide extra experimental results in Tables 1 and 2 of **General Response** section. Please refer to the general response for a detailed analysis of actual end-to-end latency and profiling cost. \n\nIn short, we can observe from Table 1 that FastGen achieves significant end-to-end speed-up across all the generation settings. For example, given batch_size = 1, FastGen\u2019s relative speed-up rises from 16.04% to 55.0%, as the generation length grows from 512 to 16k. The phenomenon can also be observed in other batch settings. This analysis confirms that FastGen can achieve major speed-up in real development, especially in long-generation settings. \n\nAdditionally, Table 2 shows the profiling time of the LLaMA65b in different generation length settings. We can observe that the profiling time only accounts for a very small percentage of the total generation duration, up to 0.35% in our tested cases. Also, the overhead decreases as the generation length increases, dropping to 0.07% when the generation length comes to 1024. \n\nIn conclusion, the overhead introduced by the profiling step is nearly negligible, which confirms FastGen\u2019s potential for real-world deployment.\n\n**W2: It would be nice to look into the structure of KV in the multi-query attention**\n\nR2: Thanks for the suggestions. Since multi-query attention essentially eliminates all KV heads to one, it already eliminates the memory cost and inference time to a large extent, leaving the benefit of pruning the KV cache marginal. Moreover, it could be non-trivial to find a universal pruning strategy for all query heads so we leave the exploration for future work. However, we agree that it would be interesting to look into this setting."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6547/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700721038355,
                "cdate": 1700721038355,
                "tmdate": 1700722339039,
                "mdate": 1700722339039,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]