[
    {
        "title": "IGTO: Individual Global Transform Optimization for Multi-Agent Reinforcement Learning"
    },
    {
        "review": {
            "id": "i1NUMfZJp8",
            "forum": "QGk2UhrC0Z",
            "replyto": "QGk2UhrC0Z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3268/Reviewer_Kp6o"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3268/Reviewer_Kp6o"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces the Individual-Global Transform-Optimal (IGTO) condition, allowing for inconsistent individual-global actions while ensuring equivalent policy distributions. The authors also develop the Individual-Global Normalized Transformation (IGNT) rule, which can be integrated into existing CTDE-based algorithms and helps achieve optimum convergence of individual-global policies. Extensive experiments demonstrate the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1.\tThis paper proposes the IGTO condition, which alleviates the limitations of the IGM condition.\n2.\tThe paper presents a novel method for obtaining individual transformed policy improvement through global joint policy optimization in the centralized training procedure.\n3.\tThe methods are designed with theoretical guarantees, and the experiments provide evidence of their effectiveness."
                },
                "weaknesses": {
                    "value": "Overall, this is a good paper and I have not found significant weakness yet.\nI would lean towards accepting this paper. Meanwhile, I will also pay attention to the opinions and discussions of other reviewers.\n\nThere are some typos such as Eq.14.\nSome symbols in the definitions lack explanations, making it difficult to understand such as Eq.5."
                },
                "questions": {
                    "value": "1.\tIGTO is based on Theorem 1 where the transformation is performed sequentially. Whether the order will influence the final results?\n2.\tFrom Eq.13, the joint policy is restricted to some set of intractable policies. Whether this global operation may lead to the suboptimal results of individual policies.\n3.\tHow the method will perform in more complex scenarios in SC2 such as corridor, 6h_vs_8z?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3268/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3268/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3268/Reviewer_Kp6o"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3268/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698325428358,
            "cdate": 1698325428358,
            "tmdate": 1699636275373,
            "mdate": 1699636275373,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "eCCiBm2Uxo",
            "forum": "QGk2UhrC0Z",
            "replyto": "QGk2UhrC0Z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3268/Reviewer_gq26"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3268/Reviewer_gq26"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a new condition called Individual-Global Transform-Optimal (IGTO) to allow inconsistent individual-global actions while ensuring the equivalency of their policy distributions in Multi-Agent Reinforcement Learning (MARL). The authors propose a rule called Individual-Global Normalized Transformation (IGNT) to satisfy the IGTO constraint and integrate it into existing MARL algorithms. Theoretical proofs show that individual-global policies can converge to the optimum under the IGNT rule. The authors demonstrate the proposed method effectiveness through experiments on StarCraft Multi-Agent Challenge (SMAC) and Multi-Agent Particle Environment (MPE)."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Proposes the Individual-Global Normalized Transformation (IGNT) rule, which can be seamlessly integrated into existing MARL algorithms, offering a practical solution to satisfy the IGTO condition\n\n- Providing detailed explanations and proofs in the appendix, enhancing the transparency and reproducibility of the proposed approach"
                },
                "weaknesses": {
                    "value": "- This work is less novelty, by adding the normalized transformation to FOP. The majority of the proofs are similar to FOP.\n- IGNT is more suitable for policy-based methods. However, chosen baselines are almost value-based methods. Especially, MADDPG in MPE is ignored.\n- Lack of motivation about why we need the normalized transformation, especially if the transformation is invertible. \n- Lack of ablation about the chosen transformation function. For example, an Identity matrix, which $|G_i| = 1$ can be chosen, so that after transformation, actions are the same."
                },
                "questions": {
                    "value": "- Could you provide an example of the normalized transformation? Especially, since the action is discrete in SMAC, u_i is an index of one action.\n- It is unclear to me about how to adapt IGNT to value-based decomposition MARL methods, e.g., QMIX. If incorporating individual policy networks and target policy networks for each agent into QMIX, the ablation study should include such modifications.\n- Why FOP cannot learn well in 3s_vs_5z? FOP can reach around 70% win-rate in their original paper in MMM2."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics concern."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3268/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698836670865,
            "cdate": 1698836670865,
            "tmdate": 1699636275287,
            "mdate": 1699636275287,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "pevTX91oui",
            "forum": "QGk2UhrC0Z",
            "replyto": "QGk2UhrC0Z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3268/Reviewer_s6zd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3268/Reviewer_s6zd"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents an individual-global action-transformed condition named IGTO, which releases the IGM or IGO restriction. To achieve this, it designs a bijective function for each agent to convert actions and proves that the converted actions can converge to the optimal policy. This method can be seamlessly implanted into many CTDE-based algorithms."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper attempts to find a solution to the optimal action of multi-agent from a new perspective, which is interesting.\n\n2. IGNT is suitable for discrete and continuous actions and can be used in many agent reinforcement learning methods."
                },
                "weaknesses": {
                    "value": "1. The motivation of this work is not clearly presented. Why do we need transformed actions? What are the benefits brought by this new method?\n\n2. The experiments of this method are not convincing. More IGM methods such as QPLEX [ICLR 21], WQMIX [NeurIPS 21], ResQ [NeurIPS 22], etc. could be added to prove the effect of this work.\n\n3. This paper provides a proof of improvement for the policy-based method, however, the proof of the value decomposition method is not yet clear."
                },
                "questions": {
                    "value": "1.  IGM or IGO strictly requires the optimal joint actions to be consistent with the optimal individual behaviors, which may lead to unsatisfied performance in some complicated environments. Could you describe a scenario where the IGM or IGO conditions could lead to poor performance? \n\n2. A closely related work is missing. ResQ [1] is a decomposition-based MARL method which learn a value decomposition by using a nonlinear function. \n\n3. For the discrete or continuous action space of an agent, how to ensure that all actions converted by the bijective function f_i are valid, and they should include all actions in the raw action space. It is unclear to me how it ensures this condition.\n\n4. In value-based decomposition MARL methods\uff0cDo $Q_{jt}^* (\\tilde{u} |\\tau)$ and $[Q_i^* ( \\tilde{u}_i|\\tau)]$ satisfy IGM condition ? Can the author provide the complete training procedure based on value decomposition such as QMIX.\n\nReferences\n\n[1] ResQ: A Residual Q Function-based Approach for Multi-Agent Reinforcement Learning Value Factorization, NeurIPS 2022"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3268/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3268/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3268/Reviewer_s6zd"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3268/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698911490271,
            "cdate": 1698911490271,
            "tmdate": 1699636275228,
            "mdate": 1699636275228,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "RuGw6E5QQF",
            "forum": "QGk2UhrC0Z",
            "replyto": "QGk2UhrC0Z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3268/Reviewer_ujuj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3268/Reviewer_ujuj"
            ],
            "content": {
                "summary": {
                    "value": "The authors try to release the restriction that the optimal joint actions should\nbe consistent with the optimal individual behaviors. They propose the Individual-Global-Transform-Optimal condition, IGTO, which transforms the joint action via a bijection function. The authors claim that individual-global policies can converge to the optimum under this condition. The proposed method is easy to implement and can be integrated into many existing MARL methods seamlessly."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ The paper is well-organized.\n+ The experiments are extensive. IGNT achieves strong performance gains in many tasks compared with the backbone algorithm. The experiment settings and hyper-parameters are detailed. I think it is easy to reproduce the results."
                },
                "weaknesses": {
                    "value": "First, is it really a restriction that the optimal joint actions should\nbe consistent with the optimal individual behaviors? I think it is a natural fact and can be satisfied in all environments. Can you propose a case (maybe matrix games) where the optimal joint action is inconsistent with the optimal individual actions, and perform experiments on it to show that your method can achieve the optimum as claimed?\n\nThe condition that is Jacobian determinant is 1 is derived by the change of variable formula. However, the formula requires that the Jacobian exists. In discrete action space, the function is not differentiable. Can you give us a numerical case with discrete action space to show the original actions, the transformed actions, the bijection function, and the Jacobian determinant? It would be helpful to understand your algorithm.\n\nMoreover, without considering the Jacobian determinant, the function F is bijection. Does it really increase the representation abilities or release the restriction of individual actions? I cannot see the meaning of transforming the actions using a bijection function.\n\nConsidering the strong performance of IGNT, I would like to increase my score if the questions are addressed."
                },
                "questions": {
                    "value": "See Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3268/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699173763361,
            "cdate": 1699173763361,
            "tmdate": 1699636275148,
            "mdate": 1699636275148,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Lgly7wjRC9",
                "forum": "QGk2UhrC0Z",
                "replyto": "RuGw6E5QQF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3268/Reviewer_ujuj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3268/Reviewer_ujuj"
                ],
                "content": {
                    "comment": {
                        "value": "I see that in implementation the transformation function is preformed on the action probabilities. But in proof, u is a joint action, not action probability. So the proof seems to be wrong. Can you clarify the mismatch between proof and implementation?"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3268/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700644473160,
                "cdate": 1700644473160,
                "tmdate": 1700656705855,
                "mdate": 1700656705855,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KLIzNC192d",
                "forum": "QGk2UhrC0Z",
                "replyto": "RuGw6E5QQF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3268/Reviewer_ujuj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3268/Reviewer_ujuj"
                ],
                "content": {
                    "comment": {
                        "value": "The authors seem to think I am unfamiliar with deep RL.\n\nMy concern is the theoretical analysis, which is independent from implementation. First, **Theorem 1 requires the Jacobian of discrete actions, so the Theorem is wrong.** Second, if the Theorem holds when replacing actions with action probabilities, you should re-write the proof because the proof is not trivial. Does Theorem 1 hold if u is action probability?"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3268/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700655929705,
                "cdate": 1700655929705,
                "tmdate": 1700656681836,
                "mdate": 1700656681836,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Q2tFwU8Mat",
                "forum": "QGk2UhrC0Z",
                "replyto": "RuGw6E5QQF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3268/Reviewer_ujuj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3268/Reviewer_ujuj"
                ],
                "content": {
                    "comment": {
                        "value": "The gradient of $y$ w.r.t the input action vector $x$ is ill-defined if $x$ is discrete, whether it is represented with vectors or not. \n\nFor example, $x \\in [0,1,2,3], f = x^2$, $f'(2)$ does not exist. $x \\in [(0,0,0,1),(0,0,1,0),(0,1,0,0),(1,0,0,0)], f = xx^T$, $f'((0,1,0,0))$ does not exist.\n\nLet us check where the Jacobian comes from. It is derived from the lines below Eq. 30, the change of variable formula. However, this technique can only be applied in continues variable. In discrete variable, the Jacobian cannot be defined. And we should use $\\sum$ to deal with the change of discrete variable."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3268/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700663838861,
                "cdate": 1700663838861,
                "tmdate": 1700664121009,
                "mdate": 1700664121009,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IFJWTVenBQ",
            "forum": "QGk2UhrC0Z",
            "replyto": "QGk2UhrC0Z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3268/Reviewer_H7GA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3268/Reviewer_H7GA"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an Individual-Global Normalized Transformation (IGNT) rule that maps a sample from a simple density i.e., Gaussian policy. In this way, the restriction of IGO is released and the global value function is able to represent complex situations. Experimental results show that the proposed method outperforms many state of the art baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "See questions"
                },
                "weaknesses": {
                    "value": "See questions"
                },
                "questions": {
                    "value": "This paper provides a simple but powerful method to release the restriction of IGO. By mapping the action to a distribution, the global value network can perform much better than multiple baselines. Theoretical analysis is also sound and provides the guarantee of convergence. The paper is interesting and easy to follow. However, I still have some concerns:\n\n1. The proposed method is similar to the normalization of actions, which is widely used in many methods as a trick to improve performance. What is the difference between the proposed method and the widely-used normalization?\n\n2. In the paper, the soft objective function is adopted. It seems that the proposed method can also apply to the normal objective function. Could authors explain why they must be combined? Since there is no ablation study to show the performance without the soft objective function, it is hard to say why the performance improved.\n\n\ntypos: a Individual-Global Normalized Transformation (IGNT) rule that map ..... -> an .... that maps .....;"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3268/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699281465664,
            "cdate": 1699281465664,
            "tmdate": 1699636275088,
            "mdate": 1699636275088,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]