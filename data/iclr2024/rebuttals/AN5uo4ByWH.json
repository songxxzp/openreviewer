[
    {
        "title": "Curve Your Attention: Mixed-Curvature Transformers for Graph Representation Learning"
    },
    {
        "review": {
            "id": "NarxU6hGvB",
            "forum": "AN5uo4ByWH",
            "replyto": "AN5uo4ByWH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5253/Reviewer_g83M"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5253/Reviewer_g83M"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a Product-Stereographic Transformer, a generalization of Transformers towards operating on the product of constant curvature spaces. The work also provides a kernelized approach to non-Euclidean attention for further efficiency. The authors perform various experiments on graph reconstruction and node classification to demonstrate their model performance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The paper is overall well-organized and the writing is easy to follow. \n\n2. The authors evaluate on many datasets and models to showcase their model performance."
                },
                "weaknesses": {
                    "value": "1. In the Related Work section, the authors are missing reference to a fundamental recent paper learning jointly on both hyperbolic and spherical spaces in a unified end-to-end pipeline: \nKDD 2022: Iyer et al. 2022. Dual-Geometric Space Embedding Model for Two-View Knowledge Graphs. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '22). Association for Computing Machinery, New York, NY, USA, 676\u2013686. https://doi.org/10.1145/3534678.3539350\n\n2. In the Preliminaries, the authors should also provide formulas for the retraction operators including interpretation of the exponential mapping and log mapping operations e.g., why the Euclidean tangent space is needed etc. \n\n3. In the Experiments section, it seems strange that the authors are utilizing synthetic geometric graph datasets, when their motivation was that hierarchies and cycles are commonly found in real-world datasets. As such, the motivation behind using Table 1 needs to be defined better. \n\n4. The novelty of this work is also lacking. Chami et. al, already propose the using of retraction operators (exponential and log mapping that can be generalized to any non-Euclidean geometric space of constant curvature K. Further, product spaces have also already been proposed. Moreover, even integrating both hyperbolic and spherical spaces jointly has been proposed by Iyer et. al. This work just seems to be a combination of all of the above works."
                },
                "questions": {
                    "value": "The design choice behind why the authors are using a product-stereographic space to model various topologies of the data is also unclear. Why not consider embeddings different topologies in different spaces instead of just considering one product space? It seems to me that this direction has not even been explored."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5253/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698716580792,
            "cdate": 1698716580792,
            "tmdate": 1699636524308,
            "mdate": 1699636524308,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "3JiTjylJA5",
            "forum": "AN5uo4ByWH",
            "replyto": "AN5uo4ByWH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5253/Reviewer_c5LD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5253/Reviewer_c5LD"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a  for generalizing Transformer architectures to operate on non-Euclidean spaces. The main contributions are:\n\n- Proposes FPS-T, which generalizes Transformers to operate on the product-stereographic model. This allows each layer to learn curvature values for different attention heads.\n\n- Applies FPS-T to graph representation learning by integrating it with the Tokenized Graph Transformer. It uses a kernelized approximation to reduce computational complexity.\n\n- Evaluates FPS-T on synthetic and real-world graph reconstruction and node classification tasks. Finds it can learn suitable curvatures and outperform Euclidean Transformers."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The research proposes an exciting and novel extension of Transformers to non-Euclidean geometry, which has not been explored before. Generalizing attention mechanisms to curved spaces is a contribution."
                },
                "weaknesses": {
                    "value": "- The motivation for non-Euclidean Transformers is not fully justified. The introduction claims they are necessary for modeling hierarchical and cyclic graphs but does not provide compelling evidence current Euclidean Transformers fail on such structures. More analysis on the limitations of existing methods is needed.\n\n1. The stereographic model limits flexibility compared to input-dependent curvatures. Methods like heterogeneous manifolds (cited in the paper) can adapt curvature per node/edge based on features. Fixing curvature by attention head may be too restrictive. The paper could experiment with more adaptive curvature mechanisms.\n\n2. Scalability is a concern. The largest graph evaluated has only 4,000 nodes and 88k edges. More experimentation on large real-world networks is important to demonstrate practical value. \n\n3. No transformer-based baselines are compared.\n\n4. Ablation studies could provide more insight. For example, how do learned curvatures evolve during training? How sensitive is performance to curvature initialization? Are some attention heads more \"non-Euclidean\" than others?"
                },
                "questions": {
                    "value": "You claim Euclidean Transformers are inadequate for modeling hierarchical and cyclic graphs. However, recent works like Graphormer show strong performance on tasks like molecular property prediction that involve such structures. Can you provide more concrete evidence on the limitations of existing methods? Comparisons to recent graph Transformers on suitable benchmarks would help make this case.\n\n\nHave you experimented with more adaptive, input-dependent curvature mechanisms? Fixing curvature by attention head seems restrictive. Heterogeneous manifolds allow varying curvature per node/edge. How do learned curvatures in FPS-T compare to feature-based curvature?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5253/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698783510455,
            "cdate": 1698783510455,
            "tmdate": 1699636524216,
            "mdate": 1699636524216,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "DLWxp69ZjP",
            "forum": "AN5uo4ByWH",
            "replyto": "AN5uo4ByWH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5253/Reviewer_Wzei"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5253/Reviewer_Wzei"
            ],
            "content": {
                "summary": {
                    "value": "The authors present a new global attention-based graph Transformers that operates entirely on the product of spaces with constant curvature, relying on stereographic product models. Building on these global attention mechanisms, the authors extend tokenized graph transformers from a Euclidean framework (TokenGT) to a non-Euclidean framework (FPS-T). They further consider the approximation of pairwise products in attention layers using a popular linear approximation technique that mimics feature maps, so that FPS-T becomes linear in the number of nodes and edges instead of quadratic. They then compare these two transformers on graph reconstruction tasks considering synthetic and real datasets. Finally, they evaluate FPS-T on 8 well-known node classification tasks, including homophilic and heterophilic graphs. FPS-T outperforms the compared methods on both task types."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "-\tOverall the paper is well-written.\n-\tThe proposed transformer is new and relevant\n-\tSynthetic graph reconstruction experiments on specific geometry are relevant and show that FPS-T outperforms its Euclidean counterpart Token-GT.\n-\tFPS-T is shown to outperform several methods on the reconstruction of real networks, especially when graphs have significantly negative sectional curvatures e.g lines, cycles and trees.\n-\tFPS-T outperforms benchmarked methods on 6 out of 8 node classification tasks, especially in heterophilic settings."
                },
                "weaknesses": {
                    "value": "-\t1. **Rather incremental** The design of FPS-T is clearly an adaptation of the work of (Bachmann & al, 2020) to transformer-like architectures.\n-\t2. **No theoretical insights** There are no significant theoretical analysis that would provide insights on specific features of FPS-T: e.g when graphs whose sectional curvature distribution has a large variance what would be a good constant curvature space (or product of spaces) to discriminate them ?\n-\t3. **Few unclear parts remain**:\n\t - i) I think that equation 4 is wrong: expressions of Q and K seem wrong as a such linear transformation will not embed a stereographic embedding in the tangent space in V. are there missing log_V mappings ?\n         - ii) Equation 6 is not clearly explained. Moreover a conformal factor of 1 seems to lead to a ill-defined aggregation function, this point should be clarified by authors.\n         - iii) Could you further characterize the resulting function $exp_0( f_{act} (log_0))$ when $f_{act}$ is not differentiable everywhere as the ReLU activation function ?\n         - iv) Section 4.5 is not totally clear: I believe that the \u2018kernelization\u2019 aims at approximating the softmax activation $\\sigma(< Q_i, K_j>)$ not just the inner product $< Q_i, K_j>$ otherwise I do not see the point. Moreover no context is provided w.r.t the current SOTA to reduce the computational cost of transformers.\n-\t4. **Incomplete experiments and analysis**:\n        - i) Under exploited synthetic experiments: could you provide curvatures for the designed graphs so that we can quantify their correspondences to estimated constant curvatures in FPS-T? Could you perform a sensitivity analysis w.r.t the embedding dimensions and the number of considered heads for both FPS-T and Token_GT?  \n         - ii) No ablation study w.r.t learning the curvatures instead of taking fixed ones, e.g according to modes of the estimated sectional curvatures. This could be at least considered on the toy datasets.\n         - iii) Potential fairness issues in the benchmarks on real-world networks which should be discussed by authors: a) Most of benchmarked methods depend on a considerably fewer hyperparameters and the validation grid seems to considerably differ from original paper, while also fitting transformer-based approaches better. Moreover these methods tend to be considerably faster than FPS-T so fitting original validation grids in the paper seems affordable. Could authors provide performances distributions across validated hyperparameters to get an idea of the method robustness? ; b) Laplacian Eigenvectors are by default included in transforms and no considered for GNNs while there are Spectral augmentation techniques that could also be used.; c) the duality w.r.t hops vs number of layers is clearly different between GNN-based and transformer-based approaches. I would suggest to use e.g Jumping Knowledge concatenation of embeddings for both methods to relax the dependency to a well-chosen validated hyperparameter.\n        - iv ) SOTA methods for node classification tasks are not present in the benchmark and clearly seem to outperform FPS-T e.g [A, B]\n\n\n[A] Luan, S., Hua, C., Lu, Q., Zhu, J., Zhao, M., Zhang, S., ... & Precup, D. (2022). Revisiting heterophily for graph neural networks. Advances in neural information processing systems, 35, 1362-1375.\n\n[B] He, M., Wei, Z., & Xu, H. (2021). Bernnet: Learning arbitrary graph spectral filters via bernstein approximation. Advances in Neural Information Processing Systems, 34, 14239-14251."
                },
                "questions": {
                    "value": "I invite the authors to discuss the above-mentioned weaknesses and to answer the questions (potentially implying additional experiments) I have associated with them in order to complete my development."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5253/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698877202889,
            "cdate": 1698877202889,
            "tmdate": 1699636524132,
            "mdate": 1699636524132,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]