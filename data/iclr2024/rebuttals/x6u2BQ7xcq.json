[
    {
        "title": "Tag2Text: Guiding Vision-Language Model via Image Tagging"
    },
    {
        "review": {
            "id": "RETplhNAvl",
            "forum": "x6u2BQ7xcq",
            "replyto": "x6u2BQ7xcq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1106/Reviewer_Nwx3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1106/Reviewer_Nwx3"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes the use of tags as part of an otherwise standard V&L model. Tags are incorporated by parsing the captions to produce tags within a vocabulary, and then simply predicting the tags in a standard fashion. Secondly, tags are used, in combination with image features, to predict the caption text in an autoregressive manner. Good results are shown for tagging and for captioning, while some level of control over the generated text by using input tags is also obtained."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "I really like the paper. It is clearly motivated, well written, technically well executed, has clear novelty, and has good experimental results. But I want to highlight one thing that is surprisingly hard to find: it seems useful. The reasons: 1) Current V&L models are trained with long captions, which means small tag-like queries are not well supported. There are indeed some object detection methods a-la CLIP (owl-vit and co) but they offer different functionalities and have different requirements. So this is a good addition to the V&L toolbox. 2) it offers a way of controlling caption generation through the use of input tags. \n\n\nMinor suggestions (up to the authors and no reply needed):\nFig. 1 might get a bit confusing as both the \"prior work strategy\" and the current strategy are included in the same flow graph.\nTable 2 shows the last 3 methods seem apart from the rest but it's unclear why they are separated."
                },
                "weaknesses": {
                    "value": "The method is based on some relatively standard techniques that are however well executed and put together. While not much of a minus, but maybe a reason for an accept vs strong accept."
                },
                "questions": {
                    "value": "1) The tag system works based on pre-existing vocabulary. What happens with out-of-vocabulary terms? \n2) Have you tested retrieval based on short queries? E.g. retrieval using a term within the vocabulary vs some short retrieval query that is not part of the vocabulary (e.g. adjective + noun) vs retrieval with long captions typical of flickr/coco"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1106/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698404785479,
            "cdate": 1698404785479,
            "tmdate": 1699636036829,
            "mdate": 1699636036829,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "033tFegMSw",
                "forum": "x6u2BQ7xcq",
                "replyto": "RETplhNAvl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1106/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1106/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Nwx3"
                    },
                    "comment": {
                        "value": "Thanks for your encouraging words and constructive comments! \n\nWe sincerely appreciate your time in reviewing the paper, and our point-to-point responses to your comments are given below.\n\n\n### Additional Highlight Contributions ###\n\n> *\"I really like the paper. It is clearly motivated, well written, technically well executed, has clear novelty, and has good experimental results. But I want to highlight one thing that is surprisingly hard to find: it seems useful. The reasons: 1) Current V&L models are trained with long captions, which means small tag-like queries are not well supported. There are indeed some object detection methods a-la CLIP (owl-vit and co) but they offer different functionalities and have different requirements. So this is a good addition to the V&L toolbox. 2) it offers a way of controlling caption generation through the use of input tags. \"*\n\n\nIn response to your valuable insights:\n\n* (1) We share the same viewpoint that current V&L models fall short in handling small tag-like queries. By simultaneously supporting both tagging and V&L tasks, Tag2Text significantly extends its applicability in the field.\n\n\n* (2) The flexibility of Tag2Text to integrate tagging guidance, either comprehensive recognized image tags or any user-specified tags, indeed offers an enhanced method for controlling caption generation.\n\n\nWe have refined the caption of Figure 2, and will carefully incorporate these insights into our revised manuscript to further highlight its contributions to the field. Thanks very much!\n\n\n### Clarifications on Table 2 ###\n\n> *\"Table 2 shows the last 3 methods seem apart from the rest but it's unclear why they are separated.\"*\n\nThanks for the constructive suggestions.\n\nThe last three methods in Table 2 are distinct due to their use of pseudo-label data augmentation or higher-scale pre-training data. We have refined the presentation of Table 3 in the revision.\n\n\n### Handling Out-of-Vocabulary Terms ###\n\n> *\"The tag system works based on pre-existing vocabulary. What happens with out-of-vocabulary terms? \"*\n\nIn response to your query about out-of-vocabulary terms:\n\n\n* **Captioning Impact.** Our findings indicate that the out-of-vocabulary tags do not affect the caption resulting. We analyze the reason is that only tags within the vocabulary are utilized for the Image-Tag-Text generation training task.\n\n* **Out-of-Vocabulary Recognition.** For the image tagging branch, Tag2Text remains in the pre-existing vocabulary of 3,429 categories, unable to recognize out-of-vocabulary categories. Nevertheless, for out-of-vocabulary recognition, the image-text alignment branch can be utilized. We anticipate its performance to be similar with CLIP and BLIP under the same training dataset.\n\n\n* **Future Enhancements.** We are exploring enhancements to advance Tag2Text into an stronger open-set tagging model in the future works. One potential research route involves employing an off-the-shelf text encoder to encode pre-existing tag embeddings for training, enabling it to generalize open-vocabulary tag embeddings based on embedding similarity during inference. Current experiments in this direction have yielded effective results.\n\n\n\n### Retrieval with Short Queries ###\n\n> *\" Have you tested retrieval based on short queries? E.g. retrieval using a term within the vocabulary vs some short retrieval query that is not part of the vocabulary (e.g. adjective + noun) vs retrieval with long captions typical of flickr/coco \"*\n\n\nThank you for raising the aspect of retrieval with short queries, which is indeed a practical and valuable scenario.\n\n\n* **Lack of Benchmarks.** However, there is currently a lack of established benchmarks for short query retrieval. Our experiment attempts of only using short query tags indicated inferior performance compared to longer captions on the Flickr benchmark. \n\n\n* **Real-World Evaluation.** We have conducted internal evaluations with practical examples. Our findings demonstrate that when handling short queries including in-vocabulary terms, Tag2Text significantly outperforms CLIP and BLIP. However, for short queries involving out-of-vocabulary terms, Tag2Text did not show a marked improvement.\n\n\n* **Future Enhancements.** In light of these findings, we are committed to enhancing Tag2Text's efficacy in processing short queries involving out-of-vocabulary terms. We are exploring advancements in open-set tagging capabilities to address this challenge."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1106/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699888833809,
                "cdate": 1699888833809,
                "tmdate": 1700739024665,
                "mdate": 1700739024665,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "V7u8zGLppk",
            "forum": "x6u2BQ7xcq",
            "replyto": "x6u2BQ7xcq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1106/Reviewer_Kr1V"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1106/Reviewer_Kr1V"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new Visual Language Pretraining (VLP) framework named as Tag2Text, which introduces image tagging into vision-language models to guide the learning of visual-linguistic features. It utilizes large-scale annotation-free image tags parsed from image-text pairs through text semantic parsing, and demonstrates a foundational image tagging capability with superior zero-shot performance. Tag2Text re-introduces tag guidance into detector-free vision-language models by seamless integrating image tagging, effectively enhancing the performance of both generation-based tasks and alignment-based tasks. The proposed framework is evaluated over a wide range of downstream benchmarks, and achieves state-of-the-art results with similar model sizes and data scales."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1.\tThe idea of the utilization of image tags parsing from large-scale image-text pairs is interesting, efficient and effective from improving the performance of vision-language models.\n2.\tThe framework of Tag2Text employs a multi-task pretraining approach, including Tagging, Generation, and Alignment. These tasks are reasonable and share the same visual features obtained from the image encoder, guaranteeing the efficiency of the framework.\n3.\tA large number of experimental results over image tagging, image captioning, and image-text retrieval tasks prove the effective of the proposed methods."
                },
                "weaknesses": {
                    "value": "1.\tThe detail of text semantic parser is not clear. Although it is based on existing work of [Wu et al. 2019], it should make clear how to obtain the corresponding tags in the paper.\n2.\tIn Fig. 2, it is difficult to understand which are users\u2019 input desired tags, since they share the same forms of the recognized image tags.\n3.\tIn the Image-Tag-Text Generation paragraph, the introduction of this task is not clear. According to Fig. 4(c), it seems the text embedding should not be used as input, which is conflicting with the introduction in the corresponding paragraph.\n4.\tIn the experiment on controllability analysis, it is not clear how the threshold of tagging head to control the tagging guidance.\n5.\tThere are some typos in the manuscript."
                },
                "questions": {
                    "value": "Please try to address the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1106/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698408177227,
            "cdate": 1698408177227,
            "tmdate": 1699636036750,
            "mdate": 1699636036750,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lKE8TZpwRl",
                "forum": "x6u2BQ7xcq",
                "replyto": "V7u8zGLppk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1106/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1106/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Kr1V"
                    },
                    "comment": {
                        "value": "Thanks for your encouraging words and constructive comments. \n\nWe carefully address your concerns in specific comments below. And if you have any further concerns, we would be keen to address them as well.\n\n\n### More details of text parser. ###\n\n> *\"The detail of text semantic parser is not clear. Although it is based on existing work of [Wu et al. 2019], it should make clear how to obtain the corresponding tags in the paper. \"*\n\nThank you for the constructive suggestion. \n\nThe parser operates based on the dependency tree, a grammatical structure that maps syntactic relationships within a sentence. With a set of established semantic rules is applied to the dependency tree, the parser identifies *'entities (=head + modifier)'* and their *'relationships'* based on the given sentence. For instance, given the sentence \"A red alarm clock is on a wooden desk\", the parser automatically parse this as: *'head': ['alarm clock', 'desk']*, *'modifier': ['red', 'wooden']*, *'relation': ['on']*.\n\n\nWe have included these details and examples in the Section 3.2.\n\n\n\n### Clarification on 'users specified tags' in Figure 2. ###\n\n> *\"In Fig. 2, it is difficult to understand which are users\u2019 input desired tags, since they share the same forms of the recognized image tags. \"*\n\nWe are sorry for this confusion. In Figure 2, all green underlined tags within *'Tag2Text (User Specified)'* refer to user-specified tags. And we try to clarify Figure 2:\n\n*  Benefits from the powerful tagging capability of Tag2Text, Tag2Text can integrate the recognized image tags to automatically generate comprehensive and detailed captions.\n\n\n*  Tag2Text also allows users to input specified tags to generate corresponding captions. This flexibility, as highlighted by Reviewer Nwx3, offers a way of controlling caption generation through the use of input tags.\n\n\nWe have refined the captions of Figure 2 to clearly distinguish them.\n\n\n\n### Clarification on Image-Tag-Text Generation Task. ###\n\n> *\"In the Image-Tag-Text Generation paragraph, the introduction of this task is not clear. According to Fig. 4(c), it seems the text embedding should not be used as input, which is conflicting with the introduction in the corresponding paragraph. \"*\n\n\n Sorry for this confusion. During the Image-Tag-Text Generation training process, the text embeddings are utilized as ground truths to optimize the model via Language Modeling Loss. In the Image text generation comparison of Figure 4, all the text embeddings are utilized as ground truths during training and as model outputs during inference.\n\n\nThanks for your question, we have clarified these details Section 3.3.\n\n\n### Clarification on tagging guidance. ###\n\n> *\"In the experiment on controllability analysis, it is not clear how the threshold of tagging head to control the tagging guidance. \"*\n\n\nDuring the image tagging inference process, the tagging head outputs logits (ranging from 0 to 1) for each category. These logits are compared to a set threshold to determine the output tags. When the logit exceed this threshold, the corresponding tag category is outputted. Therefore, the tagging guidance can be controlled by adjusting the threshold. For instance, a lower threshold yields more image tags, resulting in higher recall. On the contrary, a higher threshold increases precision.\n\n\nThank you for the valuable question. We have added above details in Section C of our revised manuscript for better clarity."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1106/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699888809622,
                "cdate": 1699888809622,
                "tmdate": 1700735579624,
                "mdate": 1700735579624,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ekIi9di4rb",
            "forum": "x6u2BQ7xcq",
            "replyto": "x6u2BQ7xcq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1106/Reviewer_gXiT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1106/Reviewer_gXiT"
            ],
            "content": {
                "summary": {
                    "value": "This work introduces TAG2TEXT, incorporating object tag information into vision-language pre-training. The authors demonstrate its effectiveness across various downstream tasks. In comparison to widely-used object detectors, this method is not only faster but also enriches the model with finer-grained information."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The idea is straightforward and easy to grasp.\n- Given the high cost of human labeling and the limited diversity in generating captions for Image Captioning models, such as BLIP, finding new methods for generating comprehensive captions is both challenging and valuable.\n- This work offers a potential benefit in the context of person re-identification (REID).\n- The motivation behind this approach is also quite appealing.\n- Notably, the Tag method diverges from the commonly used Faster R-CNN-based object detectors and demonstrates significantly improved speed."
                },
                "weaknesses": {
                    "value": "- Data and Pre-training Settings: The author of the paper used a 4M setting, which includes training data from COCO (Common Objects in Context) and Visual Genome. NoCaps data is sourced from OpenImages and COCO. Importantly, this work does not incorporate any out-of-distribution data. The success of this approach on COCO-related tasks is attributed to the similarity between the pre-training data and COCO style, and it's noted that many works beyond BLIP face similar challenges.\n\n- TagEval Task: TagEval is mentioned as a task, but it is not considered popular, and its persuasiveness is limited. Models trained on tags are noted to perform well in this case.\n\n- Tag Introduction in Pre-training: The introduction of tags in pre-training is not a novel idea. There are existing works, like OSCAR, that have explored the concept of using tags to improve vision-language pre-training."
                },
                "questions": {
                    "value": "In Figure 1(b), the authors mention 'Actions.' Could you please clarify the source of this particular type of data or tag? Are there any documented instances of failure cases for Tag2Text?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Discrimination / bias / fairness concerns"
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1106/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698672104777,
            "cdate": 1698672104777,
            "tmdate": 1699636036677,
            "mdate": 1699636036677,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ySapSpMllI",
                "forum": "x6u2BQ7xcq",
                "replyto": "ekIi9di4rb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1106/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1106/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer gXiT (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to read and review our paper.\n\nWe carefully address your concerns in specific comments below. And if you have any further concerns, we would be keen to address them as well.\n\n### Evaluation on Out-Of-Distribution Dataset. ###\n> *\"This work does not incorporate any out-of-distribution data. The success of this approach on COCO-related tasks is attributed to the similarity between the pre-training data and COCO style, and it's noted that many works beyond BLIP face similar challenges. \"*\n\nWe recognize your major concern about evaluating on out-of-distribution datasets. We try to address this from two perspectives.\n\n* **Text Perspective**. In our study, we follow the official NoCaps benchmark [1] and other well-established studies [2-8] as our training and evaluation settings. Notably, NoCaps categorizes benchmarks into in-distribution and **out-of-distribution subsets** based on COCO labels. In Table 5 of our paper, we report these metrics demonstrating that Tag2Text outperforms existing models, particularly in out-of-distribution scenarios. This evidences the ability of Tag2Text to generalize beyond COCO-style datasets in handling a **diverse range of textual contexts**.\n\n* **Image Perspective**. In addition to textual contexts, we shared the same viewpoint on the necessity of including more out-of-distribution images in evaluation benchmarks. Therefore, in Table 1 of our paper, we have made progress for conducting comprehensive evaluation on the out-of-distribution OpenImages benchmark [9], which is a well-established benchmark in image tagging [10-12]. Significantly, Tag2Text achieves superior zero-shot performance even comparable to fully supervised models (**Tag2Text: 83.4 (zero-shot) vs. ML-Decoder 85.5 (full supervised)**), **without including any OpenImages training images in its training stage**. This result demonstrates the effectiveness of Tag2Text across **diverse image domains**.\n\n\nBased on above analysis, we respectfully disagree with the view of *\"this work does not incorporate any out-of-distribution data.\"* Since our experiments includes evaluation on out-of-distribution datasets to proves the effectiveness of our proposed methods, which is appreciated by Reviewer Kr1V and Nwx3.\n\n\n### Novelty Concerns in Tag Introduction. ###\n\n> *\"Tag Introduction in Pre-training: The concept of using tags in pre-training is not new, with examples like OSCAR in the field. \"*\n\n\nWe acknowledge this point and have tried our best to articulate our novel contributions, especially in comparison to works like OSCAR, in the Introduction of our paper:\n\n\n* **Unidirectional vs. Bidirectional Improvements.** OSCAR utilizes tags to enhance vision-language models in a unidirectional manner. For getting image tags, OSCAR directly utilizes an off-the-shelf object detector (Faster RCNN).\nIn contrast, Tag2Text offers **bidirectional enhancements on both image tagging and vision-language models**. For image tagging, **Tag2Text pioneer utilizing large-scale image-text pairs instead of manual annotations, achieving superior zero-shot ability.**\n\n\n* **Object Detector vs. Image Tagging**. Although using tags based on the object detector is useful, the trend in recent studies [4-8,13-14] is **shifting towards detector-free vision-language models**. Since the frozen object detector restricts the model's capacity and is time-consuming. Tag2Text **re-introduces the valuable tag guidance** into detector-free models via image tagging. This approach enables data scaling, and is both efficient and effective in improving VL tasks.\n\n\n### Clarifications on 'Actions' in Figure 1(b) ###\n\n> *\"In Figure 1(b), the authors mention 'Actions.' Could you please clarify the source of this particular type of data or tag?  \"*\n\nWe are sorry for this confusion. \n\nIn Section 3.2, we detail the source of image tags. Specifically, we parse 4 million image-text pairs and select the most frequently occurring tag categories as our label system, including objects, scenes, attributes, actions. The action categories  (e.g., 'fly', 'stand', 'climb') are obtained from the *'relationship'* items after text parsing. We also provide more tag category details in Appendix B, including tag category statistics in Table 7, an illustration in Figure 8, and the overlapping with other public datasets in Table 8 and Table 9.\n\nWe have added an example in Section 3.2 of our revised manuscript."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1106/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699888769012,
                "cdate": 1699888769012,
                "tmdate": 1700738743872,
                "mdate": 1700738743872,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6arWg9OcEo",
                "forum": "x6u2BQ7xcq",
                "replyto": "ekIi9di4rb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1106/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1106/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer gXiT (2/2)"
                    },
                    "comment": {
                        "value": "### Failure Cases of Tag2Text ###\n> *\"Are there documented instances of failure cases for Tag2Text?\"*\n\n* **Hallucinatory Captions**. Tag2Text benefits from its powerful tagging capabilities. As depicted in Figure 6, there is a strong correlation between captioning performance and tagging guidance performance. In practical applications, we observe that incorrect user-provided tags may lead to hallucinatory captions. \n\n* **Small Objects**. In addition, evaluating a tagging model capable on 3,429 categories is also challenging. Our quantitative comparison and visual validations reveal that Tag2Text efficiently recognizes common objects and scenes, yet struggles in small objects (e.g., spoon or baseball). Our empirical experiments indicate that increasing the resolution during fine-tuning significantly improves performance on these small objects.\n\nThanks for your constructive question, we have added the Section G of \"Limitation\" in our revised manuscript.\n\n\n### Reference ###\n\n[1] Nocaps: Novel object captioning at scale, in CVPR 2019.\n\n[2] Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks, in ECCV 2020.\n\n[3] VinVL: Revisiting Visual Representations in Vision-Language Models, in CVPR 2021.\n\n[4] BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation, in ICML 2022.\n\n[5] Multi-Grained Vision Language Pre-Training: Aligning Texts with Visual Concepts, in ICML 2022.\n\n[6] Injecting Semantic Concepts Into End-to-End Image Captioning, in CVPR 2022.\n\n[7] GIT: A Generative Image-to-text Transformer for Vision and Language, in Arxiv 2022.\n\n[8] BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models, in ICML 2023.\n\n[9] The open images dataset v4. in IJCV 2020.\n\n[10] Asymmetric Loss for Multi-Label Classification, in ICCV 2021.\n\n[11] Simple and Robust Loss Design for Multi-Label Learning with Missing Labels, in Arxiv 2021\n\n[12] ML-Decoder: Scalable and Versatile Classification Head, in WACV 2023.\n\n[13] Align before Fuse: Vision and Language Representation Learning with Momentum Distillation, in NeurIPS 2021.\n\n[14] An Empirical Study of Training End-to-End Vision-and-Language Transformers, in CVPR 2022"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1106/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699888784860,
                "cdate": 1699888784860,
                "tmdate": 1700735882617,
                "mdate": 1700735882617,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nSByvSvlr8",
                "forum": "x6u2BQ7xcq",
                "replyto": "ekIi9di4rb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1106/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1106/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Addition Zero-Shot Evaluations"
                    },
                    "comment": {
                        "value": "# Addition Zero-Shot Evaluations\n\nRegarding to your major concern of \"Evaluation on Out of Distribution Dataset\", we have responded in \"Response to Reviewer gXiT (1/2)\", that **we have conducted zero-shot evaluations on NoCpas and OpenImages**. \n\nDespite the fact that we have conducted sufficient experiments, to further address your concern, we conducted addition zero-shot evaluations on **NUS-WIDE** [1], a well-established tagging benchmark including 81 categories. Similarly, **all images in NUS-WIDE are out-of-distribution data**, since Tag2Text did not utilize any NUS-WDIE training images during its training process. The results are presented in the table below and have been included in Section E of the revised manuscript.\n\n\nNotably, **Tag2Text also demonstrates superior zero-shot performance**, exceeding both CLIP and BLIP, while utilizing much less training data. We find it challenging to find another zero-shot benchmark for image captioning besides NoCpas. However, considering Tag2Text benefits from its tagging ability, we contend these results demonstrate the powerful generalization ability of Tag2Text.\n\n\n| Zero-Shot Nus-Wide Comparison | Pretrain Images | F1   | Precision | Recall |\n|-------------------------------|-----------------|------|-----------|--------|\n| CLIP                          | 400M            | 46.0 | 54.0      | 40.1   |\n| BLIP                          | 129M            | 45.0 | 54.0      | 38.6   |\n| Tag2Text                      | 14M             | **46.4** | **54.7**      | **40.3**   |\n\n\nPlease feel free to share if you have further concerns, we would be keen to address them as well.\n\n[1] NUS-WIDE: a real-world web image database from National University of Singapore, ACM MM 2009."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1106/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700320994171,
                "cdate": 1700320994171,
                "tmdate": 1700737158605,
                "mdate": 1700737158605,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MtYflFGaJd",
            "forum": "x6u2BQ7xcq",
            "replyto": "x6u2BQ7xcq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1106/Reviewer_pdv8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1106/Reviewer_pdv8"
            ],
            "content": {
                "summary": {
                    "value": "This paper describes a straightforward approach to improve Vision-Language models by using tag information extracted from image captions. By utilising standard techniques and losses they show very decent results on tagging, captioning and retrieval tasks"
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well written and easy to follow. \n2. The method is simple and is shown to work well. The idea of using image tags to aid VL pre-training makes a lot of sense. \n3. The results are not always SOTA but are very good. Results on multiple tasks/datasets are provided. I believe the results are sufficient to show that the main idea behind the paper works as well as expected."
                },
                "weaknesses": {
                    "value": "I think the main problem with the paper is that all of its components have been proposed before so the paper looks more like a re-implementation of known ideas with more recent architectures and pipelines which is of course expected to work better. Specifically the main idea of using tags to aid VL pre-training appears in many works including OSCAR or more recently in DiHT (Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training) while other losses used like I2T are very commonly used in most works in VL pre-training. But the method for sure can serve as strong baseline. \nSomewhat less important concern: as most experiments are on COCO/Flick which are datasets very close to the ones used for training I am wondering whether the authors could carry out an experiment on out-of-domain data ."
                },
                "questions": {
                    "value": "No specific questions"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1106/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698771103292,
            "cdate": 1698771103292,
            "tmdate": 1699636036582,
            "mdate": 1699636036582,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vTwrXVaMKU",
                "forum": "x6u2BQ7xcq",
                "replyto": "MtYflFGaJd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1106/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1106/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer pdv8"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your encouraging feedback and constructive comments. Our point-to-point responses to your comments are given below.\n\n\n###  Introduction of Tag Guidance. ###\n\n> *\"The main idea of using tags to aid VL pre-training appears in pervious works such as OSCAR. But the method for sure can serve as strong baseline.\"*\n\nWe acknowledge this point and have tried our best to articulate our novel contributions, especially in comparison to works like OSCAR, detailed in the Introduction our paper:\n\n\n* **Unidirectional vs. Bidirectional Improvements.** OSCAR utilizes tags to enhance vision-language models in a unidirectional manner. For getting image tags, OSCAR directly utilizes an off-the-shelf object detector (Faster RCNN).\nIn contrast, Tag2Text offers **bidirectional enhancements on both image tagging and vision-language models**. For image tagging, **Tag2Text pioneer utilizing large-scale image-text pairs instead of manual annotations, achieving superior zero-shot ability.**\n\n\n* **Object Detector vs. Image Tagging**. Although using tags based on the object detector is useful, the trend in recent studies [1-7] is **shifting towards detector-free vision-language models**. Since the frozen object detector restricts the model's capacity and is time-consuming. Tag2Text **re-introduces the valuable tag guidance** into detector-free models via image tagging. This approach enables data scaling, and is both efficient and effective in improving VL tasks.\n\n### Evaluation on out-of-distribution dataset. ###\n\n> *\"Somewhat less important concern: As most experiments are on COCO/Flick which are datasets very close to the ones used for training I am wondering whether the authors could carry out an experiment on out-of-domain data.\"*\n\nOur experiments include comprehensive evaluation on out-of-distribution datasets:\n\n* **Image Captioning.** Following other well-established studies [3-9], we perform zero-shot evaluation on the NoCaps benchmark [10]. Notably, NoCaps categorizes benchmarks into in-distribution and **out-of-distribution subsets** based on COCO labels. In Table 5 of our paper, we report these metrics demonstrating that Tag2Text outperforms existing models, particularly in out-of-distribution scenarios. This evidences the ability of Tag2Text to generalize beyond COCO-style datasets in handling a **diverse range of textual contexts**.\n\n\n* **Image Tagging.** We further extend our evaluation on the out-of-distribution OpenImages benchmark [11], which is a well-established benchmark in image tagging [12-14]. Significantly, Tag2Text achieves **superior zero-shot performance** even comparable to fully supervised models, **without including any OpenImages training images in its training stage**. This result demonstrates the effectiveness of Tag2Text across **diverse image domains**.\n\nFurthermore, we have also conducted an addition zero-shot evaluations on NUS-WIDE [15] in Section E of our revised manuscript.\n\nWe hope our responses satisfactorily address your concerns. And if you have any further concerns, we would be keen to address them as well.\n\n\n\n[1] Align before Fuse: Vision and Language Representation Learning with Momentum Distillation, in NeurIPS 2021.\n\n[2] An Empirical Study of Training End-to-End Vision-and-Language Transformers, in CVPR 2022\n\n\n[3] BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation, in ICML 2022.\n\n[4] Multi-Grained Vision Language Pre-Training: Aligning Texts with Visual Concepts, in ICML 2022.\n\n[5] Injecting Semantic Concepts Into End-to-End Image Captioning, in CVPR 2022.\n\n[6] GIT: A Generative Image-to-text Transformer for Vision and Language, in Arxiv 2022.\n\n[7] BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models, in ICML 2023.\n\n[8] Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks, in ECCV 2020.\n\n[9] VinVL: Revisiting Visual Representations in Vision-Language Models, in CVPR 2021.\n\n[10] Nocaps: Novel object captioning at scale, in CVPR 2019.\n\n[11] The open images dataset v4. in IJCV 2020.\n\n[12] Asymmetric Loss for Multi-Label Classification, in ICCV 2021.\n\n[13] Simple and Robust Loss Design for Multi-Label Learning with Missing Labels, in Arxiv 2021\n\n[14] ML-Decoder: Scalable and Versatile Classification Head, in WACV 2023.\n\n[15] NUS-WIDE: a real-world web image database from National University of Singapore, ACM MM 2009."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1106/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699888683315,
                "cdate": 1699888683315,
                "tmdate": 1700738602863,
                "mdate": 1700738602863,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "njwe29tDXc",
                "forum": "x6u2BQ7xcq",
                "replyto": "MtYflFGaJd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1106/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1106/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Addition Comparison with DiHT"
                    },
                    "comment": {
                        "value": "# Comparison with DiHT\n\nWe also notice your feedback regarding DiHT [1]. We further clarify the differences between Tag2Text and DiHT.\n\n## Additional Image Captioning\n\nA significant difference is that DiHT is a alignment model as CLIP. Conversely, Tag2Text provides comprehensive and and detailed image captions, benefits from its powerful tagging capabilities.\n\n## Advanced Image Tagging\n\nIn the table below, we further compare the zero shot tagging performance of DiHT on OpenImages. Notably, despite DiHT's improvements over CLIP, it falls short of Tag2Text, even though DiHT utilizes a significantly larger training image dataset. We attribute to their different alignment paradigms:\n\n* **Gobal Image Features with Text**. Although DiHT filters image text pairs through text parsing, it relies on the alignment of global image features with text via dot product interaction, similar to CLIP.\n\n\n* **Sptial Image Features with Tag**.\nTag2Text introduces a more fine-grained alignment of image spatial features with tags (parsed from texts) through an efficient recognition decoder. This approach is particularly effective for image tagging, since the tags often correspond to multiple image regions and reside at the token level within the text.\n\n\n| Zero-Shot OpenImages Comparison | Training Images | F1   | Precision | Recall |\n|---------------------------------|-----------------|------|-----------|--------|\n| CLIP                            | 400M            | 63.0 | 77.9      | 52.9   |\n| DiHT                            | 438M            | 66.3 | 77.0      | 65.3   |\n| Tag2Text                        | 14M             | **72.7** | **80.1**      | **66.6**   |\n\n\nWe have incorporated these insights into our revised manuscript to further highlight its contributions. Specifically, we have emphasized the fine-grained tagging alignment in Section 3.3 and added the comparison results of DiHT in Table 1.\n\nWe appreciate your valuable feedback which inspires us a lot.\n\n\n[1] Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training, in CVPR 2023."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1106/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700018617489,
                "cdate": 1700018617489,
                "tmdate": 1700735993710,
                "mdate": 1700735993710,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VlsCVYBflf",
            "forum": "x6u2BQ7xcq",
            "replyto": "x6u2BQ7xcq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1106/Reviewer_N6Aw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1106/Reviewer_N6Aw"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel framework for vision-language pre-training. The core insight here is that extracting tags from text paired with images offers robust semantic guidance for vision-language pre-training, and these tags can be easily mined from an existing pipeline. Leveraging these mined tags, the paper presents a multi-task learning framework designed for vision-language pre-training. This framework concurrently handles image captioning, image tagging (multi-label classification), and Image-Text Matching. The paper conducts an extensive array of experiments, with the results demonstrating the significant impact of image tagging in this pre-trained framework."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Including image tagging task to vision language pretraining seems to be promising, which particularly benefit image captioning.\n2. The proposed design that combines multi-tasks for vision language pretraining is interesting.\n2. The experimental results are promising.\n3. The paper is well-written in general and easy to comprehend."
                },
                "weaknesses": {
                    "value": "1. This paper discusses VL pretraining methods that are either based on 1) object detection or 2) image tagging (the proposed method) and argues that image tagging is faster and introduces significantly fewer parameters. However, this may not be a compelling motivation for choosing image tagging, as object detectors are fixed (with no additional learnable parameters) and only need to be executed once before training, incurring marginal computational cost compared to the training phase.\n\n2. It would be beneficial for this paper to incorporate specific mathematical formulations to provide a more comprehensive description and discussion of the fundamental problems that require resolution.\n\n3. There is room for improvement in the typesetting.\n\n4. Table 1 appears to be disorganized, making it unclear which numbers to focus on and compare. It is advisable to separate results for different tasks into distinct tables.\n\n5. The \"SOTA\" comparison for image tagging (multi-label classification) seems to omit a substantial portion of recent works. As a result, the reviewer maintains a skeptical stance concerning the associated claims and conclusions.\n\n6. Metrics such as Precision, Recall, and F1 score, which are commonly used for image tagging and multi-label classification, are notably absent from the results.\n\n7. The construction of the tag category system appears to involve human annotation in the process (Section 3.2), which contradicts the earlier claim of being an \"automatic\" approach (Section 1).\n\n8. Conducting an ablation study on the choice of vocabulary set (tag set) size to be mined could offer valuable insights into the proposed method."
                },
                "questions": {
                    "value": "1. Could the author give a short explanation of the meaning of title \"Tag2Text\"?\n2. Performing a multi-label classification with 3,429 categories is non-trivial. The reviewer wonder if the authors come across any difficulties? And what the performance of the learned classifiers on those category (for example, on a validation set of VL pretraining)?\n3. Have the authors ablate VL pretraining framework with a single task, for example, Image-Text alignment only based VL, Image-tagging only based VL, Image captioning only based VL?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1106/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698822018451,
            "cdate": 1698822018451,
            "tmdate": 1699636036499,
            "mdate": 1699636036499,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hwWX58Dpmd",
                "forum": "x6u2BQ7xcq",
                "replyto": "VlsCVYBflf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1106/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1106/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer N6Aw (1/2)"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your detailed comments and helpful feedback on our work. \n\nWe carefully address your concerns in specific comments below. And if you have any further concerns, we would be keen to address them as well.\n\n### Object Detector vs. Image Tagging. ###\n> *\"This paper discusses VL pretraining methods that are either based on 1) object detection or 2) image tagging (the proposed method) and argues that image tagging is faster and introduces significantly fewer parameters. However, this may not be a compelling motivation for choosing image tagging, as object detectors are fixed and only need to be executed once before training, incurring marginal computational cost compared to the training phase.\"*\n\nThanks for your insightful question. We try to address this from two perspectives.\n\n* **Capacity Limitation**. Despite a frozen object detector reduces computational costs during training, it **lacks the ability for bidirectional optimization in an end-to-end manner**. Consequently, the inherent limitations in a frozen object detector can restrict the capabilities of vision-language (VL) models, as highlighted in [1-3]. More recent studies [1-6] have shifted towards detector-free VL models, resulting in the discarding of valuable tags. \n\n    To this end, we re-introduce tagging guidance into detector-free models. Our proposed image tagging utilizes large scale image tags in accordance with image-text pairs, **ensuring the capabilities of both VL tasks and image tagging**.\n* **Inference Efficiency**.  Regarding inference efficiency, object detectors are typically more time-consuming compared to image tagging (e.g., **Object Detector: 153ms** vs. **Image Tagging: 40ms**).\n\nAs a result, our image tagging enables data scaling, and is both effective and efficient in improving VL tasks.\n\n### Mathematical Formulations. ###\n> *\" It would be beneficial for this paper to incorporate specific mathematical formulations to provide a more comprehensive description and discussion of the fundamental problems that require resolution.\"*\n\nThanks for your valuable suggestion. \n\nWe have provided mathematical formulations to detail different pre-training tasks in Appendix A.2. \n\n### SOTA Comparison for Image Tagging. ###\n> *\" The \"SOTA\" comparison for image tagging (multi-label classification) seems to omit a substantial portion of recent works. As a result, the reviewer maintains a skeptical stance concerning the associated claims and conclusions.\"*\n\nThanks for your constructive question.\n\nTo assess the tagging capability of Tag2Text, recognizing 3,429 categories, we employed multiple benchmarks and selected **SOTA models capable on these benchmarks**. Our comparison includes both tagging models (e.g., ML Decoder [7], MKT [8]) and VL models (e.g., CLIP [9], BLIP [4], BLIP-2 [7]). Notably, ML-Decoder continues to demonstrate exceptional performance on the OpenImages LeaderBoard [10], serving as a **powerful fully supervised benchmark** for our analysis.\n\nAs detailed in Table 1 of our paper, Tag2Text achieves superior zero-shot performance on OpenImages, without including any OpenImages training images in its training stage. Significantly, this performance is comparable to fully supervised models of ML-Decoder (**Tag2Text: 83.4 (zero-shot) vs. ML-Decoder 85.5 (full supervised)**). We contend these results highlights the robust tagging capabilities of Tag2Text.\n\n### Evaluation Metrics for Image Tagging. ###\n> *\" Metrics such as Precision, Recall, and F1 score, which are commonly used for image tagging and multi-label classification, are notably absent from the results.\"*\n\nThanks for your question. As detailed in Table 1 of our paper, we have indeed included a variety of evaluation metrics, encompassing mAP, Precision, and Recall, to comprehensively assess our model's performance.\n\n* **mAP**. We utilize mAP as the primary metric when comparing with other tagging models, due to its well-established status in the field of image tagging.\n\n* **Precision and Recall**. For comparison with vision-language models, where direct calculation of mAP is challenging, we employ Precision and Recall as our main metrics. \n\nIn response to your feedback, the additional F1 score comparisons are presented below. Notably, Tag2Text demonstrates significant enhancements across various datasets. These results have been included in Table 2 of our revised manuscript.\n\n\n\n| F1 Comparison | Training Images | Evaluation Paradigm | TagEval | OpenImages | COCO |\n|---------------|-----------------|---------------------|---------|------------|------|\n| CLIP| 400M  | Alignment | 63.4| 63.0| 48.2 |\n| BLIP| 129M  | Alignment | 65.7| 64.8| 54.3 |\n| BLIP| 129M  | Captioning| 58.6| 56.6| 55.7 |\n| BLIP-2 | 129M  | Captioning| 58.2| 58.1| 59.1 |\n| Tag2Text| 14M   | Captioning| 65.9| 62.7| 62.7 |\n| Tag2Text| 14M   | Tagging   | **78.6**| **72.7**| **71.5** |"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1106/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699888574866,
                "cdate": 1699888574866,
                "tmdate": 1700736654597,
                "mdate": 1700736654597,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GrfMDnq1ju",
                "forum": "x6u2BQ7xcq",
                "replyto": "VlsCVYBflf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1106/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1106/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer N6Aw (2/2)"
                    },
                    "comment": {
                        "value": "### Clarification on Tag Category System Construction ###\n> *\" The construction of the tag category system appears to involve human annotation in the process (Section 3.2), which contradicts the earlier claim of being an \"automatic\" approach (Section 1).\"*\n\nWe are sorry for this confusion. \n\nExtracting large-scale tags from texts through automatic text parsing is entirely automated with no manual annotation, as stated in Section 1: *\"The pre-training image tags are obtained through automatically text semantic parsing\"*\n\nIn Section 3.2, in order to build a more comprehensive label system, we filter the tag categories with a minimal amount of manual effort. For example, we merge synonyms such as 'person' and 'human'. We have added this example in Section 3.2 of our revised manuscript.\n\n### Impact on Vocabulary Set Size ###\n> *\"Conducting an ablation study on the choice of vocabulary set (tag set) size to be mined could offer valuable insights into the proposed method..\"*\n\nWe appreciate this constructive suggestion!\n\nIn response, we have expanded the vocabulary set from 3,429 to 4,585 categories and compared the performances, as presented in the table below. Notably, the tagging performance decrease with the larger vocabulary set. We attribute to two possible reasons:\n\n* The increased complexity in training with more categories.\n\n* The additional categories leading to more noise, as they lack sufficient training data, thereby impacting the model's efficiency.\n\nThese results and analyses have been incorporated in Section B of our revised manuscript.\n\n| Vocabulary Set Size|TagEval|OpenImages|\n|-|-|-|\n| 3,429 | 81.7| 84.1|\n| 4,585 | 80.3| 83.1|\n\n### Clarification on Model Name \"Tag2Text\" ###\n> *\"Could the author give a short explanation of the meaning of title \"Tag2Text\"?.\"*\n\nSorry for the confusion. The name \"Tag2Text\" is chosen to vividly represent the core functionality of our model, which refers to the integration of image tags as guiding elements into text generation. Benefits from its powerful tagging capabilities, Tag2Text is able to generate more comprehensive and and detailed text descriptions, as illustrated in Figure 2 of our paper. \n\nIn addition, Tag2Text also allows users to input specified tags to generate corresponding captions. This flexibility, as highlighted by Reviewer Nwx3, offers a way of controlling caption generation through the use of input tags.\n\n### Image Tagging with 3,429 Categories ###\n> *\"Performing a multi-label classification with 3,429 categories is non-trivial. The reviewer wonder if the authors come across any difficulties? And what the performance of the learned classifiers on those category?\"*\n\nThank you for raising this important point!\n\nWe share the same viewpoint that performing a tagging model with 3,429 categories presents significant challenges, primarily due to the scarcity of training data for such a large number of categories. To address this, we pioneer the utilization of large-scale image-text pairs instead of manual annotations. This approach, combined with our multi-task and pretrain-finetune manners, are proven effective for image tagging as detailed in Table 5 of our paper.\n\n\nEvaluating a tagging model on such a wide category range is also challenging. Our quantitative comparison and visual validations reveal that Tag2Text efficiently recognizes common objects and scenes, yet struggles in small objects (e.g., spoon or baseball). Our empirical experiments indicates that increasing the resolution during fine-tuning significantly improves performance on these small objects.\n\nThanks for your constructive question, we have added the Section G of \"Limitation\" in our revised manuscript.\n\n\n### Ablation Study with Single Task ###\n> *\"Have the authors ablate VL pretraining framework with a single task, for example, Image-Text alignment only based VL, Image-tagging only based VL, Image captioning only based VL?.\"*\n\nThanks for your insightful question.\n\nIn our paper, we have reported the results with only either VL tasks or tagging task. We summarize them in the table below. The table clearly demonstrates the effectiveness of our bidirectional enhancements on both image tagging and VL tasks.\n\n|Pre-Training Objectives|Source of our Paper|Tagging|Captioning| Retrieval |\n|-|-|-|-|-|\n|Tagging|Table 5 Row 5|74.7|-|-|\n|Captioning + Alignment |Table 4 Row 2 |-|123.3|74.4|\n|Tagging + Captioning + Alignment|Table 5 Row 6 / Table 4 Row 3|76.4 / 82.9(+ft)|124.6|74.9|"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1106/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699888617854,
                "cdate": 1699888617854,
                "tmdate": 1700736462928,
                "mdate": 1700736462928,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "X1YSbJphWq",
                "forum": "x6u2BQ7xcq",
                "replyto": "VlsCVYBflf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1106/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1106/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A kind reminder"
                    },
                    "comment": {
                        "value": "Dear Reviewer N6Aw:\n\nThanks again for your detailed comments and helpful feedback. The deadline for the public comment is approaching. We are keen to ensure that our revisions and responses align with your expectations and address all your concerns effectively. Please feel free to let us know if you have other questions, we want to solve them for you.\n\nBest,\n\nAuthors"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1106/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700704770994,
                "cdate": 1700704770994,
                "tmdate": 1700704793685,
                "mdate": 1700704793685,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]