[
    {
        "title": "Rotation has two sides: Evaluating Data Augmentation for Deep One-class Classification"
    },
    {
        "review": {
            "id": "EldXopVNDM",
            "forum": "Ad81awoBVS",
            "replyto": "Ad81awoBVS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8740/Reviewer_jwSM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8740/Reviewer_jwSM"
            ],
            "content": {
                "summary": {
                    "value": "This work targets improving OCC performance by discriminating the RAIs in the training dataset, which is inspired by a surprising observation: there exists a strong linear relationship between the accuracy of rotation prediction and the performance of OCC. The proposed distribution matching-based method is interesting and proved to be effective."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The idea is novel and interesting.\n2. The proposed method is promising and well-analyzed.\n3. The paper writing is good."
                },
                "weaknesses": {
                    "value": "1. In the first stage of the proposed method, there is no reason presented for the choice of contrastive pre-training.\n2. There are no large-scale dataset evaluations, such as imagenet.\n3. There is no direct quantitative evaluation of the RAI predictions. For instance, can the authors annotate a test set with binary labels of whether or not a sample is RAI?"
                },
                "questions": {
                    "value": "1. In Page 1 \"In cases where real-world outliers are lacking, one typical solution is to generate negative samples by applying geometric transformations, such as rotation, to the training samples.\", why are the augmented samples negative?\n2. In Sec.3.1, are other pre-trained models suitable for stage 2? Why?\n3. In Sec.3.2, how to ensure there is one sample in p_i belonging to the input domain after xy-shuffling?\n4. How to pick the RAI samples according to Eq. (2\uff09\uff1f\n5. There is no definition of p(R_{r}(x)) before it is been used in Eq.(2). And the end of Eq.(2) should be a period."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8740/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697473284275,
            "cdate": 1697473284275,
            "tmdate": 1699637097172,
            "mdate": 1699637097172,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1iYDZjBaRc",
                "forum": "Ad81awoBVS",
                "replyto": "EldXopVNDM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8740/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8740/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer jwSM"
                    },
                    "comment": {
                        "value": "We sincerely thank Reviewer jwSM for providing insightful suggestions. Below, we address your constructive comments individually.\n\n\n**W1**: The reason for the contrastive pre-training in the first stage.\n\nContrastive pre-training enables the model to extract non-trivial image representations, allowing us to measure the distribution discrepancy in the representation space instead of low-level raw pixel space. The first stage of contrastive pre-training is crucial to make the entire pipeline unsupervised, although the method is not limited to self-supervised feature extractors. Alternatively, using a more powerful feature extractor pre-trained on large-scale datasets in a supervised manner yields improved recall and precision scores. See **Q2** for experimental details.\n\n\n**W2**: Large-scale dataset evaluation.\n\nWe conduct experiments on the TinyImageNet which has 100,000 images of 200 classes. As the two-stage pipeline in our manuscript, the first stage pre-trains the encoder by using SimCLR and the second stage learns rotation distribution. Likewise, we use the prediction (Eq.(2)) to identify RAI and non-RAI. Given the partition of RAI and non-RAI, we again pre-train the model by using SimCLR, following the practice [5] to deal with RAIs and non-RAIs, i.e., RAIs and their rotations are postive while non-RAIs and their rotations are negative. For fair comparison, we keep the same hyper-parameters in the pre-training except for the different separations of RAIs and non-RAIs. We adopt liear probing and report the top-1 classification accuracy. We improve the baseline by +2.59% and we attribute the gain to the enhanced distinction between RAI and non-RAI.\n\n| TinyImageNet | RAI ratio (%) |  top-1 accuracy (%) |\n|--------------|---------------|---------------------|\n|     PNDA     |      31.3     |        37.17        |\n|     Ours     |      28.9     |     39.76(+2.59)    |\n\n\n**W3**: Missing quantitative evaluation of the RAI predictions.\n\n\nFor quantitative evaluation, binary labels of RAI and non-RAI for the CIFAR-10 training set are manually annotated. The rotation prediction model [3] is initially employed to identify RAI samples incorrectly predicted at 0 degrees, which are then corrected manually as ground-truth RAIs. The remaining samples are designated as non-RAI. The prediction precision and recall of RAI, organized by class, are reported in the table. The table shows that our method can accurately identify RAIs in the training set. \n\n|Class             | plane |  car  | bird |  cat | deer |  dog | frog | horse | ship  | truck | Mean |\n|------------------|-------|-------|------|------|------|------|------|-------|-------|-------|------|\n|RAI ratios (%)    |  15.3 |  0.9  | 17.0 | 18.5 |  7.8 |  8.5 | 14.9 |  2.5  |   2.5 |  2.9  |  9.1 |\n|   Recall         |  91.1 |  97.7 | 92.6 | 89.2 | 81.3 | 88.3 | 77.8 |  93.6 |  89.6 |  87.8 | 84.7 |\n|   Precision      |  88.3 |  89.6 | 89.8 | 84.3 | 84.8 | 71.4 | 88.6 |  80.7 |  88.2 |  81.1 | 88.9 |"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8740/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700739375699,
                "cdate": 1700739375699,
                "tmdate": 1700739375699,
                "mdate": 1700739375699,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "oj88jn6vBk",
            "forum": "Ad81awoBVS",
            "replyto": "Ad81awoBVS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8740/Reviewer_Yjs1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8740/Reviewer_Yjs1"
            ],
            "content": {
                "summary": {
                    "value": "Targeting improvement of existing OCC, this paper makes an observation of the strong linear relationship between the rotation prediction and the performance of OCC. To the end, this paper proposes a two-stage framework where in the first stage, standard contrastive learning is used, while in the second stage, semantics-preserving samples are selected from the augmented dataset. Experiments are conducted on several anomaly detection benchmarks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1.\tThe contribution is clear, and the motivation sounds reasonable.\n2.\tAnalysis on the impact of rotation prediction on OCC is intuitive.\n3.\tExperiments show the effectiveness of the proposed method."
                },
                "weaknesses": {
                    "value": "1. While the analysis is intriguing, its applicability remains limited to rotation-related datasets and methods. For instance, it may not be suitable for numerous real-world anomaly detection tasks, such as MVTec and VisA.\n\n2. The improvements, as depicted in Table 1, are somewhat modest compared to existing methods.\n\n3. An essential evaluation is missing. This paper identifies RAI images and treats them differently from the original method. However, it is unclear whether the observed improvement stems from these RAI images alone. Assessing the performance of RAI images separately might lead to a more substantial improvement.\n\n4. The paper's structure could be improved. There is significant overlap in the information presented in Figures 1, 2, 4, 5, 6, and 7. It may be advisable to move figures like 4-7 to the supplementary section.\n\n5. The rationale behind the authors' decision to use the version of UniCon without soft aggregation and hierarchical augmentation remains unclear. Since hierarchical augmentation is an integral module within UniCon, it is advisable for the authors to use the full version of UniCon as the baseline for their study."
                },
                "questions": {
                    "value": "See the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8740/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697770659190,
            "cdate": 1697770659190,
            "tmdate": 1699637096996,
            "mdate": 1699637096996,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DGtQ2VLRW0",
                "forum": "Ad81awoBVS",
                "replyto": "oj88jn6vBk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8740/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8740/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Yjs1"
                    },
                    "comment": {
                        "value": "We sincerely thank Reviewer Yjs1 for providing insightful suggestions. Below, we address your constructive comments individually.\n\n\n**W1**: Its applicability remains limited to rotation-related datasets and methods.\n\nOur work extends beyond rotation-related datasets and methods. We additionally conduct experiments on translation distribution prediction in the SVHN dataset (See the Section 4.3 in the updated manuscript). Our experimental evaluation on MvTec-AD, adopting patch representations of 32x32 built upon UniCon-HA, demonstrates our method's superiority compared to counterparts that incorporate rotation augmentation. Though our method lags behind state-of-the-art anomaly detectors, we underline the need for tailored pretext tasks [1] or leveraging pre-trained models [2] in the context of industrial anomaly detection. \n\nThe reported Image/pixel-level AUROC scores are as follows:\n\n|         | RotNet | DROC [3] | UniCon-HA [4]| Cutpaste [1] | Ours | \n|---------|--------|----------|--------------|--------------|------|\n|  Image  |   71.0 |   86.5   |      89.8    |     95.2     | 90.6 |\n|  Pixel  |   92.6 |   90.2   |      94.3    |     96.0     | 95.2 | \n\n\n\n**W2**: The improvements (Table 1) are somewhat modest.\n\nOur approach, altering loss functions to handle semantics-preserving/-shifting samples separately, consistently achieves improvements on powerful baselines. While the average improvement over ten classes on one-class CIFAR-10 (Table 1) is not significantly high, it is noteworthy that different classes contain varying RAI ratios. Larger improvements are achieved for classes with a higher proportion of RAIs. For example, for the cat class, which has the largest number of RAIs (see Figure 4(c)), we obtain +1.0% based on CSI and +1.2% based on UniCon.\n\n\n**W3**: Unclear reasons for improvement.\n\nBased on CSI, we present separate results for RAI and non-RAI on one-class CIFAR-10. The table below displays the results for the classes which include a relatively high proportion of RAI samples. Our method significantly enhances OCC performance on RAI, especially for classes with a larger proportion of RAI samples, such as plane and cat. \n\n|              |         |     plane    |    bird    |     cat    |    deer    |     dog    |     frog   |     Mean   |\n|--------------|---------|--------------|------------|------------|------------|------------|------------|------------|\n|RAI ratios (%)|         |  20.2        | 18.6       | 24.4       |  8.1       |  9.7       | 12.9       | 14.74      |  \n|   CSI        |  RAI    |  73.1        | 90.2       | 79.4       | 92.0       | 91.4       | 92.4       | 86.4       |  \n|   CSI + Ours |  RAI    |  84.2(+11.1) | 93.1(+2.9) | 85.3(+5.9) | 92.8(+0.8) | 92.5(+1.1) | 92.8(+0.4) | 90.1(+3.7) |\n|   CSI        | non-RAI |  93.2        | 96.5       | 90.8       | 95.2       | 94.9       | 97.4       | 94.7       | \n|   CSI + Ours | non-RAI |  93.4(+0.2)  | 96.5(+0.0) | 91.9(+1.1) | 95.6(+0.4) | 95.2(+0.3) | 97.7(+0.3) | 95.1(+0.4) |  \n\n**W4**: Paper structure.\n\nFigure 1 and Figure 2 illustrate the correlation between rotation prediction and OCC. Figures 4-7 provide detailed analyses of the effects of RAIs and non-RAIs. We reorganize the structure in our final version.\n\n\n**W5**: Use the complete version of UniCon.\n\nBuilt upon the complete version of UniCon-HA, which is a very strong OCC detector, it is noted that we also achieve the consistent improvement (+0.2%) on one-class CIFAR-10. \n\n\n\n[1] Li et al. Cutpaste: Self-supervised learning for anomaly detection and localization. In CVPR, 2021.\n\n[2] Paul et al. Uninformed students: Student-teacher anomaly detection with discriminative latent embeddings. In CVPR, 2020.\n\n[3] Sohn et al. Learning and evaluating representations for deep one-class classification. In ICLR, 2021.\n\n[4] Wang et al. Unilaterally aggregated contrastive learning with hierarchical augmentation for anomaly detection, In ICCV, 2023."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8740/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700739034455,
                "cdate": 1700739034455,
                "tmdate": 1700739034455,
                "mdate": 1700739034455,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "fkhiCDYr1J",
            "forum": "Ad81awoBVS",
            "replyto": "Ad81awoBVS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8740/Reviewer_vKuc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8740/Reviewer_vKuc"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a technique that can learn the rotation distribution of images within a dataset. The approach is motivated by the authors' study of one-class classification (OCC), which involves predicting whether data belongs to a particular class seen in training or is anomalous. Specifically, the authors are investigating the seeming strong linear relationship between rotation prediction accuracy and OCC. The authors attribute this to transformation bias, where samples that are semantic-preserving vs. semantic-shifting lead to different behavior when training OCC with contrastive learning approaches.\n\nTo this end, the authors propose a two-stage approach for learning the transformation distribution: 1) perform a standard contrastive self-supervised representation learning phase, and 2) transformation distribution estimation. For 2, the authors create a dataset consisting of the original samples in {0, 90, 180, and 270} degrees and learn a differentiable sampler with Gumbel Softmax to predict images that preserve semantics (i.e., rotating it does not necessarily change the orientation at which the picture must have been taken). Finally, MMD is used to perform density matching in the representation space to identify such semantic-preserving samples.\n\nThe authors use their model to 1) visually show a good model in learning RAIs vs. non-RAIs, 2) a correlation between the amount of RAIs in training and OCC performance, and 3) that using their approach consistently adds ~1% gain to OCC."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The strengths of the paper include a clear motivation, strong analysis of the correlation between rotation prediction and OCC, and its unsupervised data-driven approach. The intuition to use a predictor in conjunction with the contrastive pretrained representations and density estimation to align the dataset makes sense for extracting out the transformation distribution exhibited within the training set. Understanding this distribution is an interesting task. The visual results of selecting RAI vs. non-RAI images are compelling. The issues with existing OCC approaches are well-analyzed and the the proposed approach provides modest but consistent improvement to existing OCC approaches."
                },
                "weaknesses": {
                    "value": "One weakness is that there is not a quantifiable way to measure the accuracy in RAI vs. non-RAI determination. One reason for this is that RAI images may be classified as 0, making it hard to separate these images from the truly non-RAI images. There is also not a discussion / inclusion of failure modes to understand where the model may succeed vs. fail. Another weakness is that the utility of the transformation distribution may be larger but is focused on OCC, and datasets used to evaluate OCC are from CIFAR-10, which may have different characteristics than some of the anomaly detection settings where labeled data of out-of-distribution samples could be limited. I also think it is odd that OCC is a common thread / motivator of the paper but the description of the technique for OCC is not in the approach section. The claim of the paper then is a bit broad, in some parts reading as if it is most concerned about OCC and other parts about the distribution being learned."
                },
                "questions": {
                    "value": "1. In Section 4.2: it seems that images with a prediction of 0 is deemed non-RAI and anything else is deemed to be RAI. Couldn't RAI images still be predicted with an angle of 0? How many of the RAI images are classified as angle 0 and how many images classified as angle 0 are actually RAI?\n\n2. Do you have any examples / analysis of failure modes where images like those in figure 5 are mistakenly predicted to be RAI vs. non-RAI?\n\n3. Can you clarify the rule used in section 4.4 to determine if a sample is semantically-shifted? Which r is used?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8740/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8740/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8740/Reviewer_vKuc"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8740/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698721214972,
            "cdate": 1698721214972,
            "tmdate": 1699637096881,
            "mdate": 1699637096881,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QrB1gTWNHw",
                "forum": "Ad81awoBVS",
                "replyto": "fkhiCDYr1J",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8740/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8740/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer vKuc"
                    },
                    "comment": {
                        "value": "We sincerely thank Reviewer vKuc for providing insightful suggestions. Below, we address your constructive comments individually.\n\n**W1(Q1)**: Quantitative accuracy in RAI vs. non-RAI determination.\n\nFor quantitative evaluation, binary labels of RAI and non-RAI for the CIFAR-10 training set are manually annotated. The rotation prediction model [3] is initially employed to identify RAI samples incorrectly predicted at 0 degrees, which are then corrected manually as ground-truth RAIs. The remaining samples are designated as non-RAI. The prediction precision and recall of RAI, organized by class, are reported in the table. The table shows that our method can accurately identify RAIs. The results can be further improved by leveraging a more powerful pre-trained feature extractor, as demonstrated in **W2**. \n\n \nTrainig set:\n|Class             | plane |  car  | bird |  cat | deer |  dog | frog | horse | ship  | truck | Mean |\n|------------------|-------|-------|------|------|------|------|------|-------|-------|-------|------|\n|RAI ratios (%)    |  15.3 |  0.9  | 17.0 | 18.5 |  7.8 |  8.5 | 14.9 |  2.5  |   2.5 |  2.9  |  9.1 |\n|   Recall         |  91.1 |  97.7 | 92.6 | 89.2 | 81.3 | 88.3 | 77.8 |  93.6 |  89.6 |  87.8 | 84.7 |\n|   Precision      |  88.3 |  89.6 | 89.8 | 84.3 | 84.8 | 71.4 | 88.6 |  80.7 |  88.2 |  81.1 | 88.9 |\n\n\n\n**W2(Q2)**: Failure modes.\n\nIn fact, the last table shows that our model has misclassified images and these incorrect predictions stem from imperfect representations learned in contrastive pre-training, which are contextually biased towards spurious scene correlations [1, 2]. To address this, we suggest using a more powerful feature extractor pre-trained on large-scale datasets. For instance, leveraging an encoder pre-trained on ImageNet-1k has resulted in improved outcomes in the identification of both RAI and non-RAI. The table shows the results on the training set of CIFAR-10.\n\n|  pre-training dataset  |  pre-training way |  Recall |  Precision |\n|------------------------|-------------------|---------|------------|\n|   one-class CIFAR-10   |  self-supervised  |  84.7   |    88.9    |\n|        CIFAR-10        |     supervised    |  93.6   |    90.3    |\n|      ImageNet-1k       |     supervised    |  95.5   |    93.5    |\n\n\n\n\n**W3**: Transformation distribution vs. OCC.\n\nWhile rotation prediction as a pretext task has been explored in the self-supervised literature and facilitates bundles of downstream tasks, its significance becomes more pronounced in OCC, where the effectiveness relies solely on the design of the pretext task to capture normal patterns. Furthermore, our empirical experiments reveal a strong linear relationship between the accuracy of rotation prediction and the performance of OCC. This observation motivates us to delve into investigating how the separation between RAI and non-RAI impacts OCC. While we have achieved improved results in multi-class classification on TinyImageNet, evaluated by linear probing. We have revised the manuscript to provide additional clarity on this aspect.\n\n| TinyImageNet | RAI ratio (%) |  top-1 accuracy (%) |\n|--------------|---------------|---------------------|\n|     PNDA     |      31.3     |        37.17        |\n|     Ours     |      28.9     |     39.76(+2.59)    |\n\n**Q1**: Determination of RAI and non-RAI.\nYes. Images with a prediction of 0 are deemed non-RAI, i.e., $argmax_r{p_r(x))}$ $r \\in $ {0,90,180,270} equals 0, while those with any other predictions are considered RAI. \n\n**Q3**: Determination of a semantically-shifting sample. Which $r$ is used?\n\nSemantically-shifting samples are determined by comparing the original image x and its rotated version $R_r(x)$ ($R_r$ denotes rotating $x$ by $r$ degrees, $r$ belongs to {0, 90, 180, 270}). An image $R_r(x)$ satisfying $p(R_r(x)) < p(x)$ is considered semantically-shifting. The determination is conducted by considering all four rotation angles. Note that for a RAI sample, at least one rotated sample is semantically-preserving.\n\n\n[1] Mo et al. Object-aware Contrastive Learning for Debiased Scene Representation. In NeurIPS 2021.\n\n[2] Purushwalkam et al. Demystifying Contrastive Self-Supervised Learning: Invariances, Augmentations and Dataset Biases. In NeurIPS, 2020.\n\n[3] Miyai et al. Rethinking rotation in self-supervised contrastive learning: Adaptive positive or negative data augmentation. In WACV, 2023."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8740/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738638900,
                "cdate": 1700738638900,
                "tmdate": 1700738638900,
                "mdate": 1700738638900,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "q2b9R7WYCY",
            "forum": "Ad81awoBVS",
            "replyto": "Ad81awoBVS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8740/Reviewer_Soam"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8740/Reviewer_Soam"
            ],
            "content": {
                "summary": {
                    "value": "The study makes a surprising discovery: a strong linear relationship exists between the accuracy of rotation prediction and the performance of OCC and they show that representations learned from transformations already present tend to be less effective. To address this, the paper proposes a staged learning-based framework for one-class classification (OCC) that aims to identify semantics-preserving images. The framework consists of two stages: self-supervised representation learning and transformation distribution estimation."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- I think the authors do a great job at explaining and motivating the problem they are working on.\n- The paper seems novel in its exploration of the relationship between rotation prediction and one-class classification (OCC). It highlights a surprising observation of a strong linear relationship between the performance of rotation prediction and the performance of OCC.\n- The authors back up their approach on empirical observations, highlighting the importance of effective data transformations and the potential decrease in effectiveness if transformations are already present in the dataset. This empirical foundation strengthens the credibility of their proposed solution. Their approach seems to be very well-motivated\n- The experiments though little are well-designed, valid, and exhaustive, with comparison to a range of baselines as well as some ablation studies."
                },
                "weaknesses": {
                    "value": "- A big weakness right now is the lack of extensive empirical validation. The authors currently only perform experiments on CIFAR-10 and their experiments on other kinds of transformations are also very limited. Though the authors show interesting results for another transformation, it is immensely difficult with this set of results to comment on how well their approach could work.\n- One of the really interesting findings from this paper is about transformation bias, and representations learned from transformations already present tend to be less effective. I believe this would be well-shown by experiments across multiple datasets and multiple kinds of models."
                },
                "questions": {
                    "value": "- How does the analysis across other kinds of popular transformations look like and does this approach still hold, I would encourage the authors to include talking about other transforms even if they do not seems to work well.\n- This shouldn't use in-text citations\n\n> facturing defect detection (Bergmann et al., 2020; 2019) and medical diagnosis Schlegl et al. (2017)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8740/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8740/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8740/Reviewer_Soam"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8740/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698837479874,
            "cdate": 1698837479874,
            "tmdate": 1699637096773,
            "mdate": 1699637096773,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1sr8PJCuJN",
                "forum": "Ad81awoBVS",
                "replyto": "q2b9R7WYCY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8740/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8740/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Soam"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your recognition of the novelty and motivation behind our work. Below, we address your constructive comments individually.\n\n**W1**: Lack of empirical validation on datasets other than CIFAR-10.\n\nWe additionally conduct experiments on the TinyImageNet which has 100,000 images of 200 classes. As the two-stage pipeline in our manuscript, the first stage pre-trains the encoder by using SimCLR and the second stage learns rotation distribution. Likewise, we use the prediction (Eq.(2)) to identify RAI and non-RAI. Given the partition of RAI and non-RAI, we again pre-train the model by using SimCLR, following the practice [5] to deal with RAIs and non-RAIs, i.e., RAIs and their rotations are postive while non-RAIs and their rotations are negative. For fair comparison, we keep the same hyper-parameters in the pre-training except for the different separations of RAIs and non-RAIs. We adopt liear probing and report the top-1 classification accuracy. We improve the baseline by +2.59% and we attribute the gain to the enhanced distinction between RAI and non-RAI.\n\n| TinyImageNet | RAI ratio (%) |  top-1 accuracy (%) |\n|--------------|---------------|---------------------|\n|     PNDA     |      31.3     |        37.17        |\n|     Ours     |      28.9     |     39.76(+2.59)    |\n\nFor the experiments on SVHN, refer to responses to W2 for details.\n\n\n**W2 (Q1 and Q2)**: Other transformations.\n\nTo address the need for exploring other transformations, we have extended our evaluation to include translation on the Street View House Numbers dataset (SVHN). The goal is to classify the central digit in each image into one of the categories (0 through 9). We consider five translation directions (original, up, down, left, right) with an 8-pixel shift. Notably, our approach is not confined to self-supervised pre-training. The first stage trains the ResNet-18 from scratch in a supervised way, achieving 96.24% accuracy on the test set. The second stage predicts the 5-way translation distribution. As shown in the updated manuscript, it showcases the model\u2019s ability to correctly identify off-centered digits. See Section 4.3 in the updated manuscript for more discussion on the experiments on SVHN.\n\n\n**W3**: Experiments across other models.\n\nWe have explored ViT-based models, specifically ViT-B/16, for rotation distribution prediction in CIFAR-10. Additionally, we considered variations in architecture types and pre-training strategies (self-supervised vs. supervised). For quantitative evaluation, binary labels of RAI and non-RAI for the CIFAR-10 training set are manually annotated. The provided table demonstrates the architecture-agnostic nature of our method, with ResNet-18 leading to a larger improvement than ViT-B/16. This is attributed to the lack of inductive bias in ViT, which struggles to learn local relations well with a small amount of data, such as CIFAR-10. The results also highlight our method's preference for the supervised training strategy, emphasizing the significance of well-trained image representations, as indicated by average recall and precision values across the 10 classes in CIFAR-10.\n\n\n| Architecture | pre-train (1st stage) |   Recall  |  Precision  |\n|--------------|-----------------------|-----------|-------------|\n|    PNDA[1]   |           /           |   77.1    |    62.0     |\n|   ResNet-18  |    self-supervised    |   84.7    |    88.9     |\n|   ResNet-18  |      supervised       |   93.6    |    90.3     |\n|   ViT-B/16   |    self-supervised    |   82.6    |    83.6     |\n|   ViT-B/16   |      supervised       |   90.2    |    85.4     |\n\n\n\n\n   \n[1] Miyai et al. Rethinking rotation in self-supervised contrastive learning: Adaptive positive or negative data augmentation. In WACV, 2023."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8740/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738370688,
                "cdate": 1700738370688,
                "tmdate": 1700738370688,
                "mdate": 1700738370688,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4RCPhGJonO",
            "forum": "Ad81awoBVS",
            "replyto": "Ad81awoBVS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8740/Reviewer_h8uB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8740/Reviewer_h8uB"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel method to improve performance on one-class classification problem. They observe a strong correlation between the accuracy of rotation prediction and one-class classification. They introduce a two-stage unsupervised framework that differentiates rotations that are semantic preserving (rotation-agnostic images) vs semantic shifting (non-rotation agnostic images) to enhance performance on the one-class classification benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The method introduced detects rotations that remain unchanged and are semantically similar to original images, ensuring their exclusion as outliers. This method demonstrates sufficient generalizability for other transformation (ex., gaussian noise), as supported by section 4.3. I feel that this contribution is of sufficient interest to the research community.\n- The results on OCC presented in the paper outperform the baselines and can be added on top on existing methods. It emphasizes the significance of identifying the transformations present in the original dataset before incorporating them into the pretext task for learning."
                },
                "weaknesses": {
                    "value": "- Related works section of the paper is difficult to follow and seems incomplete.\n    - The subheading of **One-Class Classification** is particularly confusing as it lacks comprehensive discussion on the relevant OCC literature. The authors directly diverge to self-supervised learning methods for OCC. It fails to give complete picture of OCC for non-experts. I would suggest the authors to briefly also give an introduction to OCC and existing generative methods or point readers to more detailed survey paper [1].\n    - There exists a confusion regarding anomaly detection and OCC cited in related work, in which cases are they both considered the same?\n- Missing relevant citation: the authors seem to be missing an important citation on rotation estimation [2]. I would suggest the authors to include it for completeness of related work.\n\n[1] One-Class Classification: A Survey, arxiv 2021\n\n[2] Self-Supervised Representation Learning by Rotation Feature Decoupling, CVPR 2019"
                },
                "questions": {
                    "value": "- My major suggestions are summarized in Weakenesses section\n- In introduction: \u201cWhile rotation has been a widely used technique in the literature for OCC\u2026\u201d missing citations, please add them here?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8740/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8740/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8740/Reviewer_h8uB"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8740/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699209522587,
            "cdate": 1699209522587,
            "tmdate": 1699637096659,
            "mdate": 1699637096659,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iMKj8RriBD",
                "forum": "Ad81awoBVS",
                "replyto": "4RCPhGJonO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8740/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8740/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer h8uB"
                    },
                    "comment": {
                        "value": "Thank you for appreciating and acknowledging our work. We address your constructive comments below:\n\n**W1**: Related works.\n\nWe have incorporated more discussion on relevant OCC literature in the revised manuscript. Anomaly detection (AD) and OCC share similarities, aiming to identify instances deviating from normal training data and providing a binary classifier to determine normality or anomaly. The key distinction lies in the training data characteristics. AD considers both normal and anomalous instances during training, while OCC is confined to only normal instances in training. Therefore, OCC can be regarded as a special case of AD. When all training data belongs to one class, AD and OCC methods are expected to yield similar results.\n\n\n**W2(Q2)**: Missing citations.\n\nWe appreciate the reviewer's suggestion for additional references. We previously noted that [1] uses the similar idea with [2] to identify RAI and non-RAI by training a binary classifier on the noisy dataset while [2] improves the idea by leveraging an extra validation set to approximate the epoch just before over-fitting. We have included it and discussed its relevance in the revised version.\n\n[1] Fent et al. Self-Supervised Representation Learning by Rotation Feature Decoupling. In CVPR, 2019.\n\n[2] Miyai et al. Rethinking rotation in self-supervised contrastive learning: Adaptive positive or negative data augmentation. In WACV, 2023."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8740/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700737966598,
                "cdate": 1700737966598,
                "tmdate": 1700737966598,
                "mdate": 1700737966598,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]