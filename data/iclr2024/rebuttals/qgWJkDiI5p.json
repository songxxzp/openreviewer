[
    {
        "title": "Fast Equilibrium of SGD in Generic Situations"
    },
    {
        "review": {
            "id": "ea6YD1xUJQ",
            "forum": "qgWJkDiI5p",
            "replyto": "qgWJkDiI5p",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1851/Reviewer_pBQz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1851/Reviewer_pBQz"
            ],
            "content": {
                "summary": {
                    "value": "To understand the behaviors of normalization in deep learning, Li et al. (2020) proposes the Fast Equilibrium conjecture: the scale-invariant normalized network, when trained by SGD with $\\eta$ learning rate and $\\lambda$ weight decay, mixes to an equilibrium in $\\tilde{O}(\\frac{1}{\\eta \\lambda})$ steps, as opposed to classical $e^{O((\\eta \\lambda)^{-1})}$ mixing time. Recent works by Wang & Wang (2022) and Li et al. (2022c) further proved this conjecture under different sets of assumptions.\n\nThis paper instead proves the fast equilibrium conjecture in full generality by removing the non-generic assumptions of Wang & Wang (2022) and Li et al. (2022c) that the minima are isolated, that the region near minima forms a unique basin, and that the set of minima is an analytic set. Their main technical contribution is to show that with probability close to 1, in exponential time trajectories will not escape the attracting basin containing its initial position."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "**Originality:** 1) They first analyze the generality of assumptions used in existing work and then successfully remove non-generic assumptions, which is very important to reduce the gap between the theory and the experiments 2) They use Arnold-Kliemann's condition instead of Kliemann's condition to remove the analyticity assumption 3) They make use of large deviation principle of Dembo-Zeitouni in Dembo & Zeitouni (2010, Chapter 5) instead of the Freidlin-Wentzell\u2019s original theory to show that with very high probability, the trajectory will not escape from the basin in exponential time to remove the unique basin assumption\n\n**Quality:** I'm not familiar with the theoretical techniques but I feel it has good quality.\n\n**Clarity:** It looks quite clear in most cases but needs some modifications for some tiny writing errors.\n\n**Significance:** Okay but not very significant for the following two reasons: 1) It relaxes the assumptions for an existing conjecture instead of discovering some new phenomena, 2) The fast equilibrium conjecture is not as important as many other things in deep learning, such as the generalization ability of deep neural networks, the puzzles in large language models, the interplay among the model, the algorithm, and the data, etc."
                },
                "weaknesses": {
                    "value": "1) They only conduct experiments on MNIST which is almost a linearly-separable dataset, which is not good for deep learning analysis. I suggest the authors conduct the experiments on other more difficult datasets, such as CIFAR-10.\n2) There are some tiny errors in the paper, so I'd suggest the authors to proofread their paper more carefully. \n- They sometimes use an uncommon citation format for references in some places. For example,\n\"The works by Bovier et al. and Shi et al. (Bovier et al., 2004; Shi et al., 2020)\" may be changed as \"Bovier et al. (2004) and Shi et al. (2020)\"; \"Li et al. made certain assumptions in (Li et al., 2022c)\" may be changed as \"Li et al. (2022c) made certain assumptions\".\n- Similarly, they seem use wrong references in some places. For example,\n\"We now stop assuming Assumption 1.3.(ii) and decompose\" => I feel it's Assumption 1.3. (i) instead of Assumption 1.3 (i) since they are talking about removing the unique basin assumption (Assumption 1.3 (i));\n\"Recall that (Li et al., 2022c)) also assumes Assumption 1.3.(i), but that can be dropped by the discussion in Chapter 3 above.\" => I feel it's Assumption 1.3 (ii) instead of Assumption 1.3 (i) since Chapter 3 discusses removing the analyticity assumption (Assumption 1.3. (ii)); \n\"Figure 5 shows that V11 and V22 stabilizes near similar but different values,\" => I think they mean Figure 1 instead of Figure 5 here."
                },
                "questions": {
                    "value": "I have a question about the classical mixing time: You use $e^{O(\\eta^{-1})}$ in the abstract but use $e^{O((\\eta \\lambda)^{-1})}$ in the Introduction. I feel $e^{O((\\eta \\lambda)^{-1})}$ looks more natural to me. Could you clarify this?\n\nSimilarly, $\\tilde{O}(1/\\eta\\lambda)$ in the abstract may need to be changed as $\\tilde{O}(\\frac{1}{\\eta \\lambda})$."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics concerns."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1851/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1851/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1851/Reviewer_pBQz"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1851/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698535961706,
            "cdate": 1698535961706,
            "tmdate": 1699636115475,
            "mdate": 1699636115475,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RENwqOe2tO",
                "forum": "qgWJkDiI5p",
                "replyto": "ea6YD1xUJQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1851/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1851/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank you for your valuable comments. Some explanations in response to your remarks are below and we will address these concerns in a revised version accordingly. \n\n1. We are running an experiment with CIFAR 10 and will add it to the revised version. \n2. We will correct the typos and inaccuracies you pointed out.\n3. You are right about the exponents in the abstract. They should be $e^{O( (\\eta\\lambda )^{-1}) } $ and $\\tilde{O}(\\frac{1}{\\eta\\lambda})$ respectively. We will fix them."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1851/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700091576770,
                "cdate": 1700091576770,
                "tmdate": 1700091576770,
                "mdate": 1700091576770,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GXk2iXlGw2",
            "forum": "qgWJkDiI5p",
            "replyto": "qgWJkDiI5p",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1851/Reviewer_LagX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1851/Reviewer_LagX"
            ],
            "content": {
                "summary": {
                    "value": "In this technical paper, the authors deal with fast convergence of networks with normalized steps to the solution.  The authors successfully prove their result and many insights are given on practical training of these networks in the experiments section."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This is a very technical paper, very mathematical, and centered around proving one conjecture in the literature. I like this, and I also enjoyed reading the paper: it is very well written and has pleasant notation. What I especially liked is the clear statement of the assumption and remarks 1.4 and 1.5. The authors also showed they mastered the subject with many useful citations and discussions around existing results. Though I did not unfortunately find the time to go through the proofs, the narrative and the impeccable discussion in the main paper leave no doubts on correctness. On a technical level, I was always curious about projections of the Brownian motion on the sphere in the context of Langevin dynamics, so I will for sure get back to this paper in the future to find more details on this. I also have a question, which you can find in the proper section below.\n\nI placed the contribution as fair since I think the result does not explain the fast convergence of normalized networks compared to not-normalized (motivation in the abstract). Please comment on this if you think your result motivates this, I am happy to revise!"
                },
                "weaknesses": {
                    "value": "I guess one obvious question is \"can you have more experiments\". I think this might be silly in this paper, but maybe there is something you can do to reach people outside the very technical domain. One thing I think would be useful is to illustrate the rates known in the literature and the conjecture - maybe with evidence from some datasets and some networks. I think you can attract the interest of many people if you have a headline figure showing the speed of convergence to the stationary distribution and exactly the rate you prove.\nA thing I found a bit confusing is going back-and-forward on between $O(1/\\eta)$ and $e^{O(\\eta^{-1})}$ results. That got me a bit thinking and I have a question (below)."
                },
                "questions": {
                    "value": "1) This is something I am pretty sure I could solve on my own with a bit of thinking, but I guess it hints to lack of clarity in some parts of the intro: I find a bit of contradiction between the sentences (1, abstract) \"scale-invariant normalized network, mixes to an equilibrium in\n$O(1/\\eta\\lambda)$ steps, as opposed to classical $e^{O(\\eta^{\u22121})}$ mixing time\" and (2, intro) \"When normalization is used,  the effective learning rate for the renormalized parameter vector will stabilize around $O((\\eta\\lambda^{-1/2} )$ and in consequence $e^{O((\\eta\\lambda)^{-1}}$ is replaced with $e^{O((\\eta\\lambda)^{-1/2}}$\". I think this is a bit unclear, can you please explain? \n\n2) From my SDE knowledge, I always understood that convergence to the stationary distribution is dominated by the drift. What is the convergence rate -- to a local minimum -- if you drop the noise term? I think providing an analysis of this setting will certainly help the reader understand the proof in a simplified setting. \n\n3) The SDE you study certainly is a model for SGD in the setting you study. However, I am a bit worried about the noise structure. Is there some guarantee in previous literature that constant gradient noise in the not-normalized setting translates to the noise projection structure you study in the normalized case? I am talking about formula 4-5 in comparison to the discrete update."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1851/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698842822286,
            "cdate": 1698842822286,
            "tmdate": 1699636115395,
            "mdate": 1699636115395,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0lUmQQLw0R",
                "forum": "qgWJkDiI5p",
                "replyto": "GXk2iXlGw2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1851/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1851/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank you for your valuable comments. Some explanations in response to your remarks are below and we will address these concerns in a revised version accordingly. \n\n0. Response to [\u201c I think you can attract the interest of many people if you have a headline figure showing the speed of convergence to the stationary\u201d]. Thanks for this suggestion. Experiments where the empirically observed rates of convergence are polynomial were contained in the original paper (Li et. al 2020) where the Fast Equilibrium Conjecture was first asked. We will add references to figures from that paper. \n1. Response to Question 1. Thanks for pointing out the unclarity of the writing here. The difference between the polynomial and exponential times is explained in the response 2 below. The difference between $(\\eta\\lambda)^{-1}$ and $(\\eta\\lambda)^{-\\frac12}$, is due to the fact that (Bovier et al., 2004; Shi et al., 2020) studied neural networks without normalization, which involved an analysis on $R^d$. With normalization and weight decay together, (Li et al. 2020) proved that the problem, via the polar coordinate decomposition written as (4) and (5) in our paper, can be considered as a problem on $S^{d-1}$. After this polar decomposition step, the effective learning rate is $O((\\eta\\lambda)^{\\frac12})$, which results in replacements of exponents appearing in convergence times.\n2. Response to Question 2. In this case, the observed convergence to equilibrium is dominated by the diffusion. The drift regulates how fast the trajectories fall near the bottom of a basin (the \u201cdescent stage\u201d), i.e. how fast the loss function value converges. However, the trajectories then start a random walk all around the bottom of the basin. This is the \u201cdiffusion stage\u201d, whose time scale is regulated by the diffusion term. (The third stage takes an exponentially long time when trajectories leave the basin to nearby basins but this cannot be observed in practice because of the exponential time scale.)\n3. Response to Question 3.  Even though some earlier papers assumed a constant gradient noise structure, our current paper does not need to assume constant gradient noise. In fact, equations (4) and (5) were proved in (Li et al. 2022) and didn\u2019t require constant noise structure."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1851/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700091060593,
                "cdate": 1700091060593,
                "tmdate": 1700091060593,
                "mdate": 1700091060593,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ofexroW8MT",
            "forum": "qgWJkDiI5p",
            "replyto": "qgWJkDiI5p",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1851/Reviewer_yR6a"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1851/Reviewer_yR6a"
            ],
            "content": {
                "summary": {
                    "value": "This paper provides a strengthened proof of the fast equilibrium conjecture that was proved in the previous works (Wang &Wang (2022); Li et al. (2022c)) by removing the non-generic assumptions of the unique basin and that the set of minima is an analytic set. In order to remove these additional assumptions, this paper mainly adopts a purely probabilistic method rather than the spectral analysis that was used in the previous works. \n\nToward this goal, this work shows that trajectories would not escape from the initial basin in exponential time."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper is clearly organized and well-structured so that it is easy for the readers to grasp the main contributions of this work.\n2. I like the fact that this work provides solid and well-supported arguments (Remarks 1.4 and 1.5) that Assumptions 1.2 are natural and Assumptions 1.3 are non-generic. These motivate removing these less natural assumptions well and signifies the contribution of this work. \n3. The main result that the Fast Equilibrium conjecture holds without the assumptions of the unique basin and that the set of minima is an analytic set is significant, and it contributes well to the theoretical understanding of the effects of normalization layers. \n4. While I did not check all the proof details, I followed the proof at a high level."
                },
                "weaknesses": {
                    "value": "It could be good to add some theoretical reasonings (in addition to being natural) about why Assumption 1.2 might be essential to prove the Fast Equilibrium conjecture."
                },
                "questions": {
                    "value": "1. Will the noise structure affect the convergence rate?\n2. Would it be possible to achieve a similar result if $L$ had a homogeneity degree  $> 0$?\n3. Are all the remaining three assumptions essential to prove the Fast Equilibrium conjecture? Would it be possible to even weaken the assumptions?\n4. Minor:\nIn the first line in Section 4, \"Assumption 1.3. (ii)\" -> \"Assumption 1.3. (i)\"?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1851/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1851/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1851/Reviewer_yR6a"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1851/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698885333334,
            "cdate": 1698885333334,
            "tmdate": 1699636115306,
            "mdate": 1699636115306,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vGvciZtf6o",
                "forum": "qgWJkDiI5p",
                "replyto": "ofexroW8MT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1851/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1851/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank you for your valuable comments. Some explanations in response to your remarks are below and we will address these concerns in a revised version accordingly. \n\n1. Response to [Question 1]. The noise structure does affect the convergence rate. But as long as Assumption 1.2 is satisfied, the noise structure wouldn\u2019t affect the asymptotic order of the convergence rate. \n2. Response to [Question 2]. This is a very interesting question worth further exploration, but is not covered by our result as we rely on a polar decomposition of SDEs (written as (4), (5) and (6) in the original version, the one before current revision), from (Li et al. 2020)  that depends on the current homogeneity structure.\n3. Response to [Weaknesses] and [Question 3]. You are absolutely right that the conditions in Assumption in 1.2 are essential for our analysis and we will add explanations in a revision. To be precise:\n * Assumption 1.2 (i) is essential because without this assumption, the SDE would not be equivalent to a SDE on the sphere $S^{d-1}$ (written as (6) in the original version, the one before current revision), which is crucial to our analysis. Without this assumption, similar analysis can probably be formulated on $R^d$ instead of the $S^{d-1}$ coordinate but there will be new technical obstacles to overcome. Since the original fast equilibrium conjecture was asked for normalized neural nets, we restrict our study to the current setting. \n * Assumption 1.2 (ii) is important because if not, a trajectory may stay near a critical point (for example a saddle point) for a very long period of time, and thus it would not be able to converge within a polynomial time.  \n* The reason why we need Assumption 1.2 (iii) is that if the span is not the whole tangent space, but instead a subspace of the tangent space, then the diffusion will be restrained to this subspace, which a priori may be very fractal and existing mathematical theory is not sufficient to guarantee a unique equilibrium in limit.\n4. We will fix this typo."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1851/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700090807997,
                "cdate": 1700090807997,
                "tmdate": 1700090807997,
                "mdate": 1700090807997,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YerNW6aQif",
                "forum": "qgWJkDiI5p",
                "replyto": "vGvciZtf6o",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1851/Reviewer_yR6a"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1851/Reviewer_yR6a"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the responses. My questions are addressed, and I keep my rating for this work."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1851/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700695789175,
                "cdate": 1700695789175,
                "tmdate": 1700695789175,
                "mdate": 1700695789175,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Dz8jbIEAgO",
            "forum": "qgWJkDiI5p",
            "replyto": "qgWJkDiI5p",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1851/Reviewer_jkii"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1851/Reviewer_jkii"
            ],
            "content": {
                "summary": {
                    "value": "This paper proves the fast equilibrium conjecture for SGD on neural nets with normalization layers in a more general setting than previous works Li et al., 2022c and Wang & Wang, 2022. Specifically, it shows that the conjecture still holds without the unique basin and analyticity assumptions made in Li et al., 2022c. The theoretical results are further supported by experiments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper is mathematically solid. It extends the conditions for the fast equilibrium conjecture to a more general setting, making a good technical contribution to the community."
                },
                "weaknesses": {
                    "value": "1. Referring to the second experiment as \"stochastic weight averaging\" (SWA) is inappropriate, as SWA averages the model parameters at different iterations along the same trajectory. Conversely, the approach in this paper averages the parameters at the same iterations from different trajectories.\n\n2. Missing discussion of related works, which \n\n   - argue that the iterates stay in the same basin for a significant amount of time when starting from the same initialization. \n     - Frankle, J., Dziugaite, G. K., Roy, D., & Carbin, M. (2020, November). Linear mode connectivity and the lottery ticket hypothesis. In *International Conference on Machine Learning* (pp. 3259-3269). PMLR.\n     - Gupta, V., Serrano, S. A., & DeCoste, D. (2019, September). Stochastic Weight Averaging in Parallel: Large-Batch Training That Generalizes Well. In *International Conference on Learning Representations*.\n\n   - Also analyze the dynamics of SGD / Local SGD near the manifold of minimizers\n     - Damian, A., Ma, T., & Lee, J. D. (2021). Label noise sgd provably prefers flat global minimizers. *Advances in Neural Information Processing Systems*, *34*, 27449-27461.\n     - Gu, X., Lyu, K., Huang, L., & Arora, S. (2022, September). Why (and When) does Local SGD Generalize Better than SGD?. In *The Eleventh International Conference on Learning Representations*.\n\n3. The paper is abstract in its current form. It would be beneficial if the authors could provide specific examples where the removed assumptions may be too restrictive.\n\nBTW, I did not have time to check the proof. So, my final rating will be influenced by the evaluations of the other reviewers."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1851/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699595751147,
            "cdate": 1699595751147,
            "tmdate": 1699636115216,
            "mdate": 1699636115216,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JW2BocLZ11",
                "forum": "qgWJkDiI5p",
                "replyto": "Dz8jbIEAgO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1851/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1851/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank you for your valuable comments. Some explanations in response to your remarks are below and we will address these concerns in the revised version accordingly. \n1. We would change the name of SWA to \u201cweight average over trajectories\u201d.  \n2. We will add the references you pointed out.\n3. The previous assumption of analyticity is restrictive because the regularity of the loss function is decided by that of the activation function. Even though popular activation functions such as Sigmoid are analytic, a priori one could use smooth but not analytic functions. The one basin assumption is restrictive as we do not see empirical evidence of proof that $L$ only has one basin. In fact, the experiments at the end of the paper suggests that there are multiple basins."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1851/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700090147480,
                "cdate": 1700090147480,
                "tmdate": 1700090147480,
                "mdate": 1700090147480,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eXepsK6ygE",
                "forum": "qgWJkDiI5p",
                "replyto": "JW2BocLZ11",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1851/Reviewer_jkii"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1851/Reviewer_jkii"
                ],
                "content": {
                    "title": {
                        "value": "Response acknowledged"
                    },
                    "comment": {
                        "value": "The authors' response has well addressed my concerns. I will keep my positive rating."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1851/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700111736603,
                "cdate": 1700111736603,
                "tmdate": 1700111736603,
                "mdate": 1700111736603,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]