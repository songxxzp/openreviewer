[
    {
        "title": "Retrieving Texts by Abstract Descriptions"
    },
    {
        "review": {
            "id": "GZCdOonYY2",
            "forum": "xWrAAsdKoX",
            "replyto": "xWrAAsdKoX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9195/Reviewer_nR3E"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9195/Reviewer_nR3E"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses the challenge of locating texts in a large document collection based on abstract descriptions of their content. The authors argue that current text embeddings and semantic search solutions are inadequate for this task, as they lack a well-defined notion of similarity. They propose a new model that significantly improves retrieval by utilizing a consistent and well-defined similarity based on abstract descriptions. The model is trained using positive and negative pairs sourced through prompting a LLM. The authors highlight the limitations of existing search techniques, including keyword-based retrieval, dense similarity retrieval, QA-trained dense retrieval, and query-trained dense retrieval. They emphasize the need for a specific type of similarity, referred to as description-based similarity, which captures the relation between abstract descriptions and concrete instances within documents. They demonstrate the effectiveness of their proposed model in retrieving relevant texts based on abstract descriptions and suggest that their approach can enhance knowledge discovery in various data-intensive domains, including legal, medical, and scientific research. Overall, the paper emphasizes the importance of a well-defined similarity measure for effective semantic search and presents a novel approach that leverages the strengths of LLMs to achieve a retrieval task that is not feasible using traditional text generation capabilities."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "They evaluate the effectiveness of their proposed \"Abstract-sim\" model in sentence retrieval based on abstract descriptions, comparing it with several baseline models. The evaluation includes both human and automatic evaluations. For the human evaluation, the researchers conducted a crowd-sourced evaluation of retrieval performance for 201 random descriptions, comparing their model with several strong sentence encoder models. The results of the human evaluation indicate that the \"Abstract-sim\" model outperforms the baselines significantly, with an average of close to 4 out of 5 sentences deemed relevant for the query, while the baseline models had significantly lower performance, ranging between 1.61 to 2.2 sentences. The automatic evaluation was carried out to assess the model's robustness to misleading results. The authors generated a dataset of valid and invalid descriptions, and their model demonstrated superior performance in terms of precision at various retrieval points, with the largest disparity observed at precision@1. Their modell achieved a precision@1 score of 85%, compared to 7~3% for the strongest baseline model. The paper emphasizes the potential of leveraging large language models for generating tailored training datasets, despite their limitations in direct retrieval tasks. Their results indicate that the proposed model, trained on a dataset specifically tailored to the task, performs significantly better than standard sentence-similarity models."
                },
                "weaknesses": {
                    "value": "lack of comparisons with state of the art retrieval models and neural rankings. \n\nGuo, Jiafeng, et al. \"A deep look into neural ranking models for information retrieval.\" Information Processing & Management 57.6 (2020): 102067.\n\nMitra, Bhaskar, and Nick Craswell. \"Neural models for information retrieval.\" arXiv preprint arXiv:1705.01509 (2017)."
                },
                "questions": {
                    "value": "I think the baselines for their algorithms are pretty simple"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9195/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698611862783,
            "cdate": 1698611862783,
            "tmdate": 1699637157127,
            "mdate": 1699637157127,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3v3nWQb4Au",
                "forum": "xWrAAsdKoX",
                "replyto": "GZCdOonYY2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9195/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9195/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "We appreciate your detailed evaluation of our submission and the constructive feedback you provided. The recognition of the improved results we find in both human and automatic evaluations motivates the need to tailor models to our notion of similarity. We agree that this work underscores the potential of utilizing large language models for crafting tailored training datasets. Regarding the issue you raised with the baselines, we appreciate the suggestion to evaluate on additional existing encoders. Please see our general response (\u201cBaselines\u201d) for additional results on 4 existing SOTA models."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9195/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699913555187,
                "cdate": 1699913555187,
                "tmdate": 1699914065907,
                "mdate": 1699914065907,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "F83ZYRnoM7",
            "forum": "xWrAAsdKoX",
            "replyto": "xWrAAsdKoX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9195/Reviewer_Uira"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9195/Reviewer_Uira"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel retrieval task designed to fine sentences that exemplify the ``instance-of'' property related to a given query. To achieve this, the paper constructs a dataset using a large language model and utilizes this dataset to develop a dense retrieval model. Experimental results from a manually constructed dataset demonstrate that the proposed dense retriever outperforms baseline models."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper effectively delineates the problem at hand by highlighting its distinction from existing research. This clear exposition aids readers in comprehending the subject matter. Furthermore, the employment of crowd-workers to curate a new retrieval dataset is commendable, as it promises to significantly benefit future search research."
                },
                "weaknesses": {
                    "value": "The main concerns regarding this paper are:\n\n- While the paper emphasizes that the retrieval based on description-based similarity is different from the existing retrieval, the description-based similarity also belongs to the similarity between texts. Existing methods measure this text similarity through learning from query-document (sentence) pairs, while the proposed method learns from query (description)-sentence pairs. Thus, from this viewpoint, the proposed approach seems to address a specific instance of text-to-text similarity rather than introducing a fundamentally new form of similarity-based search.\n\n- The retrieval in this paper appears specialized in a specific domain and not universally applicable. It would be better to explain in detail the actual application that requires this proposed retrieval.\n\n- While the paper innovates by introducing a new dataset, the retriever itself lacks novelty. Essentially, it is the same as the existing methods that train encoders, previously used in dense retrieval, and subsequently use nearest-neighbor search techniques.\n\n- Recent dense retrieval research has seen the emergence of diverse encoders and similarity techniques, such as Colbert and PLAID. It's necessary for this paper to evaluate the efficacy of its proposed method by incorporating a variety of encoders and similarity metrics in the experiments."
                },
                "questions": {
                    "value": "Q1: Generating data with GPT often leads to the issue of hallucination. How was this tackled in this study?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9195/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698815945778,
            "cdate": 1698815945778,
            "tmdate": 1699637157018,
            "mdate": 1699637157018,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "eYnvXmAJnx",
                "forum": "xWrAAsdKoX",
                "replyto": "F83ZYRnoM7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9195/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9195/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "We appreciate your engagement with our work and would like to address the claim that our proposed approach merely addresses a specific instance of text-to-text similarity rather than introducing a fundamentally new form of similarity-based search. \n\n**The retrieval in this paper appears specialized in a specific domain and not universally applicable.** Indeed, we do not aim to propose a \u201cuniversally applicable\u201d solution, and doubt that such a solution exists. Which current similarity approach would you consider universal in the sense that you mention? The very core of this paper is pointing out major limitations in the corpus-based, \u201cuniversal\u201d notion of similarity, which is ill-defined. We empirically show that models trained without an explicit notion of similarity *fail* to generalize to the very natural use case of retrieving text by abstract description. As you noted, \u201cthe description-based similarity also belongs to the similarity between texts\u201d. We did not claim otherwise. This work is focused on similarity between texts, and our contribution is in introducing and formally defining a unique notion of similarity which is as useful as it is not addressed by current approaches. \n\n**The retriever itself lacks novelty:** We clarify that our primary contribution lies not in the intricacies of modeling but in drawing attention to a critical limitation in existing methodologies. True innovation, in our view, at times does not come from improving conventional modeling techniques. Rather, it involves reevaluation of implicit, unexplored blind spots within existing frameworks. Our work addresses the inadequacy of the current understanding of similarity, which we posit is a substantial and crucial contribution, equal in importance to advancements on the modeling side. We respectfully disagree with the assumption that innovation must come in the form of a new model. Please also refer to the general response for additional explanation about the innovation in this work.\n\n**Different encoders:** Please note that Colbert is a late-interaction model, whose application is orthogonal to our approach. Please see our general response for results on 4 additional SOTA encoders.\n\n**Hallucination in GPT-generated descriptions:** The concern is justified. However, our dataset is not a goal by itself. Rather, it is used to train an encoder, *which we then thoroughly evaluate in both human and automatic evaluation*. As such, even if the dataset contains biases of all sorts (which it probably does, like any other dataset), it is still useful for tackling the problem of enabling search by abstract descriptions. At the same time, please note that as we describe in the paper, we *did conduct an human evaluation study of the generated descriptions*, focusing on their faithfulness to the original text. As we report in the paper this human evaluation indicated that *the descriptions are largely of high quality and are faithful to the text*. Could you please explain if there are unconvincing aspects in our evaluation?"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9195/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699913508587,
                "cdate": 1699913508587,
                "tmdate": 1699913508587,
                "mdate": 1699913508587,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oyhemFMWgS",
                "forum": "xWrAAsdKoX",
                "replyto": "eYnvXmAJnx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9195/Reviewer_Uira"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9195/Reviewer_Uira"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. My primary concern, regarding the novelty of the proposed method for addressing the task, remains unresolved. While the paper mentions how this method differs from existing ones, the actual approach to the solution appears to be merely fine-tuning using the constructed dataset. In light of this concern, I have decided to maintain my original score."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9195/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700735604105,
                "cdate": 1700735604105,
                "tmdate": 1700735604105,
                "mdate": 1700735604105,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Z8dzmu65nB",
            "forum": "xWrAAsdKoX",
            "replyto": "xWrAAsdKoX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9195/Reviewer_s5K1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9195/Reviewer_s5K1"
            ],
            "content": {
                "summary": {
                    "value": "This paper posits that the similarity reflected in embeddings is often ill-defined and inconsistent, which can be suboptimal for various practical use cases. To address this issue, the paper adopts a novel approach. It leverages off-the-shelf large language models to generate multiple descriptions for a given document. Subsequently, it conducts sentence retrieval tasks based on these descriptions to enhance the retrieval task's ability to capture abstract semantic information. As no suitable public dataset is available for the new retrieval settings, the authors introduce a new dataset for training and evaluating their model. The results on this proposed dataset indicate that the trained model outperforms the baselines in both human evaluations and automatic assessments."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper offers a fresh perspective on the traditional retrieval task, highlighting the limitations of term-based and vector-based matching approaches. It introduces a novel description-based matching approach and enumerates its advantages over these traditional methods.\n\n2. To validate the effectiveness of the proposed method, the authors construct a new dataset based on descriptions using Wiki data. They employ off-the-shelf large language models for extensive data collection and annotation, underscoring the rigor and comprehensiveness of their approach.\n\n3. The paper is exceptionally well-written, ensuring that it is easily accessible and comprehensible for readers, making it a valuable contribution to the field."
                },
                "weaknesses": {
                    "value": "1. I totally agree that the term-based and vector-based retrieval frameworks are not perfect and may lead to some problem in practice. However, I wonder that is it really a new of the proposed description-based framework, because as mentioned in the paper, author just modify the dataset and change the meaning of relevance. Moreover, the model used in the paper is also vector-based method.\n\n\n2. Using large language model to generate training dataset is risky in two folds. Firstly, it may not cover all the aspects of a given document form the generated descriptions, so that it may missing some information of the document. Second, it may also contain duplicate aspects of one document so that after model training, some aspects will be strength or biased by the data.\n\n3. It may not a fair comparison between proposed method and baselines. As mentioned in Weakness 1, the meaning of relevance is changed. The proposed method train and evaluate on the same data distribution is evidently better than the model test on OOD distribution."
                },
                "questions": {
                    "value": "1. It is an interesting paper that expand the view of relevance. However, the major concern is that the formulation of description-based framework is also weak and lack some theoretical support, which is the same as the vector-based one. I think how to formulate the description-based relevance is the vital problem in the next version.\n2. How to make sure abstract description is what we actually need in practical search? Some statistic study may be involved as former evidence.\n3. The comparison of proposed method and other baselines should be fairer. Furthermore, some strong dense retrieval baselines should also be involved in the experiments.\n - ANCE: Approximate nearest neighbor negative contrastive learning for dense text retrieval\n - BERM: BERM: Training the Balanced and Extractable Representation for Matching to Improve Generalization Ability of Dense Retrieval\n - TAS-B: Efficiently teaching an effective dense retriever with balanced topic aware sampling.\n - Contriever: Unsupervised dense information retrieval with contrastive learning."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9195/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698885608834,
            "cdate": 1698885608834,
            "tmdate": 1699637156908,
            "mdate": 1699637156908,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Xds5NG4epA",
                "forum": "xWrAAsdKoX",
                "replyto": "Z8dzmu65nB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9195/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9195/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "We appreciate your engagement with our submission and the constructive feedback provided. The recognition of our paper's fresh perspective, novel description-based matching approach, rigor, and comprehensiveness is encouraging. We look forward to addressing the concerns you raised:\n\n**Risks of Using Large Language Models:** The concern you raise is justified. However, our dataset is not a goal by itself. Rather, it is used to train an encoder, *which we then thoroughly evaluate in both human and automatic evaluation*. As such, even if the dataset contains biases of all sorts (which it probably does, like any other dataset, human-generated or not), it is still useful for tackling the problem of enabling search by abstract descriptions. Please note that as we describe in the paper, *we did conduct a human evaluation study of the generated descriptions*, focusing on their faithfulness to the original text. This human evaluation indicated that *the descriptions are largely of high quality and are faithful to the text*. Furthermore, human evaluation is also performed to evaluate the results retrieved by the final model trained on the GPT-generated data, and this model is significantly better than any of the baselines (figure 2). \n\n**Fairness in Comparison with Baselines:** We emphasize that our modification of the dataset is a deliberate choice to highlight the limitations of existing frameworks and address the specific challenges of abstract semantic information retrieval. We see it essential to point out to the fact that the SOTA models fail to capture this notion of similarity, despite them being trained on orders of magnitude more data. In other words, we empirically show the limitations of existing SOTA models in capturing the exact sense of similarity we are arguing for. Please see also \u201cinnovation\u201d under the general response.\n\n**Theoretical Support for Description-Based Relevance and Practical Need for Abstract Descriptions:** Our emphasis is on the conceptual innovation rather than the modeling side: we explicitly state in our introduction that our goal is to introduce a unique form of similarity\u2014description-based similarity. This entails defining a specific notion of retrieval by similarity, and describing its usefulness in certain information-seeking scenarios. Please also refer to the sections \u201cinnovation\u201d and \u201cUse Cases of Description-Based Search\u201d in our general response.\n\n**Inclusion of Strong Dense Retrieval Baselines:** We added 4 SOTA text encoders to the automatic evaluation - please refer to the general response under \u201cBaselines\u201d.\n\n**How to make sure abstract description is what we actually need in practical search?** The idea to perform a user study that empirically evaluates the kind of queries end-users are interested in is highly interesting. Such study has the potential to uncover specific use cases where the ability to search by abstract descriptions is particularly beneficial. We believe, however, that it is beyond the scope of this work: we do discuss in detail several naturally occurring scenarios where such ability is warranted, and the primary focus of our work is to showcase the limitations of existing methods in fulfilling this requirement and to propose a language model-assisted technique to address it. While a user study could offer supplementary insights, we view it as a distinct contribution, separate from the specific goals outlined in our present work."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9195/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699913464915,
                "cdate": 1699913464915,
                "tmdate": 1699913464915,
                "mdate": 1699913464915,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FdyI6cljWM",
                "forum": "xWrAAsdKoX",
                "replyto": "Xds5NG4epA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9195/Reviewer_s5K1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9195/Reviewer_s5K1"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed response. I understand your contribution to the task designing and the benchmark construction. It seems you found some weaknesses in current retrieval models. However, my major concern is the solution to this problem, which just fine-tuned on the proposed dataset does not convince me that it is a hard task for further exploring. \n\nI suggest that if the paper focuses on the task designing and the benchmark construction, you need to leave more space to analyze the recent problem and the cause of the problem rather than the performance comparison.\n\nBased on the major concern mentioned above, I will not change my score."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9195/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700102777315,
                "cdate": 1700102777315,
                "tmdate": 1700102777315,
                "mdate": 1700102777315,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2AdR9WqGS9",
            "forum": "xWrAAsdKoX",
            "replyto": "xWrAAsdKoX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9195/Reviewer_34Co"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9195/Reviewer_34Co"
            ],
            "content": {
                "summary": {
                    "value": "The authors in this paper propose a different search approach by retrieving sentences based on abstract descriptions of their content.  The authors demonstrate the shortcomings of the current methods of text embeddings and propose a metho to improve them. The authors created a dataset using LLMs to capture the notion for similarity and use the same to train an encoder whose representations are better than the state-of-the-art. Specifically, the authors used GPT-3 to generate positive and misleading descriptions for sentences from the English Wikipedia dataset. The authors utilize a pretrained sentence embedding model and fine-tune it with contrastive learning to train their model for the task of aligning sentences with their descriptions. They use two encoders \u2013 one as a sentence encoder and the other as a description encoder. Limitations of the approach were not discussed in the paper."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The authors propose a novel approach to generate abstracts instead of the regular search methods. \n2. The authors have used both human evaluation and automatic evaluation to evaluate the proposed model."
                },
                "weaknesses": {
                    "value": "1. In the abstract, did you mean \u201cinconsistent\u201d instead of \u201cnon-consistent\u201d? In the Introduction, \"This make the\u201d --> \u201cThis makes the\u201d, \"well defined\u201d --> \u201cwell-defined\u201d. There are several such grammatical errors, and it would benefit the authors to run the text through any of the free grammar tools available. Also, the authors can recheck the camel cases of sentences and sub-headers (full stop or no full stop?).\n2. What are the different use cases of the proposed description-based search in documents? The authors can discuss some different case studies or use cases to convince readers."
                },
                "questions": {
                    "value": "-"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9195/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699290513681,
            "cdate": 1699290513681,
            "tmdate": 1699637156778,
            "mdate": 1699637156778,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yGPD8vg0ft",
                "forum": "xWrAAsdKoX",
                "replyto": "2AdR9WqGS9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9195/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9195/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "We appreciate your review of our submission and the constructive feedback provided. We highly value the recognition of our proposed novel approach to retrieval by description, which is distinct from regular search methods. Additionally, we appreciate the acknowledgment of our comprehensive evaluation strategy, which include both human and automatic evaluation. \n\n**Grammatical Errors:** Thanks for pointing out grammatical errors. We apologize for any oversight in our proofreading. In our revised manuscript, we addressed the issues you raised, and we will revise the manuscript again for the CR version.\n\n**Use Cases of Description-Based Search:** Please refer to the general response under \u201cUse Cases of Description-Based Search\u201d and \"Innovation\" for a detailed response."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9195/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699913444542,
                "cdate": 1699913444542,
                "tmdate": 1699913826515,
                "mdate": 1699913826515,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cojbNUXIEy",
                "forum": "xWrAAsdKoX",
                "replyto": "yGPD8vg0ft",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9195/Reviewer_34Co"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9195/Reviewer_34Co"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for responding to the question raised in the weaknesses. I have read through the general response and while I am not thoroughly convinced with the use cases (the ones mentioned in the response are generic, I was hoping to see something more specific, like a case study), I will retain my score because the work is novel and can contribute to the domain with some minor changes addressed."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9195/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699971630776,
                "cdate": 1699971630776,
                "tmdate": 1699971630776,
                "mdate": 1699971630776,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "L70NimZKKO",
            "forum": "xWrAAsdKoX",
            "replyto": "xWrAAsdKoX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9195/Reviewer_SM29"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9195/Reviewer_SM29"
            ],
            "content": {
                "summary": {
                    "value": "The paper defines a new task that retrieves text based on abstract descriptions. The specific kind of similarity between text and abstract description are defined and hand curated examples were used in the instructions to LLM to generate training data. The proposed method works better than other sentence/text retrievers trained with the general definition of sentence similarity on the test data designed for this task."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The text and abstract description similarity is a very interesting type of similarity and would value the information retrieval field. I think the strength of the paper is to design the prompts to gather the text/description pairs that satisfy the definition."
                },
                "weaknesses": {
                    "value": "The paper proposed an interesting new task. I'm confident it will be useful for some application or existing retrieval applications. However, the paper didn't explore what will benefit from this new task as much.\n\nAlso, I would think it is pretty straight forward to see that the proposed method would outperform a general purpose retrieval or sentence similarity model. Those method are not trained or finetuned using the same training data, which defines the relationship of sentence and its abstraction."
                },
                "questions": {
                    "value": "Questions:\nHow is precision @k decreases when k is increasing, especially for the proposed method?\n\nSuggestions:\nI think this is a interesting task with value, but I think it is worth to explore what end task would be benefit from this new task, or how this task post challenges to existing retrieval models, if any.\n\nMinor typos:\n  - page 8. Settings. \"invalid-recall@k\" is missing the @ sign."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9195/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699510801624,
            "cdate": 1699510801624,
            "tmdate": 1699637156662,
            "mdate": 1699637156662,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SLvlCXDUzH",
                "forum": "xWrAAsdKoX",
                "replyto": "L70NimZKKO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9195/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9195/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "We appreciate the insightful feedback you provided, particularly the acknowledgment of the description-based similarity as an interesting type of similarity and its potential value to the information retrieval field. We refer to the specific issues you raised:\n\n**Exploration of Applications:** We appreciate your acknowledgment of the interesting nature of our proposed task but recognize the need for a more detailed exploration of its potential applications. Please refer to the general response under \u201cUse Cases of Description-Based Search\u201d and under \u201cInnovation\u201d for a detailed response.\n\n**Expected Outperformance of the Proposed Method:** Our emphasis is on drawing attention to the need to retrieve by description, and showcasing the limitations of existing encoders in this task. We find it essential to test models on our data to showcase the limitations of SOTA general-purpose encoders in capturing the notion of similarity we argue is useful. Their failure is a nontrivial finding, as these models were trained on orders of magnitude more data. It is not possible to highlight this inherent limitation of existing models without evaluating them on data tailored for our notion of similarity. Please refer also to our explanation under \u201cinnovation\u201d in the general response. \n\n**How is precision @k decreases when k is increasing?** Indeed,as we note in the paper, the improvement over the baselines is smaller when k increases. Our hypothesis is that this is a by-product of the standard contrastive learning objectives we employ, which focus on the top result, and do not directly optimize the tail of the distribution (in other words, the loss takes into account a single positive at a time). We agree this is an interesting phenomenon to study in future work (particularly - can we design contrastive learning methods that take the tail of the distribution into account?)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9195/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699913409926,
                "cdate": 1699913409926,
                "tmdate": 1699948584786,
                "mdate": 1699948584786,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]