[
    {
        "title": "Plan-Seq-Learn: Language Model Guided RL for Solving Long Horizon Robotics Tasks"
    },
    {
        "review": {
            "id": "kyfOLi0oSD",
            "forum": "hQVCCxQrYN",
            "replyto": "hQVCCxQrYN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2911/Reviewer_oGjq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2911/Reviewer_oGjq"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose Plan-Seq-Learn (PSL) to address long-horizon robotics tasks from scratch with a modular approach using motion planning to bridge the gap between abstract language and low-level control learned by RL. The authors experiment with 20+ single and multi-stage robotics tasks from four benchmarks and report success rates of over 80% from raw visual input, out-performing previous approaches."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "originality\nThe authors propose PSL: 1) breaks up the task into sub-sequences (Plan), 2) uses vision and motion planning to translate sub-sequences into initialization regions (Seq), 3) train local control policies using RL (Learn).\n\nquality\nExperiments show that the proposed method outperforms previous methods in simulation.\n\n\nclarity\nThe paper is basically well-organized and clearly written.\n\nsignificance\nAs an LLM-based approach, the authors have made some progress."
                },
                "weaknesses": {
                    "value": "It is a paper about robotics. However, experiments are based on simulations only.\n\nIt is about long horizon robotics tasks. However, the largest number of stages is 5 in the experiments. \n\nFrom the perspective of long horizon robotics tasks, it is not clear how the method may proceed forward. See detail below."
                },
                "questions": {
                    "value": "1.\n\"Large Language Models (LLMs) are highly capable of performing planning for long-horizon robotics tasks\"\nThis is very arguable. There are evidences that it is not the case. And if it is so, there is no need to write the paper.\n\nSee e.g., \nOn the Planning Abilities of Large Language Models--A Critical Investigation, 2023.\nReasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks, 2023\n\n2.\n\"Language models can leverage internet scale knowledge to break down long-horizon tasks (Ahn et al., 2022; Huang et al., 2022a) into achievable sub-goals\"\nHow valid is such claim? Why is it so? What if the tasks are not available or not frequent in Internet texts?\n\nHow can we guarantee the decomposition of tasks always work?\nWhat if it does not work?\nHow can we guarantee the optimality of the decomposition of tasks?\nWhat if it is not optimal?\nThe current work uses simulations to validate the proposed method. There will be sim2real gap. How to bridge such gap? \n\nHow to improve the current work? If there is something wrong in the task decomposition stage, it is hard or impossible to make improvements, and a pre-trained or fine-tuned LM may be called for. It is beyond the current work. However, the point is, it is not clear how the proposed method deal with such issues. \n\n\n3.\nEnd-to-end vs hierarchical approaches, there are tradeoffs. The paper focuses on the advantages of hierarchical approaches and disadvantages of end-to-end approaches. Desirable to discuss from both sides.\n\n4.\n\"This simplifies the training setup and allowing the agent to account for future decisions as well as inaccuracies in the Sequencing Module.\" \nFor some mistakes at the higher level, a lower level RL can not deal with.\n\n5.\nTable 2, Multistage (Long-horizon) results. 5 stages are not quite long-horizon, and the success rate may be as low as .67 \u00b1 .22"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2911/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2911/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2911/Reviewer_oGjq"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2911/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698539151734,
            "cdate": 1698539151734,
            "tmdate": 1699636234622,
            "mdate": 1699636234622,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mrcs8us9FE",
                "forum": "hQVCCxQrYN",
                "replyto": "kyfOLi0oSD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2911/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2911/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer oGjq Part 1"
                    },
                    "comment": {
                        "value": "We thank the reviewer for providing a detailed review and for appreciating the strength of our experimental results and quality of writing. \n\n**\u201cIt is a paper about robotics. However, experiments are based on simulations only.\u201d**\n\nIn this work, we focus on studying the question: how can we train policies to solve long-horizon robotics tasks? Our main contributions are an algorithm for guiding RL agents for learning low-level control using LLMs via motion planning and a series of insights for practically training such policies. To study this question effectively, we perform extensive empirical evaluations on 25 tasks across 4 evaluation domains, validating the strength of our method on established benchmark tasks for comparison. We leave extensions to the real world for future work and briefly discuss two possible directions for doing so: 1) sim2real transfer by training local policies in simulation and chaining them using motion planning and LLMs at test time 2) directly running PSL in the real world as it is far more efficient to train than E2E methods.\n\n**\u201cIt is about long horizon robotics tasks. However, the largest number of stages is 5 in the experiments.\u201d**\n\nWe emphasize that even solving tasks with up to 5 stages is quite difficult for end-to-end based methods; they make no progress at all in most cases beyond 1-2 stages. Furthermore, we only evaluate tasks with up to 5 stages, as a majority of the benchmarks for robotic control have only up to that many stages, not because our method cannot be applied beyond 5 stages. As shown in the experiments section, prior methods such as E2E [1], RAPS [2], MoPA-RL [3], TAMP [4] and SayCan [5] do not reliably solve the benchmark tasks with up to 5 stages. In contrast, for our method, whether we have 1, 5 or even 10 stages (we include these new results at the end of this response), we can still solve the task because our modular, hierarchical method effectively decomposes the task and simplifies the learning problem significantly. \n\n**\u201c\"Large Language Models (LLMs) are highly capable of performing planning for long-horizon robotics tasks\" This is very arguable. There are evidences that it is not the case. And if it is so, there is no need to write the paper.\u201d**\n\nWe have toned down this point in the updated version of the paper by instead stating that LLMs _have been shown to be_ capable of _high-level_ planning. Our changes are shown here in italics.\n\nWhile LLMs perform poorly on general purpose planning as noted in the work cited by the reviewer [6], in our work the LLM is not required to perform general purpose or fine-grained planning. Instead, in PSL, the LLM is only outputting a very _coarse_ high-level plan - where to go and how to leave the region - which is simple and does not require significantly complex reasoning ability. For such tasks, we find that the semantic, internet-scale knowledge in LLMs is sufficient to produce high-quality plans. Empirically, on the tasks we consider, the LLM achieves 100% planning performance. \n\nWe additionally note that our method is not necessarily tied to using an LLM as a task planner. We can also use classical task planners such as STRIPS [7] and use an LLM to simply translate the natural language prompt into a format for task planning as done in LLM + P [8]. In this way, we inherit the guarantees and benefits of classical task planners while guiding the RL agent to efficiently solve the task from a natural language task description. \n\n**\u201c\"Language models can leverage internet scale knowledge to break down long-horizon tasks (Ahn et al., 2022; Huang et al., 2022a) into achievable sub-goals\" How valid is such claim? Why is it so? What if the tasks are not available or not frequent in Internet texts?\u201d**\n\nWe have re-written this statement to be appropriately qualified: \u201c_Prior work (...) has shown that when appropriately prompted,_ language models _are capable of leveraging_ internet scale knowledge\u2026\u201d There is a large body of recent work [5, 9, 10, 11, 12] in this area that empirically illustrates such capabilities for long-horizon robotics tasks. We have updated the main paper with this change. Our changes are shown here in italics.\n\nHowever, we emphasize the existence (or lack thereof) of general purpose planning capabilities of LLMs is orthogonal to the claims of our paper. Our focus, with respect to the Plan Module, is on a simple, coarse planning interface for the LLM with demonstrably high performance across a wide range of robotics tasks."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2911/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700260478632,
                "cdate": 1700260478632,
                "tmdate": 1700260478632,
                "mdate": 1700260478632,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CbR1EUDhTP",
                "forum": "hQVCCxQrYN",
                "replyto": "3Wxox7L4pv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2911/Reviewer_oGjq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2911/Reviewer_oGjq"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for authors' careful rebuttal to reviews and update. \nHowever, there are still concerns if the paper follows a promising approach.\n\nOne example:  from the rebuttal, \"If the LLM outputs the incorrect plan, we hypothesize that the agent will default to performing as well as (or perhaps slightly worse than) E2E [1]. To evaluate this, we ran an experiment to evaluate the performance of PSL when provided the incorrect high-level plan. Please see the response to Reviewer KErm in which we describe the experiment in detail.\"\n\nIf so, what is the meaning of a high-level plan?\nIt appears that the authors want to use LLM even LLM may not generate good high-level plans.\n\nI will keep the score."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2911/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700663765661,
                "cdate": 1700663765661,
                "tmdate": 1700663765661,
                "mdate": 1700663765661,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "80PQ7uL9UX",
            "forum": "hQVCCxQrYN",
            "replyto": "hQVCCxQrYN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2911/Reviewer_KErm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2911/Reviewer_KErm"
            ],
            "content": {
                "summary": {
                    "value": "In past works, people utilized LLM's internet-scale of knowledge to give robots sufficient information when planning for long-horizon tasks. However, the author believes that it is important for a robotic system to be capable of online improvement over at least low-level control policies at the same time. Otherwise, with the lack of a library for pre-trained skills in every other scenario, robots aren't able to learn very well. To this end, the paper proposes a framework, PLAN-SEQ-LEARN, that utilizes both LLM's ability to guide agent's planning and RL's ability for online improvement. The experiments show that not only did PSL's performance surpass SOTA visual-based RL methods through the help of LLM, but it also performed better than SayCan for its ability to improve with online learning."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "**Motivation and intuition**\n- Classical approaches to long-horizon robotics that can struggle with contact-rich interactions are convincing.\n- Use LLM for high-level planning guiding RL policy to solve robotic tasks online without pre-determined skills.\n\n\u200b\n**Novelty**\n- The idea of utilizing RL to learn low-level skills under the framework of LLM planning is intuitive and convincing.\n\u200b\n\n**Technical contribution**\n- Integrates LLM task planning, motion planning, and RL techniques.\n- Avoid cascading failures by learning online using RL algorithms.\n\n\u200b\n**Clarity**\n- The overall writing is clear. The authors utilize figures well to illustrate the ideas. Figure 2 clearly shows the whole idea of PSL.\n- This paper provides a clear and detailed description of how to integrate the task planning module, motion planning module, and RL learning module.\n\n**Related work**\n- Give plenty of related works with short but clear descriptions.\n\u200b\n**Experimental results**\n- The overall performance on single-stage and multistage benchmark tasks is good."
                },
                "weaknesses": {
                    "value": "**Clarity**\n- Although details of how LLM was used were clearly written inside Appendix D, I feel like the author could illustrate the details in the main paper and also a better explanation of how stage termination and training details are implemented. Since how LLM was involved in this work seems to be one of the contributions of this paper, I do feel like making this part intuitive is a must.\n\n\u200b\n**Method**\n- Trade-off: Planning without a library of pre-defined skills is mentioned as a strength in the paper, but this comes at the cost of relearning the whole process compared to other methods.\n- Also the paper seems to overlook the fact that the learning might fail. Did not see how the method handles this situation.\n- What would the PSL react to the situation when the agent failed to reach the termination condition?\n- What would happen if there are more than enough terms for the LLM to choose from, for example, unlearnable skill terms that may confuse the LLM in choosing?\n\n\u200b\n**Related work**\n- Although Citing 'Inner Monologue' and 'Bootstrap Your Own Skills(BOSS)', they are not used for comparison or experiments, as these methods share many similarities. Therefore, it's a bit of a missed opportunity.\n\n\u200b\n**Experimental conclusions**\n- In section 4.3, the author noted that \"For E2E and RAPS, we provide the learner access to a single global fixed view observation from O^global for simplicity and speed of execution, as we did not find meaningful performance improvement in these baselines by incorporating additional camera views.\". However, this results in an unfair comparison because PSL has taken O^local as an additional input, and may cause some questionable issues. If performances are similar, I believe that adding O^local for E2E and RAPS would result in a more convincing conclusion that PSL performs better."
                },
                "questions": {
                    "value": "As stated above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2911/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2911/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2911/Reviewer_KErm"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2911/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698597943169,
            "cdate": 1698597943169,
            "tmdate": 1700662126037,
            "mdate": 1700662126037,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0Xx76PKKMN",
                "forum": "hQVCCxQrYN",
                "replyto": "80PQ7uL9UX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2911/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2911/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer KErm Part 1"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their detailed review and for recognizing the clear motivation for PSL, novelty of our method, clarity of writing and strength of our experimental results. \n\n**\u201cI feel like the author could illustrate the details in the main paper and also a better explanation of how stage termination and training details are implemented.\u201d**\n\nWe have updated the discussion in Section 3.3 of the main paper to include additional details regarding the stage termination and LLM planning implementation details. We have also included further details in Section 3.5 of the main paper. Additionally, we emphasize we will release the code for PSL upon acceptance - enabling the community to replicate our results. The code will unambiguously specify the requested implementation details.\n\n**\u201cAlthough Citing 'Inner Monologue' and 'Bootstrap Your Own Skills(BOSS)', they are not used for comparison or experiments, as these methods share many similarities.\u201d**\n\nFor our experiments, the high-level planning success rate is 100% - the bottleneck is performing effective low-level control. To that end, prompting techniques such as Inner Monologue [1] would not affect the performance. Inner Monologue would achieve the same results as SayCan [2]. Furthermore, Inner Monologue could be readily incorporated into PSL to improve planning performance when necessary; we leave this extension to future work.\n\nWith regards to BOSS [3], as we note in the paper, this is concurrent work with our own. It was released on Arxiv on October 16, 2023 - after the ICLR submission deadline. That notwithstanding, there are several reasons why comparisons to BOSS is currently infeasible: 1) The code for BOSS is not released and re-implementing the method is non-trivial as it requires training policies online using IQL in the loop with an LLM. Once the code is released, we will attempt to perform a fair comparison if possible. 2) Their method operates with a different assumption set than ours: existence of a pre-trained skill library while we evaluate training from scratch to learn unseen low-level skills 3) BOSS specifically uses a language labeled demonstration dataset to pre-train skills - no such dataset exists on most of the tasks we evaluate. Furthermore, the environment code for BOSS is not publicly available either - they use a \u201cmodified version of the ALFRED [4] simulator\u201d which is not released to our knowledge. Finally, we would like to note that the contribution of BOSS is orthogonal to our own, our method focuses on learning to efficiently solve a single task while BOSS aims to expand a pre-existing repertoire of skills. In principle PSL can be combined with BOSS to efficiently learn and incorporate a new skill into an existing library, particularly when starting from any empty skill set.\n\n**\u201cTrade-off: Planning without a library of pre-defined skills is mentioned as a strength in the paper, but this comes at the cost of relearning the whole process compared to other methods.\u201d**\n\nIn our experiments, we show that learning policies from scratch can outperform methods that use pre-trained/defined skills such as SayCan [2] and RAPS [5] by over 2x in terms of raw success rate. Online learning enables the agent to adapt its low-level control to the task it is solving while avoiding cascading failures. However, we acknowledge that pre-trained skill libraries come with many practical benefits and in principle PSL can also take advantage of and fine-tune pre-defined skills as we discussed above. We leave this extension to future work. Ultimately, we agree with the reviewer that it is desirable to combine online learning with pre-defined skills, however, in this work our aim was to study and improve the learning process in isolation from pre-defined skills."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2911/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700260079474,
                "cdate": 1700260079474,
                "tmdate": 1700260079474,
                "mdate": 1700260079474,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "w8TFZufwiu",
                "forum": "hQVCCxQrYN",
                "replyto": "U89CGChFLg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2911/Reviewer_KErm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2911/Reviewer_KErm"
                ],
                "content": {
                    "title": {
                        "value": "Response to Author Rebuttal"
                    },
                    "comment": {
                        "value": "I have carefully read the reviews submitted by other reviewers, and the rebuttal and the revised paper provided by the authors. I appreciate the efforts put into answering my questions and improving this submission. In that regard, I am raising my score to 8."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2911/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700662111262,
                "cdate": 1700662111262,
                "tmdate": 1700662111262,
                "mdate": 1700662111262,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "XH8omrvJnd",
            "forum": "hQVCCxQrYN",
            "replyto": "hQVCCxQrYN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2911/Reviewer_ue89"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2911/Reviewer_ue89"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new method/framework called Plan-Seq-Learn (PSL) for solving long-horizon robotics tasks. The key idea is a decomposition of long robotic manipulation tasks, and then tackle each part using a reasonable method. Specifically, they combine LLM for highly abstract task planning, off-the-shelf visual pose estimator and motion planner (AIT*) for sequencing each sub-tasks, and RL for the sub-tasks. This allows PSL to leverage the advantage of each module. Extensive experiments show PSL can efficiently solve 20+ long-horizon robotics tasks, outperforming prior methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The approach is sensible and reasonable to leverage current popular methods for robot learning - LLM for high-level planning, classical motion planner for efficient collision-free path planning, and RL for the contact-rich manipulation stage.\n2. The general framework is novel in combining these techniques although each part is not entirely new. And the paper clearly explains how to use their advantages in solving long-horizon tasks.\n3. Extensive experiments show reasonable/good results regarding their claims and methods."
                },
                "weaknesses": {
                    "value": "1. The long-horizon task seems to be divided into only 'grasp' and 'place' (from the paper and appendix), it is unclear if there are more sub-tasks / skills that the LLM divided. From the webpage, I find other tasks besides the pick-and-place series so wonder how to implement these."
                },
                "questions": {
                    "value": "1. As the whole task is decomposed into stage 1, sequencing, stage 2, ..., stage n. Does it need to redesign the reward function of the RL process? Moreover, how does the sparse and dense reward influence the learning process?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2911/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2911/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2911/Reviewer_ue89"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2911/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698754200915,
            "cdate": 1698754200915,
            "tmdate": 1699636234441,
            "mdate": 1699636234441,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nXvo7eHvTx",
                "forum": "hQVCCxQrYN",
                "replyto": "XH8omrvJnd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2911/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2911/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ue89"
                    },
                    "comment": {
                        "value": "We thank the reviewer for recognizing the novelty of our modular framework and our extensive experiments as well as for appreciating the strengths of our results on a wide range of tasks and domains.\n\n**\u201cThe long-horizon task seems to be divided into only 'grasp' and 'place' (from the paper and appendix), it is unclear if there are more sub-tasks / skills that the LLM divided. From the webpage, I find other tasks besides the pick-and-place series so wonder how to implement these.\u201d**\n\nTo clarify, PSL is not limited to only using \u2018grasp\u2019 and \u2018place\u2019 termination conditions. In general, it can take advantage of any stage termination condition when performing LLM planning. The RL agent can then learn the corresponding local control policies. We simply require the following: a function that takes in the current state or observation(s) of the environment and evaluates a binary success criteria as well as a natural language descriptor of the condition for prompting the LLM (_e.g._ \u2018grasp\u2019 or \u2018place\u2019). Then the LLM can subdivide the task based on these conditions. We have updated the paper to make this point clear.\n\nFurthermore, in our experimental results, we do not only use \u2018grasp\u2019 and \u2018place\u2019 conditions. We also use conditions such as \u2018push\u2019 (OS-Push), \u2018open\u2019 (RS-Door, K-Microwave, K-Slide) or \u2018turn\u2019 (K-Burner). These conditions can be readily estimated using vision: they are all dependent on pose estimates. We have updated the paper to include detailed descriptions of how to estimate pushing, opening and turning. Finally, we refer the reviewer to our reply to Reviewer KErm, in which we experimentally show that our method is not dependent on the user specifying exactly the termination conditions that are necessary for the task. We find that providing a superset of termination conditions will also work; the LLM will only output sub-tasks that are necessary for solving the given task. \n\n**\u201cDoes it need to redesign the reward function of the RL process?\u201d**\n\nAs we note in Section 4.3 of the main paper, we do not modify the reward function of the environment for any task. Instead, we use the Plan and Sequence modules to move the RL agent to relevant regions of space and specify conditions for exiting those regions (stage termination conditions). The RL agent then learns local interaction based on the overall task reward. \n\n**\u201cMoreover, how does the sparse and dense reward influence the learning process?\u201d**\n\nOur experiments include results on dense (Robosuite [1], Metaworld [2], Obstructed Suite [3]) and sparse (Kitchen [4]) reward tasks. We find that PSL performs well in both settings. One reason why PSL is capable of effectively solving sparse reward tasks is that it addresses one major component of the exploration problem in sparse settings: finding the object with which it needs to interact. By initializing the RL agent close to the region of interest, we greatly increase the likelihood that random exploration leads to coincidental successes which can be used to bootstrap learning. \n\n[1] Y. Zhu, J. Wong, A. Mandlekar, and R. Martin-Martin. robosuite: A modular simulation framework and benchmark for robot learning. arXiv preprint arXiv:2009.12293, 2020.\n\n[2] T. Yu, D. Quillen, Z. He, R. Julian, K. Hausman, C. Finn, S. Levine. \"Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning.\" Conference on Robot Learning, pages 1094\u20131100, PMLR, 2020.\n\n[3] J. Yamada, Y. Lee, G. Salhotra, K. Pertsch, M. Pflueger, G. S. Sukhatme, J. J. Lim, P. Englert. \"Motion Planner Augmented Reinforcement Learning for Obstructed Environments.\" Conference on Robot Learning, 2020.\n\n[4] J. Fu, A. Kumar, O. Nachum, G. Tucker, S. Levine. \"D4RL: Datasets for Deep Data-Driven Reinforcement Learning.\" arXiv preprint arXiv:2004.07219, 2020."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2911/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700259974426,
                "cdate": 1700259974426,
                "tmdate": 1700259974426,
                "mdate": 1700259974426,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xfz5rVocGR",
                "forum": "hQVCCxQrYN",
                "replyto": "XH8omrvJnd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2911/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2911/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal Followup"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nWe would like to follow up on our rebuttal as there is only one day remaining of the discussion period. If there are any outstanding concerns that you would like us to address, please let us know. Thank you and we look forward to your response."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2911/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700585325099,
                "cdate": 1700585325099,
                "tmdate": 1700585356579,
                "mdate": 1700585356579,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]