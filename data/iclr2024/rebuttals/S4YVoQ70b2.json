[
    {
        "title": "Characterising Partial Identifiability in Inverse Reinforcement Learning For Agents With Non-Exponential Discounting"
    },
    {
        "review": {
            "id": "IR5apZb4YT",
            "forum": "S4YVoQ70b2",
            "replyto": "S4YVoQ70b2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3737/Reviewer_HKSo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3737/Reviewer_HKSo"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores the challenge of partial identifiability in Inverse Reinforcement Learning (IRL) under the condition of non-exponential discounting. Specifically, it focuses on a hyperbolic discounting model, which is characterized by temporal inconsistency, thereby inducing non-stationarity in the underlying Markov Decision Process (MDP).\n\nTo address the effects of this temporal inconsistency, the paper proposes a series of behavioral models: the resolute policy, the naive policy, and the sophisticated policy. For each of these models, the paper succinctly summarizes their properties, encompassing the uniqueness of the optimal value function, the stochastic nature of the policy, and the stationarity of the policies across varying time steps.\n\nInterestingly, the paper defines the identifiability of reward functions in relation to the optimal policy under an exponential discounting setting. This appears contradictory to the paper's main focus on non-exponential discounting.\n\nThe theoretical findings indicate that no regularly resolute, regularly naive, or regularly sophisticated behavioral model is identifiable under non-exponential discounting or a non-trivial acyclic transition function. These results suggest that IRL is incapable of inferring sufficient information about rewards to identify the correct optimal policy. Consequently, it is implied that IRL alone is insufficient to thoroughly characterize the preferences of such agents."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The matter of identifiability in Inverse Reinforcement Learning (IRL) under a non-exponential discounting setting has yet to be explored in prior studies.\n\n2. The paper's overall structure is logically organized and easily navigable. Definitions are meticulously presented, supplemented with numerous intuitive examples to facilitate reader understanding of the core content.\n\n3. The theoretical findings are well presented, thereby supporting the claims made in the paper."
                },
                "weaknesses": {
                    "value": "1. It's challenging to comprehend the concept of sophisticated policy as delineated in Definition 7. For instance, it's unclear why the policy, $\\pi(\\xi)$, is not dependent on the time step and how it correlates with step-wise policies. Similarly, it's puzzling why the Q function $Q^\\pi(\\xi,a)$ is also independent of the time step. Given that the optimal policy can vary at each time step, it becomes complex to determine which strategy exhibits more \"sophistication\". In many Markov Decision Processes (MDPs), the so-called sophisticated policy is not singular. The paper states that \"$\\pi$ is sophisticated if it only takes actions that are optimal given that all subsequent actions are sampled from $\\pi$.\" Could you clarify this definition? Specifically, I'm interested in understanding how one would define optimality in a non-stationary MDP that spans across different (or all) time steps.\n\n2. The definition of identifiability appears to be founded on an exponentially discounted MDP, even though the paper focuses on a non-exponentially discounted setting. The paper attempts to provide some intuitive explanations, but they fall short in terms of persuasiveness. If the term 'optimality' has a clear definition under different behavior models, then the term 'identifiability' should also exhibit the capacity to characterize these models.\n\n3. This paper lacks empirical studies to substantiate its arguments. The main results suggest that IRL alone may be inadequate to fully characterize the preferences of agents in a non-exponentially discounted setting. However, a potential solution has not been proposed, and it is yet unclear how the existence of non-identifiability impacts empirical performance. It would be beneficial to see these points addressed in future research."
                },
                "questions": {
                    "value": "1. why the policy, $\\pi(\\xi)$, is not dependent on the time step and how it correlates with step-wise policies?\n2. why the Q function $Q^\\pi(\\xi,a)$ is also independent of the time step ?\n3. How to understand the \"sophisticated policy\"?\n4. how the existence of non-identifiability impacts empirical performance?\n5. What potential solutions could address the issue of non-identifiability in IRL?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3737/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698337910999,
            "cdate": 1698337910999,
            "tmdate": 1699636329825,
            "mdate": 1699636329825,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4GWVc11Iah",
                "forum": "S4YVoQ70b2",
                "replyto": "IR5apZb4YT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3737/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3737/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "We thank reviewer HKSo for their review! Our responses to your questions are as follows:\n\nWeaknesses:\n1. To say that a policy is sophisticated is essentially to say that it satisfies a certain stability criterion --- namely, if $\\pi$ is sophisticated, then the agent never has any (local) incentive to deviate from $\\pi$. One way to understand this is that a sophisticated policy is somewhat analogous to a Nash equilibrium, if the agent at each time step is thought of as a separate decision maker (which is justified by the fact that its preferences are not temporally consistent). Another way to understand it is that if a policy $\\pi$ is updated using policy gradients, then a sophisticated policy will be a sort of local optimum (though it may not necessarily be an attractor point, in the same way as a Nash equilibrium may not be an attractor point). This also means that it is not always clear how to compare sophisticated policies (again, in the same way as how it is not always clear how to select between Nash equilibria); a policy is simply either sophisticated or not. Thus, sophisticated policies do not give us an unambiguous notion of \"optimality\" (though they are nonetheless a solution concept).\n2. We have choosen to quantify identifiability in terms of exponentially discounted optimal policies, because even if humans discount hyperbolically, we typically want to create systems that discount exponentially --- this is why exponential discounting is vastly more common in the RL literature. Therefore, we think the most relevant formalisation is to assume that the observed demonstrator discounts hyperbolically, but that the policy which will be computed with the learnt reward function discounts exponentially. Other choices could be made instead, which might lead to different results. Of course, some alternative formalisations would end up being quite trivial. For example, if $P$ is the equivalence relation under which $R_1 \\equiv_P R_2$ iff $R_1$ and $R_2$ have the same sophisticated policies, and $f : \\mathcal{R} \\to \\Pi$ returns a maximally supportive sophisticated policy, then it is immediate from the definition that $f$ is $P$-identifiable, etc. This setup would thus not allow for the derivation of any \"deep\" results.\n3. We agree that experimental evaluations could be interesting, but we consider this to be out of scope for this paper. Similar recent works on identifiability in IRL are also typically theoretical, rather than experimental (see Dvijotham & Todorov, 2010; Cao et al., 2021; Kim et al., 2021; Skalse et al., 2022; Schlaginhaufen & Kamgarpour, 2023; Metelli et al., 2023). Note that the empirical performance of an IRL algorithm for hyperbolic discounting will be heavily dependent on the prior distribution from which the ground truth reward function is sampled, which limits the generalisability of such results. In particular, for any inductive bias of the learning algorithm there exists a prior distribution for the ground truth reward such that the counterexamples identified by our theorems will occur with high probability.\n\nQuestions:\n1. Note that $\\xi$ is a trajectory --- this means that $\\pi(\\xi)$ *does* depend on the time step, since it may be sensitive to the length of $\\xi$.\n2. Again, note that $\\xi$ is a trajectory, rather than a state.\n3. See above.\n4. See above.\n5. We think that the most promising solutions will rely on incorporating prior information, or data from other data sources. However, exploring these options is beyond the scope of this paper.\n\nWe hope that this clarifies our analysis, and that the reviewer might consider increasing their score!"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3737/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699888519801,
                "cdate": 1699888519801,
                "tmdate": 1699888519801,
                "mdate": 1699888519801,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UDJ4heEROW",
                "forum": "S4YVoQ70b2",
                "replyto": "4GWVc11Iah",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3737/Reviewer_HKSo"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3737/Reviewer_HKSo"
                ],
                "content": {
                    "comment": {
                        "value": "I am still struggling with the term \"identifiability\" under the non-stationary policy and MDP. Since all the policies are locally optimal, are the rewards also locally identifiable? or since we consider an exponentially discounted MDP when we define the term \"identifiability\", we can utilize the corresponding time consistency to define globally identifiable rewards.\n\nAs for the assumption \"assume that the observed demonstrator discounts hyperbolically, but that the policy which will be computed with the learnt reward function discounts exponentially.\". I don't think such an assumption is intuitive and well justified, especially when it utilizes time-consistent policies (under exponential discounting) to explain the time-inconsistent (under hyperbolical discounting) behaviors. Without empirical results, it is hard to evaluate the effect of such an assumption."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3737/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700389035087,
                "cdate": 1700389035087,
                "tmdate": 1700389035087,
                "mdate": 1700389035087,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cgmejcSNZz",
            "forum": "S4YVoQ70b2",
            "replyto": "S4YVoQ70b2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3737/Reviewer_nfpA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3737/Reviewer_nfpA"
            ],
            "content": {
                "summary": {
                    "value": "The paper defines novel MDP concepts based on novel definitions of discount factors. The authors started by presenting in the background the standard exponential discounting setting. Then they define the non-exponential setting, defining in section 4 the optimality conditions for the policies. Finally, they studied when behavioral models for inverse reinforcement learning are identifiable."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper provides novel results on partial identifiability in IRL with non-exponential discounting. \n\n- They provide the first theoretical results on IRL in a non-exponential discounting setting."
                },
                "weaknesses": {
                    "value": "- The main weakness of the work is the motivation of it. The authors do not provide enough reasons why we need to consider a different discounted setting with respect to the exponential discounted one. In literature, when is it used the hyperbolic setting? Why is it relevant in practice? Moreover, If the setting is more general and reasonable, I think it would be better to present directly it in the background section rather than presenting the standard exponential discounted ones and then the new setting.\n\n- The main focus of the paper is (reading the abstract) on Inverse Reinforcement Learning, but, in the end, the IRL contribution of the paper is condensed into only one page and a half. \n\n- There are no experimental or numerical evaluations of the proposed approach at least to show why the proposed setting is relevant."
                },
                "questions": {
                    "value": "- A reward function is optimal under more than one policy. Then, why is the behavioral model defined as a mapping between $\\mathcal{R} \\rightarrow \\Pi$ and not $\\mathcal{R} \\rightarrow P^\\Pi$?\n\n- Proposition 1 seems to be not easy to verify. How can we understand if an MDP satisfies it?\n\n- Why is it relevant to choose discount factors that are not temporally consistent? Can the change in preference of an agent be described with a change in the reward function?\n\n- If in the end, we are using exponential discounting to find our optimal policy why do we need to study a different setting before?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3737/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698764803414,
            "cdate": 1698764803414,
            "tmdate": 1699636329747,
            "mdate": 1699636329747,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WX5bXU9e42",
                "forum": "S4YVoQ70b2",
                "replyto": "cgmejcSNZz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3737/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3737/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "First of all, we would like to thank reviewer nfpA for their thoughts and questions. Our responses to your feedback and questions are as follows:\n\nWeaknesses:\n1. First of all, we would like to note that the main motivation behind our work is the usage of IRL in the context of *preference elicitation*. That is, the setting where we wish to use IRL to learn a representation of the actual preferences of a human subject. In this setting, it is important that we use a behavioural model that actually represents the decision making process of a human as closely as possible. Now, a vast amount of research in behavioural psychology, and related fields, shows that humans are better modelled as using hyperbolic discounting rather than exponential discounting --- we provide references to some of this research in the introduction to the paper. This means that a behavioural model with hyperbolic discounting will be more accurate than (and hence preferable to) a behavioural model with exponential discounting, all other things being equal. Note also that this is not necessarily the case when IRL is used in the context of *imitation learning*, since it in this context is not fundamentally important that the learnt reward function represents the actual preferences of the demonstrator, as long as it helps the imitation learning process.\n2. Yes, the main results on IRL are given in Section 5, but all results in Section 3 and 4 build towards these results. It would not be possible to derive (or even state) the results in Section 5 without first presenting the results given in Section 3 and 4.\n3. We agree that experimental evaluations could be interesting, but we consider this to be out of scope for this paper. Similar recent works on identifiability in IRL are also typically theoretical, rather than experimental (see Dvijotham & Todorov, 2010; Cao et al., 2021; Kim et al., 2021; Skalse et al., 2022; Schlaginhaufen & Kamgarpour, 2023; Metelli et al., 2023).\n\nQuestions:\n1. This choice was made to make our analysis more tractable. For example, if an optimality criterion corresponds to more than one policy, then we may assume that the demonstrator has some fixed criterion for breaking ties between them. This assumption is not fundamentally important, and can be lifted, at the cost of making some of the theorem statements and proofs more messy. For example, both Theorem 6 and 7 would go through without much modification.\n2. To say that an MDP is episodic is simply to say that it cannot run forever; most MDPs used in practice satisfy this criterion. For example, note that any MDP with a bounded time horizon is episodic. The definition used in Proposition 1 is simply somewhat more general.\n3. As noted before, the current literature in the behavioural sciences suggests that humans are better modelled as using hyperbolic discounting, rather than exponential discounting. For this reason, we consider the setting with non-exponential discounting in IRL to be both very relevant and under-explored.\n4. To quantify the \"size\" of $\\mathrm{Am}(f)$, we need to determine if all reward functions in $\\mathrm{Am}(f)$ share some relevant property. There are multiple choices that could be made here, but we have choosen to use (exponentially discounted) optimal policies, for the reasons outlined in Section 5. In short, even if humans discount hyperbolically, we typically want to create systems that discount exponentially --- this is why exponential discounting is vastly more common in the RL literature. Therefore, we should assume that the observed demonstrator discounts hyperbolically, but that the policy which will be computed with the learnt reward function discounts exponentially.\n\nWe hope that this clarifies the context of our paper, and that the reviewer will consider increasing their score! We believe that this setting is very relevant, and that our results meaningfully extend the existing literature in interesting and non-trivial ways."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3737/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699888343797,
                "cdate": 1699888343797,
                "tmdate": 1699888343797,
                "mdate": 1699888343797,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OsdaaTFYVz",
            "forum": "S4YVoQ70b2",
            "replyto": "S4YVoQ70b2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3737/Reviewer_wg8J"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3737/Reviewer_wg8J"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the partial identifiability problem in IRL with non-exponential discounting; the authors provide their theoretical conclusion that for some behavioral models with non-exponential discounting, the partial identifiability problem persists."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "There are a few theoretical results that seems quite interesting and potentially significant. I appreciate the clear definitions and background. However, I am unable to determine whether these results are easily ported results or more original findings."
                },
                "weaknesses": {
                    "value": "So much of the proof is deferred to the appendix, it would be helpful if a proof sketch is summarized in the main text."
                },
                "questions": {
                    "value": "From R we could get to different f(R), which is denoted Am(f), a set of rational models follows R. Rather than knowing this set is singleton, I think a more important question maybe how small the set is, and whether it is contiguous. Do you think non-exponential discounting effects contiguity?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3737/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699073487528,
            "cdate": 1699073487528,
            "tmdate": 1699636329655,
            "mdate": 1699636329655,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "C1wUtzqATk",
                "forum": "S4YVoQ70b2",
                "replyto": "OsdaaTFYVz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3737/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3737/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "We thank reviewer wg8J for their feedback!\n\nAs for the first point, our results and proofs are original and non-trivial extensions of the existing literature. Only a small handful of papers have studied non-exponential discounting in IRL, and none of them have considered the (quite important) problem of partial identifiability. Concepts analogous to sophisticated, na\u00efve, and resolute policies have been proposed in the Decision Theory literature, but doing the work of porting these over to the RL setting is non-trivial, and has (to the best of our knowledge) not been done before. As you can see from our proofs, it is not straightforward to show that these policies are guaranteed to exist in all MDPs, etc. The results about identifiability (in Section 5) are completely original.\n\nThe proofs take up ~10 pages, so it would difficult to let the main text include even proof sketches for all our results. However, we can move the MDP construction used in the proof of Theorem 6 to the main text, since this proof is quite central and since the MDP construction is fairly illuminating. We can also add a few short remarks to other proofs, such as e.g. \"we can use the Kakutani fixed-point theorem to show that...\", etc.\n\nAs for the last point, we would expect $\\mathrm{Am}(f)$ to be contiguous in the non-exponential setting, but we do not have a proof of this fact. We should also note that we agree that it is not very important whether or not $\\mathrm{Am}(f)$ is a singleton. Indeed, in both the exponential and the non-exponential setting, $\\mathrm{Am}(f)$ is infinitely large (both in the sense that it has an infinite cardinality, and in the sense that it has an infinite Lebesgue measure). Therefore, we think it is more important to know whether or not all reward functions in $\\mathrm{Am}(f)$ are \"equivalent\" in some relevant sense. This is why we use the notion of $P$-identifiability to express our results in Section 5.\n\nWe hope that this clarifies our results and contributions, and that the reviewer may consider increasing their score!"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3737/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699888144753,
                "cdate": 1699888144753,
                "tmdate": 1699888144753,
                "mdate": 1699888144753,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]