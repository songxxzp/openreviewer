[
    {
        "title": "Plugin estimators for selective classification with out-of-distribution detection"
    },
    {
        "review": {
            "id": "qymMn8yNq8",
            "forum": "DASh78rJ7g",
            "replyto": "DASh78rJ7g",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6357/Reviewer_WNPt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6357/Reviewer_WNPt"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors explore a novel setting, selective classification with OOD detection (SCOD), which combines two common settings in machine learning, selective classification (SC) and out-of-distribution (OOD) detection. This investigation hold promise, as recent studies have highlighted the propensity for challenging in-distribution (ID) samples to be mistakenly classified as OOD. The authors provide a theoretical foundation for understanding SCOD, and they introduce a method that adeptly integrates existing SC and OOD detection techniques. The experimental results affirm that the proposed method performs admirably in SCOD settings."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The integration of SC and OOD in the setting is intriguing, as it more closely resembles real-world scenarios.\n2. This paper is well-presented, providing a detailed certification process.\n3. The proposed method demonstrates superior results in both settings."
                },
                "weaknesses": {
                    "value": "1. How to choose hyperparameters is crucial for this combined method, and it would be beneficial if the authors could provide more detail.\n2. Evaluating performance on datasets that are exclusively OOD or SC is meaningful, as the proportion of each may vary across different scenarios.\n3. The performance in the absence of OOD samples is not outstanding, with some results even falling below those of the baselines."
                },
                "questions": {
                    "value": "1. How to determine the PI_in* through inspection of logged data?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6357/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698368375440,
            "cdate": 1698368375440,
            "tmdate": 1699636701100,
            "mdate": 1699636701100,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "08CLlHq6Yo",
                "forum": "DASh78rJ7g",
                "replyto": "qymMn8yNq8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6357/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6357/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer WNPt"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the encouraging comments and questions.\n\n> How to choose hyperparameters is crucial for this combined method, and it would be beneficial if the authors could provide more detail.\n\nOur method requires the specification of two parameters $c_{\\rm in}$ and $c_{\\rm out}$. As noted in Section 4.3, these are in turn chosen based on the abstention budget $b_{\\rm rej}$ and the cost of non-rejection $c_{\\rm fn}$ provided by the practitioner. \n\nSpecifically, for Lagrange multiplier $\\lambda$, we may expand $c_{\\rm out} = c_{\\rm fn} - \\lambda \\cdot (1 - \\pi^*_{\\rm in})$ and $c_{\\rm in} = \\lambda \\cdot \\pi^*_{\\rm in}$, where $\\pi^*_{\\rm in}$ is the proportion of ID samples we expect in the test population. \n\nThe Lagrange multiplier $\\lambda$ is the only hyper-parameter that needs to be tuned. We tune  $\\lambda$ so that the resulting rejector has a rejection rate of $b_{\\rm rej}$ on a held-out set. Table 5 in the appendix summarizes the tuned and derived parameters.\n\n> How to determine the $\\pi^*_{\\rm in}$ through inspection of logged data?\n\nSorry for the confusion. We simply meant that if one has a sample of historical data, it could be possible to perform manual labelling on a subset to identify the proportion of OOD samples, or equally, $1 - \\pi^{*}_{\\rm in}$.\n\n> Evaluating performance on datasets that are exclusively OOD or SC is meaningful, as the proportion of each may vary across different scenarios.\n\n**In Table 7, we include results for two extreme settings**, where the test set has 1% OOD samples and 99% OOD samples (to avoid numerical overflow issues with the metrics, we retain a small fraction of ID/OOD samples in each case). \n\nWhen all (or most) of the samples are ID, we find that the MSP method becomes very competitive, as this is known to be a strong baseline for selective classification tasks. \n\nWhen all (or most) of the samples are OOD, all the methods yield similar performance. This is because the optimal rejection strategy in this case is to declare almost all samples as OOD, and rejecting fewer samples is bound to incur a high cost. So irrespective of the method used, the AUC-RC metric, which averages the risk over different rejection rates, yields the same value."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6357/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700604950002,
                "cdate": 1700604950002,
                "tmdate": 1700605276264,
                "mdate": 1700605276264,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7PlFaHQ3aF",
            "forum": "DASh78rJ7g",
            "replyto": "DASh78rJ7g",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6357/Reviewer_ZjGh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6357/Reviewer_ZjGh"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a new view of the framework for selective classification and out-of-distribution (SCOD) detection into a unified classifier, rejector tuple problem. Their framework allows for plugging in any existing OOD similarity score to perform rejection under different scenarios of data access, e.g., only in-distribution or a mixture of unlabeled IND and OOD data. They also showcase a statistical formulation for the problem and derive the Bayes optimal solution from it. They provide an empirical evaluation of the popular image classification benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "An extensive theoretical formulation of the SCOD problem, with derivations for the optimal classifier rejector pair and an alternative surrogate loss.\n\nThe experimental suite is large, with experiments on both CIFAR and ImageNet datasets, comparing to previous SCOD methods (SIRC) and traditional OOD detection metrics, but not selective classification methods."
                },
                "weaknesses": {
                    "value": "* No new insights are given on better estimating the prediction confidence or the probability ratio between in and out distributions.\n\n* Since no guarantees can be drawn for either $s_{ood}$ or $s_{sc}$, (8) or lemma 4.1 does not inherit any further guarantees. \n\n* Loss-based results are only shown for the CIFAR benchmark, not ImageNet.\n\n* The AUC-RC is compared against OOD methods but not against selective classification/misclassification detection methods such as [1], [2], [3], etc. Adding them to the benchmark would further strengthen the experiments.\n\n**Notation**: sometimes the notation is a little bit confusing. $L$ is the number of classes, the loss function, and the Lagrangian. Also, $[\\cdot]$ is a set, but {${\\cdot}$} is also a set. \n\n**Typo**: the second term of the surrogate loss in Algorithm 1 differs from the one in (10) (sampling over (x,y) instead of x). Notation might become heavy, but maybe introducing the marginal could be helpful. In algorithm 1, line 5 is not a probability if $\\hat{s}$ maps into $\\mathbb{R}$.\n\nReferences:\n\n[1] SelectiveNet. Geifman, Y. \"SelectiveNet: A Deep Neural Network with an Integrated Reject Option.\" ICML 2019. /abs/1901.09192.\\\n[2] ConfidNet. Corbi\u00e8re et al. \"Addressing Failure Prediction by Learning Model Confidence.\" NeurIPS 2019. /abs/1910.04851.\\\n[3] Doctor. Granes et. al. \"DOCTOR: A Simple Method for Detecting Misclassification Errors.\" NeurIPS 2021. /abs/2106.02395."
                },
                "questions": {
                    "value": "1. Could the authors show the steps to go from (3) to (4) to make $c_{in}$ and $c_{out}$ appear as in 4.3?\n2. Could the excessive loss bound in Lemma 4.1 be rewritten by considering the estimation error of $P_{in}(y|x)$, $\\pi^*_{in}$, and $\\hat{s}_{ood}$?\n3. The constraint considered in (3) takes into account the rejection rate on the test distribution. Why not consider the rejection rate only on the in-distribution like (1) and (2)?\n4. Assumption (A2) in page 6 states that $P_{out}(x) = 0$ for $x$ in $S_{in}^{\\*}$. How to guarantee this in practice? I.e., how to build $S_{in}^{\\*}$? The proposed strategy in the footnote considers $(x,y)$ and not simply $x$. The way I see it is that it is impossible to obtain a strict $S^*_{in}$ without full knowledge on $P_{out}$.\n5.  For CIFAR, the proposed SCOD learning in algorithm 1 does not seem to yield better results than training only on the CE loss and using heuristic scores to perform SCOD. Could the authors elaborate on potential limitations on why this is the case?\n6. How does the inlier rejection option perform on this task compared to the proposed method and existing OOD detectors? Please check the references cited in the Weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6357/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6357/Reviewer_ZjGh",
                        "ICLR.cc/2024/Conference/Submission6357/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6357/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698401530548,
            "cdate": 1698401530548,
            "tmdate": 1700671221367,
            "mdate": 1700671221367,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DALrSurHaZ",
                "forum": "DASh78rJ7g",
                "replyto": "7PlFaHQ3aF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6357/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6357/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ZjGh [Part 1]"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the detailed comments and questions.\n\n> Could the authors show the steps to go from (3) to (4)\n\nWe have updated Appendix G with a more detailed derivation going from Equation 3 to Equation 4. See also Appendix B, which provides an argument for the appropriateness of solving the Lagrangian under mild distributional assumptions. \n\n> Could the excessive loss bound in Lemma 4.1 be rewritten by considering the estimation error?\n\nThanks for the suggestion. In this setting, we assume access to existing SC and OOD detection scorers, and we bound the SCOD error in terms of how well these scores do on their respective tasks. Importantly, we don\u2019t make specific assumptions about how these scores are obtained. \n\nIf however we made more assumptions (e.g., we assume that they are the result of performing ERM with a certain surrogate loss), one could have an analogue to the generalisation analysis in Appendix C, which further bounds the estimation error of the individual scores. We will add a comment on this.\n\n> The constraint considered in (3) takes into account the rejection rate on the test distribution. Why not consider the rejection rate only on the in-distribution like (1) and (2)?\n\nThis is a reasonable question. Appendix F discusses a potential alternative formulation for SCOD which indeed considers a constraint on the in-distribution. Note that this does not change the general form of the Bayes-optimal scorer, and thus does not change the general plug-in SCOD strategy. Indeed, this is equivalent to (3) with a particular (distribution-dependent) choice of constraint $b_{\\rm rej}$.\n\nRegarding why constraining the rejection rate on the test distribution can make more sense: we are motivated by scenarios where the practitioner allocates a budget of the total fraction of abstentions they are comfortable making on test data. These abstentions could _either_ be due to samples being close to the decision boundary, _or_ due to samples being OOD.  \n\nThis formulation is natural in applications where an abstention  results in the sample being deferred to a larger model or a human expert at an additional monetary cost. In this case, the *abstention typically incurs the same cost whether it was an ID or OOD sample that was rejected*, and it is critical that the total proportion of abstentions be within a fixed budget\n\n> Ensuring Assumption (A2) in page 6 is satisfied\n\nIn the case of image classification with a fixed set of classes (as considered in Footnote 1), the proposal is to collect \u201ccanonical\u201d samples for each class (e.g., unambiguous cat images, unambiguous dog images). Since out-of-distribution samples are, by definition, those drawn from a wholly distinct distribution than the training sample, it should be the case that $\\mathbb{P}_{\\rm out}(x) = 0$ for such \u201ccanonical\u201d samples.\n\n> For CIFAR, the proposed SCOD learning in algorithm 1 does not seem to yield better results than training only on the CE loss and using heuristic scores to perform SCOD. Could the authors elaborate on potential limitations on why this is the case?\n\nWe believe the reviewer is referring to Table 2, where the **OOD samples seen during training are different from those used during test time**. In this case, our loss-based estimators (Algorithm 1) may sometimes perform worse than our black-box estimators, despite the latter being trained only on ID samples. This is primarily due to the mismatch between the OOD datasets used during training and testing.\n\nThe reviewer\u2019s observation does not however hold with the CIFAR100 experiments in **Table 3**, where we use samples from the *same* OOD dataset during both training and testing (albeit a part of a noisy \u201cwild\u201d dataset). Here, Algorithm 1 (last row) is seen to provide substantial improvements in 4 out 6 cases.\n\n### Suggestions\n\n> How does the inlier rejection option perform on this task compared to the proposed method and existing OOD detectors?... The AUC-RC is compared against OOD methods but not against selective classification methods\n\nPlease note that in Appendix I.4, we provided a comparison against a representative baseline from the SC literature, namely, the cost-sensitive softmax cross-entropy loss of (Mozannar and Sontag, 2020). We find similar conclusions to the results reported in the body. Additionally, the MSP method we compare against in all tables is the same as Chow\u2019s rule in the SC literature (Chow, 1970).\n\nIn the interest of further expanding the comparison against SC methods, **we have added results for the selective classification method of DOCTOR** method of Granese et al. (2021) [3] in **Tables 2, 3 and 13**.  This method yields similar performance as the MSP, MaxLogit and Energy scorers that use a base model trained with ID samples alone."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6357/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700604652389,
                "cdate": 1700604652389,
                "tmdate": 1700605216448,
                "mdate": 1700605216448,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pWOrJwUspW",
                "forum": "DASh78rJ7g",
                "replyto": "LX3QxHpK0l",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6357/Reviewer_ZjGh"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6357/Reviewer_ZjGh"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the author's for addressing all my questions and acknowledge their effort put into the rebuttal. All my concerns were satisfactorily answered and I raise my score accordingly."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6357/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700671194805,
                "cdate": 1700671194805,
                "tmdate": 1700671194805,
                "mdate": 1700671194805,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "M6t8ylBwuL",
            "forum": "DASh78rJ7g",
            "replyto": "DASh78rJ7g",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6357/Reviewer_nGxu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6357/Reviewer_nGxu"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses the problem of selective classification with OOD detection (SCOD), where the goal is to learn a classifier, rejector pair that can abstain on \u201chard\u201d in-distribution samples as well as OOD samples. The unification of the selective classification (SC) and OOD detection areas has recently been explored, but its formal underpinnings have not been fully developed. The paper presents a formal statistical analysis of SCOD and derives the Bayes-optimal solution for the classifier, rejector pair. This solution generalizes the Bayes-optimal solutions for the SC and OOD detection problems considered separately. Based on this solution, they propose plug-in estimators for optimally combining confidence scores for SC and density-ratio scores for OOD detection. This provides a principled way of combining existing scoring methods from the SC and OOD detection areas to address the SCOD problem. They address two settings, the first one being a black-box setting with only in-distribution training data, and the second one being a loss-based setting where one additionally has access to an unlabeled mixture of in-distribution and OOD data."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- Strong theoretical work that unifies the seemingly disparate literatures of selective classification and OOD detection. The proposed statistical formulation and Bayes optimal solution for the classifier and rejector is a general result that can guide the design of selective classifiers that can reject on both uncertain in-distribution (mis-classified) inputs and OOD inputs. Although the areas of selective classification and OOD detection have been independently studied well, a combined analysis of the problems in a principled setting has been lacking and somewhat heuristic. This paper addresses the gap. \n\n- The proposed plug-in estimators specify how to optimally combine existing confidence scores from the selective classification literature and density-ratio scores from the OOD detection literature. Therefore, it allows researchers in these areas to leverage, in a principled way, existing scoring methods for selective classification and OOD detection. The novelty of the paper does not lie in new approaches to estimate the confidence scores or the density-ratio scores, but rather in how to optimally combine them for the SCOD problem.\n\n- They also propose a loss-based approach for learning the classifier and rejector by leveraging an unlabeled mixture of in-distribution and OOD data \u201cin the wild\u201d (similar to Katz-Samuels et al., 2022). \n\n- Overall, the paper was interesting and insightful to read. There is a lot of discussion and results in the appendices which could be useful to researchers in this area."
                },
                "weaknesses": {
                    "value": "1. The experiments mainly focus on semantic OOD (or far OOD) inputs, where there is no intersection in the label space of the in-distribution and OOD. It is also important to consider covariate-shifted OOD, e.g. which are caused due to common corruptions, noise weather changes etc. Some results on the covariate-shifted OOD data would strengthen the paper.   \n\n2. The method requires a few constants or hyper-parameters to be set. For instance, the cost of false negatives $c_{fn}$, the maximum rejection rate $b_{rej}$, the proportion of inlier and OOD data in the unlabeled set $S_{mix}$, the choice of training OOD data $P^{tr}_{out}$. The results in the main paper are for specific choices of these parameters (understably due to the page limit), but there is not much discussion or takeaways on how these parameters affect the performance. Some discussion on this would be useful. \n\n3. Minor: there is lack of clarity in some parts of the paper, which could be improved. Please see the Questions section. \n\n4. The code has not been made available and some implementation details are missing."
                },
                "questions": {
                    "value": "### 1. Conditional probability?\nIn the formulation of selective classification, would it not be better to use the conditional probability of misclassification given the input is accepted, i.e. $P_{in}(y \\neq h(x) \\~|\\~ r(x) = 0)$? Also, under the subsection `Evaluation metrics` on page 8, the joint risk is divided by the total number of accepted inputs, which seems to be consistent with the conditional probability.\n\n### 2. On the Lagrangian\nIt is not clear to me how to arrive at the Lagrangian in Eqn (4) from the objective (3). Based on my simplification of the Lagrangian from the SCOD objective (3), I am getting a slightly different form than that in Eqn (4). Specifically, I get $c\\_{in} = \\lambda \\pi^{\\star}\\_{in}$ and $c\\_{out} = c\\_{fn} - \\lambda (1 - \\pi^{\\star}\\_{in})$. Referring to Section 4.3, it seems like $c_{in}$ and $c_{out}$ specified here may have been swapped? \n\nAlso, it seems to me the multiplier of the first term in Eqn (4) is $(1 \\~-\\~ c\\_{out} \\~-\\~ ((1 - \\pi^{\\star}\\_{in}) / \\pi^{\\star}\\_{in}) \\\\, c\\_{in})$, rather than $1 - c\\_{in} - c\\_{out}$. \n\nIt is certainly possible I missed/messed something, but would appreciate some clarification. \n\n### 3. Need for strictly-inlier dataset\nIt seems to me that the strictly-inlier dataset $S^{\\star}\\_{in}$ is only needed for estimating the mixture proportion $\\hat{\\pi}\\_{mix}$. It is not clear why the labeled inlier dataset $S_{in}$ (with the labels discarded) cannot be used in place of $S^{\\star}\\_{in}$ for estimating $\\hat{\\pi}_{mix}$. Any theoretical reason for this?\n\n### 4. Estimation of $\\pi^{\\star}\\_{in}$\nPlease clarify if $\\pi^{\\star}\\_{in}$ used in the formulation for SCOD Eqn (3) can be estimated using $\\hat{\\pi}_{mix}$? Does it have to be estimated at test time as mentioned in Footnote 3 on page 7? From my understanding, the mixture dataset $S\\_{mix}$ is collected \u201cin the wild\u201d during deployment, which should give a good idea of $\\pi^{\\star}\\_{in}$ as well. \n\n### Suggestions for the Theorems/Lemmas\n- Would help to restate the Lemma/Theorem statements in the appendix.\n- Some comments/takeaways on Lemma 4.1 and Lemma 4.2 would be useful.\n- In Lemma 3.1, I believe it should be: Let $(h^\\star, r^\\star)$ denote any minimiser of (3) (not (2)).\n- In the proof of Lemma 3.1 (Appendix A), it seems like the reject class $\\perp$ is allowed to be part of the classifier $h(x)$ output. However, the classifier is originally defined as $h : \\mathcal{X} \\mapsto [L]$. Please clarify this point. \n- Some pointers could be added to the proofs. For example, Eqn (13) on page 18 follows from Pinsker\u2019s inequality, which is worth mentioning.\n- Lemma 4.2: it should be $s^\\star$ not $r^\\star$.\n- Lemma 4.2: it would be useful to clarify that $p_{\\perp}(x)$ is an approximation for $P^\\star(z = 1 \\~|\\~ x)$. The $\\perp$ symbol is commonly used for rejection, whereas here it corresponds to the probability of accepting. \n- Lemma 4.2: for introducing coupling implicitly it should be $s(x) = u^T \\Phi(x)$ (no dependency on $y\u2019$ here).\n- Typo in the first line of the Proof of Lemma 4.2 on page 17: it should be $s^\\star(x) = \\log(P^\\star(z=1 | x) / P^\\star(z=-1 | x))$.\n\n### Rejector definition\nMight be better to define the rejectors $r^\\star(x)$ and $r_{BB}(x)$ in Eqn (5) and Eqn (8) directly using the indicator function $\\mathbb{1}[\\cdot]$.\n\n### On the Algorithm\n- Line 3 of Algorithm 1: should it be $\\hat{f} : \\mathcal{X} \\mapsto \\mathrm{R}^L$? That is, $\\hat{f}$ predicts a logit for each of the $L$ classes.\n- Line 7 of Algorithm 1: Can directly specify the final classifier $\\hat{h}(x) = \\arg\\max_{y} \\hat{f}_y(x)$ since it does not depend on Eqn (8). Can also specify that $s\\_{sc}(x) = \\max\\_{y} \\hat{f}_y(x)$ in Eqn (8).\n\n### Other points\n1. Under `Baselines` on page 8: reference for energy-based scorer should be (Liu et al., 2020b) and (Hendrycks & Gimpel, 2017) should also be cited for MSP.\n\n2. In Table 2, why do methods such as MSP, MaxLogit, and Energy (which do not use OOD training data) have different performance under the two settings: $P^{tr}\\_{out}=$ Random300K and $P^{tr}\\_{out}=$ OpenImages? \n\n3. From Table 4, it seems that the performance of `Plug-in BB [L_1]` is consistently worse than `SIRC [L_1]`, despite the former (proposed) method being more principled. Please explain this discrepancy. \n\n4. I think it would be useful to provide results of this method using the Deep NN method (Sun et al., 2022), especially on the ImageNet dataset, since it seems like the grad-norm scorers are not providing good estimates of the density ratio.  \n\n5. It might be worth citing the following paper which characterizes the Bayes-optimal detector for mis-classification detection.  \nDoctor: A simple method for detecting misclassification errors, https://proceedings.neurips.cc/paper_files/paper/2021/hash/2cb6b10338a7fc4117a80da24b582060-Abstract.html"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6357/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699315247040,
            "cdate": 1699315247040,
            "tmdate": 1699636700864,
            "mdate": 1699636700864,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gTP0AJu2VH",
                "forum": "DASh78rJ7g",
                "replyto": "M6t8ylBwuL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6357/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6357/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer nGxu [Part 1]"
                    },
                    "comment": {
                        "value": "We warmly thank the reviewer for their encouraging comments and detailed feedback.\n\n\n> In the formulation of selective classification, would it not be better to use the conditional probability of misclassification given the input is accepted\n\nIndeed, there are two closely related formulations for classification problems with an abstention option: one uses the conditional error $\\mathbb{P}(y \\neq h(x) \\mid r(x) = 0)$ (as exemplified in, e.g., SelectiveNet), while the other uses the joint error $\\mathbb{P}(y \\neq h(x), r(x) = 0)$ (as exemplified in, e.g., Ramaswamy et al. 2018). \n\nThere is a one-to-correspondence between the two formulations: owing to the constraint on $\\mathbb{P}(r(x) = 1)$ in Equation 1, it is not hard to see that\n$$min_{h, r}~ \\mathbb{P}(y \\neq h(x) \\mid r(x) = 0) \\colon \\~ \\mathbb{P}(r(x) = 1) \\leq b$$\n$$min_{h, r}~ \\frac{\\mathbb{P}(y \\neq h(x), r(x) = 0)}{P(r(x) = 0)} \\colon \\~ \\mathbb{P}(r(x) = 1) \\leq b$$\n$$min_{h, r, a}~ \\frac{\\mathbb{P}(y \\neq h(x), r(x) = 0)}{a} \\colon \\~ \\mathbb{P}(r(x) = 1) \\leq b,\\~ \\mathbb{P}(r(x) = 0) \\geq a$$ \n$$min_{h, r, a}~ \\frac{\\mathbb{P}(y \\neq h(x), r(x) = 0)}{a} \\colon \\~ \\mathbb{P}(r(x) = 1) \\leq \\min(b, 1 - a)$$\n\nThus, for a fixed a, the problem is equivalent to Equation 1, with a modified choice of the constraint on $\\mathbb{P}(r(x) = 1)$. The Bayes-optimal classifier is thus unaffected.\n\nWe agree that discussion of this point is worthwhile, and have added this to Appendix F. We note also that Appendix F had a discussion of a slightly different SCOD formulation.\n\n> It is not clear to me how to arrive at the Lagrangian in Eqn (4) from the objective (3)... it seems like $c_{\\rm in}$ and $c_{\\rm out}$ specified here may have been swapped?\n\nThanks for the catch! The reviewer is correct: there was a typo with the constants $c_{\\rm in}, c_{\\rm out}$ being swapped. This has been fixed. We have also updated Appendix G with a more detailed derivation of going from Equation 3 to Equation 4, and updated the discussion in Section 4.3.\n\n> Need for strictly-inlier dataset\n\nThe reviewer is correct that the strictly-inlier dataset $S^*_{\\rm in}$ is only needed to estimate the mixing weight $\\pi_{\\rm mix}$.\n\nPer Lemma E.1, we use the assumption that $\\\\mathbb{P}\\_{\\rm out}(x) = 0$ for $x \\in S^*_{\\rm in}$ to estimate $\\pi_{\\rm mix}$. Note that if we just used $x \\in S_{\\rm in}$, we may not necessarily have $\\\\mathbb{P}\\_{\\rm out}(x) = 0$: the latter would only hold under the assumption that the support of $\\\\mathbb{P}\\_{\\rm in}$ and $\\\\mathbb{P}\\_{\\rm out}$ are disjoint. We allow for the two distributions to have overlapping support in general (e.g., outliers might correspond to draws from a low (but non-zero) density region of $\\\\mathbb{P}\\_{\\rm in}$).\n\n> Please clarify if $\\pi^*_{\\rm in}$ used in the formulation for SCOD Eqn (3) can be estimated using $\\hat{\\pi}_{mix}$?\n\n$\\pi^*_{\\rm in}$ is the proportion of inliers in the test set, while $\\pi_{\\rm mix}$ is the proportion in the wild set. As the reviewer points out, indeed $\\pi^*_{\\rm in}$ may be equal to $\\pi_{\\rm mix}$ in practice, as the wild set is drawn from a production system. \n\nIn our experiments, we additionally consider cases where $\\pi^*_{\\rm in}$ can be different from $\\pi_{\\rm mix}$; in such cases, we will have to obtain estimates $\\pi^*_{\\rm in}$ through other means, such as through the practitioner or through manual inspection of historical data.\n\n> In the proof of Lemma 3.1 (Appendix A), it seems like the reject class  is allowed to be part of the classifier  output. Please clarify this point.\n\nSorry for the confusion. One can either work with a classifier-rejector pair $(h, r)$, or an augmented classifier $\\bar{h} \\colon \\mathcal{X} \\to [ L ] \\cup \\{ \\perp \\}$. We have updated the proof of Lemma 3.1 to rewrite statements involving $h(x) = \\perp$ to $r(x) = 1$.\n\n> In Table 2, why do methods such as MSP, MaxLogit, and Energy (which do not use OOD training data) have different performance under the two settings: Random300K and OpenImages?\n\nWe thank the reviewer for spotting this discrepancy between the two sets of results. This difference is due to a subtle implementation detail that we had overlooked.\n\nAll our models are trained on the same dataset, with each batch comprising both ID and OOD samples. When training the base model for MSP, MaxLogit, and Energy, we compute the loss only on the ID samples from a batch, and ignore the OOD samples. However, because the base ResNet model may internally perform other operations such as batch normalization that, for example, compute averages across the entire batch of ID + OOD samples, we find that the trained models still have a mild dependence on the OOD samples in the training data.\n\nWe have now re-run the MSP, MaxLogit, and Energy results in Tables 2 and 3 on a dataset consisting exclusively of only ID samples. **The conclusions remain the same**, with one or more of the proposed plug-in methods often performing better than these baselines."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6357/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700603243902,
                "cdate": 1700603243902,
                "tmdate": 1700603243902,
                "mdate": 1700603243902,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "J3bs2sveMW",
                "forum": "DASh78rJ7g",
                "replyto": "L3O34DbQIV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6357/Reviewer_nGxu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6357/Reviewer_nGxu"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed responses and revision of the paper with additional results. My concerns have been addressed."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6357/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700659613287,
                "cdate": 1700659613287,
                "tmdate": 1700659613287,
                "mdate": 1700659613287,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DV8KDD61Ur",
                "forum": "DASh78rJ7g",
                "replyto": "M6t8ylBwuL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6357/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6357/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you!"
                    },
                    "comment": {
                        "value": "We are glad your concerns were satisfactorily addressed! Thanks for the positive and encouraging feedback."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6357/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700673889031,
                "cdate": 1700673889031,
                "tmdate": 1700673974310,
                "mdate": 1700673974310,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]