[
    {
        "title": "Adaptive deep spiking neural network with global-local learning via balanced excitatory and inhibitory mechanism"
    },
    {
        "review": {
            "id": "awQ6KH0Dlp",
            "forum": "wpnlc2ONu0",
            "replyto": "wpnlc2ONu0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission557/Reviewer_p6P6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission557/Reviewer_p6P6"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes the Excitation-Inhibition Mechanism-assisted Hybrid Learning (EIHL) algorithm for training spiking neural networks. The algorithm combines the global learning and local learning together, and achieves better results than individual learning rules, as demonstrated through the experiments."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The method is inspired by biological mechanisms. Researchers in the SNN community should be encouraged to seek inspiration from neuroscience."
                },
                "weaknesses": {
                    "value": "1.The authors do not provide a clear and comprehensive description of their method in section 4. The review raises several concerns, outlined as follows:\n\n (i). What does the notation $x$ mean? Is it a global parameter for the entire network? How does it \"gradually increase\" during the STDP period and \"decay by itself\" during the STBP period?\n\n (ii). What is the precise formula for the operation $thresh(\\cdot)$? It appears that $b$ remains unchanged during the STDP period based on Alg. 1. Does this imply that $thresh(b)$ remains constant in each STDP period?\n\n (iii). Could the authors provide further elaboration on Eq. 7? Why is gradient descent necessary for updating the weight in the STDP period? How is dL/dx computed, given that L does not seem to be differentiable with respect to x? Also, why do x and W share the same dimension? The reviewer thinks that it should be a scalar. Additionally, why is the \"contraction factor\" $a$ needed?\n\n (iv). Regarding alg. 1, how is the \"current sparsity\" calculated? Why is \"Curr S >= Pre S\" required in the algorithm?\n\n\n2.The contribution of this work is not very clear at the current stage. In the reviewer's understanding, this work aims at improving the current hybrid learning algorithms which \"have some theoretical and practical shortcomings and need further improvement\". However, there is no theoretical (mathematical) results in the paper and the biological plausibility of the proposed hybrid method is not inadequately clarified. Furthermore, the practical preformance is relatively unsatisfactory: \n\n(i). The improvement based on the global learning rule STBP is minimal. In essence, STBP represents a special case of the proposed EIHL with specifically chosen hyperparameters. Consequently, EIHL can consistently yield slightly superior results to STBP through randomness and careful hyperparameter tuning.\n\n(ii). The performance notably lags behind the latest research. Especially, the SOTA results of DVS_CIFAR10 is 20% better than the proposed method.\n\n(iii). There is no comparison between this work and other hybrid methods regarding accuracy and biological plausibility.\n\n(iv). No experiments on large-scale datasets.\n\nIn summary, the motivation and contribution of the paper remain ambiguous. The reviewer perceives this work as merely a combination of two exsiting methods without convincing reasons. \n\n3.This paper is not well-written. Several sentences lack coherence, making the overall presentation disjointed. The presentation of equations is arbitrary and non-standard. The resolution of Fig. 1 is low."
                },
                "questions": {
                    "value": "Could the authors provide a more comprehensive explanation of the excitation-inhibition mechanism and how it is used in this work? Although the authors keep mentioning it, the reviewer cannot understand how excitatory and inhibitory synapses are handled differently and how they are balanced."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission557/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission557/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission557/Reviewer_p6P6"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission557/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698391270407,
            "cdate": 1698391270407,
            "tmdate": 1700735235815,
            "mdate": 1700735235815,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "loCEMtkUiO",
                "forum": "wpnlc2ONu0",
                "replyto": "awQ6KH0Dlp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission557/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission557/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Weaknesses \u2160(part 1)"
                    },
                    "comment": {
                        "value": "We appreciate the feedback from the reviewer. We hope our subsequent responses have adequately addressed their concerns and questions regarding the paper.\n\n* **The authors do not provide a clear and comprehensive description of their method in section 4.**\n\n*Theoretical analysis:*\n\nFirstly, the challenge of hybrid learning lies in the integration of global and local learning. While current hybrid learning methods have achieved excellent results in terms of accuracy[1], they could do slightly better in integrating the low energy consumption advantage of local learning. Therefore, based on the exciting Long-Term Potentiation (LTP) results of global learning and the inhibitory Long-Term Depression (LTD) results of local learning, an excitatory-inhibitory mechanism is adopted to balance the two.\n\nSecondly, in the cerebral cortex, the excitatory mechanism can enhance synaptic strength[2], while the inhibitory mechanism can weaken it[3,4]. Unlike previous works[5,6] that directly distinguish between excitation and inhibition at the synaptic and neuronal levels, we use the network connection status to differentiate between excitatory and inhibitory states.\n\nFinally, according to the excitatory-inhibitory mechanism, excitation and inhibition are automatically balanced. That is, when the network is overly excited, it should be inhibited, and when it is in an overly inhibited state, it should be excited. We propose an Excitatory-Inhibitory Hybrid Learning (EIHL) method, which better integrates the advantages of local and global learning, resulting in a model with high accuracy and low power consumption.\n\n> [1] Wu Y, Zhao R, Zhu J, et al. Brain-inspired global-local learning incorporated with neuromorphic computing[J]. Nature Communications, 2022, 13(1): 65.\n\n> [2] Malenka R C, Nicoll R A. Long-term potentiation--a decade of progress?[J]. Science, 1999, 285(5435): 1870-1874.\n\n> [3] Abraham W C, Bear M F. Metaplasticity: the plasticity of synaptic plasticity[J]. Trends in neurosciences, 1996, 19(4): 126-130.\n\n> [4] Li X, Steffens D C, Potter G G, et al. Decreased between\u2010hemisphere connectivity strength and network efficiency in geriatric depression[J]. Human brain mapping, 2017, 38(1): 53-67.\n\n> [5] Kern F B, Chao Z C. Short-term neuronal and synaptic plasticity act in synergy for deviance detection in spiking networks[J]. PLOS Computational Biology, 2023, 19(10): e1011554.\n\n> [6] Zhu G, Zhang Z, Zhang X Y, et al. Diverse Neuron Type Selection for Convolutional Neural Networks[C]//IJCAI. 2017: 3560-3566."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission557/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700385865703,
                "cdate": 1700385865703,
                "tmdate": 1700386007375,
                "mdate": 1700386007375,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Bd8OdAR8Na",
                "forum": "wpnlc2ONu0",
                "replyto": "awQ6KH0Dlp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission557/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission557/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Weaknesses \u2160(part 2)"
                    },
                    "comment": {
                        "value": "*Formula explanation:*\n\nWe use the degree of network connectivity to distinguish between excitation and inhibition, so we set a target sparsity $b$ as the threshold for the network to be in an over-inhibited state. In local learning, due to the effect of LTD, the network sparsity gradually increases. When it exceeds the target sparsity $b$, the network is considered to be in an over-inhibited state, and the network will switch to excited global learning. The contraction formula Eq.6 accelerates the process of the network gradually becoming sparse in local learning.\n\n$\\ \\mathcal H(x) = thresh(b) \\cdot x \\cdot a ,\\ \\ \\ \\  a \\in(0\uff0c1)\uff0c x \\in \\mathbb{N}   \\ \\ \\ Eq.6$\n\nActually, $\\mathcal H(x)$  is a weight value, and contraction implies that weights less than the boundary value $\\mathcal H(x)$  will shrink until they disconnect. Moreover, the target sparsity $b$ is a percentage, not a weight value, so $thresh(b)$ is the mapping of b on the weight distribution. In simpler terms, if the weights that are less than $thresh(b)$ in each layer are set to 0, the current sparsity will directly reach the target sparsity $b$. The network will immediately exhibit an over-inhibited state and will switch to excited global learning. However, defining the range and setting it to 0 directly is too crude and will cause a lot of unnecessary losses. Therefore, $x \\cdot a$ is to give $\\mathcal H(x)$ a slow expansion process from 0. $a$ first divides the $thresh(b)$ into multiple scales, and as $x$ gradually increases, $\\mathcal H(x)$ will also slowly increase until it equals $thresh(b)$.\n\nHowever, we believe that directly setting the weights that are less than the boundary value $\\mathcal H(x)$ to 0 is still crude. Therefore, we fix the update direction of the weights in the boundary value $\\mathcal H(x)$ to only move towards 0, and then set the weights in the zero neighborhood to 0, to achieve the LTD result in a relatively smooth manner.\n\n$\\ \\mathcal{W}' = \\mathcal{W} - \\text{lr} \\cdot \\nabla \\mathcal{W},\\ \\ \\ \\  \\text{lr} > 0   \\ \\ \\ Eq.7 $\n\n$\\mathcal{W}$ denotes the weight that has not been updated, $\\mathcal{W}'$ denotes the weight that has been updated, $\\nabla W$ is the weight update amount of STDP and $\\text{lr}$ denotes the learning rate.\n\nSummarize the contraction operation of EIHL on local learning, that is, The update direction of the weights within the $\\mathcal H(x)$ boundary can only tend to 0, and the weight area close to 0 can be directly set to 0. Then $x++$ , $\\mathcal H(x)$ gradually expands, and the final set to 0 area reaches the $thesh(b)$ area, the current sparsity reaches the target sparsity $b$, and switches to the excited global learning."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission557/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700386071762,
                "cdate": 1700386071762,
                "tmdate": 1700386071762,
                "mdate": 1700386071762,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ILekounfP9",
                "forum": "wpnlc2ONu0",
                "replyto": "awQ6KH0Dlp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission557/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission557/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Weaknesses \u2160(part 3)"
                    },
                    "comment": {
                        "value": "* **The review raises several concerns, outlined as follows:\n(i). What does the notation $x$ mean? Is it a global parameter for the entire network? How does it \"gradually increase\" during the STDP period and \"decay by itself\" during the STBP period?**\n  \n  $x$ represents the quantity of weights that need to be reduced. $x$ is a global parameter for the entire network. $x$ in STBP and STDP every certain number of batches respectively $x--$ and $x++$, corresponding to the contraction boundary $\\mathcal H(x)$  gradually contracting and expanding.\n* **(ii). What is the precise formula for the operation $thresh(b)$? It appears that $b$ remains unchanged during the STDP period based on Alg. 1. Does this imply that $thresh(b)$ remains constant in each STDP period?**\n\nThe target sparsity $b$ is a percentage, not a weight value, so $thresh(b)$ is the mapping of b on the weight distribution. $b$ and $thresh(b)$ remains unchanged during the STDP period.\n\n* **(iii). Could the authors provide further elaboration on Eq. 7? Why is gradient descent necessary for updating the weight in the STDP period? How is dL/dx computed, given that L does not seem to be differentiable with respect to x? Also, why do x and W share the same dimension? The reviewer thinks that it should be a scalar. Additionally, why is the \"contraction factor\" $a$ needed?**\n\nWe are extremely grateful to the reviewer for pointing out the error in Eq.7.\n\nFirstly, we believe that directly setting the weights that are less than the boundary value $\\mathcal H(x)$ to 0 is still crude. Therefore, we fix the update direction of the weights in the boundary value $\\mathcal H(x)$ to only move towards 0 by the Eq.7, and then set the weights in the zero neighborhood to 0, to achieve the LTD result in a relatively smooth manner.\n\nSecondly, as the reviewer said, the gradient in the gradient descent formula is $\\nabla \\mathcal{W}$, not $\\frac{d \\mathcal L} {d x}$, which is a mistake in our writing.\n\nFinally, $x \\cdot a$ is to give $\\mathcal H(x)$ a slow expansion process from 0. $a$ first divides the $thresh(b)$ into multiple scales, and as $x$ gradually increases, $\\mathcal H(x)$ will also slowly increase until it equals $thresh(b)$.\n\n* **(iv). Regarding alg. 1, how is the \"current sparsity\" calculated? Why is \"Curr S >= Pre S\" required in the algorithm?The authors do not provide a clear and comprehensive description of their method in section 4.**\n\nFirstly, the current sparsity is equal to the proportion of weights that are 0 among all the weight parameters. This is given in section 5.2.\n\nSecondly, we directly set the threshold for over-inhibition as $b$, but we did not specify a particular value for the threshold of over-excitation. The criterion for judging the state of over-excitation is \u201cCurr S >= Pre S\u201d. We believe that in global learning, when the sparsity no longer decreases, the network presents a state of over-excitation.\n\nFinally, we fully respect the reviewer\u2019s comments and have reorganized section 4. We believe this will enhance the clarity and comprehensiveness of our work. Thank you for your valuable feedback.\n\n*We hope that these revisions can help the reviewers to have a clearer understanding of our paper. If the reviewers have any questions or suggestions, please feel free to tell us, and we will improve them as soon as possible.*"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission557/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700386118925,
                "cdate": 1700386118925,
                "tmdate": 1700386200862,
                "mdate": 1700386200862,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rpX45CUW3T",
                "forum": "wpnlc2ONu0",
                "replyto": "awQ6KH0Dlp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission557/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission557/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Weaknesses \u2161 (part 1)"
                    },
                    "comment": {
                        "value": "We appreciate the feedback from the reviewer. We hope our subsequent responses have adequately addressed their concerns and questions regarding the paper.\n\n* **The contribution of this work is not very clear at the current stage. In the reviewer's understanding, this work aims at improving the current hybrid learning algorithms which \"have some theoretical and practical shortcomings and need further improvement\". However, there is no theoretical (mathematical) results in the paper and the biological plausibility of the proposed hybrid method is not inadequately clarified. Furthermore, the practical preformance is relatively unsatisfactory:**\n\nWe apologize for the confusion, and then according to the reviewer\u2019s suggestion, we re-summarized the contributions of this work:\n\n1. We propose an excitation-inhibition mechanism-assisted hybrid learning(called EIHL), which can combine the high accuracy of global learning and the low energy consumption of local learning and showed excellent accuracy and sparsity in the experiments on three datasets and two models.\n2. We obtained inspiration from neuroscience and adopted the excitation-inhibition mechanism to solve the problem of how to reasonably integrate hybrid learning. The experimental results showed that EIHL also achieved sparsity advantages, which made it possible to deploy on hardware.\n3. We used the neural excitation-inhibition mechanism to achieve the integration of global and local learning and implemented it by adjusting the weights. This not only provides a new perspective for the field of SNN training methods but also prepares for the generalization of EIHL to the ANN domain.\n\n* **(i). The improvement based on the global learning rule STBP is minimal. In essence, STBP represents a special case of the proposed EIHL with specifically chosen hyperparameters. Consequently, EIHL can consistently yield slightly superior results to STBP through randomness and careful hyperparameter tuning.**\n\nWe appreciate the reviewer\u2019s point, but first of all, our motivation is to propose a hybrid learning method that can well integrate the high-precision advantage of global learning and the low-energy consumption advantage of local learning. According to the experimental results analysis, our proposed EIHL achieved the established goal, with higher accuracy than global learning and higher sparsity than local learning. Instead of simply pursuing the improvement of accuracy.\n\nSecondly, in the neural excitation-inhibition mechanism, it is the minority of inhibitory neurons that inhibit the majority of excitatory neurons, and the inhibitory effect is crucial[1-3]. Therefore, we think that the minority of inhibited local learning in EIHL is also crucial.\n\nFinally, in the Tab.1, especially on the DVS dataset, the accuracy of EIHL is 2.7% and 4.35% higher than STBP on the two models, respectively, which is not a gap that can be achieved by simply tuning parameters. In Tab.3, EIHL has higher sparsity than STBP by 10% or even 30% under higher accuracy than STBP, which is obviously not achievable by simply tuning parameters.\n\n* **(ii). The performance notably lags behind the latest research. Especially, the SOTA result of DVS_CIFAR10 is 20% better than the proposed method.**\n\nWe agree with the reviewer\u2019s comments and apologize for that, and we will use more advanced models to achieve higher baseline accuracy in future work. The model used in this paper is simple, which directly affects the quality of the results. In addition, there are many parameters that can be adjusted in a network, such as the optimizer, the number of iterations, the learning rate, the learning rate decay rate, etc. There are too many factors that affect the accuracy of the network. Therefore, we also accept the reviewer\u2019s suggestions, and in future work, we will conduct experiments with a better baseline model and parameters.\n\n> [1] Simeone, & Rho, J. M. (2009). Antiepileptic Drugs: Antiepileptic Drug Mechanisms. In Encyclopedia of Basic Epilepsy Research (pp. 59\u201366). Elsevier Inc. https://doi.org/10.1016/B978-012373961-2.00160-0\n\n> [2] Shea, Thomas. (2021). An Overview of Studies Demonstrating that ex vivo Neuronal Networks Display Multiple Complex Behaviors: Emergent Properties of Nearest-Neighbor Interactions of Excitatory and Inhibitory Neurons. The Open Neurology Journal. 15. 3-15. 10.2174/1874205X02115010003\n\n> [3] Wang X J. Macroscopic gradients of synaptic excitation and inhibition in the neocortex[J]. Nature Reviews Neuroscience, 2020, 21(3): 169-178."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission557/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700386339294,
                "cdate": 1700386339294,
                "tmdate": 1700386339294,
                "mdate": 1700386339294,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "K5FTVfnTxN",
                "forum": "wpnlc2ONu0",
                "replyto": "awQ6KH0Dlp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission557/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission557/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Weaknesses \u2161 (part 2)  and \u2162"
                    },
                    "comment": {
                        "value": "* **(iii). There is no comparison between this work and other hybrid methods regarding accuracy and biological plausibility.**\n\nWe appreciate the reviewer\u2019s comments and have conducted a comparative experiment with other hybrid learning algorithms. Below is a performance comparison experiment between EIHL and Excitatory-Inhibitory Cooperative Iterative Learning (EICIL)[4]. EICIL is a hybrid training method that simulates the excitatory and inhibitory behaviors of biological neurons and seamlessly integrates them into the training process of Spiking Neural Networks (SNNs). EICIL proposes two training methods:  the iteration using the Surrogate Gradient Method(GS\uff09 method and STDP-BW(which incorporates the backpropagation technique into the STDP model) method as GSI, and the iteration using the GS method and STDP-BW-GS method as GSGI.\n\nWe conducted experiments on CIFAR10 using Spiking Resnet18 and Spiking VGG11, respectively. The number of epochs was set to 200, and the learning rates were 2-2e and 2-3e. This experiment aims to provide a comprehensive comparison between the proposed method and existing techniques.\n\n| Learning |       Model      | Sparsity.(%) | &Accuracy.(%) |     Model     | Sparsity.(%) | &Accuracy.(%) |\n|:--------:|:----------------:|:------------:|---------------|:-------------:|:------------:|---------------|\n|   EIHL   | Spiking Resnet18 |     17.17    |     90.25     | Spiking VGG11 |     13.10    |     85.75     |\n|    GSI   | Spiking Resnet18 |     0.00     |     89.32     | Spiking VGG11 |     0.00     |          85.63    |\n|   GSGI   | Spiking Resnet18 |     0.00     |         88.95      | Spiking VGG11 |     0.00     |       85.66        |\n\nThe experimental results show that the EIHL method still has superior accuracy and unique sparsity advantages.\n\n* **(iv). No experiments on large-scale datasets.**\n\nWe appreciate the reviewer\u2019s comments. We are trying to experiment on large-scale datasets, but the rebuttal time is limited, we hope the reviewer understands.\n\n* **In summary, the motivation and contribution of the paper remain ambiguous. The reviewer perceives this work as merely a combination of two exsiting methods without convincing reasons.**\n\nFirst of all, the motivation of this paper is to reasonably integrate local and global learning, and to be compatible with the advantages of low power consumption of local learning and high accuracy of global learning.\n\nSecondly, the contribution of this paper is:\n\n1. We propose an excitation-inhibition mechanism-assisted hybrid learning(called EIHL), which can combine the high accuracy of global learning and the low energy consumption of local learning and showed excellent accuracy and sparsity in the experiments on three datasets and two models.\n2. We obtained inspiration from neuroscience and adopted the excitation-inhibition mechanism to solve the problem of how to reasonably integrate hybrid learning. The experimental results showed that EIHL also achieved sparsity advantages, which made it possible to deploy on hardware.\n3. We used the neural excitation-inhibition mechanism to achieve the integration of global and local learning and implemented it by adjusting the weights. This not only provides a new perspective for the field of SNN training methods but also prepares for the generalization of EIHL to the ANN domain.\n\nFinally, the purpose of this paper is to explore how to integrate local learning and global learning, rather than to create two new learning algorithms.\n\n* **This paper is not well-written. Several sentences lack coherence, making the overall presentation disjointed. The presentation of equations is arbitrary and non-standard. The resolution of Fig. 1 is low.**\n\nWe have carefully checked and polished the whole paper, and we have tried our best to make the paper more readable and coherent. Then, we also re-optimized our figures and formulas to present our work more clearly.\n\n*We hope that these revisions can help the reviewers to have a clearer understanding of our paper. If the reviewers have any questions or suggestions, please feel free to tell us, and we will improve them as soon as possible.*\n\n> [4] Shao Z, Fang X, Li Y, et al. EICIL: Joint Excitatory Inhibitory Cycle Iteration Learning for Deep Spiking Neural Networks[C]//Thirty-seventh Conference on Neural Information Processing Systems. 2023."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission557/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700386413731,
                "cdate": 1700386413731,
                "tmdate": 1700388249779,
                "mdate": 1700388249779,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "od1ULoXlPq",
                "forum": "wpnlc2ONu0",
                "replyto": "awQ6KH0Dlp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission557/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission557/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Questions (part 1)"
                    },
                    "comment": {
                        "value": "* **Could the authors provide a more comprehensive explanation of the excitation-inhibition mechanism and how it is used in this work? Although the authors keep mentioning it, the reviewer cannot understand how excitatory and inhibitory synapses are handled differently and how they are balanced.**\n\n*Theoretical analysis:*\n\nFirstly, the challenge of hybrid learning lies in the integration of global and local learning. While current hybrid learning methods have achieved excellent results in terms of accuracy[1], they could do slightly better in integrating the low energy consumption advantage of local learning. Therefore, based on the exciting Long-Term Potentiation (LTP) results of global learning and the inhibitory Long-Term Depression (LTD) results of local learning, an excitatory-inhibitory mechanism is adopted to balance the two.\n\nSecondly, in the cerebral cortex, the excitatory mechanism can enhance synaptic strength[2], while the inhibitory mechanism can weaken it[3,4]. Unlike previous works[5,6] that directly distinguish between excitation and inhibition at the synaptic and neuronal levels, we use the network connection status to differentiate between excitatory and inhibitory states.\n\nFinally, according to the excitatory-inhibitory mechanism, excitation and inhibition are automatically balanced. That is, when the network is overly excited, it should be inhibited, and when it is in an overly inhibited state, it should be excited. We propose an Excitatory-Inhibitory Hybrid Learning (EIHL) method, which better integrates the advantages of local and global learning, resulting in a model with high accuracy and low power consumption.\n\n> [1] Wu Y, Zhao R, Zhu J, et al. Brain-inspired global-local learning incorporated with neuromorphic computing[J]. Nature Communications, 2022, 13(1): 65.\n\n> [2] Malenka R C, Nicoll R A. Long-term potentiation--a decade of progress?[J]. Science, 1999, 285(5435): 1870-1874.\n\n> [3] Abraham W C, Bear M F. Metaplasticity: the plasticity of synaptic plasticity[J]. Trends in neurosciences, 1996, 19(4): 126-130.\n\n> [4] Li X, Steffens D C, Potter G G, et al. Decreased between\u2010hemisphere connectivity strength and network efficiency in geriatric depression[J]. Human brain mapping, 2017, 38(1): 53-67.\n\n> [5] Kern F B, Chao Z C. Short-term neuronal and synaptic plasticity act in synergy for deviance detection in spiking networks[J]. PLOS Computational Biology, 2023, 19(10): e1011554.\n\n> [6] Zhu G, Zhang Z, Zhang X Y, et al. Diverse Neuron Type Selection for Convolutional Neural Networks[C]//IJCAI. 2017: 3560-3566."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission557/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700388066053,
                "cdate": 1700388066053,
                "tmdate": 1700388433745,
                "mdate": 1700388433745,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Jlbs16W8x7",
                "forum": "wpnlc2ONu0",
                "replyto": "awQ6KH0Dlp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission557/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission557/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Questions (part 2)"
                    },
                    "comment": {
                        "value": "*Formula explanation:*\n\nWe use the degree of network connectivity to distinguish between excitation and inhibition, so we set a target sparsity $b$ as the threshold for the network to be in an over-inhibited state. In local learning, due to the effect of LTD, the network sparsity gradually increases. When it exceeds the target sparsity $b$, the network is considered to be in an over-inhibited state, and the network will switch to excited global learning. The contraction formula Eq.6 accelerates the process of the network gradually becoming sparse in local learning.\n\n$\\ \\mathcal H(x) = thresh(b) \\cdot x \\cdot a ,\\ \\ \\ \\  a \\in(0\uff0c1)\uff0c x \\in \\mathbb{N}   \\ \\ \\ Eq.6$\n\nActually, $\\mathcal H(x)$  is a weight value, and contraction implies that weights less than the boundary value $\\mathcal H(x)$  will shrink until they disconnect. Moreover, the target sparsity $b$ is a percentage, not a weight value, so $thresh(b)$ is the mapping of b on the weight distribution. In simpler terms, if the weights that are less than $thresh(b)$ in each layer are set to 0, the current sparsity will directly reach the target sparsity $b$. The network will immediately exhibit an over-inhibited state and will switch to excited global learning. However, defining the range and setting it to 0 directly is too crude and will cause a lot of unnecessary losses. Therefore, $x \\cdot a$ is to give $\\mathcal H(x)$ a slow expansion process from 0. $a$ first divides the $thresh(b)$ into multiple scales, and as $x$ gradually increases, $\\mathcal H(x)$ will also slowly increase until it equals $thresh(b)$.\n\nHowever, we believe that directly setting the weights that are less than the boundary value $\\mathcal H(x)$ to 0 is still crude. Therefore, we fix the update direction of the weights in the boundary value $\\mathcal H(x)$ to only move towards 0, and then set the weights in the zero neighborhood to 0, to achieve the LTD result in a relatively smooth manner.\n\n$\\ \\mathcal{W}' = \\mathcal{W} - \\text{lr} \\cdot \\nabla \\mathcal{W},\\ \\ \\ \\  \\text{lr} > 0   \\ \\ \\ Eq.7 $\n\n$\\mathcal{W}$ denotes the weight that has not been updated, $\\mathcal{W}'$ denotes the weight that has been updated, $\\nabla W$ is the weight update amount of STDP and $\\text{lr}$ denotes the learning rate.\n\nSummarize the contraction operation of EIHL on local learning, that is, The update direction of the weights within the $\\mathcal H(x)$ boundary can only tend to 0, and the weight area close to 0 can be directly set to 0. Then $x++$ , $\\mathcal H(x)$ gradually expands, and the final set to 0 area reaches the $thesh(b)$ area, the current sparsity reaches the target sparsity $b$, and switches to the excited global learning.\n\n*We hope that these revisions can help the reviewers to have a clearer understanding of our paper. If the reviewers have any questions or suggestions, please feel free to tell us, and we will improve them as soon as possible.*"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission557/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700388469095,
                "cdate": 1700388469095,
                "tmdate": 1700388484579,
                "mdate": 1700388484579,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VXSGUPqR2l",
                "forum": "wpnlc2ONu0",
                "replyto": "awQ6KH0Dlp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission557/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission557/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up discussion"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nThank you sincerely for taking the time to review our work. We greatly appreciate your valuable feedback. If you have any further questions or concerns, we would be more than happy to address them promptly before the approaching deadline. Your input is crucial to improving the quality of our work.\n\nAlternatively, if you feel that the concerns you initially raised have been adequately addressed, we kindly request that you consider updating your evaluation to reflect this. Your updated evaluation would be immensely helpful to us.\n\nOnce again, we extend our gratitude for your time and effort in reviewing our work. We look forward to your response.\n\nThank You.\n\nPaper 557 Authors"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission557/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700719006314,
                "cdate": 1700719006314,
                "tmdate": 1700719006314,
                "mdate": 1700719006314,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WHDZ2PoDBq",
                "forum": "wpnlc2ONu0",
                "replyto": "awQ6KH0Dlp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission557/Reviewer_p6P6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission557/Reviewer_p6P6"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the great efforts the authors made in the rebuttal. Several concerns have been addressed.\n\nNow the description of the proposed method is clearer, and I understand the real meaning of excitation-inhibition mechanism in this work.\n\nNow I am conviced that the proposed method can achieve good sparsity. However, I still holds the idea that the overall performance is not good. Given the simplicity of the datasets and the effectiveness of the utilized network architectures, it is feasible to get similar accuracy and sparsity through straightforward regularization of a fine-tuned STDP model. Consequently, the true effectiveness of the proposed method requires further validation.\n\nMinor issue: the presentation of equations is still non-standard. For example, Eqns. 2 and 3 are like isolated items that do not belong to any sentences.\n\nOverall, in light of the addressed concerns and the efforts of the authors, I have revised my score from 3 to 5. Nevertheless, I maintain a relatively reserved evaluation."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission557/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700735202378,
                "cdate": 1700735202378,
                "tmdate": 1700735283062,
                "mdate": 1700735283062,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "kVMENvJBbF",
            "forum": "wpnlc2ONu0",
            "replyto": "wpnlc2ONu0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission557/Reviewer_K674"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission557/Reviewer_K674"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a hybrid learning method that uses the neural excitation and inhibition mechanism to assist local learning and global learning (called EIHL), by simulating the biological neural excitation and inhibition mechanism to adjust the network connection state, thus integrating local learning and global learning. The experimental results also show that this method has advantages in accuracy and sparsity compared to separate local learning and global learning."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper is inspired by the biological neural excitation-inhibition mechanism and proposes a new hybrid learning method for SNN, which is more brain-like than previous methods and has originality. Moreover, this paper has some practical significance from both the biological perspective and the accuracy and sparsity of the experimental results. Furthermore, the whole paper is logically coherent and fluent, and the language is concise and clear."
                },
                "weaknesses": {
                    "value": "The Fig.1 in this paper that explains the EIHL algorithm is too simple and not detailed enough, only showing the connection processing between the convolutional layer and the IF neuron layer. This figure could consider adding another layer to show the specific operation of the algorithm more finely."
                },
                "questions": {
                    "value": "Is the EIHL method only applied to the convolutional layer? If not, I hope it can be reflected in the figure. I suggest updating and optimizing Fig.1."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission557/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission557/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission557/Reviewer_K674"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission557/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698455695348,
            "cdate": 1698455695348,
            "tmdate": 1700697208428,
            "mdate": 1700697208428,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uvb0R9CoZF",
                "forum": "wpnlc2ONu0",
                "replyto": "kVMENvJBbF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission557/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission557/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Weaknesses and Questions"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their feedback. We hope we have addressed their concerns and questions regarding the paper.\n\n* **The Fig.1 in this paper that explains the EIHL algorithm is too simple and not detailed enough, only showing the connection processing between the convolutional layer and the IF neuron layer. This figure could consider adding another layer to show the specific operation of the algorithm more finely.**\n* **Is the EIHL method only applied to the convolutional layer? If not, I hope it can be reflected in the figure. I suggest updating and optimizing Fig.1.**\n\nWe apologize for the reviewer\u2019s concern about Figure 1, we agree with the reviewer\u2019s comments, and we have optimized Figure 1.\n\nFig.1 is a description of the EIHL learning process. From the figure, it can be seen that after local learning, the synaptic connections weaken or even disconnect. And in global learning, the disconnected synapses can also reconnect and have the opportunity to learn again. Then, answer the reviewer\u2019s questions about Fig.1: The EIHL method is applied to all layers that need to be trained. Because the main layer is the convolutional layer, only the convolutional layer is marked in the graph. But we thank the reviewer for pointing this out, and accept the reviewer\u2019s opinion to refine Fig.1 to better present the EIHL algorithm.\n\nWe are very grateful to the reviewers for their precise comments on this paper. We accept the reviewers\u2019 suggestions and polish the paper accordingly, hoping to eliminate the reviewers\u2019 concerns."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission557/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700388561262,
                "cdate": 1700388561262,
                "tmdate": 1700388561262,
                "mdate": 1700388561262,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VPlrZ1LWUi",
                "forum": "wpnlc2ONu0",
                "replyto": "uvb0R9CoZF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission557/Reviewer_K674"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission557/Reviewer_K674"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed comments. The responses have addressed all my concerns and comments. After reading the other reviews, I still believe this paper presents a contribution worth of acceptance. Therefore, I would like to raise my score to 8."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission557/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700697183998,
                "cdate": 1700697183998,
                "tmdate": 1700697183998,
                "mdate": 1700697183998,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kmL0alEx0h",
                "forum": "wpnlc2ONu0",
                "replyto": "kVMENvJBbF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission557/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission557/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up discussion"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nWe are very grateful for your favorable assessment of our work. Your choice to increase the score from 6 to 8 is very motivating and highly valued. We are thrilled to know that our efforts have been recognized and that our work has met your expectations.\n\nWe also want to use this chance to ask if you have any further questions or doubts about our work. Your feedback is essential to us, and we want to make sure that we resolve any remaining problems before the final submission. We look forward to your response.\n\nThank You.\n\nPaper 557 Authors"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission557/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700719182056,
                "cdate": 1700719182056,
                "tmdate": 1700719182056,
                "mdate": 1700719182056,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8uwyFPWoCP",
            "forum": "wpnlc2ONu0",
            "replyto": "wpnlc2ONu0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission557/Reviewer_UpZe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission557/Reviewer_UpZe"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an Excitation-Inhibition Mechanism assisted hybrid Learning (EIHL) algorithm. Inspired by the biological neural excitation-inhibition mechanism, it achieves adaptive adjustment of spiking neural network connectivity, and automatically alternates between global and local learning according to the growth or decay of synaptic strength which depends on the excitation-inhibition mechanism. It also conducts three experiments to demonstrate that this method has higher accuracy than global learning, and higher sparsity than local learning."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper proposes a new hybrid method of local and global learning, which is regulated by the biological neural excitation-inhibition mechanism. The paper also argues that the excitation-inhibition mechanism leads to sparse results for neural networks, which is an innovative perspective. Moreover, the language and logic of the introduction and method description are clear and smooth. Finally, the paper presents EIHL as a new point in the field of SNN training methods, and also provides new insights for ANN training methods, which has some significance."
                },
                "weaknesses": {
                    "value": "The paper conducted three experiments to verify the performance advantages of the method, but the details of the third experiment are less described. The third experiment should specify which layer or the whole network is randomly pruned at different levels, and also explain the specific operation of random pruning."
                },
                "questions": {
                    "value": "I would like to ask the author, is the random pruning at different levels applied to the whole network or to a specific layer? And will the synapses that are randomly cut off at the beginning be restored in the later learning or remain disconnected? Maybe it should be explained in the third experiment."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission557/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699069678273,
            "cdate": 1699069678273,
            "tmdate": 1699635982753,
            "mdate": 1699635982753,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3WGjiXXcvp",
                "forum": "wpnlc2ONu0",
                "replyto": "8uwyFPWoCP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission557/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission557/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Weaknesses and Questions"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their feedback, and we appreciate their recognition of the contribution of our work. We also apologize for the confusion that the reviewer still has. We hope that our answers can clear up the reviewer\u2019s doubts.\n\n* **The paper conducted three experiments to verify the performance advantages of the method, but the details of the third experiment are less described. The third experiment should specify which layer or the whole network is randomly pruned at different levels, and also explain the specific operation of random pruning.**\n* **I would like to ask the author, is the random pruning at different levels applied to the whole network or to a specific layer? And will the synapses that are randomly cut off at the beginning be restored in the later learning or remain disconnected? Maybe it should be explained in the third experiment.**\n\nWe apologize for the confusion caused by the insufficient description of the third experiment. This paper conducted three experiments in total: the first one is an evaluation of the sparsity and accuracy of EIHL, the second one is the influence of the contraction boundary value in EIHL, and the third one is a comparison of the performance of the model on different disconnect degrees.\n\nThe different degrees of random pruning operation in the third experiment, it is performed after the weight initialization. For example, if the pruning degree is 20%, then the pruning object is all the weight parameters that need to be trained. The pruning step is performed layer by layer, and 20% of the weights are randomly selected and set to 0 in each layer, and their positions are recorded. In each subsequent gradient update, they and their gradients are set to 0. Therefore, the randomly selected weights will remain disconnected, which is why the sparsity and pruning degrees of STBP and STDP in Tab.3 are consistent.\n\nWe are very grateful for the reviewer\u2019s comments on this paper. We accept the reviewer\u2019s opinions and have supplemented the description of the third experiment in the paper according to the above description. We hope that our explanation has eliminated the reviewer\u2019s concerns."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission557/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700388519088,
                "cdate": 1700388519088,
                "tmdate": 1700388519088,
                "mdate": 1700388519088,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "26Yb6y73Vk",
                "forum": "wpnlc2ONu0",
                "replyto": "8uwyFPWoCP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission557/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission557/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up discussion"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nWe are very glad to receive your positive evaluation of our work. Your recognition of our efforts and the quality of our work means a lot to us.\n\nMeanwhile, we hope that our previous responses have addressed your questions and concerns. In addition, we would like to take this opportunity to inquire if you have any further questions or issues regarding our work. We value your feedback and we want to ensure that we have responded to all your comments and suggestions. If you have any additional queries or problems, please feel free to contact us before the final submission deadline.\n\nThank You.\n\nPaper 557 Authors"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission557/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700719226383,
                "cdate": 1700719226383,
                "tmdate": 1700719226383,
                "mdate": 1700719226383,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ibz9oC318r",
            "forum": "wpnlc2ONu0",
            "replyto": "wpnlc2ONu0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission557/Reviewer_iXWZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission557/Reviewer_iXWZ"
            ],
            "content": {
                "summary": {
                    "value": "The authors proposed an Excitation-Inhibition Mechanism-assisted Hybrid Learning (EIHL) algorithm for training Spiking Neural Networks (SNNs), a learning algorithm that hybrid local learning rule and global learning rule. \nExperiments on CIFAR10/100 and DVS-CIFAR10 showed that EIHL outperforms other methods in terms of accuracy and sparsity."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Fusion of Global and Local Learning Rules: According to the authors, the integration of both global and local learning paradigms could potentially pave the way for attaining enhanced performance and energy efficiency in neural networks.\n\n2. Performance on CIFAR Benchmark: The authors successfully demonstrated an improvement in performance on the CIFAR dataset when compared to the traditional backpropagation technique."
                },
                "weaknesses": {
                    "value": "1. The authors should elucidate their contributions more explicitly in order to provide a comprehensive understanding of the research.\n\n2. In the context of hybrid learning, 'STDP' process employs a contraction curve to facilitate Long-Term Depression. Nevertheless, the authors have not adequately expounded upon the association between LTD and STDP, and the proposed method do not have a dependence of the spike timing. It's not clear why excitation should be like STBP and depression should be like STDP.\n\n3. The accuracy of references should be ensured. For example, the paper states, \"Spike-Timing Dependent Plasticity (STDP) was proposed based on these rules by Caporale & Dan (2008).\" However, the discovery of STDP predates 2008. Caporale & Dan (2008) is a review paper."
                },
                "questions": {
                    "value": "1. In the context of the hybrid learning rule, what is the significance of excitatory and inhibitory synapses, given that STDP and STBP do not appear to rely on the distinction between these synapse types? Furthermore, it seems that excitatory and inhibitory synapses are not typically delineated in deep spiking neural networks.\n\n2. Weight pruning is a technique employed in deep learning to increase network sparsity by eliminating the smallest weights. Please elucidate the distinctions between the 'STDP' process in EIHL and weight pruning techniques in deep learning.\n\n3. Kindly review the terminology and references utilized in the manuscript to ensure a more precise and coherent presentation of the research study."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission557/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission557/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission557/Reviewer_iXWZ"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission557/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699543956904,
            "cdate": 1699543956904,
            "tmdate": 1699635982688,
            "mdate": 1699635982688,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0yGoZltUIH",
                "forum": "wpnlc2ONu0",
                "replyto": "ibz9oC318r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission557/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission557/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Weaknesses"
                    },
                    "comment": {
                        "value": "We appreciate the feedback from the reviewer. We hope our subsequent responses have adequately addressed their concerns and questions regarding the paper.\n\n**Weakness \u2160:**\n\n* **The authors should elucidate their contributions more explicitly in order to provide a comprehensive understanding of the research.**\n\nWe apologize for the confusion, and then according to the reviewer\u2019s suggestion, we re-summarized the contributions of this work:\n\n1. We propose an excitation-inhibition mechanism-assisted hybrid learning(called EIHL), which can combine the high accuracy of global learning and the low energy consumption of local learning and showed excellent accuracy and sparsity in the experiments on three datasets and two models.\n2. We obtained inspiration from neuroscience and adopted the excitation-inhibition mechanism to solve the problem of how to reasonably integrate hybrid learning. The experimental results showed that EIHL also achieved sparsity advantages, which made it possible to deploy on hardware.\n3. We used the neural excitation-inhibition mechanism to achieve the integration of global and local learning and implemented it by adjusting the weights. This not only provides a new perspective for the field of SNN training methods but also prepares for the generalization of EIHL to the ANN domain.\n\n**Weakness \u2161:**\n\n* **In the context of hybrid learning, 'STDP' process employs a contraction curve to facilitate Long-Term Depression. Nevertheless, the authors have not adequately expounded upon the association between LTD and STDP,**\n\nSpike-Timing Dependent Plasticity (STDP)[1] is a synaptic plasticity mechanism that depends on the firing sequence of pre- and post-synaptic neurons. In STDP, if the pre-synaptic neuron fires before the post-synaptic neuron, synaptic strength increases; if the pre-synaptic neuron fires after the post-synaptic neuron, synaptic strength decreases. Long-Term Potentiation (LTP) and Long-Term Depression (LTD) are the two components that constitute STDP[2]. However, significant LTP only occurs at synapses with relatively low initial strength, whereas the extent of LTD does not show an obvious dependence on the initial synaptic strength[3]. Therefore, the overall effect of STDP tends to exhibit LTD.\n\n* **and the proposed method do not have a dependence of the spike timing.**\n\nOur approach does not rely on spiking timing, yet the original intention of Spiking Neural Networks (SNN) is to emulate brain systems, distinguishing it from Artificial Neural Networks (ANN). Therefore, our process of addressing the hybrid learning problem through biological inspiration aligns with the SNN field, and there is no necessity to utilize spiking timing. Lastly, it is precisely because we do not depend on spiking timing that EIHL can be conveniently extended to the ANN field.\n\n* **It's not clear why excitation should be like STBP and depression should be like STDP.**\n\nDue to BP\u2019s powerful learning capabilities, most weights increase after STBP learning, which corresponds to the excitation of LTP[4]. STDP, on the other hand, exhibits an overall LTD effect, corresponding to inhibitory LTD[4].\n\n**Weakness \u2162:**\n\n* **The accuracy of references should be ensured. For example, the paper states, \"Spike-Timing Dependent Plasticity (STDP) was proposed based on these rules by Caporale & Dan (2008).\" However, the discovery of STDP predates 2008. Caporale & Dan (2008) is a review paper.**\n\nFirst of all, we would like to thank the reviewer for pointing out the mistake about the STDP reference, and we apologize for this mistake. We have corrected the STDP reference in section 2.1 to \u201cBased on these rules, spike-timing dependent plasticity (STDP) was proposed by Song et al. (2000)[1].\u201d We appreciate the reviewer\u2019s correction.\n\n*We hope that these revisions can help the reviewers to have a deeper understanding and a clearer understanding of the contributions and innovations of our paper. If the reviewers have any questions or suggestions, please feel free to tell us, and we will improve them as soon as possible.*\n\n> [1] Song S, Miller K D, Abbott L F. Competitive Hebbian learning through spike-timing-dependent synaptic plasticity[J]. Nature neuroscience, 2000, 3(9): 919-926.\n\n> [2] Zenke F, Gerstner W, Ganguli S. The temporal paradox of Hebbian learning and homeostatic plasticity[J]. Current opinion in neurobiology, 2017, 43: 166-176.\n\n> [3] Bi G, Poo M. Synaptic modifications in cultured hippocampal neurons: dependence on spike timing, synaptic strength, and postsynaptic cell type[J]. Journal of neuroscience, 1998, 18(24): 10464-10472.\n\n> [4] Abraham W C, Bear M F. Metaplasticity: the plasticity of synaptic plasticity[J]. Trends in neurosciences, 1996, 19(4): 126-130."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission557/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700385333320,
                "cdate": 1700385333320,
                "tmdate": 1700385333320,
                "mdate": 1700385333320,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZOr0LSnNu4",
                "forum": "wpnlc2ONu0",
                "replyto": "ibz9oC318r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission557/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission557/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Questions \u2160"
                    },
                    "comment": {
                        "value": "We appreciate the feedback from the reviewer. We hope our subsequent responses have adequately addressed their concerns and questions regarding the paper.\n\n**Question \u2160:**\n\n* **In the context of the hybrid learning rule, what is the significance of excitatory and inhibitory synapses, given that STDP and STBP do not appear to rely on the distinction between these synapse types?**\n\nFirstly, the challenge of hybrid learning lies in the integration of global and local learning. While current hybrid learning methods have achieved excellent results in terms of accuracy[1], they could do slightly better in integrating the low energy consumption advantage of local learning. Therefore, based on the exciting Long-Term Potentiation (LTP) results of Spike-Timing-Dependent Plasticity (STDP) and the inhibitory Long-Term Depression (LTD) results, an excitatory-inhibitory mechanism is adopted to balance the two.\n\nSecondly, in the cerebral cortex, the excitatory mechanism can enhance synaptic strength[2], while the inhibitory mechanism can weaken it[3,4]. Unlike previous works[5,6] that directly distinguish between excitation and inhibition at the synaptic and neuronal levels, we use the network connection status to differentiate between excitatory and inhibitory states.\n\nFinally, according to the excitatory-inhibitory mechanism, excitation and inhibition are automatically balanced. That is, when the network is overly excited, it should be inhibited, and when it is in an overly inhibited state, it should be excited. We propose an Excitatory-Inhibitory Hybrid Learning (EIHL) method, which better integrates the advantages of local and global learning, resulting in a model with high accuracy and low power consumption.\n\n* **Furthermore, it seems that excitatory and inhibitory synapses are not typically delineated in deep spiking neural networks**\n\nFor this reason, this paper does not adopt the approach of explicitly distinguishing between excitatory and inhibitory synapses in the network. Instead, it differentiates between excitation and inhibition based on the connection status of the network.\n\n> [1] Wu Y, Zhao R, Zhu J, et al. Brain-inspired global-local learning incorporated with neuromorphic computing[J]. Nature Communications, 2022, 13(1): 65.\n\n> [2] Malenka R C, Nicoll R A. Long-term potentiation--a decade of progress?[J]. Science, 1999, 285(5435): 1870-1874.\n\n> [3] Abraham W C, Bear M F. Metaplasticity: the plasticity of synaptic plasticity[J]. Trends in neurosciences, 1996, 19(4): 126-130.\n\n> [4] Li X, Steffens D C, Potter G G, et al. Decreased between\u2010hemisphere connectivity strength and network efficiency in geriatric depression[J]. Human brain mapping, 2017, 38(1): 53-67.\n\n> [5] Kern F B, Chao Z C. Short-term neuronal and synaptic plasticity act in synergy for deviance detection in spiking networks[J]. PLOS Computational Biology, 2023, 19(10): e1011554.\n\n> [6] Zhu G, Zhang Z, Zhang X Y, et al. Diverse Neuron Type Selection for Convolutional Neural Networks[C]//IJCAI. 2017: 3560-3566."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission557/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700385508882,
                "cdate": 1700385508882,
                "tmdate": 1700385508882,
                "mdate": 1700385508882,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zGHeSkcEbj",
                "forum": "wpnlc2ONu0",
                "replyto": "ibz9oC318r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission557/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission557/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Questions \u2161 and \u2162"
                    },
                    "comment": {
                        "value": "We appreciate the feedback from the reviewer. We hope our subsequent responses have adequately addressed their concerns and questions regarding the paper.\n\n**Question \u2161:**\n\n* **Weight pruning is a technique employed in deep learning to increase network sparsity by eliminating the smallest weights. Please elucidate the distinctions between the 'STDP' process in EIHL and weight pruning techniques in deep learning.**\n\nWe apologize for the confusion regarding the distinctions between the \u2018STDP\u2019 process in EIHL and weight pruning techniques in deep learning.\n\nFirstly, weight pruning techniques in deep learning are more of an engineering technique, aimed at reducing the complexity and computation of the model. However, the \u2018STDP\u2019 process in EIHL is part of the hybrid training method, aimed at mimicking LTD to suppress network excitation.\n\nSecondly, weight pruning techniques in deep learning do not fully simulate the working principle of biological neural systems, but the \u2018STDP\u2019 process in EIHL is a form of synaptic plasticity. It is just that the concept of pruning has some similarities with synaptic plasticity.\n\nFinally, in terms of the specific operation, the \u2018STDP\u2019 process in EIHL and weight pruning techniques in deep learning have some similarities. Similarity: They are both implemented by removing the smallest weights. Difference: Pruning is to directly set the weights that are less than a certain value or a certain percentage of the weight distribution to zero. The \u2018STDP\u2019 process in EIHL is to fix the update direction of the weights that are less than H(x) according to Eq.7, and then set the weights in the zero domain to zero. Moreover, the proportion of weights that need to be shrunk in the \u2018STDP\u2019 process in EIHL is automatically adjusted according to the increase of the target sparsity b, not artificially specified.\n\n**Question \u2162:**\n\n* **Kindly review the terminology and references utilized in the manuscript to ensure a more precise and coherent presentation of the research study.**\n\nWe are very grateful for the reviewer\u2019s comments, and we have carefully checked and polished our paper, correcting the citation errors and incoherent parts. We have tried our best to express the paper more accurately and coherently.\n\n*We hope that these revisions can help the reviewers to have a clearer understanding of our paper. If the reviewers have any questions or suggestions, please feel free to tell us, and we will improve them as soon as possible.*"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission557/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700385560354,
                "cdate": 1700385560354,
                "tmdate": 1700385560354,
                "mdate": 1700385560354,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PwEPO6q0lz",
                "forum": "wpnlc2ONu0",
                "replyto": "ibz9oC318r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission557/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission557/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up discussion"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nThank you so much for your review -- please let us know if you have any remaining questions or concerns so that we can address them before the deadline coming soon. Alternatively, if you feel that your original concerns are addressed, we would appreciate updating your evaluation to reflect that.\n\nThank You.\n\nPaper 557 Authors"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission557/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700719056088,
                "cdate": 1700719056088,
                "tmdate": 1700719056088,
                "mdate": 1700719056088,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IWcgQ9SiIu",
                "forum": "wpnlc2ONu0",
                "replyto": "ibz9oC318r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission557/Reviewer_iXWZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission557/Reviewer_iXWZ"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your comment"
                    },
                    "comment": {
                        "value": "I have carefully read the authors' rebuttal and while I acknowledge their efforts to address the concerns raised. However, I remain unconvinced that the contributions presented in the paper are significant enough to warrant a change in my evaluation. STDP is a important part of EIHL, however the 'STDP' training algorithm in EIHL is not STDP,  since it does not dependent on spike time and contradict to the defination of STDP. I encourage the authors to be careful of the basic concepts in neuroscience. Therefore, I will maintain my original score for the paper."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission557/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700736326778,
                "cdate": 1700736326778,
                "tmdate": 1700736326778,
                "mdate": 1700736326778,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]