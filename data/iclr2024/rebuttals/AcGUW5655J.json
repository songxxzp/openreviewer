[
    {
        "title": "Constraining Non-Negative Matrix Factorization to Improve Signature Learning"
    },
    {
        "review": {
            "id": "vpBgcW4yaA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8304/Reviewer_G6Gq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8304/Reviewer_G6Gq"
            ],
            "forum": "AcGUW5655J",
            "replyto": "AcGUW5655J",
            "content": {
                "summary": {
                    "value": "This paper proposed a combination model from NMF and SEM to conduct signature learning, and validated its effectiveness through two experiments w.r.t. matrix completion and link prediction tasks.\n\nOverall, this work looks solid but with very limited novelty."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The writting logic is good."
                },
                "weaknesses": {
                    "value": "1. the proposed SMF could be viewed as a combination of NMF and SEM, so its novelty is limited.\n2. some mathematical symbols are not clearly stated, such as A and X in section 4.1, and \\alpha in section 4.2.\n3. the selected competitive baselines can be more richful, such as spareNMF [1], GraphNMF [2]. \n\n\n[1] Patrik O. Hoyer: Non-negative Matrix Factorization with Sparseness Constraints. J. Mach. Learn. Res. 5: 1457-1469 (2004) \n[2] Shangming Yang, Zhang Yi, Mao Ye, Xiaofei He: Convergence Analysis of Graph Regularized Non-Negative Matrix Factorization. IEEE Trans. Knowl. Data Eng. 26(9): 2151-2165 (2014)"
                },
                "questions": {
                    "value": "I have no questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8304/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697160541099,
            "cdate": 1697160541099,
            "tmdate": 1699637032419,
            "mdate": 1699637032419,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "A99gVrngdM",
                "forum": "AcGUW5655J",
                "replyto": "vpBgcW4yaA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8304/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8304/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer's time and insightful comments. Here, we provide a detailed response to each point raised in the review.\n\n**The proposed SMF could be viewed as a combination of NMF and SEM, so its novelty is limited.**\n\nWe thank the reviewer for bringing out this point. We will improve the explanation of how our method relates to NMF and SEM in the revised version of the manuscript.\n\nSMF is not a straightforward combination of NMF and SEM. It can be thought of as a new model that relies on NMF and constrains the space of solutions by using ideas inspired by SEM. SEM itself is not directly plugged into the model. We proposed a \u201cself-expressive term\u201d that differs from the original term used by SEM. One of our insights was including the low dimensional representation into the self-expressive term and observing that this results in representations that can preserve better the proximities from original sub-spaces. We then derived the multiplicative rules of our model and showed that SMF can take advantage of both NMF and SEM strategies. \n\n\n**The selected competitive baselines can be more richful, such as spareNMF [1], GraphNMF [2]**\n\n\nWe thank the reviewer for this suggestion.\n\nAlthough we agree that regularizations used by sparseNMF [1] and GraphNMF[2] can help to obtain better representations, we did not select them as competitive baselines for the following reasons:\n\n- Our proposed method, SMF, and our competitive baseline (regularized NMF) already rely on L1 norm and L2 norm regularizations. Because the sparseness constraints used by sparseNMF also rely on L1 and L2 norms, we think that adding it as a competitive baseline would not be very informative for our discussions. We believe that sparseNMF would have a different behavior than the regularized NMF used as our baseline only if one wants to customize the sparsity according to the particularities of a given problem. However, in a general scenario, we believe that the chosen baseline (regularized NMF) already incorporates the main advantages mentioned by [1].\n\n- Our paper focused on association data. Although in theory graphNMF could be applied to decompose any nonnegative matrix, its main goal is finding a parts-based representation space from an input matrix in which each column corresponds to a sample vector. Then it constructs a knn graph based on the distances between the vectors. The applicability of graphNMF was shown in the context of recovering better parts of images.  For an association matrix, it is not clear whether the rows or columns should correspond to nodes in the graph, and which parts should be recovered in the learned representations. Thus, we believe that there is no strong motivation for applying graphNMF to an association matrix. \n\nIt is also important to notice that the main idea of SMF is incorporating constraints inspired by SEM into an NMF model. Thus, both sparseNMF and graphNMF could be adapted as well to incorporate these constraints."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8304/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670166262,
                "cdate": 1700670166262,
                "tmdate": 1700678633704,
                "mdate": 1700678633704,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pFbJPqBhzo",
            "forum": "AcGUW5655J",
            "replyto": "AcGUW5655J",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8304/Reviewer_NTRu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8304/Reviewer_NTRu"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a Non-negative Matrix Factorization (NMF) approach, named Self-Matrix Factorization (SMF), which factorizes association matrix into low dimensional representations. The proposed solution is derived from Self-Expressive Models (SEM). The representations or so-called signatures in the paper are learned by imposing  the reconstruction to use only data points that lie in the same low dimensional subspace. This way can guarantee the solutions are within some confined space. In addition, the paper states that the proposed method is robust and also can capture meaningful relationships. Experimental results showed that the proposed SMF product comparable or better prediction results than other existing NMF or SEM methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is easy to read and understand. The concept of proposed solution is simple. \n2. The low dimensional representations are meaningful and the intra-class distance of the factorization is larger compared to results from other methods.\n3. Results are produced by real world datasets, and the precision-recall curve is comparable to other methods."
                },
                "weaknesses": {
                    "value": "1. It is somewhat incremental improvement from SEM. The results are comparable, not significantly better. \n2. How to interpret the meaningful relationships in the low dimensional representations need more elaboration."
                },
                "questions": {
                    "value": "1. Some metrics are not well explained in the paper. What is the formula to compute Z-score? \n2. Are the low dimensional data representations stable? How would it change if small amount of data are added or deleted from association matrix?  \n3. In figure 2, Precision at top-K decreases in the proposed method, especially for movielens dataset. Is there any explanation?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No concern"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8304/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8304/Reviewer_NTRu",
                        "ICLR.cc/2024/Conference/Submission8304/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8304/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698671739116,
            "cdate": 1698671739116,
            "tmdate": 1699892959342,
            "mdate": 1699892959342,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7mguCXUgtB",
                "forum": "AcGUW5655J",
                "replyto": "pFbJPqBhzo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8304/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8304/Authors"
                ],
                "content": {
                    "title": {
                        "value": "review possibly of a different paper"
                    },
                    "comment": {
                        "value": "We believe that the reviewer has uploaded the review for a different paper."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8304/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699891195011,
                "cdate": 1699891195011,
                "tmdate": 1699891195011,
                "mdate": 1699891195011,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "baTnEL3aoZ",
                "forum": "AcGUW5655J",
                "replyto": "7mguCXUgtB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8304/Reviewer_NTRu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8304/Reviewer_NTRu"
                ],
                "content": {
                    "comment": {
                        "value": "The correct review has been updated. Thanks for pointing out the mistake."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8304/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699893036751,
                "cdate": 1699893036751,
                "tmdate": 1699893036751,
                "mdate": 1699893036751,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Sn8Yd3KfGD",
                "forum": "AcGUW5655J",
                "replyto": "pFbJPqBhzo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8304/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8304/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer's time and insightful comments. Here, we provide a detailed response to each point raised in the review.\n\n**It is somewhat incremental improvement from SEM. The results are comparable, not significantly better.**\n\n\nWe thank the reviewer for bringing out this point. Although we agree that some results are comparable for recommendation tasks, and we do mention this in the paper, we think that this is not necessarily a weakness, since the main goal of the paper is to show that we can obtain more meaningful representations. Thus, our main results are shown in the Signature Analysis section, whereas the recommendation tasks illustrate that better representations can lead to better or comparable predictive power. Nevertheless, it is interesting to notice that in the overall picture of the recommendation tasks, SMF has a significantly lower RMSE than SEM while also maintaining a higher precision than NMF.\n\n\n**How to interpret the meaningful relationships in the low dimensional representations need more elaboration.**\n\nWe thank the reviewer for this suggestion. We will clarify the interpretation analysis and what we mean by \u201cmeaningful relationships\u201d in the revised version of the manuscript. \n\nWhen we say that our model captures meaningful relationships between objects, we mean that the similarity between two signatures reflects similarities in terms of latent processes underlying the data. Since we assume that the latent features of our model encode relevant processes, we expect that the learned signatures of two objects will be similar if they share latent processes, and dissimilar, otherwise. To test this hypothesis, we leverage additional knowledge about the objects, which was not utilized during the learning process. This enables us to cluster objects that are expected to share latent processes. For instance, drugs within the same ATC category are likely to share molecular mechanisms. If our hypothesis holds true, we expect that drugs within the same ATC category will exhibit more similar signatures than those from different categories.\n\n\n\n\n**Some metrics are not well explained in the paper. What is the formula to compute Z-score?**\n\nThank you for pointing this out. We will include the formula and explanation in the revised version of the manuscript.\n\nWe use the Z-score from the two-sample z-test to measure the difference between signature similarities of objects that are from the same group and objects that are from different groups. Given the average signature similarity of objects that are in the same group ($\\mu_in$), the average signature similarity of objects that are in different groups ($\\mu_out$), and the corresponding standard deviations (\\sigma_{in} and \\sigma_{out}), the z-score formula is:\n\n$Z = \\frac{ \\mu_{in} - \\mu_{out}}{\\sqrt{\\frac{\\sigma_{in}^2}{n_{in}} + \\frac{\\sigma_{out}^2}{n_{out}}}}$,\n\nwhere $n_{in}$ and $n_{out}$ are the number of object pairs intra- and inter-groups, respectively. \n\nWe can interpret the z-score as a normalized distance that measures how different two distributions are by adjusting the difference between the means according to their standard deviation.\n\n\n**Are the low dimensional data representations stable? How would it change if small amount of data are added or deleted from association matrix?**\n\nAs evidenced by our experiments, the generated signatures consistently yield stable results across different runs. To address the missing data concern, we ran experiments similar to the ones detailed in Section 4.3 for signature analysis, but we specifically trained the model using only 90% of the known interactions in the SIDER dataset. Notably, even with this reduced dataset, the difference in results compared to using the full matrices is minimal.\n\nHere we reported the means of the Z-Score difference between the means for both models NMF and SMF:\n\n| Models | ATC $1^{st} Level$ | ATC $2^{nd} Level$ | ATC $3^{rd} Level$ |\n| --------- |  :--------------: | :--------------: | :--------------: |\n| NMF | $-0.1013$ | $0.3934$ | $0.7411$ |\n| SMF | $2.9746$ | $7.0157$ | $8.5293$ |\n\nWe plan to run similar experiments for the other datasets and include them in the revised version of the manuscript.\n\n\n**In figure 2, Precision at top-K decreases in the proposed method, especially for movielens dataset. Is there any explanation?**\n\nFor that particular case, the first top-K recommendations already contain a high amount of true positives, which results in high precision. As the value of K increases, the amount of true positives does not increase accordingly, as most of them were already captured in the highest value for K. Note that the same trend can be observed for SEM when examining the precision score for K equal to 20 and upwards."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8304/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670007767,
                "cdate": 1700670007767,
                "tmdate": 1700670007767,
                "mdate": 1700670007767,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "isZXuZf6By",
            "forum": "AcGUW5655J",
            "replyto": "AcGUW5655J",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8304/Reviewer_oagP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8304/Reviewer_oagP"
            ],
            "content": {
                "summary": {
                    "value": "This paper revisits the Nonnegative Matrix Factorization problem, by adding extra constraints inspired by the Self Expressive Models. The authors present some empirical results on different accuracy and qualitative metrics."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The paper is well-written and has a good empirical analysis of the chosen datasets."
                },
                "weaknesses": {
                    "value": "The area of NMF is very well studied, and the truth is that with more modern methods inspired by relational learning, GNNs can do a much better job at the applications in which NMF has been used. The paper has some good intuitions, and the idea of introducing the Self Expressiveness on the W matrix subspace is interesting. However, we don't have any theoretical guarantees that this is going to work in general. I would accept intuition as a guidance, but:\n- NMF suffers from local minima, and this needs to be studied extensively, even empirically\n- The number of experiments is too small to get some significant guarantees. We need a much bigger pool of benchmarks\n- Scalability is another issue not examined. This is strongly tied to runtimes and convergence steps needed"
                },
                "questions": {
                    "value": "Can the authors quantify the robustness to local minima?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8304/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698810600795,
            "cdate": 1698810600795,
            "tmdate": 1699637032153,
            "mdate": 1699637032153,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ObKpovmX0k",
                "forum": "AcGUW5655J",
                "replyto": "isZXuZf6By",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8304/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8304/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response(1/2)"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer's time and insightful comments. Here, we provide a detailed response to each point raised in the review.\n\n**The area of NMF is very well studied, and the truth is that with more modern methods inspired by relational learning, GNNs can do a much better job at the applications in which NMF has been used.**\n\nWe acknowledge the reviewer's point regarding NMF being well studied and that there are different methods that can achieve a better recommendation performance. However, it\u2019s important to note that NMF remains a prevalent and impactful technique within the field, and holds the potential to outperform even deep learning-based architectures [1] while obtaining meaningful representations for objects. \nIn line with the conference's focus on learning representations, our paper aims to enhance the learning of the meaningful representations that you would normally obtain from NMF. These are learned from complementary analysis strategies and extracting patterns from observed association data. This aspect holds significance as SMF has the ability to provide insights into aspects of the problem, such as identifying categories of drugs that were not explicitly involved in the training process.\nIn the paper, we report recommendation metrics only as indirect evidence of the capabilities of SMF in the context of recommendation tasks. We don\u2019t claim that this version of matrix decomposition is an effective state-of-the-art recommender. While graph neural networks (GNN) and other deep learning strategies are frequently employed in various domains, including recommendation systems, their interpretability often lags behind. In contrast, tasks such as interpretability are more intuitive for NMF and SEM.\nWe intend to revise the manuscript by incorporating the relevant citation and providing a more detailed explanation to elucidate this particular point.\n\n[1] Rendle, Steffen, et al. \"Neural collaborative filtering vs. matrix factorization revisited.\" Proceedings of the 14th ACM Conference on Recommender Systems. 2020.\n\n**NMF suffers from local minima, and this needs to be studied extensively, even empirically.**\n\nWe acknowledge that the optimization process described in this paper suffers from local minima, similar to many other machine learning frameworks in the literature, and we do not believe this is a significant problem for the task of learning meaningful representations. Our reported results obtained from different runs of our framework with different initializations were shown to be stable, indicating that SMF is able to consistently propose similar solutions. We will modify the paper to clarify that the proposed optimization process does not offer any robustness to local minima.\n\n**The number of experiments is too small to get some significant guarantees. We need a much bigger pool of benchmarks.**\n\nTo address this concern, we selected an additional dataset, called ModCloth [2], where users assign ratings to clothes. Due to the time constraints in the rebuttal phase, we conducted experiments only on one additional dataset. We also had to skip the exhaustive hyperparameter tuning and perform a subsetting of the dataset to manage computational time. In the revised version of the manuscript, we intend to include the full set of experiments on the complete or a less subsetted, version of this dataset.\n\nAfter randomly subsetting the data, we obtain a data matrix $C \\in \\mathbb{R}^{n_c \\times m_c}$ for $ n_c = 5419$ clothing items and $m_c = 32.089$ users with a density of $\\approx 0.05 \\%$. Each clothing item belongs to $1$ of $66$ different categories that are used for the signature analysis. The results shown in the table below correspond to one run of NMF, SEM and SMF. Similarly to the other datasets, SMF clearly outperforms the other methods and achieves better category separation in contrast to NMF.\n\n\n\n| Models | RMSE    | Correlation | AUROC  | AUPRC   | Z-score |\n| ------ | :-----: | :---------: | :----: | :-----: | :-----: |\n| NMF    | $1.5847$ | $0.0801$ | $0.6871$ |$1.64\\mathrm{e}{-4}$ | $62.34$ |\n| SEM    | $4.3112$ | $0.0423$ | $0.6230$ |$2.50\\mathrm{e}{-4}$ | (-) |\n| SMF    | $1.3291$ | $0.1012$ | $0.7515$ |$2.84\\mathrm{e}{-4}$ | $238.35$ |\n\n\n[2] Misra, Rishabh, Mengting Wan, and Julian McAuley. \"Decomposing fit semantics for product size recommendation in metric spaces.\" Proceedings of the 12th ACM Conference on Recommender Systems. 2018."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8304/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668648815,
                "cdate": 1700668648815,
                "tmdate": 1700678505778,
                "mdate": 1700678505778,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]