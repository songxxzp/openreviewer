[
    {
        "title": "On-Policy Policy Gradient Reinforcement Learning Without On-Policy Sampling"
    },
    {
        "review": {
            "id": "4fu9YJ80h1",
            "forum": "U4RoAyYGJZ",
            "replyto": "U4RoAyYGJZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4129/Reviewer_KNrT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4129/Reviewer_KNrT"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new algorithm, PROPS, which seeks to entail more efficient off-policy learning by drawing inspirations from on-policy learning algorithms, i.e., making the update data distribution stay close to the learner policy (on-policy). The paper provides a new algorithm, with experimental results that show improvements over baselines such as PPO."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is interesting in that it tries to improve off-policy learning by drawing inspirations from on-policy learning algorithms, and the intuition that one should try to make the data to be as on-policy as possible. The paper hinges on an observation that for the learning to be on-policy, it suffices for the data distribution to be on-policy, and one does not always have to use the same policy for the update."
                },
                "weaknesses": {
                    "value": "The idea of using near on-policy update or constrained update to improve off-policy learning has been extensively studied in the RL literature, and as a result, the constrained update rule proposed in the paper is not very novel.  Though the idea of using off-policy distribution that matches on-policy update is interesting, it does not appear enough to constitute a technically solid algorithmic contribution. Indeed, the main idea presented in the paper is not fundamentally different from the trust region policy updates, and does not present additional insight beyond what's already in the existing literature."
                },
                "questions": {
                    "value": "=== **PPO objective and CLIP objective** ===\n\nThe PPO objective appears similar as the CLIP objective in Eqn 4. It is important to note that PPO takes $\\theta$ and $\\theta_{\\text{old}}$ to be close by policy (in practice one iteration away from each other). In the limit when these two policies are very close, the update approximates the policy gradient update. In light of this, in order for the update to be sensible for CLIP, we also need $\\theta$ and $\\phi$ to be close to one another. In practice this would be softly enforced by the KL loss. Does that sound right? What would be the major difference between CLIP and PPO objectives, despite the technical differences on the clipping operation and KL regularizations.\n\n=== **CLIP objective** ===\n\nIn Eqn 4 and 5 we specify a loss for both $\\theta$ and $\\phi$, do we update them both at the same time? Or is one updated at a slower rate compared to another.\n\nIf $\\theta$ and $\\phi$ are practically close to each other, then the CLIP objective is effectively very similar to the idea of trust region policy updates.\n\n=== **On-policy and off-policy** ===\n\nI like the comment that on-policy learning should really be to match the learning distribution, not the exact policy. However, it might not pay off much to read too much into the terminology itself -- I think much of the community would agree that the naive implementation of on-policy learning would be to learn from the same policy, whereas on-policy samples are all that's needed to perform the update. In practice since the policy keeps drifting away, it is very difficult to maintain even on-policy samples or sub-sample those from the replay buffer.\n\nI think the paper has a very nice starting point to identify this conceptual insight, but didn't dive deep enough to address the issue. The CLIP objective is not fundamentally different from trust region policy updates, which have been in the literature since the beginning of deep RL.\n\n=== **Comparison in Fig  5** ===\n\nWhat's PPO-buffer exactly? Does it correspond to running PPO on off-policy samples naively? I'd feel that PROMPS is a slightly more adaptive variant of PPO-buffer, so that it can make use of buffer data as well. But I am surprised about the empirical gains, since PPO-buffer, though using off-policy data, still applies clipping and hence enforces a trust region constraint during the update. This means the policy update still maintains its stability over time, even when using very off-policy samples."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No concerns."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4129/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698690258011,
            "cdate": 1698690258011,
            "tmdate": 1699636378276,
            "mdate": 1699636378276,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bBMKDePhx4",
                "forum": "U4RoAyYGJZ",
                "replyto": "4fu9YJ80h1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4129/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4129/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate your thoughtful review and are glad that you found our approach to be interesting!\n\nWe are slightly confused by some of these comments and hope we can clarify our novelty during the discussion period. We would like to emphasize that the core novelty of our work is how we demonstrate that *off-policy sampling* can reduce sampling error and improve the data efficiency of *on-policy* policy gradient algorithms. In other words, the core novelty is not only the PROPS update we use to adapt our behavior policy but moreover the fact that we are using a separate behavior policy for on-policy learning in the first place. So while KL penalties and clipping (as in PPO) are not novel on their own, our use of them to enable scalable adaptive sampling with the behavior policy is novel to the best of our knowledge. \n\nBelow, we address your comments and questions.\n\n# **Comparing the PPO objective and the CLIP objective of PROPS**\n\nThe major difference between these objectives is that CLIP is an objective for *behavior policy* optimization and the PPO objective is for optimizing the *target policy.* In CLIP, we weight behavior policy updates by -1 to increase the probability of under-sampled actions, where as PPO weights policy changes by the advantage $A(s, a)$ to increase the probability of good actions with large advantages. Note that we use the PPO objective to optimize the target policy in both PROPS and vanilla PPO.\n\n# **Are $\\theta$ and $\\phi$ close to each other?**\n\nYour understanding is correct. At the start of each PROPS update, we set $\\phi \\gets \\theta$ (line 3 in Algorithm 2), and then make a location adjustment to $\\phi$ that increases the probability of under-sampled actions. You are also correct in saying that regularization helps to keep the behavior policy close to the target policy and that the PROPS update is similar to a trust-region update.\n\n# **Are $\\theta$ and $\\phi$ updated at different frequencies?**\n\nThe behavior policy is updated more frequently than the target policy so that the behavior policy has a chance to reduce sampling error before each target update. In our experiments, the behavior policy is updated every 256 timesteps and the target policy is updated every 1024 - 8192 timesteps.\n\n# **Why we emphasize the difference between on-policy data vs. on-policy sampling**\n\nWhile we agree that the formulation of the on-policy update does indeed point to on-policy data as the key desiderata, unfortunately, we know of many sources that distinguish on-policy learning by its sampling distribution, including some prominent educational resources:\n\n1. Sutton & Barto\u2019s RL textbook [1] states, \u201cthe distinguishing feature of on-policy methods is that they estimate the value of a policy while using it for control.\u201d\n2. Lecture 5 of David Silver\u2019s RL course [2] states that on-policy algorithms \u201clearn about policy \u03c0 from experience sampled from \u03c0.\u201d\n3. The taxonomy of RL algorithms provided in OpenAI\u2019s Spinning Up package [3] states that with on-policy algorithms, \u201ceach update only uses data collected while acting according to the most recent version of the policy.\u201d\n\nBecause the view that \"on-policy refers to a sampling procedure\" is so prominent, we wanted to elaborate on the distinction between on-policy data and on-policy sampling to emphasize how this popular perception is not a reality. We will clarify these points in our revisions and hope to further discuss them with reviewers.\n\n# **PPO-Buffer and its performance**\n\nYour understanding is correct; PPO-Buffer naively reuses historic off-policy data without any algorithmic changes, and PROPS is indeed an adaptive version of PPO-Buffer. \n\nPPO-Buffer outperforms PPO in Walker2d-v4 and Humanoid-v4, which indeed suggests that PPO\u2019s clipping mechanism may be robust to stale data. In fact, OpenAI reused historic data in a similar fashion when using PPO to train the RL agents that ultimately defeated the world champions in the Dota 2 game [4]. However, we note that in most tasks we consider, PPO-Buffer performs comparably to PPO.\n\nThank you again for your review, and please let us know if our response clarifies your comments and questions! We are happy to discuss any follow-up comments.\n\n# **References**\n\n[1] Sutton, Richard S., and Andrew G. Barto. \"Reinforcement learning: An introduction.\" MIT press, 2018.\n\n[2] Silver, David. \"RL Course by David Silver, Lecture 5: Model-Free Control.\" University College London, Computer Science Department, 2015. https://www.davidsilver.uk/wp-content/uploads/2020/03/control.pdf\n\n[3] Achiam, Joshua. Spinning Up in Deep Reinforcement Learning. Github, 2018. https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html#a-taxonomy-of-rl-algorithms\n\n[4] Berner et. al. \"Dota 2 with Large Scale Deep Reinforcement Learning.\" Arxiv 2019."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4129/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700175928059,
                "cdate": 1700175928059,
                "tmdate": 1700176203854,
                "mdate": 1700176203854,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "x9BfkSKFwc",
            "forum": "U4RoAyYGJZ",
            "replyto": "U4RoAyYGJZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4129/Reviewer_8MkC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4129/Reviewer_8MkC"
            ],
            "content": {
                "summary": {
                    "value": "This work focuses on sampling error problem of on-policy RL algorithms. Following the previous work ROS, this paper introduces the understanding: on-policy learning requires on-policy data, not on-policy sampling. This paper proposes a new method called Proximal Robust On-Policy Sampling (PROPS), aiming at adaptively correcting sampling error in previously collected data by increasing the probability of sampling actions that are under-sampled with respect to the current policy. Two more mechanisms are utilized to address the issues of destructive large policy updates and gaussian policy updates. The proposed method is evaluated in MuJoCo continuous control tasks and three discrete-action tasks, against PPO and PPO-Buffer, ROS."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is almost clear and overall well-written.\n- Intuitive motivating examples are used which help smooth reading.\n- The experiments are conducted with both learning performance comparison and sampling error analysis."
                },
                "weaknesses": {
                    "value": "My main concern lies at the mismatch between *the idea* mentioned multiple times in the first half of the paper (i.e., adaptively corrects sampling error in previously collected data by increasing the probability of sampling actions that are under-sampled with respect to the current policy) and *the practical method* proposed later:\n\n- The idea is to adapt the behavior policy so that the resultant data distribution can be more on-policy, as illustrated in Figure 1.\n- However, the practical method is to adapt the behavior policy to fit the collected data (with the gradient $\\nabla_{\\phi}L=-\\nabla_{\\phi} \\sum_{(s,a) \\in D} \\log \\pi_{\\phi}(a|s)$ and its improvement).\n\n&nbsp;\n\nFor some specific point, the second paragraph of Section 5 says, \u2018To ensure \u2026 updates to $\\pi_{\\phi}$ should attempt to increase the probability of actions which are currently under-sampled with respect to $\\pi_{\\theta}$\u2026 the gradient $\\nabla_{\\phi}L=-\\nabla_{\\phi} \\sum_{(s,a) \\in D} \\log \\pi_{\\phi}(a|s)$ provides a direction to change $\\phi$ such that under-sampled actions have their probabilities increased\u2019. I do not get the point. Since the expectation is over the collected data, the gradient seems to be an MLE based on the collected data (or imitation learning) and it is almost the same as Equation 7 in the appendix (with a difference in max and min).\n\nPlease correct me if I misunderstand it.\n\n&nbsp;\n\nBesides, I feel that the example and discussion used in Section 4 are improper. The main idea of the paper and the illustrative example in Figure 1 focus on the distribution mismatch of *action*; while the discussion in Section 4 is mainly about the distribution mismatch of *outcome*, which cannot be known in practical learning process (i.e., we have no idea about the existence of a large reward). This turns to be a little bit off the track to me.\n\n&nbsp;\n\nThe empirical results are not convincing enough to me, especially under the concerns I mentioned above. \n\nImportant baselines ROS and GePPO are not included in Figure 5. Computational cost like wall-clock time is not discussed in the main body of the paper. Besides, the ablation on the KL term in learning performance is expected."
                },
                "questions": {
                    "value": "1) Since there could be under-sampled actions, there should be also over-sampled actions. How are they considered in the proposed method? \n\n2) Why is ROS not included in learning performance comparison like Figure 5? In addition, I think it is necessary to include GePPO as a baseline, since the proposed method aims at improving learning efficiency of on-policy RL.\n\n3) How is the computation cost of PROPS, especially the wall-clock time?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4129/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4129/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4129/Reviewer_8MkC"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4129/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698725364204,
            "cdate": 1698725364204,
            "tmdate": 1700574006430,
            "mdate": 1700574006430,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2pe6DtUeak",
                "forum": "U4RoAyYGJZ",
                "replyto": "x9BfkSKFwc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4129/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4129/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Rebuttal (1/2)"
                    },
                    "comment": {
                        "value": "We would first like to thank the reviewer for their comments and questions; we\u2019re glad you found our paper well-written and appreciated our motivating examples. We believe your comments can be easily addressed with small changes to our current paper. We describe these in our response below and kindly ask you to consider raising your rating.\n\n## **Perceived mismatch between the idea and practical method**\n\nWe believe this concern arises from a simple misunderstanding of the direction of the ROS/PROPS update, and we will clarify this point in the main paper. The key point is that we are doing gradient *ascent* on the negative log-likelihood (NLL) rather than gradient *descent* as done in MLE learning. You correctly point out that MLE learning would have a mismatch with the goal of our work. However, since we use gradient *ascent* on the NLL, the update has the desired effect of increasing the probability of under-sampled actions.\n\nIt is important to note that the ROS and PROPS algorithms do not aim to fully optimize their objectives. Rather, at each iteration, the behavior policy is set equal to the target policy and then the gradient is used to make a local adjustment which decreases the probability of over-sampled actions, thereby increasing the probability of under-sampled actions.\n\n# **Example in Section 4**\n\nYou are correct in that PROPS focuses on correcting the state-conditioned distribution of actions as opposed to the distribution of trajectories (since we generally don\u2019t know the trajectory distribution). To convey intuition for how sampling error could impact learning, our example in Section 4 focused on sampling error in the outcomes space. However, we see how this was confusing to multiple reviewers and so we can make a few minor modifications to make the example focus on action probabilities.\n\nIn particular, we can add the following action probabilities to Figure 2: \n\n$\\pi(left | s_0) = 0.5$, \n\n$\\pi(right | s_0) = 0.5$,\n\n$\\pi(left | s_L) = 0.02$,\n\n$\\pi(right | s_L) = 0.98$\n\nWhere, for brevity, we use $left$ = \u201cattempt to move\u201d and $right$ = \u201cremain standing\u201d. These action probabilities preserve the outcome probabilities originally listed.\n\nAfter 100 visits to $s_0$, the agent may never sample the $left$ branch at $s_L$ and believe that the $left$ branch at $s_0$ always leads to a return of -1 (falling). Having observed a return of +1 (standing) in the $right$ branch at $s_0$, the agent reduces the probability of choosing the $left$ branch at $s_0$, ultimately making the agent more likely to converge to a suboptimal standing policy. We can help correct this sampling error by increasing the probability of sampling the under-sampled $left$ branch at $s_L$.\n\nWe hope our response clarifies the soundness of the ROS/PROPS updates as well as the example in section 4. If you have further comments or questions regarding our empirical results, we would be more than happy to discuss further!\n\n# **Baselines**\n\nWe considered these baselines but ultimately decided to exclude them from the main RL experiments shown in Figure 5.\n\nWe exclude ROS from our RL experiments for two reasons:\n1. Since ROS fails to reduce sampling error even when the target policy is fixed (Fig. 3 in the main paper and Fig. 7 and 8 in Appendix C of the supplemental material), we expect ROS to also fail to reduce sampling error in the more difficult RL setting where the target policy is continually updated. \n2. ROS is too expensive to run in the RL setting. ROS updates the behavior policy *every timestep* using a gradient computed over *all data* in the agent\u2019s buffer, whereas PROPS updates the target policy every 256 timesteps using minibatches sample from the agent\u2019s buffer.\n\nWe can clarify both of these reasons in our revisions.\n\nPROPS and GePPO are orthogonal approaches to improve the data efficiency of on-policy learning: PROPS uses adaptive data collection to reduce sampling error, while GePPO reweights data according to historic policies *after* data collection. We ultimately decided that the comparison distracted from our main focus of determining whether adaptive off-policy sampling could produce sufficiently on-policy data for effective learning. For the purpose of answering the main questions of our study, the empirical evaluation is complete as-is."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4129/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700172106935,
                "cdate": 1700172106935,
                "tmdate": 1700172106935,
                "mdate": 1700172106935,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "j9h4c9fj77",
                "forum": "U4RoAyYGJZ",
                "replyto": "x9BfkSKFwc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4129/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4129/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Rebuttal (2/2)"
                    },
                    "comment": {
                        "value": "# **Ablations on KL regularizer and clipping mechanism**\n\nWe ablate PROPS\u2019s KL regularizer and the clipping mechanism in Appendix C and D and find that both help reduce sampling error and improve agent performance:\n* Appendix C, Fig. 9 shows how clipping and regularization affect PROPS\u2019s ability to reduce sampling error with respect to a *fixed* target policy. In this setting, clipping generally has a stronger effect on sampling error reduction than regularization.\n\n* Appendix D, Fig 12 shows how clipping and regularization affect PROPS\u2019s ability to reduce sampling error in the RL setting with a *continually changing* target policy. In this setting, both regularization and clipping are crucial to reducing sampling error.\n\n* Appendix D, Fig 13 shows IQM return achieved by PROPS during RL training with and without regularization or clipping. In nearly all tasks, PROPS performs best with both clipping and regularization.\n\n# **Wall-clock time comparisons**\n\nWe can include wall-clock time comparisons of PROPS, PPO, and PPO-Buffer in our revisions. We will share our results in a follow-up comment once we have them. \n\nIntuitively, we expect PROPS will take roughly twice as much time as PPO-Buffer; both algorithms perform updates using the same amount of data, though PROPS learns two policies. \n\nAgain, please let us know if our response clarifies your comments and questions! We will happily discuss any follow-up comments you may have."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4129/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700172337446,
                "cdate": 1700172337446,
                "tmdate": 1700172337446,
                "mdate": 1700172337446,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FivQR33r6X",
                "forum": "U4RoAyYGJZ",
                "replyto": "j9h4c9fj77",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4129/Reviewer_8MkC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4129/Reviewer_8MkC"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors' Rebuttal"
                    },
                    "comment": {
                        "value": "I appreciate the response provided by the authors. Some of my concerns are addressed.\n\nFor one more questions, the author mentioned 'Rather, at each iteration, the behavior policy is set equal to the target policy and then the gradient is used to make a local adjustment which decreases the probability of over-sampled actions, thereby increasing the probability of under-sampled actions' in the response. According to Algorithm 1, it seems that the behavior policy is set to the target policy only once at the beginning. Is it a mistake? Is line 4 ought to be between line 6 and line 7?"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4129/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700573970531,
                "cdate": 1700573970531,
                "tmdate": 1700573970531,
                "mdate": 1700573970531,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "fc4bXTU9XE",
            "forum": "U4RoAyYGJZ",
            "replyto": "U4RoAyYGJZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4129/Reviewer_ErDF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4129/Reviewer_ErDF"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new adaptive sampling approach to reducing sampling error in off-policy policy optimization. Specifically, the paper proposes to collect data such that the data distribution is more on-policy to the target policy. They do this by training a separate behavior policy according to a objective function which encourages the behavior to take actions which correct the sampling distribution of the replay buffer. They show some empirical improvements in standard benchmarks compared to PPO."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "Overall, I think the proposed method is quite reasonable and an interesting approach to correcting for the use of off-policy data in policy optimization. I see two weaknesses in the paper as currently presented."
                },
                "weaknesses": {
                    "value": "I am updating my score based on conversations w/ the authors. Specifically, I appreciate the re-focus and the additions for clarity.\n\n\n--- before edits ----\n\nOverall, I think the proposed method is quite reasonable and an interesting approach to correcting for the use of off-policy data in policy optimization. I see two weaknesses in the paper as currently presented.\n\n## Characterization of on-policy updates (i.e. data or sampling)\n\nI believe the paper presents an inaccurate characterization of the history of on-policy vs off-policy algorithms, and this detracts from the overall presentation of the paper. The core of my issue is the characterization that the field is uncertain if on-policy updates require on-policy data or on-policy sampling. \n\nTo understand my complaint, I want to begin with policy evaluation and think about how these principle ideas transitioned into generalized policy iteration and concepts surrounding the usual suspects of on-policy learning (i.e. those using \u201con-policy sampling\u201d). If we look at the fundamentals of policy evaluation in dynamic programming we see a very clear picture of the ideal update (i.e. using the probabilities of the state distribution and policies directly), but at the cost of knowing the transition probabilities and having to sweep over all states and actions for a single update. When improving the policy we go between a policy improvement step (i.e. maximize the learned value function) and the policy evaluation step. Generalized policy iteration made this update more general by enabling the ability to improve a policy without running policy evaluation for every state and action. \n\nFrom generalized policy iteration we can get to many of our on-policy algorithms such as TD, sarsa, on-policy actor-critic algorithms, and many others with the inclusion of sampling from distributions instead of knowing the distributions ahead of time. This type of on-policy learning uses a transition and throws it away, meaning any policy improvement done will always appear in the data _and_ there is no stale buffer of data. From this trajectory of literature it is clear, on-policy algorithms are designed and work well with data that is distributed according to the target policy. The easiest way to get this data is through sampling according to the behavior.\n\nWith the inclusion of replay buffers, even on-policy algorithms are likely to have off-policy drifting in the data used to train them. We see these problems accumulate in bad behavior (see Deadly Triad) and hacks (e.g. Target Network) designed to navigate this problem to try and get more out of the data an agent experiences. \n\nThe paper correctly notice a difference in the sampling error when the data isn\u2019t infinitely large, where the adaptive sampling method presented here is perfectly suited to fill this gap and try and correct for the discrepancies in the distribution of actions (which might influence the distribution of states, but I\u2019m unclear how this influence would evolve). \n\nTo wrap this up, I don\u2019t have a complaint about the method, and am personally very excited about more active techniques of the agent modifying its behavior to get more favorable data distributions. I just think the narrative is misleading, and a dichotomy presented by the authors is ill-conceived and distracting.\n\n## Example in section 4\n2. The example used in section 4 is quite confusing in the context of off-policy and on-policy policy optimization. The main confusion from my perspective is the confusion between a trajectory and an action, which propagates throughout the paper in that we are only correcting the distribution according to the action distribution and no the state distribution.\n\n\n## Some suggestions and questions about the empirical section:\n1. How were the hyperparameters chosen for all the methods?\n2. I think the empirical section would benefit from the inclusion of an off-policy baseline to see how much progress is being made by the adaptive sampling method. While you don\u2019t necessarily have to beat this baseline, including SAC could be beneficial to interpreting the results of your method!"
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4129/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4129/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4129/Reviewer_ErDF"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4129/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698790341045,
            "cdate": 1698790341045,
            "tmdate": 1700681168476,
            "mdate": 1700681168476,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ztlpaVIMc2",
                "forum": "U4RoAyYGJZ",
                "replyto": "fc4bXTU9XE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4129/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4129/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the in-depth comment on the framing of our work. We completely agree with the historical framing you provide,  so we believe a minor change can clarify the narrative. However, before doing so, we would like to explain why we framed the paper as we did and discuss how we can combine your suggestions with our original motivation.\n\nWhile we agree that the historical trajectory does indeed point to on-policy data as the key desiderata for on-policy policy gradient methods, unfortunately, we know of many cases where on-policy learning is defined by the sampling distribution, including some prominent educational resources:\n1. Sutton & Barto\u2019s RL textbook [1] states, \u201cthe distinguishing feature of on-policy methods is that they estimate the value of a policy while using it for control.\u201d\n2. Lecture 5 of David Silver\u2019s RL course [2] states that on-policy algorithms \u201clearn about policy \u03c0 from experience sampled from \u03c0.\u201d\n3. The taxonomy of RL algorithms provided in OpenAI\u2019s Spinning Up package [3] states that with on-policy algorithms, \u201ceach update only uses data collected while acting according to the most recent version of the policy.\u201d\n\nBecause the view that \"on-policy refers to a sampling procedure\" is so prominent, we chose a more pointed narrative to emphasize that it is the empirical distribution of data that really matters for accurate gradient estimates. In a sense, our work is pointing back to the historical trajectory you give and saying that we really do want to have samples weighted according to their actual probability and not just generated according to a specific process.\n\nOverall, we believe that there is not a large gap between our view and the reviewer\u2019s view and that we can fine-tune the discussion in our paper to note the historical trajectory given. Specifically, it will be straightforward to 1) add discussion on the historical basis of these methods and say that, yes, clearly on-policy data is the goal, and 2) clarify that we aim to correct a popular misconception in how these algorithms are understood. We hope that we can discuss this point more with the reviewer over the next week to make the narrative as strong as possible.\n\n[1] Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 2018.\n\n[2] Silver, David. \"RL Course by David Silver, Lecture 5: Model-Free Control.\" University College London, Computer Science Department, 2015. https://www.davidsilver.uk/wp-content/uploads/2020/03/control.pdf\n\n[3] Achiam, Joshua. Spinning Up in Deep Reinforcement Learning. Github, 2018. https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html#a-taxonomy-of-rl-algorithms"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4129/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700077758260,
                "cdate": 1700077758260,
                "tmdate": 1700077758260,
                "mdate": 1700077758260,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7nkakBeNM9",
                "forum": "U4RoAyYGJZ",
                "replyto": "ztlpaVIMc2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4129/Reviewer_ErDF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4129/Reviewer_ErDF"
                ],
                "content": {
                    "comment": {
                        "value": "Great finds! Yes after further consideration, you are correct in that there seems to be a misleading nature to some of these conversations when moving to the case of acting in an environment which I have forgotten (and/or misplaced in my head). I love the adaptive sampling approach, so am more than happy to come to some middle ground here to make the paper's narrative better.\n\nI think the crux of this comes from the fact that when jumping to the RL case (i.e. no longer in dynamic programming land). When jumping to sampling (of any kind) there will always be differences in the empirical and true distributions, but when we discuss acting in the environment in RL this isn't communicated or addressed in the current curriculum most students are exposed to. I believe the best way to address that in this paper would be to discuss the historical context briefly (i.e. how we go from DP to sampling approaches) and discuss how the empirical distribution and true distributions are likely not the same within the confines of the agent's finite life (i.e. our infinite data assumptions are bogus and we should deal with it).\n\nI totally agree with the author on all their points, and now better understand where they were coming from. I think the pointed approach could work here, but you need to show mastery on the historical context of your statements to prevent people like me from getting grumpy :). While I think this can work, I think getting to the heart of your motivation \"the empirical and true distributions are not the same in a finite life\" would be more straightforward and would not come off as denigrating prior work (just my 2 cents). \n\nI think this could thread the needle quite well and likely won't need a considerable amount of work. If this can be fixed, the comments from multiple authors about the example in section 4 addressed, and some of the comments about the clarity in the experimental section, then this paper would be a great contribution (even for a future conference if it doesn't get in here)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4129/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700082258603,
                "cdate": 1700082258603,
                "tmdate": 1700082258603,
                "mdate": 1700082258603,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ooS0A0JsOW",
                "forum": "U4RoAyYGJZ",
                "replyto": "fc4bXTU9XE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4129/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4129/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Rebuttal (2)"
                    },
                    "comment": {
                        "value": "We greatly appreciate your response regarding our clarification of the motivating narrative! We'll provide another update on that soon. In the meantime, we wanted to use this separate post to respond to your other comments regarding the example in section 4 and a few experimental details. \n\n# **Example in section 4**\n\nYou are correct in that PROPS focuses on correcting the state-conditioned distribution of actions as opposed to the distribution of trajectories (since we generally don\u2019t know the trajectory distribution). To convey intuition for how sampling error could impact learning, our example in Section 4 focused on sampling error in the outcomes space. However, we see how this was confusing to multiple reviewers and so we can make a few minor modifications to make the example focus on action probabilities.\n\nIn particular, we can add the following action probabilities to Figure 2: \n\n$\\pi(left | s_0) = 0.5$, \n\n$\\pi(right | s_0) = 0.5$,\n\n$\\pi(left | s_L) = 0.02$,\n\n$\\pi(right | s_L) = 0.98$\n\nWhere, for brevity, we use $left$ = \u201cattempt to move\u201d and $right$ = \u201cremain standing\u201d. These action probabilities preserve the outcome probabilities originally listed.\n\nAfter 100 visits to $s_0$, the agent may never sample the $left$ branch at $s_L$ and believe that the $left$ branch at $s_0$ always leads to a return of -1 (falling). Having observed a return of +1 (standing) in the $right$ branch at $s_0$, the agent reduces the probability of choosing the $left$ branch at $s_0$, ultimately making the agent more likely to converge to a suboptimal standing policy. We can help correct this sampling error by increasing the probability of sampling the under-sampled $left$ branch at $s_L$.\n\nPlease let us know if further clarification is needed on this updated example!\n\n# **Hyperparameters:**\n\nWe tune all algorithms using a hyperparameter sweep described in Appendix F (Tables 2 and 3). We perform this sweep separately for all tasks and all algorithms and report results for top-performing hyperparameters. \n\n# **Off-policy baseline:**\n\nWe agree that it would be interesting to compare PROPS to an off-policy algorithm. We\u2019ll have to get back to you on this matter; we first want to integrate your feedback on the narrative we crafted."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4129/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700170813298,
                "cdate": 1700170813298,
                "tmdate": 1700170813298,
                "mdate": 1700170813298,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "roEa683tMW",
                "forum": "U4RoAyYGJZ",
                "replyto": "RF5YH4K4L8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4129/Reviewer_ErDF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4129/Reviewer_ErDF"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you for your effort! I believe this makes the paper much better, and prevents onerous people like me from getting grumpy :)! I will update my score accordingly."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4129/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700681112591,
                "cdate": 1700681112591,
                "tmdate": 1700681112591,
                "mdate": 1700681112591,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qhxTrilShn",
            "forum": "U4RoAyYGJZ",
            "replyto": "U4RoAyYGJZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4129/Reviewer_FJW4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4129/Reviewer_FJW4"
            ],
            "content": {
                "summary": {
                    "value": "This paper looks at the problem of using replay buffers with on-policy methods. The replay buffer contains data generated from various (past) policies but on-policy methods (in practice) rely on data being on-policy.\n\nBuilding on [Zhong et al., 2022](https://proceedings.neurips.cc/paper_files/paper/2022/file/f2dbede0879b9d04ceb30f1b8b476b27-Paper-Conference.pdf), the paper explores adapting the behavior policy (the policy generating the data) in such a way that the replay, as a whole, looks on-policy with respect to the target policy (the policy being learned). (In this sense, the problem being studied can be seen as using on-policy methods for off-policy learning.)\n\nWhile Zhong et al., 2022, considered the problem of policy evaluation, this paper considers the control problem. The proposed method, PROPS, works as follows:\ntrain a policy to generate \"diverse\" data, with data from the replay. This policy solves an RL problem where the reward is -1 for taking the observed action and zero otherwise.\ngenerate data with the policy from step 1.\ntrain the target policy with data from the replay.\n\nThe policy in (1) is trained not to be too different from the target policy (the policy being trained in 3), but the the regime in (1) is slightly off-policy. The regime in (3) is also slightly off-policy. So the paper proposes to use PPO for both steps (1) and (3) as a way to mitigate the slight off-policiness.\n\nThe paper shows that the data generated by PROPS is more on-policy than the data generated by PPO. Moreover, PROPS outperforms PPO and the \"ad-hoc\" PPO with a replay in a control benchmark. Importantly, PROPS works well with the replay whereas the ad-hoc PPO with a replay works worse than PPO in some of the tasks and better in others.\n\nOverall I am happy with this paper, but I am divided in my assessment of its significance. On the one hand, it is a good study of the problem of making on-policy methods work with replays (and thus become more sample efficient). On the other hand, it is unclear to me whether we should use PROPS, or other adapted on-policy methods instead of an off-policy method when learning with a replay (e.g., MPO)."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is well executed, and the method is described in enough detail that I feel confident I could reproduce it."
                },
                "weaknesses": {
                    "value": "I don't think the paper has any serious flaws, but there are some areas it can be improved:\n* the presentation can be improved to emphasize some points about the paper and the main contributions\n* the significance of the results should be discussed in more detail\n\nI note in passing that the paper is not specifically concerned with representation learning."
                },
                "questions": {
                    "value": "### Comments\n\nThank you for your submission.\n\nI think the introduction can be clarified a bit more for the distinction between on-policy sampling and on-policy data. My guess is that the focus on the distinction is not as helpful as getting to the point that on-policy methods need on-policy data for the updates, and the data collection should take into account what the data distribution in the replay will look like.  In the paper you show that if the method samples data on-policy the replay ends up looking quite off-policy.\n\nI would even consider using PPO-buffer as the example to motivate the issue. \"Consider the following method. Repeat: Add data to a replay with latest policy, update the latest policy with PPO updates. This method clearly samples the data on-policy, because the behavior and target policy are the same before the policy updates are applied. However, the data in the replay may not be on-policy, because the replay buffer collects data generated by various past policies. Therefore the example method may fail at policy improvement because PPO updates rely on near on-policy data in order to work as intended.\" A diagram of a replay buffer with \"chunks\" of data from various policies might also help quickly understand the issue.\n\nAlong these lines, I think the point in the beginning of section 4 would be easier to get across if you used an example with a replay buffer where the data comes from multiple policies. I think trying to argue that a sample from a single policy may look off-policy is trickier, and in practice it's not so much of a problem (PPO works just fine with on-policy sampling to generate the data for each update).\n\nI partially disagree with paragraph 5 of section 4. I don't think off-policy corrections should be dismissed as \"may increase variance\". Some off-policy corrections that clip importance weights mitigate gradient variance in exchange for some bias, for example, PPO. I would say that the real problem is that, while off-policy corrections can mitigate off-policiness, methods that use them typically still suffer as data gets more off-policy. So they may still benefit from different sampling strategies for collecting experience, even with off-policy corrections in place.\n\nI think there are typos in Eq. 4. I think the minus sign should be outside of clip.\n\nIn my opinion the KL penalty in Eq. 5 and the KL rule in line 9 of Alg. 2 are under-investigated. In my experience the KL regularizer in Eq. 5 contributes to make collected data more on-policy, so I think it would be helpful to understand how much the KLs are contributing to the replay buffer being on-policy.\n\nWe can also see the impact of the KL in Figure 3. The KL regularization actually increases the on-policiness of the data. In contrast, even though ROS was meant (by motivation) to also increase the on-policiness of the data, it does not. It is also worth noting that making the data more on-policy might also mean that the clipping and regularization are not letting the updated target policy deviate too much from the pre-update target policy. So maybe with regularization even on-policy sampling might generate more on-policy data.\n\nAnother point on ROS: I find the idea of the ROS loss to generate more diverse data a bit delicate to work with, in the sense that you want a policy that generates a bit less of the actions seen, but not completely optimizing the -1 reward. If you end up rewarding the agent with -1 all the time, the learning dynamics might take you to a uniform policy, but for the problem itself any policy is an optimal policy.\n\nI suggest placing Figure 5 earlier, close to where it is discussed.\n\nPlease consider sorting the references alphabetically instead of by citation order, as it will be easier to refer to.\n\n### Questions\n\nWhat is the contribution of the paper in a broader context? The contributions are clear to me for the problem of on-policy methods with a replay, but what about the broader problem of increasing sample-efficiency by using a replay? Do you think adjusted on-policy methods with a replay are a suitable alternative to off-policy methods?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4129/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698837741228,
            "cdate": 1698837741228,
            "tmdate": 1699636378006,
            "mdate": 1699636378006,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "E8W0dhCMsc",
                "forum": "U4RoAyYGJZ",
                "replyto": "qhxTrilShn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4129/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4129/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank the reviewer for their positive review! We\u2019re pleased to see you appreciate our contributions towards improving the data efficiency of on-policy learning. \n\nWe want to begin by answering your question about the work\u2019s broader impact. The core contribution of our work is that we demonstrate how *off-policy sampling* can reduce sampling error and improve the data efficiency of *on-policy* policy gradient algorithms. Our work also emphasizes that on-policy algorithms simply require on-policy data and are agnostic to how the data is generated \u2013 a point which is often overlooked in the literature. \n\nBelow, we now address your comments and questions.\n\n# **The distinction between on-policy sampling and on-policy data in the Introduction**\n\nWe framed the introduction in this way to emphasize that the focus with on-policy algorithms should be on the data, not the procedure for generating the data. In the literature, we find that this point is often overlooked. For instance,  Sutton & Barto\u2019s RL textbook [1] states that \u201cthe distinguishing feature of on-policy methods is that they estimate the value of a policy while using it for control.\u201d \n \n# **ROS update**\n\nThis is a great point that we will clarify in the main paper. ROS is indeed delicate to work with, and a key aspect of our our is work is designing a method (PROPS) which makes ROS more stable in higher dimension benchmark tasks. PROPS, like ROS, does not aim to fully optimize its objective but instead uses the gradient to make a local adjustment to the behavior policy that increases the probability of under-sampled actions. PROPS\u2019s clipping mechanism and KL regularization help ensure that this behavior update remains local.\n\n# **Ablations on KL regularizer and clipping mechanism**\n\nWe ablate PROPS\u2019s KL regularizer and the clipping mechanism in Appendix C and D and find that both help reduce sampling error and improve agent performance:\n\n* Appendix C, Fig. 9 shows how clipping and regularization affect PROPS\u2019s ability to reduce sampling error with respect to a *fixed* target policy. In this setting, clipping generally has a stronger effect on sampling error reduction than regularization.\n\n* Appendix D, Fig 12 shows how clipping and regularization affect PROPS\u2019s ability to reduce sampling error in the RL setting with a *continually changing* target policy. In this setting, both regularization and clipping are crucial to reducing sampling error.\n\n* Appendix D, Fig 13 shows IQM return achieved by PROPS during RL training with and without regularization or clipping. In nearly all tasks, PROPS performs best with both clipping and regularization.\n\n# **Should we prefer PROPS over other off-policy methods?**\n\nWe view on-policy learning with replay as a means to improve the data efficiency of on-policy learning rather than an alternative to off-policy learning. On-policy and off-policy algorithms have different advantages and disadvantages, and the choice of algorithm typically depends on what advantages are most relevant for a given task. For instance, PPO has played a role in several RL success stories [*e.g.*, 2, 3] in part because it is empirically stable and robust to hyperparameter choices.\n\n# **Example in Section 4**\n\nWhile we agree that an example with a replay buffer would clearly illustrate how historic off-policy data can bias updates, we wanted to emphasize that sampling error can be a problem even when all data in our buffer comes from the current target policy. In fact, in Fig. 15 and 16 of Appendix E in the supplemental material, we show that PROPS can improve the data efficiency of on-policy learning even when the replay buffer contains *no historic data* (PROPS with b=1).\n\n# **Suggested edits and typos**\n\nThanks for pointing out all of these! We\u2019ll fix these in our revision:\n\n* We will clarify the limitation of off-policy approaches under large distribution shift.\n* We will try to keep figures on the same page on which they are referenced.\n* You\u2019re correct that minus should be outside the clip function in PROPS\u2019s clipped objective--nice catch \n* We will order references alphabetically.\n\nPlease let us know if you have any follow-up questions or comments. We are more than happy to discuss!\n\n# **References**\n\n[1] Sutton, Richard S., and Andrew G. Barto. \"Reinforcement learning: An introduction.\" MIT press, 2018.\n\n[2] Berner et. al. \"Dota 2 with large scale deep reinforcement learning.\" Arxiv 2019.\n\n[3] Vinyals et. al. \"Grandmaster level in starcraft ii using multi-agent reinforcement learning.\" Nature 2019."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4129/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700331877474,
                "cdate": 1700331877474,
                "tmdate": 1700331877474,
                "mdate": 1700331877474,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]