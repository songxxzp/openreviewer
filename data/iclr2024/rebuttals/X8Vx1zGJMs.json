[
    {
        "title": "HumanNorm: Learning Normal Diffusion Model for High-quality and Realistic 3D Human Generation"
    },
    {
        "review": {
            "id": "WBInjgdjZM",
            "forum": "X8Vx1zGJMs",
            "replyto": "X8Vx1zGJMs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3422/Reviewer_LRtj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3422/Reviewer_LRtj"
            ],
            "content": {
                "summary": {
                    "value": "In this work, the authors propose a framework for text-to-3d human generation. Specifically, two stages are devised. In the first stage, the authors propose to finetune the text-to-normal and text-to-depth models conditioned on view-dependent prompts, and further use SDS to supervise the DMTet geometry. In the second stage, the normal map is used as controlnet condition to texture SDS. Some tricks like progressive generation and multi-step SDS are used to enhance performance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The task of text-to-3d human generation is important to various applications and research.\n\n2. The overall method and pipeline make sense to me in general."
                },
                "weaknesses": {
                    "value": "1. Limited functionality of this pipeline compared to previous studies. Previous text-to-3d human generation works, like AvatarCLIP, DreamAvatar, DreamHuman, TADA, AvatarVerse, all enable the animatable 3d human. That is, they can give a sequence of pose to animated the generated 3d avatars, while this work fails to do this. This hinders further application of this work. Although the authors claim that this method is generalizable to general domains, not limited to human, I don't see any qualitative/quantitative experiments to verify this proof. \n\nImho, in terms of the general text-to-3d generation, the authors don't present any experiments to verify this. In terms of text-to-3d human generation, this method fails to do the training-free animation.\n\n2. Though finetuning text-to-depth and text-to-normal makes sense to me. Does such isolated training of two models give match results? If not guaranteed, the SDS gradient from depth and normal models could be inconsistent and stochastic, making it hard to normalize.\n\n3. Why not use both depth and normal in the texture generation stage by give to the controlnet?\n\n4. The generated human results still seem blurry to me."
                },
                "questions": {
                    "value": "listed above. The main issue is that, such framework can not animate at inference time, which is inferior to previous baselines and meaningless to industrial use."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3422/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697611463039,
            "cdate": 1697611463039,
            "tmdate": 1699636293835,
            "mdate": 1699636293835,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "lFft1SafRp",
            "forum": "X8Vx1zGJMs",
            "replyto": "X8Vx1zGJMs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3422/Reviewer_KiDQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3422/Reviewer_KiDQ"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a method to construct 3D geometry and the appearance of human using the data generated from 2D diffusion model. The key idea is to enhance the geometric details of the 3D model by using generated normal and depth. To enable the generation of surface normal and depth, this paper introduces a method to convert the text-to-image model to text-to-normal (or depth). The generated normal and depth maps are further conditioned in the geometry-guided diffusion model that can generate 2D image aligned with geometry. The generated data are used to construct a 3D model with DMTET representation in a progressive and coarse-to-fine framework. Some qualitative results are demonstrated."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The reconstruction results of high-frequency details are quite impressive. I agree that this is possible by the 3D reconstruction supervised with the generated normal maps. \n- Showing the feasibility of the idea for converting text-to-image model to text-to-normal is interesting."
                },
                "weaknesses": {
                    "value": "While the overall visual quality seems quite impressive, I have many complaints about experiments and missing details in explanation (which prevents from reproduction of this paper).\n\n[Experiments]\n- A research paper should \u201cquantify\u201d the idea. Since it is not a technical report but a research paper, \u201cno quantitative validation\u201d highly suppresses the strength of this paper. While this paper claims a lot of aspects, it only shows the visual results with limited subjects. How can the user study with 50 represent general human perception? This also applies to the ablation study, which requires quantitative evaluation. \n\n-  The validation and demonstration are not aligned with the authors\u2019 claim. The main idea is to conver the 2D image generation model to text-to-normal and normal aligned 2D generation. Validation should reflect the claim. Even for the qualitative results, this paper must show not only 3D human results but also text-to-generation results. Also, many quantitative and qualitative validation needs to be aligned with this claim. \n\n\n[Description and missing details]\nThe explanation in the main script is not kind. While the global objective of higher-level idea is quite deliverable, many details are missing. In particular, the details to build a 3D reconstruction model are quite missing; and it was impossible to fully understand \u201cprogressive geometry generation\u201d and \u201ccoarse-to-fine texture generation\u201d. Please address the questions and concerns in the question section."
                },
                "questions": {
                    "value": "1) In Figure 1, this paper says that it can generate 3D human from text. However, isn\u2019t it reconstruction (from the generated 2D images and surface normal), not directly 3D generation? If so, this sentence is misleading.\n2) For training text-to-surface normal with ground truth, how did the author accurately match the training data and associated text prompt of the diffusion model?\n3) Other than the front, side, and back views, how could the pipeline generate the images from other views using prompt?\n4) In the paper, the authors comment that, the training of generation pipeline takes 2hours. Is that the computational time for \u201cper-subject\u201d? How about other methods? Are their methods also designed for a per-subject reconstruction? Do existing works also require such training time for a specific text?\n5) How much time does it take to train per-subject reconstruction?\n6) How many views and generated images are required to train the DMTET (for 3D reconstruction) for a specific person?\n7) \u201c... we transform the normal maps zn0 by the rotation R of the camera parameters.\u201d: what does it mean?; and how could this mitigate the view-dependent artifacts?\n8) How did you apply the multistep perceptual loss in the denoising process? \n9) In the section of \u201cProgressive Geometry Generation.\u201d, what is progressive has encoding?; and how do they play an important role in DMTET?; what does it mean by mask? Do you mean by 2D/3D mask? Why is using the mask allowing the network to focus on low-frequency part?\n10) How did you compute the camera poses over the generated images?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "-"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3422/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698034580199,
            "cdate": 1698034580199,
            "tmdate": 1699636293751,
            "mdate": 1699636293751,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "vRnkTNdtLR",
            "forum": "X8Vx1zGJMs",
            "replyto": "X8Vx1zGJMs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3422/Reviewer_6hVc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3422/Reviewer_6hVc"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes HumanNorm, incorporating normal and depth maps to the text-to-human generation. Experiments shows that HumanNorm outperforms existing text-to-3D methods in both geometry and texture quality."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. I quite appreciate that authors explore the use of normal and depth maps into 2D diffusion models for text-to-human generation. This indeed involves the quality of generated 3D human.\n2. Authors provide lots of figures in the paper to show visual quality results. The detailed experimental settings are also provided."
                },
                "weaknesses": {
                    "value": "1. For normal and depth maps generation, authors propose to finetune the 2D diffusion model. As the normal and depth information is the key to learn the details of 3D human, I am wondering if you directly use images from 2D diffusion model to predict the corresponding SMPL/SMPLX and use this as the supervison to learn the normal and depth, will you get similar results as shown in the paper? One advantage of SMPL/SMPLX is that it can represent the characteristics of humans, e.g., human's hand will have 5 fingers. It seems that you methods learns misleading results for hand (with 6 fingers). \n2. In your paper, you mention \"transform the normal maps z_0^n by the rotation R of the camera parameters\". Could you explain how do you rotate the normal maps?\n3. When compared with DreamHuman and Avtarverse, it seems that the visual performance improvement is not large. Could authors give more detailed comparison?\n4. Authors miss some references on 3D Human generation, e.g., \n1). Efficient 3D Articulated Human Generation with Layered Surface Volumes.\n2). Layer-wise 3D Human Generation with Diffusion Model. \nI strongly suggest authors checking the paper list on https://github.com/weihaox/awesome-digital-human. \n5. Although it is good to involve normal and depths into text-to-human generation, the overall technical contribution is not very high.  If authors could propose some insights on designing model architectures or training strategy based on the observations from experiments, it would add more strength to this paper. \n\nI will consider improving my rating if my above concerns are resolved."
                },
                "questions": {
                    "value": "Questions are listed above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3422/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698333808195,
            "cdate": 1698333808195,
            "tmdate": 1699636293637,
            "mdate": 1699636293637,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "OfZBTis8eb",
            "forum": "X8Vx1zGJMs",
            "replyto": "X8Vx1zGJMs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3422/Reviewer_gv3j"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3422/Reviewer_gv3j"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a pipeline named HumanNorm for high-quality and realistic 3D human generation. The HumanNorm approach employs a normal-adapted diffusion model and a normal-aligned diffusion model to generate high-fidelity normal maps corresponding to prompts with view-dependent text and to learn to generate color images aligned with the normal maps. The paper also proposes a progressive geometry generation strategy for high-quality geometry and a coarse-to-fine texture generation approach for realistic texture. The results demonstrate that HumanNorm significantly improves both geometry and texture quality, and outperforms previous text-to-3D methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper modifies SDS loss for normal and depth map generation. The normal map used here could eliminate the Janus Artifacts, a vital problem in 3D human generation. It fine-tunes the diffusion model with extra realistic human data on normal, depth, and color map generation. This avoids the limitation of previous methods' cartoon-style results, which suffer from the pre-trained diffusion model distribution.\nGenerally, this paper is good, though some of the contributions are similar to previous papers."
                },
                "weaknesses": {
                    "value": "This modification of SDS loss seems trivial, and the pipeline is kind of traditional.\nThe contributions like coarse to fine generation, and view-dependent text prompt is also usual in previous papers.\nI have not found any solutions for de-reflection since the THuman2 and other 3D human scan datasets have some problems in surface lighting. The reflection of human skin and clothes are different. Maybe a method to decouple environmental lights from the albedo color should be used."
                },
                "questions": {
                    "value": "Do you have any thoughts on decoupling environmental lights from the albedo color? I think the reflection of human skin and clothes are different."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3422/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3422/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3422/Reviewer_gv3j"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3422/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698743283293,
            "cdate": 1698743283293,
            "tmdate": 1699636293563,
            "mdate": 1699636293563,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]