[
    {
        "title": "MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning"
    },
    {
        "review": {
            "id": "GUfks2Zefp",
            "forum": "5KojubHBr8",
            "replyto": "5KojubHBr8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4878/Reviewer_eWon"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4878/Reviewer_eWon"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces an interleaved image-text dataset built upon public VQA/captioning/Reasoning datasets such as Flickr/VQAv2/MSRVTT. It proposes several schemes to convert existing datasets into interleaved-image-text formats. The proposed dataset is used to train a VQA model that can take multiple images as input to answer questions (following user instructions). It shows state-of-the-art results on recent VQA benchmarks such as MME/MMBench, and demonstrated ability to improve downstream task performance using few-shot image-text samples. However, the writing of this paper is extremely bad, consisting of numerous typos, grammatical mistakes, and sentences that are difficult to understand. There are lots of missing details regarding dataset curation and evaluation protocols."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The proposed dataset with 5.8M samples could be useful for the multimodal community. The proposed MMICL model demonstrates top-tier performance on recent VQA benchmarks (MME/MMBench) compared to earlier models such as LLaVA and MiniGPT-4."
                },
                "weaknesses": {
                    "value": "First, I find it **extremely hard to follow the methodology** in this paper. For example, as dataset curation is one of the most important contributions of this work, I am not able to understand what efforts the authors put in order to gather a massive 5.8M interleaved-image-text datasets. For instance, Could you explain how you manage to collect the sample as shown in Figure 3(b) and 4? In these two figures, it is shown two people quarreling with each other; this requires the original dataset to not just have bounding boxes for person1 and person2, but also the action relation (quarreling) between these two bounding boxes. I would appreciate if the authors could point us to the actual dataset used to gather this particular example, and explain if extra (and perhaps costly) human annotation is required to get such samples.\n\nThe authors should also provide more visual examples from each dataset and more detailed description on how you convert them into instruction-following formats. For example, the current Section 2.3 is very hard to understand. It says *\u201cNext, we employ ChatGPT to rewrite the instructions to describe the key characteristics of each task accurately\u201d* \u2014 what does it mean by *\u201ckey characteristics of each task\u201d* and how did ChatGPT come up with them?\n\nThere are **lots of missing details** regarding model evaluation. \u2028\n\n- For one example, in Table 4, how did the authors select the few-shot samples? Will different few-shot samples affect the performance? No variance/std is provided in Table4.\n\n- How did you manually split ScienceQA-IMG into two groups that require images or not? Did you hire human annotators for this task? What rationales did the annotators employ?\n\n- Table 8 shows that Flickr/VQAv2 are used in your training set. Then how could Table 4 claim that your model achieves good \u201czero-shot\u201d and \u201cfew-shot\u201d performance on Flickr/VQAv2?\n\n- Table 8 attempted to show the licenses for all training datasets, but almost half are labeled with \u201cUnknown\u201d license. This is counter-intuitive because some widely used datasets such as Flickr have well-documented licenses listed on their websites. \n\n- Do you use the same datasets during both stage-1 and stage-2 pre-training?\n\n- As you show on Table 8, Winoground only has 800 samples. How could there be an image score of 44.99?\n\nFinally, it is unclear if the performance boost really comes from having more interleaved-image-text data, because the current paper does not perform ablation on the selection of training data.\n\nThe paper has way too many typos. To list a few:\n- Figure 1: missing a white space \u201cMMICLtypically\u201d.\n- Page 2: fall should be fail? \u201ctheir context schemes fall to\u201d\n- Page 3: missing a period \u201cmultiple images In Fig. 2.b\u201d\n- Figure 3: missing a period \u201cunified format\u201d\n- Page 4: missing a white space \u201cMMICLin\u201d\n- Page 8: hinds or hints \u201cprovide hinds\u201d"
                },
                "questions": {
                    "value": "I think the current writing really hurts the paper. In addition to improve the writing quality, it is unclear what the key innovation / scientific insight is. The authors should ablate the use of training data in order to highlight the advantage of including interleaved image-text samples, which seems extremely costly to obtain."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics review needed."
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4878/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4878/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4878/Reviewer_eWon"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4878/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698382019028,
            "cdate": 1698382019028,
            "tmdate": 1699636472280,
            "mdate": 1699636472280,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GXAJURy02q",
                "forum": "5KojubHBr8",
                "replyto": "GUfks2Zefp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4878/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4878/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Respond to the valuable advice and comments from Reviewer eWon (1/N)"
                    },
                    "comment": {
                        "value": "Thanks for your constructive reviews and thoughtful feedback. It is encouraging to find your belief in our work's potential to contribute the multimodal community.\n\nBelow, we provide detailed replies to your comments and hope we can resolve your major concerns.\n\n### **Q1:**\n\n> First, I find it **extremely hard to follow the methodology** in this paper. For example, as **dataset curation** is one of the most important contributions of this work, I am not able to understand what efforts the authors put in order to gather a massive 5.8M interleaved-image-text datasets. For instance, Could you explain **how you manage to collect the sample as shown in Figure 3(b) and 4**? In these two figures, it is shown two people quarreling with each other; this requires the original dataset to not just have bounding boxes for person1 and person2, but also the action relation (quarreling) between these two bounding boxes. I would appreciate if the authors could point us to the actual dataset used to gather this particular example, and explain if extra (and perhaps costly) human annotation is required to get such samples.The authors should also provide more **visual examples from each dataset and more detailed description on how you convert them into instruction-following formats.** For example, the current Section 2.3 is very hard to understand. It says *\u201cNext, we employ ChatGPT to rewrite the instructions to describe the key characteristics of each task accurately\u201d* \u2014 what does it mean by *\u201ckey characteristics of each task\u201d* and how did ChatGPT come up with them?\n\n### **A1 :**\n\nWe apologize for any confusion and appreciate your thorough review. We are committed to clarifying our methodology and dataset curation process. The relevant sections have been updated. Below, we provide more details for your convenience.\n\n> **Q1.1**: Dataset curation pipeline\n> \n- We have seamlessly integrated the MIC dataset using the open-source dataset into our data construction automatic pipeline at the **lowest cost**. The **only** manual **effort** required by humans is to **handcraft one instruction template** for each task. Then we expand **these templates with ChatGPT** to ensure diversity in the MIC dataset and prevent the MMICL from overfitting on specific datasets.\n- The newly added **Fig 4, Fig 9 and Fig 10** in our revised paper present a **detailed illustration of data construction**.\n- We have **included the detailed dataset curation pipeline** in **Sec 2.3 and Appendix D** in the revised paper.\n\n> **Q1.2**: How you manage to collect the sample as shown in Figure 3(b) and 4?\n> \n- As mentioned in Section 2.3, we obtain the kind of interleaved-image-text data you referred to above, with **bounding boxes and entity references** from th**e VCR dataset**.\n    \n    We crop images from the VCR dataset using **object bounding boxes  provided** in the dataset to produce intertwined multi-modal data with closely related images. For each instance, it contains **an image**, several **object bounding boxes** for different entities appearing in the image, and the question for the image. All the **objects** have **been marked in the question as object reference labels**, so we can easily **replace** these labels with cropped images of the corresponding objects. Please kindly refer to **Fig 4** in our revised paper.\n    \n    Specifically, take the instance you mentioned above as an example: in the original dataset, it has bounding boxes of the two people marked as **man_1** and **woman_1**. And the corresponding question\uff1a\n    \n    > \"Are the [man_1] and [woman_1] quarreling?\"\n    > \n\nTherefore, simply replacing `man_1` and `women_1`  with the cropped images of them will result in an instance of our dataset.\n\n- We have made our tool **public**, which can be used to transform new datasets in accordance with our proposed context scheme through an automatic pipeline.\n- In summary, you can **create your own MIC dataset** using other open-source datasets by our released automatic pipeline. The only effort you need is to **write a few suitable instruction template**s for the datasets. And ask the ChatGPT to expand the instruction templates."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4878/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700419839605,
                "cdate": 1700419839605,
                "tmdate": 1700482926774,
                "mdate": 1700482926774,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DTgk69jtI7",
                "forum": "5KojubHBr8",
                "replyto": "GUfks2Zefp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4878/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4878/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Respond to the valuable advice and comments from Reviewer eWon (2/N)"
                    },
                    "comment": {
                        "value": "> **Q1.3**: Provide more visual examples from each dataset\n> \n- **Visual Examples and Dataset Curation:**\n    \n    To enhance understanding, except for the example in Figure 3, we have provided **more visual examples in Fig.4, Fig.9, and Fig.10  in the revised paper**, illustrating how open-source datasets are converted into the MIC dataset in accordance with our proposed context scheme.\n\n> **Q1.4**: How you convert them into instruction-following formats?\n> \n- Allow me to provide you with a clearer overview and a **detailed description of the data collection pipeline**:\n    - **Hand-Crafted Instruction Templates:**\n        1. We scrutinize samples from each used dataset and handcraft instruction templates tailored to each dataset.\n    - **Templates Expansion by ChatGPT:**\n        \n        We use ChatGPT to expand these instruction templates, adding diversity to the MIC dataset and preventing overfitting to specific data structures.\n        \n        1. When expanding with ChatGPT, we give it the dataset description and our hand-craft instruction and ask chatgpt to **rewrite and** expand **these instruction templates**.\n        2. For each instance in the open-source dataset, we randomly select an expanded instruction template and fill in the data to obtain a dataset with diverse instruction formats. We aim to **enhance the diversity of MIC data** formats using these augmented instruction templates, thereby preventing model overfitting on specific data structures.\n    - **Data Construction Pipeline:**\n        \n        We use the open-source datasets with instructions to construct the MIC dataset **according to our proposed context scheme**, involving **image declaration, interconnected images, and multi-modal in-context format** data:\n        \n        1. **Image declaration**:\n            \n            For each instance in every dataset, we create an \"**image declaration\" section**. This section takes the form of\n            \n            > \"image 0 is <image0>{visual embedding}\" or \"image 0: <image0>{visual embedding}\".\n            > \n            \n            Its purpose is to **establish a targeted connection** between language context and visual embedding.\n            \n            As the number of images in an instance increases, the image IDs correspondingly increase.\n            \n            Its placement is flexible. It can either appear at the start of the question concatenated with the original instance or surface within the text. This step is applied to every instance in the MIC dataset.\n            \n        2. **Multi-modal data with interconnected images\uff1a**\n            \n            **This step draws from two primary data sources:**\n            \n            1. **Video Language Datasets (e.g., MSRVTT and MSRVTTQA)**: We extract eight frames per video from these datasets, transforming the original video-language task into a multi-image task.\n            2. **VCR dataset:** This dataset provides object bounding boxes and entity references for each object in related questions and answers. We crop images from the VCR dataset using these bounding boxes, replacing entity references in the context with the cropped images. The original image is also included at the beginning of the input context.\n        3. **Multi-modal in-context format data:** For each instance in a specific dataset, we sample questions and corresponding answers from the dataset to construct few-shot exemplars, ensuring no overlap between exemplars and instances. This process results in the creation of Multi-modal In-Context Format Data. The number of exemplars varies randomly, ranging from 0-shot to eight-shot.\n- We have **open-sourced a tool** to transform new datasets using our proposed context scheme, allowing others to create their own MIC datasets at minimal cost. The tool utilizes the expanded instruction templates and original dataset samples to generate instances following our format.\n- We have revised the whole **Sec 2.3 and provided more details about the dataset construction in Appendix D** in the revised paper.\n\nWe hope this detailed explanation provides clarity on the dataset curation process and methodology used in our work. We appreciate your valuable feedback, and we **have revised the paper** to better articulate these aspects."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4878/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700420029725,
                "cdate": 1700420029725,
                "tmdate": 1700420043742,
                "mdate": 1700420043742,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XquwCmUNV9",
                "forum": "5KojubHBr8",
                "replyto": "GUfks2Zefp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4878/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4878/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Respond to the valuable advice and comments from Reviewer eWon (3/N)"
                    },
                    "comment": {
                        "value": "### **Q2:**\n> For one example, in Table 4, how did the authors select the few-shot samples? Will different few-shot samples affect the performance? No variance/std is provided in Table4.\n\n### **A2** :\n- Thank you for your valuable feedback, and we appreciate the opportunity to provide a more comprehensive explanation and address this concern.\n    \n    We **randomly sample the  few-shot samples** in the dataset where the task belongs for all given tasks during the training and evaluation and ensure there is no gap between sampled examples and the question.\n    \n    Using VQAv2 as an example, we evaluate MMICL on a 2000-instance VQAv2 dataset. We assess the robustness of MMICL performance using different seeds for random sampling of in-context learning examples. This demonstrates the robustness of MMICL.\n    \n    | Seed | 0-example | 1024 | 4096 | 16384 | std |\n    | --- | --- | --- | --- | --- | --- |\n    | VQA accuracy | 69.54 | 69.98 | 69.94 | 70.21 | **0.1186** |\n- Results show that sampling of few-shot samples does not greatly affect the performance.\n\n### **Q3:**\n> How did you manually split ScienceQA-IMG into two groups that require images or not? Did you hire human annotators for this task? What rationales did the annotators employ?\n\n### **A3** :\n- We appreciate your inquiry and are glad to clarify it for you.\n    \n    In the test set of Science-QA, each item consists of a question and several choices. Out of **4221 items**, we sampled **2017 instances that contain multimodal information** incorporating both text and image, a subset we refer to as **ScienceQA-IMG**, while the remainder only includes text. However, we observe that not all questions within these 2017 items **necessarily require answers** based on image information.\n    \n    Consequently, we **engage eight locally sourced, highly-educated individuals,** **all pursuing Master's or Ph.D. degrees, to annotate the subset**. The standard for annotation is whether an understanding of the image sample is essential to answer the questions in the test set.\n    \n- To illustrate this, we **provide two examples of questions** and their corresponding choices:\n    \n    > a) Question: Which animal is also adapted to be camouflaged among dead leaves?  Choices: [ \"strawberry poison frog\", \"Surinam horned frog\" ];\n    > \n    \n    > b) Question: Which property do these three objects have in common? Choices: ['shiny', 'slippery', 'opaque'].\n    > \n    \n    Clearly, the first question can be answered without reference to the image, while the second can not. As such, ScienceQA-IMG can be divided into two groups: questions that require images for answers(**1012**) and those that do not(**1005**).\n    \n    When considering these two groups, MMICL achieves the highest and least significant performance (**82.60 vs. 82.00 on Don't Require Visual Information** & **81.70 vs. 60.70 on Require Visual Information** when compared with InstructBLIP).\n    \n\nThis clearly demonstrates that our model successfully addresses the issue of **language bias hallucination** in Visual Language Models (VLMs), which are prone to overlooking visual content.\n\n- We have included the necessary information about the ScienceQA-img dataset **in Appendix Q** in the revised paper.\n\n### **Q4:**\n> Table 8 shows that Flickr/VQAv2 are used in your training set. Then how could Table 4 claim that your model achieves good \u201czero-shot\u201d and \u201cfew-shot\u201d performance on Flickr/VQAv2 ?\n\n### **A4 :**\n- We sincerely apologize for any confusion caused by the use of inappropriate terminology. In our context, the terms \"zero-shot\" and \"few-shot\" were not ideal choices to describe the model's performance.\n- Rather than emphasizing the model's zero-shot capabilities, our primary focus is on **contrasting performance with and without ICL examples.** These examples serve to validate that, for MMICL, providing a **certain quantity of ICL examples enhances performance.**\n- To avoid any further misunderstanding, we have revised the terminology as follows in **Table 4**:\n    - **\"zero-shot\" -> \"w/o ICL examples\"**\n    - **\"4-shot\" -> \"w/ ICL examples (4)\"**\n\nWe believe these adjustments accurately reflect our intentions and clarify the model's performance in the specified scenarios. We hope this clarification adequately addresses your problems.\n\n### **Q5:**\n> Table 8 attempted to show the licenses for all training datasets, but almost half are labeled with \u201cUnknown\u201d license. This is counter-intuitive because some widely used datasets such as Flickr have well-documented licenses listed on their websites.\n\n### **A5** :\n- In response to your observation regarding the \"Unknown\" license in Table 8, we fully recognize the importance of transparency in dataset licenses. We have taken your feedback seriously, and **now completed the missing license information** for the datasets in the revised paper.\n\nWe are committed to upholding the standards of the open-source community and ensuring the clarity of licensing details."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4878/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700420343973,
                "cdate": 1700420343973,
                "tmdate": 1700482915519,
                "mdate": 1700482915519,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LzvdCEUQtF",
                "forum": "5KojubHBr8",
                "replyto": "GUfks2Zefp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4878/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4878/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Respond to the valuable advice and comments from Reviewer eWon (4/N)"
                    },
                    "comment": {
                        "value": "### **Q6:**\n\n> Do you use the same datasets during both stage-1 and stage-2 pre-training?\n\n### **A6** :\n\n- As outlined in our paper in **section 3.1 and detailed in Appendix G**, during stage-1 pre-training, we adhere to the settings of Blip-2, employing the **COCO captioning data and LAION-400M data**. In contrast, during stage-2 pre-training, we exclusively utilize the MIC dataset. Consequently, **apart from** the inclusion of the **COCO caption dataset** in both stages, there is **no overlap** in the datasets employed during stage-1 and stage-2 pre-training.\n\nWe hope this clarification addresses your query adequately.\n\n### **Q7:**\n\n> As you show on Table 8, Winoground only has 800 samples. How could there be an image score of 44.99?\n\n### **A7** :\n\nWe sincerely appreciate your diligence in pointing out the discrepancy in the number of samples and the reported image score for Winoground. We acknowledge this discrepancy, which seems to be a problem encountered across multiple works [1][2][3].\n\n- To ensure a fair comparison, we adhered to the evaluation code of [1]. Upon investigation, we identified that the issue arises from the **use of bfloat16** in the tensor calculation, which can lead to a **loss of precision**.\n- We compute and save the image-text score using the **bfloat16 data format**, and when we calculate the image score using tensor calculation and the **tensor.item()** function, there is a loss of precision when the data is cast into **float32**.\n\nWe apologize for this misrepresentation and have rectified the error by updating the result to **45.00**.\n\n[1]Wang, T., Lin, K., Li, L., Lin, C. C., Yang, Z., Zhang, H., ... & Wang, L. (2023). Equivariant Similarity for Vision-Language Foundation Models. *arXiv preprint arXiv:2303.14465*.\n\n[2]Dou, Z. Y., Xu, Y., Gan, Z., Wang, J., Wang, S., Wang, L., ... & Zeng, M. (2022). An empirical study of training end-to-end vision-and-language transformers. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition* (pp. 18166-18176).\n\n[3]Wu, Y., Zhang, P., Xiong, W., O\u011fuz, B., Gee, J.C., & Nie, Y. (2023). The Role of Chain-of-Thought in Complex Vision-Language Reasoning Task.\n\n### **Q8:**\n\n> Finally, it is unclear if the performance boost really comes from having more interleaved-image-text data, because the current paper does not perform ablation on the selection of training data.\n\n### **A8** :\n\nThank you for your valuable feedback, and we appreciate the opportunity to provide a more comprehensive explanation and address this concern.\n\n- In response to your suggestion, we have **conducted an ablation study** on the training data to elucidate the performance boost attributed to our proposed scheme design.\n- Specifically, we conduct ablation experiments using the Instructblip-xl as the backbone model in the following settings to verify the effectiveness of **interleaved image-text** during multi-modal ICL tuning:\n    - **MMICL(Instructblip-xl):** original model\n    - **Paired-only**: model only trained on the **single image-text pairs data**\n    - **Multi-image-only:** model only trained on the **interleaved image-text data pairs**\n    - **ICL-only:** model only trained on the **in-context learning format data**\n    \n    We use **the equal amount** of data as in stage 2 of MMICL for the  ablation models.\n    \n    - We employ the ablation study on both the **general benchmark(MME)** and **multi-image reasoning datasets(Icon-QA & NLVR2)** to comprehensively evaluate the sources of the observed performance improvement in MMICL. The ablation results affirm that our **interleaved image-text data scheme is a key factor** in observed improvements, providing clarity on its contribution to MMICL's performance.\n    \n    | Task | MMICL | Paired-only | Multi-image-only | ICL-only |\n    | --- | --- | --- | --- | --- |\n    | MME_Perception(Overall) | **1303.59** | 1238.99 | 1141.02 | 1207.70 |\n    | MME_Cognition (Overall) | **370.71** | 316.79 | 345.36 | 333.21 |\n    | Icon-QA | **58.12** | 52.80 | 51.95 | 54.35 |\n    | NLVR2 | **72.45** | 56.65 | 62.63 | 59.60 |\n- The ablation study clearly demonstrates that the observed improvement in MMICL's performance is indeed **attributable to our proposed scheme and the incorporation of multi-modal ICL tuning.** Single image-text pair data for **SFT does not yield comparable enhancements** in the targeted areas, underscoring the unique value of our proposed method.\n- We have included the ablation study and analysis in the second sub section of **Sec. 3.6** in the revised paper.\n\nWe trust that this additional information addresses your concern regarding the impact of interleaved-image-text data on performance. If you have any further questions or require additional details, please do not hesitate to reach out.  We hope that our response has addressed your concerns and kindly ask if you could consider increasing your score accordingly."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4878/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700420547643,
                "cdate": 1700420547643,
                "tmdate": 1700482274457,
                "mdate": 1700482274457,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "juLbevoTJ0",
                "forum": "5KojubHBr8",
                "replyto": "GUfks2Zefp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4878/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4878/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Respond to the valuable advice and comments from Reviewer eWon (5/N)"
                    },
                    "comment": {
                        "value": "### **Q9:**\n\n> The paper has way too many typos. To list a few:\n>- Figure 1: missing a white space \u201cMMICLtypically\u201d.\n>- Page 2: fall should be fail? \u201ctheir context schemes fall to\u201d\n>- Page 3: missing a period \u201cmultiple images In Fig. 2.b\u201d\n>- Figure 3: missing a period \u201cunified format\u201d\n>- Page 4: missing a white space \u201cMMICLin\u201d\n>- Page 8: hinds or hints \u201cprovide hinds\u201d.\n>\n\n### **A9** :\n\nThanks for pointing them out. All fixed. We've conducted a comprehensive typo-check to ensure a typo-free final version.\n\n\n### **Q10:**\n\n> I think the current writing really hurts the paper. In addition to improve the writing quality, it is unclear what the **key innovation / scientific** insight is. The authors should **ablate the use of training data** in order to highlight the advantage of including interleaved image-text samples, which seems **extremely costly** to obtain.\n\n### **A10** :\n\nWe deeply appreciate your feedback. We appreciate your meticulous review of our paper, and please allow us to provide a more comprehensive explanation:\n\n> **Q10.1**:  In addition to improve the writing quality, it is unclear what the key innovation / scientific insight is.\n> \n- We believe the **key innovation insight** is that we present a **novel context scheme** in which incorporating three different designs and a multi-modal in-context-tuning **together augmenting the ICL ability of the VLM (we've highlighted this in the introduction).**\n    \n    The proposed context scheme contains three designs:\n    \n    - **image declaration**, which helps the VLM to **understand text-to-image reference**. It makes our model outperform others by 13-points on Winoground.\n    - **multiple interconnected images format**, which helps the VLM to better **understand the relationship between multiple images**. It makes our model perform multiimage reasoning and get 12-points improvement on Raven\n    - **unified multi-modal ICL data format**, which helps the VLM to better **learn from multi-modal in-context demonstrations**. It makes our model show impressive multi-modal ICL performance across various tasks.\n    \n    These three designs address the limitations of VLMs when dealing with understanding complex multi-modal prompts and make our model outperform others.\n    \n- We've done the ablation of the training data to demonstrate the advantage of our proposed scheme design. The ablation shows that the **improvement of the MMICL came from our proposed scheme and multi-modal ICL tuning. You may find these results in our response A8.**\n- Further, the results of the ScienceQA-Img experiment displayed in Sec. 3.6 demonstrates that MMICL effectively helps tackle the **language bias hallucination issue** inherent to the VLM, which tends to ignore visual content in the presence of extensive text. This characteristic enables the MMICL to outperform other models, even in single-image tasks. Our intuition is the interconnected image format in MMICL training forces the model to reason across text and multiple images, therefore mitigating the hallucination issue.\n\n> **Q10.2**: The authors should ablate the use of training data in order to highlight the advantage of including interleaved image-text samples, which seems extremely costly to obtain.\n> \n- We've conducted an ablation study on training data. Please take a look at the response A8 above.\n- It's worth noting that, as you mentioned earlier, the interleaved data we used for ICL tuning is \"**extremely costly** to obtain.\" **on the contrary**, we have seamlessly integrated the MIC dataset using the open-source dataset into our automatic data construction pipeline **at minimal cost.**\n\nWe obtained the interleaved-image-text data you referred to and **bounding boxes and entity references** from **the existing dataset**. We successfully incorporated them into our MIC dataset **at a low cost**.\n\nPlease kindly refer to **Fig 4, Fig 9, and Fig 10.**\n\n- For example, in the VCR dataset, a raw image is provided along with two bounding boxes for two people labeled as `man_1` and`woman_1`(**Figure 3**). The corresponding question for the image is\n    \n    > \"Are the [man_1] and [woman_1] quarreling ?\"\n    > \n    \n    To process this, we first need to crop the sub-images from the raw image based on the bounding boxes. Finally, replacing `man_1` and`woman_1` with cropped images of them will result in an instance of our dataset.\n    \n- We have **open-sourced a tool** that can be used to transform new datasets in accordance with our proposed context scheme at the lowest cost. You can create MIC data through an **automatic pipeline**.\n\nWe have made modifications to our paper to better highlight our contributions and cost-effectiveness in data collection."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4878/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700420944367,
                "cdate": 1700420944367,
                "tmdate": 1700482294330,
                "mdate": 1700482294330,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Gpo1pqAAjm",
            "forum": "5KojubHBr8",
            "replyto": "5KojubHBr8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4878/Reviewer_QUmJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4878/Reviewer_QUmJ"
            ],
            "content": {
                "summary": {
                    "value": "The author proposed a new kind of in-context learning, called multi-modal in-context learning, to deal with multi-modal inputs. To train model, the author proposed a novel context scheme and constructed the Multi-modal In-Context Learning (MIC) dataset. Experiments are conducted to show the advantages on several well known datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The targeting problem is interesting.\n\n2. Improvements are achieved on several well-known datasets.\n\n3. Extensive experimental results are provided."
                },
                "weaknesses": {
                    "value": "1. It seems that the explicit meaning of MMICL is not mentioned in this paper. Some explanations might be helpful.\n\n2. The experiments are conducted on T5 family models, which are encoder-decoder architecture models. Why not use the decoder-only models?\n\n3. Model sizes should be included in some tables. Otherwise, it is not easy to compare the competing models.\n\n4. It seems that bigger model does not always have better performance. Is there any explanations for that?"
                },
                "questions": {
                    "value": "See the above section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "no such concerns."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4878/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698752968497,
            "cdate": 1698752968497,
            "tmdate": 1699636472197,
            "mdate": 1699636472197,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "A9h5GSGFWA",
                "forum": "5KojubHBr8",
                "replyto": "Gpo1pqAAjm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4878/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4878/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Respond to the valuable advice and comments from Reviewer QUmJ"
                    },
                    "comment": {
                        "value": "We feel very excited to receive such a high rating from you for our paper. It makes us feel that our efforts are worthwhile! It is encouraging to see you find our work targeting an interesting problem.\n\nRegarding your concerns:\n\n### **Q1:**\n\n> It seems that the explicit meaning of MMICL is not mentioned in this paper. Some explanations might be helpful.\n    \n    \n\n### **A1** :\n\n- We sincerely appreciate your thoughtful feedback. In this paper, the term MMICL stands for \"vision-language **M**odel with **M**ulti-modal **I**n-**C**ontext **L**earning\"\n\nThe overarching goal of MMICL is to empower Vision-Language Models (VLMs) by incorporating multi-modal in-context learning. This approach aims to enhance the model's ability to understand and generate responses in complex, multi-modal scenarios.\n\nWe appreciate your observation, and we have included the explicit meaning of MMICL in the revised paper.\n\n\n### **Q2**:\n\n> The experiments are conducted on T5 family models, which are encoder-decoder architecture models. Why not use the decoder-only models?\n\n### **A2** :\n\n- Thank you for your inquiry. We selected the T5 family models as the backbone for our experiments. At the time of conducting this study, some decoder-only models, like Vicuna, had not been released. Consequently, we focused our experiments on the Flan backbone model.\n- Since then, we **have developed and released** the **Vicuna-based MMICL model**.\n    \n    We believe that incorporating experiments on the Vicuna-based MMICL model further highlights the novelty and robustness of our proposed method. Later, we will add the evaluation results of the Vicuna-based MMICL model in the final version.\n    \n\nIf you have any additional questions or specific areas you would like further details on, please feel free to let us know.\n\n\n### **Q3:**\n\n> Model sizes should be included in some tables. Otherwise, it is not easy to compare the competing models.\n\n### **A3**:\n\n- Thank you for your insightful observation. Including the model size in the tables makes comparing the competing models easier. We have duly noted your suggestion and taken immediate action to address this concern.\n- In the revised version, we have **included the model sizes** alongside each corresponding model in **Table 1 and Table 5**.\n\nWe appreciate your diligence in reviewing our work and providing constructive feedback. Your advice improves our work! Thank you!\n\n### **Q4:**\n\n> It seems that bigger model does not always have better performance. Is there any explanations for that?\n\n### **A4** :\n\nWe appreciate your inquiry and are pleased to provide clarification on this phenomenon.\n\n- The phenomenon of **smaller models outperforming larger counterparts** in certain tasks is not unique to MMICL; it has been observed **across various VLMs** [1] [2].\n    \n    For instance, the xl version of Blip2 demonstrates superior performance compared to the larger XXL version across **multiple datasets** such as Nocaps, Iconqa, and VizWiz [1].\n    \n- Our method, trained on the **basis of Blip2 and InstructBlip**, appears to **inherit** this characteristic.\n- **Similar trends** are evident in other models like **Flamingo**, where the smaller version exhibits more advanced few-shot abilities than the larger model **across diverse datasets** such as STAR, FLICKR, and iVQA [3].\n\nNotably, this phenomenon is particularly observable in tasks such as image captioning and open-domain question answering.\n\nThe nuanced dynamics between model size and performance represent an intriguing area for exploration. In our future work, we plan to delve deeper into this phenomenon, conducting a more comprehensive analysis to unravel the underlying factors.\n\nWe appreciate your thoughtful consideration of this aspect, and your feedback continues to enrich our understanding and future research directions.\n\n[1] Liu, H., Li, C., Wu, Q., & Lee, Y. J. (2023). Visual instruction tuning. *arXiv preprint arXiv:2304.08485*.\n\n[2] Yu, W., Yang, Z., Li, L., Wang, J., Lin, K., Liu, Z., ... & Wang, L. (2023). Mm-vet: Evaluating large multimodal models for integrated capabilities. *arXiv preprint arXiv:2308.02490*.\n\n[3]Alayrac, J. B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., ... & Simonyan, K. (2022). Flamingo: a visual language model for few-shot learning. *Advances in Neural Information Processing Systems*, *35*, 23716-23736."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4878/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700418379947,
                "cdate": 1700418379947,
                "tmdate": 1700418379947,
                "mdate": 1700418379947,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VE3JLbWSZB",
            "forum": "5KojubHBr8",
            "replyto": "5KojubHBr8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4878/Reviewer_d1dY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4878/Reviewer_d1dY"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a context learning scheme for multimodal large language models and constructs a instruction dataset. It demonstrates the model's comprehension ability when dealing with complex multi-modal prompts and uncovers its potential for in-context learning. Overall, this work leans more towards a technical report, but it is comprehensive."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Compared to open-/flamingo, kosmos, the model can more easily handle multiple image inputs and their relationships.\n\n2. Through prompt engineering, it unifies more tasks.\n\n3. More instruction tuning data has been obtained through chatgpt."
                },
                "weaknesses": {
                    "value": "The synthetic dataset, training paradigms, and model structure have not undergone an ablation study, so the specific gain is unclear.\n\nThis work has expanded further on the data level compared to works such as InstrutBlip and LLaVA, and has achieved better results, which I am not surprised by. However, its design is slightly complex, and it contains a lot of prompt engineering. I believe this work is above the borderline, but it may not be classified as a 'good' paper here."
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4878/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698829080812,
            "cdate": 1698829080812,
            "tmdate": 1699636472102,
            "mdate": 1699636472102,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1mshZ1nVDx",
                "forum": "5KojubHBr8",
                "replyto": "VE3JLbWSZB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4878/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4878/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Respond to the valuable advice and comments from Reviewer d1dY (1/N)"
                    },
                    "comment": {
                        "value": "Thanks for your constructive reviews. It is encouraging to see you find our work to be comprehensive.\nBelow, we provide detailed replies to your comments and hope we can resolve your major concerns.\n\n### **Q1:**\n\n> The synthetic dataset, training paradigms, and model structure have not undergone an ablation study, so the specific gain is unclear.\n\n### **A1 :**\n\nThank you for your insightful comment, and we genuinely appreciate your constructive feedback.\n\nWe commit to providing a clearer explanation of ablation the present in our paper.\n\n- **In Section 3.6**, we have **performed an ablation study on training paradigms** to assess the impact of multi-modal in-context tuning. The results, as depicted in Table 6, showcase significant improvements **across all types and sizes of models**. Notably, tasks involving multiple images, such as IconQA-img and Bongard-HOI, demonstrate substantial performance gains (15.75 and 21.05 points improvement, respectively). This underscores the effectiveness of our proposed ICL tuning in handling complex multi-modal prompts and accomplishing challenging tasks.\n- In addition to the training paradigms, we have extended our ablation study to **include training data and model structure**.\n    - Specifically, we conduct ablation experiments using the Instructblip-xl as the backbone model in the following settings to verify the effectiveness of **our proposed interleaved image-text data pairs and in-context learning data:**\n       - **MMICL**: This is the standard model.\n       - **w/o context scheme**: This model is trained only with instructions using all datasets from stage 2 of MMICL, without any additional design in context format and model architecture.\n       - **w/o image declaration**: This model is trained without image declaration but includes multi-modal data with interconnected images and in-context data.\n       - **w/o in-context format**: This model, devoid of in-context format data, comprises multi-modal data with related images and image declaration.\n       - **w/o interrelated images**: This version of the model is trained without any interconnected images. Its training pipeline contains multi-modal in-context data and image declaration.\n    \n    We use the **equivalent amount** of data as in stage 2 of MMICL for the ablation models.\n    \n    - We employ the ablation study on the **general benchmark(MME)** and **multi-image reasoning datasets(Raven, Icon-QA & NLVR2)** to comprehensively evaluate the sources of the observed performance improvement in MMICL. The ablation results affirm that our **interleaved image-text data scheme is a key factor in** observed improvements, clarifying its contribution to MMICL's performance. The superiority of MMICL is driven by the collective impact of our design elements\u2014removing any component cannot guarantee the superior performance of our model. Each component of our design significantly contributes to different aspects of our model.\n    \n   | Task | MMICL | w/o context scheme | w/o image declaration | w/o in-context format | w/o interrelated images |\n   | --- | --- | --- | --- | --- | --- |\n   | MME_Perception | **1303.59** | 1238.99 | 1170.87 | 1141.02 | 1207.70 |\n   | MME_Cognition | **370.71** | 316.79 | 341.07 | 345.36 | 333.21 |\n   | Icon-QA | **58.12** | 52.80 | 47.15 | 51.95 | 54.35 |\n   | NLVR2 | **72.45** | 56.65 | 61.0 | 62.63 | 59.60 |\n   | Raven | **32.00** | 8.00 | 18.00 | 28.00 | 16.00 |\n    \n    The comprehensive ablation study emphasizes that the observed performance improvement in MMICL is **attributable to our proposed scheme, model structure, and the incorporation of multi-modal ICL tuning.**\n    \n    Additional single image-text pair data for  SFT(supervised fine tuning) does not yield comparable enhancements in the targeted areas, underscoring the unique value of our proposed method\n    \n- We have **included the ablation experiments** and analysis in the **second section of Sec 3.6** in the revised paper.\n\nWe trust that this additional information addresses your concerns, and we welcome any further inquiries or feedback you may have."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4878/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700418774149,
                "cdate": 1700418774149,
                "tmdate": 1700654467957,
                "mdate": 1700654467957,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hPDYfNHTo9",
                "forum": "5KojubHBr8",
                "replyto": "VE3JLbWSZB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4878/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4878/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Respond to the valuable advice and comments from Reviewer d1dY (2/N)"
                    },
                    "comment": {
                        "value": "### **Q2:**\n\n> This work has expanded further on the data level compared to works such as InstrutBlip and LLaVA, and has achieved better results, which I am not surprised by. However, its design is slightly complex, and it contains a lot of prompt engineering. I believe this work is above the borderline, but it may not be classified as a 'good' paper here.\n\n### A2 :\n\nWe deeply appreciate your comprehensive feedback and would like to provide a detailed explanation to address your concerns\uff1a\n\n- Firstly, it's important to clarify that our work **is not reliant on prompt engineering.** MMICL **consistently demonstrates robust results** across various experiments using a single, standardized prompt:\n\n> \"Use the image 0: <image0>{visual token} as the visual aid to help you answer the question accurately.{Question}.\"\n\n\nWe futher evaluate the performance of MMICL in different datasets under different seeds to sample fewshot-examplars. Due to time constraints, we evaluate MMICL on a 2000-instance VQA dataset.\n\nWe employed different randomly sampled fewshot-examplars for each instance during evaluation, and it still showcases stable and consistent results. Additionally, it can be observed that for the dataset without a fixed format (Flickr, Websrc), their performance stability is worse than others (Hatefulmeme).\n\n| seed | Flickr | Websrc | VQAv2 | Hatefulmeme |\n| --- | --- | --- | --- | --- |\n| 1024 | 88.00 | 19.25 | 69.98 | 64.60 |\n| 4096 | 88.49 | 19.85 | 69.94 | 64.47 |\n| 16384 | 87.21 | 19.65 | 70.21 | 64.58 |\n| std | **0.5289** | **0.2494** | **0.1186** | **0.0565** |\n\n- Moreover, in the collection of the MIC dataset for ICL tuning, we employed an **automatic pipeline** to effortlessly generate MIC-format data.\n    \n    To enhance the diversity of instruction templates in the MIC dataset, we leveraged ChatGPT to **expand instructional formats** based on our handwritten template. To further underscore our commitment to **avoiding prompt engineering**, we want to highlight that the utilization of ChatGPT aimed to **enhance the diversity** of the MIC dataset's instructions. These templates were not employed as prompt engineering tactics to artificially boost our model's performance.\n    \n    All the instruction template we used in the collection of the MIC dataset is detailed in **Appendix G**.\n    \n- Our primary innovation lies in the introduction of a **novel context scheme**, which incorporates three distinct designs and integrates **multi-modal in-context tuning** to enhance the ICL ability of VLM.\n    \n\n    The proposed context scheme contains three designs:\n\n    1.  image declaration, which helps the VLM  to **understand text-to-image reference**. Demonstrates superior performance, surpassing other models by 13 points on Winoground.\n     2. multiple interconnected images format, which helps the VLM to **better understand the relationship between multiple images.** Enables our model to excel in multi-image reasoning, achieving a 12-point improvement on Raven.\n     3. unified multi-modal ICL data format, which helps the VLM to better l**earn from multi-modal in-context demonstrations.** Shows impressive multi-modal ICL performance across various tasks.\n    \n    The experiments underscore the success of our proposed context scheme in **effectively tackling the limitations.**\n    \n\n- Furthermore, the ablation study above demonstrates that **using additional data to facilitate SFT cannot adequately overcome these limitations**, further asserting the significance and value of our suggested strategy.\n- It is crucial to note that these designs are **not merely instances of prompt engineering**; rather, they **address the limitations** of Vision-Language Models in handling complex multi-modal prompts.\n    \n    The experiments, including ablations across synthetic datasets, training paradigms, and model structures, affirm that the observed improvements in MMICL indeed stem from our proposed scheme and Multi-modal ICL tuning.\n    \n- Moreover, the ScienceQA-Img experiment results in **Sec. 3.5** showcases MMICL's effectiveness in **mitigating the hallucination issue** inherent in VLMs. By strategically leveraging ICL tuning, MMICL outperforms other models, even in single-image tasks. Intuition: The context format of MMICL during training forces the model to reason across text and multiple images, therefore helping to combat the hallucination issue.\n\nWe hope this detailed explanation clarifies the rationale behind our design choices and underscores the significance of our contributions. If you have any further questions or require additional clarification, please feel free to reach out."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4878/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700419507705,
                "cdate": 1700419507705,
                "tmdate": 1700730555606,
                "mdate": 1700730555606,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2Jm49THxs0",
            "forum": "5KojubHBr8",
            "replyto": "5KojubHBr8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4878/Reviewer_iVLP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4878/Reviewer_iVLP"
            ],
            "content": {
                "summary": {
                    "value": "The paper explores a pioneering context schema, designed to bolster the VLM's proficiency in effectively handling multimodal inputs and facilitating in-context learning. This method mainly aims to construct the Text-Image Interleaved Dataset (MIC dataset) with the objective of enhancing the VLM's in-context learning capabilities. There are three distinct formats in the MIC dataset, including single image format, multiple interconnected images format, and in-context format, which respectively are tailored for different instances of various vision-language tasks. In experiments, the authors demonstrate that the proposed method (MMICL) outperforms baselines on a wide range of general vision-language tasks, especially on MME and MMBench benchmarks. Moreover, MMICL also shows in-context learning ability on some traditional vision-language tasks in the few-shot setting."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ This paper offers a novel perspective on how to construct image-text interleaved instruction datasets. It highlights how VLM (Vision-Language Model) can simultaneously acquire in-context learning ability during the visual instruction tuning process.\n+ The MIC dataset proposed by this paper covers a wide range of vision-language tasks, including Image Captioning, Video Caption, VQA, VideoQA, Visual Reasoning, Visual Dialogue, and Image Classification.\n+ The experiments conducted on a wide range of general vision-language tasks are very comprehensive. The average results on the MME and MMBench benchmarks demonstrate the impressive zero-shot performance of MMICL compared with other baselines."
                },
                "weaknesses": {
                    "value": "+ The few-shot performance improvement of MMICL appears to be somewhat limited. For instance, in Table 4, except for the VizWiz dataset, in some cases (e.g., FLAN-T5-XXL and Instruct-FLAN-T5-XL), MMICL does not demonstrate significant improvements on other test datasets when adding in-context examples.\n+ The lack of exploration of various interleaved image-text data formats for VLM's in-context learning ability is evident. It is unclear whether only the in-context format (equation 3) can enhance in-context learning ability or if the multiple interconnected images format (equation 2) can also play a certain role."
                },
                "questions": {
                    "value": "+ In Table 4, I observe that MMICL with different backbones exhibits significant performance differences in certain tasks. For instance, the performance gap between MMICL (Instruct-FLAN-T5-XL) and MMICL (Instruct-FLAN-T5-XXL) on Flickr 30k is as large as 34.6. A similar trend can be seen in the VizWiz dataset, with InstructBLIP (Flan-XL) scoring 32.08, while InstructBLIP (Flan-XXL) drops to 15.11. This phenomenon is absent in InstructBLIP. What could be the reason for this? \n+ In Table 4, MMICL (FLAN-T5-XL) shows an improvement of approximately 25.13 in the 4-shot setting compared to 0-shot, whereas MMICL (FLAN-T5-XXL) only improves by 3.82. It's perplexing that increasing the backbone's size leads to a significant decline in few-shot performance. What could possibly be the underlying reason for this?\n+ How many visual tokens does each image take up? Does the input of multiple images take up a lot of LLM's input context length?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4878/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4878/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4878/Reviewer_iVLP"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4878/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699347494762,
            "cdate": 1699347494762,
            "tmdate": 1699636471991,
            "mdate": 1699636471991,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "T3O12yTTm8",
                "forum": "5KojubHBr8",
                "replyto": "2Jm49THxs0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4878/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4878/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Review of Submission4878 by Reviewer iVLP (1/N)"
                    },
                    "comment": {
                        "value": "Thanks for your constructive reviews. It is encouraging to see you find our work offering a novel perspective on enhancing the ICL ability of VLM.\n\nBelow, we provide detailed replies to your comments and hope we can resolve your major concerns.\n\n### **Q1:**\n> The few-shot performance improvement of MMICL appears to be somewhat limited. For instance, in Table 4, except for the VizWiz dataset, in some cases (e.g., FLAN-T5-XXL and Instruct-FLAN-T5-XL), MMICL does not demonstrate significant improvements on other test datasets when adding in-context examples.\n### **A1 :**\nWe appreciate your thorough examination of MMICL's few-shot performance and are committed to addressing your concern to  bolster the validity of our claims.\n\n- Firstly, we sincerely appreciate your pinpointing this issue, enhancing our understanding of MMICL's in-context-learning capabilities.\n    \n    Upon further examination, we identified nuances in the evaluation process on the Flickr30k dataset. Specifically, random-sampled in-context-learning examples introduce a prefix and \":\" in the answer by using such template:\n    \n    > Please provide a description or identification of the subject in image 0:<image0>{embedding}. :: The answer to the question is : {caption} ::\n    > \n    These in-context-learning examples affected MMICL's output in the in-context learning setting, causing it to be more likely to output with a similar prefix and an extra \":\".\n    \n    - And the abnormal performance of MMICL(Instructblip-xxl) is also due to its repetition of the question at the beginning of the response and the additional \":\" at the end of the answer.\n    \n     Unfortunately, the standard preprocessing tools were unable to filter out this additional context, which reduces the cider score of the MMICL in both zero-example and 4-example scenarios.\n    \n    This means that, despite the fact that MMICL has **actually improved through in-context learning**, inaccurate cider score calculations instead result in an abnormal performance decrease for MMICL.\n    \n    Therefore, by recalculating the model outputs with the appropriate preprocessing and filtering out extraneous information, the **abnormal** performance of MMICL **disappears** and the MMICL despite **significant in-context learning ability.**\n    \n    The revised evaluation results, shown in the table below, depict the accurate performance of MMICL on this dataset.\n    \n    | Model | Flickr 30K | WebSRC | VQAv2 | Hateful Memes | VizWiz |\n    | --- | --- | --- | --- | --- | --- |\n    | MMICL(FLAN-T5-XL) (Zero-Shot) | **83.47** | 12.55 | 62.17 | 60.28 | 25.04 |\n    | MMICL(FLAN-T5-XL) (4-Shot) | **83.84** | 12.30 | 62.63 | 60.80 | 50.17 |\n    | MMICL(FLAN-T5-XXL) (Zero-Shot) | **85.03** | 18.85 | 69.99 | 60.32 | 29.34 |\n    | MMICL(FLAN-T5-XXL) (4-Shot) | **89.27** | 18.70 | 69.83 | 61.12 | 33.16 |\n    | MMICL(Instruct-FLAN-T5-XL) (Zero-Shot) | **82.68** | 14.75 | 69.13 | 61.12 | 29.92 |\n    | MMICL(Instruct-FLAN-T5-XL) (4-Shot) | **88.31** | 14.80 | 69.16 | 61.12 | 33.16 |\n    | MMICL(Instruct-FLAN-T5-XXL) (Zero-Shot) | **73.97** | 17.05 | 70.30 | 62.23 | 24.45 |\n    | MMICL(Instruct-FLAN-T5-XXL) (4-Shot) | **88.79** | 19.65 | 70.56 | 64.60 | 50.28 |\n    \n    Notably, MMICL's performance on the Flickr30k dataset improves significantly with in-context-learning examples, as demonstrated by the revised scores.\n    \n    We have also **revised the Table 4** in the revised paper. We sincerely appreciate your careful review and feedback. Your feedback has made our paper better.\n    \n- Secondly, MMICL exhibits substantial zero-shot performance improvements compared to baseline methods (Kosmos-1 and Flamingo) across various datasets.\n    \n    The reason why the inclusion of examples in MMICL does not lead to a two-digit improvement is that in scenarios where performance is **already high**, the assistance provided by these examples **is quite limited**.\n    \n- **Multi-Modal ICL Capability in Out-of-Domain Scenarios:**\n    \n    Apart from the experiments in Sec. 3.6, **Table 25 in Appendix P** highlights MMICL's impressive generalization to an unseen domain in **Minecraft**, even if the images are extremely different compared to the images used by training. MMICL further enhances its understanding of **Minecraft objects with the inclusion of ICL examples**. With 4-shot exemplars, MMICL demonstrates an **8.6-point improvement over the model without ICL examples**. This improvement is **more pronounced with increased examples and model size**, resulting in a 10-point increase compared to the model without ICL examples. The nuanced performance gains in the few-shot setting demonstrate the adaptability and generalization capabilities of MMICL.\n    \n\nWe hope this detailed explanation successfully addresses your concern. Your advice and scrutiny have significantly contributed to the quality of our paper, and we are sincerely grateful for your time and expertise."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4878/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700423119351,
                "cdate": 1700423119351,
                "tmdate": 1700423119351,
                "mdate": 1700423119351,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xjDEa54ak8",
                "forum": "5KojubHBr8",
                "replyto": "2Jm49THxs0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4878/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4878/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Review of Submission4878 by Reviewer iVLP (2/N)"
                    },
                    "comment": {
                        "value": "### **Q2:**\n\n> The lack of exploration of various interleaved image-text data formats for VLM's in-context learning ability is evident. It is unclear whether only the in-context format (equation 3) can enhance in-context learning ability or if the multiple interconnected images format (equation 2) can also play a certain role.\n\n### **A2 :**\n\nThank you for your insightful comment and we genuinely appreciate your constructive feedback.\n\nWe commit to providing a clearer explanation of ablation the present in our paper.\n\n- **Extended Ablation Study:**\n    \n    To address your concern, we have extended our **ablation study** using Instructblip-xl as the backbone model to investigate how the multiple **interconnected image format** (equation 2) and the **in-context format (equation 3)** contribute to the in-context learning (ICL) ability of MMICL.\n    \n- **Ablation Models:**\n\nWe sample 2000 instances of Vizwiz and 1000 instances of Flickr for the **ablation study.**\n\n- **MMICL**: This is the standard model.\n- **w/o context scheme**: This model is trained only with instructions using all datasets from stage 2 of MMICL, without any additional design in context format and model architecture.\n- **w/o in-context format**: This model, devoid of in-context format data, comprises multi-modal data with related images and image declaration.\n- **w/o interrelated images**: This version of the model is trained without any interconnected images. Its training pipeline contains multi-modal in-context data and image declaration.\n\nWe use the **equivalent amount** of data as in stage 2 of MMICL for the ablation models.\n\n- **Comprehensive Evaluation:**\n    \n    We employ the ablation study on both Vizwiz and Flickr under the five and zero of ICL examples. We also evaluate the zero-shot three different settings on the **general benchmark(MME)** and **multi-image reasoning datasets(Icon-QA & NLVR2)** to comprehensively evaluate the sources of the observed performance improvement in MMICL.\n    \n- **Results and Conclusions:**\n    - The ablation results affirm that our **context scheme**, which **combines** image declaration **(eq1)**, interconnected images **(eq2)**, and ICL examples **(eq3)**, is a **key factor** in the observed improvements, providing clarity on **its contribution to MMICL's in-context learning ability**.\n        \n        It can be conclude that the magic of MMICL on **enhancing ICL ability is a combination of eq1&2&3**, rather than just eq3.\n        \n    - Notably, ICL-only achieves the most significant improvements compared to the other two settings in the Vizwiz dataset. This highlights that tuning with in-context format data effectively enhances the model's understanding of in-context samples. The relatively lower improvement on the Flickr dataset indicates that, in the absence of multi-image instruction tuning, the model struggles to effectively utilize in-context examples to improve its performance on tasks without fixed output formats, such as image captioning.\n- **Unique Value of Our Proposed Method:**\n    - The additional evaluation of zero-example experiments on the general benchmark and multi-image datasets further **validates the superior performance** of MMICL, which stems from the **combination of eq1, eq2, and eq3**.\n    - Our designs of the context scheme together help the VLM to show this superior performance. Meanwhile, the additional single image-text pair data for SFT(supervised fine tuning) does not yield comparable enhancements in the targeted areas, underscoring the unique value of our proposed method.\n\n| Task | ICL example | MMICL | w/o context scheme | w/o in-context format | w/o interrelated images |\n| --- | --- | --- | --- | --- | --- |\n| VIZWIZ | zero-example | 27.10 | 28.13 | 15.90 | 23.43 |\n|  | 5-example | 42.66 | 36.23 | 28.33 | 37.13 |\n|  | \u0394 | **15.56** | 8.1 | 12.43 | 13.7 |\n| Flickr | zero-example | 83.94 | 77.84 | 65.33 | 76.88 |\n|  | 5-example | 88.11 | 79.38 | 65.95 | 77.09 |\n|  | \u0394 | **4.17** | 1.54 | 0.62 | 0.18 |\n| MME_Perception(Overall) | zero-example | 1303.59 | 1238.99 | 1141.02 | 1207.70 |\n| MME_Cognition (Overall) | zero-example | 370.71 | 316.79 | 345.36 | 333.21 |\n| Icon-QA | zero-example | 58.12 | 52.80 | 51.95 | 54.35 |\n| NLVR2 | zero-example | 72.45 | 56.65 | 62.63 | 59.60 |\n| Raven | zero-example | 32.00 | 8.00 | 28.00 | 16.00 |\n\nWe trust that this additional information addresses your concerns, and we welcome any further inquiries or feedback you may have. We hope that our response has addressed your concerns."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4878/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700423266863,
                "cdate": 1700423266863,
                "tmdate": 1700730033792,
                "mdate": 1700730033792,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YuJIyuxROi",
            "forum": "5KojubHBr8",
            "replyto": "5KojubHBr8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4878/Reviewer_7yvn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4878/Reviewer_7yvn"
            ],
            "content": {
                "summary": {
                    "value": "# Summary\nThe paper introduces MMICL (Multi-modal In-Context Learning), a novel approach designed to enhance Vision-Language Models (VLMs) in understanding complex multi-modal prompts, which include multiple images and text. The authors highlight that while Large Language Models (LLMs) excel in in-context learning from text prompts, VLMs lag behind, especially when dealing with prompts that involve intricate text-to-image references and relationships among multiple images. To overcome these limitations, the paper proposes a new context scheme that includes an image declaration section and image proxy tokens to improve the in-context learning capabilities of VLMs. Additionally, the authors have constructed a new dataset tailored to train VLMs on complex multi-modal prompts. The experimental results suggest that MMICL sets a new state-of-the-art in zero-shot performance on various vision-language tasks and benchmarks, demonstrating a significant improvement in understanding text-to-image references and relationships between images. MMICL also shows a reduction in language bias, which often leads VLMs to overlook visual content."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "# Strengths\n\n1. **Enhanced Multi-modal Understanding:** MMICL's approach to handling complex prompts with multiple images and text could significantly improve VLMs' performance on downstream tasks.\n\n2. **State-of-the-Art Performance:** The paper reports new benchmarks in zero-shot performance on vision-language tasks, indicating a substantial advancement over existing models. On MME, MMICL seems to achieve the best average scores compared with current VLMs on cognition and perception tasks, indicating a strong performance. On MMBench, they also achieved SOTA performance which demonstrate the prominent ability of MMICL.\n\n3. **Reduction in Language Bias:** MMICL reduces the chances of VLMs ignoring visual content, which is crucial for accurate multi-modal reasoning. Their experiments provided promising results."
                },
                "weaknesses": {
                    "value": "# Weaknesses\n\nAlthough I can spot many advantages in MMICL and I truly believe its a wonderful model. This paper also did a lot of experiments to demonstrate its ability. But here I should address few weaknesses and the authors should better clarify it to make the work's claims more sound.\n\n1. The paper emphasizes the use of interleaved image-text data pairs and in-context learning data for training, suggesting this approach is beneficial for understanding complex multi-modal prompts. However, without experimental comparisons, it's unclear if interleaved pairs offer a substantial advantage over single image-text pairs. It's possible that similar results could be achieved without the interleaved structure. The authors should clarify whether the interleaved structure is a key contributor to performance improvements or if it primarily serves to enhance demonstrations and applicability to real-world scenarios.\n\n2. The paper's focus on MME and MMBench, which involve fixed-format questions, might not fully represent the model's ability to handle freeform answers. Benchmarks like MM-VET[1], which require freeform answers and are evaluated by GPT-4, could provide a different perspective on the model's capabilities. A more diverse set of benchmarks, including those requiring freeform answers, would offer a more comprehensive evaluation of the model's performance and generalizability.\n\n3. The paper does not fully discuss why MMICL's architecture is superior to the Flamingo or other paradigms[2] or other existing methods, which leaves the comparison incomplete. Many claims and designs in Section 2.1/2.2 seems just the author's considerations and are lacking sufficient reasons. Although conducting more comparative experiments may be challenging, the author should at least provide more explanations in these aspects to make the arguments in this paper more substantial. Instead of presenting a final result by combining all designs together, the author should provide further explanations to justify their choices.\n\nFor the dataset and training methodology to be validated, the authors should ideally show that the interleaved image-text pairs lead to better model performance than traditional single image-text pairs, across a range of tasks that include both fixed-format and freeform response requirements. Additionally, the paper should discuss any limitations of the proposed methods, such as potential overfitting to a specific data structure or benchmark, to provide a balanced and transparent evaluation of the approach.\n\n[1] Yu, Weihao, et al. \"Mm-vet: Evaluating large multimodal models for integrated capabilities.\" arXiv preprint arXiv:2308.02490 (2023).\n\n[2] Yao, Zhewei, et al. \"DeepSpeed-VisualChat: Multi-Round Multi-Image Interleave Chat via Multi-Modal Causal Attention.\" arXiv preprint arXiv:2309.14327 (2023)."
                },
                "questions": {
                    "value": "Most of my considerations are at the Weaknesses part. The authors may refer it and consider to address these questions.\n\n1. Why using interleaved image-text data for instruction tuning an VLM would be considered beneficial? In terms of the model performance, is there any experimental comparison to support this claim?\n\n2. Is there any quantitative results to support the models ability in free-form answering (evaluated by GPT-4)?\n\n3. Why the MMICL's design is better for image-text interleaved chatting than Flamingo-based or other architectures?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4878/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699461681000,
            "cdate": 1699461681000,
            "tmdate": 1699636471864,
            "mdate": 1699636471864,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4cZGxvRVz9",
                "forum": "5KojubHBr8",
                "replyto": "YuJIyuxROi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4878/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4878/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Review of Submission4878 by Reviewer 7yvn (1/N)"
                    },
                    "comment": {
                        "value": "Thanks for your constructive reviews. It is encouraging to see you believe our work is wonderful. We sincerely thank you for your time and constructive comments. Below, we provide detailed replies to your comments to resolve your concerns.\n\n### **Q1:**\n\n> The paper emphasizes the use of interleaved image-text data pairs and in-context learning data for training, suggesting this approach is beneficial for understanding complex multi-modal prompts. However, without experimental comparisons, **it's unclear if interleaved pairs offer a substantial advantage over single image-text pairs.** It's possible that similar results could be achieved without the interleaved structure. The authors should clarify whether the interleaved structure is a key contributor to performance improvements or if it primarily serves to enhance demonstrations and applicability to real-world scenarios.\n> \n\n> For the dataset and training methodology to be validated, the authors should ideally show that the **interleaved image-text pairs lead to better model performance than traditional single image-text pairs**, across a range of tasks that include both fixed-format and freeform response requirements.\n> \n\n> Why using **interleaved image-text data** for instruction tuning an VLM would be considered beneficial? In terms of the model performance, is there any **experimental comparison** to support this claim?\n> \n\n### **A1** :\n\n- **Using interleaved image-text data outperforms single image-text data by a large margin, according to ablation experiments.**\n\nWe conduct ablation experiments using the Instructblip-xl as the backbone model in the following settings to verify the effectiveness of interleaved image-text during multi-modal ICL tuning:\n\n  -  **MMICL**: This is the standard model. \n  \n  -   **w/o context scheme**: This model is trained only with instructions using all datasets from stage 2 of MMICL, without any additional design in context format and model architecture. \n  \n  -  **w/o image declaration**: This model is trained without image declaration but includes multi-modal data with interconnected images and in-context data.\n  \n  -  **w/o in-context format**: This model, devoid of in-context format data, comprises multi-modal data with related images and image declaration. \n\n  -  **w/o interrelated images**: This version of the model is trained without any interconnected images. Its training pipeline contains multi-modal in-context data and image declaration. \n\nWe use the **equivalent amount** of data as in stage 2 of MMICL for the ablation models.\n\n- We employ the ablation study on the **general benchmark(MME)** and **multi-image reasoning datasets(Raven, Icon-QA & NLVR2)** to comprehensively evaluate the sources of the observed performance improvement in MMICL. The ablation results affirm that our **interleaved image-text data scheme is a key factor in** observed improvements, clarifying its contribution to MMICL's performance. The superiority of MMICL is driven by the collective impact of our design elements\u2014removing any component cannot guarantee the superior performance of our model. Each component of our design significantly contributes to different aspects of our model. \n\n| Task | MMICL | w/o context scheme | w/o image declaration | w/o in-context format | w/o interrelated images |\n| --- | --- | --- | --- | --- | --- |\n| MME_Perception | **1303.59** | 1238.99 | 1170.87 | 1141.02 | 1207.70 |\n| MME_Cognition | **370.71** | 316.79 | 341.07 | 345.36 | 333.21 |\n| Icon-QA | **58.12** | 52.80 | 47.15 | 51.95 | 54.35 |\n| NLVR2 | **72.45** | 56.65 | 61.0 | 62.63 | 59.60 |\n| Raven | **32.00** | 8.00 | 18.00 | 28.00 | 16.00 |\n\nThe results confirm that the **performance improvement** in MMICL arises from **our proposed interleaved image-text data pairs and in-context learning data**. Additional single image-text pair data for  SFT(supervised fine tuning) does not yield comparable enhancements in the targeted areas, underscoring the unique value of our proposed method.\n\nWe have **included the ablation experiments** and analysis in the **second section of Sec 3.6** in the revised paper."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4878/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700421240376,
                "cdate": 1700421240376,
                "tmdate": 1700654093174,
                "mdate": 1700654093174,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vE6IhquE8e",
                "forum": "5KojubHBr8",
                "replyto": "YuJIyuxROi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4878/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4878/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Review of Submission4878 by Reviewer 7yvn (2/N)"
                    },
                    "comment": {
                        "value": "### **Q2**:\n\n> The paper's focus on MME and MMBench, which involve fixed-format questions, might not fully represent the model's ability to handle **freeform answers**. Benchmarks like **MM-VET**[1], which require freeform answers and are evaluated by GPT-4, could provide a different perspective on the model's capabilities. A more diverse set of benchmarks, including those requiring freeform answers, would offer a more comprehensive evaluation of the model's performance and generalizability.\n> \n> \n> Is there any **quantitative results** to support the models ability in **free-form answering** (evaluated by GPT-4)?\n> \n\n### **A2** :\n\nMMICL shows **superior performance in free-form answering tasks**, such as **WebSRC and Vizwiz**(already in Table 4), **VisDial** (Sec. 3.6) and **MSVD QA and iVQA**(Appendix K). And MMICL shows **significant improvement on MM-VET** compared to the baseline model.\n\n- **MM-VET** indeed serves as an excellent benchmark; thank you for bringing it to our attention. Below are the detailed results.\n    - We have **included the free-form answering evaluation of MMICL on MM-VET** in **Appendix R** in the revised paper.\n    \n    |  | rec | ocr | know | gen | spat | math | total |\n    | --- | --- | --- | --- | --- | --- | --- | --- |\n    | InstructBlip-t5 | 17.1 | 8.4 | 5.5 | 2.6 | 8.0 | 3.8 | 14.1 |\n    | MMICL-t5 | **29.9** | **14** | **17.4** | **11.2** | **18.1** | **3.8** | **24.8** |\n    - It is noteworthy that compared to the Flant5 backbone's instruct blip model, MMICL, with the identical backbone, **exhibits significant enhancement** on MM-VET. This testifies that our proposed method **prevents the model from overfitting a specific data structure** and substantially augments the model's **free-form answering capability**.\n    - The performance of MMICL on MM-VET is relatively low due to MMICL employing **the Flant5 backbone model**. As T5 is mainly finetuned on NLP benchmarks containing many multi-choice QA and classification datasets, it tends to **output shorter content** during freeform question answering, which is disadvantageous when evaluated with GPT4. This feature is also discovered in Instructionblip[1].\n    - The **Vicuna backbone** model generally outperforms the flant5 backbone model for open-domain QA questions.\n    - Later, we will add the evaluation result on the MM-VET using the MMICL with the vicuna backbone.\n- We have evaluated the **free-form answering ability of MMICL across various tasks.**\n    - Table 4 (**Sec. 3.4**) displays the effectiveness of MMICL on **WebSRC and Vizwiz**, the datasets targeted for open-domain question answering. Its performance on **VisDial**, a focus area for visual dialog, is presented in Table 6 (**Sec. 3.6**). Furthermore, the results of MMICL on the video datasets **MSVD QA and iVQA** are presented in Table 22 (**Appendix K**),  the datasets focus on the open-domain video question answering.\n    \n    Those experiments all together showcased MMICL\u2019s superior  performance within free-form answering scenarios.\n    \n- However, the **key innovation insight** of our paper is focused on **augmenting the ICL ability of the VLM** rather than improving the model's ability to handle freeform answers. \n\n    As shown in Sec. 3.3-Sec. 3.6, MMICL shows remarkable performance in **handling complex multi-image reasoning and multi-modal in-context learning.** The SOTA performance on the Raven and Bonground-HOI dataset shows the MMICL has impressive multi-image reasoning abilities. The 13-point improvement on the Winoground shows the MMICL understands complex text-to-image reference. Moreover, MMICL demonstrates impressive multi-modal ICL performance across various tasks.\n\n[1] Dai, W., Li, J., Li, D., Tiong, A.M., Zhao, J., Wang, W., Li, B.A., Fung, P., & Hoi, S.C. (2023). InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning. *ArXiv, abs/2305.06500*."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4878/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700421695635,
                "cdate": 1700421695635,
                "tmdate": 1700460001208,
                "mdate": 1700460001208,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YnzvZ88nXB",
                "forum": "5KojubHBr8",
                "replyto": "YuJIyuxROi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4878/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4878/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Review of Submission4878 by Reviewer 7yvn (4/N)"
                    },
                    "comment": {
                        "value": "### **Q4:**\n\n> Additionally, the paper should discuss any limitations of the proposed methods, such as potential overfitting to a specific data structure or benchmark, to provide a balanced and transparent evaluation of the approach.\n> \n\n### **A4:**\n\nThank you for your detailed review, and we appreciate the opportunity to further discuss the limitations of our current method:\n\n1. **Potential Overfitting:**\n    \n    We **mitigate the risk of overfitting** specific data structures by expanding the instruction templates with ChatGPT, **ensuring diversity** in the MIC dataset. **Table 25 in Appendix P** demonstrates the generalization ability of MMICL to an **unseen domain in Minecraft**. MMICL is able to generalize in the Minecraft domain even if the images are extremely different compared to the images used by training. This **out-of-domain understanding** ability demonstrates that MMICL does not overfit a specific data structure.\n    \n2. **Context length Constraints:**\n    \n    The **context lengt**h of the backbone language model **limits the number of images** that can be included in the input. It makes our MMICL cannot accept unlimited images. Therefore, during MIC tuning, we incorporate up to 8 images per instance. However, MMICL exhibits **robust generalization beyond this limit.** For instance, on evaluations of the Bonground-HOI dataset, MMICL achieves remarkable performance even with a context of **14 images.** Similarly, in the context of video datasets, MMICL maintains great performance with input up to **12/16 frames**. We attribute this impressive generalization to the relative position embedding employed by Flant5 in its architecture.\n    \n3. **Model Exploration on Decoder-Only Architectures:**\n    \n    We acknowledge that our exploration has focused on the T5 series models, and the effectiveness of our proposed approach to decoder-only architectures has not been fully explored. This aspect of our approach will be thoroughly investigated and analyzed.\n    \n\nWe have **included the discussion of the limitation of our work** in **Appendix T** in the revised paper."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4878/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700422712084,
                "cdate": 1700422712084,
                "tmdate": 1700481563273,
                "mdate": 1700481563273,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "66suFMMwqv",
                "forum": "5KojubHBr8",
                "replyto": "YuJIyuxROi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4878/Reviewer_7yvn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4878/Reviewer_7yvn"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors Rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for your feedback! \n\nI believe these experimental results can indeed help us better understand the method of MMICL. However, after considering the opinions of other reviewers, especially #eWon, I agree with their point that the paper lacks an adequate description of the dataset itself. Specifically, we don't know where the insights for constructing this dataset came from. Was there any insight that led to the plan of constructing the dataset with an interleaved finetuning approach being better than single image finetuning?\n\nMoreover, the introduction of this dataset is indeed lacking rigor, and there is no way to obtain even a high-level understanding of the dataset construction process from the main text. I believe this is the most crucial factor hindering my understanding of the logical fluency of this paper.\n\nUntil the aforementioned issues are clarified, I consider the contribution of this paper to be limited to a model that performs well on benchmarks, without demonstrating any significant emerging abilities. Therefore, I'm sorry to say that I currently don't think it is suitable to be accepted as an ICLR paper."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4878/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700536665877,
                "cdate": 1700536665877,
                "tmdate": 1700536665877,
                "mdate": 1700536665877,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]