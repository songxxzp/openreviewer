[
    {
        "title": "Listen, Think, and Understand"
    },
    {
        "review": {
            "id": "WHL1hg7nOW",
            "forum": "nBZBPXdJlC",
            "replyto": "nBZBPXdJlC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4061/Reviewer_wB1w"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4061/Reviewer_wB1w"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose to connect an audio encoder (AST) with a large language model (LLM) through LoRA adapter tuning for audio understanding. To fine-tune this model, the authors also propose and curate an OpenAQA-5M dataset which mixes of existing audio tasks such as audio classification, audio captioning, and sound event detection as close-ended tasks, and leverage LLMs to generate question and answer pairs given text metadata. The authors further conducted human evaluation to verify these generated data."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- OpenAQA-5M is a good contribution to provide open-ended question answering in audio domain, especially it is verified with human evaluation.\n- Ablation study shown in table 5 provides good insights for choices of LoRA params and the benefit of curriculum in staged training."
                },
                "weaknesses": {
                    "value": "- For the open-ended questions, this work seems to focus mainly on solely LLMs assisted question answer generation. It is not intuitive to understand to what extent the model relies on the input audio versus on the common sense knowledge that is already encoded in the LLMs. It would be great to define and identify beyond current close-ended tasks with new lower level tasks which really require using the audio, such as counting sound events, ordering of events, etc. These type of questions might already exist in the proposed dataset, it would provide more insights to dive deeper into those.\n- Table 5 on the right for the training curriculum, it would be great to also include the language instruction following rate. And further discuss the correlation between the classification performance and the instruction following rate, if there is any insights that can be drawn."
                },
                "questions": {
                    "value": "- The acoustic features mentioned in 3.1 are also generated from LLMs, how are these verified?\n- In the temporal analysis paragraph in 3.1, how are the understanding of order of sounds evaluated?\n- In 5.2.1, LTU can answer follow-up questions about the details, there is an example where LTU shows that the bell rings three times, are there other examples showing the counting capabilities? Is it possible to further quantify this?\n- In 5.2.1, multi-turn conversation is mentioned, are the previous audio tokens and questions also passed in as context or are the multi-turn independent each turn? This can be another interesting property for few-shot in-context learning if possible."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4061/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4061/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4061/Reviewer_wB1w"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4061/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698503684274,
            "cdate": 1698503684274,
            "tmdate": 1700692104105,
            "mdate": 1700692104105,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PWuxmtiwhd",
                "forum": "nBZBPXdJlC",
                "replyto": "WHL1hg7nOW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4061/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4061/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer wB1w (1/4) Temporal Analysis Experiments"
                    },
                    "comment": {
                        "value": "Dear Reviewer wB1w,\n\nThank you for the positive feedback and all these very valuable and constructive questions/suggestions! Let us respond to your questions point by point. \n\n---\n**Question 1 - Additional Audio-Conditioned Experiments**\n\n> For the open-ended questions, this work seems to focus mainly on solely LLMs assisted question answer generation. It is not intuitive to understand to what extent the model relies on the input audio versus on the common sense knowledge that is already encoded in the LLMs. It would be great to define and identify beyond current close-ended tasks with new lower level tasks which really require using the audio, such as counting sound events, ordering of events, etc. These type of questions might already exist in the proposed dataset, it would provide more insights to dive deeper into those.\n\nWe totally agree with the reviewer on this point! And this is the reason why we rigorously evaluate the model performance on closed-ended tasks while the main novelty on open-ended tasks. In the paper, Appendix K, we conduct a probe experiment to check if LTU can really understand the sound concepts rather than associates the acoustic concept with a sound class. In our training data, samples of the ``ambulance siren`` class are always described as ``high pitched``. To check if LTU can disentangle the concept of a high-pitch and sound class of ambulance sirens. We manually lower the pitch (with librosa.pitch_shift) of 53 evaluation audios of the ambulance siren class and check LTU's output to the question ``What is the pitch?`` on these audios. As shown in Figure 2 of the paper, LTU's prediction aligns well with the actual pitch, indicating it indeed learns the concept of pitch rather than just associating it with a specific sound class.\n\n---\n**Question 2 - Temporal Analysis  Experiments**\n> It would be great to define and identify beyond current close-ended tasks with new lower level tasks which really require using the audio, such as counting sound events, ordering of events, etc. \n\n>In the temporal analysis paragraph in 3.1, how are the understanding of order of sounds evaluated?\n\n>In 5.2.1, LTU can answer follow-up questions about the details, there is an example where LTU shows that the bell rings three times, are there other examples showing the counting capabilities? Is it possible to further quantify this?\n\nWe sincerely appreciate the reviewer's suggestion to explore the temporal analysis capability of the model, and in response, we are proceeding with the following new experiments. Specifically, we conduct experiments on the audio event order recognition task (seen in the closed-ended training stage) and audio event counting task (unseen in the closed-ended training stage) with ESC-50, a dataset not being used in training. For both experiments, we only include the cases in which LTU follows the instruction. \n\n---\n**1. Recognizing the Order of Sound Events**\n\nWe create a test set where each test sample consists of two audio samples from different categories of the ESC50 dataset, with no temporal overlapping. We then ask LTU to predict the order of the sound events with the prompt ``Which sound begins and ends first?``. We use regular expressions and cosine similarity (based on gpt-text-embedding-ada) to interpret the raw LTU output. For example, for the LTU raw output ``the applause starts first, while the footsteps follow in a rhythmic pattern afterward.``, we first extract ``applause`` and ``footsteps follow in a rhythmic pattern`` using regular expressions. Then we compare the cosine similarity between the text embedding of the ground truth ``clapping`` with the text embeddings of ``applause`` and ``footsteps follow in a rhythmic pattern``, respectively. Since the cosine similarity between the text embedding of the ground truth ``clapping`` and the LTU prediction ``applause`` is higher, we count this sample as correct.\n\nWe evaluate LTU on 550 such samples, obtaining 390 correct and 160 incorrect answers, achieving an accuracy of 70.9%. This shows that LTU can recognizes the order of sound events with a reasonable accuracy."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4061/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700618980323,
                "cdate": 1700618980323,
                "tmdate": 1700619127746,
                "mdate": 1700619127746,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yNBcZOAZBT",
                "forum": "nBZBPXdJlC",
                "replyto": "ZCa3VCEnfQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4061/Reviewer_wB1w"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4061/Reviewer_wB1w"
                ],
                "content": {
                    "comment": {
                        "value": "Thank the authors for your through responses to most of the questions. With the extra answers, I encourage the authors to include them in the paper where it is appropriate. I am increasing the rating."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4061/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700692086862,
                "cdate": 1700692086862,
                "tmdate": 1700692086862,
                "mdate": 1700692086862,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YJekjxGWUE",
            "forum": "nBZBPXdJlC",
            "replyto": "nBZBPXdJlC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4061/Reviewer_oFtB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4061/Reviewer_oFtB"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an audio foundation model, Listen, Think, and Understand by combining an existing audio encoder and LLM. It also creates a new audio dataset called OpenAQA-5M for training the proposed LTU, so that it can enhance the existing audio perception capabilities and also provide a clear explanation of training details. \n\nThe paper effectively delivers that the existing audio models have limitations in reasoning and comprehensibility and suggests that combining audio models and LLM would resolve the problems that the existing audio techniques have suffered.\n\nIt is also well-supported experimentally in two key areas: audio perception and reasoning abilities. Audio perception is evaluated through tasks like classification and captioning. Reasoning ability is evaluated through human assessment, and the paper claims that it outperforms GPT-4."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper claims that it is the first time designing a reasoning comprehension-capable model.\n2. Details of training and dataset are logical and delicate. It handles the hallucination problem of LLM by training close-ended dataset and then non-answerable question-answer pairs. It considers the training direction to be \"first to perceive, and then comprehend the sound\" so that the training starts from using close-ended datasets to open-ended datasets. The paper shows the reasonable claim that it is necessary to gradually train the model from close-ended datasets to open-ended ones because if the open-ended dataset is trained first, the model is heavily dependent on language capability so it is hard to train the audio representation.\n3. The paper is clear and easy to understand."
                },
                "weaknesses": {
                    "value": "1. There seems to be a lack of thought about model structure and loss. It is just a combination of the strong pretrained LLM and the existing audio encoder, AST. Utilizing strong pretrained LLM with multimodal inputs has an alignment issue. Thus, while concatenating the audio feature and the text feature can introduce desired performance, there could be some advancements not just combining pretrained audio model and LLM. For example, BLIP-2 [1] solves misalignment between text and image using 3 losses: 1) Image-Text matching, 2) Image-Grounded Text generation, and 3) Image Text Contrastive Learning. The simple combination of the audio model and LLM does not seem to be novel.\n2. The concurrent works show higher performances. Compared to Pengi, the closed-ended audio task performances are lower. I understand that the proposed paper is focusing on the open-ended problem, but it would be better to elaborate more in detail that the proposed paper is competitive compared to the concurrent works.\n\n[1] Li, Junnan, et al. \"Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.\" arXiv preprint arXiv:2301.12597 (2023)."
                },
                "questions": {
                    "value": "1. I don't understand 64(time) X 16(frequency) in the 5th line of Audio Encoder part in Section 2: LTU Model Architecture. Is it typo(I think it should be 8)? Or what is the meaning of frequency 16?\n2. I want to know how the dataset is formulated (Q and A) that is generated from GPT."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4061/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4061/Reviewer_oFtB",
                        "ICLR.cc/2024/Conference/Submission4061/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4061/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698656415519,
            "cdate": 1698656415519,
            "tmdate": 1700637488121,
            "mdate": 1700637488121,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "F2jjL08n3i",
                "forum": "nBZBPXdJlC",
                "replyto": "YJekjxGWUE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4061/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4061/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer oFtB (1/5) Audio-Text Alignment"
                    },
                    "comment": {
                        "value": "Dear Reviewer oFtB,\n\nThank you so much for taking the time to read our paper and providing very valuable comments and suggestions regarding the model architecture and comparing LTU with Pengi. Please see the following for our point-by-point response. Please also note that we have added substantially more materials in the revised paper (please check the updated PDF file).\n\n---\n**Question 1. Audio-Text Alignment**\n\n>There seems to be a lack of thought about model structure and loss. It is just a combination of the strong pretrained LLM and the existing audio encoder, AST. Utilizing strong pretrained LLM with multimodal inputs has an alignment issue. Thus, while concatenating the audio feature and the text feature can introduce desired performance, there could be some advancements not just combining pretrained audio model and LLM. For example, BLIP-2 [1] solves misalignment between text and image using 3 losses: 1) Image-Text matching, 2) Image-Grounded Text generation, and 3) Image Text Contrastive Learning. The simple combination of the audio model and LLM does not seem to be novel.\n\nWe understand the reviewer\u2019s concern about the architecture and are grateful for the highly constructive suggestion of adopting BLIP-2-like alignment training. Just as the reviewer points out, we connect the audio encoder with the large language model with only a linear layer and before LTU training, this audio encoder has not been trained for any audio-text alignment tasks. \n\nIn our design, the AST's output serves as *soft prompts* for the large language model. According to [1,2], these (multimodal) *soft prompts* do not necessarily need to be in the text embedding space. In practice, some multi-modal large language models in the vision community such as PaLM-E, LLaVA, and Kosmos use a similar architecture and achieve good performance. One reason for not allowing us to directly use Q-Former in BLIP-2 is that the learnable queries are not guaranteed to be temporally aligned, which is important for audio tasks.\n\nWe sincerely appreciate the reviewer's suggestion to explore an additional training stage for audio-text alignment, and in response, we are proceeding with the following new experiments.\n\n[1] Lester, Brian, et al. The Power of Scale for Parameter-Efficient Prompt Tuning. EMNLP 2021.\n\n[2] Driess, Danny, et al. PaLM-E: An Embodied Multimodal Language Model. 2023.\n\nIn the new experiment, we add an additional audio-text alignment training stage before LTU training. We test two types of alignment modules: one is the linear layer used in the original LTU, and the other is a Transformer layer, which allows for more model capacity in the alignment. We train the alignment module on 1.6M audio-text pairs (comprising audio labels or captions) for 10 epochs, using a batch size of 1536 distributed over 4 GPUs, with 384 samples on each GPU. We start with an initial learning rate of 1e-3 and halve it after each epoch. During this stage, we keep the audio and LLaMA text encoders frozen and only train the alignment module. We use the following loss:\n\n$$\n \\mathcal{L} = \\mathcal{L}_c + \\lambda \\cdot \\mathcal{L}\\_{mse}\n$$\n\n$$\n\\mathcal{L}_{mse} = \\frac{1}{N} \\sum\\_{i=1}^N (e^a_i - e^t_i)^2\n$$\n\n$$\n\\mathcal{L}_\\mathrm{c} = - \\frac{1}{N} \\sum\\_{i=1}^N {\\rm log}  \\left[ \\frac{ {\\rm exp} (s\\_{i,i}/\\tau)}{\\sum\\_{k \\neq i} {\\rm exp} (s\\_{i,k}/\\tau) + {\\rm exp} (s\\_{i,i}/\\tau)} \\right]\n$$\n\nwhere $\\lambda = 10$ is used to balance the scale between MSE loss and contrastive loss, $e^t$ and $e^a$ is the text and audio embeddings, respectively, $N = 384$ is the batch size, $s_{i,j}$ is the similarity score calculated as the dot product of normalized text embedding $e^t_i$ and audio embedding $e^a_j$, and $\\tau = 0.05$ is the temperature. This loss aligns both the scale and direction of the audio and text embeddings."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4061/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700612921386,
                "cdate": 1700612921386,
                "tmdate": 1700612921386,
                "mdate": 1700612921386,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TJBsSOdcsc",
                "forum": "nBZBPXdJlC",
                "replyto": "VTxe0eZm89",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4061/Reviewer_oFtB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4061/Reviewer_oFtB"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the author response"
                    },
                    "comment": {
                        "value": "I appreciate the authors' comprehensive explanation of my questionnaires and the additional experiments regarding my concern. Based on the thorough additional experimental results and clear explanation, I would raise my rating. Please update the paper based on the rebuttal."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4061/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700637457941,
                "cdate": 1700637457941,
                "tmdate": 1700637457941,
                "mdate": 1700637457941,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TxHUyhSEJ5",
            "forum": "nBZBPXdJlC",
            "replyto": "nBZBPXdJlC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4061/Reviewer_2qgj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4061/Reviewer_2qgj"
            ],
            "content": {
                "summary": {
                    "value": "The authors present an Audio Language Model (ALM) capable of understanding generic audio signals. The system in its current form is not designed for speech or music transcription but to provide a generic description/caption summarizing the audio. The crux of the problem to solve in such models has to do with text generation conditioned on audio signal. The fundamental strategy of approaches in this realm has been to interface an audio encoder somehow with a Large Language Model (LLM) for audio conditioned text generation. \n\nIn this work the authors propose to use Audio Spectrogram Transformer (AST) as the Audio Encoder and interface it with the LLaMA model using a simple projection layer in between with LoRA (Low RAnk) adapters added on top of LLaMA model. As has been the common practice such a network is trained on multiple audio related tasks by unifying the input output representation of the network in the form of [Audio, Question] as input to [Answer] as output format. To train this model the authors curate data from popular public datasets across 8 different tasks and augment the same audio with multiple Question answer pairs. The Question-Answer pairs are generated in 2 buckets - Close-Ended and Open Ended. In the Close-Ended bucket Questions are paraphrased using GPT3.5-Turbo for diversity. Answers are generated using a rule based system. In the Open Ended bucket Questions and Answers are generated using GPT-3.5 based on Audi metadata (audio events, captions, acoustic features, temporal information) and prompt engineering. The LTU model is then trained solely on the OpenAQA Dataset with an interesting Curriculum training. the gist of curriculum training is to go from simple to complex. Train the projection layer ---> Train AST + Projection layer + LoRA with frozen LLaMa. In terms of tasks: Simpl close ended tasks  --> Close + Open ended tasks. \n\nOverall the paper is very well written. The authors have done an excellent job in presenting the idea, exploring the idea with effective ablation studies, a straight-forward depiction of results with comparison to the right baselines and providing a plausible explanation to interpret the results in the context of baselines."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "Originality:\n1. The idea, at the time the paper was originally written, was indeed very novel as there were not many audio language models around back in May. The idea makes a lot of sense. \n2. Audio Instruction Generation (AIG) is also quite nice and interesting. \n3. The curriculum learning is yet another contribution which makes a lot of sense and the authors have proposed an intuitive curriculum and backed it up with apt ablation study to show its utility. \n4. The Human evaluation study adds a ton of value in judging LTU in my perspective. \n\n\nQuality:\n1. A high quality manuscript with sufficient experimental details. \n2. I thoroughly enjoyed reading the full paper including all the Appendix sections. Very thoughtfully done experimental ablation study. \n\nClarity: \n1. Very well written manuscript with clear non-monotonic description of details. \n\nSignificance: \n1. In my opinion this is a significant paper as it explores one of the straight-forward ways to couple an audio encoder with a trained LLM and carefully examines this coupling from several different viewpoints and contributes the OpenAQA dataset which can be a useful public resource for future research. The curriculum training is yet another aspect that makes this effort worthy as it shows that a brute force approach to just wrap in all possible audio-text paired data may not be as good overall."
                },
                "weaknesses": {
                    "value": "1. It is not clear to me what the value of OpenAQA dataset is on top of of the textual metadata available with most of these datasets. It would have been fascinating if the authors were to do an ablation study to train their model in the format of (Audio, Text) --> Text format - something similar to what PENGI does. Use the text data that comes with each dataset as is and compare this with :\n  a) Using OpenAQA (current setting)\n  b) Augmenting the original audio text pairs with OpenAQA\n\n2. It might be valuable to evaluate PENGI on Open ended tasks. My hunch is LTU would be far better than Pengi in open ended tasks although Pengi might be better on Close-ended tasks. This would probably highlight LTU significantly as the two approaches are contemporary in many ways and are very similar in the overarching goal. However the approaches taken are different. I realize that PENGI's checkpoint was probably not available when this paper was submitted but it is now. I would encourage the authors to do this comparison to show how LTU could be a more generic model which can be super useful for users to interact with an audio language model through natural text."
                },
                "questions": {
                    "value": "Question raised in Weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4061/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698779589973,
            "cdate": 1698779589973,
            "tmdate": 1699636370057,
            "mdate": 1699636370057,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "69QkwcC4d2",
                "forum": "nBZBPXdJlC",
                "replyto": "TxHUyhSEJ5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4061/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4061/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer 2qgj (1/3) Value of OpenAQA"
                    },
                    "comment": {
                        "value": "Dear Reviewer 2qgj,\n\nWe sincerely thank you for your very positive feedback and are gratified to recognize from your insightful comments a deep comprehension of our paper's content and objectives. Let us respond to your questions point by point.\n\n---\n**Question 1. Value of OpenAQA**\n\n>It is not clear to me what the value of OpenAQA dataset is on top of of the textual metadata available with most of these datasets. It would have been fascinating if the authors were to do an ablation study to train their model in the format of (Audio, Text) --> Text format - something similar to what PENGI does. Use the text data that comes with each dataset as is and compare this with : a) Using OpenAQA (current setting) b) Augmenting the original audio text pairs with OpenAQA.\n\nWe thank the reviewer for the question. As the reviewer pointed out in the summary, the OpenAQA consists of two parts: the closed-ended part (the question is paraphased by GPT but the answer is generated with a rule based algorithm based on ground truth labels), and the open-ended part. \n\nThe closed-ended part of OpenAQA is actually very similar to the Pengi training data:\n- Both the OpenAQA closed-ended dataset and the Pengi dataset use a template to formulate answers. For instance, in Pengi, the format for audio event classification is ``{event a}, {event b},...`` (e.g., ``Ambulance, Traffic noise, Accelerating``), while in OpenAQA, it is ``Label: {event a}, {event b}`` (e.g., ``Label: Ambulance, Traffic noise, Accelerating``).\n- The differences between the OpenAQA closed-ended dataset and the Pengi dataset are twofold:\n  - Pengi has only **one** question (input prompt) for each task, such as ``this is a sound of`` for audio event classification. In contrast, the OpenAQA closed-ended dataset paraphrases the question **multiple** times (about 100 times) using GPT-3.5-Turbo. Example questions for the audio event classification task include ``Generate audio labels``, ``Produce audible tags``, and ``Generate audio annotations``. This approach enhances LTU's robustness to question perturbations. The Pengi paper (Appendix C and Table 17) reports varying model performance with different input prompts. Our design aims to mitigate this issue.\n  - OpenAQA uses GPT-3.5-Turbo to generate acoustic feature descriptions, which are then utilized to train the LTU model. This enhances its ability to recognize acoustic features alongside sound classes, allowing LTU to generalize predictions for unseen sound classes based on their acoustic features. An example is ``high-pitched, rapidly changing frequency`` for the ``ambulance`` class, leading to an answer like ``high-pitched, rapidly changing frequency \u2192 Ambulance``. Pengi does not have this design.\n\nHowever, these differences are minor. Essentially, the closed-ended part of OpenAQA and the Pengi training data are generated using very similar methods.\n\nThe major difference between LTU and Pengi is the **open-ended part of OpenAQA**. Here, both the question and answer are generated by **GPT**, as opposed to a rule-based algorithm or template. We present examples in Table 2 of the paper:\n\n``Question: What can be inferred from the fact that traffic noise fades while the ambulance siren echoes?``\n\n``Answer: It can be inferred that the ambulance is approaching or near the location where the audio clip was recorded, and that the traffic is yielding to the emergency vehicle.``\n\n``Question: What mood or atmosphere does the audio clip convey?``\n\n``Answer: The audio clip conveys a sense of urgency, due to the sound of the ambulance siren.``\n\nThe answers to open-ended questions are not taken directly from the text-form labels of the original dataset, but are instead derived by GPT through background knowledge and reasoning, based on the ground truth labels. As shown in Figure 3 of the paper, over 95% of the questions in the 3.7M dataset are unique, covering a large portion of the question space. This enables LTU to answer **any** question, not just ones it has previously seen. In contrast, when Pengi encounters a new question, it struggles to respond appropriately, for example:\n\n``Question: What can be inferred from the combination of mechanical, male speech, and light engine sound events?``\n\n``Pengi: two``\n\n``LTU: It is likely that there are multiple machines or vehicles in operation nearby.``\n\n---\nIn summary, while the closed-ended part of OpenAQA is very similar to Pengi's training data, with only minor differences, the key distinction between OpenAQA and Pengi lies in OpenAQA's open-ended section, where answers are not based on original labels. It is this open-ended aspect that enables LTU to effectively address open-ended questions."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4061/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700645517611,
                "cdate": 1700645517611,
                "tmdate": 1700645534873,
                "mdate": 1700645534873,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cG1cVuNevE",
                "forum": "nBZBPXdJlC",
                "replyto": "TxHUyhSEJ5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4061/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4061/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer 2qgj (2/3) Value of OpenAQA"
                    },
                    "comment": {
                        "value": "**Question 1. Value of OpenAQA** (cont.)\n\nIn our paper, we conduct experiments to compare models trained with **closed-ended data only** (akin to Pengi) with those trained with **both closed-ended and open-ended data** (default LTU). We show results in Table 5 right and Table 14 of the paper. These can be summarized as the following table.\n\n|                  Training Setting                  | Closed-Ended Audio Classification Performance | Open-ended Question Instruction Following Rate |\n|:--------------------------------------------------:|:---------------------------------------------:|:----------------------------------------------:|\n| Closed-ended data only (stage 1,2,3)               |                      37.1                      |                      22.5                      |\n| Closed-ended data only (same training iterations)  |                      47.3                      |                        -                       |\n| Closed-ended and open-ended data (default)         |                      50.3                      |                      96.9                      |\n\nThese results demonstrate that including the open-ended section of OpenAQA leads to improvements in performance for both closed-ended and open-ended tasks. Specifically:\n- For closed-ended tasks, the inclusion of open-ended training results in enhanced performance (from 47.3 to 50.3), indicating that *learning to understand can bolster perception ability.*\n- For open-ended tasks, the model trained only with closed-ended QAs shows a markedly lower instruction following rate compared to the one trained with both types of data (22.5% vs 96.9).\n\nThis evidence justifies the importance of utilizing GPT to generate open-ended QAs in our training regimen. We hope our explanation and experiments could address the reviewer's question.\n\n---\n**Question 2. Compare with Pengi**\n\nWe thank the reviewer for the comment and for pointing us to the pretrained Pengi checkpoint. We were able to download the checkpoint and run experiments based on it. We agree that comparing LTU with concurrent work would better place LTU in the context. The reviewer is correct that Pengi is better on some closed-ended benchmarks (e.g., ESC-50), but as we will show below, on average, LTU is **slightly better** than Pengi on closed-ended tasks and significantly better for open-ended tasks.\n\nWe compare LTU with Pengi in three aspects.\n\n------\n### 1. Technical Similarity and Difference\n\n**Similarity**:\n- Architecture-wise, both Pengi and LTU connects an audio encoder with an autoregressive language model. Both models can generate text from the given audio and text prompt.\n- Performance-wise, both Pengi and LTU can do zero-shot predictions on unseen datasets. On closed-ended tasks, Pengi and LTU perform similarly. \n\n**Difference**\n- **Motivation:** Pengi focuses on *transfer learning*, i.e., using a single model for 8 audio perception tasks, while LTU focuses on *unifying audio perception with understanding*. Unlike LTU, Pengi is not designed for audio understanding and answering *free-form open-ended* questions from users.\n- **Language Model Scale:** Pengi employs GPT2-Base (124M parameters) as its language model, whereas LTU uses LLaMA (7B parameters), which is over 50x larger than GPT2-Base. Additionally, LLaMA is trained with significantly more data than GPT2. Scaling up the language model substantially enhances LTU's understanding capabilities.\n- **Training Data:** Pengi is trained on multiple audio datasets, and directly uses the original text-form labels as answers for AQA. This approach is similar to the closed-ended part of our OpenAQA dataset. However, OpenAQA also includes another 3.7M open-ended AQAs generated with the proposed *audio instruction generation* method (not generated from a template). The open-ended portion of our training data is pivotal in enabling LTU to answer *any free-form open-ended* questions.\n- **Training Curriculum:** To better unifying audio perception and generation, LTU adopts a unique perception-to-understanding training curriculum.\n- **Open-Ended Evaluation:** LTU undergoes evaluation for both closed-ended and open-ended tasks, incorporating both subjective and objective evaluations. Conversely, Pengi is evaluated solely with closed-ended tasks. Please note that the definition of an open-ended task differs between the Pengi paper and this paper; Pengi considers audio captioning as an open-ended task, while we categorize them as closed-ended tasks."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4061/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700646870591,
                "cdate": 1700646870591,
                "tmdate": 1700647198232,
                "mdate": 1700647198232,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YRcGQDk46L",
            "forum": "nBZBPXdJlC",
            "replyto": "nBZBPXdJlC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4061/Reviewer_1UHZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4061/Reviewer_1UHZ"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a multimodal large language model with the ability of general audio perception. The model combines the AST audio encoding frontend with the LLaMA large language model, utilizing LoRA for fine-tuning, resulting in a model with both audio perception and reasoning capabilities. Moreover, this paper constructed a large-scale dataset for this task, encompassing closed-ended and open-ended Q&A pairs. Extensive experiments illustrate that this model exhibits outstanding performance across various audio-related tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.The paper introduces for the first time a large language model that combines both general audio perception capabilities and language reasoning abilities, along with the datasets used for training. It is highly innovative and holds significant importance for the development of general artificial intelligence.\n\n2.Through a significant number of ablation experiments, this paper extensively researched the model's hyperparameter configurations and training strategies, offering highly instructive guidance for related work.\n\n3.The model excels in various audio-related tasks and open-ended question answering, demonstrating its outstanding performance."
                },
                "weaknesses": {
                    "value": "1.The performance of this model is closely related to both the AST encoding frontend and the LLaMA model's performance. Ablation studies on the varying parameter counts of these two components would be valuable, if possible.\n\n2.The authors may add subjective evaluations to the ablation experiments to better demonstrate that the LoRA fine-tuning strategy mitigates catastrophic forgetting issues."
                },
                "questions": {
                    "value": "1.Why is the results of LTU on open-ended problems preferred compared to those of GPT-4 in Section 5.2.2? In terms of the data generation method of open-ended Q-A pairs, LTU's inferential knowledge appears to be transferred from the data generation model, i.e. GPT 3.5 Turbo model. These experimental results seem to indicate that the performance of the student model is superior to that of the teacher model (family).\n\n2.Did the authors investigate whether errors (or hallucinations) in open-ended question generation model (i.e. GPT 3.5 turbo) could impact the performance of the LTU? In other words, is the correctness of the Q&A pairs generated by GPT from audio metadata reliable enough?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4061/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699352269193,
            "cdate": 1699352269193,
            "tmdate": 1699636369996,
            "mdate": 1699636369996,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "m7SCuCbUOj",
                "forum": "nBZBPXdJlC",
                "replyto": "YRcGQDk46L",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4061/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4061/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer 1UHZ (1/3) Impact of the model size"
                    },
                    "comment": {
                        "value": "Dear Reviewer 1UHZ,\n\nThank you for the positive feedback and all these very valuable and constructive questions/suggestions! Let us respond to your questions point by point.\n\n---\n**Question 1 - Impact of model size**\n\n>The performance of this model is closely related to both the AST encoding frontend and the LLaMA model's performance. Ablation studies on the varying parameter counts of these two components would be valuable, if possible.\n\nWe thank the reviewer for this very constructive suggestion. We conduct the following new experiments to study the impact of model size. \n\n---\n**1. Impact of the size of the large language model**\n\nThroughout the original paper, we use LLaMA model consisting of 7 billion parameters (LLaMA-7B) as the LTU language model. In this revision, we conduct experiments on LTU based on LLaMA with 13 billion parameters (LLaMA-13B). \n\n| Size |   ESC50  |   DCASE  |    VS    |    TUT   |    BJO   |    VGG   |    FSD   | AudioSet | Classif. Avg. | AudioCaps |  Clotho  | Cap. Avg. | Audio Question Instruction Following Rate | Language Question Instruction Following Rate |\n|------------|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:-------------:|:---------:|:--------:|:---------:|:-----------------------------------------:|:--------------------------------------------:|\n| LLaMA-7B   |   83.1   | **45.9** |   55.6   | **32.5** | **69.9** |   50.3   |   46.3   | **18.7** |    **50.3**   |  **17.0** |   11.9   |    14.5   |                    **96.9**                   |                     87.6                     |\n| LLaMA-13B  | **83.6** |   44.7   | **64.2** |   31.0   |   52.1   | **50.7** | **46.8** |   17.9   |      48.9     |    16.9   | **12.1** |  **14.5** |                    95.4  |     **89.6**    |\n\nAs indicated in the table above, we observe a slight decrease in audio classification performance with the increasing size of the LLaMA model while the captioning performance improves slightly with larger model sizes. There is no significant difference in the instruction following rate for pure language and audio open-ended questions. However, we note that the 13B model tends to provide more rational answers due to its more extensive knowledge base, as exemplified in the following sample:\n\n``Question: How would the audio clip be different if the power tool was being used in a larger or open space?``\n\n``LTU-7B: The sound of the power tool would likely be louder and more noticeable in an open space.`` (wrong)\n\n``LTU-13B: If the power tool was being used in a larger or open space, the sound of the power tool would likely be more diffuse and less intense than it is in this clip.`` (correct)\n\n---\n**2. Impact of the size of the audio projection module**\n\nSince large language model training is expensive, we can only train a limited number of models with our computational resources. Thus, we started with the strongest audio encoder we could find when we prepared this work (AST with CAV-MAE pretraining and finetuning). A smaller audio encoder could lead to similar performance, but unlikely to outperform the larger audio encoder. In addition, the number of parameters of our audio encoder is just about 1.3% of the LLaMA language model. \n\nThe original LTU utilizes a linear layer to project audio embeddings to the large language model input, which might be overly simple. Therefore, in this revision, we consider to use a Transformer layer instead. To ensure a fair comparison, both the linear and Transformer layers have been pretrained using audio-text contrastive learning, as detailed in Appendix F.\n\n|  Model                                        |   ESC50  |   DCASE  |    VS    |    TUT   |    BJO   |    VGG   |    FSD   | AudioSet | Classif. Avg. | AudioCaps |  Clotho  | Cap. Avg. |  Audio Question Instruction Following Rate | Language Question Instruction Following Rate |\n|------------------------------------------------|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:-------------:|:---------:|:--------:|:---------:|:----------:|:-------------:|\n| Linear Layer              | **83.2** |   45.3   | **59.5** |   30.4   |   **64.4**   |   50.0   | **46.5** |   **18.3**   |  **49.7**   |   16.8   | **11.9** |    **14.4**   |    86.5    |      **93.2**     |\n| Transformer Layer       |   80.6   | **46.8** |   55.8   |   **30.5**   |   61.9   | **51.9** |   45.7   |   16.7   |    48.7   |    16.8   |   11.5   |    14.2   |  **89.9**  |      93.1     |\n\nAs shown in the table, expanding the layer capacity results in decreased performance, which can be attributed to the introduction of a larger number of randomly initialized parameters that complicates the training process. We thus conclude that the original configuration of a linear layer is a good design choice. \n\nWe thank the reviewer again for the insightful comment, we have added these discussions to Appendix E and F of the paper."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4061/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700632588129,
                "cdate": 1700632588129,
                "tmdate": 1700633840951,
                "mdate": 1700633840951,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]