[
    {
        "title": "Understanding and Improving Adversarial Attacks on Latent Diffusion Model"
    },
    {
        "review": {
            "id": "Q7BgZtIbbU",
            "forum": "yNJEyP4Jv2",
            "replyto": "yNJEyP4Jv2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4599/Reviewer_WV9g"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4599/Reviewer_WV9g"
            ],
            "content": {
                "summary": {
                    "value": "This paper theoretically analyzes the adversarial attacks against latent diffusion models (LDM) for the mitigation of unauthorized usage of images. It considers three subgoals including the forward process, the reverse process, and the fine-tuning process. It proposes to use the same adversarial target for the forward and reverse processes to facilitate the attack. Experiments show it outperforms baselines and is robust against super-resolution-based defense."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper studies protecting images from being used by stable diffusion models without authorization, based on adversarial perturbations. This is an important research problem.\n\n2. Different from existing empirical studies, this paper proposes a theoretical framework to help understand and improve adversarial attacks.\n\n3. Experimental results show the proposed method can outperform existing baselines."
                },
                "weaknesses": {
                    "value": "1. Not clear if this method needs white-box access to the subject LDM. That is, do the adversarial attackers use the same network used by infringers? Are the adversarial examples generated on one model/method still effective on different or unknown models/methods?\n\n2. Only one defense method is evaluated. Are the adversarial samples robust to other transformations such as compression or adding Gaussian noises? Also, no adaptive defense is evaluated. If the infringers know about this adversarial attack, can they adaptively mitigate the adversarial effects?\n\n3. From my understanding, Liang & Wu, 2023 and Salman et al., 2023 are not just \"targeting the VAE\" as claimed in this paper. They attacked the UNet as well.\n\n4. Many issues in the writing. \n\n    4.1. On Page 2, \"serve as a means to\" -> \"mean\". \n\n    4.2. In Section 2.2, it says \"As shown in Figure 1 adversarial attacks create an adversarial example that seems almost the same as real examples\". However, Figure 1 only contains the generated images by the infringers instead of the adversarial examples as indicated by the title.\n\n    4.3. The references to figures and tables are incorrect such as \"Figure 5.3\", \"Table 5.2\", \"Table 5.3\", etc. In the ablation study, the caption of the table is \"Figure 5\".\n\n    4.4. Some math symbols are not defined where they first appear. For example, It would be better to mention that $\\phi$ in Section 2.1 means the VAE. I suggest to use $\\mathcal{E}\\_{\\phi}$ or $\\mathcal{E}$ consistently. What does $\\sqrt{\\bar{\\alpha\\_t}}$ (the line below equation 5) mean? The text below Equation 10, for \"$q\\_{\\phi}(v\\_t | x')$ and $q\\_{\\phi}(v\\_t | x')$\", the second one should be $q\\_{\\phi}(v\\_t | x)$. It would be better to briefly explain the N, M, and K in  Algorithm 2.\n\n    4.5 The citation of SR in section E.2 is wrong. \"Salman et al., 2023\" -> \"Mustafa et al. 2019\"."
                },
                "questions": {
                    "value": "1. Could you explain why the equation 4 holds? Why do we need to use $q$ to express the left $p$?\n\n2. Can the adversarial examples be effective for different or unknown models/methods? \n\n3. According to section E.1, the target $\\mathcal{T}$ in Equation 15 and 16 is defined as $\\mathcal{E}(x^{\\mathcal{T}})$. I can understand for the $\\mathcal{L}\\_{vae}^{\\mathcal{T}}$, it's meaningful to encourage $\\mathcal{E}(x')$ to be close to $\\mathcal{E}(x^{\\mathcal{T}})$. However, for the UNet part, what's the rationale to encourage the predicted noise at each timestep to be close to $\\mathcal{E}(x^{\\mathcal{T}})$? Because I think the final output $z_0$ should be close to $\\mathcal{E}(x^{\\mathcal{T}})$, but not the intermediate predicted Gaussian noise."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4599/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4599/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4599/Reviewer_WV9g"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4599/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698086302397,
            "cdate": 1698086302397,
            "tmdate": 1699636438758,
            "mdate": 1699636438758,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7Oja9gjnkR",
                "forum": "yNJEyP4Jv2",
                "replyto": "Q7BgZtIbbU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4599/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4599/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the review"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the insightful review and feel sorry for the late response. We have done a major revision of our paper, removing the main part of our theory and complementing extensive experimental results. Please refer to our official comment. \n\nFor the questions raised in the review, we will respond to them point by point.\n\n\nWeaknesses:\n\n1.   Not necessarily. We are highly aware that in real world settings, the attacker can have no knowledge about the infringers(totally black-box). Thus, we conduct a cross-model transferability experiment for SD1.4/SD1.5/SD2.1(please refer to section 5.3). Results show that our method remains strong when facing unknown models. Moreover, we also test our method\u2019s performance when the training prompt of LoRA is different from the one used for attacking. Our method also remains robust in this black-box setting(please refer to Appendix B.3). To simulate absolute black-box settings, we collect a piece of data from a community user (who generously shares his/her experiences), where we have no knowledge about any training detail, including the model and the prompts. The only thing we know is the noise budget is 4/255. In this case, our method still has satisfying distortion to the output image(please also refer to Appendix B.3).\n\n2.   We have conducted experiments on other common defending methods (adding gaussian noises, JPEG compression and resizing). Numerical and visualization results show that our method is also robust to these defenses. We\u2019ve also explored some adaptive defense strategies, and we highly agree on the academic significance of such defenses. However, as the goal of our method is to help the community to protect their copyrighted content, we think publishing techniques that aim to overcome the attack is not socially responsible. Therefore, we\u2019ll not release any details of such strategies in our paper.\n\n3.   Yes indeed. Thanks for your correction! We\u2019ve corrected our description in the revision.\n\n4.  Thanks for your detailed review, we\u2019ve corrected and updated the paper based on the given points.\n\n      4.1  We\u2019ve reconstructed our paper in a more fluent way and corrected grammar mistakes.\n\n      4.2  We\u2019ve updated the references and the figures.\n\n      4.3  We\u2019re sorry for the typographical errors. They have been fixed in the revision.\n\n      4.4  We did have inconsistent symbol definition in different sections. In the new version, we carefully choose our symbols and define them at the very beginning. \n\n      4.5  We\u2019ve corrected our citation mistakes in the revision.\n\nQuestions:\n\n1.   We consider all latent variables in the LDM, including the latent variables z_{0:T} (reformulated as v_{0:T} in the first version of our paper) in the forward process and the latent variables z_{0:T} in the reverse process. We apply the sum operator over all these latent variables and yield Eq 4.\n\n2.   Yes they can, as mentioned in the first point in the weaknesses section.\n\n3.   The most intuitional way is indeed to let the output $z_0$ be close to $\\mathcal{E}(x^T)$. And that is what the targeted version of Anti-DreamBooth intends to achieve(for detailed mathematical formulation of targeted Anti-DB please refer to section 3.1). However, results have shown the inferiority of this method, even compared to un-targeted ones. The intuition of our method is to \u201ctrick\u201d the prediction of U-Net to have consistent errors under various timesteps, which can then be accumulated in the denoising process. By trying to \u201cpull\u201d the prediction close to a fixed target, we can control the prediction error to be consistent and have semantic similarity to the target ($\\mathcal{E}(x^T)$), therefore, adding strong target-dependent semantic distortion to the output image. We have a great visualization for this intuition and give intuitive explainations for why targeted Anti-DB doesn\u2019t work(Please refer to Appendix B.2)."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4599/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700555874692,
                "cdate": 1700555874692,
                "tmdate": 1700556064918,
                "mdate": 1700556064918,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uIhyIKoSHb",
            "forum": "yNJEyP4Jv2",
            "replyto": "yNJEyP4Jv2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4599/Reviewer_19tS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4599/Reviewer_19tS"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a theoretical framework for understanding adversarial attacks on latent diffusion models. Based on this framework, the paper proposes a novel and efficient adversarial attack method that exploits a unified target to guide the attack process in both the forward and reverse passes of latent diffusion models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper focuses on curbing the misuse of powerful LDM-based generative models for unauthorized real individual imagery editing, which is an important topic for securing privacy.\n\n2. The theoretical foundation behind adversarial attacks on diffusion models is built, which contributes to the understanding of the behaviors of adversarial attacks."
                },
                "weaknesses": {
                    "value": "1. More thorough examination accounting for a wider range of generative techniques could further validate the method's real-world utility and limitations.  While the proposed attack focuses on the prevalent LDM framework, its generalization to other powerful generative paradigms like SDXL, DALL-E, and Deep Floyd remains untested. \n\n\n2. A more powerful baseline of PhotoGuard, i.e. Diffusion Attack is not compared to. This comparison could help gauge the true leadership of the new method. Without including this more powerful adversarial technique, the paper's claims about the proposed attack outperforming the current approaches remains uncertain.\n\n3. The authors assert a memory-efficient design but do not provide details to support this claim. Further explanation or experimental evaluation of memory usage compared to alternative approaches would help validate the proposed method's efficiency advantages."
                },
                "questions": {
                    "value": "Please refer to the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4599/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698672544427,
            "cdate": 1698672544427,
            "tmdate": 1699636438677,
            "mdate": 1699636438677,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7q2s6RbpdQ",
                "forum": "yNJEyP4Jv2",
                "replyto": "uIhyIKoSHb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4599/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4599/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the review"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the insightful review and feel sorry for the late response. We have done a major revision of our paper, removing the main part of our theory and complementing extensive experimental results. Please refer to our official comment. \n\nFor the questions raised in the review, we will respond to them point by point.\n\nWeaknesses:\n1.  Unfortunately we don\u2019t have enough time and resources to deploy SDXL and DeepFloyd IF and conduct experiments on them within the rebuttal period. But we evaluated our method on SD1.4,SD1.5 and SD2.1. Additionally, we add new experiments on cross-model transferability in Section 5.3. It shows that our attacks not only work well on Stable Diffusion 1.4 and Stable Diffusion 2.1 but also enjoy cross-model transferability that attacks whose backbone is SD1.5 work well on SD1.4 and SD2.1. Hence, we believe this can answer the question. Supported by cross-model experiment results, we\u2019re quite confident about the performance of this method in the range of LDMs. We\u2019ll continue on exploring our methods\u2019 effect on more powerful generative paradigms in the future and update the results.\n\n2.  We highly agree that the comparison between Diffusion Attack and ours is needed for deciding the leader of this field. However, the high demand of Diffusion Attack on GPU has been a challenge to our limited computation resources. Diffusion Attack requires a lot of time and more importantly, gpu memory. Under bf16/fp16 and our memory efficient optimization, Diffusion Attack still requires about 17G of gpu memory to run an attack on one single image and requires a much longer time for one round of PGD attack than other methods. This makes it hard to make a fair comparison between Diffusion Attack and ours. We\u2019ll conduct the experiment as soon as our computation resource is upgraded(will soon!). Before we have the results, we add an additional explanation in the paper about this issue.\n\n3.  Thank you for your advice! We admit the details in our original version are not sufficient, and we recognize the importance of memory efficiency, as most personal computers wouldn\u2019t have as much GPU memory as researchers do. In further experiment, we find that the numerical results of original results is higher than the actual usage, possibly due to a more aggressive caching strategy on GPUs with larger memory. We retest our result after disentangling the caching factor and add more detailed information about our strategy. We\u2019ve also tested other methods and made numerical evaluations, showing our advantages in memory efficiency. Please refer to section 4 for more details."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4599/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700555950167,
                "cdate": 1700555950167,
                "tmdate": 1700556086869,
                "mdate": 1700556086869,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rc8L3lVtjO",
            "forum": "yNJEyP4Jv2",
            "replyto": "yNJEyP4Jv2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4599/Reviewer_oH1F"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4599/Reviewer_oH1F"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposed at a theoretical framework for adversarial attacks on Latent Diffusion Model (LDM).  The key to the theory is formulating an objective to minimize the expectation of the likelihoods conditioned on adversarial samples, of which the two terms implemented within the LDM explains the adversarial attack on the VAE and on the noise predictor, respectively. In addition, a new adversarial attack combining the two types of adversarial attacks are proposed."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. Adversarial attacks on LDM is an interesting and practical problem.\n2. Various experiments are conducted."
                },
                "weaknesses": {
                    "value": "The proposed theoretical framework is not sufficiently innovative. In addition, the methodology exists many errors and the experimental verification are not sound. Specifically,\n\n1. The key formulation of minimizing the conditional likelihood is trivial. The given theoretical proof is complicated. Actually, the likelihood equivalent to the KL divergence has been well-know. From this perspective, the proof is somehow trivial.\n\n2. There exists many wrong equations.   \na. In Eq. (5), the left term q(v_t|x) should be equal to the integral of the right term. Similar issue in Eq. (8).  \nb. In the first paragraph of Sect 3.2, q(v_t|v_{0:t-1} is mistakenly formulated.  \nc. In the last paragraph of Page 4, z_{t-1} is mistakenly formulated.  \nd. In Eq. (3), the sum in terms of z is mistakenly formulated given the expectation.   \ne. For Eq. (3), (4), (9)\u2026, p()|x=x\u2019 or p()|x\u2019 is inappropriate, which should be put as the subscript or p(|x=x\u2019).  \nf. The reformulation of z as v is unnecessary.  \n\n3. From the empirical results (Table 1), the strategy of combining two adversarial attacks does not perform significantly better than the Eq. (16). This raises doubt about the effectiveness of the newly proposed attack method.\n\nMinor:\nSome references are wrongly denoted, e.g. Figure 5.3."
                },
                "questions": {
                    "value": "1. The proposed theoretical framework is not sufficiently innovative. \n\n2. The methodology exists many errors\n\n3. The experimental verification are not sound"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4599/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698727271099,
            "cdate": 1698727271099,
            "tmdate": 1699636438596,
            "mdate": 1699636438596,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1DiWXK2YTM",
                "forum": "yNJEyP4Jv2",
                "replyto": "rc8L3lVtjO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4599/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4599/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the review"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the insightful review and feel sorry for the late response. We have done a major revision of our paper, removing the main part of our theory and complementing extensive experimental results. Please refer to our official comment. \n\nAs stated in our official comment visible to all reviewers, we remove the main part of the theory in the new version of our paper and replace it with the process of how we yield our methods intuitively. The main reason is that we find our theory cannot explain some observations, while the minor reason is the mistakes. We are sincerely sorry about that and really appreciate the insight review that drives us to rethink our original theory. We believe that replacing the theory with our intuition can clear most of the theoretical problems raised by the review.\n\nFor the empirical results, as stated in our official comment, we conduct extensive experiments on cross-model validation, denoising countering, and black-box evaluation to validate the performance of our method. These results can prove that our method outperforms existing methods empirically and can serve as a practical tool.\n\nAlthough we remove the theory part in the new version of our paper, we will still respond to the theoretical questions raised by the review point by point based on the old version of our paper.\n\n\nWeaknesses:\n\n1. We agree that the previous version of our paper includes theoretical results that are trivial and cannot explain some observations. Hence, we remove the theory and replace it with the intuitive process that yields the proposed method. We sincerely hope that the reviewer can read the new version of our paper and give advice, which will significantly help us refine our paper furthermore.\n\n2. We will answer your question point by point\n\na) Theoretically it should be the integral. However, empirically the variance of q(z_0|x) is extremely small (1e-6). Hence, q(z_0|x) is exactly a deterministic function that maps x to z_0. For this reason, we do not need to formulate q(z_t|x) as an integral.\n\nb) We agree that it is wrongly formulated. However, the theoretical speculation afterwards only exploits q(v_t|v_0), which has a closed form. Hence, this error does not influence the speculation a lot.\n\nc) We agree that it is wrongly formulated and should be $z_t\\sim q(z_t|x)$.\n\nd) Eq 3. is the definition of the adversarial attack on LDM. Hence, we are not \nsure what mistakes there are in the definition. We guess it might be the formulation of expectation that we put the condition outside of the probability. We respond to this problem in e.\n\ne) p(), x=x\u2019 is a mistaken expression. However, our expression is exactly $\\mathbb{E}[p(|x)| x=x\u2019]$. This expression is commonly used in statistics. Despite that, we correct this formulation according to the review in the new version of our paper.\n\nf) We agree that the reformulation is not necessary and cancel it in the new version.\n\n3. We agree that the difference in performance cannot be explained by the theory in the first version of our paper. This is the main reason why we remove it in the new version of our paper. Instead, we attribute the success of Eq 16. (in the old version, now newly named Improved Targeted Attack, ITA) to the introduction of the target and the misalignment between human vision and diffusion models. Eq 15 (in the old version) now only serves as an easy-to-implement variant of Eq. 15 which sometimes shows superiority over SDEdit.\n\nMinor: Thanks for your correction, we\u2019ve updated to the correct references.\n\nQuestions:\n\n1. We agree that the theoretical framework cannot work in explaining the result of our proposed methods. Hence, we replace it with the intuitive process about how we yield our proposed methods and try to provide some empirical observations on why our methods outperform. Again, we apologize for our mistaken introduction of the theoretical part in the first version of our paper and sincerely hope that the reviewer can give us more advice on the updated version.\n\n2. See 1. \n\n3. We highly understand the concern about our experimental verification. We\u2019ve conducted more experiments including cross-model transferability, more ablation studies,denoising countering and so on. We\u2019ve also added more visualization to side-prove our views and numerical results and added more details in experiment setups. We hope the new version of our paper can somewhat relieve the concern.\n\nAgain, we apologize for the mistakes we have made in the first version of our paper. We try our best to correct and make up for them in the major revision. We will sincerely appreciate it if the reviewer reads the new version of our paper and gives further advice."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4599/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700556609725,
                "cdate": 1700556609725,
                "tmdate": 1700556637879,
                "mdate": 1700556637879,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "hgoSKVHZ92",
            "forum": "yNJEyP4Jv2",
            "replyto": "yNJEyP4Jv2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4599/Reviewer_8G7v"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4599/Reviewer_8G7v"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a method for improving adversarial attacks on Latent Diffusion Models (LDMs). The purpose is to generate adversarial examples preventing the LDMs from generating high-quality copies of the original images in order to protect the integrity of digital content. The authors mathematically introduce a theoretical framework formulating three sub-goals that existing adversarial attacks aim to achieve. This framework exploits a unified target to guide the adversarial attack both in the forward and the reverse process of LDM. The authors implement an attack method jointly optimizing three sub-goals, demonstrating that their method outperforms current attacks generally. The experiments are focused on the attacks on training pipelines, including SDEdit and LoRA."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tThe author conducted extensive mathematical derivations, providing mathematical explanations for the existing methods. \n\n2.\tThe experiments show that this method outperforms the baseline in most criteria and succeeded in attacking LoRA and SDEdit with Stable Diffusion v1.5."
                },
                "weaknesses": {
                    "value": "1.\tThe only backbone model used in the experiments is Stable Diffusion v1.5. \nThere are plenty of more recent LDMs, such as Stable Diffusion v2.1 [1] and DeepFloyd/IF [2]. Will this method perform well in more advanced LDMs?\n\n2.\tThe pseudo-code of algorithm 1 seems redundant and demonstrates nothing. \nIt literally equals to its description: \u201cTo optimize J_{ft}, we first finetune the model on x and obtain the finetuned model \u03b8(x). Then, we minimize J_{ft} over x\u2032 on the finetuned model \u03b8(x).\u201d\n\n\n3.\tThe target image adopted by Liang & Wu (2023), visualized in \u201cFigure E.1\u201d (Is it mislabeled in Figure 8?), is the only target image used in the experiments. Did the authors try using different target images? Will the target image affect the effectiveness of this method?\n\n4.\tThere are issues in the document layout. The labels of figures are mismatched to those in the texts. \n\n5.\tThe offset problem needs to be clarified.\nThe authors claim that \u201cThe result in Figure 5.3 implies that offset takes place in 30% - 55% of pixel pairs in \u0394_z_t and \u0394_\u03b5_\u03b8, which means that maximizing J_q pulls \u0394_z_t to a different direction of \u0394_\u03b5_\u03b8 and interferes the maximization of J_p.\u201d Could the authors further explain it and Figure 3 in Section 4.1?\n\n[1] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10684\u201310695, 2022.\n\n[2] Mikhail Konstantinov, Alex Shonenkov, Daria Bakshandaeva, and Ksenia Ivanova. Deepfloyd. https://github.com/deep-floyd/IF, 2023."
                },
                "questions": {
                    "value": "1.\tWill this method perform well in more advanced LDMs like Stable Diffusion v2.1 and DeepFloyd/IF?\n\n2.\tWill the target image affect the effectiveness of this method? Would the authors use other images as targets and test the effectiveness?\n\n3.\tIs that a typo in \u201cIn this tractable form, z\u2032_t and z_t sampled from q_\u03c6(v_t|x\u2032) and q_\u03c6(v_t|x\u2032), respectively\u201d below Equation 10?\n\n\n4.\tHow does maximizing J_q pull \u0394_z_t to a different direction of \u0394_\u03b5_\u03b8 and interfere the maximization of J_p? Could the authors further explain Figure 3 in Section 4.1?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4599/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698827908361,
            "cdate": 1698827908361,
            "tmdate": 1699636438490,
            "mdate": 1699636438490,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pH7kjlRAaT",
                "forum": "yNJEyP4Jv2",
                "replyto": "hgoSKVHZ92",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4599/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4599/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the review"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the insightful review and feel sorry for the late response. We have done a major revision of our paper, removing the main part of our theory and complementing extensive experimental results. Please refer to our official comment. \n\nFor the questions raised in the review, we will respond to them point by point.\n\nWeaknesses:\n\n1.   We add new experiments on cross-model transferability in Section 5.3. It shows that our attacks not only work well on Stable Diffusion 1.4 and Stable Diffusion 2.1 but also enjoy cross-model transferability that attacks whose backbone is SD1.5 work well on SD1.4 and SD2.1. Hence, we believe this can answer the question. As for DeepFloyd/IF, unfortunately we don\u2019t have enough time and resources to deploy it and conduct experiments on it within the rebuttal. However, we\u2019re quite confident about the performance of this method in LDMs, supported by cross-model experiment results. We\u2019ll continue on supporting more LDMs in the future and update the results.\n\n2.  We thank the reviewer for the advice. We have merged Algorithm 1 to original Algorithm 2 to become the new Algorithm 1 in the new version of our paper.\n\n3.  Yes, it does. Our new experiments in Appendix B.1 show that picking natural images as the target image yields trivial performance. Section 3.2 also demonstrates new experiments to show that it would be better to pick images with tight patterns and high contrast as the target image.\n\n4.  We have fixed this problem in the new version of our paper. \n\n5.  We have removed the offset rate in the new version of our paper since we find that it cannot explain empirical observations.\n\nQuestions:\n\nExperiments in Section 5.3 show that it performs well in SD2.1 even with backbone models of SD1.5. Due to the time limit, we do not evaluate it on DeepFloyd IF and SDXL. However, since it shares a similar structure (especially the VAE) and dataset with SD, we believe our attacks should also work well on it.\n\n1.   Answered in the response of Weakness 2.\n\n2.  No, the old version of our paper reformulates the intermediate variable z_t as v_t in the forward process. However, all these parts are removed in the new version.\n\n3.  We remove this theory since we find that some observations cannot be explained by the theory."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4599/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700556049909,
                "cdate": 1700556049909,
                "tmdate": 1700556049909,
                "mdate": 1700556049909,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YgevnTEBBE",
                "forum": "yNJEyP4Jv2",
                "replyto": "pH7kjlRAaT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4599/Reviewer_8G7v"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4599/Reviewer_8G7v"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your response. In addition to the SD family, I think conducting evaluations on more recent LDMs could help to further validate the effectiveness of the proposed method."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4599/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740838737,
                "cdate": 1700740838737,
                "tmdate": 1700740838737,
                "mdate": 1700740838737,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Vkl2nPj24f",
            "forum": "yNJEyP4Jv2",
            "replyto": "yNJEyP4Jv2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4599/Reviewer_FuAb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4599/Reviewer_FuAb"
            ],
            "content": {
                "summary": {
                    "value": "Since diffusion and other advanced generative models have been used to replicate artworks and create fake content, a line work proposes a defense mechanism that adds a kind of adversarial perturbation to the protected images to prevent the adversary from fine-tuning their model on the images. This work proposes a more thorough theoretical formulation of the problem compared to the prior work and relies on this formulation to build an empirically stronger attack."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "### Significance\n\nI believe that this paper addresses an important problem with a widespread impact on both the technical community as well as society at large. The empirical results show a convincing improvement over the prior works on two different fine-tuning methods and two datasets.\n\nI believe that introducing the target image for the adversarial objective and hence conducting a targeted attack instead of an untargeted one have a large effect on the empirical success of the attack."
                },
                "weaknesses": {
                    "value": "### Correctness and clarity of the theoretical results\n\nThe paper formulates an adversarial optimization problem particularly tailored for the latent diffusion models (LDM). The analysis guides the algorithm design to some degree (more on this later). However, due to the lack of clarity and various approximations being introduced without proper justification, the theoretical results become less convincing. I will compile all my questions and concerns from Section 3 and 4 in place:\n\n1. I am not sure what the sum $\\sum_z$ is over in Eq. (3). The expectation is already over $z$ so I am a bit confused about the summation. My guess is that the sum is over all the latent variables in the diffusion process (different $z$\u2019s in different steps). Is this correct?\n2. If my previous understanding is correct, my next question is why should the adversary care about the latent variables in the intermediate steps of the diffusion process instead of, say, the final step of the inverse process before the decoder?\n3. Based on the text, Eq. (3) should be equivalent to $\\mathbb E_{z \\sim p_{\\theta}(z|x)}[- \\log p_\\theta(z|x')]$. My question is that a slightly different formula $\\mathbb E_{z \\sim p_{\\theta}(z|x')}[- \\log p_\\theta(z|x) + \\log p_{\\theta}(z|x')]$ also seems appropriate (swapping order in the KL-divergence). Why should we prefer one to the other?\n4. Section 3.2 uses the notation $\\mathcal N(\\mathcal E(x), \\sigma_\\phi)$ instead of $\\mathcal N(f_{\\mathcal E}(x), \\sigma_{\\mathcal E})$ from Section 2.1. Do they refer to the same quantity?\n5. In the last paragraph of page 4, the Monte Carlo method must be used to estimate the mean of $p_\\theta(z_{t-1}|x)$, but I cannot find where the mean is actually used. It does not seem to appear in Eq. (10) or in Appendix A.1. I also have the same question for the variance of $p_\\theta(z_{t-1}|x)$ mentioned in the first paragraph of page 5.\n6. Related to the previous question, it is mentioned that \u201cthe variance of $z_{t-1}$ is estimated by sampling and optimizing over multiple $z_{t-1}$.\u201d It is very unclear what \u201csampling\u201d and \u201coptimizing\u201d refer to here.\n7. I do not quite see the purpose of Proposition 1. It acts as either a definition or an assumption to me. The last sentence \u201cone can sample $x \\sim w(x)$ from $p_{\\theta(x)}(x)$\u201d is also very unclear. Is the assumption that the true distribution is exactly the same as the distribution of outputs of the fine-tuned LDM?\n8. $x^{(eval)}$ is mentioned in Section 3.4 but was never defined.\n9. In Eq. (11), should both of the $\\theta(x)$\u2019s be $\\theta(x')$ instead? Otherwise, $x'$ has no effect on the fine-tuning process of the LDM.\n10. Section 4.1 is very convoluted (see details below).\n\n### Issues with the offset problem and Section 4.1\n\n**Comment #1**: I do not largely understand the purpose of the \u201coffset\u201d problem in Section 4.1. In my understanding, most of the discussion around the offset can be concluded by simply expanding the second term on the first line of Eq. (13):\n\n$$\n\\sum_{t \\ge 1}\\mathbb E_{z_t,z'_t} || \\Delta z_t +  \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha_t}}} \\Delta \\epsilon ||_2^2 \n$$\n\n$$\n= \\sum_{t\\ge 1}\\mathbb E_{z_t,z'_t} ||\\Delta z_t||_2^2 + || \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha_t}}}\\Delta\\epsilon ||_2^2 + \\frac{2\\beta_t}{\\sqrt{1 - \\bar{\\alpha_t}}}\\Delta z_t^\\top\\Delta\\epsilon\n$$\n\nSo the problem that prevents optimizing just the norm of $\\Delta z_t$ and the norm of $\\Delta \\epsilon_\\theta$ directly is the last term in the equation above (the dot product or the cosine similarity). I might be missing something here so please correct me if I\u2019m wrong.\n\n**Comment #2**: It is also unclear to me how the last line of Eq. (13) is reached and what approximation is used.\n\n**Comment #3**: In theory, there is nothing preventing one from optimizing Eq. (13) as is. The issue seems to be empirical, but I cannot find the empirical results showing the failure of optimizing Eq. (13) directly and not using the target trick.\n\n**Comment #4**: The authors \u201clet *offset rate* be the ratio of pixels where the vector $\\Delta z_t$ and $\\Delta \\epsilon_\\theta$ have different signs.\u201d If my understanding of the cosine similarity above is correct, this seems unnecessary and imprecise given that the cosine similarity is the exact way to quantify this.\n\n**Comment #5**: In the first paragraph of page 7, it is mentioned that \u201cmeanwhile, since the original goal is to maximize the mode of the vector sum of\u2026\u201d I think instead of \u201cmode,\u201d it should be \u201cmagnitude\u201d or the Euclidean norm?\n\n### Empirical contribution\n\n1. After inspecting the generated samples in Figure 11-15, my hypothesis is that the major factor contributing to the empirical result is the target pattern and the usage of the targeted attack. The pattern is clearly visible on the generated images when this defense is used, and this pattern hurts the similarity scores. This raises the question of whether the contribution comes from the theoretical formulation and optimization of the three objectives or the target. I would like to see an ablation study on this finding: (1) the proposed optimization + untargeted and (2) the prior attacks + targeted.\n2. The choice of the target $\\mathcal T$ is ambiguous. While the target pattern is shown in the Appendix, there is no justification for why such a pattern is picked over others and whether other patterns have been experimented with.\n\nOverall, I believe that the paper can have a great empirical contribution, but it seems to be clouded by the theoretical analysis which appears much weaker to me."
                },
                "questions": {
                    "value": "1. What are the approximations made on the fourth line of Eq. (22) and in Eq. (23)?\n2. Why are MS-SSIM and CLIP-SIM used as metrics for SDEdit whereas CLIP-IQA score is used for LoRA? The authors allude to this briefly, but it still largely remains unclear to me.\n3. The similarity metrics used in experiments seem to focus on the low-level textured detail rather than the style (please correct if this is not accurate). I am wondering if a better metric is the one that measures \u201cstyle similarity\u201d between the trained and the generated images. This might align better with the artwork and the copyright examples.\n4. For the results reported in Table 1, how many samples or runs are they averaged over? Based on the experiment setup, 100 images are used for training the model in total for each dataset, and they are grouped in a subset of 20. So my understanding is that there are 100/20 = 5 runs where 100 images are generated in each run? Is this correct?\n5. The fine-tuning hyperparameters for LoRA are mentioned in Section 5.1. Does the LoRA fine-tuning during the attack and the testing share the same hyperparameters? What happens when they are different (e.g., the adversary spends iterations during fine-tuning, etc.)? Can the proposed protection generalize?\n6. Have the authors considered any \u201cadaptive attack\u201d where the adversary tries to remove the injected perturbation on the images (e.g., denoising, potentially via another diffusion model) before using them for fine-tuning?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4599/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4599/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4599/Reviewer_FuAb"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4599/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698827987092,
            "cdate": 1698827987092,
            "tmdate": 1699636438359,
            "mdate": 1699636438359,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HaDEU9nrrZ",
                "forum": "yNJEyP4Jv2",
                "replyto": "Vkl2nPj24f",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4599/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4599/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the review"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer for the insightful and detailed review, which greatly helps us in the improvement of the new version of our paper. As stated in our official comment visible to all reviewers, we remove the main part of the theory in the new version of our paper and replace it with the process of how we yield our methods intuitively. We believe this can solve most of the theoretical problems raised by the review. Also, we conduct novel ablation studies on target selection to answer the question about the empirical effectiveness of our method.\n\nFollowing are our point-by-point answers of the questions raised in the review. Although most of the theoretical problems are cleared by removing the old theory from the paper, we still respond to them based on the old version of our paper. We hope the reviewer can give opinions about the new version of our paper, which would be very valuable to us.\n\nCorrectness and clarity of the theoretical results\n\n1. Yes, it is the sum over all latent variables. \n\n2. This is because the training and the sampling process is separate. Existing adversarial attacks usually attack the training process because attacking sampling process is very memory-expensive (imaging doing backprop throughout UNet for 25 or 50 times back to the input). Under this condition, considering the fact that the exact goal is to fail the sampling process and that the sampling process is done by step-by-step sampling of intermediate latent variables, it will be better if the attacker tries to make every intermediate latent variable out-of-distribution. This will interfere with every step of the sampling rather than just the final step. \n\n3. This is an interesting idea. But as our original theory cannot well explain some observations, we would leave the discussion in the future work.\n\n4. Yes, this is a clerical error.\n\n5. We make a clerical error here that the proof should be given in Appendix A.2 of our first version, where $J_p$ is $J_{unet}$ in the Appendix A.2. \n\n6. Sampling means that we sample a z_t from q(z_t|x) and optimizing means that we optimize the function based on $z_{t-1}$ which is sampled from p(z_t|z_{t-1}). This is a Monte Carlo approximation since we sample z_t for multiple times and take every z_t as a fixed value to conditionally sample $z_{t-1}$. \n\n7. This proposition demonstrates what fine-tuning can do currently. However, different fine-tuning methods do it in different ways. For example, LoRA exactly introduces a new condition c and try to make p(x|c)\\approx the true distribution. So we just briefly introduce the goal that one can sample real data sample x from the model.\n\n8. This is a clerical error. $x^{eval}$ should be x from the dataset. We use these x to evaluate how well the fine-tuned model learns the dataset.\n\n9. Yes. It is a clerical error."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4599/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700554861874,
                "cdate": 1700554861874,
                "tmdate": 1700554861874,
                "mdate": 1700554861874,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qqv3kaAxxv",
                "forum": "yNJEyP4Jv2",
                "replyto": "Vkl2nPj24f",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4599/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4599/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the review (Cont.)"
                    },
                    "comment": {
                        "value": "Issues with the offset problem and Section 4.1\n\n1. We agree that the goal can be simply expanded then optimized. That is the main reason for removing our theory. We try to optimize this goal but find that it is empirically weaker than both Eq.16 (named ITA in the new version) and Eq. 15 (named ITA+ in the new version) in the old version of our paper. We do not include the result in the new version of the paper since we remove the introduction of this goal. Our theory cannot explain this phenomenon so that it is removed.\n\n2. The approximation is about merging the first two terms of the middle line in Eq.13. The sum expectation of $\\Vert\\delta z_t\\Vert^2_2$ approximates $\\Vert\\mathcal{E}(x)-\\mathcal{E}(x\u2019)\\Vert^2_2$. However, we rethink the approximation and believe it is not appropriate.\n\n3. We try to optimize this goal and it performs similarly to the goal we discuss in the first point. Since it does not work well and is not related to our revised method, we do not include the result in the new version of our paper.\n\n4. We agree that it works very similarly to cosine similarity and thus being unnecessary.\n\n5. Yes, it does mean magnitude.\n\nEmpirical contribution\n\nWe understand that one major concern is whether the choice of target or our optimization is the key to our results. We admit that this could be an issue due to our lack of related ablation studies in the first version of our paper. Therefore, we add visualization results of choosing different targets and compare our optimization with the target version of Anti-DB. Results show that both the choice of the target and the optimization goal is the key to our strong empirical results. Please refer to Appendix B.1 for more details.\n\n1. We agree that targeted attack is the main contribution, which has now been the main topic of our method. We give out experiments on how the contrast and the pattern repetition of the target image impacts the targeted attack and discuss some empirical experience to pick the target image in the Section 3.2 of our new paper. For the recommended ablation studies, (1) The untargeted version of our attack should be Anti-DB, included in our baselines. We provide quantitative comparison and visualization to compare Anti-DB with our attacks. We also give a visual comparison between the targeted version of Ant-DB(please refer to section 3.1 for detailed definition) and our attacks under the same targets(Please refer to Appendix B.1).  As for (2) the prior attacks + targeted, we conduct a case study on it in Appendix A.2. Empirically, it is much weaker than our proposed attacks.\n\n2. As previously mentioned, Section 3.2 of the new version of our paper now discusses the issues about target selection. Also, Figure 4 shows the visualization results of choosing targets within the domain of natural images. We also give a visualization in Appendix B.3 to give intuitive explanations about the empirical success of our method."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4599/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700554916072,
                "cdate": 1700554916072,
                "tmdate": 1700554937931,
                "mdate": 1700554937931,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4Pgv1qtruF",
                "forum": "yNJEyP4Jv2",
                "replyto": "Vkl2nPj24f",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4599/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4599/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the review (Cont.)"
                    },
                    "comment": {
                        "value": "Questions:\n\n1. Both share the same approximation. We exchange the sequence of the expectation.\n\n2. Answered below.\n\n3. SDEdit conducts minor modification to the image and does not change the structure of the image and only modify details. As shown in Figure 1, all outputs of SDEdit are structurally similar to the original input data. Hence, we use MSSSIM as the structural similarity metric to measure the attack performance. Just as mentioned, we also want to measure the semantic difference (e.g. style difference), thus CLIP-SIM is used to assess the similarity of the images in the latent space of CLIP(trained by aligning text and images). We believe that CLIP-SIM is a good metric to evaluate the high-level semantic distortion of our adversarial attacks. For the same purpose, we use CLIPIQA, a clip-based, non-reference image quality metric to evaluate our text-to-image results. We\u2019ve made a mistake by claiming both CLIP-SIM and MSSSIM both measure the structural differences in our original paper. Thanks for your correction, we\u2019ve updated our explanation of the metrics. Please see \u201cSDEdit & Metrics\u201d under section 5.1 for the updated details.\n\n4. Yes, it is correct.\n\n5. They do not share the exact same hyper-parameters, since LoRA training in the attack only lasts for 5 steps and the LoRA training in evaluation lasts for 1000 steps. Although hyper-parameters of the optimizer stay the same, we do not think the result would be quite different. However, the training prompt of LoRA would strongly affect the final performance, especially when it is different from the prompt used in attacking. Therefore, we provide additional experiments to validate our results. First, we conduct experiments where different prompts are used in LoRA training(Please refer to Appendix B.2). The result  shows a degradation in the strength of the attack, but the semantic distortion on the output image is still strong. Second, we give a case of totally black-box evaluation. This case comes from a user from the art community (permission already guaranteed). He/She uses our attack with our default setups (with noise budget 4/255) to protect a batch of his/her paintings and train LoRA upon these paintings. We do not know the details of his/her LoRA training, as well as the model. We take a guess that the model is not Stable Diffusion since Stable Diffusion cannot fit animate style so well. The result shows that our attack performs fairly in this black-box case. \n\n6. For most common purification methods(e.g. JPEG compression, adding gaussian noise) , we have evaluated them in Section 5.4. Results show our methods are robust to those methods. However, we do not think new potential adaptive purification methods should be discussed in this work. Currently, no specific adaptive purification has been designed to remove the adversarial attacks on LDM. We highly agree with the academic significance of such adaptive purifications. But such purification may help the malicious individual to overcome the adversarial noise and steal copyrighted content from artists and organizations. Since these adversarial attacks on LDM are for social goodness, we do not think releasing such adaptive purification is socially responsible. Such technical attempts should be stopped by rules. Therefore, though we\u2019ve explored some potential methods for pure academic purposes, we would not release them in our paper."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4599/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700554986306,
                "cdate": 1700554986306,
                "tmdate": 1700554986306,
                "mdate": 1700554986306,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]