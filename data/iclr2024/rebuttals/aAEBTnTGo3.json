[
    {
        "title": "JoinGym: An Efficient Query Optimization Environment for Reinforcement Learning"
    },
    {
        "review": {
            "id": "dMnTa8giA4",
            "forum": "aAEBTnTGo3",
            "replyto": "aAEBTnTGo3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6528/Reviewer_jBfS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6528/Reviewer_jBfS"
            ],
            "content": {
                "summary": {
                    "value": "The problem of \"join plan enumeration\" is addressed using the Partially Observable Contextual Markov Decision Process (POCMDP). The authors conducted a comparative evaluation of join order optimization methods that utilize reinforcement learning, using a set of join query plans generated with the proposed technique."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "S1: This is a new benchmark paper for various join order optimization methods that utilize reinforcement learning (RL).\n\nS2: The research on join query plan enumeration itself is novel. Additionally, it is valuable for estimating the cost (CCM) for each query plan."
                },
                "weaknesses": {
                    "value": "W1: The paper will become more valuable by additionally discussing the pros and cons of the compared reinforcement learning (RL) methods and clearly outlining future research challenges.\n\nW2: The proposal enumerates join query plans with its cost model (CCM). Since the proposal utilizes lossy table embedding, it's important to compare the accuracy of the CCM to existing methods, such as Neo (Marcus et al. 2019).\n\nW3: Most parts of the paper heavily rely on knowledge of Markov Decision Process (MDP), which makes it difficult to read. I suggest the authors provide a preliminary overview of MDP to improve readability.\n\nW4: The abstract seems somewhat inconsistent with the main content of the paper. For example, the authors state \"key advantages of JOINGYM are usability and significantly higher throughput,\" but there is no description in the main content that pertains to usability and throughput."
                },
                "questions": {
                    "value": "Q1. How do you estimate c_h (the cardinality of the IR incurred at time h)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6528/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6528/Reviewer_jBfS",
                        "ICLR.cc/2024/Conference/Submission6528/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6528/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698560388116,
            "cdate": 1698560388116,
            "tmdate": 1700705780182,
            "mdate": 1700705780182,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BqfPDCoajZ",
                "forum": "aAEBTnTGo3",
                "replyto": "dMnTa8giA4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6528/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6528/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your valuable feedback and please find our responses below.\n\n**W1.** We've outlined three concrete future research directions as 1) long-tailed returns, 2) generalization in combinatorial optimization and 3) partial observability. Admittedly, our discussion section was cut short due to space constraints. Hence, we will add the following paragraph discussing the pros and cons of compared RL methods:\n\nTo recap, we compared four RL algorithms spanning three categories: off-policy Q-learning (DQN), off-policy actor-critic (TD3 and SAC), and on-policy actor-critic (PPO). Although PPO does not always obtain the best test-set performance, we can see in Tables 5,6 that it reliably obtains decent quantile performance in all settings of JoinGym. Hence, PPO has the advantage of being the most stable and the disadvantage of often plateauing at a sub-optimal solution. On the other hand, off-policy actor-critic methods (TD3 and SAC) perform the best for mean and $99\\%$ quantile, although PPO's CCM is usually within an order of magnitude. Hence, off-policy actor-critic methods are less stable but can sometimes converge to a better solution. DQN and DDQN tend to be uniformly worse off, which suggests that actor critic outperforms Q-learning for this problem. With this said, all of these algorithms are optimizing for the mean of a highly varied, long-tailed distribution of returns; thus, designing better algorithms that can handle long-tailed rewards will likely improve performance across all the algorithms.\n\n**W2 and Q1:** We never estimate cardinalities since JoinGym can simulate *true* cardinalities derived from our novel dataset. Thus, it does not make sense to compare against existing cost models since JoinGym does not require a cost model. Again, this is a key advantage of JoinGym.\n\n**W3.** Please see Section 4.1 for the MDP setup. We kept it short due to space constraints and can certainly point the reader to a longer exposition, e.g. Sutton and Barto, 2018.\n\n**W4.** Please see our discussion on usability and higher throughput in \"Related works: Environments for Query Optimization\" and a longer discussion in Appendix C. We acknowledge that this could have been highlighted better and we will amend our paper with the following paragraph:\n\nJoinGym improves over prior query optimization simulators in two main ways: easier to use and higher throughput. First, JoinGym is easier to use because we do not require the user to set up a DBMS or cost model while prior simulators do. In other words, starting up JoinGym is as simple as starting up CartPole or Mountain Car in OpenAI Gym. Second, JoinGym can achieve a much higher throughput because each simulator step is simply a table lookup from our novel dataset. Prior simulators either execute the query (which can take hours to days for large queries) or estimate the query cost with a cost model (which can still take seconds), both of which are slower and require more infrastructure setup. Indeed, JoinGym can reliably simulate thousands of queries per second on a standard laptop. Moreover, the cost model can have estimation errors, while JoinGym always returns the true cardinality.\n\nThank you for your valuable feedback and please let us know if you have additional questions."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6528/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700172572187,
                "cdate": 1700172572187,
                "tmdate": 1700172572187,
                "mdate": 1700172572187,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4Be7uEVOwz",
                "forum": "aAEBTnTGo3",
                "replyto": "BqfPDCoajZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6528/Reviewer_jBfS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6528/Reviewer_jBfS"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response. According to the response, I raised the score to 5: marginally below the acceptance threshold.\n\nAllow me to provide further clarification (apologies for the delayed response).\n1) Effectiveness of busy plans: Most JOS techniques predominantly focus on left-deep joins, assuming that the search space for bushy plans is extensive and that most of these plans are ineffective. However, it appears the authors regard certain bushy plans as potentially useful (possibly being the best plans), given that experiments were conducted on both left-deep and bushy plans. My question is: what advantage is gained by expanding the search space from solely left-deep plans to include bushy plans? If a substantial number of bushy plans prove to be superior, then broadening the search space to incorporate them would seem reasonable.\n2) The cost of RL: RL incurs significant costs, presenting a trade-off between efficiency and quality. Given JOINGYM's focus on benchmarking RL-based techniques for JOS, could you provide specific practical scenarios where RL-based methods outperform other techniques?\n3) It might be beneficial if the authors referenced recent non-RL works in JOS, like \"Efficiently Computing Join Orders with Heuristic Search,\" presented at SIGMOD2023. This work demonstrated that \"heuristic search finds optimal plans an order of magnitude faster than the current state of the art.\""
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6528/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700706934766,
                "cdate": 1700706934766,
                "tmdate": 1700706934766,
                "mdate": 1700706934766,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dtEvnxbtS0",
                "forum": "aAEBTnTGo3",
                "replyto": "dMnTa8giA4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6528/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6528/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer, thanks very much for replying during the discussion period and raising more valuable questions. Please find our responses below. \n\n**1.** JoinGym includes both left-deep and bushy plans so that it can faithfully simulate different modern DB systems. For example, Oracle only considers left-deep plans, while Postgres also considers bushy plans. Leis et al. (2015) also demonstrate that bushy plans can result in lower costs than the optimal left-deep plans. However, in our experiments, we found that bushy plans are much harder to learn with RL and almost always have worse performance than left-deep policies, likely due to the much larger search space as you pointed out. Nonetheless, we include bushy plan as an option in JoinGym to provide a realistic simulation environment for RL researchers.\n\n**2.** RL approaches can indeed incur costs during training, but the learned RL policies can often suggest better join plans than non-learning approaches at inference time. That is, RL is potentially better at inference while incurring costs at training. In particular, we have **new results** on how well Postgres, a strong non-learning baseline, performs on the JoinGym query suite. We ran the Postgres query optimizer on all 3300 queries of JoinGym and computed the join plans' cumulative cost multiples (CCMs). The results are shown below:\n| Query Set | Mean | p90 | p95 | p99 |\n| --------- | ---- | --- | --- | --- |\n| Train     | 1.72e+06 | 2.60e+03 | 4.92e+04 | 4.52e+06 |\n| Val       | 5.81e+05 | 4.46e+03 | 9.97e+04 | 7.10e+06 |\n| Test      | 6.09e+04 | 5.00e+03 | 3.41e+04 | 2.31e+06 |\n\nBy examining Tables 5 and 6 of the paper, we can see that RL algorithms in the left-deep disable CP JoinGym often yields better results than Postgres. For example, PPO is consistently better for train, validation and test queries. This suggests that these RL algorithms are indeed proposing high-quality joins that are competitive with Postgres.\n\n**3.** Thanks for pointing out (Haffner and Dittrich, 2023), which we'll definitely discuss and cite. (Haffner and Dittrich, 2023) focuses on improving heuristic search algorithms for shaped queries (e.g., star and clique queries), while our paper focuses on providing a valuable realistic simulator for the RL community. We highlight that our work is positioned as a \"dataset and benchmark\" contribution (as marked by the primary area of our submission), where our focus is *not* to directly improve state-of-the-art in DB systems. Rather, our key contribution is a new efficient RL environment that enables anyone with a laptop to rapidly prototype new RL ideas for the query optimization problem, without needing expensive and complex setup with real DB systems. Please also see our global response regarding this point.\n\nThanks again for your valuable feedback and please let us know if you have additional questions. If you are reassured of the value of our work by the rebuttal & discussion, then we\u2019d greatly appreciate if you could please correspondingly increase your scores. Thank you!"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6528/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700721340860,
                "cdate": 1700721340860,
                "tmdate": 1700721357868,
                "mdate": 1700721357868,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DxsCZVQnx1",
            "forum": "aAEBTnTGo3",
            "replyto": "aAEBTnTGo3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6528/Reviewer_2MKf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6528/Reviewer_2MKf"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a simulator for training RL agents to perform the task of DB query optimization. The main issue that the contribution tackles is the cost of running expensive queries on real hardware which can hinder learning efficiency (from a wall-time perspective) when training deep reinforcement learning agents. This becomes even more problematic in the case of query optimization where the search space is exponential and finding fast-executing query plans is NP-hard. The simulator uses precomputed exact cardinality for each of the possible plans in the search space, which offloads the cost of evaluating plan performance (and therefore collecting a reward) to the developers of the simulator rather than the user. The authors release the queries and intermediate relation cardinality estimates which can also be used for learning cardinality estimation models. Many RL algorithms are benchmarked using this RL environment."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Many reinforcement learning algorithms are benchmarked against the proposed simulator and the release of the dataset along with the precomputed IR cardinalities will be of use to the database and QO research communities. The background on database query optimization and traditional QO optimizations (e.g. left-deep vs bushy) also helps contextualize the contribution to the broader machine learning community and is presented well."
                },
                "weaknesses": {
                    "value": "With respect to the analysis of the environment and the cost models associated, the paper does not benchmark against performance of existing RL-for-QO systems, such as Neo (Marcus et. al) which do indeed use expensive execution frameworks. Additionally, more traditional cost models which approximate the IR estimates such as the Postgres cost model should be evaluated to solidify that precomputing the IR estimates manually is necessary for strong performance. Ideally, showing a curve of environment cost vs. learning performance curve would be useful (e.g. does using real execution latencies improve over the exact IR cardinalities which improves over the Postgres cost model?).\n\nThe dataset for learning supervised cardinality estimators is not benchmarked against other methods for learning cardinality estimators (both supervised and unsupervised) and should be benchmarked to better contextualize the contribution for the broader ML and systems community as to the datasets potential impact with respect to training estimators."
                },
                "questions": {
                    "value": "Installing postgres and querying the cost model is not particularly expensive or difficult and could be packaged into software such as an OpenAI gym environment. How does the performance compare when using the postgres cost model? What about using more advanced cardinality estimators, such as NeuroCard (Yang et al) as the cost model for the JoinGym simulator?\n\nWhat is the performance on JOB-Ext (Marcus et al, available https://github.com/RyanMarcus/imdb_pg_dataset/tree/master/job_extended) which are out-of-domain queries and not from the original JOB templates? \n\nHow does the method generalize to new schemas?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6528/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6528/Reviewer_2MKf",
                        "ICLR.cc/2024/Conference/Submission6528/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6528/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698815058545,
            "cdate": 1698815058545,
            "tmdate": 1700684946736,
            "mdate": 1700684946736,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EYnakLybba",
                "forum": "aAEBTnTGo3",
                "replyto": "DxsCZVQnx1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6528/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6528/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your valuable feedback and please find our responses below.\n\n**W1a.** As you said, Neo (Marcus et al., 2019) indeed uses expensive execution frameworks, whereas our work focuses on lightweight simulation and so our benchmark results are not directly comparable with Neo's. Our emphasis is to provide a lightweight simulator for the RL community rather than to directly improve DB systems. Please also see our global response regarding this point.\n\n**W1b.** Thanks for suggesting Postgres. We have new results on how well Postgres, a strong non-learning baseline, performs on the JoinGym query suite. We ran the Postgres query optimizer on all 3300 queries of JoinGym and computed the join plans' cumulative cost multiples (CCMs). The results are shown below:\n| Query Set | Mean | p90 | p95 | p99 |\n| --------- | ---- | --- | --- | --- |\n| Train     | 1.72e+06 | 2.60e+03 | 4.92e+04 | 4.52e+06 |\n| Val       | 5.81e+05 | 4.46e+03 | 9.97e+04 | 7.10e+06 |\n| Test      | 6.09e+04 | 5.00e+03 | 3.41e+04 | 2.31e+06 |\n\nBy examining Tables 5 and 6 of the paper, we can see that RL algorithms in the left-deep disable CP JoinGym often yields better results than Postgres. For example, PPO is consistently better for train, validation and test queries. This suggests that these RL algorithms are indeed proposing high-quality joins that are competitive with Postgres.\n\n**W2.** Our work focuses on providing a RL environment for query optimization. We believe our dataset can be useful for cardinality estimation research, but since our focus is RL, we leave supervised learning benchmarks as promising future work.\n\n**Q1a.** Please see **W1b** for our new results on Postgres's performance on JoinGym.\n\n**Q1b.** Thanks for suggesting Neurocard, which we will definitely discuss and cite. Since JoinGym already returns true cardinalities, using NeuroCard as the cost model for JoinGym does not make sense because it would only introduce estimation error and slow-down the simulation throughput. \n\n**Q2.** Our experiments are already run on a large dataset of $3300$ queries with $660$ as the test set. JOB-Ext consists of $24$ queries and is not a standard benchmark for query optimization, so adding these queries would not add much to JoinGym.\n\n**Q3.** This is a great question. If you are wondering whether it is possible to use JoinGym to simulate another schema or database, this is definitely possible -- all one needs to do is to collect a new cardinality dataset which is straightforward but computationally expensive. However, if you are wondering whether it is possible to transfer an RL policy trained on IMDb to another schema or database, this is a much harder problem. Even with same schema and fixed database, our experiments show that there is much to be improved along the three directions of (1) long-tailed returns, (2) generalization in discrete problems, and (3) partial observability. Extending JoinGym to transfer between schemas is certainly promising future work.\n\nThanks again for your valuable feedback and please let us know if you have additional questions."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6528/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700452441857,
                "cdate": 1700452441857,
                "tmdate": 1700452441857,
                "mdate": 1700452441857,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "v4UxFnBWel",
                "forum": "aAEBTnTGo3",
                "replyto": "EYnakLybba",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6528/Reviewer_2MKf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6528/Reviewer_2MKf"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the additional experiments and for the thorough response. The positioning of this work as a novel environment for RL research makes sense and the additional Postgres ablation is helpful for understanding the method. I have adjusted my score accordingly."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6528/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700684903685,
                "cdate": 1700684903685,
                "tmdate": 1700684903685,
                "mdate": 1700684903685,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IvaxqCQ3v0",
            "forum": "aAEBTnTGo3",
            "replyto": "aAEBTnTGo3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6528/Reviewer_QAMY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6528/Reviewer_QAMY"
            ],
            "content": {
                "summary": {
                    "value": "The work is related to applying reinforcement learning to decide how a join query (a query that cross-references multiple tables) can be executed most efficiently by a database system which is an NP-hard problem where existing solutions still leave a lot of room for improvement. The paper proposes a simulator/benchmark to evaluate the performance of reinforcement learning approaches and features an extensive empirical study that compares some existing approaches."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "S1) Originality: Thorough investigation of existing reinforcement learning approaches and useful tool/benchmark for future works.\n\nS2) Significance: Reinforcement learning is a reasonable approach for the join order problem and shedding light on what works best can contribute towards faster join execution not just in general, but also for particular data domains / systems.\n\nS3) Presentation: The paper presents ideas clearly, particularly figures seem to illuminate concepts well and the literature review appears to be thorough."
                },
                "weaknesses": {
                    "value": "W1) Significance: The conclusion of compared reinforcement learning approaches could be clearer in terms of which kind of predictions it enables. A non-learning baseline would also help to root the results.\n\nW2) Originality: The work does not seem to (explicitly) propose any novel approach.\n\nW3) Relevance: The focus of the paper seems to be less on reinforcement learning and more about a particular application of reinforcement learning in database management systems."
                },
                "questions": {
                    "value": "Q1) What can be learned about the performance of different reinforcement learning approaches in this benchmark and how does it compare to the performance of a non-learning approach (as used in modern commercial systems)?\n\nQ2) Do the any of the results conclude any new approach or variation of an approach that has not been previously considered?\n\nQ3) What kind of progress does this work make in terms of reinforcement learning approaches (e.g., via trial and error) beyond measuring their empirical performance?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6528/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6528/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6528/Reviewer_QAMY"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6528/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698835651764,
            "cdate": 1698835651764,
            "tmdate": 1699636733948,
            "mdate": 1699636733948,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vDce9toJHy",
                "forum": "aAEBTnTGo3",
                "replyto": "IvaxqCQ3v0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6528/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6528/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your valuable feedback and please find our responses below.\n\n**W1 and Q1.** Regarding what can be learned about RL approaches in the benchmark, we mentioned the most salient points in the discussion part of Section 5. To add more details, we'll amend it with the following paragraph, comparing pros and cons of compared RL methods for the query optimization task:\n\nTo recap, we compared four RL algorithms spanning three categories: off-policy Q-learning (DQN), off-policy actor-critic (TD3 and SAC), and on-policy actor-critic (PPO). Although PPO does not always obtain the best test-set performance, we can see in Tables 5,6 that it reliably obtains decent quantile performance in all settings of JoinGym. Hence, PPO has the advantage of being the most stable and the disadvantage of often plateauing at a sub-optimal solution. On the other hand, off-policy actor-critic methods (TD3 and SAC) perform the best for mean and \n quantile, although PPO's CCM is usually within an order of magnitude. Hence, off-policy actor-critic methods are less stable but can sometimes converge to a better solution. DQN and DDQN tend to be uniformly worse off, which suggests that actor critic outperforms Q-learning for this problem. With this said, all of these algorithms are optimizing for the mean of a highly varied, long-tailed distribution of returns; thus, designing better algorithms that can handle long-tailed rewards will likely improve performance across all the algorithms.\n \nRegarding comparison with a non-learning approach: we have new results on how well Postgres, a strong non-learning baseline, performs on the JoinGym query suite. We ran the Postgres query optimizer on all 3300 queries of JoinGym and computed the join plans' cumulative cost multiples (CCMs). The results are shown below:\n| Query Set | Mean | p90 | p95 | p99 |\n| --------- | ---- | --- | --- | --- |\n| Train     | 1.72e+06 | 2.60e+03 | 4.92e+04 | 4.52e+06\n| Val       | 5.81e+05 | 4.46e+03 | 9.97e+04 | 7.10e+06\n| Test      | 6.09e+04 | 5.00e+03 | 3.41e+04 | 2.31e+06\n\nBy examining Tables 5 and 6 of the paper, we can see that RL algorithms in the left-deep disable CP JoinGym often yields better results than Postgres. For example, PPO is consistently better for train, validation and test queries. This suggests that these RL algorithms are indeed proposing high-quality joins that are competitive with Postgres.\n\n**W2 and Q2.** Our contribution is a novel RL environment JoinGym and extensive benchmarking of existing RL algorithms. As a submission for the \"dataset and benchmarks\" primary area, our goal is to provide a RL environment to accelerate and facilitate future algorithm research.\n\n**W3 and Q3.** We believe our work will be valuable to the RL community for the following reasons. First, as mentioned in the previous point, our new efficient environment will enable anyone with a laptop to rapidly prototype new RL ideas for the query optimization problem, without needing expensive and complex setup with real DB systems. Second, in Section 5 of the paper, we have identified three clear research directions for RL: (1) dealing with long-tailed returns, (2) generalization in discrete optimization, and (3) partial observability. Please also see our global response regarding this point.\n\nThanks again for your valuable feedback and please let us know if you have additional questions."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6528/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700452365129,
                "cdate": 1700452365129,
                "tmdate": 1700452469354,
                "mdate": 1700452469354,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RNIjR12hsR",
                "forum": "aAEBTnTGo3",
                "replyto": "vDce9toJHy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6528/Reviewer_QAMY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6528/Reviewer_QAMY"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much that clears up the questions I had! Postgres apparently does an exhaustive search for smaller problem instances and genetic algorithms for larger join queries (https://www.postgresql.org/docs/current/geqo-pg-intro.html), which confirms that the optimisation problem is quite hard (making it an interesting target to learn more about RL) and could benefit from fresh perspectives (making RL an interesting approach for the problem)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6528/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700684095551,
                "cdate": 1700684095551,
                "tmdate": 1700684095551,
                "mdate": 1700684095551,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FNo95De2zU",
            "forum": "aAEBTnTGo3",
            "replyto": "aAEBTnTGo3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6528/Reviewer_wp5V"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6528/Reviewer_wp5V"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a lightweight query optimization environment for reinforcement learning and releases a cardinality dataset based on IMDB."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.This paper focus on an important problem for query optimization."
                },
                "weaknesses": {
                    "value": "1. The title of the article is very strange and difficult to understand. The \u2018query optimization\u2019 is the background of this paper. The title may be \u201cAn efficient reinforcement learning environment for query optimization\u201d instead of \u201cAn efficient query optimization environment for reinforcement learning\u201d?\n2. Do the \u2018simulator\u2019 and \u2018environment\u2019 refer to the same thing? Authors mention that \u2018our aim is to provide a lightweight yet realistic simulator\u2019 and \u2018looking up the size of intermediate tables from join sequences\u2019. The size or called cardinality estimation is a popular research filed in query optimization. They should compare their method to the existing query size simulators as follows.\n[1] Sun, J., & Li, G. (n.d.). An End-to-End Learning-based Cost Estimator. VLDB, 2020. Kipf [2] A, Kipf T, Radke B, et al. Learned cardinalities: Estimating correlated joins with deep learning. CIDR, 2019. \n[3] Yang, Z., Kamsetty, A., Luan, S., Liang, E., Duan, Y., Chen, X., & Stoica, I. (2020). Neurocard: One cardinality estimator for all tables. Proceedings of the VLDB Endowment, 14(1), 61\u201373, 2020. \n[4] Ziniu Wu, Parimarjan Negi, Mohammad Alizadeh, Tim Kraska, Samuel Madden. FactorJoin: A New Cardinality Estimation Framework for Join Queries. SIGMOD, 2023\n3. Many statements of this paper are incorrect. For example, authors mention \u2018runtime metrics are system-dependent and can only be obtained from live query executions\u2019. In fact, there exists many popular works to simulate the query size and cost [1,2,3,4].\n4. This article needs to be greatly improved in algorithm, experiment and writing before it can be accepted."
                },
                "questions": {
                    "value": "Shown in weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6528/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699000965964,
            "cdate": 1699000965964,
            "tmdate": 1699636733830,
            "mdate": 1699636733830,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bDzRAzd1IZ",
                "forum": "aAEBTnTGo3",
                "replyto": "FNo95De2zU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6528/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6528/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your valuable feedback and please find our responses below.\n\n**Q1.** Our title highlights that JoinGym is designed for the RL community, rather than to improve query optimization directly. Up until now, query optimization has not been easy for RL researchers to benchmark due to large overhead in setting up a real DBMS. Our work removes this barrier.  With JoinGym, we hope to motivate research from diverse ML/RL communities to create better algorithms for query optimization. Please also see our global response regarding this point.\n\n**Q2.** Yes, \"simulator\" is the RL environment. Thanks for bringing up prior works on cardinality estimation, which we will cite and discuss -- our work is orthogonal to [1-4] because we are using *true cardinalities* instead of estimates from cost models (which can have errors). Moreover, [1-4] do not provide a RL environment for query optimization, which is our focus and main contribution. \n\n**Q3.** By runtime metrics, we meant actual latency and wall-clock runtime of joins, which are system-dependent. Our point here is that cardinality does not depend on which machine or system the query is run on, which is one benefit to using cardinality as the cost.\n\n**Q4.** This work is positioned as a \"dataset and benchmark\" contribution, which we've indicated by selecting this primary area. Our goal with JoinGym is not to create new algorithms but rather to provide a valuable simulator for the RL community.  We emphasize that our work provides a novel and valuable \"dataset and benchmark\" for the RL community. \n\nThanks again for your valuable feedback and please let us know if you have additional questions."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6528/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700172498141,
                "cdate": 1700172498141,
                "tmdate": 1700172498141,
                "mdate": 1700172498141,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]