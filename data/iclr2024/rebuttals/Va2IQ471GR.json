[
    {
        "title": "Convergence of SVGD in KL divergence via approximate gradient flow"
    },
    {
        "review": {
            "id": "j6ngPwOrJu",
            "forum": "Va2IQ471GR",
            "replyto": "Va2IQ471GR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5426/Reviewer_t4TL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5426/Reviewer_t4TL"
            ],
            "content": {
                "summary": {
                    "value": "This article introduces a new technique, \\((\\epsilon, \\delta)\\)-approximate gradient flow, to prove the convergence of SVGD under KL-divergence."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The convergence of SVGD under KL-divergence has always been an open question, making it a topic of great interest for theoretical researchers."
                },
                "weaknesses": {
                    "value": "Assumption 6 is too strong, essentially assuming the properties of the $\\nabla\\log  p_t $ gradient directly. It determines the upper and lower bound for the time-dependent gradient and approximation, which almost brings the convergence."
                },
                "questions": {
                    "value": "For Theorem 1, $ c_0 $ could be the most crucial constant describing the system's properties. Further explanation from the authors is hoped for.\n\nRegarding the discrete algorithm, the $ \\nabla \\log p_t $ term is obviously affected by iterations, which is considered the most significant challenge. The article seems to circumvent this issue with Assumption 6, necessitating more justifications.\n\nHow to explain the non-monotonic decrease of KL in Figure 2?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5426/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698277269463,
            "cdate": 1698277269463,
            "tmdate": 1699636551398,
            "mdate": 1699636551398,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lUj8WDHSCZ",
                "forum": "Va2IQ471GR",
                "replyto": "j6ngPwOrJu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5426/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5426/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response"
                    },
                    "comment": {
                        "value": "## **Answer to weakness points and questions**\n\n- **For weakness and the 2nd question:** We agree that Assumption 6 is strong, which is one of the limitations of our study.\nIn the present study, this assumption was adopted because the priority is to first provide a convergence guarantee of SVGD in KL divergence. Pursuing convergence guarantees grounded in milder assumptions represents a crucial avenue for future research. We also believe that, even with such a strong assumption, our results will encourage research on guaranteed convergence under more realistic assumptions by making them available to the public.\n\n- **To the 1st question:** Regrettably, our AGF-based analysis only informs us of the existence of a time-independent constant $c_{0}$. \nA more intricate analysis is requisite to ascertain the qualitative properties of this constant. However, such an investigation exceeds the scope of this paper, as our primary focus is to ensure sub-linear convergence in KL divergence.\nNonetheless, based on your advice, we will consider exploring the detailed nature of this constant for a comprehensive understanding of SVGD convergence in our future work.\n\n- **To the 3rd question:**  As suggested in works like [Liu et al. (2018)], there is a possibility of exhibiting oscillatory behavior around the fixed point in the setting with finite particles. We hypothesize that a similar phenomenon may have occurred in our experiment. In the SVGD context, much is still unknown about the convergence of SVGD with finite particles in the KL divergence, partly because this paper is the first proof for the infinite particle setting.\n\n## Reference:\n\n[Liu & Wang (2018)]: Q. Liu and D. Wang. Stein Variational Gradient Descent as Moment Matching. In Advances in Neural Information Processing Systems, volume 31, pp. 8868\u20138877, 2018."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5426/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700316887170,
                "cdate": 1700316887170,
                "tmdate": 1700316887170,
                "mdate": 1700316887170,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5Oz08v1qod",
            "forum": "Va2IQ471GR",
            "replyto": "Va2IQ471GR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5426/Reviewer_hQF6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5426/Reviewer_hQF6"
            ],
            "content": {
                "summary": {
                    "value": "The authors construct a novel framework AGF to analyze convergence of SVGD in KL divergence. Based on this framework, the authors show SVGD can converge sublinearly in KL divergence."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The AGF framework can include the most existing analysis of approximate function gradient flow. It is also promising to get better results of SVGD based on this framework. Numerical experiments seem to support the theoretical results."
                },
                "weaknesses": {
                    "value": "1. The key assumption is not clear and hard to verify. And I think there are some mistakes in the proof (see Question). \n2. The authors do not really use AGF framework in the analysis of SVGD since $\\\\delta$ is always 0.\n3. Basically, the method is to prove that $I\\_{stein} (\\\\mu\\_t | \\\\pi) \\\\geq c\\_0 \\\\|\\\\nabla \\\\log\\\\frac{\\\\mu\\_t}{\\\\pi}\\\\|\\^2_{L^2(\\\\mu\\_t)}$ for some positive constant $c$ and apply LSI. The contribution is not significant enough as an ICLR paper."
                },
                "questions": {
                    "value": "1. In Assumption 6, $\\\\{\\lambda_i\\\\}$ have positive lower bounds independent of $t$. However, Assumption 5 implies the kernel has finite trace and $\\sum_i \\lambda_i< \\infty$. This will contradict Assumption 6. How should Assumption 6 be understood here?\n2. In the proof of Theorem 1, the authors claim that \"m satisfying Eq. (65) does not monotonically increase w.r.t. the iteration t\" and \"This allows us to identify a largest m as m\u2032 that does not depend on t\". I don't think the first claim can imply the second one since m can be arbitrarily large even if not monotonically increasing.\n3. If it indeed holds that $I\\_{stein} (\\\\mu\\_t | \\\\pi) \\\\geq c\\_0 \\\\|\\\\nabla \\\\log\\\\frac{\\\\mu\\_t}{\\\\pi}\\\\|\\^2_{L^2(\\\\mu\\_t)}$, then by LSI, $I\\_{stein} (\\\\mu\\_t | \\\\pi) \\\\geq c KL(\\mu_t | \\pi)$ for some positive constant $c$. This seems just eq (7), which is not valid in most practical cases by Duncan et al. (2023). What is the difference between the authors'claims and Duncan et al. from this perspective?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5426/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698309343518,
            "cdate": 1698309343518,
            "tmdate": 1699636551220,
            "mdate": 1699636551220,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZojRt68FfW",
                "forum": "Va2IQ471GR",
                "replyto": "5Oz08v1qod",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5426/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5426/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response"
                    },
                    "comment": {
                        "value": "## **Answer to weakness points and questions**\n\n- **For weaknesses 2-3, and question 3:** \n  - The assertion that $\\delta=0$ in our AGF is not readily evident from existing knowledge, such as LSI. Instead, it is a theoretical result that becomes apparent through analyzing the gradient flow of SVGD under our AGF, employing spectral decomposition and others. To commence, as [Duncan et al. (2023)] noted, conducting convergence analysis with (stein) LSI on RKHS has proven challenging. In this context, we have shown that we can evaluate convergence through \"LSI-like\" inequalities by demonstrating that SVGD, under the infinite particle setting, achieves $\\delta=0$ in our introduced AGF. Utilizing this finding, we show the sub-linear convergence of SVGD in KL divergence. All our theoretical analysis results are non-trivial and can be attained by introducing our AGF rather than relying solely on conventional LSI.\n  - Further discussion is needed to elucidate the connection between Eq. (7) outlined in [Duncan et al. (2023)] and our findings.\nIndeed, our results are tied to Eq. (7); however, [Duncan et al. (2023)] assert that satisfying Eq. (7) for **ANY** distribution $\\mu$ is challenging.\nWhile speculative, the distribution $\\mu$ may satisfy such inequality within a context that aligns with the assumptions discussed in our paper.\nThe demonstration by [Liu et al. (2023)] of linear convergence in the KL divergence for SVGD, with the constrained assumption that the target distribution is Gaussian, may imply that Eq. (7) might be valid under specific and restricted conditions.\n\n- **For question 1 (related to weakness 1):**  \n  - Assumptions 5 and 6 do not contradict. Assumption 6 states that each eigenvalue $\\lambda_{i}$ (for all $i$) has a positive upper and lower bound **independent of $t$**.\nLet us focus on the RBF kernel $k_{\\mathrm{RBF}}$ for simplicity.\nIn this case, it is known that $\\lambda_{i} = \\mathcal{O}(e^{-i \\log i})$, thereby establishing $\\lambda_{i} = \\Omega(e^{-i \\log i})$ (refer to Example 12.25 in [Wainwright, 2019]). This fact verifies the fulfillment of Assumption 6, as $\\lambda_{i}$ is a constant order w.r.t. $t$.\nAdditionally, given that the RBF kernel satisfies all conditions of Mercer's theorem (refer to Section 12.3 in [Wainwright, 2019]), we can employ the Mercer expansion for $k_{\\mathrm{RBF}}$ to derive $k_{\\mathrm{RBF}}(x,x) = \\sum_{i=1}^{\\infty} \\lambda_{i}\\phi_{i}^{2} < \\infty$. This observation confirms the validity of Assumption 5.\n\n- **For question 2 (related to weakness 1):**  Thank you for taking the time to peruse the proofs. This concern is similar to reviewer Spa3's Q.2, so please see our response to it (please see p.20 of the modified version of this paper).\n\n## Reference:\n\n[Duncan et al. (2023)]: A. Duncan, N. N \u0308usken, and L. Szpruch. On the geometry of Stein variational gradient descent.\nJournal of Machine Learning Research, 24(56):1\u201339, 2023.\n\n[Liu et al. (2023)]: T. Liu, P. Ghosal, K. Balasubramanian, and N. S. Pillai. Towards understanding the dynamics of Gaussian-Stein variational gradient descent. arXiv preprint arXiv:2305.14076, 2023.\n\n[Wainwright, 2019]: M. J. Wainwright. High-Dimensional Statistics: A Non-Asymptotic Viewpoint. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2019."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5426/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700316439562,
                "cdate": 1700316439562,
                "tmdate": 1700316439562,
                "mdate": 1700316439562,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "35gI0CsS73",
                "forum": "Va2IQ471GR",
                "replyto": "ZojRt68FfW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5426/Reviewer_hQF6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5426/Reviewer_hQF6"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response and clarification of assumption 6. I would like to keep my score since AGF with $\\delta_t=0$ together with LSI is stronger than equation 7, and it is not clear when this condition would be satisfied."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5426/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700714363671,
                "cdate": 1700714363671,
                "tmdate": 1700714363671,
                "mdate": 1700714363671,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pPtjAMEd9n",
            "forum": "Va2IQ471GR",
            "replyto": "Va2IQ471GR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5426/Reviewer_Spa3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5426/Reviewer_Spa3"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the convergence of the Stein variational gradient decent(SVGD) in the population limit. Assuming the target density satisfies the log-Sobolev inequality(LSI) and other assumptions on the kernel, the authors show the sub-linear convergence of SVGD population limit in KL-divergence, which is the first KL-divergence convergence result of SVGD population limit assuming LSI. Last, numerical experiments, employing RBF kernel, are provided in Section 5 and Appendix D to verify the sublinear convergence result."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. It is a paper with enough novelty. It combines existing results in SVGD and RKHS and prove the first convergence result of SVGD population limit in KL-divergence assuming LSI. \n2. The idea and proof techniques introduced in the paper is useful for analysis on other sampling algorithms. The idea of approximate gradient flow could be used in a large class of sampling algorithms based on Wasserstein gradient flow. The proof techniques related to RKHS could be used to study other variants of SVGD as well."
                },
                "weaknesses": {
                    "value": "1. The convergence result in this paper doesn't apply to the SVGD algorithm. I understand that the convergence of the finite-particle SVGD in KL-divergence is an open problem. But I am wondering that whether the idea and results in this paper can help to understand convergence of finite-particle SVGD."
                },
                "questions": {
                    "value": "Questions:\n1. In the first paragraph on page 1, what does the sentence ``While MCMC guarantees asymptotically obtaining unbiassed samples from the target distribution, it suffers from inefficiency due to the randomness'' mean?\n2. The proof argument between $(66)$ and $(67)$ is essential, but it is a little hard to understand. I have the following questions related to the proof:\n\n     (1) why is it enough to demonstrate that $m$ satisfying $(65)$ doesn't monotonically increase w.r.t. $t$? Even if $m$ is not monotonically increase w.r.t. $t$, $m$ could go to infinity as $t$ increases. Therefore, I think the argument should be ``it is enough to demonstrate that $m$ satisfying $(65)$ doesn't monotonically increase w.r.t. any subsequence of $t$''.\n\n     (2) I don't understand the argument `` If $m$ were to monotonically increase, the coefficient of $(b_i^{(l)})^2$ with larger indices would also increase''. How do we prove it from $(65)$?\nIt would be great if a more detailed proof could be added.\n\nComments:\n\n1. Although this paper focuses on convergence results for the infinite-particle SVGD, convergence results for finite-particle SVGD should be introduced in Section 1. For related results, I refer to the following two papers:\n\n          [1] Shi, Jiaxin, and Lester Mackey. \"A finite-particle convergence rate for stein variational gradient descent.\" arXiv preprint arXiv:2211.09721 (2022).\n\n          [2] Liu, Tianle, et al. \"Towards Understanding the Dynamics of Gaussian--Stein Variational Gradient Descent.\" arXiv preprint arXiv:2305.14076 (2023)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5426/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5426/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5426/Reviewer_Spa3"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5426/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698535159534,
            "cdate": 1698535159534,
            "tmdate": 1699636551122,
            "mdate": 1699636551122,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8YUzTEUo1x",
                "forum": "Va2IQ471GR",
                "replyto": "pPtjAMEd9n",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5426/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5426/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response"
                    },
                    "comment": {
                        "value": "## **For weakness**\nYour diligence in raising this crucial point is appreciated. To find our responses, kindly consult our official comments addressed to all reviewers. We have integrated a discussion of this specific matter into the newly introduced \"Limitation & Conclusion\" section.\n\n## **Answers to questions**\n- **To the 1st question:** We apologize for the unclear explanation. What we would like to explain here is that MCMC tends to be computationally intensive. We modified this sentence as follows: \n  - \u201c**While MCMC provides guarantees of producing asymptotically unbiased samples from the target density, it tends to be computationally intensive (Robert & Casella, 2004).**\u201d\n\n- **To the 2nd question:** We agree with your opinion. We modified some sentences and explanations. It would be too complicated to describe the corrections here, so please see p.20 of the modified version of this paper.\n\n## **Answers to comment**\nThank you for pointing that out. We agree that [1] and [2] should have been cited. We will include the following sentences with the references of these papers in the third and fourth paragraphs of Sec. 1 for both papers:\n  - \u2026like gradient descent. **Beyond the infinite particle setting, Shi & Mackey (2023) also showed that the SVGD with $n$ finite particles and an appropriate step size converges at the $\\mathcal{O}(1/\\sqrt{ \\log \\log n })$ order if the target distribution is sub-Gaussian with a Lipschitz score...**\n\n- \u2026function of SVGD. **Recently, Liu et al. (2023) showed that SVGD with finite particles achieves linear convergence in KL divergence under a very limited setting where the target distribution is Gaussian. However, the analytical approach presented in previous studies makes it difficult to conduct convergence analysis based on KL divergence in a more global setting.** The reason\u2026"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5426/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700315816561,
                "cdate": 1700315816561,
                "tmdate": 1700315816561,
                "mdate": 1700315816561,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cuJ6w1xN1u",
                "forum": "Va2IQ471GR",
                "replyto": "pPtjAMEd9n",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5426/Reviewer_Spa3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5426/Reviewer_Spa3"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the responses. The proofs in the revised version look good to me now. I have the following a minor comment on the Assumption 6:\n\n\n1. In [He et al., 2022]: He, Y., Balasubramanian, K., Sriperumbudur, B.K. and Lu, J., 2022. Regularized Stein variational gradient flow. arXiv preprint arXiv:2211.07861. Convergence of R-SVGD, which degenerates to SVGD when the parameter is 0, in KL-divergence was studied under assumptions of the RKHS kernel as well. Is it possible to compare their assumptions and Assumption 6 in this paper? This may provide more insights to finding the optimal kernels in SVGD."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5426/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700607587626,
                "cdate": 1700607587626,
                "tmdate": 1700608168465,
                "mdate": 1700608168465,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Ltfm8ti8cZ",
            "forum": "Va2IQ471GR",
            "replyto": "Va2IQ471GR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5426/Reviewer_ZqiW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5426/Reviewer_ZqiW"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on the convergence of Stein Variational Gradient Descent (SVGD) for the task of density matching when learning unknown probability densities. Specifically, the paper investigates different convergence criteria other than the Kernel Stein Discrepancy (KSD), which holds under highly restrictive assumptions, thus making it not applicable in practice. The novel criterion uses the KL divergence, the objective of SVGD, as an alternative to KSD. The core idea of the paper is to think of SVGD as an approximation of the gradient flow in the space of probability distributions. This approximation is introduced in the paper as the $(\\epsilon-\\delta)$ approximate gradient flow. With their new theoretical framework, the authors show that SVGD exhibits sub-linear convergence in the KL divergence.\n\nThe paper is well-written and scientifically sound. It provides a good level of preliminaries to follow the story of the paper and is complemented by some numerical results. The mathematical details are well-presented and structured. The authors often introduce and explain every step, referring to the appendix when necessary. I found the paper enjoyable to read.\n\nHowever, on a more critical note, I found the paper sometimes hard to parse, perhaps due to my lack of familiarity with the topic of optimal transport at such a deep level. In general, I find the derivation reasonable and mainly easy to follow. My main issue with the current submission, however, concerns the numerical experiments (not so informative) and the take-away message one has to gather from those results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is well written. It extensively reviews previous works and discusses how the present work fits the existing literature.\n- It provides some interesting theoretical insights about the convergence of SVGD in a more relaxed setting compared to the very restrictive assumptions required by the KSD convergence analysis.\n- The authors demonstrate the sublinear convergence of SVGD (Theorem 1), which represents the main result of this work."
                },
                "weaknesses": {
                    "value": "- **Outlined proof is hard to grasp:** I found the proof outline (section 4.2) particularly dense and hard to follow. Perhaps having a more intuitive discussion and providing a more qualitative intuition would help people grasp the basic idea.\n- **Broader impact and applicability:** I think the paper misses a discussion about the practical impact of a given study. More specifically, I\u2019d be interested to understand how the results introduced in this work could be used when dealing with sampling tasks and approximating unnormalized target densities.\n- **Conclusions are missing:** The paper does not have a conclusion section where the main findings of the work are summarized. I find this to be a substantial lack of the paper as it prevents the reader from drawing the necessary conclusion from the study.\n- **Lack of numerical experiments:** While I appreciate the mathematical rigor and details of the manuscript, I perceive the numerical experiments sections (Sec. 5) to be lacking. The results seem consistent with the theory, but as far as I can tell, these are not sufficient to envision any substantial benefit from the framework presented in this work in a practical setting.\n- **Take-away message from numerical experiments:** From the current version, I find it hard to understand what the central message of the numerical experiments should be."
                },
                "questions": {
                    "value": "- In the introduction, the second paragraph introduces SVGD as a method to compensate for the high variance and bias from MCMC and VI methods. I am naively wondering how this would compare to other debiasing schemes often used in the context of VI, such as neural importance sampling [1]. It would be helpful if the authors could elaborate a bit on this.\n\n- The paper always deals with the RBF kernel because it satisfies all the assumptions required for the theory to hold. However, I am wondering how easy it would be to apply the results presented in this paper to other widely adopted kernels which may not straightforwardly satisfy assumptions 1-6. Could the authors comment on this?\n\n- I understand the paper deals with the case of infinite particles for most of the discussion. This is relevant from a theoretical standpoint, although it cannot be achieved in practice. When it comes to the final discussion, where numerical experiments are shown, the number of particles is indeed considered to be finite, which prevents the bias $\\delta_t$ from vanishing. Moreover, the authors correctly state that the regime of an infinite (or even high) number of particles is practically not feasible from a computational perspective. In light of this, I wonder whether the residual bias one has to expect, resulting from the finite number of particles, can be estimated. Expanding the discussion around this point in the manuscript, I think, would be helpful. \n\n[1] [M\u00fcller, Thomas, et al. \"Neural importance sampling.\" ACM Transactions on Graphics (ToG) 38.5 (2019): 1-19.](https://arxiv.org/abs/1808.03856)\n\nMinor:\n\n- On page 3: The concept of the Radon-Nikodym derivative probably deserves a few more words. As I believe this paper could be interesting for a broad range of people who are not necessarily familiar with measure theory, I think introducing the R-N derivative with a few words or some references (perhaps just a footnote) would be helpful.\n\n- First paragraph of page 7: \u201chas shown the linear convergence in a continuous-time setting, the kernel function employed in **this** study is designed\u201d. In this sentence, \"this\" refers to the work of Huang et al. However, the sentence may confuse the reader as it may sound that **this study** refers to the present paper. I\u2019d recommend the authors reword this part slightly to avoid any potential confusion.\n\n- Last paragraph before section 4.2 \u201c[\u2026] sampling methods based on ker [\u2026]\u201d. I suspect *ker* might be a typo. If not, what do the authors mean by that?\n\n- How to obtain equation (13) from equation (9) is not trivial for me, even after looking at the appendix. Perhaps adding some intuition about it would be helpful to clarify this step, as I believe it represents one of the important results of the paper.\n\n- Below equation (16) the authors say \u201c[\u2026] we focus on the RKHS associated with $k$ given as $\\mathcal{H}=\\{\u2026}$\u201d. In the curly brackets, there\u2019s a sum over k, though none of the variables in the sum actually have a subscript k. Is this meant to be a sum over $i$ instead?\n\n- Section 4.3: kernelsatisfies -> kernel satisfies\n\n- I found the labels in Figure 1 to be too small.\n\n- It would be helpful to have the y-axis sharing the same range for the 2 left-most and 2 right-most plots of Figure 1. The same applies for the first and second rows of Figure 2. This would make it easier to compare visually."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5426/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5426/Reviewer_ZqiW",
                        "ICLR.cc/2024/Conference/Submission5426/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5426/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698684979808,
            "cdate": 1698684979808,
            "tmdate": 1700661878550,
            "mdate": 1700661878550,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "14OhteaVbw",
                "forum": "Va2IQ471GR",
                "replyto": "Ltfm8ti8cZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5426/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5426/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response"
                    },
                    "comment": {
                        "value": "## **For weakness**\n\n- **About outlined proof:** We believe that proof outlines, as in Section 4.2, constitute a vital aspect of this paper and should be articulated rigorously, even if their presentation is somewhat dense. What kind of intuitive or qualitative explanation would facilitate a more precise understanding? We would greatly appreciate it if you could provide an example of the type of explanation you would find helpful, even if it is based on personal opinion.\n- **Broader impact and applicability:** Our study revealed that the sample obtained by SVGD converges to the target density in KL divergence, albeit exclusively in the infinite particle setting. This suggests weak convergence of SVGD with infinite particles, affirming its capability to approximate the expectation by the target distribution without bias, akin to MCMC. \nThis finding could enhance users' confidence in the effectiveness and reliability of SVGD.\nIn our future work, we aim to offer theoretical guarantees for the convergence of SVGD with finite particles in KL divergence, as outlined in the official comments. These discussions and official comments are summarized in a new section titled \"Limitations and Conclusions.\"\n- **About the conclusion section:** We agree with your opinion. We have introduced a new section titled \"Limitations and Conclusions,\" specifically addressing the content outlined in the above official comment. Please see the revised version of our paper.\n- **About lack of numerical experiments and take-away message from numerical experiments:** Our study analyzes SVGD in the infinite particle setting, acknowledging its unimplementability as an initial step towards furnishing a realistic convergence guarantee for SVGD. To corroborate the validity of the analytical results derived from the infinite particle setup through numerical experiments with finite particles, it is essential to formulate an experimental design capable of completion within a realistic computation time, even when employing a huge number of particles to simulate a near-infinite particle setup. To attain this goal, employing simple examples like the log-concave distribution experiment proves useful. We believe that the discussion of numerical experiments on convergence in more practical experimental settings will constitute a vital element in our forthcoming study on ensuring the convergence of SVGD in the KL divergence within the finite particle setting detailed in our official comment.\n\n## **Answers to questions**\n- **To the 1st question:** MCMC faces scalability issues in handling big data, while VI efficiently approximates posterior distributions, transforming Bayesian inference into a deterministic or stochastic optimization problem. However, the accuracy of VI is influenced by the choice of the set of variational distributions, introducing bias when approximating complex target distributions with simpler ones. To reduce this bias, methods based on importance sampling have been studied; however, they pose the additional challenge that their effectiveness depends on the accuracy of the composition of the proposal distribution (e.g., $q(x; \\theta)$ constructed by the neural network in [1]). In this context, SVGD was developed as a more efficient and less biased algorithm that eliminates the need for the bias-reduction methods described above by allowing the removal of constraints imposed by variational distribution sets (refer to Sec. 1 in [Liu et al., 2016] for details). To clarify this point, we modified the 1st and 2nd paragraphs of Sec. 1 as follows:\n  - In the 1st paragraph: \u201c... **While MCMC provides guarantees of producing asymptotically unbiased samples from the target density, it tends to be computationally intensive (Robert & Casella, 2004). On the other hand, VI achieves a computationally efficient approximation of the target distribution through stochastic optimization under a simpler alternative distribution; however, it does not come with a guarantee of obtaining unbiased samples (Blei et al., 2017).**\u201d\n  - In the top of the 2nd paragraph: \u201c**To alleviate such sample bias while maintaining computational efficiency of VI as much as possible, Liu & Wang (2016) introduced Stein variational gradient descent (SVGD), which allows the direct approximation of the target distribution without the need for alternative distributions.**\u201d\n\n- **To the 2nd question:** Since the RBF kernel is the most standard one employed in SVGD, many studies have adopted this kernel in the theoretical or empirical analysis (e.g., [Korba et al., 2020], [Salim et al., 2022], and [Shi et al., 2023]). We followed this context as well. In other kernels employed in SVGD, the IMQ kernel ([Shi & Mackey, 2023] focused on this kernel, for example) may be able to satisfy these assumptions.\n\n(Our responses follow in the next thread.)"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5426/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700314088008,
                "cdate": 1700314088008,
                "tmdate": 1700314929865,
                "mdate": 1700314929865,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SZKj7NM1bC",
                "forum": "Va2IQ471GR",
                "replyto": "Ltfm8ti8cZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5426/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5426/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response (2)"
                    },
                    "comment": {
                        "value": "## **Answers to questions (2)**\n- **To the 3rd question:** Thank you for bringing up this important point. \nFor our responses, please refer to our official comments to all reviewers.\nWe have incorporated a discussion of this particular point into the newly added \"Limitation & Conclusion\" section.\n\n## **For MISC**\nThank you for your careful reading. We have modified almost all the matters you pointed out. Please see the revised version of our paper.\nBy labels in Fig. 1, do you mean the labels for the y-axis values in the third figure from the left? We don't consider the values between $10^{0}$ and $10^{-1}$ essential since it is sufficient to understand that the KSD value converges to smaller values as they approach the infinite particle setting.\nConcerning the y-axis range, it's important to note that KSD and KL values are incomparable. These plots aim to assess whether KSD and KL achieve sub-linear convergence in a finite particle setting. Adjusting both scales to match could result in a figure that obscures this behavior. We believe the current state provides the most intuitive means to examine sub-linear convergence.\n\n## Reference\n\n[Liu et al., 2016]: Q. Liu and D. Wang. Stein variational gradient descent: A general purpose Bayesian inference\nalgorithm. In Advances in Neural Information Processing Systems, volume 29, pp. 2378\u20132386, 2016.\n\n[Korba et al., 2020]: A. Korba, A. Salim, M. Arbel, G. Luise, and A. Gretton. A non-asymptotic analysis for Stein\nvariational gradient descent. Advances in Neural Information Processing Systems, 33:4672\u20134682, 2020.\n\n[Salim et al., 2022]: A. Salim, L. Sun, and P. Richtarik. A convergence theory for SVGD in the population limit under\nTalagrand\u2019s inequality T1. In Proceedings of the 39th International Conference on Machine Learning, volume 162, pp. 19139\u201319152, 2022.\n\n[Shi et al.,, 2023]: T. Liu, P. Ghosal, K. Balasubramanian, and N. S. Pillai. Towards understanding the dynamics of Gaussian-Stein variational gradient descent. arXiv preprint arXiv:2305.14076, 2023.\n\n[Shi & Mackey, 2023]: J. Shi and L. Mackey. A finite-particle convergence rate for Stein variational gradient descent. arXiv\npreprint arXiv:2211.09721, 2023."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5426/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700314791954,
                "cdate": 1700314791954,
                "tmdate": 1700314898295,
                "mdate": 1700314898295,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nURkj6T3ta",
                "forum": "Va2IQ471GR",
                "replyto": "Ltfm8ti8cZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5426/Reviewer_ZqiW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5426/Reviewer_ZqiW"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Authors"
                    },
                    "comment": {
                        "value": "I've read the updated version of the manuscript. I acknowledge the effort of the authors in addressing all my concerns. \nFor this reason, I decided to increase my score. \n\nI think the paper is much clearer now. Unfortunately, I got very sick over the past five days and did not come up with a satisfying idea for an outlined proof. Nevertheless, I believe the paper already benefitted from the improvements suggested by the referees. \n\nAs far as the labels in Fig. 1 are concerned, I meant *all* the labels were a bit too small, in my opinion. I think they may be hard to read for the general public without zooming in. I understand that the values between $10^0$ and $10^{-1}$ are not relevant for the analysis, though this was not my main concern."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5426/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700661825657,
                "cdate": 1700661825657,
                "tmdate": 1700661861014,
                "mdate": 1700661861014,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]