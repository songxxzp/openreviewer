[
    {
        "title": "Efficient Meshy Neural Fields for Animatable Human Avatars"
    },
    {
        "review": {
            "id": "RHXdH3Dqlz",
            "forum": "QhoehDVFeJ",
            "replyto": "QhoehDVFeJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7093/Reviewer_QZey"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7093/Reviewer_QZey"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a novel method for reconstructing animatable human avatars from videos. The proposed method, named EMA, models canonical shapes, materials, lights and motions separately using different neural fields. With an analysis by synthesis framework, those terms can be optimized using image level losses via a differentiable marching tetrahedra algorithm. A mesh-based representation can be distilled from this representation, which greatly improves its rendering efficiency. Extensive comparisons are conducted with previous methods. Improved results are observed on the H36M dataset and comparable results are observed on ZJUMOCAP."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* Extensive experimental results.\n\nThis paper conducted many experiments with comparisons with previous baselines and detailed analysis."
                },
                "weaknesses": {
                    "value": "* Better illustrations are needed.\n\nMany illustrative figures in this manuscript lack proper notation and are hard to align with the text. It would make the reader understand better about the technical details better if more details were included."
                },
                "questions": {
                    "value": "* Novelty?\n\nThe proposed method combines existing methods [1] and [2] (and many other papers in the field). Although one of the major differences is the efficiency in rendering, the same improvement can also be theoretically achieved by training a SDF and extracting a water-tight mesh for rendering afterward. It would be great to have a more thorough discussion on the merit of the current pipeline as well as other potential advantages it brings.\n\n\n\n\n\t\t\t\t\n\t\t\t\n\t\t\n\n\n* Comparison with other efficient frameworks?\n\nOn the efficiency side, the joint optimization of mesh and SDF is interesting. However, there are also many other sparse structures used for speeding up the rendering efficiency like using layered mesh representation [] and sparse volumes [3, 4]. It would enhance the quality of the manuscript if some comparisons or discussions were added.\n\n\n[1] Jacob Munkberg, Wenzheng Chen, Jon Hasselgren, Alex Evans, Tianchang Shen, Thomas Mu \u0308ller, Jun Gao, and Sanja Fidler. Extracting triangular 3d models, materials, and lighting from images. In CVPR, pp. 8270\u20138280, 2022. \n[2] Chung-Yi Weng, Brian Curless, Pratul P. Srinivasan, Jonathan T. Barron, and Ira Kemelmacher- Shlizerman. Humannerf: Free-viewpoint rendering of moving people from monocular video. In CVPR, pp. 16189\u201316199, 2022. \n[3] Tianjian Jiang, Xu Chen, Jie Song, and Otmar Hilliges. \"Instantavatar: Learning avatars from monocular video in 60 seconds.\" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16922-16932. 2023.\n[4] Edoardo Remelli, Timur Bagautdinov, Shunsuke Saito, Chenglei Wu, Tomas Simon, Shih-En Wei, Kaiwen Guo et al. \"Drivable volumetric avatars using texel-aligned features.\" In ACM SIGGRAPH 2022 Conference Proceedings, pp. 1-9. 2022.\t\t\n[5] Donglai Xiang, Fabian Prada, Timur Bagautdinov, Weipeng Xu, Yuan Dong, He Wen, Jessica Hodgins, and Chenglei Wu. \"Modeling clothing as a separate layer for an animatable human avatar.\" ACM Transactions on Graphics (TOG) 40, no. 6 (2021): 1-15."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7093/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697828218802,
            "cdate": 1697828218802,
            "tmdate": 1699636836927,
            "mdate": 1699636836927,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RLDa8bDC6G",
                "forum": "QhoehDVFeJ",
                "replyto": "RHXdH3Dqlz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7093/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7093/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Reviewer QZey"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the valuable and constructive feedback.\n\n\n**[Q1]** Illustration of the model.\n\nWe have revised the figure accordingly, including annotations to help clarify our design. The revised version has been updated.\n\n**[Q2]** Novelty.\n\n[1] is designed for static object reconstruction. Our method extends it to modeling dynamic humans. One insight is that, given the mesh extraction framework from [1], we can directly learn the forward LBS skinning without iterative and (potentially) ambiguous root-finding [3].\nWhile [2] uses learned backward skinning which is insufficient for animation and faithful reconstruction of the motion (We include a figure in the updated supp. to demonstrate the problems).\nTo the best of our knowledge, this work is the first to explore the pipeline of differentiable textured mesh + motions (forward LBS skinning + non-rigids) and demonstrate its merits like fast training, quick inference, mesh representation, and template-free property.\n\n**[Q3]** Extracting mesh for NeRF-based methods.\n\nEvery time a new pose is given, the NeRF-based methods have to predict new density and color fields for mesh extraction, which is inefficient. One can only extract the canonical mesh, yet the skinning weights are either inaccessible for backward-skinning ones or maybe (partially) wrong after discretization, and the non-rigid motions may be lost or degraded due to discretization. Besides, the threshold and resolution of the marching cube algorithm are tricky to tweak for both good shape and appearance quality. To summarize, the post-processing of NeRFs leads to either quality degradation or loss of features (like non-rigid).\n\nAdditionally, we can deem our method as a mesh-, or quantization-aware framework for modeling non-rigid. Moreover, the learned motion dynamics not only help inference rendering but also facilitate the training as a result of better inter-frame correspondences, leading to better quality.\n\n**[Q4]** More comparisons.\n\nInstantAvatar [3]  is designed for learning a neural field from a fixed-posed human-rotating monocular video or a synthetic monocular video without loose clothes, and it is unknown that it can handle non-rigid dynamics with large poses from multi-view datasets. We also do not find any qualitative results from InstantAvatar for extensive comparison. Furthermore, our rendering speed (100 FPS) is faster than InstantAvatar (15 FPS).\n\nAs for [4] and [5], they are very exciting industrial-standard research works that push the frontier of human digitization. However, the evaluation setting of [4] is unclear and [4] unfortunately does not conduct extensive comparisons of the benchmarks. The code of [5] is unavailable.\n\nUltimately, we have referred to the related literature and uploaded the newer version of the pdf.\n\n\n[1] Jacob Munkberg, Wenzheng Chen, Jon Hasselgren, Alex Evans, Tianchang Shen, Thomas Mu \u0308ller, Jun Gao, and Sanja Fidler. Extracting triangular 3d models, materials, and lighting from images. In CVPR, pp. 8270\u20138280, 2022. \n\n[2] Chung-Yi Weng, Brian Curless, Pratul P. Srinivasan, Jonathan T. Barron, and Ira Kemelmacher- Shlizerman. Humannerf: Free-viewpoint rendering of moving people from monocular video. In CVPR, pp. 16189\u201316199, 2022. \n\n[3] Tianjian Jiang, Xu Chen, Jie Song, and Otmar Hilliges. \"Instantavatar: Learning avatars from monocular video in 60 seconds.\" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16922-16932. 2023. \n\n[4] Edoardo Remelli, Timur Bagautdinov, Shunsuke Saito, Chenglei Wu, Tomas Simon, Shih-En Wei, Kaiwen Guo et al. \"Drivable volumetric avatars using texel-aligned features.\" In ACM SIGGRAPH 2022 Conference Proceedings, pp. 1-9. 2022. \n\n[5] Donglai Xiang, Fabian Prada, Timur Bagautdinov, Weipeng Xu, Yuan Dong, He Wen, Jessica Hodgins, and Chenglei Wu. \"Modeling clothing as a separate layer for an animatable human avatar.\" ACM Transactions on Graphics (TOG) 40, no. 6 (2021): 1-15."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7093/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700636433363,
                "cdate": 1700636433363,
                "tmdate": 1700700168968,
                "mdate": 1700700168968,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9WTUqztKUE",
                "forum": "QhoehDVFeJ",
                "replyto": "RHXdH3Dqlz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7093/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7093/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to your feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer QZey,\n\nThanks again for your valuable advice and supportive comments! We have responded to your initial comments. We are looking forward to your feedback and will be happy to answer any further questions you may have."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7093/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700700565526,
                "cdate": 1700700565526,
                "tmdate": 1700700565526,
                "mdate": 1700700565526,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WjJFQvxYE6",
            "forum": "QhoehDVFeJ",
            "replyto": "QhoehDVFeJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7093/Reviewer_imbQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7093/Reviewer_imbQ"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a method for learning articulable human avatar models from image data. This is accomplished by modeling the representation as a signed distance function and jointly optimizing the canonical shape, material and lights (for appearance), and skinning weights (for motion). Because the representation is modeled as a signed distance function, it can be extracted into a textured mesh and animated with learned skinning weights in order to efficiently render the human in new poses. The result quality is compared to a number of baseline works, including those optimizing textured meshes directly, and those training volumetric representations. It is demonstrated that the proposed method leads to better quality, along with the ability to change lighting and train significantly more efficiently."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "In my opinion, the strengths of the method are as follows:\n1. The paper is described clearly, and the method makes intuitive sense. Optimizing the representation's geometry (SDF) and appearance (materials and lighting), and motion (skinning weights) jointly seems like the correct approach to learn an avatar from only image data. The optimization objective proposed makes sense and seems well-posed for solving this under-constrained task.\n2. The proposed retains additional control over various factors which is unlike the existing methods. For example, volumetric methods do not model materials and lighting and thus cannot support relighting. This allows the proposed method to be applied in different applications which are not possible for other methods."
                },
                "weaknesses": {
                    "value": "In my opinion, the weaknesses of the method are as follows:\n1. I view the comparisons as not being extensive. For example, this concurrent work seems to solve the same problem [1], and they compare to other baselines which appear better than the ones used in this method. Additionally, there exist a number of methods which use the SMPL mesh for skinning weights driving motion [2][3] for which the quality appears quite good. Why are these approaches not compared to in this work, as it appears that these methods are able to generate better results qualitatively? If this method is focusing on human body avatars, why learn general skinning weights with a skeleton instead of using those from a known human body model, such as SMPL.\n2. For the efficient training, I'm not sure why the representation actually trains faster? It seems to use the entire image as opposed to individual rays as in volume rendering, and marching to find the surface also requires a number of samples of the SDF representation. This results in each iteration having more rays, and just as many samples per ray, so I don't understand why the method actually trains faster.\n\n[1] https://lukas.uzolas.com/Articulated-Point-NeRF/\n\n[2] https://machinelearning.apple.com/research/neural-human-radiance-field\n\n[3] https://tijiang13.github.io/InstantAvatar/"
                },
                "questions": {
                    "value": "I have no additional questions on the manuscript. Overall, the paper proposes a method which makes sense and learns human avatars which have capabilities that other volumetric methods are not capable of, such as relighting, efficient rendering and training, and compatibility with graphics pipeline. However, I do not understand why the existing state-of-the-art methods in generating avatars have not been compared to. If it is because they use the SMPL template for driving the motion of the human avatars, I don't view this to be a limitation, since this paper also focuses on humans. Understanding why these methods were not included in the comparisons, or adding them as comparisons, would significantly strengthen the paper and lead me to increase my score.\n\n**Update after author response**\n\nAfter reading the author response, I still remain borderline on the paper. I understand not comparing to methods which are designed for reconstructing from a monocular video, and appreciate the addition of some comparisons here. Additionally, I appreciate the additional timing results. I have thus increased my score a bit for the paper. \n\nHowever, the justification for not using the SMPL model does not seem convincing to me. If the reason is non-rigid deformations such as clothing, then it needs to be explicitly demonstrated in the paper that this is improved by the proposed method. Additionally, other contributions (such as the correction MLP from Neuman) give the ability to model these types of clothing deformations with the SMPL weights."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7093/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7093/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7093/Reviewer_imbQ"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7093/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698646897152,
            "cdate": 1698646897152,
            "tmdate": 1701024058028,
            "mdate": 1701024058028,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tCCmEQC6l6",
                "forum": "QhoehDVFeJ",
                "replyto": "WjJFQvxYE6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7093/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7093/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Reviewer imbQ"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive and informative review.\n\n\n**[Q1]** More Comparison.\n\nOur method focuses on human body avatars, which is different from the aim of Articulated Point NeRF [1]. So we do not compare the general dynamic NeRF baselines in Articulated Point NeRF.\n\nBoth Neuman [2] and InstantAvatar [3] focus on monocular video reconstruction, and do not compare themself to the standard multi-view video benchmarks like H36M and ZJU-Mocap. So directly comparing with their methods is unfair. \n\nNeuman [2] is designed for learning human NeRF and scene NeRF from a single-view video, and it only learns a frame-dependent error-correction network for observation space. So quantitative comparison is unfair. We give a qualitative comparison of ZJU-Mocap in the updated supplementary. Our results preserve more details than Neuman.\n\nInstantAvatar [3]  is designed for learning a neural field from a fixed-posed human-rotating video or a synthetic video without loose clothes, and it is unknown that it can handle non-rigid dynamics with large poses from multi-view datasets. We also do not find any qualitative results from InstantAvatar for extensive comparison. Furthermore, our rendering speed (100 FPS) is faster than InstantAvatar (15 FPS).\n\n\nUltimately, we have referred to the related literature and uploaded the newer version of the pdf.\n\n\n\n**[Q2]** Using existing skinning templates or not.\n\nIt depends. We learn geometry, appearance, and motions (skinning and non-rigid) to provide a **template-free** solution to avatar reconstruction. The problem with using templates is that:\n\n1. The templates are usually proprietary in real-world usage.\n2. There exists a gap between the skinning field of clothed humans and naked humans. Our skinning field can fill the gap via optimization.\n3. Templates are object-specific (e.g., mature humans), which hinders their wider application to other kinds of objects.\n\nHowever, there are other factors to be considered. For example, if there are limited variety of motions in the training data, the quality of the skinning field may be poor due lack of inter-frame regularization. Besides, Leveraging templates is a good choice to initialize or regularize the skinning field and speed up convergence [2,4].\n\n\n\n\n**[Q3]** Efficiency breakdown.\n\n\nThe efficiency of the method comes from two-fold: Both geometry and rendering. We provide a runtime breakdown and analyze the efficiency.\nThe runtime breakdown:\n   \n| Function                 | Time (ms)               |\n|---------------------|---------------------|\n| Extract Mesh (w/ geo. NF query, w/ non-rigid NF query)      | 11.39273 |\n| Extract Mesh (w/o geo. NF query, w/ non-rigid NF query)       | 3.08209  |\n| Render Mesh (w/ texture NF query, w/ env light query)   | 7.00726  |\n| \u2514\u2500\u2500  (texture NF query)      | 4.08976  |\n\n- \"NF\" means Neural Fields.\n- Extract Mesh: For NF query in mesh extraction, we query both the canonical SDF field and the non-rigid motion field.\n    -  In inference time, we only query the neural field once to get the canonical mesh and use the same mesh for the latter rendering. The motion field will be queried by the number of vertex of the mesh times.\n    -  In training time, we need to query the NF for every optimization step to update the SDF and non-rigid fields.\n    -  The insight is that, compared with NeRFs, our gradients only flow through the iso-surface, which is drastically less than NeRF, which where the gradients flow over the whole space.\n- Render Mesh: \n    - For rendering an NxN resolution image, we only query **O(NxN)** times for the texture field thanks to the rasterization. While NeRF-based method needs to query **O(NxNxM)**, where M is the number of points per pixel (ray). The usage of tiny-cuda-nn, a highly optimized package for neural fields, further improves the speeds.\n    - It involves both texture querying and env-light map querying.\n\n\n\n[1] https://lukas.uzolas.com/Articulated-Point-NeRF/ (https://lukas.uzolas.com/Articulated-Point-NeRF/)\n\n[2] https://machinelearning.apple.com/research/neural-human-radiance-field (https://machinelearning.apple.com/research/neural-human-radiance-field)\n\n[3] https://tijiang13.github.io/InstantAvatar/ (https://tijiang13.github.io/InstantAvatar/)\n\n[4] https://github.com/taconite/arah-release(https://github.com/taconite/arah-release)"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7093/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700636361714,
                "cdate": 1700636361714,
                "tmdate": 1700700156726,
                "mdate": 1700700156726,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VnDyzW4rfZ",
                "forum": "QhoehDVFeJ",
                "replyto": "WjJFQvxYE6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7093/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7093/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to your feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer imbQ,\n\nThanks again for your valuable advice and supportive comments! We have responded to your initial comments. Please feel free to let us know if you have any further questions."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7093/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700700503355,
                "cdate": 1700700503355,
                "tmdate": 1700700503355,
                "mdate": 1700700503355,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PmAfgyquHM",
            "forum": "QhoehDVFeJ",
            "replyto": "QhoehDVFeJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7093/Reviewer_PTA7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7093/Reviewer_PTA7"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a new framework for modeling dynamic human avatars with neural fileds. In this work, three different group of neural fields are adopted to record shape, material and motion information respectively and optimized jointly. Specifically, this work employs an 8-layer MLP to model the SDF of the canonical shapes. A 2-layer MLP with hash-encoding is adopted for efficient material query. SNARF is introduced for skinning weights, and another 4-layer MLP is used for non-rigid modeling. Finally, these neural fields are integrated under the Linear Skining framework and further rendered by a differentiable renderer to fit target images. Experimental results demonstrate that this new framework can achieve superior rendering quality with less training and inference times."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ The proposed method is technically sound. It is clever to integrating LBS model and PBR materials within Neural Fields for high-quality rendering.\n\n+ Employing environment lights and non-rigid models is also a good way to enhance the rendering results.\n\n+ The experimental results are convincing. This work achieves better results with less training and inference time."
                },
                "weaknesses": {
                    "value": "- It is difficult to read the main maniscript without supplementary materials. For example, crucial information such as Nerf structure should be included in the main maniscript.\n\n- This work would be further strengthened if more in-the-wild results were provided."
                },
                "questions": {
                    "value": "Here are some concerns:\n\n1. As the framework takes into account environmental lights and the non-rigid model, how does this method perform on in-the-wild data?\n\n2. Can this method be used for avatar modeling with soft cloth (e.g. wearing a dress)?\n\n3. What is the geometric precision of this method?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7093/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698771556438,
            "cdate": 1698771556438,
            "tmdate": 1699636836681,
            "mdate": 1699636836681,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JIqzsUVm4k",
                "forum": "QhoehDVFeJ",
                "replyto": "PmAfgyquHM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7093/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7093/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Reviewer PTA7"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the positive and constructive feedback.\n\n\n**[Q1]** Illustration of the model.\n\nWe have revised the figure accordingly, including annotations to help clarify our design. The revised version has been updated.\n\n\n**[Q2]** In-the-wild data.\n\nThe challenge of the in-the-wild data are 1) inaccurate pose tracking and parsing, and 2) inaccurate global coordinate system.\n\nWe use datasets that are in the controlled environment, as this paper focuses on building a pipeline that enjoys fast training, quick inference, mesh representation, template-free (which can be extended for a variety of objects and avoid proprietary models), and feedforward LBS for training. \n\nRecently, [1] proposed an in-the-wild dataset. We will explore the in-the-wild scenario in the future.\n\n\n**[Q3]** Soft clothes.\n\nYes, our method can be applied to soft clothes thanks to the non-rigid modeling and the template-free property. In our supp. video, there are avatars with loose clothes in clip 01:06 - 01:24 (or \"ema.supp.representation_visualization.mp4\" in supp). The mesh visualization offers a better view. Admittedly, modeling clothes dynamics like dress is a hard research problem requiring further research.\n\n\n**[Q4]** Geometry quality.\n\nIn supplementary materials, Sec. H, Mesh visualization, we qualitatively visualize the canonical meshes. Note that the number of faces for each mesh is quite small. Though increasing the resolution of tetrahedra grids may improve the details of both geometry and materials, we do not conduct this experiment for it is orthogonal to our technical contributions.\n\n\n[1] Kaufmann, M., Song, J., Guo, C., Shen, K., Jiang, T., Tang, C., Z\u00e1rate, J.J. and Hilliges, O., 2023. EMDB: The Electromagnetic Database of Global 3D Human Pose and Shape in the Wild. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 14632-14643)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7093/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700636307213,
                "cdate": 1700636307213,
                "tmdate": 1700700139741,
                "mdate": 1700700139741,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AjOx3rJimP",
                "forum": "QhoehDVFeJ",
                "replyto": "PmAfgyquHM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7093/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7093/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to your feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer PTA7,\n\nThanks again for your valuable advice and supportive comments! We have responded to your initial comments. We are looking forward to your feedback and will be happy to answer any further questions you may have."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7093/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700700594236,
                "cdate": 1700700594236,
                "tmdate": 1700700606654,
                "mdate": 1700700606654,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HQgIETQgjZ",
            "forum": "QhoehDVFeJ",
            "replyto": "QhoehDVFeJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7093/Reviewer_XfaE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7093/Reviewer_XfaE"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method called EMA (Efficient Meshy neural fields for Animatable human Avatars) for efficiently generating animatable human avatars from videos. The main goal is to overcome the shortcomings of existing volume rendering-based methods in terms of training and inference speed and to achieve compatibility with rasterization renderers for direct application to downstream tasks.The EMA method jointly optimizes explicit triangular canonical mesh, spatially varying materials, and motion dynamics through end-to-end inverse rendering. These components are encoded by separate neural fields, eliminating the need for preset human templates, rigging, or UV coordinates. The authors also use differentiable rasterization techniques to learn mesh properties and forward skinning, improving the efficiency of the method.Compared to existing methods, the EMA method has significant advantages in training and inference speed. It is highly compatible with rasterization renderers, has a short training time, and fast rendering speed. Experimental results show that the EMA method has competitive performance in novel view synthesis, generalization to novel poses, and training time and inference speed."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The EMA method achieves real-time rendering through efficient mesh rendering. Moreover, it computes the loss on the entire image, and gradients flow only on the mesh surface, resulting in improved training speed. Compared to volume rendering methods, the EMA method has a shorter training time."
                },
                "weaknesses": {
                    "value": "1.\tThe EMA method employs neural networks to encode canonical geometry, materials, and motion models. In practical applications, the complexity of these neural networks may affect the inference speed. Have the authors conducted experiments in this regard, or attempted to use simpler or more efficient neural network architectures to balance the inference speed and reconstruction quality?\n2.\tThe EMA method learns a fixed environment light and uses a Physically-Based Rendering (PBR) material model. In practical applications, would this lighting and material modeling approach potentially limit the inference speed to some extent? Is it possible to use simpler lighting and material models to further improve efficiency?\n3.\tThe EMA method employs pose-dependent non-rigid offsets to compensate for non-rigid cloth dynamics. Does this modeling approach increase computational complexity and impact inference speed? Are there any other more efficient methods to handle non-rigid cloth dynamics? Judging from the video results, it seems that EMA might also be limited in terms of non-rigid cloth dynamic modeling?\n4.\tThe EMA method relies on skeletal pose tracking of the input video. So is it conceivable that the accuracy of pose tracking might affect inference speed and result quality? Has the author considered the performance and speed of the method in the case of inaccurate pose tracking, I think this will be of great help in practical applications?\n5.\tIn the experimental part, it can be seen that in the case of fast training (10 minutes), EMA has a satisfactory result (better than ARAH). However, under the same hour, the advantage seems not obvious. And as I said before, when the pose tracking quality is poor, the EMA is also greatly affected.\n6.\tIn addition, are there other methods that also use mixed representation training methods? The improvements brought by this method have not been well explained and proven in experiments."
                },
                "questions": {
                    "value": "As stated above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7093/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698848196835,
            "cdate": 1698848196835,
            "tmdate": 1699636836579,
            "mdate": 1699636836579,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rj3tZwywB4",
                "forum": "QhoehDVFeJ",
                "replyto": "HQgIETQgjZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7093/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7093/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Reviewer XfaE (Part 1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the detailed and constructive reviews.\n\n**[Q1,Q2,Q3]** The efficiency of each module.\n\nWe provide a runtime breakdown and analyze the efficiency.\n\nThe runtime breakdown:\n   \n| Function                 | Time (ms)               |\n|---------------------|---------------------|\n| Extract Mesh (w/ geo. NF query, w/ non-rigid NF query)      | 11.39273 |\n| Extract Mesh (w/o geo. NF query, w/ non-rigid NF query)       | 3.08209  |\n| Render Mesh (w/ texture NF query, w/ env light query)   | 7.00726  |\n| \u2514\u2500\u2500  (texture NF query)      | 4.08976  |\n\n- \"NF\" means Neural Fields.\n- Extract Mesh: For NF query in mesh extraction, we query both the canonical SDF field and the non-rigid motion field.\n    -  In inference time, we only query the neural field once to get the canonical mesh and use the same mesh for the latter rendering. The motion field will be queried by the number of vertex of the mesh times.\n    -  In training time, we need to query the NF for every optimization step to update the SDF and non-rigid fields.\n- Render Mesh: \n    - For rendering an NxN resolution image, we only query O(NxN) times for the texture field thanks to the rasterization. While NeRF-based methods need to query O(NxNxM), where M is the number of points per pixel (ray). The usage of tiny-cuda-nn [4], a highly optimized package for neural fields, further improves the speeds.\n    - It involves both texture querying and env-light map querying.\n\n\n**[Q1]** Efficiency wrt. NNs for geometry, appearance, and motions as they may affect inference speed.\n\nWe follow the design of NN of the previous arts. Since they are fast in our pipeline, we do not further tweak or improve the architecture of the fields. Besides, during inference, the mesh is only extracted once. So there is no NF overhead for mesh extraction. For light and appearance, we only query O(NxN) thanks to the rasterization technique.\n\n**[Q2]** Efficiency wrt. lighting and PBR.\n\nIt is possible to use simpler ones like Spherical harmonic lighting [1], but it is not necessary as the computation overhead is, again, negligible. In addition, PBR and Env light are industrial standards in Computer Graphics, e.g., they are used in video games to achieve real-time and photo-realistic rendering [2].\n\n\n**[Q3]** Efficiency wrt. modeling dynamics\n\nWe model dynamics for better reconstruction and animation. Since we only move the coordinates of the extracted mesh, the time is negligible. To the best of our knowledge, there is no literature providing even efficient approach to modeling the clothes dynamics. In our supp. video, there are non-rigid offsets in clip 01:06 - 01:24 (or \"ema.supp.representation_visualization.mp4\" in supp). The mesh visualization offers a better view. Admittedly, modeling clothes dynamics is a hard research problem requiring further research.\n\n\n**[Q4]** The affection of inaccurate pose tracking.\n\nWe show the results on the synthetic data with poses from ZJU-MoCap.\n\n| noise | novel view |  | novel pose |  |\n| --- | --- | --- | --- | --- |\n| scale | psnr | ssim | psnr | ssim |\n| 0.05 | 24.41 | 0.931 | 23.43 | 0.913 |\n| 0.02 | 24.42 | 0.931 | 23.48 | 0.913 |\n| 0.01 | 24.43 | 0.932 | 23.48 | 0.914 |\n| 0.005 | 24.35 | 0.931 | 23.68 | 0.917 |\n| 0.0 | **25.98** | **0.950** | **25.15** | **0.938** |\n\nFor quality side:\n- Adding noises to the training poses leads to performance degradation.\nFor speed side:\n- For training, they spent almost the same convergence time.\n- For inference, the noisy poses do not affect the speed.\n\n\nThe paper focuses on building a pipeline that enjoys fast training, quick inference, mesh representation, template-free (which can be extended for a variety of objects and avoid proprietary models), and feedforward LBS for training. Therefore, we use datasets that are in a controlled environment. \n\nHow to overcome inaccurate pose tracking is an active research direction.\nRecently, [3] proposed an in-the-wild dataset. We will explore the in-the-wild scenario in the future."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7093/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700636178106,
                "cdate": 1700636178106,
                "tmdate": 1700700114777,
                "mdate": 1700700114777,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kGSEdO1xRx",
                "forum": "QhoehDVFeJ",
                "replyto": "HQgIETQgjZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7093/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7093/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Reviewer XfaE (Part 2)"
                    },
                    "comment": {
                        "value": "**[Q5]** Compare with ARAH.\n\nAlthough our method only achieves comparative performance with ARAH, the qualitative results of our method are better in Fig. 1 and Fig. 2, e.g., the rendering of ARAH is blurrier than ours. This indicates the drawback of current reference-based metrics.\n\n\n**[Q6]** Other mixed representation.\n\nWe refer to and discuss other mixed representation methods in the related works. The majority of them are volume-based, while our method is mesh-based. Compared with previous methods, the training speed is largely increased, the rendering is real-time, and the outputs are triangular meshes that are fully compatible with the industrial graphics pipeline. Here, the insight is that equipped with a differentiable meshes extractor and renderer, not only the geometry and appearance can be learned, but the motions, both rigid and non-rigid ones, can be learned in a feed-forward manner, which avoids proprietary human templates.\n\n\n\n[1] Ramamoorthi, R. and Hanrahan, P., 2001, August. An efficient representation for irradiance environment maps. In Proceedings of the 28th annual conference on Computer graphics and interactive techniques (pp. 497-500).\n\n[2] Advances in Real-Time Rendering in Games, 2022, https://advances.realtimerendering.com/s2022/index.html\n\n[3] Kaufmann, M., Song, J., Guo, C., Shen, K., Jiang, T., Tang, C., Z\u00e1rate, J.J. and Hilliges, O., 2023. EMDB: The Electromagnetic Database of Global 3D Human Pose and Shape in the Wild. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 14632-14643).\n\n[4] Muller, T., tiny-cuda-nn, https://github.com/NVlabs/tiny-cuda-nn"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7093/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700636230904,
                "cdate": 1700636230904,
                "tmdate": 1700700125921,
                "mdate": 1700700125921,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "52EyBI1Sq5",
                "forum": "QhoehDVFeJ",
                "replyto": "HQgIETQgjZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7093/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7093/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to your feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer XfaE,\n\nThanks again for your valuable advice and supportive comments! We have responded to your initial comments. Please feel free to let us know if you have any further questions."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7093/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700700644829,
                "cdate": 1700700644829,
                "tmdate": 1700700644829,
                "mdate": 1700700644829,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]