[
    {
        "title": "DAME: A Distillation Based Approach For Model-agnostic Local Explainability"
    },
    {
        "review": {
            "id": "nMxOXLuqev",
            "forum": "EAT7gmyIH2",
            "replyto": "EAT7gmyIH2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8912/Reviewer_7xDX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8912/Reviewer_7xDX"
            ],
            "content": {
                "summary": {
                    "value": "Explaining a deep neural network decision on a data point by using linear models as approximators in the locality of the data point has become a common practice. This paper argues that local linear approximation is inapt as the black boxes under investigation are often highly nonlinear. They propose a novel local attribution methods Distillation Approach for Model-agnostic Explainability (DAME) which does not use a linear model as local approximator. The method consist of training a student network to copy the prediction of the original DNN on the perturbated version of the data point along with a Mask-generator network that masks those perturbated samples. After training, this Mask-generator will be used to generate an explanation for the original DNN. DAME is evaluated on computer vision datasets using (a) IoU between the explanation and human annotation, (b) human subjective rating of the quality of the explanation, and (c) the drop in accuracy of the original DNN when the important pixels are removed. They also evaluate it using an audio dataset and a medical dataset, on which both use an IoU metric."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is clear and the work is well contextualized regarding prior works (although it could refer to more recent perturbation-based attribution methods).\n- Using distillation methods for explaining a model is an interesting idea"
                },
                "weaknesses": {
                    "value": "- The main weakness of this paper is in the evaluation. \n\t- It is because we do not know the reasoning behind DNN decisions --i.e. we do not what a good explanation of its decision is-- that we carefully develop methods for that purpose. In that sense, a subjective human evaluation of the quality of the explanation (b) is not actually informative of the quality of the explanation\n\t- The decisions of a DNN do not necessarily rely on the same features humans rely on (the opposite has previously been shown [1-2]). Hence an explanation that accurately depicts that the DNN does not use human-like features will be wrongly penalized by IoU metrics (a)\n\t- On the other hand, a standard way to evaluate attribution methods is using fidelity measure, Deletion and Insertion --introduced in RISE-- being the most widely used ones, which the paper does to compare DAME with RISE and LIME (it is not exactly clear if pixels are progressively removed as in Deletion or if all important pixels are removed at once). If Deletion is indeed used, the results of the 2 baseline are slightly surprising as RISE has been shown consistently to be better than LIME in previous work [3-4], which is not the case here.\n- Also, the motivation of the paper comes from the claim that linear models are inapt to accurately approximate non-linear models locally. An instantiation of the proposed framework with linear models is missing to make the claim more concrete.\n\n\n[1] Geirhos et al. Shortcut learning in deep neural networks. Nature Machine Intelligence. 2020.\n\n[2] Fel et al. Harmonizing the object recognition strategies of deep neural networks with humans. NeurIPS. 2022.\n\n[3] Petsiuk et al. RISE: Randomized input sampling for explanation of black-box models. BMVC. 2018\n\n[4] Novello et al. Making Sense of Dependence: Efficient Black-box Explanations Using Dependence Measure. NeurIPS. 2023"
                },
                "questions": {
                    "value": "- I was wondering if the authors have thought about running standard attribution methods on the original and student models as a sanity check that they do seem to have similar decisions for similar reasons?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8912/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8912/Reviewer_7xDX",
                        "ICLR.cc/2024/Conference/Submission8912/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8912/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698611565705,
            "cdate": 1698611565705,
            "tmdate": 1700603841438,
            "mdate": 1700603841438,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "px9BGbhaDz",
                "forum": "EAT7gmyIH2",
                "replyto": "nMxOXLuqev",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8912/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8912/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Addressing the questions"
                    },
                    "comment": {
                        "value": "We thank the reviewer 7xDx for reviewing the draft and providing valuable comments.\n\n1. ***clear, well contextualized***: Thank you for the appreciating our work.\n\n2. ***subjective evaluation not informative***: Thank you for the pointer on the evaluation. \n* We agree that there is no single way to explain away the decisions made by models.  Hence, we evaluate in a multi-pronged way, using fidelity measure, IoU, and the human subjective study.\n* The motivation for the human evaluation is that, for the samples in which the black-box model is highly confident, how do the humans evaluate saliency maps generated by different XAI methods under the assumption that the black-box model, on these samples with high classification confidence, has attended to the most salient parts of the objects. \n* There are prominent studies and regulations that highlight the importance of explanations being human comprehensible as they are the end users [2,3,4,5]. \n* The collective evaluation with multiple metrics highlighted the advantage of DAME.\n\n5. ***DNN not use human-like features .. penalized by IoU***: The IoU based evaluation is based on the assumption that, the classifier relies on the target object region for condidently right classified samples. To validate:\n* We mask away the regions of the target object annotations, and these masked images leads black box accuracy going down from  $92.3$\\% to $18.3$\\%.\n* However, masking away the regions outside the object class and keeping the object annotations leads to $76.5$\\% accuracy. Although a drop from $92.3$\\%, it validates that the annotations as primary information used by ViT model to classify the objects.\n* As pointed by the reviewer, the misclassification may be due to the short-cut learning. To probe this, we computed the IoU on the mis-classified samples while explaining them using ground truth labels and obtained IoU of $16.3$\\% ($31.4$\\% on correct samples) using DAME. It indicates that the model has failed to focus on the object regions. The drop in IoU for other XAI methods are:\n\n| method | mean IoU | IoU drop from correct samples |\n|------------|----------------|----------------------------------------------|\n| RISE       | 19.7 (14.2)    | 11.4                                         |\n| LIME       | 16.6 (13.2)    | 10.2                                         |\n| DAME       | 16.2 (13.9)    | 15.2                                         |\n\n6. ***Deletion measure***: We clarify the evaluation carried in the draft are based on removing $30$\\% pixels at once.\n\n**Reason behind $30$\\% feature deletion**: For images ($x^{(i)}$) and object annotations ($Z^{(i)}$), as shown in Figure 20,\nlet the object occupancy is:\n\\begin{equation}\n\\mu_O^{(i)} = \\frac{|(m,n): Z^{(i)}(m, n)=1, (m,n)\\in Z^{(i)}|}{|(m,n): (m,n)\\in Z^{(i)}|}X 100\\%\n\\end{equation}\nThe mean object occupancy $\\mu_O = \\frac{1}{|D|}\\sum_{k=1}^{|D|}\\mu_O^{(i)}$ is found to be $29.2$\\% ($D$ denotes the VOC dataset) as shown in Figure 20 and  hence $30$\\% important feature removal is used as a fast, one-shot approach. If gradual deletion is performed in $x$ steps, it will require $10x$ inference time.\n\nHowever, we also did gradual deletion as a standard way and found the (AUC- {**LIME: 0.340, RISE: 0.332, DAME: 0.325**}) as shown in Figure 19. [1] reported only drop in model confidence but we reported drop in accuracy also. Also note the plots and AUC values in [1] are for selected samples, but we have reported the metrics  over full VOC dataset; and DAME shows the most sensitivity to deletion.\n\n7. ***proposed framework with linear models is missing***: We used a linear student network in DAME and observe that the model fails to generate meaningful explanations. The fidelity measure at $30$\\% feature deletion results in only $24.3$\\% drop in accuracy as opposed to $56.2$\\% for the non-linear model.\n8. ***similar decisions for similar reasons?***: We removed the top-k features (based on XAI output) gradually from the image and pass the masked images through the DAME student model  as well as the black-box model to get the  scores. The DAME scores show high similarity with the black box scores with $R^2$ of $0.819$ on VOC dataset, confirming their agreement.\n\nReferences:\n1. Petsiuk, Rise: Randomized input sampling for explanation of black-box models.\n2. Nguyen. \u201cThe effectiveness of feature attribution methods and its correlation with automatic evaluation scores\u201d. NeurIPS 2021.\n3. Colin, What i cannot predict, i do not understand: A human-centered evaluation framework for explainability methods. NeurIPS 2022.\n4. Goodman. \u201cEuropean Union regulations on algorithmic decision-making and a \u201cright to explanation\u201d\u201d. AI magazine 2017.\n5. Doshi. \"Towards a rigorous science of interpretable machine learning.\" arXiv 2017.\n6. Sun, Ao, \"Explain Any Concept: Segment Anything Meets Concept-Based Explanation.\", NeurIPS 2023.\n7. Lerman, Samuel,. \"Explaining Local, Global, And Higher-Order Interactions In Deep Learning.\" ICCV 2021."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8912/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700408946295,
                "cdate": 1700408946295,
                "tmdate": 1700408946295,
                "mdate": 1700408946295,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Hll0ByFH34",
                "forum": "EAT7gmyIH2",
                "replyto": "nMxOXLuqev",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8912/Reviewer_7xDX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8912/Reviewer_7xDX"
                ],
                "content": {
                    "title": {
                        "value": "Response to author"
                    },
                    "comment": {
                        "value": "Thank you for addressing some of my points, I will raise my score accordingly.\n\n2. I still stand by my original argument.\n5. I appreciate the validation done for the IoU measure, but I still do not see its value in evaluating methods when you already use the standard fidelity metrics, especially if a fidelity-like metric is needed to validate it. Again, we are not sure that the model only cares about human-like features and we are not sure that the XAI method highlights the feature used by the model, yet using IoU requires us to make assumptions about either or both of them, hence I would be less confident in the results.\n6. Thank you for the clarification, the motivation for this choice is sound.\n7. and 8. Thank you for addressing my questions."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8912/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700603820929,
                "cdate": 1700603820929,
                "tmdate": 1700727853512,
                "mdate": 1700727853512,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "efT8le4iYy",
            "forum": "EAT7gmyIH2",
            "replyto": "EAT7gmyIH2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8912/Reviewer_RyMT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8912/Reviewer_RyMT"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a model-agnostic, gradient-free, saliency-based method to understand local behavior of black-box models. They establish the shortcomings of previous works like LIME that use a locally linear model to approximate the behavior of a neural network in a given sample\u2019s neighborhood. They use ideas from the MLX (Machine Learning from Explanations) area and propose to address this via distilling the black-box model into a smaller student model only in the sample\u2019s neighborhood. Concretely, they generate perturbations of a sample and then learn saliency masks (explanations) such that a perturbed sample masked by the saliency when passed through the student model has the same target class softmax score as the teacher. These two models, the one that learns the masks and the student that distills the black-box in a sample\u2019s neighborhood, are chained and trained together using a distillation+explanation loss (with 2 more loss terms to avoid identity learning and preserve class distributions between student and teacher). They share results of their approach on 2 vision datasets and 2 audio datasets."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThis method works in input space and not in the binary mask space like LIME does\n2.\tThe paper establishes the shortcoming of locally linear approximations with a small toy experiment.\n3.\tThey share results from many varied experiments with both quantitative metrics and qualitative samples. To quantify the quality of their explanations, they compute IoU with human annotations for samples that are classified correctly by the model \u2013 since that is the class that human annotations would be explaining.\n4.\tIt is an intuitive approach. The paper is well written and easy to follow\n5.\tThey compare with LIME, RISE, GRADCam and other gradient based methods from Integrated Gradients family.\n6.\tThe appendix is very thorough and quite informative"
                },
                "weaknesses": {
                    "value": "1.\tThe biggest bottleneck to using this approach would be having to train a whole new model to understand the behavior of the model for one single input sample.\n2.\tResults from RISE are often quite competitive in tables 1 and 2. Smooth Grad is also quite competitive.\n3.\tThis approach is akin to a gradient-based approach in the guise of gradient-free. If one was to distill the whole black-box into another model (not just in the sample\u2019s neighborhood) and then apply any gradient-based method, I believe that that would be much simpler since one won\u2019t have to train a smaller model to get an explanation for each sample and I believe it would perform competitively seeing the numbers in tables 1 and 2. So, I have doubts about why the authors have taken this round-about route. It is at least worth it to compare this with works that use distillation to understand models.\n4.\tI might have missed something here but the audio experiment results don\u2019t seem too convincing:\na.\tIn task 2, padding noise on two sides is an easy noise pattern to learn/catch. \nb.\tIn task 3, cough data says that it was manually annotated. Are there going to be any plans to release this to enable discussion/reproducibility?\n5.\tSome language in the paper such as \u201cmildly vs strongly non-linear\u201d is non-standard. This is a small nitpick."
                },
                "questions": {
                    "value": "1.\tHave the authors considered using this method on well-known spurious feature detection image datasets like Decoy-MNIST and ISIC?\n2.\tIf one was to distill the whole black-box into another model (not just in the sample\u2019s neighborhood) and then apply any gradient-based method, I believe that that would be much simpler since one won\u2019t have to train a smaller model to get an explanation for each sample and I believe it would perform competitively seeing the numbers in tables 1 and 2. So, I have doubts about why the authors have taken this round-about route. It is at least worth it to compare this with works that use distillation to understand models. I would like to get the authors thoughts on these points.\n3.\tCan the authors clarify if I have incorrectly interpreted the audio experiments setup or results? Are there going to be any plans to release manually annotated cough data to enable discussion/reproducibility?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8912/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8912/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8912/Reviewer_RyMT"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8912/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698777227052,
            "cdate": 1698777227052,
            "tmdate": 1699637121516,
            "mdate": 1699637121516,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0Z5bm6nPjS",
                "forum": "EAT7gmyIH2",
                "replyto": "efT8le4iYy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8912/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8912/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Addressing the questions of Reviewer RyMT (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer **RyMT** for reviewing the draft and providing valuable input.\n\n1. ***works in input space and not in the binary mask space***, ***establishes the shortcoming of locally linear approximations a small toy experiment***, ***results from many varied experiments with both quantitative metrics and qualitative samples***, ***intuitive approach, well written and easy to follow***, ***appendix is very thorough and quite informative***: \n\nThank you for appreciating our problem formulation, presentation and evaluation on diverse set of tasks and datasets.\n\n2. ***The biggest bottleneck to using this approach would be having to train a whole new model to understand the behavior of the model for one single input sample.***: \n\nThank you for pointing this out. Because of unavailability of gradient access to black box in gradient-free explainability, all post-hoc methods (LIME, DAME, and RISE) rely on generating  local perturbations from the  input and approximating the corresponding black box responses. In this setting, DAME takes about $50$ sec. of computation time (single A-6000 NVidia GPU on an Intel server) to train on a single sample. The reference numbers for LIME: 25 sec., and RISE: 40 sec. for the same settings. Hence, DAME is only about $25$\\% more expensive compared to baseline method of RISE, while yielding performance gains on various tasks and evaluation metrics listed in the paper and the responses.\n\n3. ***Results from RISE are often quite competitive in tables 1 and 2. Smooth Grad is also quite competitive.***:\n\nSmoothGrad requires internal access to the model in the form of gradients to generate explanations, which is highly restrictive for various black-box models. The proposed approach is gradient-free post-hoc XAI setting, with only input-output access (as described in the paper, various recent models like GPT are released without any internal access to the model architecture). Hence, gradient based XAI methods merely correspond to upper-bound for gradient free methods. Further, as seen in Table I of the main draft, gradient methods like SmoothGrad perform poorly on Transformer based models like ViT.\n\nWith regard to comparison with RISE, the  proposed  DAME framework,\n   * Improves in IoU metric over the RISE on all the audio based evaluations.\n   * Improves over the RISE/LIME on image based evaluations using the fidelity based measures, for example, deletion (please refer to Section A.12 and Figure 19 in revised draft). \n\nAlso The XAI for object/event classification should be able to shed some light on mis-classifications by the black box, apart from explaining the correctly predicted samples. To examine this, we explored generating saliency based explanations for the mis-classified samples in VOC dataset and compared them with the ground truth annotations. We found that the mean IoU, using different XAI methods, elicit a drop for all XAI methods, with the largest drop for DAME approach as shown in below Table. It indicates that the black box model may have focused on the contextual regions (outside the target class bounding box) for these samples.\n| method | mean IoU | IoU drop from correct samples |\n|------------|----------------|----------------------------------------------|\n| RISE       | 19.7 (14.2)    |     11.4                                         |\n| LIME       | 16.6 (13.2)    |     10.2                                         |\n| DAME       | 16.2 (13.9)    |     15.2                                         |\n**Table**: IoU for misclassified samples.\n   * Subjective tests indicated improved preference for the explanations provided by DAME over the other methods.\n\n4. ***This approach is akin to a gradient-based approach in the guise of gradient-free. If one was to distill the whole black-box into another model (not just in the sample\u2019s neighborhood) and then apply any gradient-based method, I believe that that would be much simpler. why the authors have taken this round-about route.***:\n\nThank you for raising it, which also warrants a discussion in the main draft about the motivation for DAME. We argue why the global distillation is not a feasible choice for post-hoc XAI methods.\n  * Global distillation requires access to the training data of the original teacher (black-box) model.  As DAME is proposed for a post-hoc XAI setting, the requirement of access to the supervised training data might pose a severe limitation to the applicability of the XAI (for example, an XAI for an LLM might need a very training corpus to generate a suitable student model). \n   * Simulating the classification bias and spurious correlation behavior of the larger black-box with a simpler student model globally might be infeasible, which may cause a divergent explanation using the student model compared to the explanations generated for the teacher model."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8912/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700597770137,
                "cdate": 1700597770137,
                "tmdate": 1700597770137,
                "mdate": 1700597770137,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XheOo3Dmj2",
                "forum": "EAT7gmyIH2",
                "replyto": "AlVCE9ZybC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8912/Reviewer_RyMT"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8912/Reviewer_RyMT"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for clarifications and discussions. I have read through this thread and other threads. At this time, I would like to stand by my original rating."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8912/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700605937182,
                "cdate": 1700605937182,
                "tmdate": 1700605937182,
                "mdate": 1700605937182,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "UvLeUcG7Uw",
            "forum": "EAT7gmyIH2",
            "replyto": "EAT7gmyIH2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8912/Reviewer_hRgW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8912/Reviewer_hRgW"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a framework for generating a learnable saliency-based explanations model, which is model-agnostic and requires only black box query access to the model. The framework consists of two models: a mask-generation module that generates the saliency maps and a student network to distill the black-box model's predictions by approximating the black-box model's local behavior near the input sample. The parameters of these two networks are learned by generating perturbations in the neighborhood of a given sample."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper addresses an important question on generating saliency map-based explanations with only black-box\naccess to a model.\n- Besides traditional tasks from Computer Vision, the paper also reports results on audio processing tasks.\n-  The paper is well-written and easy to follow."
                },
                "weaknesses": {
                    "value": "One of the critical issues with the paper is how they evaluate & the choice of baselines. Even though they consider a\na diverse set of tasks, the authors must add additional experiments to strengthen the paper.\n\nIt is hard to see whether the proposed framework offers a clear advantage over the baselines (as explanations are typically subjective).\n\nIt would also be essential to understand how architectural changes affect the results.\n\n- Does the architecture of the map generation & student network affect the performance? Does it need to be shallow\nor deeper? What are the design considerations for these networks?\n- How does the proposed method compare to, say, just distilling a smaller model from the black-box model & then\nusing the distilled network to generate saliency maps (and use these as explanations for the black-box model?)? This should be a baseline.\n- What's the need for a map-generation network in the framework? Can't we distill the black-box model through a\nstudent network exposed to the perturbations?\n- The authors should add the above two setups as baselines."
                },
                "questions": {
                    "value": "The proposed framework incurs an additional computation cost but performs worse than a simpler technique like RISE, and the improvement seems marginal.\n\nHow important are the perturbations? The mask-generation network seems to be trainable without the perturbations of inputs. It would be better to investigate the impact of the number of perturbations on explanation performance to evaluate the effectiveness of the perturbations.\n\nI also encourage the authors to consider benchmarks like CUB & AwA2 (and other benchmarks where concepts are annotated), which contain annotations of salient parts of the image; this helps them compare against some gold standards\n\nRefer to Weaknesses for additional questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8912/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8912/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8912/Reviewer_hRgW"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8912/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698847060110,
            "cdate": 1698847060110,
            "tmdate": 1700631484803,
            "mdate": 1700631484803,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "g7RWD4djqJ",
                "forum": "EAT7gmyIH2",
                "replyto": "UvLeUcG7Uw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8912/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8912/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Addressing the questions of Reviewer hRgW (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer **hRgW** for reviewing the draft and providing valuable input.\n1. ***addresses an important question on generating saliency map-based explanations with only black-box access***: \n\nThank you for appreciating the motivation of our work.\n\n2. ***Besides traditional tasks from Computer Vision, the paper also reports results on audio processing tasks.***: \n\nThank you for appreciating our experimental setup on diverse tasks.\n\n3. ***The paper is well-written and easy to follow.***: \n\nThank you.\n\n4. ***Even though they consider a diverse set of tasks, the authors must add additional experiments to strengthen the paper. It is hard to see whether the proposed framework offers a clear advantage over the baselines (as explanations are typically subjective).***: \n\nWe have performed a set of additional experiments to further highlight the advantages of DAME over other methods, and to experimentally validate the choices made in the paper.\n   * **Score agreement with black box**: We carried out experiments to examine how much the model based XAI methods (LIME and DAME) agree with the black box behavior for perturbed samples- commonly used as one of the fidelity measures. LIME and DAME tries to mimic the black box at the local neighbourhood and hence their response ideally should match with black box response for samples drawn from local neighbourhood of the input. We observe a substantially stronger agreement for DAME over LIME, as shown in Figure 22 in the revised draft.\n   * **Gradual deletion based fidelity evaluation**: We performed gradual deletion experiment (as proposed in [1]) instead of removing $30$\\% of the top features at once (as reported in the original draft). We observed a lower value of area-under-the-curve (AUC) for DAME, demonstrating its improved sensitivity to deletion, as shown in Figure 19 in the revised draft.\n   * **Probing wrongly classified samples**: The XAI for object/event classification should be able to shed some light on mis-classifications by the black box, apart from explaining the correctly predicted samples. To examine this, we explored generating saliency based explanations for the mis-classified samples in VOC dataset and compared them with the ground truth annotations. We found that the mean IoU, using different XAI methods, elicit a drop for all XAI methods, with the largest drop for DAME approach, as shown in Table 2.1 below. It indicates that the black box model may have focused on the contextual regions (outside the target class bounding box) for these samples, leading to mis-classification.\n| method | mean IoU | IoU drop from correct samples |\n|------------|----------------|----------------------------------------------|\n| RISE       | 19.7 (14.2)    |     11.4                                         |\n| LIME       | 16.6 (13.2)    |     10.2                                         |\n| DAME       | 16.2 (13.9)    |     15.2                                         |\n**Table 2.1**: IoU for misclassified samples.\n   * **Justification of map generation network**: Additional experiments regarding the need of the map generation network is performed, which justifies the two network strategy in the DAME framework as discussed in Table 2.3 below.\n   * **Additional ablation experiments**: Architecture choice of student network, and the minimum number of perturbation samples needed for the DAME framework are experimentally established as discussed in Tables 2.2 and 2.4 below.\n\n5. ***Does the architecture of the map generation & student network affect the performance? Does it need to be shallow or deeper? What are the design considerations for these networks?***: \n\nWe experimented with different architectures of the student network. Using a two layer fully connected network (FCN) following a 2 layer convolutional network (CNN) as student network gives significantly better performance than a other architecture chices as shown in Table 2.2 below.\n| **DAME variants**                               | **ResNet-101** | **Vit base-16** |\n|-------------------------------------------------|----------------|-----------------|\n| student network: 2 CNN + 1 FCN layers           | 31.8           | 30.3            |\n| student network: 2 FCN layers                   | 32.2           | 30.9            |\n| student network (paper) : 2 CNN + 2 FCN layers  | 33.3           | 31.4            |\n**Table 2.2:** IoU values obtained using different student networks with DAME.\n\nRegarding depth, a student network which is  deeper, increases the time overhead to generate the explanations for each input sample. As we attempt a local approximation only, a shallow network may be sufficient which also provides significant savings in computation time.\n\n   \n**References**:\n1. Petsiuk et al. RISE: Randomized input sampling for explanation of black-box models. BMVC. 2018"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8912/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700593084101,
                "cdate": 1700593084101,
                "tmdate": 1700593084101,
                "mdate": 1700593084101,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7Lhj0yesUk",
                "forum": "EAT7gmyIH2",
                "replyto": "3IPp7OKeDI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8912/Reviewer_hRgW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8912/Reviewer_hRgW"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the detailed response, I would encourage the authors to add these to the draft if possible, I am updating my score."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8912/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631466087,
                "cdate": 1700631466087,
                "tmdate": 1700631466087,
                "mdate": 1700631466087,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cjMSeOyN1s",
                "forum": "EAT7gmyIH2",
                "replyto": "UvLeUcG7Uw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8912/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8912/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you, Reviewer hRgW"
                    },
                    "comment": {
                        "value": "We thank Reviewer hRgW for his valuable time in going through our response and appreciating, and also increasing the score. We will surely include the responses in the final version of the draft."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8912/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631628512,
                "cdate": 1700631628512,
                "tmdate": 1700634158487,
                "mdate": 1700634158487,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Nb49J50EYR",
            "forum": "EAT7gmyIH2",
            "replyto": "EAT7gmyIH2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8912/Reviewer_kHxm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8912/Reviewer_kHxm"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes Distillation Approach for Model-agnostic Explainability (DAME), an approach which fits a non-linear model is fit in the vicinity of an input sample to to explained. The model is fit to obtain a saliency map explanation based on a teacher-student distillation approach which uses a combination of 3 loss functions. The proposed method is comprehensively evaluated on image and audio datasets using a number of evaluation techniques and shows improvement over existing local explainability methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Overall when the decision boundary is wiggly and inputs are high dimentsional, sparse linear models may not mimic the source model's behaviour around a sample and it makes sense to use a non-linear approach which might provide a better approximation. The approach proposed to generate the local saliency explanation is novel. The evaluation is quite comprehensive including fidelity-based, subjective and qualitative evaluations and comparison with 9 XAI methods."
                },
                "weaknesses": {
                    "value": "Based on the 3 loss functions that need to be handled, it seems likely that the method may not work out of the box (like LIME) and users will probably need to customize/tune hyper-parameters etc. to get the explanations right."
                },
                "questions": {
                    "value": "- How is local vicinity and distance between the given sample and perturbations defined in DAME - is this same as LIME?\n- In case of DAME, can the authors comment on local invariance of explanations (do similar inputs yield similar explanations)?\n- Would DAME be impacted by correlated featured?\n- Instead of using a masking approach to generate perturbations (e.g. LIME), if we have a realistic distribution of perturbed images (e.g. MeLIME), can the DAME pipeline still be used?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8912/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699290837401,
            "cdate": 1699290837401,
            "tmdate": 1699637121261,
            "mdate": 1699637121261,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7qb27i5IbY",
                "forum": "EAT7gmyIH2",
                "replyto": "Nb49J50EYR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8912/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8912/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Addressing the questions of Reviewer kHxm"
                    },
                    "comment": {
                        "value": "We thank the reviewer **kHxm** for reviewing the draft and providing valuable inputs.\n\n1. ***The proposed method is comprehensively evaluated on image and audio datasets using a number of evaluation techniques and shows improvement over existing local explainability methods.***: Thank you for appreciating our work and the evaluations performed on on diverse datasets and tasks.\n\n2. ***it makes sense to use a non-linear approach***, ***The approach proposed to generate the local saliency explanation is novel***, ***The evaluation is quite comprehensive including fidelity-based, subjective and qualitative evaluations and comparison with 9 XAI methods***: Thank you appreciating our approach, novelty and evaluations.\n\n3. ***Based on the 3 loss functions that need to be handled, it seems likely that the method may not work out of the box***: \nThank you for the question. Although it is true that the loss function has three components, we found that tuning them for a modality works well for multiple applications with that modality. For example, the hyperparameters tuned for ImageNet was used as is for VOC dataset, and hyperparameters tuned for ESC-10 based audio classification were used as is for the COSWARA application.\n\nMoreover, we would also like to add that the KL-div loss component contributes minimally as can be seen from the Table below. Hence, the MSE loss along with a L1 loss component (to avoid singularity) itself may constitute a  reasonable choice for the loss function in DAME.\n| **DAME variants**    | **ResNet-101** | **Vit base-16** |\n|----------------------|----------------|-----------------|\n| loss: MSE+L1         | 32.4           | 30.8            |\n| loss: MSE+L1+KL-div. | 33.3           | 31.4            |\n\n4. ***How is local vicinity and distance between the given sample and perturbations in DAME***: We used the same measure of distance between a sample and its perturbations, the L2 distance between them **in the input space**. On the contrary, LIME uses the distance as the L2 distance between them **in the binary space** ($M_{x^{(k)}}$ space, as described in Section 4.2 in the draft). \nThe distance measure in input space results in higher sensitivity because masking-off larger regions and smaller segments may not be similar.\n\n5. ***In case of DAME, can the authors comment on local invariance of explanations (do similar inputs yield similar explanations)?***:\nYes, we have verified this experimentally that similar inputs indeed generate similar explanations.\nWe have also performed an attribution experiment between the student and teacher model. \nWe removed the top-k features (based on XAI output) gradually (k increasing from 0 to 100\\%) from the image and pass the masked images through the DAME student model  as well as the blackbox model to get the  scores. The DAME scores show high similarity with the black box scores. An average coefficient of determination ($R^2$) of $0.819$ was obtained for the samples in VOC dataset, confirming the agreement between DAME student and the black box model.\n\n6. ***Would DAME be impacted by correlated featured?***:\nExamining sensitivity of XAI methods with respect to correlated features is an important area of research. While a detailed quantitative evaluation was not possible within the short rebuttal period, we have resorted to a qualitative evaluation. An example image of a cat with correlated features (the shadow associated with it) and the explanation generated by DAME is shown in Figure 21 of the updated draft.  \nThe shadow has similar shape as the cat, leading to a correlated feature in the input. However, the DAME generated explanation is not influenced by the presence of the shadow.\n\n7. ***Instead of using a masking approach to generate perturbations (e.g. LIME), if we have a realistic distribution of perturbed images (e.g. MeLIME), can the DAME pipeline still be used?***:\nWe thank the reviewer for pointing us to this resource.\nThe proposed approach is indeed agnostic to the choice of masking or perturbation generation approach.\nMeLIME proposes an effective perturbation strategy in general, that improves the explanations provided by different methods like LIME. Because this perturbation strategy essentially focuses on generating samples that better capture the representation of local vicinity of the input, it constitutes a promising experiment to explore before the camera ready version of the submission."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8912/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700589220209,
                "cdate": 1700589220209,
                "tmdate": 1700589396136,
                "mdate": 1700589396136,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XPhIi4MoxC",
                "forum": "EAT7gmyIH2",
                "replyto": "7qb27i5IbY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8912/Reviewer_kHxm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8912/Reviewer_kHxm"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the authors for their detailed response and clearly addressing issues raised by all reviewers. I appreciate it. I will continue to retain my score."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8912/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668659420,
                "cdate": 1700668659420,
                "tmdate": 1700668659420,
                "mdate": 1700668659420,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]