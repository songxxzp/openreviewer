[
    {
        "title": "Magnitude Invariant Parametrizations Improve Hypernetwork Learning"
    },
    {
        "review": {
            "id": "NE1Vc6iMLd",
            "forum": "fJNnerz6iH",
            "replyto": "fJNnerz6iH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4268/Reviewer_YXs9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4268/Reviewer_YXs9"
            ],
            "content": {
                "summary": {
                    "value": "This paper identifies a problem that contributes to the challenge of training for hypernetworks: The magnitude proportionality between the inputs and outputs of the hypernetwork. The authors demonstrate how this proportionality can result in unstable optimization processes. To mitigate this issue, they introduce a magnitude-invariant parameterization method for hypernetworks, demonstrating its effectiveness in stabilizing the training process and accelerating convergence."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The techniques proposed in this paper are indeed helpful in stabilizing the training of hypernetworks. Given that hypernetworks are widely used in machine learning. These techniques can be useful for practitioners."
                },
                "weaknesses": {
                    "value": "- The discussion regarding why input/output proportionality results in unstable training is somewhat insufficient. It is a well-known fact that neural networks with piecewise linear activation functions, such as ReLU networks, exhibit this proportionality between inputs and outputs. However, if proportionality is the main reason, why ReLU networks are not well-known for having such problems for classical tasks such as regression/classification (not in hypernetworks)? Therefore, I think more explanations or discussions are needed in the text.\n- The two strategies introduced in the paper, namely input encoding and additive output formulation, are a bit ad hoc from my perspective. While these methods undoubtedly offer practical benefits for hypernetworks, I am not sure if the contribution is significant enough. A more thorough exploration and explanation of the theoretical underpinnings that underscore the significance of these techniques would enhance the paper's contribution and provide a clearer justification for their adoption."
                },
                "questions": {
                    "value": "- I might miss the related information. Apologies if that is the case. Are there any results of combining additive output with BatchNorm/LayerNorm in Figure 5? It is interesting to see such results because it verifies that MIP has something that normalization layers cannot offer. If proportionality is the main reason, what are the theoretical reasons that MIP can solve the problem but normalization cannot?\n- If the problem is proportionality, how about we use other activation functions such as ELU or GELU? Does that solve the problem? If not, can we still say the problem is proportionality?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4268/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698683943857,
            "cdate": 1698683943857,
            "tmdate": 1699636394179,
            "mdate": 1699636394179,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iNsQmD3I29",
                "forum": "fJNnerz6iH",
                "replyto": "NE1Vc6iMLd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4268/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4268/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate the constructive feedback and comments. Addressing the\nraised questions:\n\n**Weaknesses:**\n\n> The discussion regarding why input/output proportionality results in\n> unstable training is somewhat insufficient. It is a well-known fact\n> that neural networks with piecewise linear activation functions, such\n> as ReLU networks, exhibit this proportionality between inputs and\n> outputs. However, if proportionality is the main reason, why ReLU\n> networks are not well-known for having such problems for classical\n> tasks such as regression/classification (not in hypernetworks)?\n> Therefore, I think more explanations or discussions are needed in the\n> text.\n\nThis is a natural question, that we should have addressed more directly.\nWe will revise the manuscript to include further discussion about why\nthe input/output proportionality issue is less important for regular\nneural networks. Most notably:\n\n-   Deep neural networks are often used to model high dimensional\n    unstructured data such as image, text, or audio. In these settings,\n    neural network inputs are most often standardized to have zero mean\n    and unit variance, which reduces the effect of the proportionality\n    issue. In contrast, many applications of hypernetworks have\n    relatively low input dimensionality. For example, hypernetworks are\n    often used with scalar or low dimensional inputs. The\n    proportionality issue is exacerbated in hypernetwork problems that\n    feature low dimensional or scalar input spaces.\n\n-   A similar issue has been demonstrated for some applications of\n    regular networks, like NeRFs and implicit networks. In such cases,\n    alternatives such as sinusoidal encodings and activation functions\n    have been shown to help \\[1,2\\].\n\n\\[1\\] Implicit Neural Representations with Periodic Activation Functions  - Sitzmann et al.\n\n\\[2\\] Fourier Features Let Networks Learn High Frequency Functions in\nLow Dimensional Domains - Tancik et al.\n\n**Questions:**\n\n> I might miss the related information. Apologies if that is the case.\n> Are there any results of combining additive output with\n> BatchNorm/LayerNorm in F igure 5? It is interesting to see such\n> results because it verifies that MIP has something that normalization\n> layers cannot offer. If proportionality is the main reason, what are\n> the theoretical reasons that MIP can solve the problem but\n> normalization cannot?\n\nThe reviewer is correct, Figure 5 does not include results combining the\nadditive output with BatchNorm/LayerNorm. In our experiments, adding\nnormalization layers to MIP had little effect in per-epoch convergence,\nbut it increased the per-epoch computational cost because of the\nadditional overhead of the normalization layers.\n\nMore generally, the fundamental reason normalization layers are not well\nsuited to deal with this issue is that they remove degrees of freedom by\nenforcing zero mean and unit variance in the input. Since hypernetworks\noften feature low dimensional input spaces, removing degrees of freedom\nin the input introduces substantial information loss. Enforcing constant\nmagnitude inputs has similar shortcomings. This is why our input\nencoding maps each dimension to a separate pair of coordinates.\n\n> If the problem is proportionality, how about we use other activation\n> functions such as ELU or GELU? Does that solve the problem? If not,\n> can we still say the problem is proportionality?\n\nWe found that other activations like GELU, SiLU/Swish and Tanh, while\nnot having the same proportionality property, still propagate the input\nmagnitude enough for it to be detrimental in training. MIP improves\ntraining for all choices of non-linear activation. See section C.3 of\nthe supplement."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4268/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700158497209,
                "cdate": 1700158497209,
                "tmdate": 1700158552390,
                "mdate": 1700158552390,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "V2tfAVDGyR",
            "forum": "fJNnerz6iH",
            "replyto": "fJNnerz6iH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4268/Reviewer_7ccS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4268/Reviewer_7ccS"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a novel parametrisation for hypernetworks that is magnitude invariant (MIP). The main motivation for MIP stems from the author\u2019s observation that the hypernetwork output, when using piece-wise linear activations, has a magnitude proportional to the hypernetwork input. The authors argue that this proportionality is detrimental for the hypernetwork optimization as it affects the gradient variance. MIP is then introduced as a way to remove this dependence on the magnitude by parametrising the input to the hypernetwork in terms of Fourier features that have a constant norm throughout training. Then, in order to allow for hypernetwork outputs that are non-proportional to the hypernetwork inputs, the authors propose a residual parametrisation where an auxiliary weight matrix is trained directly and the hypernetwork output is used as an additive correction to that weight matrix. The authors then show that these two modifications to standard hypernetwork training, solve the proportionality problem and improve hypernetwork performance across the board."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The authors identify a novel issue that seems to be important (based on the improvement delta from MIP) for hypernetwork training.\n- The specific MIP parametrisation is novel and practically broadly useful for any task that involves hypernetworks. \n- The experiments are relatively extensive in terms of tasks and ablation / robustness studies.\n- The paper is mostly well written and clear in the presentation of the main ideas and results."
                },
                "weaknesses": {
                    "value": "- While the authors do empirically show that MIP benefits training, it is not clear whether the increased variance could also be controlled with, e.g., appropriately chosen (i.e., lower) learning rates and (i.e., higher) momentum, in the original parametrisation (which could attain similar performance, albeit slower).\n- This is something that the authors themselves identify, but given that hypernetworks are becoming popular for fast adaptation of pertained models, e.g., [1], it is important to see whether the magnitude proportionality effect is also detrimental there. \n\n[1] HyperDreamBooth: HyperNetworks for Fast Personalisation of Text-to-Image Models, Ruiz et al., 2023"
                },
                "questions": {
                    "value": "I find the overall paper to be well written, the arguments clear and the proposed solution convincing. Therefore, I am happy to recommend acceptance. My questions and suggestions are the following:\n- It would be interesting to see whether the residual formulation of the hypernetwork closes meaningfully the gap between fully task specific parameters and the ones predicted by the hyper network, i.e., $\\theta^0 + h(E_{L2}(\\gamma); w)$. For example, what is the performance if on a new task $t$ one starts from $\\theta^0$ and just optimises for a specific number of steps on that task to get $\\theta_t^*$? Is $\\theta_t^* - \\theta_0$ related to $h(E_{L2}(\\gamma); w)$? Is the performance of $\\theta_t^*$ similar to the performance of $\\theta^0 + h(E_{L2}(\\gamma); w)$?\n- Does the update given by the hypernetwork in this residual formulation need to be dense? Is the hypernetwork only adapting a few dimensions of $\\theta^0$?\n- It is not clear why for non-scalar inputs $\\gamma$, i.e., $\\gamma \\in \\mathbb{R}^D$ with $D \\geq 2$  a simple unit normalisation transformation, i.e., $\\hat{\\gamma} = \\frac{1}{\\|\\|\\gamma\\|\\|_2}\\gamma$ would not work for removing the dependence on the magnitude. It seems to me that in this case each $\\hat{\\gamma}$ would just correspond to a different point on the hypersphere and the output of the hypernetwork would not be independent of the input."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4268/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698686455445,
            "cdate": 1698686455445,
            "tmdate": 1699636394097,
            "mdate": 1699636394097,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dNEJFdzM8j",
                "forum": "fJNnerz6iH",
                "replyto": "V2tfAVDGyR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4268/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4268/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We are thankful for the detailed, careful, and thought-provoking\nconstructive feedback and comments. Addressing the raised questions:\n\n**Weaknesses:**\n\n> While the authors do empirically show that MIP benefits training, it\n> is not clear whether the increased variance could also be controlled\n> with, e.g., appropriately chosen (i.e., lower) learning rates and\n> (i.e., higher) momentum, in the original parametrisation (which could\n> attain similar performance, albeit slower).\n\nThis is an important point. In our experiments, we controlled for this\nby testing a range of learning rates and reporting the best converging\nmodels. We consistently found that MIP was less sensitive to\nhyperparameter choices than the default formulation, and that there were\nsettings, like HyperMorph with momentum SGD, where we could not find any\nsuitable hyperparameter setting (learning rate & momentum factor) that\nwould allow the model to meaningfully train.\n\n**Questions:**\n\nThe reviewer raises many interesting questions and suggests interesting\nareas of future work. Regrettably, we do not have the time or space to\naddress many of these for this submission in sufficient detail, but we\nare thankful for the suggestions as they will inform future work.\n\n> It would be interesting to see whether the residual formulation of the\n> hypernetwork closes meaningfully the gap between fully task specific\n> parameters and the ones predicted by the hyper network, i.e.,\n> $\\theta^0 + h(E_{L2}(\\gamma);\\omega)$. For example, what is the\n> performance if on a new task one starts from $\\theta^0$ and just\n> optimizes for a specific number of steps on that task to get\n> $\\theta^\\ast_t$? Is $\\theta^\\ast_t - \\theta^0$ related to\n> $h(E_{L2}(\\gamma);\\omega)$. Is the performance of $\\theta^\\ast_t$\n> similar to the performance of $h(E_{L2}(\\gamma);\\omega)$ ?\n\nWhile not exactly the same as the setup described by the reviewer, we\ndid perform experiments where we individually trained classical networks\nfor each task\u00a0$\\theta_t$ for the HyperMorph and SSHN settings, to\nunderstand how classical networks converged for these settings. When\ncomparing the task-specific networks to the performance of the\nhypernetwork weights for the same task, we found that hypernetwork\nweights performed no worse than task-specific weights.\n\n> Does the update given by the hypernetwork in this residual formulation\n> need to be dense? Is the hypernetwork only adapting a few dimensions\n> of $\\theta^0$?\n\nIn our experiments, the contribution of the hypernetwork\u00a0$\\Delta\\theta$\nwas not particularly sparse, but perhaps this is because the learning\nobjective was not set up to promote this property in the first place. We\nbelieve that adding a sparsity constraint either to the loss, or\nstructurally (such as with low rank decompositions) would be an\ninteresting avenue for future experiments. We will include this\ndiscussion in the revised manuscript.\n\n> It is not clear why for non-scalar inputs $\\gamma$, i.e.,\n> $\\gamma \\in \\mathbb{R}^D$ with $D \\geq 2$ a simple unit normalisation\n> transformation, i.e., $\\hat{\\gamma} = 1/||\\gamma||\\gamma$ would not\n> work for removing the dependence on the magnitude. It seems to me that\n> in this case each $\\hat{\\gamma}$ would just correspond to a different\n> point on the hypersphere and the output of the hypernetwork would not\n> be independent of the input.\n\nThis is a good question that we should have addressed more directly.\nDividing by the norm of the input produces information loss in the\nhypernetwork input space, even for multi-dimensional hypernetwork\ninputs. As an example, we can consider inputs $\\gamma_A = [0.1, 0.2]$\nand $\\gamma_B = [0.4, 0.8]$, dividing them by their respective norms\nresults in\n$\\gamma_A / ||\\gamma_A|| = \\gamma_B / ||\\gamma_B|| = [0.447,0.894]$.\nWhile $\\gamma_A$ and $\\gamma_B$ correspond to different points in the\ninput space, they are mapped to the same constant norm representation.\nThis is why our input encoding maps each dimension to a separate pair of\ncoordinates.\n\nSince hypernetworks often feature low dimensional input spaces, removing\na degree of freedom in the input introduces substantial information\nloss."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4268/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700158382470,
                "cdate": 1700158382470,
                "tmdate": 1700158382470,
                "mdate": 1700158382470,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SDiOJOU5Y1",
                "forum": "fJNnerz6iH",
                "replyto": "dNEJFdzM8j",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4268/Reviewer_7ccS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4268/Reviewer_7ccS"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal"
                    },
                    "comment": {
                        "value": "I thank the authors for their response and encourage them to upload the revised manuscript with the updated discussions. Finally, one more point perhaps worthwhile to discuss is the dependence on the dimensionality. While the authors mention that HyperNetworks typically use low-dimensional representations, this is a modelling choice and in general one is free to choose any dimensionality (as opposed to standard networks where the dimensionality of the input data is given and fixed). Therefore, given this available degree of freedom, it is unclear whether Fourier features (which themselves double the input dimensionality) or simple normalization methods (that maintain the same input dimensionality) are better."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4268/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700661234880,
                "cdate": 1700661234880,
                "tmdate": 1700661234880,
                "mdate": 1700661234880,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9jfZjrwZlz",
            "forum": "fJNnerz6iH",
            "replyto": "fJNnerz6iH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4268/Reviewer_6gdA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4268/Reviewer_6gdA"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a solution, Magnitude Invariant Parametrizations (MIP), to a previously unidentified optimization problem in hypernetwork training that causes large gradient variance and unstable training dynamics. MIP, by modifying the typical hypernetwork formulation, addresses this issue without adding training or inference costs. The authors extensively test MIP, demonstrating improved stability and faster convergence in hypernetwork training across various settings. They also release HyperLight, an open-source PyTorch library, to ease the implementation of MIP and promote hypernetwork adoption. Through rigorous analysis and experimentation, the paper showcases MIP's potential in substantially enhancing hypernetwork training, marking a significant advancement in this domain."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The strengths of this paper include the identification of a novel optimization problem in hypernetwork training, the proposal of a new formulation (MIP) that addresses this issue without extra computational costs, extensive testing and comparative analysis demonstrating MIP's effectiveness, and the provision of an open-source library, HyperLight, to facilitate the practical adoption of the proposed solution in hypernetwork models. Through rigorous analysis and extensive experimentation, the paper makes a significant contribution towards improving the stability and convergence speed in hypernetwork training, providing a promising direction for the community."
                },
                "weaknesses": {
                    "value": "The paper mainly focuses on fully connected layers and common activation, initialization choices, and optimizers (SGD with momentum and Adam) in its experiments, which may not encompass a broader spectrum of hypernetwork architectures or other types of networks. There's also a mention of unexplored territories like the effect of MIP on transfer learning and other less common architectures and optimizers, indicating a scope for broader empirical validation. Furthermore, the impact of MIP on real-world applications or larger-scale problems is not thoroughly explored, which might be crucial for the adoption of this technique in practical settings."
                },
                "questions": {
                    "value": "Mentioned in the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4268/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698820226024,
            "cdate": 1698820226024,
            "tmdate": 1699636394003,
            "mdate": 1699636394003,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MVuCrj6FqG",
                "forum": "fJNnerz6iH",
                "replyto": "9jfZjrwZlz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4268/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4268/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate the constructive feedback and comments. Addressing the\nraised weaknesses:\n\n> The paper mainly focuses on fully connected layers and common\n> activation, initialization choices, and optimizers (SGD with momentum\n> and Adam) in its experiments, which may not encompass a broader\n> spectrum of hypernetwork architectures or other types of networks.\n\nWe extensively tested our proposed parametrization across learning\ntasks, primary model architectures, optimizers, initialization, and\nnon-linear activations, finding consistent improvements in all settings.\nThat said, we agree with the reviewer that hypernetwork architectures\nand applications are diverse and varied. In our work, we tried to cover\nthe settings that are most prevalent in the literature. Are there\nspecific architectures that the reviewer would like us to try?\n\n> There's also a mention of unexplored territories like the effect of\n> MIP on transfer learning and other less common architectures and\n> optimizers, indicating a scope for broader empirical validation.\n\nWe agree that other applications are an interesting avenue for future\nwork. Given that we see different (though not directionally different)\nresults for Adam and SGD (which together dominate the literature), it\nwould be interesting to see what happens with other optimizers that\nmight become available.\n\n> Furthermore, the impact of MIP on real-world applications or\n> larger-scale problems is not thoroughly explored, which might be\n> crucial for the adoption of this technique in practical settings.\n\nWe agree that evaluation on real world data is important. We did show\nthe benefits that MIP brings to practical medical imaging applications.\nThe HyperMorph task of loss-regularization for medical image\nregistration features a complex medical imaging task and solves a real\nworld problem."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4268/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700158299395,
                "cdate": 1700158299395,
                "tmdate": 1700158299395,
                "mdate": 1700158299395,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rxza8u6vOB",
            "forum": "fJNnerz6iH",
            "replyto": "fJNnerz6iH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4268/Reviewer_mEvR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4268/Reviewer_mEvR"
            ],
            "content": {
                "summary": {
                    "value": "The authors study the problem of improving the stability and efficiency of training hypernetworks. They first observe a previously unidentified problem with hypernetwork training, namely that for certain hypernetwork architectures the scaling of the input to the hypernetwork leads to the proportional scaling of the outputs. This can lead to destabilization of the training procedure and slow training. The authors propose a a novel hypernetwork formulation that removes this proportionality property. Across several architectures, tasks and optimizers, their framework leads to improved training stability and convergence."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "While normalizing inputs to neural network models is already well established best practice, to the best of my knowledge the specific application of this best practice for hypernetwork inputs has not been studied as much. I consider the fact that the input encoding approach of the authors is straightforward a plus. The experimental validation of the key claims of the papers is extensive."
                },
                "weaknesses": {
                    "value": "I am not really sure if the output encoding part of the framework fits well with the problem that the authors claim to solve. It is not clear how output encoding relates to the input and output proportionality problem. It also makes interpreting the experimental results where both input and output encoding are used harder. Intuitively, output encoding allows the model to learn the task even if the hypernetwork does nothing so it is not clear if we improve hypernetwork training or just make the hypernetwork path less critical and rely on \"classical\" parameter learning which is typically more stable. \n\nAnother thing that is not clear to me is that it is not clear to me what is special about hypernetworks so that input normalization needs to work differently than \"classical\" networks. The scaling property would hold for a \"classical\" network as well for the appropriate activations. And yet Appendix C.5 claims that standard normalization techniques of \"classical\" networks would not work for hypernetworks. Appendix C5 seems to suggest that \"classical\" normalization techniques could make the output independent of the input. It is not clear to me why this is a problem only for hypernetworks and not for \"classical\" networks.\n\nAlso it would be useful to clarify why one could not just divide the hypernetwork input by its norm, at least for the case of a multi-dimensional hypernetwork inputs. In my mind, if the problem was just the scaling of the output, this should have worked. The suggested methods go beyond just fixing the scale of the input vector as a whole so it is not very clear if the problem is indeed the input output proportionality or some more general feature scaling issue."
                },
                "questions": {
                    "value": "I have summarized above some points that were not very clear to me. I would be willing to increase my score if they were to be clarified."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4268/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4268/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4268/Reviewer_mEvR"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4268/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698837423797,
            "cdate": 1698837423797,
            "tmdate": 1700676519205,
            "mdate": 1700676519205,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6ikMQE6Fmi",
                "forum": "fJNnerz6iH",
                "replyto": "rxza8u6vOB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4268/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4268/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate the constructive feedback and comments. Addressing the\nraised questions:\n\n> I am not really sure if the output encoding part of the framework fits\n> well with the problem that the authors claim to solve. It is not clear\n> how output encoding relates to the input and output proportionality\n> problem. It also makes interpreting the experimental results where\n> both input and output encoding are used harder. Intuitively, output\n> encoding allows the model to learn the task even if the hypernetwork\n> does nothing, so it is not clear if we improve hypernetwork training\n> or just make the hypernetwork path less critical and rely on\n> \\\"classical\\\" parameter learning which is typically more stable.\n\nThis is subtle, and we should have done a better job of explaining it,\nand will do so. Using the output encoding changes the learning problem\nfor the hypernetwork. Rather than learning how to predict parameters,\nthe output encoding provides a task-independent parameter initialization\nso that the hypernetwork only has to learn the delta for each task.\nUnder the default formulation, the prior is that there is no commonality\nbetween tasks. With the output encoding, we change that prior so that\nthere is some commonality.\n\nIn Figure 5 we include an ablation of input and output encoding, where\nwe find that each component improves the learning process individually,\nand that best results are achieved when both are used jointly.\n\n\n> Another thing that is not clear to me is that it is not clear to me\n> what is special about hypernetworks so that input normalization needs\n> to work differently than \\\"classical\\\" networks. The scaling property\n> would hold for a \\\"classical\\\" network as well for the appropriate\n> activations. And yet Appendix C.5 claims that standard normalization\n> techniques of \\\"classical\\\" networks would not work for hypernetworks.\n> Appendix C5 seems to suggest that \\\"classical\\\" normalization\n> techniques could make the output independent of the input. It is not\n> clear to me why this is a problem only for hypernetworks and not for\n> \\\"classical\\\" networks.\n\nOur observation that \"normalization techniques could make the output\nindependent of the input\" was referring to the specific case of scalar\ninputs. We should have said this, and we will revise the manuscript to\nreflect that.\n\nIncluding normalization layers does make activations follow normalized\ndistributions, but they introduce learning challenges in settings with\nlow dimensional inputs. The reason why these challenges rarely arise\nwith classical networks is that classical networks are rarely used with\nlow dimensional or scalar input spaces. In contrast, hypernetworks are\noften used to predict parameters based on hyperparameters of interest,\nand frequently have low dimensional or scalar inputs.\n\n> Also it would be useful to clarify why one could not just divide the\n> hypernetwork input by its norm, at least for the case of a\n> multi-dimensional hypernetwork inputs. In my mind, if the problem was\n> just the scaling of the output, this should have worked. The suggested\n> methods go beyond just fixing the scale of the input vector as a whole\n> so it is not very clear if the problem is indeed the input output\n> proportionality or some more general feature scaling issue.\n\nThis is a good question that we should have addressed more directly.\nDividing by the norm of the input produces information loss in the\nhypernetwork input space, even for multi-dimensional hypernetwork\ninputs. As an example, we can consider inputs $\\gamma_A = [0.1, 0.2]$\nand $\\gamma_B = [0.4, 0.8]$, dividing them by their respective norms\nresults in\n$\\gamma_A / ||\\gamma_A|| = \\gamma_B / ||\\gamma_B|| = [0.447,0.894]$.\nWhile $\\gamma_A$ and $\\gamma_B$ correspond to different points in the\ninput space, they are mapped to the same constant norm representation.\nThis is why our input encoding maps each dimension to a separate pair of\ncoordinates."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4268/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700158148046,
                "cdate": 1700158148046,
                "tmdate": 1700158148046,
                "mdate": 1700158148046,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RmWlYsXK4n",
                "forum": "fJNnerz6iH",
                "replyto": "6ikMQE6Fmi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4268/Reviewer_mEvR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4268/Reviewer_mEvR"
                ],
                "content": {
                    "title": {
                        "value": "I appreciate the response"
                    },
                    "comment": {
                        "value": "I appreciate the response. and I think the suggested improvements to the manuscript will indeed help.\n\nBased on the author's comments, I am starting to see what is the hyper-network specific problem, mainly that input normalization is  more important when the input space is low dimensional. In contrast, magnitude proportionality seems a bit less convincing since the improvements by input normalization are there even for bounded activation functions. I am not sure if using magnitude proportionality as motivation is actually helping.\n\nRegarding output encoding, I understand the motivation of the encoding and I thank the authors for the explanations. I agree that output encoding changes the prior of the learning problem. I still remain uncertain about the existence of a connection between magnitude proportionality and output encoding.  The input and output encoding modifications could have been two different papers with minimal overlap beyond hypernetwork basics.\n\nAll in all, I feel the pitch of this work needs improvement. But I lean towards acceptance so I increase my rating to a 6."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4268/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700676490589,
                "cdate": 1700676490589,
                "tmdate": 1700676490589,
                "mdate": 1700676490589,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]