[
    {
        "title": "Large Language Models as superpositions of cultural perspectives"
    },
    {
        "review": {
            "id": "MzDh7uJCEU",
            "forum": "1FWDEIGm33",
            "replyto": "1FWDEIGm33",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9417/Reviewer_Fkgw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9417/Reviewer_Fkgw"
            ],
            "content": {
                "summary": {
                    "value": "In general discourse surrounding the rise of LLMs, it is common to ascribe individual characteristics to LLMs. This paper challenges this tendency suggesting that it is more evident to view LLMs as a superposition of perspectives instead of as individuals. The paper provides empirical demonstrations to make this point. In particular, the experiments included show that LLM responses are context dependent in ways that differ from humans. The paper calls into question the use of psychological questionnaires to examine LLMs. The main contribution is the introduction of \"perspective controllability\" and an empirical demonstration to probe whether LLMs are robust to perspective shift effects and how different LLMs compare in terms of their perspective controllability."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "(1) The experiments seem thorough and the paper states that reproducibility and transparency has been a priority."
                },
                "weaknesses": {
                    "value": "(1) The framing of the paper needs significant improvement. The argument seems to go something like this: LLMs tend to be context-dependent. Humans tend to be stable across contexts. Therefore, LLMs should not be assumed to be human-like. Probing an LLM with questions derived from a field that assumes a human subject is flawed. Making general conclusions from results based on these questions is also flawed. LLM as a superposition not an individual is proposed is a new metaphor. This new metaphor motivated the study of perspective change in LLMs, which is the focus of this paper.\n\nNotice there are multiple jumps in this line of argument. First, the fact that LLMs are context-dependent needs to be reconnected to the point about LLMs being seen as individuals. You do not need to provide evidence that LLMs are not human-like to make this point. Second, the paper briefly argues that probing an LLM with questions derived from psychology is problematic but this point is not properly fleshed out or supported directly by results. Third, the point that general scientific conclusions are therefore problematic has not been properly made. Fourth, the reference to quantum mechanics is an interesting inspiration for said metaphor but is not a sound analogy in that language models do not operate in the quantum regime. Further, this inspiration is not necessary to make the argument laid out in this paper. Fifth, the main final point which is that studying perspective change in LLMs to study induction techniques is disconnected from the rest of these points and could stand as an interesting topic in itself.\n\n(2) Conclusions are overstated. The paper states \"we will see that discarding the old metaphor may question the interpretation of recent studies aiming at characterizing the values, personality traits, social skills or moral values of LLMs using tools developed to measure attributes of human psychology\". The current status of the argument has not led to this conclusion directly. The paper needs to reconnect and build out a cohesive careful argument in order to support this claim.\n\n(3) Exposition could be greatly improved throughout for clarity and precision. For instance, the related works section is written as part of the argument that recent work uses \"LLM as an individual\" metaphor, which should be discussed as such. The paper states \"There has been a lot of research studying large language models using tools from psychology...\" before the paper has fully developed the argument for what it means to view \"LLM as an individual\". It is more standard to use the related works section to contextualize this work in reference to existing literature not necessarily to support the content of your argument. Further, the paper states \"All these works aim to make general conclusions about LLMs\nbehavior, personality, or abilities, but they do not explore how personality traits expressed through\nbehaviour can change in unexpected ways over diverse unrelated contexts.\" which seems to say that the difference is in the focus on changes due to unrelated contexts. It would have been clearer to simply state that this work is related to other studies of personality traits but diverges in its focus on context-based shifts in performance. But for some reason, the section is written in a way that requires the reader to parse this out. \"At first glance, these might seem like examples of the unexpected perspective\nshift effect, however these effects are both common in humans, and their effect on the perspective\nchange is intuitive.\" This sentence is unclear and way too dense. And again, not exactly positioned properly in related work section if the function is to be part of the overall argument of the paper. The section continues with \"The second part of our paper studies how models\u2019 values and personality expression can be controlled, i.e., the expected perspective shifts due to context changes.\" This marks a shift in tone where the section is now describing the paper instead of the related work. Again, a sign of expository improvement needed.\n\n(4) The main focus is not clearly defined. The first sentence in the methods section states, \"This paper aims at uncovering the existence of unexpected perspective shift effects i.e. how context can impact the values and personality traits expressed by LLMs in unwanted, unexpected ways\". This is the definition provided. It is unfortunately unclear."
                },
                "questions": {
                    "value": "(1) What is the technical definition of \"unexpected perspective shift effects\"?\n\n(2) How do you distinguish between expected and unexpected? Expected by whom?\n\n(3) What is the technical definition of \"perspective controlability\"?\n\n(4) What is the theoretical basis for equation (1) ?\n\n(5) How does this compare to other measures of predictive inconsistency? Why is \"context\" so specifically interesting in this paper?\n\n(6) How do you define induced perspective? Can you offer theoretical analysis to support your measure?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9417/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698688639435,
            "cdate": 1698688639435,
            "tmdate": 1699637186829,
            "mdate": 1699637186829,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qAbeQuVuc6",
                "forum": "1FWDEIGm33",
                "replyto": "MzDh7uJCEU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9417/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9417/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Fkgw (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their response and for acknowledging the thoroughness of our experiments. We address the reviewer's comments below.\n\n---\n\n\"\"\"\n\nThe argument seems to go something like this: LLMs tend to be context-dependent. Humans tend to be stable across contexts. Therefore, LLMs should not be assumed to be human-like. Probing an LLM with questions derived from a field that assumes a human subject is flawed. Making general conclusions from results based on these questions is also flawed. LLM as a superposition not an individual is proposed is a new metaphor. This new metaphor motivated the study of perspective change in LLMs, which is the focus of this paper.\n\n\"\"\"\n\nOur argument is much more straightforward (human-likeness is irrelevant). Psychological questionnaires are used on humans to make general conclusions (e.g. this person has that value profile). This is valid because humans tend to have stable value profiles (and those questionnaires assume that). The same is not valid for LLMs because they are highly context-dependent (which we show), i.e. the assumption underlying the questionnaires is broken.\n\nWe do not argue that using psychological tools for LLMs is flawed in itself, rather that it is not as straightforward. We argue that making general conclusions (like this model has that personality or ability) about highly context-dependent systems (LLMs) from a single context is flawed. \n\nWe believe that papers using questionnaires on LLM are both relevant and interesting, but that their conclusions can be greatly improved by analyzing how the expressed values/knowledge/abilities change along contexts. \nFor example, in some use cases one could prefer to use a model that has more stable values along many contexts to a model that has a more preferable value profile which was only tested in one context. And currently, we lack studies on the former.\n\n---\n\n\"\"\"\n\nFirst, the fact that LLMs are context-dependent needs to be reconnected to the point about LLMs being seen as individuals. You do not need to provide evidence that LLMs are not human-like to make this point.\n\n\"\"\"\n\nThe \u201cLLM as an individual\u201d metaphor implies stability over contexts, because individuals have been shown to exhibit high value stability.\n\n---\n\n\"\"\"\n\nSecond, the paper briefly argues that probing an LLM with questions derived from psychology is problematic but this point is not properly fleshed out or supported directly by results.\nThird, the point that general scientific conclusions are therefore problematic has not been properly made. \n\n\"\"\"\n\nWe empirically show how questionnaires give different conclusions based on trivial changes in context. This provides direct evidence against forming general conclusions about LLMs from single-context questionnaires.\n\n---\n\n\"\"\"\n\nFourth, the reference to quantum mechanics is an interesting inspiration for said metaphor but is not a sound analogy in that language models do not operate in the quantum regime. Further, this inspiration is not necessary to make the argument laid out in this paper. \n\n\"\"\"\n\nThe introduction of superpositions is meant merely as an analogy and a metaphor.\nWe do not claim that our metaphor is necessary to make the argument, rather the metaphor is introduced merely to better explain our argument in another intuitive way. The \u201cindividual as LLMs\u201d metaphor is questioned, hence we suggest a new one which more aligned to our findings.\n\n---\n\n\"\"\"\n\nFifth, the main final point which is that studying perspective change in LLMs to study induction techniques is disconnected from the rest of these points and could stand as an interesting topic in itself.\n\n\n\"\"\"\n\nThe paper studies how value profiles change based on context. This implies that we should study how this change can happen in unexpected ways (due to trivial changes in the context), but also in expected ways (by explicitly instructing the model to express some values).  We focus on the former in the first part of the paper, and on the latter in the second part."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9417/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700678481696,
                "cdate": 1700678481696,
                "tmdate": 1700678481696,
                "mdate": 1700678481696,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pfCqf1kmPM",
            "forum": "1FWDEIGm33",
            "replyto": "1FWDEIGm33",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9417/Reviewer_7GoP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9417/Reviewer_7GoP"
            ],
            "content": {
                "summary": {
                    "value": "The authors make the case that LLMs show a sort of superposition of cultural perspectives, since their outputs, as measured by standard tests widely change according to the input. The authors consider value and personality tests in order to measure different \"cultural perspectives\" in LLMs. One of the goals of the authors is to measure the consistency of the outputs of the LLMs in an experimentally sound way. The evaluation is carried on several LLMs."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ The paper is indeed timely, there is a lot of interesting ideas to explore around LLMs and this is indeed a good example of an interesting paper in the area."
                },
                "weaknesses": {
                    "value": "- The study that the authors carried out is indeed interesting, but unfortunately, it seems to me that the actual assessment of the results is somehow \"hyped\": at the end these are probabilistic models highly dependent on the prompt and it is somehow expected that they exhibit a variety of personal values, cultural values, personality traits. The authors highlight the fact they observe \"unexpected perspective shift effects\". However, in my opinion, it would be more surprising to see consistency. \n- It is very difficult to understand which inputs led to a change of perspectives. In my opinion, this is a key problem of the paper since small variations might have a significant effects on the outputs. Also, for this reason, it is very difficult to judge the actual consistency of the outputs of the LLMs in the experiments carried out by the authors.\n- Superposition is a wrong term in my opinion given the probabilistic nature of LLMs. In fact, even the same input might lead to different outputs.\n- The term controllability appears to me inappropriate, since the authors are not measuring actual \"controllability\" of the outputs in my opinion.\n- The selection and the analysis of the application of the induction methods are not completely clear, especially with respect to the underlying research hypotheses at the basis of the study design."
                },
                "questions": {
                    "value": "- What is the exact definition of cultural perspective you consider in the paper? What is the relation between cultural perspective and personal values?\n- Which kind of inputs did you use for measuring the change of perspectives? (the supplementary material does not consider sufficient material for reproducibility in my opinion).\n- It seems that the authors report the fact that LLMs are not \"coherent\" as the key finding of their paper. Indeed, it is always good to see measurement studies, but the reviewer wonders if this can be considered as something unexpected. After all, the models are trained on a variety of sources. Were the authors expecting a different result? \n- Do you have any data about the influence of the training datasets on the experimental results that you showed in this paper?\n- Can you discuss Formula (1) in details? How do you analyze the outputs of the LLMs? How do you calculate the mean in this formula?\n- It would be good to know the reasoning beyond the selection of the term \"controllability\". This appears an unusual choice for the phenomena you study in this paper.\n- Can you please discuss the effects of the induction methods in relation to their effects on the outputs of the LLMs?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9417/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698878911095,
            "cdate": 1698878911095,
            "tmdate": 1699637186722,
            "mdate": 1699637186722,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "t450Nyui1t",
                "forum": "1FWDEIGm33",
                "replyto": "pfCqf1kmPM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9417/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9417/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to 7GoP"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their comment and for finding the paper interesting and timely. We address the reviewer's comments below.\n\n------------------------------------\n\n\"\"\"\n\nthe actual assessment of the results is somehow \"hyped\": at the end these are probabilistic models highly dependent on the prompt and it is somehow expected that they exhibit a variety of personal values, cultural values, personality traits. The authors highlight the fact they observe \"unexpected perspective shift effects\". However, in my opinion, it would be more surprising to see consistency.\n\n\"\"\"\n\nWe agree that it is not surprising to see drastic, unexpected context-based changes in LLMs. Indeed, this is the underlying intuition of this paper. However, we believe that this view, and its fundamental implications, are not globally shared by the AI scientific community.\nThis is evidenced by the body of research aiming to make general conclusions about LLMs\u2019 capabilities/personality/knowledge from psychological questionnaires.\n\nOn a more general note, we believe that it is not sufficient to only be aware of this. Rather, it is also important to study models in terms of their robustness to the perspective shift effect. For example, in some use cases one would prefer to use a model that has more stable values along many contexts to a model that has a more preferable value profile but which was only tested in one context. And currently, we lack studies comparing the stability of models along contexts.\n\nDuring the rebuttal, we aimed to further address this issue by providing such a comparison. We define metrics based on three types of value stability from psychology literature (mean-level, rank order, and ipsative) and systematically compare models that were more controllable in Table 1. Please refer to appendix D for these systematic experiments, and appendix C for a detailed analysis of ChatGPT on the three types of stability with respect to changes observed in previous human studies.\n\n--------------------------------------------\n\n\"\"\"\n\nIt is very difficult to understand which inputs led to a change of perspectives. In my opinion, this is a key problem of the paper since small variations might have a significant effects on the outputs. Also, for this reason, it is very difficult to judge the actual consistency of the outputs of the LLMs in the experiments carried out by the authors.\n\n\"\"\"\n\nIn our experiments, we evaluate each perspective along 50 permutations in the order of suggested answers. We therefore show that the induced value profile is consistent along permutations. This is further backed by the statistical analysis, which shows that the value profiles are consistent and different.\n\nOn a more general note, the difficulty of predicting which inputs will cause which value profiles to form is the point of this paper (as they will form in unexpected ways). This highlights the problem of evaluating LLMs from a single context.\n\n---------------------------------------\n\n\"\"\"\n\nThe term controllability appears to me inappropriate, since the authors are not measuring actual \"controllability\" of the outputs in my opinion\n\n\"\"\"\n\nIn the second part of the paper we turn towards \u201cexpected\u201d changes, i.e. how we can intentionally induce a value profile. Here, we explicitly define the target values for the model to express. We refer to a form of conditional controllability where we control the model through its inputs.\n\n--------------------------------------------------\n\n\"\"\"\n\nThe selection and the analysis of the application of the induction methods are not completely clear, especially with respect to the underlying research hypotheses at the basis of the study design.\n\n\"\"\"\n\nIn the paper, we argue that there are two types of changes, \u201cunexpected\u201d and \u201cexpected\u201d. Unexpected changes refer to those which are hard to predict and which we would ideally like to remove from all models. Expected changes refer to the ability of a model to be controlled, and these changes should be removed or not depending on the use case or even depending on the induction method (e.g. 2nd/3rd person, user/system message). For example, in some use case, one could prefer a model that is highly controllable by the system message and not controllable by the user message. We believe that models should be compared on their susceptibility to both types of changes.\nThe reviewer\u2019s comment refers to the second part of the paper, where we turn to \u201cexpected\u201d changes. There we outline the standard ways one would consider defining the target values (e.g. system message or user message)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9417/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700677779504,
                "cdate": 1700677779504,
                "tmdate": 1700677779504,
                "mdate": 1700677779504,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PMFn1oXv5c",
            "forum": "1FWDEIGm33",
            "replyto": "1FWDEIGm33",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9417/Reviewer_P79N"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9417/Reviewer_P79N"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the ability of language models to answer psychological questionnaires. Past research has used these tests designed for humans to try to probe LLMs. The study tests model robustness in answering questionnaires under different contexts or conditions (e.g. writing code, prefixing with a random wikipedia page) which are unrelated to the question in the questionnaire and observe there are significant changes in responses. Further, the paper introduces the notion of perspective controllability and aims to test which models can be guided to answer questions in a certain way.\n\nThe conclusions are that LLMs are unreliable in answering trait questions, with unrelated perturbations leading to different results and that most models are not controllable, albeit some models exhibit some degree of controllability."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Sound methods for statistical analysis of results.\n\nCreative approaches to test robustness of models.\n\nAdequately challenges the assumption of 'LLMs as individuals' for measuring traits."
                },
                "weaknesses": {
                    "value": "Primarily, I think the model needs to have more robust results to understand better what and which types of models behave in different ways. Namely:\n\nThe first experiment (Section 4.1) is only performed using a single model (ChatGPT).\n\nThe models can be better selected for experimentation to facilitate understanding the machanisms that lead to consistent or inconsistent results in Table 1. I think the key comparison directions could be along these axes: base model, models from the same series and different size, base vs. chat. vs. instruct vs. RLHF.\n\nI think the experiments lead into another metaphor than 'superposition of cultural perspectives'. For a perspective to hold, it would have to be consistent across inputs i.e. to produce consistent results when conditioned in the same way. The results show that the conditioning changes results in unexpected and inconsistent ways. Hence, my conclusion from these experiments would be that LLMs lack awareness or knowledge of a perspective.\n\nIn general, I consider using questionnaires about traits is a bit tricky or ill posed in this context. The questionnaires for traits are usually built as a proxy for behaviors e.g. 'make friends easily' loads on the intra/extraversion scale; so it would be perhaps more suitable (and robust?) to have these framed as test on a behavior e.g. at a party where you don't know anyone and some one is sitting also alone, do you approach to strike up a conversation with them? (yes - more likely extravert, no - more likely intravert).\n\nAnother aspect worth mentioning is that in addition to the test-retest validity which is brought up in the paper as stability over different ways of providing context before asking the questionnaire question, one could also measure the variance inside each questionnaire, as multiple questions load on the same factor and the variance across these should also be low by design (i.e. people would respond to questions about extraversion similarly)."
                },
                "questions": {
                    "value": "NA"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9417/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9417/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9417/Reviewer_P79N"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9417/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698994512911,
            "cdate": 1698994512911,
            "tmdate": 1699637186602,
            "mdate": 1699637186602,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sE6OLKRUiB",
                "forum": "1FWDEIGm33",
                "replyto": "PMFn1oXv5c",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9417/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9417/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to P79N (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their comments and for acknowledging our methodology and challenge of the \u2018'LLMs as individuals' assumption. We address the reviewer's comments below.\n\n--------------------------------------------------------------\n\n\"\"\"\n\nThe first experiment (Section 4.1) is only performed using a single model (ChatGPT).\n\n\"\"\"\n\nWe focused on one model in this first set of experiments because this enabled us to analyze the effect in more detail (with respect to the page limit). We studied this effect in ChatGPT on 9 scenarios (3 types of changes (conversations, formats, Wikipedia)  x 3 questionnaires) and with detailed statistical analysis.  For that purpose, we chose ChatGPT as it was among the most controllable models in Table 1.\n\nHowever, we agree that systematically comparing various models on their susceptibility to this effect is a relevant addition, and we thank the reviewer for this suggestion. \nDuring the rebuttal, we defined aggregated metrics based on three types of value stability from psychology literature (mean-level, rank order, and ipsative) and systematically compared models which were more controllable in Table 1.\nWe compared three RLHF models (GPT-3.5-0613, GPT-3.5-0301, and OpenAssistant), two instruction fine-tuned models (Upstage LLaMa 1 and 2), and one DPO fine-tuned model (Zephyr). All these are open-sourced, except for the two GPT models. The most stable models for mean-level, rank-order and ipsative stability were OpenAssistant, GPT-3.5-0613 and GPT-3.5-0301 respectively. These results also imply that models vary in terms of the type of stability they exhibit. For example, GPT-3.5-0301 shows higher rank-order and ipsative stability, but lower mean-level stability. Please refer to appendix D for these experiments, and appendix C for a detailed analysis of ChatGPT with respect to the three types of stability with respect to changes observed in previous human studies.\n\n-----------------------------------------------\n\n\"\"\"\n\n\u2026 in Table 1. I think the key comparison directions could be along these axes: base model, models from the same series and different size, base vs. chat. vs. instruct vs. RLHF.\n\n\"\"\"\n\nWe agree that discussion along those axes is relevant. In the paper, we discuss the effect of RLHF by comparing GPT-3.5 from March and from June (see second to last paragraph in section 4.2). Here we observe a shift of controllability from the user message to the system message. Some additional observations, which we added to the paper, can be made from the current choice of models. When comparing GPT-3.5-instruct-0914 to GPT-3.5-turbo-0301/0613 we can see that RLHF appears to greatly increase controllability in this model. Furthermore, when comparing the raw LLama-65B to the instruction fine-tuned one (Upstage-LLama-65b-instruct) we can see that the instruction fine-tuning likewise appears to greatly increases controllability. \n\nA limiting factor for the analysis of model size is the accessibility of sufficiently capable smaller models. For example, the smaller versions of GPT-3 (ada, curie, babbage) all express very low controllability. Similarly, all LLaMa 1 models expresses very low controllability (in the paper, we only show the biggest LLaMa model, but we evaluated smaller ones as well). We believe that this is because they are not able to sufficiently understand the task (values, questions, MCQ format, etc.). However, we believe that experiments on different versions of the newer LLaMa-2 models would be a valuable addition.\n\n--------------------------------------------\n\n\"\"\"\n\nI think the experiments lead into another metaphor than 'superposition of cultural perspectives'. For a perspective to hold, it would have to be consistent across inputs i.e. to produce consistent results when conditioned in the same way. \n\n\"\"\"\n\nThis is indeed the way we approach a perspective in our experiments. We evaluate each perspective along 50 permutations in the order of suggested answers. We provide a statistical analysis, which shows that the value profiles are consistent and different. \nFor instance, we show that a perspective (which is unrelated to values) will induce some value profile that will consistently hold along permutations in the prompt. What is unexpected is the relation of the perspective (e.g. cpp code) to the value change (e.g. decrease in benevolence)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9417/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700677221607,
                "cdate": 1700677221607,
                "tmdate": 1700677221607,
                "mdate": 1700677221607,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wXp9GL22N6",
            "forum": "1FWDEIGm33",
            "replyto": "1FWDEIGm33",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9417/Reviewer_B9Pb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9417/Reviewer_B9Pb"
            ],
            "content": {
                "summary": {
                    "value": "This paper challenges the view of large language models (LLMs) as individuals and proposes a new metaphor: \"LLMs as superpositions of perspectives\". The authors conducted experiments that demonstrate unexpected perspective shifts in personal values, cultural values, and personality traits. LLMs changed their responses depending on contexts, and even context variations not related to the target topics led to significant changes in the values and personality traits they expressed. The authors also compared four different perspective induction methods (prompts) to assess whether they could control the models' perspectives (perspective controllability)."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This paper studies large language models (LLMs), which is a hot topic in the current society.\n- The paper challenges some existing views on LLMs trying to understand them better, giving some warnings of the potential danger of the existing views.\n- The paper explores if the \"perspectives\" could be controlled, by suggesting four induction methods."
                },
                "weaknesses": {
                    "value": "I struggled to understand the importance of this problem, even after reading the paper. It is unclear what the implications and potential applications of this work are. The paper confirms that LLMs do not give consistent responses, and that LLMs are not like humans, as shown in Experiments and discussed in Discussion. However, it is not clear what the paper suggests (besides proposing a new metaphor) and why this is critical."
                },
                "questions": {
                    "value": "1. Could you elaborate on the definition of a perspective in this paper? \"A perspective is conceptualized\nas a context from which a model is required to simulate a behavior\". \n2. it is not clear what the paper suggests (besides proposing a new metaphor) and why this is critical."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9417/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698996162716,
            "cdate": 1698996162716,
            "tmdate": 1699637186492,
            "mdate": 1699637186492,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5zqMwRBsrj",
                "forum": "1FWDEIGm33",
                "replyto": "wXp9GL22N6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9417/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9417/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to B9Pb"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their comments, which we address below:\n\n---------------------\n\"\"\"\n\nit is not clear what the paper suggests (besides proposing a new metaphor) and why this is critical.\n\n\"\"\"\n\nThe motivation and contribution of this paper rests on the body of research using psychology questionnaires to study LLMs. Many of these papers aim to form general conclusions about LLMs based on their responses to those questionnaires. We show how questionnaires\u2019 results drastically change in an unexpected fashion due to trivial (seemingly unrelated) context changes. \n\nThese empirical results have several important implications:\n\n1) They question the generality of conclusions made in the aforementioned papers. \n\n2) This opens up various avenues extending those papers. For example, currently LLMs are compared on what value/knowledge/abilities they express in one context, but a relevant analysis which should be added is also how that value/knowledge/ability expression changes along different contexts. During the rebuttal, we add a systematic comparison of models on three types of value stability (see Appendix D in the new pdf) along context changes that appear orthogonal to values.\n\n--------------------------------------\n\n\"\"\"\n\nCould you elaborate on the definition of a perspective in this paper? \"A perspective is conceptualized as a context from which a model is required to simulate a behavior\"\n\n\"\"\"\n\n\nThe notion of perspective we describe is similar, yet more general, to the notion of \u201crole-playing\u201d. In role-playing, one uses a prompt to provide context, enabling the LLMs to generate text as if it was playing the role of a character in a particular situation. So here the LLMs acts by taking the perspective of the character in this particular situation. However, contexts do not necessarily describe a \u201ccharacter\u201d, i.e. features of an individual: it could only describe a situation, use a particular kind of language, or particular objects or pieces of information. We use \u201cperspective\u201d to speak about this form of non-human-like role play. However, we agree this is only an intuitive concept and metaphor. Also, contrary to the comments of reviewers, it was not our primary intention to show that this metaphor is the only possible one: rather, we aim to show that methods used in many papers studying the capabilities of LLMs using psychology questionnaires provide results that should call for very careful interpretations, and we propose extensions of these methods to have a better picture of the properties of LLMs. We propose the metaphor to better explain our argument."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9417/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700676710134,
                "cdate": 1700676710134,
                "tmdate": 1700676710134,
                "mdate": 1700676710134,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]