[
    {
        "title": "Efficient Precision and Recall Metrics for Assessing Generative Models using Hubness-aware Sampling"
    },
    {
        "review": {
            "id": "dylTlAjFhU",
            "forum": "W6xD7K1ajR",
            "replyto": "W6xD7K1ajR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2573/Reviewer_KcFU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2573/Reviewer_KcFU"
            ],
            "content": {
                "summary": {
                    "value": "Drawing from the two redundant problems of Kynk\u00e4\u00e4nniemi et al. (2019) that employing representative subsets of generative and real samples would give the similar results as standard Precision and Recall (P&R) ratio, and empirical observations of the dataset that 1) samples of similar hubness values have the similar ratios of 1 vs. 0 in P&R, and 2) phi^prime with high hubness values are enough for manifold identification, the authors propose a method using subsets of generative and real samples with certain hubness criterion in conjunction with approx. k-NN to reduce time and space complexity."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The observations in Fig. 1 and 2 are intriguing.\nThe authors dissected ratio and manifold identification as separate components and conducted well-supported experiments.\nThe results are promising."
                },
                "weaknesses": {
                    "value": "The observations in Fig. 1 and 2 are highly empirical while they serve as necessary foundations of the method."
                },
                "questions": {
                    "value": "The description of Fig. 2 is confusing. For example, \"Hubness\" and \"non_hubness\" are only explained the the main text not in the description of the figure. And I cannot understand \"the times a sample is included in the k-NN hypersphere of a sample of the other distribution, i.e., valid \u03c6\u2032 (FFHQ)\".\nPlease add theoretical analysis of the interesting observations in section 4.2.\nA brief introduction of approximate k-NN method would be helpful (but since I am not an expert in this filed, it depends on you).\nSince the observations are highly empirical, could you add more experiments about t choice (experiments in table 5)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2573/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2573/Reviewer_KcFU",
                        "ICLR.cc/2024/Conference/Submission2573/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2573/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698789301329,
            "cdate": 1698789301329,
            "tmdate": 1699902866205,
            "mdate": 1699902866205,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jCp4HH1yDh",
                "forum": "W6xD7K1ajR",
                "replyto": "dylTlAjFhU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2573/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2573/Authors"
                ],
                "content": {
                    "title": {
                        "value": "[Friendly reminder] There might be some technical issue with openreview submission system, as the review doesn't seem to be for our paper"
                    },
                    "comment": {
                        "value": "Dear Reviewer KcFU,\n\nWe would like to thank you for taking the time to review our paper. However, it appears there may have been a technical issue with the openreview system as the attached review does not seem to be for our submitted work. We would be very grateful if you could please take a look again and upload the intended review for our paper. \n\nWe look forward to receiving your feedback on our work and addressing any concerns you may have. Please do not hesitate to contact us if you need any clarification or have any other questions. We greatly appreciate you taking the time to provide your expert assessment of our research.\n\nThank you in advance and best regards,\n\nThe authors"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2573/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699866909133,
                "cdate": 1699866909133,
                "tmdate": 1699866909133,
                "mdate": 1699866909133,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vuKK81k4ap",
                "forum": "W6xD7K1ajR",
                "replyto": "jCp4HH1yDh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2573/Reviewer_KcFU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2573/Reviewer_KcFU"
                ],
                "content": {
                    "title": {
                        "value": "Updated"
                    },
                    "comment": {
                        "value": "I genuinely apologize for the confusion."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2573/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699902948447,
                "cdate": 1699902948447,
                "tmdate": 1699902948447,
                "mdate": 1699902948447,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7w6XDW47TC",
                "forum": "W6xD7K1ajR",
                "replyto": "dylTlAjFhU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2573/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2573/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' Response"
                    },
                    "comment": {
                        "value": "Thank you for your comments and we hope our responses address your concerns.\n\n> **Q1:** The observations in Fig. 1 and 2 are highly empirical while they serve as necessary foundations of the method.\n\n**Re:** As we responded to Reviewer SeVE (Q3), \nwe acknowledge the comment that our contributions are currently empirical in nature. While developing theoretical understanding is important, we believe there is also value in empirical studies exposing real-world phenomena about deep learning systems and their applications. As evidenced by the rapid practical progress in deep learning, empirical findings have driven advancement even when lacking theoretical grounding initially.\nHowever, we agree formalizing the observed trends is an interesting direction and leave it to future work.\n\n> **Q2:** The description of Fig. 2 is confusing. For example, \"Hubness\" and \"non_hubness\" are only explained the the main text not in the description of the figure. And I cannot understand \"the times a sample is included in the k-NN hypersphere of a sample of the other distribution, i.e., valid \u03c6\u2032 (FFHQ)\". Please add theoretical analysis of the interesting observations in section 4.2. A brief introduction of approximate k-NN method would be helpful (but since I am not an expert in this filed, it depends on you). Since the observations are highly empirical, could you add more experiments about t choice (experiments in table 5).\n\n**Re:** Thank you for your suggestions. We have i) added an explanation of \"Hubness\" and \"non_hubness\" in the figure description; ii) added a brief introduction of the approximate k-NN algorithm in Sec. C (appendix) of the revised paper; iii) added more experiments about $t$ choices in Table 15 Sec. A.6 (appendix) of the revised paper.\n\nFor the meaning of \"the times a sample is included in the k-NN hypersphere of a sample of the other distribution, i.e., valid \u03c6\u2032 (FFHQ)\", we have included an illustration figure and relevant discussions in Sec. D (appendix) of the revised paper. Unfortunately, we are unable to provide a theoretical analysis at this stage, as the hubness phenomenon is still largely an empirical observation, with limited theoretical understanding developed so far."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2573/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700378805390,
                "cdate": 1700378805390,
                "tmdate": 1700378805390,
                "mdate": 1700378805390,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "G42pLWdX7O",
                "forum": "W6xD7K1ajR",
                "replyto": "7w6XDW47TC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2573/Reviewer_KcFU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2573/Reviewer_KcFU"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for response and explaining. As you said you can't give a theoretical basis for your claims, and there's no guarantee these properties apply to every dataset. Using this method might affect metric calculations, potentially misleading researchers. Since you didn't solve my concern, my score remains unchanged. I appreciate your efforts and the new findings."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2573/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700431727794,
                "cdate": 1700431727794,
                "tmdate": 1700431727794,
                "mdate": 1700431727794,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3nuzuvI1D6",
            "forum": "W6xD7K1ajR",
            "replyto": "W6xD7K1ajR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2573/Reviewer_SeVE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2573/Reviewer_SeVE"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses the problem of efficiently assessing generative models on their precision and recall. Intuitively, precision of a generative model measures the quality of samples produced, and recall measures the coverage or diversity of the samples. Unlike scalar evaluation metrics like inception score and FID, computing precision and recall is much more computationally intensive (quadratic complexity in samples, as opposed to linear) because of the need for measuring pairwise distances between the samples. This paper exploits the \"hubness\" property of high-dimensional datasets to speed up the computation of precision and recall. \n\nTo estimate precision and recall of a model with output distribution $\\hat p$ against a true distribution $p$, we need to estimate how much of $p$  is covered by $\\hat p$ and vice-versa. A popular way to do this (proposed by Kynka\u00a8anniemi et al 2019) is by measuring how many samples of $p$ fall within the support of $\\hat p$ where the support is approximated by a union of hyperspheres centered around samples from $\\hat p$ with radii being the distance to kth nearest neighbors. (There are other ways to estimate precision and recall, for example, Simon et al 2019 use a discriminator to classify samples from both distributions, but this paper focuses on the Kynka\u00a8anniemi et al method.) The hubness phenomenon results in a few samples from both $p$ and $\\hat p$ to be the most popular nearest neighbors to almost all points. This paper exploits this by first using a linear time algorithm to find these \"hubs\" and then use these to compute P&R.\nThrough experiments, the paper demonstrates the savings in compute and storage, as compared to Kynka\u00a8anniemi et al P&R evaluation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper well written, and explains the proposed method clearly.\n- The experiments convincingly demonstrate savings on compute time and storage for real world datasets, across a variety of model architectures.\n- The ablation study and the experiment on robustness against the truncation trick are a nice addition to the experiments section."
                },
                "weaknesses": {
                    "value": "- The proposed speedup is specific to one particular way of P&R estimation i.e. using the Kynka\u00a8anniemi et al 2019 method based on nearest neighbors. This method only gives two scalar values corresponding to P and R. In contrast, Simon et al. ICML 2019 method gives the whole PR curve. \n- The proposed method seems to work well when the P&R values are \"reasonably good\" i.e., away from 0 and 1. It is not clear how well the method works in corner cases. It would be good to check this with toy experiments on high dimensional Gaussian mixtures for which P&R take corner values close to 0 and 1 also. \n- Although the experiments are convincing, no theoretical guarantees are provided that bound the approximation error."
                },
                "questions": {
                    "value": "- As I stated above, it would be interesting to see how well the proposed approximations hold up on models with relatively poor P&R, not just good models. It would be good to check this with toy experiments on high dimensional Gaussian mixtures for which P&R take corner values close to 0 and 1 also."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2573/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698837721952,
            "cdate": 1698837721952,
            "tmdate": 1699636194230,
            "mdate": 1699636194230,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NKZOcMxKSM",
                "forum": "W6xD7K1ajR",
                "replyto": "3nuzuvI1D6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2573/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2573/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' Response"
                    },
                    "comment": {
                        "value": "Thank you for your comments and we hope our responses address your concerns.\n\n> **Q1:** The proposed speedup is specific to one particular way of P&R estimation i.e. using the Kynka\u00a8anniemi et al 2019 method based on nearest neighbors. This method only gives two scalar values corresponding to P and R. In contrast, Simon et al. ICML 2019 method gives the whole PR curve.\n\n**Re:** Thank you for your suggestion, since our method is based on [1], we follow it and have included the P&R curves against the parameter of the truncation trick in Fig. 3, Sec. A.3 (appendix) of the revised paper.\nThe results show that our method approximates the original P&R curves well on both FFHQ and LSUN-Church datasets. \n\n[1] Kynk\u00e4\u00e4nniemi, T., Karras, T., Laine, S., Lehtinen, J. and Aila, T., 2019. Improved precision and recall metric for assessing generative models. Advances in Neural Information Processing Systems, 32.\n\n> **Q2:** The proposed method seems to work well when the P&R values are \"reasonably good\" i.e., away from 0 and 1. It is not clear how well the method works in corner cases. It would be good to check this with toy experiments on high dimensional Gaussian mixtures for which P&R take corner values close to 0 and 1 also.\n\n**Re:** Good question! As shown in the P&R curves figure newly included in Fig. 3, Sec. A.3 (appendix) of the revised paper, our method also works well in corner cases.\n\n\n> **Q3:** Although the experiments are convincing, no theoretical guarantees are provided that bound the approximation error.\n\n**Re:** We acknowledge the comment that our contributions are currently empirical in nature. While developing theoretical understanding is important, we believe there is also value in empirical studies exposing real-world phenomena about deep learning systems and their applications. As evidenced by the rapid practical progress in deep learning, empirical findings have driven advancement even when lacking theoretical grounding initially.\nHowever, we agree formalizing the observed trends is an interesting direction and leave it to future work."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2573/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700378626435,
                "cdate": 1700378626435,
                "tmdate": 1700378626435,
                "mdate": 1700378626435,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "x61Te1KC7q",
            "forum": "W6xD7K1ajR",
            "replyto": "W6xD7K1ajR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2573/Reviewer_ctE8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2573/Reviewer_ctE8"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents efficient Precision and Recall (eP&R) metrics for evaluating deep generative models trained on large-scale datasets, which provide nearly identical results to the original P&R metrics with less computational costs. The authors propose a hubness-aware sampling method to remove two kinds of calculating redundancy in original P&R metrics. Besides, the efficiency of eP&R is further improved by adopting approximate k-NN methods. Experiments conducted confirm the effectiveness and generalizability of the eP&R metrics."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This work proposes efficient precision and recall (eP&R) metrics for assessing generative models to approximate results as the original P&R metrics with lower consumption in time and space. Specifically, eP&R metrics reduce time complexity from $O(n^2logn)$ to $O(mnlogn)$ and reduce space complexity from $O(n^2)$ to $O(mn)$, where $m$ is less than $n$.\n2. In addition, an approximate k-NN algorithm is employed for the identification of hub samples to further improve the efficiency of eP&R metrics."
                },
                "weaknesses": {
                    "value": "1. The authors indicate in Sec. 4.3 that the numbers of hub samples, i.e. $m_r$ and $m_g$ are far less than the number of samples of original sets, i.e. $n$. However, the specific conditions for the validity of this conclusion are not provided. From the experimental results in Table 2, the ratio of $O(m_r)$ or $O(m_g)$ to $O(n)$ is about 0.6, which is not consistent with the statement $m_r \\ll n, m_g \\ll n$.\n2. In Observation 4.1 and Figure 1, the authors roughly divide hubness values into three groups and claim that samples with similar hubness values are effective representative samples in P&R ratio calculation, which lacks generality and specific analysis. Further illustration is needed to explain why the hubness value split points are chosen as 12 and 24, and whether this observation holds in many other datasets. Observation 4.2 and Figure 2 have the same issue.\n3. In Sec. 4.2, the authors point out the insensitivity of hubness-aware sampling to exact k-nearest neighbor (k-NN) results, which might be confusing since in Table 4, the Precision and Recall change greatly when k is taken from 3 to 10. Therefore, specific mathematical descriptions are required to substantiate this viewpoint.\n4. The font size of the annotations in Figure 1 and Figure 2 is too small to identify clearly. Besides, the explanation for (a) in Figure 2 is unclear, which can be directly replaced by 'hubness' and 'non-hubness'."
                },
                "questions": {
                    "value": "1. In Sec. 4.3 in the third stage of complexity analysis for eP&R, why calculating pairwise distances for samples between $\\Phi_h^{hub}$ and  $\\Phi_r$ instead of  calculating pairwise distances for samples in $\\Phi_h^{hub}$ ?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2573/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2573/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2573/Reviewer_ctE8"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2573/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698845937958,
            "cdate": 1698845937958,
            "tmdate": 1699636194138,
            "mdate": 1699636194138,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8XiRCwfak5",
                "forum": "W6xD7K1ajR",
                "replyto": "x61Te1KC7q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2573/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2573/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' Response"
                    },
                    "comment": {
                        "value": "Thank you for your comments and we hope our responses address your concerns.\n\n> **Q1:** The authors indicate in Sec. 4.3 that the numbers of hub samples, i.e. $m_r$ and $m_g$ are far less than the number of samples of original sets, i.e. $n$. However, the specific conditions for the validity of this conclusion are not provided. From the experimental results in Table 2, the ratio of $O(m_r)$ or $O(m_g)$ to $O(n)$ is about 0.6, which is not consistent with the statement $m_r \\ll n, m_g \\ll n$.\n\n**Re:** We agree that the definition of \"$\\ll$\" is subjective and may cause confusion. However, we hope to clarify that in our work, we used a threshold of $t=3$ (Sec. 5.4, \"Choice of threshold $t$\"), which results in a ratio of $O(m_r)$ or $O(m_g)$ to $O(n)$ of around 0.38 (Table 7 in the supplementary materials). \nWe have removed the \"$\\ll$\" notion and included example ratios in our revised paper.\n\n> **Q2a:** In Observation 4.1 and Figure 1, the authors roughly divide hubness values into three groups and claim that samples with similar hubness values are effective representative samples in P&R ratio calculation, which lacks generality and specific analysis. Further illustration is needed to explain why the hubness value split points are chosen as 12 and 24. Observation 4.2 and Figure 2 have the same issue.\n\n**Re:** First, we provide a further analysis to show that the performance of our eP&R is insensitive to the group split points (e.g., 12 and 24).  \n\n- [Observation 4.1 and Fig. 1]. As shown in (a) and (b) below, the ratios of binary scores are similar for each hubness value, which validates the insensitivity of the choice of group split points.\n\n(a) All 70k images in the FFHQ dataset\n|Hubness Value| 1     | 2     | 3     | 4     | 5     | 6     | 7     | 8     | 9     | 10    | 11    | 12    | 13    | 14    | 15    | 16    | 17    | 18    | 19    | 20    | 21    | 22    | 23    | 24    |\n|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|\n|Binary Score = 1| 9038  | 7489  | 5795  | 4579  | 3525  | 2741  | 2188  | 1842  | 1474  | 1244  | 931   | 818   | 679   | 593   | 508   | 425   | 361   | 305   | 272   | 246   | 200   | 177   | 153   | 160   |\n|All Samples| 13382 | 11292 | 8615  | 6815  | 5250  | 4158  | 3235  | 2664  | 2190  | 1791  | 1389  | 1231  | 1046  | 883   | 776   | 640   | 543   | 469   | 409   | 361   | 291   | 264   | 232   | 230   |\n|Ratio| 0.675 | 0.663 | 0.673 | 0.672 | 0.671 | 0.659 | 0.676 | 0.691 | 0.673 | 0.695 | 0.670 | 0.665 | 0.649 | 0.672 | 0.655 | 0.664 | 0.665 | 0.650 | 0.665 | 0.681 | 0.687 | 0.670 | 0.659 | 0.696 |\n\n\n(b) 70k images generated by StyleGAN2\n|Hubness Value| 1     | 2     | 3     | 4     | 5     | 6     | 7     | 8     | 9     | 10    | 11    | 12    | 13    | 14    | 15    | 16    | 17    | 18    | 19    | 20    | 21    | 22    | 23    | 24    |\n|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|\n|Binary Score = 1| 10030 | 9150  | 7375  | 5858  | 4555  | 3664  | 2952  | 2266  | 1848  | 1515  | 1358  | 1120  | 911   | 800   | 646   | 522   | 456   | 374   | 350   | 306   | 261   | 228   | 187   | 178   |\n|All Samples| 12083 | 10988 | 8833  | 7015  | 5452  | 4402  | 3561  | 2743  | 2237  | 1835  | 1624  | 1344  | 1117  | 992   | 762   | 635   | 539   | 467   | 420   | 379   | 317   | 277   | 217   | 208   |\n|Ratio| 0.830 | 0.833 | 0.835 | 0.835 | 0.835 | 0.832 | 0.829 | 0.826 | 0.826 | 0.826 | 0.836 | 0.833 | 0.816 | 0.806 | 0.848 | 0.822 | 0.846 | 0.801 | 0.833 | 0.807 | 0.823 | 0.823 | 0.862 | 0.856 |\n\n- [Observation 4.2 and Fig. 2] As shown in (a) and (b) below, the ratios of hubness samples increase quickly to 1 with the increase of $|\\phi'|$, which validates the insensitivity of the choice of group split points.\n    \n(a) All 70k images in the FFHQ dataset\n|$\\|\\phi'\\|$| 0     | 1     | 2     | 3     | 4     | 5     | >=6   |\n|-------|-------|-------|-------|-------|-------|-------|-------|\n|Hubness| 11922 | 8625  | 6290  | 4540  | 3516  | 2741  | 10818 |\n|Non-hubness| 13254 | 9089  | 6519  | 4598  | 3537  | 2749  | 10818 |\n|Ratio| 0.900 | 0.949 | 0.965 | 0.987 | 0.994 | 0.997 | 1     |\n\n(b) 70k images generated by StyleGAN2\n|$\\|\\phi'\\|$| 0     | 1     | 2     | 3     | 4     | >=5   |\n|-------|-------|-------|-------|-------|-------|-------|\n|Hubness| 11792 | 6596  | 4136  | 2745  | 1860  | 5531  |\n|Non-hubness| 13482 | 7210  | 4328  | 2788  | 1874  | 5531  |\n|Ratio| 0.875 | 0.915 | 0.956 | 0.985 | 0.992 | 1.000 |\n\nWe have included these Tables and relevant discussions in Table 10 and Table 11 in Sec. A.4 (appendix) of the revised paper."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2573/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700377867446,
                "cdate": 1700377867446,
                "tmdate": 1700377867446,
                "mdate": 1700377867446,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LYqS6cuEFi",
                "forum": "W6xD7K1ajR",
                "replyto": "x61Te1KC7q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2573/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2573/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' Response 2"
                    },
                    "comment": {
                        "value": "> **Q2b:** Whether this observation holds in many other datasets.\n\n**Re:** To demonstrate the generality of these observations, we include the corresponding results on the LSUN-Church dataset below, where our observations still hold. \n\n- [Observation 4.1]:\n\n(a) All 120k images in the LSUN-Church dataset\n|Hubness Value| 1     | 2     | 3     | 4     | 5     | 6     | 7     | 8     | 9     |\n|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|\n|Binary Score = 1| 51045 | 17414 | 6653  | 2906  | 1413  | 658   | 355   | 202   | 108   |\n|All Samples| 75650 | 25836 | 9866  | 4296  | 2074  | 1006  | 562   | 291   | 153   |\n|Ratio| 0.675 | 0.674 | 0.674 | 0.676 | 0.681 | 0.654 | 0.631 | 0.694 | 0.708 |\n\n(b) 100k images generated by StyleGAN2\n|Hubness Value| 1     | 2     | 3     | 4     | 5     | 6     | 7     | 8     | 9     | 10    |\n|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|\n|Binary Score = 1| 24459 | 4658  | 1081  | 405   | 161   | 71    | 35    | 19    | 9     | 6     |\n|All Samples| 55401 | 10564 | 2385  | 957   | 371   | 154   | 81    | 38    | 21    | 12    |\n|Ratio| 0.441 | 0.441 | 0.453 | 0.423 | 0.434 | 0.461 | 0.432 | 0.500 | 0.429 | 0.500 |\n\n- [Observation 4.2]:\n  \n(a) All 120k images in the LSUN-Church dataset\n|$\\|\\phi'\\|$| 0     | 1     | 2     | 3     | 4     | 5     | >=6   |\n|-------|-------|-------|-------|-------|-------|-------|-------|\n|Hubness| 29986 | 18482 | 12819 | 9106  | 6749  | 5037  | 20441 |\n|Non-hubness| 32669 | 19615 | 13346 | 9213  | 6773  | 5040  | 20441 |\n|Ratio| 0.918 | 0.942 | 0.961 | 0.988 | 0.996 | 0.999 | 1.000 |\n\n(b) 100k images generated by StyleGAN2\n|$\\|\\phi'\\|$| 0     | 1     | 2     | 3     | 4     | 5     | 6     | >=7   |\n|-------|-------|-------|-------|-------|-------|-------|-------|-------|\n|Hubness| 15731 | 7063  | 3984  | 2405  | 1505  | 963   | 728   | 2007  |\n|Non-hubness| 17590 | 7821  | 4158  | 2443  | 1513  | 965   | 729   | 2007  |\n|Ratio| 0.894 | 0.903 | 0.958 | 0.984 | 0.995 | 0.998 | 0.999 | 1.000 |\n\nWe have included these Tables and relevant discussions in Table 12 and Table 13 in Sec. A.4 (appendix) of the revised paper.\n\n> **Q3:** In Sec. 4.2, the authors point out the insensitivity of hubness-aware sampling to exact k-nearest neighbor (k-NN) results, which might be confusing since in Table 4, the Precision and Recall change greatly when k is taken from 3 to 10. Therefore, specific mathematical descriptions are required to substantiate this viewpoint.\n\n**Re:** We believe there is a misunderstanding. In Sec. 4.2, by \"insensitivity\", we meant that our eP&R metric is insensitive to the choice of *exact* vs. *approximate* k-NN algorithms but not the choice of $k$. Thus, it does not contradict the results in Table 4. We have clarified this in Sec. 4.2 of the revised paper.\n\nIn our paper, we use HNSW [1] as the approximate k-NN algorithm, which achieves high accuracies above 99% and has been widely used by the community. Please see [1] for more details about the validity of the algorithm.\n\n[1] Yu A Malkov and Dmitry A Yashunin. Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs. IEEE transactions on pattern analysis and machine intelligence, 42(4):824\u2013836, 2018.\n\n> **Q4:** The font size of the annotations in Figure 1 and Figure 2 is too small to identify clearly. Besides, the explanation for (a) in Figure 2 is unclear, which can be directly replaced by 'hubness' and 'non-hubness'.\n\n**Re:** We have fixed these in the revised paper."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2573/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700378095443,
                "cdate": 1700378095443,
                "tmdate": 1700378095443,
                "mdate": 1700378095443,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "s9HOmo67PY",
                "forum": "W6xD7K1ajR",
                "replyto": "x61Te1KC7q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2573/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2573/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' Response 3"
                    },
                    "comment": {
                        "value": "> **Q5:** In Sec. 4.3 in the third stage of complexity analysis for eP&R, why calculating pairwise distances for samples between $\\mathbf{\\Phi}_r^{hub}$ and $\\mathbf{\\Phi}_r$ instead of calculating pairwise distances for samples in $\\mathbf{\\Phi}_r^{hub}$?\n\n**Re:** Good question! As the table below shows, we calculate the pairwise distances between $\\mathbf{\\Phi}_r^{hub}$ and $\\mathbf{\\Phi}_r$ as it provides lower approximation errors than calculating pairwise distances for samples in $\\mathbf{\\Phi}_r^{hub}$. \nWe conjecture the reason is that $\\mathbf{\\Phi}_r^{hub}$ is much sparser than $\\mathbf{\\Phi}_r$ and thus the pairwise distances for samples in it will be much larger than those of the original P&R, resulting in much larger $k$-NN hyperspheres that increase the approximation error.\nThe same conclusion holds for $\\mathbf{\\Phi}_g^{hub}$ and $\\mathbf{\\Phi}_g$.\n\nWe have included the table and relevant discussions in Table 14, Sec. A.5 (appendix) of the revised paper.\n\n|       | Pairwise distances between $\\mathbf{\\Phi}\\_r^{hub}$ and $\\mathbf{\\Phi}\\_r^{hub}$|       |               |                | Pairwise distances between $\\mathbf{\\Phi}\\_r^{hub}$ and $\\mathbf{\\Phi}\\_r$   | | |                 |\n| ----- | -------------------------------------------------  | ----- | --------------| ---------------|  ---------------------------------  | ------| --------------| --------------|\n| t     | P                                                  | R     | P Error(\\%) | R Error(\\%)  | P                                   | R     | P Error(\\%) | R Error(\\%) | \n| 1     | 0.713                                              | 0.484 | 0.4           | 1.8            | 0.717                               | 0.500 | 0.3           | 0.0           |\n| 2     | 0.723                                              | 0.506 | 1.0           | 2.6            | 0.718                               | 0.501 | 0.3           | 0.0           |\n| 3     | 0.746                                              | 0.534 | 4.2           | 8.3            | 0.719                               | 0.501 | 0.4           | 0.2           |\n| 4     | 0.768                                              | 0.562 | 7.3           | 14.0           | 0.726                               | 0.507 | 1.6           | 0.6           |\n| 5     | 0.787                                              | 0.588 | 9.9           | 19.3           | 0.730                               | 0.515 | 1.9           | 0.6           |"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2573/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700378461212,
                "cdate": 1700378461212,
                "tmdate": 1700378461212,
                "mdate": 1700378461212,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]