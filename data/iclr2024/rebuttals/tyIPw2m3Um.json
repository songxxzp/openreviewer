[
    {
        "title": "Probability-dependent gradient decay in large margin softmax"
    },
    {
        "review": {
            "id": "VDz8b5jYLT",
            "forum": "tyIPw2m3Um",
            "replyto": "tyIPw2m3Um",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1676/Reviewer_kFkh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1676/Reviewer_kFkh"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the impact of adding a $\\beta$ term in the softmax equation. Let $m$ be the number of classes, they propose:\n$$p = \\frac{e^x}{\\sum_i e^{x_i} + \\beta e^x}, \\quad \\text{with } x \\in R^{m}, \\text{and } p \\in R^{m}$$\nWhen used alongside the cross-entropy loss, the formulation of the loss on a training sample $(x,y)$ becomes:\n$$\\ell(x,y) = -\\log(p_y) = - \\log(\\frac{e^{x_y}}{\\sum_i e^{x_i} + \\beta e^{x_y}}) $$\nThey show how $\\beta$ enforces a soft margin and modulates the gradient magnitude depending on the probability $p_y$. More precisely, they show how small $\\beta$s increase the gradient magnitude for larger probabilities---promoting a soft margin---, while larger $\\beta$s reduce the gradient magnitude for larger probabilities. They theoretically derive this observation and validate it empirically on $4$ vision datasets. Moreover, they draw a parallel with curriculum learning and calibration."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "I find this work well motivated: the softmax function is ubiquitous in modern machine learning and studying its various caveats is important. \nThe connection with calibration is interesting, and the results in figure 6 are very promising. Especially, the calibration improves as $\\beta$ increases, which allows the model to be less influenced by current samples having a $p_y$ close to $1$."
                },
                "weaknesses": {
                    "value": "I found two main weaknesses in this work. The first one consists of the overall lack of clarity. I find the paper hard to read. Here are some parts I found confusing:\n- \"MSE takes into account more complex optimization scenarios\": What do you mean by that?\n- \"Hard mining strategy\": you could briefly introduce what this is. \n- in section 2, you talk about $J_j$ before introducing it\n- In figure 3: there are no legends for the top row, and the caption does not help to clarify the different curves being shown, the main text is also unclear about those i.e. what are \"post-training samples\", are those test samples? \n- In figure 3 still, it should be mentioned in the legend or in the caption that the different groups correspond to samples of varying difficulty\n- \"If we make excessive demands on the margin, some post-training samples cannot get any chance and will be \"sacrificed\" according to the soft curriculum learning strategy [...]\", what do you mean? \n- \"So it is convinced that the general softmax [...]\"\n- \"The beta smaller is, the gradient smoother is\"\n- \"Warm-up strategy achieves even better results.\"\n- \"Curriculum design that divides samples is crucial to curriculum learning idea.\"\n- \"Besides, based the previous analysis [...]\"\n- In figure 6: the y axis mentions accuracy and but also shows confidence, this is confusing and could be clarified in the caption. \n\nThe second weakness is the lack of rigor in the experiments: \n- In figure 4: which model is being used? How many parameters? How were the hyperparameters tuned? Are those averaged over multiple seeds, if yes can we see the standard deviation? \n- In table 2: The different values are quite close, and it is difficult to evaluate the robustness of the improvement without standard deviation. It should be possible to run the same experiment with different seeds for some of the smaller datasets. \n- In table 3: same as above, I would love to see standard deviations \n- For all the experiments, which experimental protocol was followed: which architecture, tuning, seeds, optimizer, ... I couldn't find those in the appendix either"
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1676/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1676/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1676/Reviewer_kFkh"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1676/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698766899496,
            "cdate": 1698766899496,
            "tmdate": 1699636095798,
            "mdate": 1699636095798,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RzI3IobHZe",
                "forum": "tyIPw2m3Um",
                "replyto": "VDz8b5jYLT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1676/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission1676/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1676/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer kFkh"
                    },
                    "comment": {
                        "value": "Thank you to the reviewer for taking the time to evaluate our paper and providing constructive the suggestion.\n\nFollowing the reviewer's guidance, we have recognized the poor presentation of our paper. Considering the readability of the paper, we have modified the presentation in section1 and section2 to improve presentation of the paper. In the original version, certain statements were confusing. We have made corresponding revisions to enhance clarity. Additionally, in response to the reviewer's advice, we have supplemented the standard deviation of the results in our experiments. \n\nWe have provided detailed point-to-point responses to the reviewer's inquiries as followings.\n* Q1: \"MSE takes into account more complex optimization scenarios\": What do you mean by that?\n* A1: We have recognized that the meaning of this sentence was ambiguous. In the current version of the paper, this statement has been replaced with an introduction to model calibration, aiming to better elucidate the content presented in the paper.\n\n\u201dHowever, the modern models often demonstrate inadequate confidence calibration in probability distribution through Softmax mapping. Specifically, these probability outputs display unwarranted over-confidence (Guo et al., 2017). Furthermore, researchers have identified that achieving high accuracy in classifiers and calibrating the model confidence are distinct objectives (Wenger et al., 2020). This scenario emphasizes the necessity to study the calibration of model output uncertainties in optimization.\u201d\n\n* Q2: \"Hard mining strategy\": you could briefly introduce what this is.\n* A2: Thank you for the suggestion. We have briefly introduced the hard mining strategy in the third paragraph of Section 1.\n\n* Q3: In section 2, you talk about J_i before introducing it\n* A3: We have noticed the poorly articulated expression in Section 2. In the current version, we have polished Eq. (1)-(6). Furthermore, we have added explanations regarding J in Eq. (1). \n\n* Q4: In figure 3: there are no legends for the top row, and the caption does not help to clarify the different curves being shown, the main text is also unclear about those i.e. what are \"post-training samples\", are those test samples?\n* A4: The term \"post-training samples\" refers to samples with low confidence in the early stages of training. We have revised the confusing expression to use \"challenging samples.\" Additionally, we have updated the captions for each figure to better describe the content of the figures.\n\n* Q5: \"If we make excessive demands on the margin, some post-training samples cannot get any chance and will be \"sacrificed\" according to the soft curriculum learning strategy [...]\", what do you mean?\n* A5: This sentence indicates that certain samples consistently exhibit low confidence when the margin is set too large. We have replaced this confusing statement with the following illustration.\n\n\u201cIf we make excessive demands on the margin, some challenging samples always remain low confidence $\\hat p$ according to the stringent curriculum learning sequence.\u201d\n\n* Q6: \"So it is convinced that the general softmax [...]\"\n* A6: The meaning of this sentence is that Softmax could be characterized by two hyperparameters, $\\tau$ and $\\beta$. In the current version, we have replaced some inappropriate terms, as illustrated by the following sentence:\n\n\u201cSo it is convinced that Softmax cross-entropy could be characterized with these two hyperparameters $\\tau $ and $\\beta$.\u201d\n\n* Q7: \"The beta smaller is, the gradient smoother is\"\n* A7: Regarding the function $J\\left( {{z_c}} \\right)$ presented in equations (9)-(12), we determine that $\\max {\\left| {\\nabla _{{z_c}}^2J\\left( {{z_c}} \\right)} \\right|_2}$ equals a constant when ${p_c} = \\frac{1}{{1 + \\beta }}$. By considering the local range ${p_c} \\in \\left[ {0,0.5} \\right]$, we can derive a local $L$-constraint for ${\\left| {\\nabla _{{z_c}}^2J\\left( {{z_c}} \\right)} \\right|_2}$, as illustrated in equation (18). The original sentence was inaccurately expressed. We have replaced this confusing sentence with the following statement.\n\n\u201cThe $\\beta$ smaller is, L-constrain in the early phase of optimization smaller is.\u201d\n\n* Q8: \"Warm-up strategy achieves even better results.\"\n* A8: As depicted in Figure 4, the warm-up strategy yields higher accuracy compared to a fixed gradient decay coefficient. We have replaced the original confusing sentence with the following statement:\n\n\"As a result, the warm-up strategy achieves higher accuracy compared to a fixed gradient decay coefficient.\""
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1676/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700215270605,
                "cdate": 1700215270605,
                "tmdate": 1700215270605,
                "mdate": 1700215270605,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IQB83WpRpx",
                "forum": "tyIPw2m3Um",
                "replyto": "VDz8b5jYLT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1676/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission1676/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1676/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer kFkh"
                    },
                    "comment": {
                        "value": "* Q9: \"Curriculum design that divides samples is crucial to curriculum learning idea.\"\n* A9: Thank you for the reviewer's thorough examination. The original sentence aimed to convey that the design of the curriculum necessitates distinguishing between easy and challenging samples, which is crucial for the entire curriculum learning. We acknowledge the poor expression in the original sentence and have replaced it with the following clarification:\n\n\u201cThe design of the curriculum, specifically in evaluating the difficulty of individual samples, plays a pivotal role in curriculum learning idea.\u201d\n\n* Q10: \"Besides, based the previous analysis [...]\"\n* A10: The original sentence aimed to convey that these challenging samples may exhibit low confidence when the gradient decay rate is excessively small. We recognize the inaccuracies in the original wording, and in the current version, this confusing sentence has been replaced with the following clarification.\n\n\u201cBased the previous analysis in section 4.2, over-small gradient decay rate may impair the performance since some challenging samples containing important information remain low confidence $\\hat p$.\u201d\n\n* Q11: In figure 6: the y axis mentions accuracy and but also shows confidence, this is confusing and could be clarified in the caption.\n* A11:  Thank you for your suggestions. We have revised the caption of figure 6 to provide more detailed descriptions of their content. Additionally, to better introduce the concept of model calibration, we have incorporated relevant descriptions into section 1 and section 2. The modified caption is presented below:\n\n\u201cConfidence and reliability diagrams with ResNet18 on CIFAR-100. ($bins=10$) In each subplot, the left plot illustrates the sample distribution in individual bins, while the right plot displays the average confidence and accuracy in each bin. Ideally, calibration aims for consistency between accuracy and average confidence in each bin. It indicates that a smaller gradient decay rate $\\beta$ is associated with more pronounced miscalibration of the model, while a larger gradient decay rate mitigates this issue.\u201d\n\n* Q12: In figure 4: which model is being used? How many parameters? How were the hyperparameters tuned? Are those averaged over multiple seeds, if yes can we see the standard deviation?\n* Q13: In table 2: The different values are quite close, and it is difficult to evaluate the robustness of the improvement without standard deviation. It should be possible to run the same experiment with different seeds for some of the smaller datasets.\n* Q14: In table 3: same as above, I would love to see standard deviations.\n* Q15: For all the experiments, which experimental protocol was followed: which architecture, tuning, seeds, optimizer, ... I couldn't find those in the appendix either.\n* A12 to Q12-15: We sincerely appreciate the constructive suggestions from the reviewer. The model employed in Fig. 4 is a three-layer fully connected network. Detailed parameter settings and all experimental details, including architecture, tuning, seeds, and optimizer, are provided in Appendix A.1. To offer more general conclusions, all hyperparameters are set without cumbersome tuning. In addition, all experimental results are averages of five different random seeds and the specific random seeds are given in Appendix A.1. The random seed configurations were consistent across different methods. Furthermore, following reviewer's recommendation, we have included standard deviation in the results of repeated experiments in Tables 2 and 3."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1676/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700215661693,
                "cdate": 1700215661693,
                "tmdate": 1700215661693,
                "mdate": 1700215661693,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iDEwjsqJAe",
                "forum": "tyIPw2m3Um",
                "replyto": "VDz8b5jYLT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1676/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission1676/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1676/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer kFkh"
                    },
                    "comment": {
                        "value": "Dear Reviewer kFkh,\n\nThank you for your valuable feedback on our submission. We have read your comments carefully and have addressed them in our rebuttal. We would be grateful if you could acknowledge if our responses have addressed your comments. Thank you again for your time and consideration.\n\nBest regards,\n\nAuthors of paper 1676."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1676/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700215698023,
                "cdate": 1700215698023,
                "tmdate": 1700215698023,
                "mdate": 1700215698023,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mKf9xp4TYm",
            "forum": "tyIPw2m3Um",
            "replyto": "tyIPw2m3Um",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1676/Reviewer_KFsP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1676/Reviewer_KFsP"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the influence of introducing a margin parameter $\\beta$ in the softmax operator for classification problems. The idea is to relate the margin parameter $\\beta$ to the decay rate of the gradients so that the classification confidence can be manipulated. The margin parameter also gives rise to some improved performance in image data. I would view the margin parameter $\\beta$ as the major contribution of the paper, as existing study often focuses on the temperature parameter."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper provides a very detailed guide to understand the influence of the margin parameter $\\beta$ on the decay rate of the gradient. The study looks comprehensive and correct, which leads to the successful empirical verification.\n\nMost of the paper is well organized, although some part needs additional care."
                },
                "weaknesses": {
                    "value": "Section 2 needs a revision. See more in questions section.\n\nAs far as I can tell, the classification error improvement is a bit marginal. The baseline accuracy should correspond to $\\beta = 1$ and the highlighted best obtained errors may not have significant improvement. Although this evaluation might be objective, but this concern can be partially addressed by providing a standard deviation computed in multiple runs, so that the statistical significance can be verified.\n\nGiven the concerns above, I am giving a negative rating. However, I am willing to discuss with the authors on the significance of the proposed method and potentially raise the score."
                },
                "questions": {
                    "value": "The grammar around Equations (1) -- (6) should be polished.\n\nIn Equation (4), (5) and (6), is there a bracket around $z_i - z_c$?\n\nWhat is hidden in the approximate equality in Equation (5)?\n\nFigure 3 has a vague description: \"confidence of some samples during training\". The font in the figures is small."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1676/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1676/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1676/Reviewer_KFsP"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1676/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698807818555,
            "cdate": 1698807818555,
            "tmdate": 1700698313647,
            "mdate": 1700698313647,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "O1lmYhuLeS",
                "forum": "tyIPw2m3Um",
                "replyto": "mKf9xp4TYm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1676/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission1676/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1676/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer KFsP"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer for taking the time to evaluate our paper and providing constructive the suggestion.\n\n* We appreciate the suggestions and questions raised by the reviewer under \"Weaknesses\" and \"Questions.\" Following the reviewer's guidance, we recognized deficiencies in the presentation in Section 2. We have revised the presentation in Section 2 to better elucidate \"Preliminaries\". Eq. (1) to Eq. (6) have been polished for a clearer presentation.\n\n* Eq. (4), (5), and (6) in the original manuscript have been refined and now appear as Eq. (3) in the current version. This presentation demonstrates an approximation between Softmax cross-entropy and the max function, contributing to a better understanding of the character of the two hyperparameters.\n\n* In addition, we have adjusted the font size of figure 3 and other figures to more clearly convey the meaning of the visuals. Besides, we have revised the caption of figures to provide more detailed descriptions of their content. In experiments, we also present the average results under five different random seeds and the corresponding standard deviations in Tables 2 and 3 to support our conclusions and improve the rigor of the experiment."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1676/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700214336491,
                "cdate": 1700214336491,
                "tmdate": 1700214336491,
                "mdate": 1700214336491,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SJy7A1yC5F",
                "forum": "tyIPw2m3Um",
                "replyto": "mKf9xp4TYm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1676/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission1676/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1676/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer KFsP"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer's willing in discussing the significance of our paper. In response, we would like to provide additional explanations about the significance of our work. \n\nAs mentioned by the reviewer in \u201cWeaknesses\u201d, the warm-up scheme shows an improvement over the previously proposed adjustment for Softmax, but the improvement is not significant. \nFollowing review\u2019s guidance and concerns, we repeated the results more times and presented the mean and standard deviation of the results in Tab.2 and Tab.3 to support our conclusions. Considering that these two approaches address the problem from different perspectives, where A-Softmax statically considers the margin in the output space, while the warm-up scheme dynamically adjusts gradient decay coefficients during the optimization process, we believe that the presentation of such results can provide some new insights.\n\nOn the other hand, our paper also focuses on the uncertainty estimation of the classification results, i.e., whether the model probability outputs can reflect the accuracy prediction in classification. Model calibration is important in practical applications. Perfect calibration of neural network can be realized when the confidence score reflects the real probability that the classification is classified correctly. Formally, the perfectly calibrated network satisfied ${\\rm P}\\left( {\\hat y = y|\\hat p = p} \\right) = p$ for all $p \\in \\left[ {0,1} \\right]$. However, probability outputs of modern model display unwarranted over-confidence. Our work reveals that probability-dependent decay rate is a factor contributing to the over-confidence of modern models in classification. The optimization with a small gradient decay rate presents a more stringent curriculum learning for samples, meaning that challenging samples only gain confidence after easy samples have been sufficiently trained to a certain level of confidence. This leads to different samples having greater distinctiveness in the optimization. Correspondingly, our presented analysis, detailed in Section 3, along with more experimental results shown in Fig. 6, Table 3, and Appendix, demonstrates that a substantial increase in the model-calibrated criterion ECE and MCE can be achieved when employing large probability-dependent gradient decay in modern model. To better present the work in model calibration, we have modified presentation in section 1 and section 2 to emphasize this point.\n\nIn addition to methodological innovations, we think that our work can provide some new insights and understanding, especially concerning concepts such as model calibration, large-margin Softmax, probability-related gradient decay, curriculum learning, and related topics in deep learning optimization."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1676/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700214438428,
                "cdate": 1700214438428,
                "tmdate": 1700214438428,
                "mdate": 1700214438428,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nfl5PdTTft",
                "forum": "tyIPw2m3Um",
                "replyto": "SJy7A1yC5F",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1676/Reviewer_KFsP"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission1676/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1676/Reviewer_KFsP"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Authors,\n\nSorry for a late reply. I really appreciate your detailed response and additional experimental results, as well as the revisions in the paper. With the standard deviation in Table 1 and 2, I can see some improvements are indeed statistically significant and the value of the method is better supported. Therefore, I raised the score to a 5 (in fact should be right on the borderline). The major reason of preventing a clear acceptance recommendation is that the performance improvement is still marginal in places, like CIFAR-10 ResNet18 architecture. The standard deviation well covered the difference between the average classification error."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1676/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700698599985,
                "cdate": 1700698599985,
                "tmdate": 1700698599985,
                "mdate": 1700698599985,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yHqs9qkFAl",
            "forum": "tyIPw2m3Um",
            "replyto": "tyIPw2m3Um",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1676/Reviewer_cHJU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1676/Reviewer_cHJU"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a modification to softmax when using softmax in conjunction with cross-entropy for classification tasks to combat the following problem: when the magnitude of the partial derivative of the softmax with respect to the class outputs decays rapidly, model tends to overfit (but converges faster). On the other hand, if the magnitude of the partials decays too slowly, the model takes a longer time to converge (but tends to generalize better). The modification introduces a hyperparameter $\\beta$ where small $\\beta$ encourages rapid decay of the partials while large $\\beta$ encourages slow decay of the partials. To combine the best of both worlds, the paper proposes a warm-up scheme by starting with a small $\\beta$ so that the model will converge quickly and then increase $\\beta$ to discourage overfitting and overconfidence."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Paper proposes a simple modification to softmax in conjunction with a warm up scheme with respect to the margin parameter $\\beta$ to get faster convergence and better generalization."
                },
                "weaknesses": {
                    "value": "The warm-up scheme does not seem to provide a significant advantage over prior proposed modifications to softmax (e.g A-softmax) or does worse according to table 2 in the paper."
                },
                "questions": {
                    "value": "What does the training loss look like across epochs for the warm-up schedule (more specifically could you plot another curve in figure 5 displaying the loss over epochs for the warm-up schedule)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1676/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698823207145,
            "cdate": 1698823207145,
            "tmdate": 1699636095631,
            "mdate": 1699636095631,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SD1kfchqk8",
                "forum": "tyIPw2m3Um",
                "replyto": "yHqs9qkFAl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1676/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission1676/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1676/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer cHJU"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer for taking the time to review our paper.\n\n* Firstly, in response to the reviewer's queries, we have added supplementary information regarding the warm-up training loss curve in Fig. 5.\n\n* Secondly, considering review\u2019s concerns, we would like to provide additional explanations about the significance of our work.\n\nAs mentioned by the reviewer in \u201cWeaknesses\u201d, the warm-up scheme shows an improvement over the previously proposed adjustment for Softmax, but the improvement is not significant. Addressing review\u2019s concerns about the performance improvement, we repeated the results more times and presented the mean and standard deviation of the results in Tab.2 and Tab.3 to support our conclusions. Furthermore, considering that these two approaches address the problem from different perspectives, where A-Softmax statically considers the margin in the output space, while the warm-up scheme dynamically adjusts gradient decay coefficients during the optimization process. Therefore, we believe that the presentation of such results can provide some new insights.\n\nOn the other hand, our paper also focuses on the uncertainty estimation of the classification results, i.e., whether the model probability outputs can reflect the accuracy prediction in classification. Perfect calibration of neural network can be realized when the confidence score reflects the real probability that the classification is classified correctly. Formally, the perfectly calibrated network satisfied ${\\rm P}\\left( {\\hat y = y|\\hat p = p} \\right) = p$ for all $p \\in \\left[ {0,1} \\right]$. However, probability outputs of modern model display unwarranted over-confidence. Our work reveals that a probability-dependent gradient decay rate is a factor contributing to the over-confidence of modern models in classification. The optimization with a small gradient decay rate presents a more stringent curriculum learning for samples, meaning that challenging samples only gain confidence after easy samples have been sufficiently trained to a certain level of confidence. This leads to different samples having greater distinctiveness in the optimization. Correspondingly, our presented analysis, detailed in Section 3, along with more experimental results shown in Fig. 6, Table 3, and Appendix, demonstrates that a substantial increase in the model-calibrated criterion Expected Calibration Error (ECE) can be achieved when employing large probability-dependent gradient decay in modern model. To better present the work in model calibration, we have modified presentation in section 1 and section 2 to emphasize this point.\n\nIn addition to methodological innovations, our work can provide some insights and understanding, especially concerning concepts such as model calibration, large-margin Softmax, probability-dependent gradient decay, curriculum learning, and related topics in deep learning optimization."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1676/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700214016268,
                "cdate": 1700214016268,
                "tmdate": 1700214016268,
                "mdate": 1700214016268,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]