[
    {
        "title": "SoftTreeMax: Exponential Variance Reduction in Policy Gradient via Tree Expansion"
    },
    {
        "review": {
            "id": "njytNuol9a",
            "forum": "p5jBLcVmhe",
            "replyto": "p5jBLcVmhe",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5083/Reviewer_eGu2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5083/Reviewer_eGu2"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose SoftTreeMax in this paper, which is a new method that aim to mitigate the high sample complexity and large variance of policy gradient methods by employing planning. It extends traditional logits with the multi-step discounted cumulative reward and the logits of future states. It is shown that tree expansion helps reduce gradient variance. The variance decays exponentially with the planning horizon, and the closer the induced transitions are to being state-independent, the faster the decay. With approximate forward models, the resulting gradient bias diminishes with the approximation error while retaining the same variance decay. SoftTreeMax reduces the gradient variance by three orders of magnitude in Atari, leading to better sample complexity and improved performance compared to distributed PPO."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper proposes a novel approach, SoftTreeMax, to mitigate the large variance and high sample complexity for policy gradient methods by leveraging tree expansion and softmax. While there have been related works that study the softmax operation in policy gradient or value-based approaches, SoftTreeMax is unique in its focus on tree expansion to reduce variance. The paper is well-written and easy to follow, with most claims being well-discussed within the paper. The problem of mitigating large variance and high sample complexity for policy gradient methods is a significant challenge in RL, and SoftTreeMax provides a promising solution."
                },
                "weaknesses": {
                    "value": "One weakness of the paper is in its experimental evaluation section. While the paper presents promising results for SoftTreeMax in Atari, some of the claims made are not well-supported. For example, the paper does not include enough baselines to make a fair comparison with SoftTreeMax. This makes it difficult to determine the extent of SoftTreeMax's improvement over existing methods.\n\nAdditionally, the paper lacks in-depth comparison with other related methods. While the paper compares SoftTreeMax with distributed PPO, it does not provide a comprehensive comparison with other state-of-the-art methods in the field. This makes it difficult to determine the generalizability of SoftTreeMax and its performance in comparison to other methods."
                },
                "questions": {
                    "value": "> Policy gradient methods suffer from large variance and high sample complexity. To mitigate this, we introduce \u2014a generalization of softmax that employs planning.\n\nHowever, in the experimental evaluation part, only PPO is used as the baseline algorithm. There have been many efforts to reduce the variance and improve sample complexity for policy gradient methods (e.g., including a baseline). It is therefore better to also compare state-of-the-art approaches that also solve the same problem.\n\n> We do so by sub-sampling only the most promising branches at each level. Limiting the width drastically improves runtime, and enables respecting GPU memory limits, with only a small sacrifice in performance.\n\nDoesn\u2019t this also introduce additional variance by sub-sampling and pruning?\n\n>  For depths $d \\geq 3$, we limited the tree to a maximum width of 1024 nodes and pruned trajectories with low estimated weights.\n\nDoes it suffer from a limitation when used with a larger value of $d$, which may lead to a more significant limitation of the allowed maximum width considering costs?\n\n> Figure 3: Reward and Gradient variance: GPU SoftTreeMax (single worker) vs PPO (256 GPU workers).\n\nDoes it perform more sample-efficient than baseline methods (not compared in terms of final performance or actual wall-clock time)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5083/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698652274501,
            "cdate": 1698652274501,
            "tmdate": 1699636499152,
            "mdate": 1699636499152,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sI3BxwIjNK",
                "forum": "p5jBLcVmhe",
                "replyto": "njytNuol9a",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5083/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5083/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. Please find our answers in the following.\n\n\n**W1: Add baselines \u2026 to determine the extent of SoftTreeMax's improvement over existing methods?** Following this important comment, we now added a SOTA baseline: EfficientZero [Ye et al., 2021]. We chose EfficientZero because it is a highly sample-efficient version of MuZero -- one of the best known RL algorithms today -- and is also open source. Due to the time limitation of the rebuttal, we present the experiments on four games, for a runtime of 96 hours. The results are given here: https://ibb.co/6Zd24yk. As seen in the plots, SoftTreeMax surpasses EfficientZero in all games but one. We will add full experiments in the final version. \n\n\nExperiment details: We used the same hardware to run all experiments, as described in Appendix B.1.  For \u201cSoftTreeMax\u201d, we present the best depth out of those in Appendix B.2, Figure 4. \n\nWe thank the reviewer for their useful suggestion, which shows that there are indeed better baselines than PPO to compare to. With that said, we also recall that the PPO baseline we run here has \u201cenhanced\u201d sample efficiency due to its multiple GPU actors.\n\n**Q1: There have been efforts to reduce the variance and improve sample complexity (e.g., including a baseline). Compare to SoTA that solves that problem?** \nThank you for bringing this critical point. Our implementations of SoftTreeMax and SoftMax PPO already employ a baseline subtraction technique. This means that on top of our technique to reduce \u201cdirect\u201d variance (i.e. SoftTreeMax), we also run a SOTA technique to reduce \u201cestimation\u201d variance (i.e. baseline subtraction). Importantly, SoftTreeMax puts forward a completely new category of variance reduction techniques because it shrinks the *inherent variance of the gradient* itself as defined in Section 2.1 below Eq. (1), instead of the variance of the *gradient estimator*. As such, it can be applied orthogonally in addition to previous variance reduction techniques. \n\nWe will clarify this point in the revised version. \n\n**Q2: What is the effect of sub-sampling and pruning on the variance?** We perform sub-sampling as part of the pruning process, beyond depth 3. Its effect on variance is complex but small, and may vary with the numbers of pruned nodes. It is small because scores in the leaves are exponentiated to form the policy; thus, most pruned nodes have very low probability and the effect is insignificant compared to the computational gain. Nonetheless, in ablation studies, we saw that extreme pruning leads to low performance and slow convergence. \n\n\n\n**Q3: Can you use higher depths than d=3 without limitation?** Not with the current hardware. Beyond a depth of 3, the GPU memory becomes a bottleneck and without pruning searches in such depths become impossible. With Pruning, the complexity of applying the policy grows linearly with the depth. Supplemental Figure 5 In Appendix B.3 compares the number of online steps taken during one week for several depths up to 8. Indeed, for higher depths, the complexity of the policy inference becomes a limiting factor for training. We do note that this cost depends heavily on the simulator itself, and for RL problems with faster simulators higher depths are more tractable. \n\n**Q4: Is there a better way to compare sample efficiency instead of run-time?** To answer the question, we first define \u201csample efficiency\u201d more carefully. We distinguish \u201conline environment interactions\u201d from \u201cplanning interactions\u201d (i.e. tree search predictions).  If we compare only online interactions, we will overestimate the sample efficiency of SoftTreeMax because it will hide the extra planning iterations during policy inference. However, if we also naively add in the planning interactions, we will underestimate the sample efficiency of SoftTreeMax. The reason being that the computational complexity of planning is lower than online interaction, thanks to the GPU batching mechanism that our tree search takes advantage of. \n\nFor a fair comparison, we chose the metric of the overall wallclock time that accounts for both outer and inner iteration complexity. Appendix B.2, Figure 4, shows a sweet spot that balances online vs. planning iterations. This occurs at different depths, depending on the Atari game, with optimal data efficiency usually obtained at $d>0$. Appendix B.3, Figure 5, complements Figure 4 and highlights the need for fewer outer (online) interactions as the tree deepens. For more details, see the discussions in Sections B.2 and B.3."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5083/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700519303056,
                "cdate": 1700519303056,
                "tmdate": 1700519303056,
                "mdate": 1700519303056,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nNjEoPiscB",
            "forum": "p5jBLcVmhe",
            "replyto": "p5jBLcVmhe",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5083/Reviewer_Xrnz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5083/Reviewer_Xrnz"
            ],
            "content": {
                "summary": {
                    "value": "The article introduces a new family of policies called SoftTreeMax, which are a model-based generalization of the popular softmax used in reinforcement learning (RL). SoftTreeMax policies replace the standard policy logits with the expected value of trajectories that originate from specific states and actions. These policies aim to reduce the high variance of policy gradients and improve RL performance.\n\nThe article contains theoretical analysis, including variance bounds for SoftTreeMax, that demonstrates how the gradient variance decays exponentially with the planning horizon. Additionally, the article discusses how the gradient bias introduced by an approximate forward model diminishes with the approximation error.\n\nExperimental results comparing SoftTreeMax to distributed Proximal Policy Optimization (PPO) demonstrate that SoftTreeMax leads to better sample complexity and improved performance in various Atari games, with significantly lower gradient variance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The methods introduced in this paper are shown to reliably reduce the variance of PG, which is derived theoretically and then verified in experiments. The paper is clearly written, provides mathematical proofs and practical implementations of the results, and seems like a meaningful incremental contribution."
                },
                "weaknesses": {
                    "value": "Lack of experiments with probabilistic environments."
                },
                "questions": {
                    "value": "- What do you mean by \"reward and variance are negatively correlated\" on page 8?\n- The definition of Var_x(X) seems to have a typo.\n- How would you expect the sampling variance to impact the policy gradient if the expectations cannot be computed exactly?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5083/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698835286039,
            "cdate": 1698835286039,
            "tmdate": 1699636499065,
            "mdate": 1699636499065,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0zilq4axYo",
                "forum": "p5jBLcVmhe",
                "replyto": "nNjEoPiscB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5083/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5083/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. Please find our answers in the following.\n\n\n**Q1: What do you mean by \"reward and variance are negatively correlated\" on page 8?** We refer to the empirical finding in Figure 3. Its data shows that low variance is correlated with high reward.\n\n**Q2: Can you fix the typo in Var$_x(X)$ definition?** Typo fixed, thank you. \n\n**Q3: How would sampling impact the policy gradients if expectations cannot be computed exactly?** We perform sub-sampling as part of the pruning process, beyond depth 3. In ablation studies, we saw that extreme pruning leads to low performance and slow convergence. This is because sub-sampling increases the bias of the gradient, as it is no longer an exact expectation over trajectories. Sub-sampling also affects the variance, but the effect is complex and may vary for different choices of pruned nodes."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5083/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700519103854,
                "cdate": 1700519103854,
                "tmdate": 1700519103854,
                "mdate": 1700519103854,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "22KDMpWCuu",
            "forum": "p5jBLcVmhe",
            "replyto": "p5jBLcVmhe",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5083/Reviewer_84Hk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5083/Reviewer_84Hk"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes SoftTreeMax, which uses planning to reduce the policy gradient variance. In particular, the authors proposed two variants, i.e., C-SoftTreeMax and E-SoftTreeMax, where logits are re-defined as Eq. (2). They show that the variance of the proposed gradient decays exponentially w.r.t. $d$ (trajectory depth). They also characterize gradient bias by approximation errors. Experiments on Atari shows that the proposed methods achieve better performance and lower variance than PPO."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is well-written, with clear introduction of the settings, methods, and results.\n2. Combining policy gradient and tree search seems very interesting.\n3. Experiments verify the proposed methods, an they look promising."
                },
                "weaknesses": {
                    "value": "1. It is confusing to me where the exponential decay of variance is from, i.e., from the design or the fact that the policy is nearly deterministic, and therefore not clear to me if reducing both gradient and variance would benefit (please see the question below)."
                },
                "questions": {
                    "value": "Looking at Lemma 4.1 and Lemma 4.3, it seems the exponential decay of variance is from $\\nabla_\\theta \\log{ \\pi_{\\theta}(\\cdot | s) } $. If $\\pi_{\\theta}(\\cdot | s)$ has softmax parameterization then this basically means the policy is nearly deterministic? If this is true, then this also means the policy gradient has to be close to zero (softmax policy has almost zero gradient near deterministic policies), which is expected to slow down the convergence. Could you explain why reducing both gradient and variance to exponentially small would help learning?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5083/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699108370939,
            "cdate": 1699108370939,
            "tmdate": 1699636498976,
            "mdate": 1699636498976,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Lcv4wDSeQn",
                "forum": "p5jBLcVmhe",
                "replyto": "22KDMpWCuu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5083/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5083/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. Please find our answers in the following.\n\n\n**Q1: When the policy is nearly deterministic, how do softmax and SoftTreeMax behave?** In short: Indeed, softmax may become ``stuck\u201d in deterministic policies, but this does not happen in SoftTreeMax. This question highlights the advantage of SoftTreeMax over softmax. \n\nIn more detail, the softmax gradient is indeed almost zero near deterministic policies, as also observed in [Mei et al., 2020a]. In contrast, SoftTreeMax avoids these local optima by integrating the reward into the policy itself. More formally, for softmax, $\\nabla_\\theta\\log \\pi_\\theta(a|s) = e_{a} - \\pi_\\theta(\\cdot|s),$ where $e_{a} \\in [0,1]^A$ is the vector with 0 everywhere except for the $a$-th coordinate. Thus, for a deterministic policy, we get $\\theta(s, a) \\rightarrow \\infty$ for one $a$ per $s$; this expression will be zero for a deterministic policy. In contrast, from Lemmas 4.3 and 4.6 it follows that the SoftTreeMax gradient will not be zero in the respective cumulative or exponential variants. \n\n**Weakness & Q2: Why would reducing both gradient and variance to exponentially small help learning?** Thank you for a very important question. The key observation is that SoftTreemax does not reduce the gradient uniformly (which would have been equivalent to a change in learning rate). While the gradient norm shrinks, the gradient itself scales differently along the different eigenvectors, as a function of problem parameters ($P$, $\\theta$) and our choice of behavior policy ($pi_b$). This allows SoftTreeMax to learn a good \u201cshrinkage\u201d that, while reducing the overall gradient, still updates the policy quickly enough. We will clarify this in the revised paper. \n\nFigure 3 in the paper presents empirical evidence that the theory holds in our case. It depicts the gradient variance in each (environment, depth),  showing a strong correlation between lower variance and higher reward. In some games, performance is non-monotonic, because all experiments used the same computational budget (same HW for the same amount of limited time). So, in some cases the variance was very small, each training iteration took very long to compute, and fewer iterations were completed by the end of the run."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5083/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700519009526,
                "cdate": 1700519009526,
                "tmdate": 1700519009526,
                "mdate": 1700519009526,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "blkPbvDjAK",
            "forum": "p5jBLcVmhe",
            "replyto": "p5jBLcVmhe",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5083/Reviewer_vDhi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5083/Reviewer_vDhi"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new policy gradient, named softtreemax, which combines the tree search within the policy gradient method. The authors . We analyze the gradient variance of SoftTreeMax and reveal how tree expansion helps reduce this variance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The idea of incorporating the tree search within the policy gradient is novel.\n\n2. The variance analysis is solid.\n\n3. The proposed softmax tree method is also extended to infinite action space.\n\n4. Multiple experiments are conducted to demonstrates the necessity of reducing the variance of PG for improving performance and the empirical performance advantage of the proposed method."
                },
                "weaknesses": {
                    "value": "See questions."
                },
                "questions": {
                    "value": "1. When there is approximation error in the model P and r, what are the variance of SoftTreeMax? Do the claimed exponential variance reduction still hold with the approximate model? Is it worth to make the PG, a model-free method, to a model-based method by combining it with the tree search.\n\n2. Although authors mention that formally proving the conjectured global convergence with fast rate as in (Mei et al 2020b) is subject to future work. It is hard to demonstrate its advantage over the traditional SoftMax policy gradient, or more generally traditional policy gradient methods without comparing the sample complexity between SoftTreeMax policy gradient and traditional SoftMax policy gradient. The main missing piece is that the reduction in variance does not necessarily imply the faster convergence of smaller sample complexity if bringing such variance reduction needs to use a form of policy gradient sacrifice the performance in the deterministic setting (For example, I am not sure whether the proposed SoftTreeMax policy gradient will even converge in the derterministic setting).\n\n3. There are some relevant papers that address related problems that authors may need to add to the related work.\n\n(1) Optimization Methods for Interpretable Differentiable Decision Trees in Reinforcement Learning, Andrew Silva, et al,. 2020 AISTATS.\n\n(2) On the Global Optimum Convergence of Momentum-based Policy Gradient, Yuhao Ding, et al, 2022 AISTATS. \n\nIt is important to compare with (1) to evaluate whether this paper is still the first work on proposing a differentiable parametric policy that combines tree expansion with PG. (2) also studies the convergence and the variance reduction for softmax PG."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5083/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699235096533,
            "cdate": 1699235096533,
            "tmdate": 1699636498882,
            "mdate": 1699636498882,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pOO1AB9FVK",
                "forum": "p5jBLcVmhe",
                "replyto": "blkPbvDjAK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5083/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5083/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. Please find our answers below.\n\n**Q1a : Do the claimed exponential variance reduction still hold with the approximate model?** Thank you for raising this important question. Yes, the variance decays exponentially at a rate dictated by $\\hat{P}^{\\pi_b}$ instead of $P^{\\pi_b}.$ As the two terms get closer when the approximation error decays, the rate naturally also becomes closer due to eigenvalue continuity. To study the consequence of using an approximate model, we analyzed the bias in Theorem 4.8. \n\nTo formally see why the variance decays exponentially in the approximate case, recall the three key steps in our proof of Theorem 4.4:\n1. Eigendecomposition of $P^{\\pi_b}$ with the eigenvalue 1 not repeating and noting that the leading eigenvector is $1_A$ (the all ones vector of size $A$).\n\n2. Noting that $P_s 1_S = 1_A.$\n\n3. Noting that $[I_A - 1_A \\pi^C_{d, \\theta}] 1_A = 0.$\n\nThe first two steps also hold for the approximate model since these steps only rely on row-stochasticity (leaving aside the pathological case where the eigenvalue $1$ repeats in $\\hat{P}^{\\pi_b}$, which will not occur when the latter is close to $P^{\\pi_b}.$). Finally, the third step is valid as long as $\\hat{\\pi}_{d, \\theta}^C$ is a distribution, which is trivially true. \n\nWe will add this new discussion to the revised version. \n\n\n\n**Q1b: Is it useful to make PG, a model-free method, to a model-based method by combining it with tree search?** We believe that PG as a model-based method has great potential to improve upon its model-free version. To demonstrate this beyond what was shown in the submitted version regarding PPO, we now added a SOTA baseline: EfficientZero [Ye et al., 2021]. We chose EfficientZero because it is a highly sample-efficient version of MuZero -- one of the best known model-based algorithms today -- and is also open source. The results are given here: https://ibb.co/6Zd24yk. As seen in the plots, SoftTreeMax surpasses EfficientZero in all games but one. We will add full experiments in the final version. \n\nExperiment details: We used the same hardware to run all experiments, as described in Appendix B.1.  For \u201cSoftTreeMax\u201d, we present the best depth out of those in Appendix B.2, Figure 4. \n\n\n**Q2a: Direct comparison of sample complexity is missing.** For a fair comparison, we estimate sample complexity through wall-clock time; We explain in detail the considerations for this decision in our reply to Q4, R eGu2. \n\nFor wall clock time, in Appendix B.2, Figure 4, we provide the convergence plots as a function of wallclock time. It shows a clear benefit for SoftTreeMax over distributed PPO with the standard softmax policy. In most games, PPO with the SoftTreeMax policy shows very high sample efficiency: it achieves higher episodic reward even though it observes far fewer episodes, for the same running time. In addition, in Appendix B.3, Figure 5, we provide convergence plots of SoftTreeMax where the x-axis is the number of online interactions with the environment, thus excluding the tree expansion complexity. The plot demonstrates a monotone improvement as a function of the tree depth.\n\n**Q2b: Does a reduction in variance necessarily imply faster convergence?** \nThere are several theoretical results that support this relation. First, the general fact that variance reduction translates to faster convergence was established in stochastic approximation theory. In the context of PG, several recent works proved improved convergence rates thanks to variance reduction techniques such as SVRG [Papini et al., 2018] (see also [Liu et al., 2020]). \n\nFor SoftTreemax discussed here, note that SoftTreemax does not reduce the gradient uniformly (which would have been equivalent to a change in learning rate). While the gradient norm shrinks, the gradient itself scales differently along the different eigenvectors, as a function of problem parameters ($P$, $\\theta$) and our choice of behavior policy ($pi_b$). This allows SoftTreeMax to learn a good \u201cshrinkage\u201d that, while reducing the overall gradient, still updates the policy quickly enough.  \n\nFigure 3 in the paper presents empirical evidence that supports the theory in our case. It depicts the gradient variance in each (environment, depth),  showing a strong correlation between lower variance and higher reward. In some games, performance is non-monotonic, because all experiments used the same computational budget (same HW for the same amount of limited time). So, in some cases, the variance was very small, each training iteration took very long to compute, and fewer iterations were completed by the end of the run."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5083/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700518794039,
                "cdate": 1700518794039,
                "tmdate": 1700519920251,
                "mdate": 1700519920251,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]