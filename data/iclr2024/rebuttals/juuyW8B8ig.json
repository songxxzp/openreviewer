[
    {
        "title": "Language-Informed Visual Concept Learning"
    },
    {
        "review": {
            "id": "ZAmziNmCU6",
            "forum": "juuyW8B8ig",
            "replyto": "juuyW8B8ig",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission320/Reviewer_jNDi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission320/Reviewer_jNDi"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a method for learning concepts by distilling knowledge in a text-to-image generative model. The method assumes concept axes, in each of which specific information of an input image is encoded (like colors, materials, and object categories). The method learns a designated encoder for each concept axis with generated images. For training the encoders, the method uses an anchor loss for each axis based on a VQA model to further disentangle the axes and a reconstruction loss. The method is evaluated qualitatively and quantitatively (CLIPScore and human evaluation)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "(1) The paper is well-written. I can easily follow what is done in the method.\n\n(2) The method is simple and can be trained in 12 hours only with less than 1000 generated images, yet outperforming the similar existing methods. \n\n(3) The image generation results are really nice compared to the existing approaches."
                },
                "weaknesses": {
                    "value": "(1) The performance of the method is mostly shown in qualitative evaluations. The quantitative evaluation only shows the performance of image generation by modifying some concept axes (and human evaluation). I think the paper would be better if it came with an objective quantitative evaluation of the obtained concepts themselves in some ways (though I didn\u2019t come up with any good approaches for this). \n\n(2) Related to (1), I\u2019m not sure if CLIPScore is really sensitive to arbitrary combinations of concepts. Some references or experimental results may help understand the experiment. \n\n(3) The paper\u2019s purpose is not sufficiently clear. Is it to learn concepts for image generation? Or is it for some other downstream tasks?"
                },
                "questions": {
                    "value": "I would like to have some responses for (1)-(3) in the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission320/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698570370310,
            "cdate": 1698570370310,
            "tmdate": 1699635958648,
            "mdate": 1699635958648,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "X7xzQVXXWV",
                "forum": "juuyW8B8ig",
                "replyto": "ZAmziNmCU6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission320/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission320/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response for Reviewer jNDi"
                    },
                    "comment": {
                        "value": "**Q1 - Additional quantitative evaluation**\n\nTo better quantify the performance on capturing nuanced colors, we designed a new metric based on color statistics for the task of concept editing as an additional quantitative evaluation. \n\nSpecifically, we compare the color of a 16 \u00d7 16 patch cropped around the center (focusing on the object) from both the input image and the image generated by the method being evaluated. The original size of the image is  64 \u00d7 64. We then compute the MSE of the mean color of the two patches, and report the average of the metric across all examples in Figure 11 in Table 1. Results suggest that our method captures colors more accurately compared to the baselines.\n\n\n**Q2 - Additional Qualitative results accompanying the CLIP-score evaluation**\n\nIn addition to the results in the main paper, in Figure 11 of the `Rebuttal PDF`, we provide more qualitative examples accompanying the experiments in Table 1. From these visual examples, we observed that the color nuances captured by ours are more accurate compared to the BLIP-2 baseline. However, since the CLIP-based metric specified in Section 4.3 of the paper cannot fully capture the minute differences, the BLIP-2 baseline still  achieves comparable scores to our method despite this evident gap in visual results. To better quantify such visual differences in colors, we further designed a new metric based on color statistics, as explained in Q3 above.\n\n\n**Q3 - Purpose of the framework**\n\nThe goal of visual concept learning is to extract visual representations in a structured manner akin to human perception. Humans interpret and organize the visual world into a hierarchy of abstract concepts, such as `object categories`, `colors`, `styles` etc. With this abstracted visual concept representation, we can easily picture a *new* visual instance with various concept compositions, such as `a red banana`.\n\nIn this paper, we aim at developing a method that can automatically learn to extract disentangled image representations along a number of language-specified concept axes (`category`, `color`, etc).\nWith these disentangled representations, we can then recompose them to *generate new compositions of visual concepts*, capturing visual nuances which would otherwise often exceed the limitations of language."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission320/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700736591420,
                "cdate": 1700736591420,
                "tmdate": 1700736591420,
                "mdate": 1700736591420,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "U0L4F5k725",
            "forum": "juuyW8B8ig",
            "replyto": "juuyW8B8ig",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission320/Reviewer_Bcg9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission320/Reviewer_Bcg9"
            ],
            "content": {
                "summary": {
                    "value": "The authors claim that their proposed model can learn a language-informed visual concept representation, by simply distilling large pre-trained vision-language models."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors claim that their proposed model can learn a language-informed visual concept representation, by simply distilling large pre-trained vision-language models."
                },
                "weaknesses": {
                    "value": "1. What is concept representation learning? Is concept learning just the mutual translation of text and images?\n\n2. In the experiments, the authors primarily focus on conducting investigations using synthetic datasets. However, it raises concerns about the generalizability of the conclusions/findings obtained from synthetic datasets to real-world datasets.\n\n3. The concept learning should focus more on the understanding of concepts, especially at different granularities of the same concept."
                },
                "questions": {
                    "value": "Please refer to Weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission320/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698668162685,
            "cdate": 1698668162685,
            "tmdate": 1699635958574,
            "mdate": 1699635958574,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "whlErmAMmn",
                "forum": "juuyW8B8ig",
                "replyto": "U0L4F5k725",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission320/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission320/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response for Reviewer Bcg9"
                    },
                    "comment": {
                        "value": "**Q1 - What is visual concept representation learning?**\n\nThe goal of visual concept learning is to extract visual representations in a structured manner akin to human perception. Humans interpret and organize the visual world into a hierarchy of abstract concepts, such as object `categories`, `colors`, `styles` etc. With this abstracted visual concept representation, we can easily picture a *new* visual instance with various concept compositions, such as `a red banana`.\n\nIn this paper, we aim at developing a method that can automatically learn to extract disentangled image representations along a number of language-specified concept axes (`category`, `color`, etc).\nWith these disentangled representations, we can then recompose them to *generate new compositions of visual concepts*, capturing visual nuances which would otherwise often exceed the limitations of language.\n\n\n**Q2 - Real-world datasets**\n\nWe provide new inference results on real-world images in Figures 1 to 5 from the Rebuttal PDF, spanning various types of objects, including furniture, kitchen utensils, fruits, and artwork. Despite *only* being trained on images generated by diffusion-based models, the concept encoders generalize well to diverse, complex real-world images, including *unseen* types of objects, materials, colors, and styles. Note that some of the examples originally presented in the main paper also came from real photos, such as Figure 5 from the main paper."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission320/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700736548601,
                "cdate": 1700736548601,
                "tmdate": 1700736548601,
                "mdate": 1700736548601,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RoZNR8UakC",
            "forum": "juuyW8B8ig",
            "replyto": "juuyW8B8ig",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission320/Reviewer_dyyk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission320/Reviewer_dyyk"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a new framework for Visual Concept Learning. By introducing a set of concept encoders, concept embeddings could be extracted from the input image, which could be recomposed later to produce desired image. The experiments showed that these concept embeddings could capture the visual nuances and they are disentangled with each other. Besides, this framework can learn the shared concepts across instances(images), in other words, it is more efficient than previous methods.\n\nThe paper proposes a new framework for VCL task that avoids massive human annotation, and could boost the research in related fields. It also drew the attention to the research direction of using continuous embeddings (instead of relying on generic words) as the visual concept descriptors. The concept is commendable, although the depiction falls short of perfection and would greatly benefit from additional elaboration and intricate explanations."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "a. Proposed the idea to use BLIP-2 generated embeddings as \u201canchor\u201d (pseudo ground-truth embeddings) to \u201cseparate\u201d the entangled concept embeddings.\n \nb. This work proposed a framework to tackle Visual Concept Learning without human annotation, which is much more efficient than the previous works.\n \nc. Unlike most of the Textual Inversion techniques, this work could capture the concept appearing in different images. Therefore, it does not require a retraining on each image. Also, the learning efficiency is higher because it can learn from a larger pool of images."
                },
                "weaknesses": {
                    "value": "a. The datasets used for the experiment were small and simple. It is not guaranteed that the claimed conclusions could be maintained when this framework is applied on more complex datasets (with much more concepts). The idea of using anchors to ensure that the embeddings are disentangle is great, however more experiments on larger datasets should be done to prove it. Given that there are only 2 to 3 concepts in each domain, the sparsity of concepts might be one of the reasons why the embeddings are disentangle.\n \nb. The effectiveness of L^{anchor} is not fully explained. The L^{anchor} is omitted during the test-time optimization procedure to avoid \u201cover-committing to the coarse text anchors\u201d. However, in the ablation experiment, the paper claims \u201cdisabling L^{anchor} deteriorates the performance\u201d. It seems kind of contradictory, the paper should explain more about why \u201cdisabling L^{anchor}\u201d is desired during one phase but it leads to unsatisfactory results in general evaluation.\n \nc. The ablation test is not fully explained. In \u201cEditing Category\u201d column, the results of \u201cw/o Encoder & L^{anchor}_k\u201d is actually higher than the results of \u201cw/o L^{anchor}_k\u201d in two metrics. This does not fully conform to the conclusion, quote, \u201cand further removing the encoder decreases the overall decomposition performance\u201d.\n \nd. It is hard for this framework to generalize to new \u201cconcept\u201d. From what I understood, this framework could effectively generalize to new \u201cmode\u201d of seen \u201cconcept\u201d (like new style or new color), but not to new \u201cconcept\u201d. When applied to new concepts, e.g. \u201csize\u201d or \u201cshape\u201d, the corresponding concept encoders need to be trained. Also, from my perspective, we can\u2019t only train the concept encoder of the new concepts. Because the sentence template \u201ca photo of <e1> with <e2> color and <e3>  and <e4>...\u201d needs to cover at least the majority of the concepts appeared to generate an image close enough to the original input image. Based on this understanding, when this framework is extended to new concepts, the trained concept encoders (of the seen concepts) need to be retrained together with the new ones. This setting is not more efficient than previous methods."
                },
                "questions": {
                    "value": "a. On page 5, in sentence \u201ctext encoder c_{theta} of the T2I diffusion model\u2026\u201d, it should be \u201cpart of the text encoder c_{theta}\u201d. Because a text encoder should take \u201ctext\u201d as input rather than \u201ctext embeddings\u201d. The original sentence might be confusing.\n \nb. On page 5, in formula (1), there is no explanation about the N and U notation. I assume they represent \u201cmultivariate normal distribution\u201d and \u201cuniform distribution\u201d respectively. It would be more clear to annotate.\n \nc. On page 5, in sentence \u201cso that they can directly inserted into the text embeddings\u201d, it seems that a \u201cbe\u201d is missed.\n \nd. The paper should mention the backbone T2I model earlier. It is first mentioned on page 5, it would be better to do it earlier.\n \ne. It would be better if the choice of using \u201c12 CLIP layers\u201d over \u201c6 out of 12 tokens like Gal et al. 2023\u201d is explained more in detailed.\n \nf. More details could be added about the test-time lightweight finetuning process."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission320/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698736498640,
            "cdate": 1698736498640,
            "tmdate": 1699635958480,
            "mdate": 1699635958480,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "K45psd0Egi",
                "forum": "juuyW8B8ig",
                "replyto": "RoZNR8UakC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission320/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission320/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response for Reviewer dyyk"
                    },
                    "comment": {
                        "value": "**Q1 - More complex datasets**\n\nIt is worth highlighting that despite training on simple, synthetic datasets, the model generalizes well to diverse, complex *real-world images* as discussed in the general response above. Figures 1-5 show results on real-world photos of various types of objects, including furniture, kitchen utensils, fruits, and artwork. Despite the *unseen* types of objects, materials, and colors, the model still extracts disentangled and compositional concept embeddings, showing the strong generalization capability of the proposed framework to complex real-world images.\n\n\n**Q2 - Omitting the anchor loss during test-time optimization**\n\nDuring *training time*, $L^{anchor}$ encourages disentanglement of the concept embeddings across different concept axes while remaining in the subspace of the text embedding along their respective axes, as explained in Section 3.2. During *test time*, the concept encoders have already learned to encode only axis-specific concepts. The main objective is then to find a concept embedding within a specified axis that is consistent with the input image, which can be enforced by $L^\\text{recon}$ (Equation (1)) alone. Omitting $L^{anchor}$ is beneficial as it helps the model to better preserve visual details of the input instance which are hard to capture with texts, *e.g.*, the style of paintings in Figure 24. \n\n\n**Q4 - Explanations on ablation study results**\n\nThanks for pointing this out. In fact, both ablation baselines \u2018w/o Encoder & $L^{anchor}_k$\u2019 and \u2018w/o $L^{anchor}_k$\u2019 fail to learn disentangled concept embeddings, as illustrated in Figure 8 in the main paper as well as the additional results in Figure 11 of the `Rebuttal PDF`.\nThe CLIP-based score is no longer indicative of the performance difference of the two baselines.\nWe have carefully updated the conclusion in Section 4.4 in the paper.\n\n\n**Q5 - Extending with new concept axes**\n\nOur model can be extended with more concept axes by training new concept encoders *without retraining the existing ones*, as shown in Figures 9 and 10. Given a model trained with concept encoders for `category` and `color`, we additionally train a third encoder for `material` while keeping the other two encoders *frozen*. With this progressive training procedure, the model is able to extend to new concept axes while maintaining the disentanglement of the frozen encoders.\n\n\n**Q6 - Other clarifications**\n\nThanks for pointing them out. We revised the paper accordingly, marked in blue."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission320/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700736494394,
                "cdate": 1700736494394,
                "tmdate": 1700736494394,
                "mdate": 1700736494394,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zWJ8Ddf3TE",
            "forum": "juuyW8B8ig",
            "replyto": "juuyW8B8ig",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission320/Reviewer_qJGK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission320/Reviewer_qJGK"
            ],
            "content": {
                "summary": {
                    "value": "The authors use multiple visual encoders to disentangle various visual concepts from images. These visual concepts are defined as vector axes based on natural language description. The proposed framework performs a simple training on a synthetic dataset that learns disentangled vectors for each concept. These disentangled vectors can be combined with language (similar to textual inversion paper) to generate images containing combined concepts. They show how their method disentangles and generates new images with variably joined concepts better than existing work."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper explores an interesting idea of separating concepts in images in the visual domain in a novel manner\n2. Interesting use of a VQA model to augment their training setup\n3. Good use of diagrams to explain idea \n4. Clearly showcase qualitative improvements for selected cases"
                },
                "weaknesses": {
                    "value": "1. The generality of method on real world images (i.e. where visual concepts are not that easily disentangled) is unclear\n2. Limited evaluation (only one set of quantitative numbers)\n3. Some missing details (refer questions below)\n\n* Table 1: Are you reporting CLIP score and human evaluation on same table?? \nPlease point out CLEARLY that these are two different metrics in the Table caption. Or please separate into two Tables. This is highly confusing."
                },
                "questions": {
                    "value": "* Immediate question - why don't concept axis vectors collapse to same as text embeddings? Explain this more. \n* DeepFloyd - please cite appropriately \n* Consider more discussion on Textual Inversion (as related work), maybe in supplementary at least. Highlight cases where this is better than directly using text. \n* The work in [1] explores language defined concept axes in video domain - maybe an interesting comparison to discuss in related work  \n* Please include BLIP-based baseline results also in Table 1\n* Can you add more CLIP-score based (or a different new metric based) evaluations for other task (like concept extrapolation)? More quantitative evaluation could really strengthen the paper\n\n[1] Ranasinghe, K., & Ryoo, M., Language-based Action Concept Spaces Improve Video Self-Supervised Learning, NeurIPS 2023"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission320/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission320/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission320/Reviewer_qJGK"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission320/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698785932643,
            "cdate": 1698785932643,
            "tmdate": 1699635958402,
            "mdate": 1699635958402,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VBKuV7xb9W",
                "forum": "juuyW8B8ig",
                "replyto": "zWJ8Ddf3TE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission320/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission320/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response for Reviewer qJGK"
                    },
                    "comment": {
                        "value": "**Q1 - Real-world Images**\n\nWe provide new inference results on real-world images in Figures 1 to 5 from the Rebuttal PDF, spanning various types of objects, including furniture, kitchen utensils, fruits, and artwork. Despite *only* being trained on images generated by diffusion-based models, the concept encoders generalize well to diverse, complex real-world images, including *unseen* types of objects, materials, colors, and styles. Note that some of the examples originally presented in the main paper also came from real photos, such as Figure 5 from the main paper.\n\n\n\n**Q2 - Additional quantitative evaluation**\n\nTo better quantify the performance on capturing nuanced colors, we designed a new metric based on color statistics for the task of concept editing as an additional quantitative evaluation. \n\nSpecifically, we compare the color of a 16 \u00d7 16 patch cropped around the center (focusing on the object) from both the input image and the image generated by the method being evaluated. The original size of the image is  64 \u00d7 64. We then compute the MSE of the mean color of the two patches, and report the average of the metric across all examples in Figure 11 in Table 1. Results suggest that our method captures colors more accurately compared to the baselines.\n\n\n**Q3 - Why concept embeddings don\u2019t collapse to word embeddings?**\n\nDuring training time, the anchor loss (Equation (2)) encourages the encoder predictions to converge to a *meaningful* subspace within the word embedding space [1]. This ensures that these embeddings can be readily visualized by a pre-trained text-to-image generation model, and improves the compositionality across different concept axes, as shown in Figure 8 in the main paper.\n\nThe anchor loss is only a *soft* constraint with a small weight in addition to the reconstruction loss (Equation (1)) and therefore does not enforce the concept embedding predictions to be identical to the word embeddings. Empirically this is also the case, as shown in Figure 8. In these figures, we compare the concept embeddings predicted by the `color` encoder to the text embedding of the training-time BLIP-2 label, e.g. \u201cblue\u201d from Figure 8, and the former preserves the specific color of the input image while the latter does not.\n\n[1] Encoder-based Domain Tuning for Fast Personalization of Text-to-Image Models. Rinon Gal, Moab Arar, Yuval Atzmon, Amit H. Bermano, Gal Chechik, Daniel Cohen-Or. 2023.\n\n\n**Q4 - Textual Inversion vs. directly using text inputs**\n\nThanks for the suggestion. We included the following discussion in the appendix: \n> As discussed in Section 3.1, compared to directly using text as inputs for image generation, using techniques like Textual Inversion (Gal et al., 2022), our model is able to capture more nuanced visual details of a particular image with continuous embeddings, instead of discrete words. This can be illustrated in the empirical results in Figures 11 and 12 as well as the results in the main paper, which show that our method preserves the nuances from input images more accurately than the BLIP-2 baseline which uses texts for conditional generation.\n\n\n**Q5 - Other clarifications.**\n\n- We have updated Table 1 in the main paper to make sure the CLIP-based scores and human evaluation scores are clearly separated.\n- We\u2019ve included the proper citation for DeepFloyd. We apologize for missing it.\n- Thanks for the reference! We\u2019ve included a discussion in the related work section. \n- The quantitative evaluation for the BLIP-based baseline is added to Table 1."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission320/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700736441129,
                "cdate": 1700736441129,
                "tmdate": 1700736441129,
                "mdate": 1700736441129,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]