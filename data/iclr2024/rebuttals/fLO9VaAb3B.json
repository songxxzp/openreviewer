[
    {
        "title": "Alphazero-like Tree-Search can guide large language model decoding and training"
    },
    {
        "review": {
            "id": "jnEXnaWfWT",
            "forum": "fLO9VaAb3B",
            "replyto": "fLO9VaAb3B",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4470/Reviewer_1bFk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4470/Reviewer_1bFk"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces TS-LLM, a tree-search learning framework tailored for Large Language Models (LLMs), drawing inspiration from AlphaZero. The framework sets itself apart from prior works by providing a unique method to enhance the performance and capabilities of LLMs in reasoning tasks and RLHF alignment."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- TS-LLM differs significantly from methods like Tree of Thought (TOT) and Reasoning via Planning (RAP), focusing less on reasoning tasks and not relying on large-scale models or human-designed prompts.\n- The framework is adaptable for various tasks and LLM sizes.\n- TS-LLM includes a learned value function and a final-step outcome reward model (ORM), enhancing performance in both decoding during inference and training."
                },
                "weaknesses": {
                    "value": "Despite its merits, the paper does not demonstrate a significant improvement over previous works. Major concerns are:\n\n1. The performance improvement is unclear and somewhat unconvincing.\n   - **GSM8k:** The results are inferior compared to the previous work (ToT-BFS).\n   - **Game24:** Despite MCTS-$\\alpha$ winning, it requires **nearly twice** the number of tokens compared to other approaches.\n   - **PrOntoQA:** Similarly, it requires **nearly twice** the number of tokens compared to other approaches.\n   - **RLHF:** (See the next point)\n\n2. Misapplication of RLHF Task: This paper's approach to the RLHF task seems misaligned with its intended purpose. Instead of tackling the fundamental challenges of RLHF, the study opts for a smaller agent (125M) and compensates with a higher token count to achieve elevated rewards through an open-source Reward Model. While this method may yield higher scores during training and potentially quicker convergence due to reduced sampling, the extended time required for \"search\" processes negates these benefits. Ultimately, this approach fails to address the crucial issues inherent to RLHF."
                },
                "questions": {
                    "value": "1. In Table 4, why do DFS/BFS/MCTS yield identical results for the RLHF task? This paper should clarify this.\n\n2. In Figure 1, the presentation should be further clarified. The authors did not evaluate Game24 on a token-level, however the figure may mislead readers.\n\n3. Is the term \"$k$-node\" used in the context of BFS equivalent in meaning to \"$k$-node\" in the context of MCTS? \n\n4. Do BFS and DFS utilize the same Large Language Model (LLM) as MCTS-$\\alpha$ and MCTS-Rollout, or do they employ GPT-4 as their original Tree of Thought (ToT) configuration?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "Not applicable"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4470/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698593918659,
            "cdate": 1698593918659,
            "tmdate": 1699636422729,
            "mdate": 1699636422729,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "R1nO8A55Og",
                "forum": "fLO9VaAb3B",
                "replyto": "jnEXnaWfWT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4470/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4470/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 1bFk (1/2)"
                    },
                    "comment": {
                        "value": "We thank you for your valuable feedback and comments. We have modified several parts of our paper according to your suggestions.\n\n**Question 1: The performance improvement is unclear and somewhat unconvincing.**\n\nThanks for your comment. We totally agree with your observation. There are three things we want to clarify:\n1. **We want to clarify that TS-LLM\u2019s DFS/BFS variants are different from that in Tree of Thought (ToT). They are TS-LLM\u2019s variants instead of ToT baselines.** The main difference is that we use a learned value function instead of prompting advanced LLMs, which is one of our important contributions. We have shown in Table 4 the result of BFS and MCTS-alpha on Game24 by prompting LLaMA2-7B model as value function and ORM. It can exactly be regarded as an instance of ToT over small LLMs. Our conclusion is that prompting a small LLM can not offer accurate value estimation and the performance of tree-search is even worse than simple CoT baseline.\n2. For path@1 results in Table 2, MCTS-alpha and MCTS-rollout do consume more tokens and it is reasonable to question whether it is mainly because of more token consumptions. Our Q4\u2019s experiment can help to clarify such an issue. We additionally conduct experiments on search aggregation shown in Figure 3, where you can scale up your token consumption by running multiple times of search in MCTS/MCTS-alpha/MCTS-rollout/DFS or setting larger beam size in BFS. Specifically, the second row of Figure 3 can clearly demonstrate the performance of each tree-search algorithm when given the same token consumption. Basically, MCTS-alpha and MCTS-rollout are still dominant in most cases.\n3. At last, we never claim in our paper that MCTS-alpha/rollout is the new SOTA over all evaluation tasks. Our experiments are not designed to show new SOTAs or the most efficient tree search algorithm. **We treat different search algorithms as TS-LLM\u2019s variants and equally present their pros and cons. We believe every algorithm has its limitations and no one can claim superiority in all scenarios.** For example, as we illustrate in our analysis, MCTS, BFS, and DFS generally perform well in shallow search problems while MCTS-\u03b1 and MCTS-Rollout are dominant in deep search problems.\n\nWe thank you for your comment and understand that readers may have similar misunderstandings. To make this more clear, we\u2019ve also modified this and incorporated all three points in our rebuttal revision. Specifically, we modify the Benchmark algorithms subsection in Section 4.1 to include more clarifications among benchmark algorithms and our experiment objectives. We\u2019ve also modified the results analysis in Q1, to incorporate the Path@N result shown in Figure 3 for a better presentation.\n\n**Question 2: Misapplication of RLHF Task: This paper's approach to the RLHF task seems misaligned with its intended purpose. Instead of tackling the fundamental challenges of RLHF, the study opts for a smaller agent (125M) and compensates with a higher token count to achieve elevated rewards through an open-source Reward Model. While this method may yield higher scores during training and potentially quicker convergence due to reduced sampling, the extended time required for \"search\" processes negates these benefits. Ultimately, this approach fails to address the crucial issues inherent to RLHF.**\n\nThank you for your comment, we fully agree with you that (1) we do not offer a solution that can solve the RLHF core problem like reward function learning (since we choose an open-source reward model) (2) The tree-search operations bring in additional computational burdens. \n\nHere what we are trying to provide is an alternative algorithm, in contrast to PPO or rejection sampling-based training, when given a fixed reward function. Since we are not a paper studying RLHF, this is just a showcase to present the applicability of the TS-LLM framework over tasks beyond reasoning. This is also a good example to show MCTS-alpha can do deep tree search in LLM generation (since we set a really wide and deep tree in the RLHF experiment.) In addition, there is one additional advantage of the tree-search algorithms. Section 3.2 has presented that we do not need to finetune the LLM generator using Rejection sampling finetuning or PPO, and can let the model generate reliable answers by pure inference.\n\n**Question 3: In Figure 1, the presentation should be further clarified. The authors did not evaluate Game24 on a token-level, however the figure may mislead readers.**\n\nThanks for your comment, we have added the clarification in the footnote of Figure 1.\n \n**Question 4: Is the term k-node used in the context of BFS equivalent in meaning to k-node in the context of MCTS?**\n\nExactly. The k is mentioned in the last paragraph of Section 3.1, as the number of nodes we subsample during the leaf node expansion. In other words, it is how many child nodes we have given a parent node."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4470/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699903636189,
                "cdate": 1699903636189,
                "tmdate": 1699904029863,
                "mdate": 1699904029863,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bt0TS1gvjA",
                "forum": "fLO9VaAb3B",
                "replyto": "jnEXnaWfWT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4470/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4470/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 1bFk (2/2)"
                    },
                    "comment": {
                        "value": "**Question 5: In Table 4, why do DFS/BFS/MCTS yield identical results for the RLHF task? This paper should clarify this.**\n\nThanks for your comments and suggestions. We do clarify this in our paper in the last two sentences of the first paragraph in Q1\u2019s answer.\n\nLet us clarify this more. When evaluating path@1 performance, we only generate one path given a search algorithm. It means MCTS cannot take the backward step (because it only conducts backward operation after it reaches the terminal node), BFS beam size is 1, and DFS cannot prune node and only get 1 path. In this case, MCTS, BFS, and DFS degenerate to greedy search over value. Results are different on the first 3 tasks because they are using sentence-level node expansion, so the action nodes are based on sampling. For the RLHF task, we derive the node by conducting a forward process and extract the top 50 tokens, it is deterministic as long as the input is the same.\n\nWe fully agree that we need more explanations and have modified the analysis results in the first paragraph of our Q1\u2019s answer. We\u2019ve also clarified the Path@1 setting and additionally added a new result analysis by referring to the Path@N result shown in Figure 3. In this setting, MCTS, BFS and DFS will be different so we can conduct a better analysis. We\u2019ve also added more explanations in the caption of Table 2 to better illustrate the reason why DFT/BFS/MCTS yield identical results.\n\n**Question 6: Do BFS and DFS utilize the same Large Language Model (LLM) as MCTS-alpha and MCTS-Rollout, or do they employ GPT-4 as their original Tree of Thought (ToT) configuration?**\n\nAll search algorithms utilize the same LLM, which is LLaMA2-7B in the first three tasks and GPT-2 in the RLHF alignment task. We want to show that our framework is applicable to all different LLMs of any size and directly applying ToT pipeline to them may not work."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4470/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699903677293,
                "cdate": 1699903677293,
                "tmdate": 1699903677293,
                "mdate": 1699903677293,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1hjXJxUolP",
                "forum": "fLO9VaAb3B",
                "replyto": "jnEXnaWfWT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4470/Reviewer_1bFk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4470/Reviewer_1bFk"
                ],
                "content": {
                    "title": {
                        "value": "Response to Question1&2"
                    },
                    "comment": {
                        "value": "### Question 1: The performance improvement is unclear and somewhat unconvincing.\n\nIn your response to the question regarding performance improvement, you emphasize that your paper does not claim that MCTS-alpha/rollout establishes a new state-of-the-art (SOTA) across all evaluation tasks, and that your experiments were not designed to demonstrate new SOTAs or the most efficient tree search algorithm. While this clarification is appreciated, **it might be beneficial for the paper to more explicitly state the specific contexts or scenarios where MCTS-alpha and MCTS-Rollout demonstrate their strengths, beside from \"going deeper\".**\n\nconsidering the use of value networks in MCTS is **not a novel approach**, a deeper exploration of how your implementation improves upon existing models would further strengthen your argument.\n\n### Question 2: Misapplication of RLHF Task\nIf the primary goal is not to address the core problems of RLHF but to present an alternative algorithm, it might be more beneficial for readers if you explicitly compare your method with existing approaches like PPO and rejection sampling-based training. This comparison would help in understanding the unique advantages or limitations of your method in the RLHF domain.\nMoreover, while using an open-source reward model simplifies the process, it might also be worthwhile to discuss how this choice influences the generalizability and applicability of your findings. If your method shows promise in specific scenarios or configurations, clarifying these conditions would add valuable context for your readers, helping them understand where your approach fits in the broader landscape of RLHF solutions."
                    }
                },
                "number": 43,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4470/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700551830147,
                "cdate": 1700551830147,
                "tmdate": 1700551993383,
                "mdate": 1700551993383,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "595IrFHNxs",
                "forum": "fLO9VaAb3B",
                "replyto": "jnEXnaWfWT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4470/Reviewer_1bFk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4470/Reviewer_1bFk"
                ],
                "content": {
                    "title": {
                        "value": "Other Comments"
                    },
                    "comment": {
                        "value": "## Clarity on BFS and MCTS Variants\nIn your text, you replaced \"BFS\" with \"BFS-V,\" suggesting a variant. However, this change might not be clear to readers that it is equivalent to Beam Search. For greater precision and clarity, changing the term to \"Beam Search\" would be more appropriate and straightforward. This adjustment ensures that the concept is immediately recognizable and accurately represented.\n\nSimilarly, the use of MCTS needs clarification. If the paper employs a variant of MCTS from *Sampled MuZero,* mention this earlier. This distinction is crucial to avoid confusion, especially since it differs not only from traditional MCTS but also from the MCTS used in \"AlphaZero.\"\n\n## Fairness in Baseline Comparison\nYour paper uses CoT-greedy as a baseline, which may not provide a fair comparison with your methods. If your research builds upon ToT and RAP, it's essential to include and compare their results as well. While it's noted that these studies use GPT-4 and your method employs a smaller model, this contrast is precisely what readers need to understand. It's crucial to demonstrate why your method is preferable, outlining **any trade-offs or improvements they might expect.**"
                    }
                },
                "number": 45,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4470/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700553427134,
                "cdate": 1700553427134,
                "tmdate": 1700553600251,
                "mdate": 1700553600251,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "y4sXzAXPBv",
                "forum": "fLO9VaAb3B",
                "replyto": "jnEXnaWfWT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4470/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4470/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 1bFk [1/2]"
                    },
                    "comment": {
                        "value": "We thank you for your valuable feedback and comments. \n\n**Question 1: While this clarification is appreciated, it might be beneficial for the paper to more explicitly state the specific contexts or scenarios where MCTS-alpha and MCTS-Rollout demonstrate their strengths, beside from \"going deeper\". considering the use of value networks in MCTS is not a novel approach, a deeper exploration of how your implementation improves upon existing models would further strengthen your argument.**\n\nWe do explicitly state the strengths of MCTS-alpha and MCTS-Rollout in our analysis of Q1 and mention that these two algorithm variants dominate when dealing with deep-search problems while the rest three perform quite well in shallow one. \n\nCould you elaborate more on ' implementation improves upon existing models '? Or do you mean existing algorithms such as TOT and RAP? If so, please check the following answer to Question 5.\n\n**Question 2: Misapplication of RLHF Task**\n\nWe do explicitly compare our methods to existing approaches like PPO and rejection sampling-based training. That is mainly in Q5 and Table 6 when we discuss how such a process can guide training.\n\nThank you for your suggestion, we will clarify more about the choice of the reward model and further clarify the specific configuration.\n\n\n**Question 3: In your text, you replaced \"BFS\" with \"BFS-V,\" suggesting a variant. However, this change might not be clear to readers that it is equivalent to Beam Search. For greater precision and clarity, changing the term to \"Beam Search\" would be more appropriate and straightforward.**\n\nThank you for your feedback and suggestions. For the name of BFS-V, it doesn\u2019t mean it\u2019s just a variant. As we mentioned in Section 3.2.2, page 5, it means **\u201cwith value function based tree-pruning\u201d**, suggesting the pruning is done by a learned value function which is different from Tree of Thought (ToT). We do agree with you that it can be called \u201cBeam Search\u201d, however, we maintained the name \u201cBFS\u201d to keep consistency with Tree of Thought paper. And then we added a suffix \u201c-V\u201d to show the main difference between the BFS(Beam Search) algorithm in TSLLM and Tree of Thought. To make it more clear, we also say in the main paper that \u201cBFS-V can be regarded as a beam-search with cumulative reward as the objective\u201d in Section 3.2.2, page 5."
                    }
                },
                "number": 46,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4470/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700563424991,
                "cdate": 1700563424991,
                "tmdate": 1700563510286,
                "mdate": 1700563510286,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "L5cjNrZoZt",
            "forum": "fLO9VaAb3B",
            "replyto": "fLO9VaAb3B",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4470/Reviewer_xk9Q"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4470/Reviewer_xk9Q"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method to use tree search algorithms to improve the decoding capability of LLMs. There are a couple of existing methods that have been applying tree search to improve the reasoning and planning capability of LLMs. For instance, Tree-of-thought (ToT) uses depth/breadth-first search to improve the planning capability of LLMs. However, the paper points out there are two major drawbacks in the prior work. First, the current methods mainly focus on enhancing LLM's reasoning ability. It is unclear if these methods can be applied to different kinds of tasks such as RLHF. Second, the existing methods heavily rely on hand-engineering prompts. As a result, such algorithms lack their applicability and heavily rely on both well-designed prompts and large-scale LM. As a result, the paper proposes a method that is based on MCTS. The main algorithm consists of two major steps: (1) policy improvement: it first generates tree-search trajectories based on the LLM, the value function that predicts the returns, and the reward model that predicts the end return; and (2) policy evaluation: based on the generated dataset from policy improvement, it trains the value function and reward model to further improve the performance. Several experiments are shown in section 4. Table 2 shows the mixed signal of whether the proposed method, MCTS, is better than the existing approach or not. Table 3 shows an ablation study of the MCTS model in Game24 tasks with a different allowable computation budget. Finally, the paper concludes by saying that it proposes a new training paradigm."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The idea of adding a value function and a reward function to further improve the decoding capability of LLM is a natural extension of the direction of applying a traditional planning algorithm to LLMs.\n2. The experiments are very extensive, and the writing about the proposed method is easy to understand."
                },
                "weaknesses": {
                    "value": "1. Based on the result in Table 2, it is unclear if the proposed method is better than the existing method such as BFS (ToT). For instance, in GSM8K, the BFS is better than the proposed methods such as MCTS-\\alpha and MCTS-Rollout. We spent a good amount of time getting value functions and the reward model working, but the performance of the proposed method is still worse than the simple baseline. As a result, I am not convinced that this approach will work better or not.\n2. Second, it seems that most of the experiments are about improving the value function and reward function, not about fine-turning the LLM. For example, if we do fine-tuning Table 2, are we able to improve the performance? Essentially, I want to see more results based on the question 5 written in the paper: Can TS-LLM further train LLM? I am happy to increase the score if the answers are answered."
                },
                "questions": {
                    "value": "See the weakness section"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4470/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698789473787,
            "cdate": 1698789473787,
            "tmdate": 1699636422632,
            "mdate": 1699636422632,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zxyluufAml",
                "forum": "fLO9VaAb3B",
                "replyto": "L5cjNrZoZt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4470/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4470/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xk9Q"
                    },
                    "comment": {
                        "value": "We thank you for your valuable feedback and comments. We have modified several parts in our paper according to your suggestions.\n\n**Question 1: Based on the result in Table 2, it is unclear if the proposed method is better than the existing method such as BFS (ToT). For instance, in GSM8K, the BFS is better than the proposed methods such as MCTS-\\alpha and MCTS-Rollout\u2026 As a result, I am not convinced that this approach will work better or not.**\n\nThank you for your comment.  There are three things we want to clarify:\n1. **We want to clarify that TS-LLM\u2019s DFS/BFS variants are different from that in Tree of Thought (ToT). They are TS-LLM\u2019s variants instead of ToT baselines.** The main difference is that we use a learned value function instead of prompting advanced LLMs, which is one of our important contributions. We have shown in Table 4 the result of BFS and MCTS-alpha on Game24 by prompting LLaMA2-7B model as value function and ORM. It can exactly be regarded as an instance of ToT over small LLMs. Our conclusion is that prompting a small LLM can not offer accurate value estimation and the performance of tree-search is even worse than simple CoT baseline.\n2. Table 2 represents path@1 results. A much more comprehensive result can be seen in our Q4\u2019s experiment. We additionally conduct experiments on search aggregation shown in Figure 3, where you can get Path@N results and scale up your token consumption by running multiple times of search in MCTS/MCTS-alpha/MCTS-rollout/DFS or set larger beam size in BFS. Specifically, the bottom line of Figure 3 can clearly demonstrate the performance of each tree-search algorithm when given the same token consumption. Basically, MCTS-alpha and MCTS-rollout are still dominant in most cases.\n3. At last, we never claim in our paper that MCTS-alpha/rollout is the new SOTA over all evaluation tasks. Our experiments are not designed to show new SOTAs or the most efficient tree search algorithm. **We treat different search algorithms as TS-LLM\u2019s variants and equally present their pros and cons. We believe every algorithm has its limitations and no one can claim superiority in all scenarios.**\n\nTo make this more clear, we\u2019ve also modified this and incorporated all three points in our rebuttal revision. Specifically, we modify the Benchmark algorithms subsection in Section 4.1 to include more clarifications among benchmark algorithms and our experiment objectives. We\u2019ve also modified the results analysis in Q1, to incorporate the Path@N result shown in Figure 3 for a better presentation.\n\n**Question 2: Second, it seems that most of the experiments are about improving the value function and reward function, not about fine-turning the LLM. For example, if we do fine-tuning Table 2, are we able to improve the performance? Essentially, I want to see more results based on question 5 written in the paper: Can TS-LLM further train LLM?**\n\nWe do present experimental results by finetuning Table 2 and Figure 3. The specific results are presented in Table 5 and Table 6. In Table 5, we present how we can leverage tree-search examples in further finetuning the LLM-based value function. In Table 6, we show more comprehensive results over GSM8K and RLHF alignment and present how tree-search examples can increase LLM generation by finetuning LLM policy $\\pi_{\\theta_0}$ to $\\pi_{\\theta_1}$, LLM value $V_{\\theta_0}$ to $V_{\\theta_1}$ and both. Does this address your concerns?"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4470/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699903849845,
                "cdate": 1699903849845,
                "tmdate": 1699903935415,
                "mdate": 1699903935415,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "i3HyttkgBa",
            "forum": "fLO9VaAb3B",
            "replyto": "fLO9VaAb3B",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4470/Reviewer_E6Fb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4470/Reviewer_E6Fb"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a set of MCTS-based algorithms for language generation using LLM that can replace beam search or further fine-tune the LLM. The general idea is to train a value/reward model based on ground truth correctness for reasoning task or rewards in datasets used for RLHF. By unifying the notion of desire paths in a flexible model, the implicit policy of vanilla LLM decoding can be combined with exploration or valuations of incomplete states, so heuristic-search algorithms like MCTS become feasible.\n\nThe paper considers token and sentence action spaces. The resulting set of decoding can be aggregated in multiple ways.\n\nThe experiments use zero-shot Chain-of-Thought (CoT) via fine-tuning as a baseline. The datasets include some common reasoning ones, a new one in reasoning and another one on RLHF. \n\nFor a more fair comparison, the results aim to compare algorithms under similar amounts of tokens generated, allowing COT to generate further paths for posterior aggregation.\n\nUnder the specific hyper-parameters used, the empirical results show MCTS as a promising direction."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Separation of concerns between the LLM as a selection of action/tokens vs the value of a state that can integrate information about past sequences and possible continuations.\n- Multiple algorithms cover most of the meaningful combinations.\n- Attempt to fair comparison on the effort to the number of tokens\n- Diverse set of datasets\n- Systematic combination of algorithms and settings\n- Some discussion on the challenges of the implementation"
                },
                "weaknesses": {
                    "value": "- No details on the value/reward model besides the mention that it is a decoder-only model.\n- Explanation of the algorithms doesn\u2019t clarify when the value/reward models are used.\n\t- Some details are buried in the appendix but need to be clearer.\n\t- In particular, knowing how the evaluation function propagates information for training and during search is critical.\n- Some settings need to be fully explained and unclear what version was used.\n\t- For instance, the appendix says that sometimes intra or inter-tree search is used, but it\u2019s not clear which experiment uses which setting.\n\t- Part of that can be implied from the algorithm descriptions, but that\u2019s not the reader\u2019s job.\n- No discussion on how the hyper-parameters were set.\n\t- This is especially critical for MCTS, as it might be very sensitive to them.\n\t- Many other details, such as the 0.3 in the Diriletch distribution for adding noise, are important.\n- No specific statistics about the behaviour of the algorithms beyond the aggregated number of tokens.\n\t- For instance, knowing the number of roll-outs would be useful.\n- No discussion of the wall-time for decoding\n\t- While the paper discussed implementations, it\u2019s not clear what the actual performance might be. While the implementation can be challenging and improved if this method becomes more popular, we need to know about the current cost of solving the tasks.\n\t- For instance, keeping the tree in GPU memory can be expensive. Moving tensors in and out could reduce GPU utilization.\n- Analysis can be improved\n\t- For instance: *Highlight [page 8]:* The results of CoT-SC ORM-vote@10 underscore the diversity of sampled data in learning a better ORM.\n\t- But this is not affecting the MCTS ones, so the claim about the diversity is only for the ORM.\n\n\nGiven the lack of details on how the hyper-parameters were set, and how sensitive the results are to them, it\u2019s hard to qualify the contribution, and how likely these algorithms are to perform well in other domains. This perception can change depending on the answer of the authors.\n\nOther issues:\n- The notion of aggregation is misleading (Sect 3.2.3)\n\t- Aggregation refers to using a set of things to get something new, but in this case, we are just selecting an output, not combining them.\n- The most relevant work is in the appendix:  *Highlight [page 13]:* The most relevant work, CoRe (Zhu et al., 2022), proposes to finetune both the reasoning step generator and learned verifier for solving math word problems using MCTS for reasoning decoding which is the most relevant work to ours.\n\t- Please fix the reference to ACL 2023.\n- Make sure all references are used in the paper.\n\t- For instance, Self-consistency (4.1) doesn\u2019t cite the reference already in the paper.\n\nSuggestion / minor issues\n- Given that MCTS combines both the policy and the value, perhaps a comparison with BFS or DFS is not the more meaningful. Algorithms based on A* rely on the accumulated cost g \u2014in this case inverse to the likelihood of the prefix\u2014 and h, an heuristic estimating the value of the rest of path. While the value function in the paper does not take into account the length, it\u2019s still possible to consider their combination.\n- Please define the state $s$ by itself, not inside $g$:\n\t- *Highlight [page 4]:* given state s and action a, is known as: g ( s = (x 0:L\u22121, y 0:t\u22121), a = y t) = (x 0:L\u22121, y 0:t).\n- Why defining $g$ in page 4? Section 3.1 uses \\phi_theta\n- *Highlight [page 6]:* This helps us juxtapose the performance\n\t- Juxtapose? Perhaps it\u2019s a meaningful comparison\n- *Highlight [page 7]:* For value and ORM training, the data are generated by randomly sampling the SFT policy\u2019s rollouts on the training set\n\t- Please define SFT. I guess it\u2019s supervised fine-tuning. \n- Section \u201cBackground of Monte Carlo Tree-search Algorihtms\u201d should say which algorithm is closer to alpha-zero."
                },
                "questions": {
                    "value": "- Is there any restriction about the context size of the LLM and the value/reward models?\n- Was any fine-tuning or training using multiple-GPUS or the A800 were used to run independent experiments?\n- Value and reward models\n\t- Is this a single decoder-only model that produces the intermediary values and the final rewards, or are they different models?\n\t- If they are different models, can you report on the difference between v^_theta() vs r^_theta() when evaluated in final states?\n- hyper-parameters:\n\t- How were the hyper-parameters setup, from Sect 4.1, task setup until the constants for MCTS.\n\t- How much budget was used to set these numbers?\n\t- How sensitive are the results to the particular parameters?\n- In general, how hard should it be to adapt the proposed methods to another domain?\n- Is BFS taking into account the likelihood of the previous path? The text only says to keep the ones with maximal value.\n\t- (Suggestion: see comment about A* above)\n\t- If it completely ignores the likelihood, then why does the decoding do something useful?\n- What are the computational requirements for only training the value/reward models without fine-tuning?\n- What is the distribution of wall-time per algorithm and dataset?\n\t- Section \u201cLimitation and future work\u201d discuss how limited is the implementation but it\u2019d still be informative to know what\u2019s the current status\n- Are there any meaningful combinations that were not tested due to complexity?\n\t- Section D includes details but it\u2019s not clear if all the challenges were overcome or some meaningful experiments might be missing.\n\t- This answer should be included in the discussion, or at least in appendix B. \n- Were the samples in the dataset always selected from the train set?\n- Is Path@N counting the number of paths up to a leave node or the number of separated paths? \n\t- For instance, for beam-search with size K, the number of leaves is at most K, but there were other paths partially explored that were discarded along the way.\n- Path@N for CoT-SC variants\n\t- *Highlight [page 7]:* For CoTSC variants, we present Path@N, and N is determined by maintaining the same level of token computation as TS-LLM\u2019s variants\n\t- How is the same level of token computation maintained? For instance, it could be the maximum average of the tokens used for all the variations of MCTS. Or perhaps, it was run with multiple values of N, and then return bigger N under the token of the other method. But I\u2019d expect that to be done per sample."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4470/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699202410927,
            "cdate": 1699202410927,
            "tmdate": 1699636422557,
            "mdate": 1699636422557,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dYvSH2hvKL",
                "forum": "fLO9VaAb3B",
                "replyto": "i3HyttkgBa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4470/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4470/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer E6Fb (1/3)"
                    },
                    "comment": {
                        "value": "We really thank you for your valuable feedback and helpful suggestions. Here\u2019s our response to your feedbacks and questions.\n\n**Question 0: Writing issues and more clarifications for TS-LLM.**\n\nAccording to your suggestion, we\u2019ve made the following modifications and clarifications to our paper to address the writing issue you mentioned:\n1. We\u2019ve corrected the wrong version of the citation of CoRe and cited the self-consistency paper in Section 4.1. \n2. We\u2019ve modified the description of MCTS variants in the paper in Section 3.2.2 and added a comparison of the tree-search algorithms in TS-LLM in Appendix C.2. \n3. We\u2019ve changed the \u201cjuxtapose\u201d on page 6 Section 3.4 into a more precise expression. \n4. We\u2019ve defined the state $s$ in Section 3.2.1 outside the transition function $g$. \n5. We\u2019ve defined SFT as \u201csupervised finetuning\u201d in Section 4.1. \n6. We\u2019ve modified the analysis to make it more clear and precise in Q3\u2019s analysis in Section 4.2.\nWe\u2019ve added the necessary clarification about whether intra- or inter-tree search is used in Table 2 and Table 3 and clarify the detailed setups in Appendix D.5.\n\n**Question 1: The notion of aggregation is misleading (Sect 3.2.3). Aggregation refers to using a set of things to get something new, but in this case, we are just selecting an output, not combining them.**\n\nWe totally understand your concern that the notion of \u2018aggregation\u2019 might be misleading. While we adopted it from the previous papers CoT-SC(https://arxiv.org/abs/2203.11171) and   RAP(https://arxiv.org/abs/2305.14992) for consistent terminology.\n\n\n**Question 2: No details on the value/reward model besides the mention that it is a decoder-only model.**\n\nAs we mentioned in section 3.2.1, the value/reward model architecture is simple, we append a value head (which is an MLP that outputs a scalar, e.g. a linear layer) to a decoder-only transformer. For GSM8k, Game24, and ProntoQA, the decoder-only transformer is the pretrained LLaMA2-7b without the `lm_head` module. For the alignment task, the decoder-only transformer is the pretrained GPT2-small without the same output `lm_head`. And we used the pretrained weights as the initialization.\n\n\n**Question 3: No specific statistics about the behaviour of the algorithms beyond the aggregated number of tokens. For instance, knowing the number of roll-outs would be useful.**\n\nThank you for your comment. We want to confirm the meaning of \u2018roll-outs\u2019 here. If it means the number of sequences a certain search algorithm outputs, we\u2019ve already shown it in the first row of Figure 3 and Appendix D.6. We hope for your further explanation about the definition of \u2018roll-outs\u2019 and we will try to show you the results.\n\n\n**Question 4: No discussion of the wall-time for decoding**\n\nThank you for your comment. We\u2019ve added different algorithms\u2019 decoding wall-time of GSM8k, Game24 and ProntoQA in Table 13~15 in Appendix D.9. The results demonstrated the gap of computational efficiency between CoT/CoT-SC decoding and tree-search algorithms in TS-LLM, due to the complicated search procedures and extra computation introduced by calling value functions in the intermediate states. \n\nWe totally agree that computational efficiency is an important issue and address it in Appendix B. But note that our current implementation is just an algorithm prototype without any engineering acceleration. We are continuously working on accelerating our tree search framework codebase including caching past key/values on the tree to eliminate repeated computations and offloading tensors on RAM, etc. \n\n\n**Question 5: Given that MCTS combines both the policy and the value, perhaps a comparison with BFS or DFS is not the more meaningful. Algorithms based on A\\* rely on the accumulated cost g \u2014in this case inverse to the likelihood of the prefix\u2014 and h, an heuristic estimating the value of the rest of path. While the value function in the paper does not take into account the length, it\u2019s still possible to consider their combination.**\n\nYes, we agree with your ideas. This can actually be viewed as a dense reward setting in which, at each step, there is a nontrivial reward. It is natural and heartening to extend our method to this setting like extending from AlphaZero(https://arxiv.org/pdf/1712.01815.pdf) to MuZero(https://www.nature.com/articles/s41586-020-03051-4).\n\n**Question 6: Is BFS taking into account the likelihood of the previous path? The text only says to keep the ones with maximal value. If it completely ignores the likelihood, then why does the decoding do something useful?**\n\nNo, BFS can be viewed as beam search with our learned value and ORM. However, the likelihood is not totally ignored. It will implicitly influence the search process since the actions proposed by LLMs are sampled w.r.t. the likelihood and then treated as i.i.d. sentences/actions in BFS. The decoding is useful since BFS will leverage the value function to choose the top K sentences/actions at each step."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4470/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699903345653,
                "cdate": 1699903345653,
                "tmdate": 1699904097314,
                "mdate": 1699904097314,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qp0PfX94w1",
                "forum": "fLO9VaAb3B",
                "replyto": "i3HyttkgBa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4470/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4470/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer E6Fb (2/3)"
                    },
                    "comment": {
                        "value": "**Question 7: Why defining $g$ in page 4? Section 3.1 uses \\phi_theta**\n\nWe guess you mean $\\pi_\\theta$ in Section 3.1. They have different meanings here, $\\pi_\\theta$ is the policy that takes an input $x$ and outputs actions. While $g$ is defined in the dynamics transition function of the problem, for the tasks we are testing now the transition function is deterministic, i.e. appending output tokens to the prefix/prompt string. The transition function $g$ can be considered being influenced by the LLM policy $pi_\\theta$ as $g(s, a) = g(s, a \\sim \\pi_\\theta(\\cdot | s))$\n\n**Question 8: Is there any restriction about the context size of the LLM and the value/reward models?**\n\nThe context size of the LLM and value/reward models are restricted by the pretrained length of the LLM and the decoder-only transformer in the value/reward model.\n\n**Question 9: Was any fine-tuning or training using multiple-GPUS or the A800 were used to run independent experiments?**\n\nAs we mentioned in Appendix D.2, the finetuning and training were conducted on 8 NVIDIA A800 GPUs.\n\n**Question 10: Is this a single decoder-only model that produces the intermediary values and the final rewards, or are they different models?**\n\nYes, we use one decoder-only transformer model with a value head to output scalars at each token.\n\n**Question 11: How were the hyper-parameters setup. How much budget was used to set these numbers? How sensitive are the results to the particular parameters?**\n\nDuring our experiments, we found that the performance and token consumption are sensitive to several hyperparameters: first is the general hyperparameters of building the search trees, e.g. tree-max-depth and tree-max-width, for the former, we choose the max-depth according to the statistics of the distribution of the number of steps from the dataset sampled by the LLM policy on the training set; while for the latter we\u2019ve shown in Table 3 that there is a trade-off between token consumption and the size of the search space.\nMCTS variants (MCTS-alpha, MCTS-rollout and MCTS) are also sensitive to hyperparameters, we keep most hyperparameters as the default values used in AlphaZero and MuZero except for $c_{init}$ which we select a value from two possible ones {0.3, 3.0} when computing $c_{puct}$ to control exploration and exploitation in MCTS variants.\nFor the Dirichlet noise, this might be important since there is some guess that DeepMind sets this hyperparameter according to the statistics of the dataset collected for each task. However, due to the limit of computation resources, we just adopted the default value in AlphaGo as 0.3, which is specified for chess (refer to Page 14 in the AlphaZero paper (https://arxiv.org/pdf/1712.01815.pdf) and Appendix B in MuZero(https://www.nature.com/articles/s41586-020-03051-4))."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4470/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699903411221,
                "cdate": 1699903411221,
                "tmdate": 1699903741249,
                "mdate": 1699903741249,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "82ifb06vsj",
                "forum": "fLO9VaAb3B",
                "replyto": "i3HyttkgBa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4470/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4470/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer E6Fb (3/3)"
                    },
                    "comment": {
                        "value": "**Question 12: In general, how hard should it be to adapt the proposed methods to another domain?**\n\nOne of the contribution of TS-LLM is that it is generally applicable. The minimal requirement of our proposed framework is a set of problems/tasks and an oracle reward function(maybe rule-based) on them. Therefore,  this requirement can be satisfied for a wide range of NLP tasks and decision-making tasks.\n\n**Question 13: What are the computational requirements for only training the value/reward models without fine-tuning?**\n\nFirst, we need to do inference on the training problem set with the LLM policy to sample enough data for value/reward training. The value/reward model are then trained on such data. Since the network architecture of the value/reward model we used was almost the same as the LLM policy, finetuning such a model has a similar computational requirement as finetuning an LLM. Although we did update all weights of the value/reward model which have the same size as the LLM policy which indeed requires advanced GPUs like A800, for users with limited computational resources, using a smaller decoder-only transformer or techniques like LoRA would be an efficient and effective solution.\n\n**Question 14: Are there any meaningful combinations that were not tested due to complexity?**\n\nThere are a lot of potentials to try and we have listed some in Appendix B discussing limitations and future work. For example, more complex tree construction strategies such as mixed sentence/token-level node expansion, adaptive node-max-width w.r.t. depth, and rejecting semantically similar action nodes to increase search efficiency and diversity.\n\n**Question 15: Were the samples in the dataset always selected from the train set?**\n\nYes. For the finetuning of both the LLM policy and the value/reward model, we construct the dataset from the samples in the training set.\n\n**Question 16: Is Path@N counting the number of paths up to a leave node or the number of separated paths? For instance, for beam-search with size K, the number of leaves is at most K, but there were other paths partially explored that were discarded along the way.**\n\nFor BFS(beam-search) and DFS, the answer is the number of separated paths. For MCTS variants, this means running the search algorithm N times, they are not ensured to be distinct.\n\n**Question 17: How is the same level of token computation maintained? For instance, it could be the maximum average of the tokens used for all the variations of MCTS. Or perhaps, it was run with multiple values of N, and then return bigger N under the token of the other method. But I\u2019d expect that to be done per sample.**\n\nWe just choose N which is close to the most tokens used in all tree search algorithms. We want to clarify here such a process is reasonable. We\u2019ve counted the number of tokens for the CoT outputs of each task, we found that the number of tokens among every problem in all tasks has small standard deviations compared to the average values (e.g. for GSM8k it\u2019s $101 \\pm 36$, for Game24 it\u2019s $79 \\pm 3$, for ProntoQA it\u2019s $92 \\pm 17$)."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4470/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699903443272,
                "cdate": 1699903443272,
                "tmdate": 1699903755229,
                "mdate": 1699903755229,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yTCsEGdH0Y",
                "forum": "fLO9VaAb3B",
                "replyto": "i3HyttkgBa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4470/Reviewer_E6Fb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4470/Reviewer_E6Fb"
                ],
                "content": {
                    "title": {
                        "value": "Thank you"
                    },
                    "comment": {
                        "value": "Thank you for your comments.\n\nOn BFS: since this is not using the LLM likelihood at all, the algorithm seems to be out of the scope of the paper. Right?\nIf you want to keep it, this needs further justification.\n\nInstead of BFS, the natural algorithm to compare is A*. For DFS, the alternative would be IDA*, which might be tricky to implement.\nI think that A* is the baseline for comparison, no BFS.\n\nA* uses the accumulated cost (cost is inverse to the accumulated likelihood) and an estimation of the future.\n\nRegarding the roll-out, we generally want to understand the search effort besides the implementation detail. A typical metric in search is the number of expansions: how many LLMs calls. Another one is the number of calls to the other models. As the different algorithms use some models and not others, this would reinforce the explanation of the algorithm. The roll-outs may only be for the cases where the leaves are reached.\n\nThese metrics are crucial to understand the trade-off of the algorithms."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4470/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700087367954,
                "cdate": 1700087367954,
                "tmdate": 1700087634246,
                "mdate": 1700087634246,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uM2XzkP96y",
                "forum": "fLO9VaAb3B",
                "replyto": "i3HyttkgBa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4470/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4470/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer E6FB"
                    },
                    "comment": {
                        "value": "Really thank you for your feedback!\n\nFor the BFS/DFS algorithm, we do not think it is out of the scope of our paper. Firstly, we want to clarify that, in our paper, DFS/BFS are TS-LLM\u2019s variants instead of ToT baselines. We never claim in our paper that MCTS-alpha/rollout is the new SOTA over all evaluation tasks. We believe every algorithm has its limitations and no one can claim superiority in all scenarios. For example, as we illustrate in our analysis, BFS/DFS generally perform well in shallow search problems. \nSecondly, for MCTS/MCTS-alpha and MCTS-rollout, they need the LLM likelihood and LLM value function, while for DFS/BFS, they still need the LLM value function to help prune the tree (and implicitly rely on the log probability to expand nodes by sampling). Learning a value function is one of our important contributions and DFS/BFS can help us verify the quality of value function. Could you explain more on why it is necessary to explicitly include LLM likelihood in the search process?\n\nFor the A* algorithm, we fully agree this is another search algorithm we should include in TS-LLM's variants. We are working on adding this algorithm, but it takes some time to rerun all our tasks and tune the search hyperparameters.\n\nFor the rollout, the token number we monitor in our experiments (token number in Table 2 and the x-axis in the second row of Figure 3), can exactly represent the node expansion times. This is another contribution we have - we want to conduct a fair comparison (Section 3.4) which prior works neglect. We sum up all the new tokens generated by LLM every time the node expands. In addition, we think the statistic of the number of generated tokens is better than mere node expansion times. Note that it also takes the expense of each expansion into consideration since it is possible that different node expansions may generate different lengths of sentences (number of tokens), resulting in varying computation expenses. \n\nAbout another metric like the number of calls to other models, as we described in TS-LLM, we do have calls of the LLM value/reward model. But again, the computation expense of calling value/reward model after each expansion can be estimated accurately by the number of newly generated tokens."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4470/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700105595761,
                "cdate": 1700105595761,
                "tmdate": 1700107323378,
                "mdate": 1700107323378,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Jx6BaL3u4L",
                "forum": "fLO9VaAb3B",
                "replyto": "i3HyttkgBa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4470/Reviewer_E6Fb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4470/Reviewer_E6Fb"
                ],
                "content": {
                    "title": {
                        "value": "Good point"
                    },
                    "comment": {
                        "value": "Good point. If somehow BFS was considering all the tokens, then it would not be using the LLM likelihoods. That BFS would be out of the scope of the paper. However, BFS must consider all the tokens. \n\nSo, the algorithm called BFS in the submission is not BFS because it\u2019s using the LLM likelihood to prune the tree. So it\u2019s actually a two steps process: that might be mimicking A*: the token continuation discarded can be seen has having infinite cost g. The ones preserved have a finite cost depending on the depth. The models proposed in the submission are used to decide among the new nodes, such that older nodes are always expanded first. \nThis can be exactly A* if we consider the cost g increasing by a number bigger that the maximum value that the models can provide. I think it\u2019s, so just increasing g by 2 with each token or sentence (don\u2019t remember now) should be enough to see the BFS of the paper as an instance of A*.\n\nSo, I agree now that BFS is in a the scope of the paper. (Here is for me a demonstration of the value of an interactive discussion with the authors).\n\nHowever, the name BFS not right, even if the previous work using BFS for LLM use it. As for the aggregation, the fact a few papers of a few years ago used a terminology doesn\u2019t mean that new papers should continue to pollute with confusion the archive of accepted papers. It\u2019s totally ok to use a different term, as far as the manuscript mentions that previous recent work use another term.\n\nI suggest not using *aggregation* and I suggest that just calling this algorithm BFS leads to confusion. Perhaps it\u2019s a matter of emphasizing that this is doing BFS in the space after pruning with the LLM.\n\nThat view suggests that the results might be very different depending on the amount of pruning. No pruning doesn\u2019t use the LLM. Prune greedily, keeping one solution is greedy search. In the middle, more or less pruning might lead to very different behaviour. Less pruning is like higher recall while BFS chooses among them.\n\n*New question*: what\u2019s the fundamental difference between beam search and BFS? Perhaps beam search is just fixing how many solutions are preserved while BFS might take a different number of nodes at different points of the search. Perhaps flexible beam search would be a more precise name.\n\n*new question*: did you test different amount of pruning for BFS? Without such variation, this quote in the last comment doesn\u2019t hold:\n\n> DFS/BFS can help us verify the quality of value function. \n\nBecause there selection of the amount of pruning might depend on the domain, and finding a the magic amount of pruning be non trivial, especially if going off distribution during testing.\n\nLet\u2019s call Oracle-BFS the one using the optimal pruning by LLM for producing the result. The performance of Oracle-BFS could dominate in some domains variations ot MCTS that doesn\u2019t have such oracle and doesn\u2019t commit to a specific amount of pruning. On the contrary, BFS as in the paper  might not say much because perhaps the pruning is close to optimal or suboptimal. The demonstration is interesting but I don\u2019t think it\u2019s more than a demonstration.\n\nI remain open to the possibility that further explanation on the pruning might convince me otherwise.\n\nThanks"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4470/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700107757745,
                "cdate": 1700107757745,
                "tmdate": 1700107799529,
                "mdate": 1700107799529,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "euXdYb0EDR",
                "forum": "fLO9VaAb3B",
                "replyto": "Jx6BaL3u4L",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4470/Reviewer_E6Fb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4470/Reviewer_E6Fb"
                ],
                "content": {
                    "title": {
                        "value": "Got It"
                    },
                    "comment": {
                        "value": "Ok. It was right there in the manuscript:\n\n> BFS can be regarded as a beam-search with cumulative reward as the objective. In BFS, nodes with k-largest values are maintained and expanded in each step.\n\nSo, perhaps BFS can be the name, but the significance depends on the value k more heavily than other algorithms. \n\n*question*: what is the k used for BFS? Is it the Tree Max Width in table 1?\n\n*question*: how sensitive are the results to the selection of k for BFS and related parameters in the other algorithms?\n (this might be have been answered  above but it\u2019s a good recap) \nAn algorithm more robust to k is more promising for new domains."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4470/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700108293432,
                "cdate": 1700108293432,
                "tmdate": 1700108293432,
                "mdate": 1700108293432,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "91Vm0DMaJJ",
                "forum": "fLO9VaAb3B",
                "replyto": "euXdYb0EDR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4470/Reviewer_E6Fb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4470/Reviewer_E6Fb"
                ],
                "content": {
                    "comment": {
                        "value": "Regarding my last question: path@1 is reported as equivalent to greedy search but for path@N -when applies- and the default setting is less clear.\n\nThe comment above says:\n\n> while for [tree max width] we\u2019ve shown in Table 3 that there is a trade-off between token consumption and the size of the search space. \n\nTable 3  shows variations for Game24 but not for the others.\n\n**KEY POINT**:\n\n**The so-called \u201cnode construction\u201d studied in Question 2 or the paper might be a very hard to tune parameter. Therefore, the value of the contribution might be tainted as finding the right number might be very hard. ideally, we should see tables like table 3 but for the other two domains.**\n\nSo my question about the sensibility becomes more critical. For instance, Table 3 suggests that if pruning of aggressive (only 6), then perhaps BFS would be simpler, cheaper and better.\n\nI\u2019m really curious about how the max width was set. Even looking at the dataset is itself problematic as a the method would depend on expertise about the domain.\n\nI\u2019m afraid I\u2019m a bit concerned about this issue and it might affect my scores negative. I hope this discussing dissipate my increasing doubts.\n\nI\u2019ll stop for now. Two last comments:\n* what about calling it \u201creward beam search\u201d instead of BFS?\n- the term \u201cnode construction\u201d is also confusing. I understand the engineering motivation but it should be called something like pruning or selection. Expansion or prediction might work. Otherwise, I\u2019d check the terminology for the popular beam search that is usually more crisp."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4470/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700110263763,
                "cdate": 1700110263763,
                "tmdate": 1700110263763,
                "mdate": 1700110263763,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "axPPayOVch",
                "forum": "fLO9VaAb3B",
                "replyto": "i3HyttkgBa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4470/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4470/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer E6Fb (1/2)"
                    },
                    "comment": {
                        "value": "Thanks for your feedback and helpful suggestions. Here\u2019s our response to your questions.\n\n**Question 1: What is the k used for BFS? Is it the Tree Max Width in table 1? Did you test different amount of pruning for BFS? How sensitive are the results to the selection of k for BFS and related parameters in the other algorithms?**\n\nLet us explain this more clearly. **Tree-max-width and k are two different hyperparameters. Tree Max Width determines the number of child nodes every time the tree expands while k represents the number of child nodes we maintain (based on top k value) from the whole candidates.** For example, assuming we maintain k nodes at depth d, when expanding nodes for depth d+1, we will expand these k nodes to have **(k \\* tree-max-width) child nodes, from which we select the top k nodes. After this, we have k nodes at depth d+1 and the search continues**. The k can also be treated as beam size, which is a hyperparameter to control how many distinct paths are to be sampled on the tree in the BFS(Beam-Search) algorithm. \n\nAnd Yes, we do test it in Q1/Q4 and Figure 3. The beam size k controls the max number of returned sequences and it corresponds to $N$ in Path@N experiments, as is shown in the first row of Figure 3. To make the setting of Path@N results clearer, we\u2019ve also added explanations about the detailed settings used in the aggregation experiments in Appendix D.9.\n\nAbout the sensitivity of k, our experiment in Figure 3 has shown that, in GSM8k/ProntoQA/Game24, the performance of BFS is proportional to the size of k (It is reasonable because we have an ORM to help rerank all final k solutions to determine a final solution, and larger k represent larger candidate space). RLHF experiment is a special case where we find BFS does not work in deep search problems. So if you ignore the extra computational burden brought by increasing k/N, in most cases, you can obtain better performance by using larger K. In this case, you won\u2019t need to determine the specific k. However, with k growing, you also have to consider the extra computational burden because you need to expand/evaluate more nodes in BFS. So this is also a trade off between larger search candidates (larger k) and heavier computation demands.\n\n\n**Question 2: The sensitivity of hyperparameter such as how the max width was set.**\n\nWe fully understand your concern about how sensitive are the results to the hyperparameters. This is exactly what we want to present in our paper. We want to present a comprehensive and systematic analysis about the key components of tree-search guided inference and training in LLMs including alternatives of tree-search algorithms, training of value function, aggregation (or maybe called reranking) of multiple searches, potentials to continuously improve the total framework, and the possible trade-off between different tree construction(expansion) settings (i.e. choices of sentence-/token-level action nodes,  the **tree-max-width** and tree-max-depth).\n\nWe agree with you that tree construction hyperparameters might greatly influence the performance and computation cost of tree-search algorithms. Therefore, we conducted ablations on Game24, which is, among the four tasks tested in the experiments, a typical task to show **how the selection of tree-max-width could affect the search space upper bound and the final performance**. That\u2019s because if one intermediate reasoning step (computation of two remaining numbers) is wrong (impossible to reach the final result of 24), the total subtree from this node is meaningless since all of them should be wrong answers. This phenomenon was also pointed out in the Tree of Thought paper that most failures often happened in the very early steps of Game24.\n\nLet us illustrate more on how we choose the hyperparameter of tree-max-width. For RLHF, we choose it as the default hyperparameter in LLM decoding( top_k=50). For GSM8K/Game 24/ProntoQA, we first experiment with a small tree-max-width and increase it to see the performance gain by increasing the search space upper bound. This is similar with k mentioned in Q1. The larger the search space, the better the performance. Finally, we determine the tree-max-width by fairly considering the tradeoff between the search space limit and computational burden (This is why in Table 3, width=50 is better than width=20 but we still choose width=20 in Table 1). The procedure we follow is a common way for most search algorithms when given a new environment and it is not hard to tune at all. You can always customize your tree search space based on your requirements for the performance and computation budget."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4470/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700139815423,
                "cdate": 1700139815423,
                "tmdate": 1700140353354,
                "mdate": 1700140353354,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "s5RFTWWDDv",
                "forum": "fLO9VaAb3B",
                "replyto": "i3HyttkgBa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4470/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4470/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer E6Fb (2/2)"
                    },
                    "comment": {
                        "value": "**Question 3:  For instance, Table 3 suggests that if pruning of aggressive (only 6), then perhaps BFS would be simpler, cheaper and better.**\n\nFirstly, when we set the tree-max-width as 6, we are not conducting pruning, we are limiting the search space upper bound of all algorithms by limiting the number of child nodes an intermediate node can expand.  And it is totally fine that BFS in some cases is better than MCTS-alpha/MCTS-rollout. We have addressed this problem in our global response and our previous response to you. BFS is one of TS-LLM\u2019s variants instead of merely a baseline. Specifically, we said:\n\nWe never claim in our paper that MCTS-alpha/rollout is the new SOTA over all evaluation tasks. Our experiments are not designed to show new SOTAs or the most efficient tree search algorithm. We treat different search algorithms as TS-LLM\u2019s variants and equally present their pros and cons. We believe every algorithm has its limitations and no one can claim superiority in all scenarios.\n\nAnd the final experimental results validate our belief. BFS/DFS/MCTS algorithms perform well in shallow search problem but struggle in deep search problem (RLHF).\n\n\n**Question 4: What about calling it \u201creward beam search\u201d instead of BFS?**\n\nThank you for your comment. We think reward beam search is also an alternative. But to align with previous papers such as Tree of Thought, we also think it\u2019s reasonable to mention the name BFS. So to consider both, what do you think about terms like BFS-V/DFS-V (abbreviation for BFS/DFS with value pruning)?\n\n\n**Question 5: The term \u201cnode construction\u201d is also confusing.**\n\nWe understand your concerns. We have replaced the term with \u201cnode expansion\u201d in Q2."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4470/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700139861306,
                "cdate": 1700139861306,
                "tmdate": 1700140140662,
                "mdate": 1700140140662,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lksHbDPWwU",
                "forum": "fLO9VaAb3B",
                "replyto": "s5RFTWWDDv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4470/Reviewer_E6Fb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4470/Reviewer_E6Fb"
                ],
                "content": {
                    "title": {
                        "value": "Rest of the answers"
                    },
                    "comment": {
                        "value": "Thank you for the prompt response and the clarifications.\n\nI didn\u2019t acknowledge the rest of the answers. Thank you.\n\nI realized there was a mistake about the take-away from the work, but perhaps was a fair confusion. The work is not about combining extra models with an LLM. The paper says it clearly: this is inspired in AlphaZero and variants. So, it makes sense to use the same pretrained LLM, only adding a head for the value. This is not a limitation but doubling down on the AlphaZero idea where agents live forever in the same game so it makes sense to have a shared representation for the different models, as the models are specialized on a particular task.\n\nIn the submission, \u201clearning a value function\u201d might not convey the right message, as *a value function* might as well be completely independent. It could be a decision tree over the state/text, for instance. Instead, this work is about a form of fine-tuning. \n\nThis confusion was behind my original Q2, Q8, Q10 and Q13.\n\n**This must be clarified since the abstract: the new models are using the same pretrained LLM.**\n\nThat can be in the text, but perhaps a small figure after Fig 1 can illustrate that the experiments are about a given pre-trained LLM is extended with another head, and how those heads are the functions used in the algorithms. It be even better if that figures says which algorithm uses which model.\n\nRegarding other responses, your answer to my original Q3 show there I was a confusion we elaborated in further comments.\n\nI\u2019ll go back to k, N and expansion in the next comments."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4470/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700145941333,
                "cdate": 1700145941333,
                "tmdate": 1700145941333,
                "mdate": 1700145941333,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uUlxQ8Qc4v",
                "forum": "fLO9VaAb3B",
                "replyto": "lksHbDPWwU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4470/Reviewer_E6Fb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4470/Reviewer_E6Fb"
                ],
                "content": {
                    "title": {
                        "value": "Terminology"
                    },
                    "comment": {
                        "value": "In the body of the paper, it is not important to follow the terminology used in a recent paper used, not matter how influential. I understand using those terms in the abstract, while keeping the interpretation open to adjust in the introduction. But from the introduction is better to be precise, and the fast-pace of publication is not helping with that.\n\n* I\u2019m ok with BFS-V. At least it invite the reader to understand what it is, instead of assuming that the algorithm is standard BFS.\n* Node expansion is a good choice as the terminology is used for describing MCTS.\n* Aggregation vs selection."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4470/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700145970381,
                "cdate": 1700145970381,
                "tmdate": 1700145970381,
                "mdate": 1700145970381,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QECURixPMn",
                "forum": "fLO9VaAb3B",
                "replyto": "uUlxQ8Qc4v",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4470/Reviewer_E6Fb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4470/Reviewer_E6Fb"
                ],
                "content": {
                    "title": {
                        "value": "Remove inter-tree for a clear state space?"
                    },
                    "comment": {
                        "value": "For this paper, it doesn\u2019t make sense to look for SOTA, I agree. Instead, the attempt to do a systematic evaluation is much better. However, the systematic evaluation of search algorithms only makes sense in the same state space.\n\nInter-tree is not looking at the same state space, it can be more expensive, and if resampling was crucial, then we might want to revisit all algorithms to allow resampling. For instance, an extension of the other TS algorithms that resamples part of a past beam, equivalent to a *restart* as commonly done in SAT solvers and in some combinatorial optimization search algorithms.\n\nI suggest to move *inter-tree algorithms* to \n* another question in Section 4.2\n* or move it all together to the appendix. \n\nI prefer the latter, as it would allow us to \n* reduce the number of configurations presented in the paper.\n* revise the problem formulation, section 3.1, to end by clarifying that in this paper the the policy is deterministic once the LLM prediction is performed, whether a token level or a sentence level, constituting a proper state space.\n\nSo far it\u2019s mostly adding confusion. That\u2019s almost an idea to be fully studied in a follow-up paper. Restart in combinatorial optimization helps to alleviate the issue of \u201cearly commitment\u201d, as an early decision might lead to not finding good solutions. In this case, resembling might be a way to escape a region full of bad predictions.\n\nMoving away inter-tree to results or appendix would simplify the discussion around tree width, k, N, and expansions. \n\nIn that regard, the variable $k$ is used with multiple meanings:\n\n* In Sect 3.2.2, for MCTS-rollout: \u201ck complete answers\u201d, for BFS: \u201ck largest values\u201d.\n* A comment above says that k is N, at least for BFS:\n\n> The beam size k controls the max number of returned sequences and it corresponds to  in Path@N experiments, as is shown in the first row of Figure 3.\n\nThe record in the literature doesn\u2019t help: sometimes beam-size k is confounded with top-k answers. Let\u2019s not do that.\n\n**Question**: the comments above explain the difference between k and the tree-max-width. To be completely sure: in all the algorithms, does expanding a node/state produce *tree-max-width* states?\n\nIf so, then I suggest saying in 3.1 that the state space is determined by the LLM policy that requires a hyper-parameter that we will call the tree-max-width.\nAnd say in 4.1 that the tree-max-width defines the state space for all the algorithms. This is simpler to explain by moving inter-tree to a later question or the appendix. I think it\u2019s a distraction. I understand the temptation given the interesting performance of MCTS-alpha inter-tree in Fig 3, Game 24. But that shouldn\u2019t matter in a paper that is not trying to claim SOTA."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4470/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700147482634,
                "cdate": 1700147482634,
                "tmdate": 1700147482634,
                "mdate": 1700147482634,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6By6uG9707",
                "forum": "fLO9VaAb3B",
                "replyto": "QECURixPMn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4470/Reviewer_E6Fb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4470/Reviewer_E6Fb"
                ],
                "content": {
                    "title": {
                        "value": "Task parameters \u2014 a key issue"
                    },
                    "comment": {
                        "value": "Thank you for your comments on the parameter selection. This is subtle but important.\n\nOne thing is changing the parameters of the algorithm, and something else is changing the parameters of the task. Table 1 is like a description of the dataset. The amount of train and test data is given in the dataset, then why are tree max width and tree max depth there? **Please make that clear**. I suggest moving those columns to the right of the table, adding a vertical bar to separate them, and explaining in the caption that they were tuned for this paper.\n\nIt\u2019s very hard to remove the feeling that the results might be different from other values for tree-max-width/depth. In any case, the paper must include a description of the protocol for choosing the parameters in detail, including values tested. \n\nAbove you said:\n\n> For GSM8K/Game 24/ProntoQA, we first experiment with a small tree-max-width and increase it to see the performance gain by increasing the search space upper bound.\n\nI don\u2019t know what\u2019s small and how much is the performance gain. I do know that changing the tree-max-* parameters changes the state space and that search algorithms behave differently in different search spaces.\n\nMoreover, which performance gain was observed? If the paper is accepted, the reader might wonder if the gain of the MCTS variants was more important in the selection.\n\nAbove you said:\n\n> This is why in Table 3, width=50 is better than width=20 but we still choose width=20 in Table 1\n\nTable 3 can also be read as if one is satisfied with under 50% performance, the algorithm doesn\u2019t matter. If one needs over 70% performance, the algorithm doesn\u2019t matter. However, there are some state spaces where the proposed algorithms might offer an advantage.\n\n**Determining the width and depth of a problem is not trivial, at all**. They change the task because they change the state space. The fact that the authors considered it easy for the dataset tested in the submission. Suppose we were using these algorithms for code generation or for task-oriented dialogue. It\u2019s plausible that the different samples require a diversity of depth, and perhaps of width. If the paper were concerned with searching within the same state-space, and results were presented with different tree-max-width/depth, then the takeaway is that if we have a way to tune those parameters in the task, then the algorithms become relevant.\n\nI think making this clear would make the paper stronger. That would also justify better the notion of comparing the same number of tokens, as they are part of the same state space.\n\nConclusion-wise, the observations for RLHF and Game24 are more clear. For the first, the default setting was used (make sure you cite who set that value to 50). And for Game24 we get some different scenarios. For the other two tasks, I just don\u2019t know.\n\nI\u2019m not sure if we should accept the paper without a table similar to Table 3 but for GSM8k and PrOntoQA. Even then, we need a protocol of how the specific widths were selected. (By the way, please add to section E qualitative examples for GSM8k and PrOntoQA. According to Table 1, they should be small).\n\nThe wall time in section D.9 shows that the time for Game24 and  PrOntoQA is comparable, so perhaps that\u2019s feasible there.\n\nQuestions about D.9: Is this the wall time of all the predictions or a mean per task? That\u2019s not clear in some parts of the body of the paper when the number of tokens is discussed. Please double-check that."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4470/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700150701248,
                "cdate": 1700150701248,
                "tmdate": 1700150701248,
                "mdate": 1700150701248,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "N3iCaFT4UE",
                "forum": "fLO9VaAb3B",
                "replyto": "6By6uG9707",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4470/Reviewer_E6Fb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4470/Reviewer_E6Fb"
                ],
                "content": {
                    "title": {
                        "value": "Other comments"
                    },
                    "comment": {
                        "value": "At this point, I still think the submission could be better at avoiding confusion about k and path@N. path@N is about the path but it doesn\u2019t describe the expansion of a state. By the end of section 3.2, this should be clear.\n\nLet me add something to my previous comment titled \u201cRemove inter-tree for a clear state space?\u201d: The comment in D.5 mentions only mentions CoT-SC-Tree. Perhaps it should also mention the other CoT algorithms. \n\nAnother addition: the caption of Table 2 says \u201cSentence-level tasks still have variations by sampling while the token-level task is deterministic\u201d. For sentence-level, the state space changes with the random seed and the order in which the expansions are done. For intra-tree, the sample gets fossilized once the LLM is called, so I suggest emphasizing this when referring to Table 2.\n\nFinally, I invite the authors to discuss the engineering challenge. Even better if that discusses the literature and/or popular libraries like Hugging Face or Triton. If this paper turns out to be influential, it might lead to optimizing LLM libraries so decoding supports the flexibility required for reducing the wall time of algorithms that in principle might be more effective. GPU memory utilization is a key issue: it might have contributed to making beam search the default choice, as beams might be easier to vectorize as sparse expansions revisit other parts of the search space. Future hardware might provide better support. Nvidia\u2019s push for supporting a form of sparsity is motivated by the actual requirements of the algorithms. \n\nI'll stop my comments for a couple of days. I think another round should make this clear. Please take your time."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4470/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700150790581,
                "cdate": 1700150790581,
                "tmdate": 1700150790581,
                "mdate": 1700150790581,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MYEe7DBgEZ",
                "forum": "fLO9VaAb3B",
                "replyto": "N3iCaFT4UE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4470/Reviewer_E6Fb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4470/Reviewer_E6Fb"
                ],
                "content": {
                    "title": {
                        "value": "Summary"
                    },
                    "comment": {
                        "value": "This is a quick note to summarize my request in the long comments posted above after your last answer.I\n\nFrom my perspective, **the key missing pieces are**:\n\n1. Clarify that the search space depends on the parameters max-width and max-depth, and that is *not* a parameter of the algorithm.\n1. Confirm whether in intra-tree the result of the LLM policy is fossilized also for sentences. That is, for a single sample, the LLM is called only once per input, and then it\u2019s all reused. Remark that as each algorithm expand nodes in different order, even with the same seed, the state space is different, but at least is fixed and of similar size without chance to resampling.\n1. A table similar to Table 3 but for GSM8k and PrOntoQA. If one of them is missing, then that dataset should be presented as an extra result or limited experiments. Perhaps as an additional question in the result section. When presented the tasks, mention that such domain, the experiments were limited.\n1. A more detailed description of the protocol for setting the max-width, including how additional values for max-width are selected for the new variations of table 3.\n1. Clarifying that comparison among search algorithms is better within the same state space, and that different width might lead to different trade-off. No SOTA.\n1. Make a decision about inter-tree. If there is only one algorithm doing that, then perhaps move to question in 4.2 of to appendix. If some of the baselines is also doing inter-tree, then consider a separated table, or a) organize the rows in the current tables so comparable numbers are contiguous, and b) use different line style (continuous vs dashed?) in the figures.. In any case, clarify that the number of tokens is not comparable.\n1. Clarify that the proposed models are implemented as an additional head on top of the same representation, so the models cannot be used in another LLM without re-training or paying the price of decoding twice with a large model. (Btw, question: when processing a sample, are the models added to the same architecture, so the new model produces both the new tokens and then estimations? Or are those independent calls with shared weights?\n1. Make sure that for all the hyper-parameters, the protocol for selection is discussed, as well as the sensibility to the parameter. For instance, in D.4 sometimes the same parameters are used for 3 task, and a different value for another one ($c_{init}$, number of simulations before setting an action). In particular, discuss the computation cost of setting those values. \n\nExtra points.\n* There is no need to test in other LLMs. My point was that the paper should be written as it happened: it was always the same representation, and that\u2019s ok. Cite state of shared weights in AlphaZero and other work.\n* Table 3 has no variance but the appendix says that 3 seeds were used.\n\nThere are other actionable requests and comments in my previous comments, please address them in a revised version too. For instance, the lack of qualitative examples for 2 of the 4 datasets."
                    }
                },
                "number": 27,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4470/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700230054262,
                "cdate": 1700230054262,
                "tmdate": 1700230054262,
                "mdate": 1700230054262,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aXWxNtmCoZ",
                "forum": "fLO9VaAb3B",
                "replyto": "i3HyttkgBa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4470/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4470/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer E6Fb (2/3)"
                    },
                    "comment": {
                        "value": "**About the Value function:**\n\n**Question 8: Clarification that the proposed models are implemented as an additional head on top of the same representation. When processing a sample, are the models added to the same architecture, so the new model produces both the new tokens and then estimations? Or are those independent calls with shared weights?**\n\nThank you for your suggestions. We choose to use the term \u201cLLM-based\u201d value in our paper to help clarify that we use an LLM based value function. We also have made more clarifications in Section 3.2.1 and the model and training detail subsection in Section 4.1. Specifically, we describe that \u201ctypically, LLM value\u2019s decoder is adapted from original LLM policy $\\pi_\\theta$\u2019s decoder, or alternatively, the LLM value $v_\\theta$ and policy $\\pi_\\theta$ can have a shared decoder\u201d. \n\nLet me clarify our current implementation. In our current implementation, we use a separate LLM policy and LLM value function and their decoders are not shared. This is because our critic learning needs to have training samples from the SFT policy (So our critic can be seen as an on-policy value function for the SFT sampling policy). Our experimental procedure is: We first finetune the base LLM using CoT examples in the training set, to form SFT policy. Then we sample from the SFT policy to construct the critic learning training examples. And finally we get our LLM-based value function by finetuning the base LLM (with additional value heads) using these critic training examples. Though the decoders are not shared among policy and value, they are adapted from the base model. So they may still be quite similar.\n\nAnd we agree with you that the shared structure might be better for computational efficiency since you do not need to call the model twice and maybe a shared decoder can get rid of all burdens brought by value function evaluation (you can reuse most computation from the policy generation). We also mention it in Appendix D.11 when discussing engineering potentials. There are several ways to achieve it. We can:\n1. Instead of using SFT-tuned model, we can use few-shot prompted based model to generate the samples for value function training. And then we train the SFT tuned policy and value function with shared decoders.\n2. We have some offline data that help me train the value function in advance.\n3. We are doing the training in an online manner so the critic function can be iteratively trained, similar to the setting in AlphaZero.\n\nCurrently, we are working on adding a new experiment about the shared decoder by using the same data we train the separate value and policy LLM. We want to show how such model sharing might influence the performance and also efficiency. \n\n**Question 9: There is no need to test in other LLMs. My point was that the paper should be written as it happened: it was always the same representation, and that\u2019s ok. Cite state of shared weights in AlphaZero and other work.**\n\nThank you for your suggestions. Please refer to our response in Q8 and we have cited the state of shared weights in AlphaZero (Figure 1.b in AlphaZero\u2019s paper).\n\n**Other topics:**\n\n**Question 10:  Make sure that for all the hyper-parameters, the protocol for selection is discussed, as well as the sensibility to the parameter.**\n\nWe add a new subsection in Appendix D.10, to fully discuss our protocol for choosing all tree-search-related hyperparameters in our paper.\n\n**Question 11: Discussion about engineering challenge.**\n\nThank you for your suggestion. We added 5 points in Appendix D.11 (previous D.9) about the engineering challenges and how we can further increase the efficiency with more engineering efforts."
                    }
                },
                "number": 30,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4470/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700416163589,
                "cdate": 1700416163589,
                "tmdate": 1700433196199,
                "mdate": 1700433196199,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wSPOV33dGH",
                "forum": "fLO9VaAb3B",
                "replyto": "iWvbMrmSD1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4470/Reviewer_E6Fb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4470/Reviewer_E6Fb"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks. This is very thorough. I\u2019ll check in detail later. Meanwhile, on Q8: do those fine-tuning freeze any layer or update everything? In any case, discussing that would be insightful. Look at the success of LORA and other methods that offer cheap fine-tuning. \n\nI\u2019m looking forward to seeing results over different tree-width for the other two datasets. The reader should see them so they judge themselves, so the possible doubt about fine-tuning the treewidth disappears. It might be good to see cases if that happens, where MCTS has disadvantages."
                    }
                },
                "number": 32,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4470/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700427090653,
                "cdate": 1700427090653,
                "tmdate": 1700427090653,
                "mdate": 1700427090653,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JtRDEwZJ4y",
                "forum": "fLO9VaAb3B",
                "replyto": "wSPOV33dGH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4470/Reviewer_E6Fb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4470/Reviewer_E6Fb"
                ],
                "content": {
                    "comment": {
                        "value": "Aggregation: good point. That\u2019s technically like a marginal but don\u2019t bother formalizing that.  Just mentions that the aggregation can just select and answer or do another operation."
                    }
                },
                "number": 33,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4470/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700427192448,
                "cdate": 1700427192448,
                "tmdate": 1700427192448,
                "mdate": 1700427192448,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZSYxPKQ7vz",
                "forum": "fLO9VaAb3B",
                "replyto": "JtRDEwZJ4y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4470/Reviewer_E6Fb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4470/Reviewer_E6Fb"
                ],
                "content": {
                    "comment": {
                        "value": "I went over the PDF. It\u2019s more clear now.\n\nRegarding the new tables like table 3, I think the analysis needs more insights. For instance, it turns out that PrOntoQA is almost saturated. At that point it might not matter which algorithm works better, so it's not very informative. For the analysis, perhaps you might want to take into account the number of tokens. \n\nIt wasn't clear to me in this last quick read whether the numbers of tokens are correctly account for the @10 baseline. I don't remember if that was a fair comparison or not. \n\nJust make sure everything is clear so the information is at hand for the internal discussion among the reviewers.\n\nI won't check for further changes before that. Meanwhile, thank you for the constructive dialogue."
                    }
                },
                "number": 34,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4470/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700449244880,
                "cdate": 1700449244880,
                "tmdate": 1700449244880,
                "mdate": 1700449244880,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YWrw72Qz03",
                "forum": "fLO9VaAb3B",
                "replyto": "r4h5RhUChj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4470/Reviewer_E6Fb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4470/Reviewer_E6Fb"
                ],
                "content": {
                    "comment": {
                        "value": "That\u2019s fair. My comment was confusing. I revisited Q1 in the paper: please ignore my comment about @10. That was precisely for making a more appropriate comparison.\n\nBy the way, Fig 3a, bottom, says #Forward instead of #token. That should be fixed or explained in the caption. Is best-of-three? I didn\u2019t find that anywhere else in the PDF using search. \n\nGoing back to the results: Q1 in the paper is asking the question for a fixed tree-width. Q2 asks about other tree-width. I\u2019m okay with that order as far as the conclusions of the other tree-widths are in the body of Q2. The paper looks more acceptable if Q2 confirms what was shown in detail in Q1. Otherwise, some results hidden in D.6 should have been in the body of the paper.\n\nPhrases like this should be revised to emphasize that this is not a parameter of the algorithm. \n\n> The almost doubled performance boost from 43.8 to 80.7 indicates the **importance of appropriate expansion size and tree max-width**, improving TSLLM\u2019s performance upper bound.\n\nI\u2019d be ok with \u201cimpact of different expansion size and tree-width\u201d. I won\u2019t check the whole paper for them. Please do so.\n\n## Effective search space\n\nI mean, in general, imagine a set of 3 problems where most of the likelihood of prediction of the LLM goes to an unknown k1, k2, k3 sentences/tokens. In that case, setting the tree-width larger than ki won\u2019t help, and reducing the under ki would be detrimental. Of course, more problems don\u2019t have a fixed ki, and many algorithms can be bad at recovering from early mistakes (the hope is a variation of MCTS or A*, or restarts/inter-tree). (This analysis is typical of the search literature, by the way. For a recent paper about beam search, see Sofia Lemons, Carlos Linares L\u00f3pez, Robert C. Holte, Wheeler Ruml: Beam Search: Faster and Monotonic. ICAPS 2022: 222-230)\n\nSo, as you are not looking for SOTA and not exploring systematically the best tree-width, then you can only conclude about what you observed. Do not give too much importance to the chosen tree-width.\n\n### Back to Q2 in the paper\n\n**Required**: The main conclusions in D.6 should be in Q2. Reference Q2 and Table 3 from D.6.\n\nPersonally, I think the search algorithms should matter less shallow the tree, as shallow trees become more of a classification problem. \n\nToo bad PrOntoQA is not very informative in the experiments with other tree-widths. This might be the case where the effective tree-width is just low. Perhaps this would behave differently with an LLM with lower capacity, but that\u2019s out of the scope of the current paper. I\u2019d try that if the paper were rejected, or add it on your own if it gets accepted.\n\nThat's it. Thanks again."
                    }
                },
                "number": 37,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4470/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700479625849,
                "cdate": 1700479625849,
                "tmdate": 1700479625849,
                "mdate": 1700479625849,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GXVTVWnRgB",
                "forum": "fLO9VaAb3B",
                "replyto": "MzbRpsGNve",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4470/Reviewer_E6Fb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4470/Reviewer_E6Fb"
                ],
                "content": {
                    "comment": {
                        "value": "I meant trying, for instance, PrOntoQA with Llama2-7b and different tree-width. Perhaps despite the lower performance, there might be differences across algorithms."
                    }
                },
                "number": 39,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4470/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700501818252,
                "cdate": 1700501818252,
                "tmdate": 1700501818252,
                "mdate": 1700501818252,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fQQzmvJ7XQ",
                "forum": "fLO9VaAb3B",
                "replyto": "GXVTVWnRgB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4470/Reviewer_E6Fb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4470/Reviewer_E6Fb"
                ],
                "content": {
                    "comment": {
                        "value": "Beyond PrOntoQA, I meant that across different tree-width, the proposed algorithms can sometimes offer an advantage, and the disadvantages are no so significant when they appear. In any case, whatever is your conclusion in D6, it should be in Q2."
                    }
                },
                "number": 40,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4470/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700501948962,
                "cdate": 1700501948962,
                "tmdate": 1700501948962,
                "mdate": 1700501948962,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HOPWWAnSev",
            "forum": "fLO9VaAb3B",
            "replyto": "fLO9VaAb3B",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4470/Reviewer_Zruf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4470/Reviewer_Zruf"
            ],
            "content": {
                "summary": {
                    "value": "The work describes a new framework called TS-LLM that enhances the reasoning abilities of large language models (LLMs) during both inference and training. Traditional approaches like Chain-of-Thought and Tree-of-Thought rely on human-designed prompts and methods like beam search or sampling, but they are limited in scope and scalability. TS-LLM overcomes these limitations by incorporating a tree-search algorithm guided by a learned value function, similar to AlphaZero. This framework is versatile, applicable to various tasks, and can work with LLMs of any size without the need for complex prompting strategies. Its effectiveness is demonstrated through empirical evaluations in different areas, including reasoning, planning, and RLHF alignment, showing that it can handle complex tree searches up to 64 steps deep."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The experiment results are impressive.\n- The framework can work on different kinds of tasks. \n- The method makes sense."
                },
                "weaknesses": {
                    "value": "- After ToT, using MCTS to search is not too novel. This paper uses MCTS directly.  However, I think it can still be accepted if this is the first paper that successfully uses MCTS.\n- This paper needs to have more experiments to show the improvement. For example, more models and more tasks. For more tasks, maybe you can find some here \u201chttps://github.com/Ber666/llm-reasoners\u201d. \n- The paper should discuss which kinds of tasks are better for using MCTS rollout and which are better for MCTS alpha."
                },
                "questions": {
                    "value": "Have you tried using both rollout and value together? If the speed of rollout and the speed of the value network are very different, you can reference the paper \u201cMultiple Policy Value Monte Carlo Tree Search.\u201d"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4470/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699591470464,
            "cdate": 1699591470464,
            "tmdate": 1699636422470,
            "mdate": 1699636422470,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wrin61qnvY",
                "forum": "fLO9VaAb3B",
                "replyto": "HOPWWAnSev",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4470/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4470/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Zruf"
                    },
                    "comment": {
                        "value": "We thank you for your valuable feedback and comments.\n\n**Question 1: This paper needs to have more experiments to show the improvement. For example, more models and more tasks. For more tasks, maybe you can find some here \u201chttps://github.com/Ber666/llm-reasoners\u201d.**\n\nThank you for your suggestions. In fact, we do refer to \u201chttps://github.com/Ber666/llm-reasoners\u201d for some of our experiments including GMS8K, ProntoQA and Game24. We also add a brand new RLHF task to demonstrate the general applicability of our TS-LLM framework. You can have a more detailed look at their repo and it turns out that most environments they provide do not contain any benchmark results (the table in their Experiment Results section in README).\n\n**Question 2: The paper should discuss which kinds of tasks are better for using MCTS rollout and which  are better for MCTS alpha.**\n\nThank you for your comment. The only difference between MCTs-Rollout and MCT-alpha is that MCTS-rollout conducts the search from the beginning while MCTS-alpha conducts the search from the last search node. We think MCTS-rollout is just an offline version of MCTS-alpha since it reconducts the full search from the beginning every time. Our conclusion is, basically MCTS-rollout and MCTS-alpha will be good at the same kind of task, while MCTS-rollout can scale up token consumption to search for better generation. According to your suggestion, we have added a discussion to the MCTS-Rollout description in section 3.2.2.\n\n**Question 3: Have you tried using both rollout and value together? If the speed of rollout and the speed of the value network are very different, you can reference the paper \u201cMultiple Policy Value Monte Carlo Tree Search.\u201d**\n\nThank you for your comment and we will refer to this paper. Currently, we use two separate networks to conduct rollout and value generation respectively and the speed of rollout and the speed of value estimation are similar. But note that we can also utilize a shared LLM decoder for both policy and value network so the rollout and value estimation can be conducted simultaneously and be more efficient."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4470/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699903214088,
                "cdate": 1699903214088,
                "tmdate": 1699903214088,
                "mdate": 1699903214088,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "u2nl7hIsdB",
                "forum": "fLO9VaAb3B",
                "replyto": "wrin61qnvY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4470/Reviewer_Zruf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4470/Reviewer_Zruf"
                ],
                "content": {
                    "title": {
                        "value": "Comments"
                    },
                    "comment": {
                        "value": "Q1:\nI know that you have included some of the task. I was just saying that it will be better to have more. Also you didn't replay the using different models part. \n\nQ2:\nI am confused. When we talk about rollout in MCTS, it normally means that after we select a leaf node, we conduct a rollout (simulation) from the leaf node with a rollout policy instead of evaluated the leaf node with a value function. Are we on the same page?\n\nQ3:\nunderstood.  It will still be cool if you can show that the combining version can be best at all tasks."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4470/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700158181881,
                "cdate": 1700158181881,
                "tmdate": 1700158181881,
                "mdate": 1700158181881,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]