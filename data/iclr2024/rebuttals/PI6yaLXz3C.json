[
    {
        "title": "Fairness-Aware Attention for Contrastive Learning"
    },
    {
        "review": {
            "id": "9QdOFUJcfO",
            "forum": "PI6yaLXz3C",
            "replyto": "PI6yaLXz3C",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8894/Reviewer_K853"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8894/Reviewer_K853"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores the combination of fairness methods and attention-based techniques in machine learning to reduce bias and improve model effectiveness. It proposes innovative approaches to minimize bias in machine learning algorithms, particularly in the context of graph-based data. The paper employs attention mechanisms to guide the model to focus on data that is less likely to introduce bias. It also discusses the role of contrastive learning in bringing similar data points closer together in the feature space, contributing to a more equitable model. The paper provides a thorough technical foundation, making it a valuable guide for those interested in the topic."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is notable for its creative fusion of methods to improve fairness and attention-based techniques to combat bias, bringing a fresh perspective to existing research. It lays a solid technical foundation, serving as a detailed guide for those new to the field as well as seasoned experts. Given the growing emphasis on fairness in machine learning, the relevance of the paper is heightened."
                },
                "weaknesses": {
                    "value": "The paper falls short in clearly describing the empirical tests conducted to validate its findings, leaving room for improvement. Questions about the scalability of the proposed methods also remain unanswered, making it uncertain how they would perform on larger datasets or in different domains. In addition, the paper doesn't address the potential trade-offs between fairness and other issues such as accuracy, nor does it explore the ethical considerations associated with using machine learning to reduce bias."
                },
                "questions": {
                    "value": "Please see the Strengths and Weaknesses sections."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8894/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698466599208,
            "cdate": 1698466599208,
            "tmdate": 1699637118928,
            "mdate": 1699637118928,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PoDgu32mYL",
                "forum": "PI6yaLXz3C",
                "replyto": "9QdOFUJcfO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8894/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8894/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Comments"
                    },
                    "comment": {
                        "value": "**Need for further empirical results and experimental details**\n\nPlease see the general comment for all reviewers addressing these points.\n\n**What are the potential trade-offs between and fairness and accuracy?**\n\n**Answer**\n\nWe thank the reviewer for drawing our attention to fairness-accuracy tradeoff. For the colorMNIST task, we find that sparseFARE obtains top fairness and (shared) top accuracy. This implies sparseFARE weakly Pareto dominates the other baseline models InfoNCE, Fair-InfoNCE, and CCLK. For the CelebA task, we observe that SparseFARE Pareto dominates all the kernel models except for Linear and Polynomial. SparseFARE nonetheless attains a better fairness-accuracy tradeoff than Linear and Polynomial and so is still preferable. Additionally noteworthy is that over the course of training we observe sparseFARE improving both accuracy and fairness together, implying that sparseFARE is able to learn representations that do not necessarily need to compromise fairness for higher accuracy.\n\n\n**What are the ethical considerations of using ML to reduce bias?**\n**Answer**\n\nWe thank the reviewer for pointing out this important consideration. We have included the following discussion in Appendix E. We present the discussion here as well for convenience.\n\nIn terms of ethical considerations of using machine learning for bias reduction, we argue that fairness-aware machine learning is a powerful and ethically well-motivated approach to the problem of unfair outcomes in AI, and in particular the conceptualization of the problem we use and our proposed methods also offer benefits in terms of ethics.\n\nWe note that there are two, interconnected prevalent ethical issues in fair ML. The first is that almost all fair ML literature simplifies the problem of fairness to simple binaries and the second is that fairness metrics (which are typically built atop these binaries) and the choice of which to use themselves involve value judgements that can disadvantage certain people. People have intersectional identities and invariably belong to multiple groups simultaneously. When it comes to choosing fairness metrics,  inherent to the majority of approaches in fair ML is that the researcher or practitioner decide what definition of fairness to use for their model. It has been shown that various definitions of fairness are not only mutually inconsistent but also prioritise different groups in different scenarios [1]. In a sense then, solving for fairer ML models only pushes the problem from the model and onto the practitioner, as a \u2018fairer\u2019 model itself advantages and disadvantages different groups under different settings.\n\nThese two ethical considerations motivate the approach of our paper to conceptualise fairness in a more general setting where sensitive attributes can be continuous and multi-dimensional and fairer models are measured in terms of sensitive information removal. This conception avoids the ethical issues of binaries and fairness metrics.\n\nWe do note however that there still exist ethical concerns with our approach in terms of explainability. Measuring fairness by sensitive information removal (by measuring loss from a trained classifier) does not have an intuitive scale or unit of measurement for discussing the fairness or unfairness of a model. Although we can compare two models in terms of which is fairer, saying a model is fair because it scores some number in MSE has little intuitive meaning. Being unable to communicate the specifics of how a learned representation has removed sensitive information and how will affect downstream classifiers risks undermining confidence in fair ML as well and creating accountability problems as well.\n\nDespite the explainability issue, we nonetheless believe that this approach represents a promising and exciting direction in fair ML that deal with substantive existing ethical issues. We hope that one area of future research may be deriving theoretical frameworks that can derive guarantees between sensitive information removal from debiased representations and upper bounds on downstream fairness metrics. This would develop a practical link to well-known ideas of fairness and how unfair outcomes could appear in worst-case scenarios.\n\n[1]: Garg, Pratyush, John Villasenor, and Virginia Foggo. \"Fairness metrics: A comparative analysis.\" 2020 IEEE International Conference on Big Data (Big Data). IEEE, 2020."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8894/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700624631292,
                "cdate": 1700624631292,
                "tmdate": 1700642266330,
                "mdate": 1700642266330,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qV9PYQcXiL",
            "forum": "PI6yaLXz3C",
            "replyto": "PI6yaLXz3C",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8894/Reviewer_Nbdh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8894/Reviewer_Nbdh"
            ],
            "content": {
                "summary": {
                    "value": "This work is concerned with the problem of fair representation learning, and in particular with how to debias representations in contrastive self-supervised learning. The authors identify limitation with current approaches, in that modelling assumptions about bias attribute are too strong, and they suggest instead a way to condition similarity scores between pairs of positives (or negatives) to a bias attribute via a proposed variant of a \u201cself-attention\u201d mechanism. At the same time, they extend their architectural intervention to a sparsified attention scheme using locality-sensitive hashing which the goal of masking out interactions between pairs which may help with the task. In addition, they propose alternative contrastive learning losses for training with supervision from the bias attribute."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Creating methods for debiasing representations learnt from training datasets containing spurious-correlations, label-imbalances, or sensitive attributes is an important problem.\n\nThe authors use existing literature on the relation between self-attention operator and kernels [1] to derive a similarity score for pairs which is conditioned on bias attribute information. Exploring new formulations of conditional similarity scores can be an interesting avenue.\n\n[1] Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and Ruslan Salakhutdinov. 2019. Transformer Dissection: An Unified Understanding for Transformer\u2019s Attention via the Lens of Kernel. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)."
                },
                "weaknesses": {
                    "value": "1. The paper is poorly written. There are notation problems (e.g. in section 2 the matrix $U$ of similarity scores is overloaded, in section 3.3 there is no $z_i$ appearing in the loss even if it is sampled), incorrect math statements (like in section 3.1 that $\\phi(g(y))$ is used to estimate $\\mathbb{E}_{y|z} \\phi(g(y))$), missing important citations (like [1] for the derivations in page 5 of attention as a kernel-based similarity), and often times definitions (such as for the dataset used and the \u201cbias removal\u201d evaluation metric) are not self-contained in the paper.\n2. Novelty concerns: the FAREContrast objective function is essentially the same as the one described at [2].\n3. Sparsifying the attention is poorly motivated, and it incurs a considerable implementation cost for the induced performance benefit over the considered baselines.\n4. Empirical evaluation is very limited. The authors consider a variant of ColorMNIST, which is not described in the paper, and measure the top-1 test accuracy and a bias removal evaluation metric, which is not described. The paper needs to consider more benchmarks, such as CelebA (classifying hair color while the sensitive attribute is gender) [see benchmarks, 3], and evaluate according to fair/group-robust classification performance metrics (instead of iid accuracy), such as a group-balanced (or worst-case) test accuracy (depending on the dataset) and/or Equalized Odds [4, see 5 on how it is applied].\n\n[2] Yao-Hung Hubert Tsai, Martin Q Ma, Han Zhao, Kun Zhang, Louis-Philippe Morency, and Ruslan Salakhutdinov. Conditional contrastive learning: Removing undesirable information in self- supervised representations. arXiv preprint arXiv:2106.02866, 2021c.  \n[3] Sagawa, Shiori, et al. \"Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization.\" arXiv preprint arXiv:1911.08731 (2019).  \n[4] Hardt, Moritz, Eric Price, and Nati Srebro. \"Equality of opportunity in supervised learning.\" Advances in neural information processing systems 29 (2016).   \n[5] Zhang, Fengda, et al. \"Fairness-aware contrastive learning with partially annotated sensitive attributes.\" The Eleventh International Conference on Learning Representations. 2022.  \n[6] Tsai, Yao-Hung Hubert, et al. \"Conditional contrastive learning with kernel.\" arXiv preprint arXiv:2202.05458 (2022)"
                },
                "questions": {
                    "value": "See Weaknesses above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8894/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8894/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8894/Reviewer_Nbdh"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8894/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698874480688,
            "cdate": 1698874480688,
            "tmdate": 1699637118819,
            "mdate": 1699637118819,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "stCpirKAbS",
                "forum": "PI6yaLXz3C",
                "replyto": "qV9PYQcXiL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8894/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8894/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Comments on Notation Problems"
                    },
                    "comment": {
                        "value": "**Notation in section 2 for matrix U is overloaded**\n\n**Answer**\n\nWe thank the reviewer for drawing our attention to this potential source of confusion. We noticed there is a potentially connected typo in which we wrote $U = [e^{f(x_i, y_j)}]_{ij} $ . As U is a matrix with $(i,j)$ element given by these similarity scores, this should of course be that the $[U]_ij = e^{f(x_i, y_j)}$. Regarding overloaded notation, the two references to the definition of U is simply first a general representation of U followed by an instantiation of what U is for our purposes. Specifically, the matrix U is introduced as the data matrix such that when transformed by the values projection matrix W_V, we obtain the values V. That is, $V = UW_V$. Given our FARE set up, the untransformed values in our case is the matrix of similarity scores. Hence in our model U is the matrix whose $(i,j)$ element is given by $e^{f(x_i, y_j)}$. In other words, naming U as the matrix of similarity scores $e^{f(x_i, y_j)} $ is just a specification of the data that takes the place of the values for our model. \n\n\n**In section 3.3, there is no $z_i$ appearing in the loss**\n\n**Answer** \n\nWe thank the reviewer for pointing this out and have fixed the notation. In particular FARE is now defined as an operator that takes as argument the whole batch, $FARE( \\{ (x_i, y_i, z_i) \\}^b_{i=1} )$. This clarifies the fact that FARE computes a finite sample estimate of the conditional similarity score between $(x_i, z_i)$ and $y$ for $y \\sim P_{Z = z_i}$ by taking in information over the whole batch and weighing each sample according to their protected attribute.\n\t\t\n**There are missing citations, for example [1], on page 5 for the derivation of attention as kernel-based similarity, for example**\n\n**Answer**\n\nWe agree that [1] is highly relevant related work and indeed we include reference to it in our Related Work section. However, [1] is not cited here because our derivation doesn\u2019t rely on theorems or results from [1] or other work, aside from the use of the kernel density estimator approach to estimate the joint and marginal densities which is cited. We also note that [1] is different to our work in the sense that [1] takes the attention mechanism and decomposes it into parts that, by replacing with various kernels, derive new and enriched attention mechanisms. Our work, on the other hand, doesn\u2019t involve decomposing attention nor replacing any component of attention with kernel similarity metrics. Our work shows instead that the estimation of the similarity score (when conditioning on some additional attribute) in the contrastive objective can be approximated by an attention mechanism. In this sense we don\u2019t perform any decompositions and we go straight from conceptualising the similarity score as an inner product to deriving an attention mechanism without appealing to kernel similarity metrics in between. For these reasons we agree that [1] is a relevant work to understand some of the research community\u2019s work in enriching attention, however we do not cite it on this page due to the differences mentioned.\n\n**There is an incorrect math statement in section 3.1 that $\\phi(g(y))$ is used to estimate $E_{y | z} [\\phi(g(y)))] $**\n\n**Answer**\n\nWe apologise for the confusion regarding notation. To clarify, we don\u2019t use $\\phi(g(y))$ to estimate $E_{y | z} [\\phi(g(y)))] $. Rather, we use $E_{y | z} [\\phi(g(y)))] $  to estimate $\\phi(g(y))$. $\\phi(g(y))$ is the unknown quantity of interest and we use its expectation to estimate it, expanding the definition of the expectation using joint and marginal densities which we then estimate with kernel density estimators. This allows us to derive an estimator of $\\phi(g(y))$.\n\n[1]: Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and Ruslan Salakhutdinov. 2019. Transformer Dissection: An Unified Understanding for Transformer\u2019s Attention via the Lens of Kernel. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8894/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700622350727,
                "cdate": 1700622350727,
                "tmdate": 1700625201963,
                "mdate": 1700625201963,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "x2NALl07gp",
                "forum": "PI6yaLXz3C",
                "replyto": "qV9PYQcXiL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8894/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8894/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to General Comments"
                    },
                    "comment": {
                        "value": "**Need for further empirical results and experimental details**\n\nPlease see the general comment for all reviewers addressing these points.\n\n**What are the differences between FAREContrast and the objective in [2]**\n\n**Answer**\n\nThe key difference between the objective in [2] and FAREContrast is that FAREContrast contains learnable attention scores as conditioning coefficients on the sample similarity scores whereas [2] does not. [2] therefore is used only to train in the MLP and encoder where FAREContrast trains the MLP, encoder, and attention projection matrices in one end-to-end loop. \n\n**Is sparseFARE sufficiently well motivated and does its induced performance benefit justify the implementation cost?**\n\n**Answer**\n\nWe argue that the benefits of sparseFARE do indeed justify its implementation cost. In particular, we note that in colorMNIST sparseFARE Pareto dominates all models and is significantly more computationally more efficient than the kernel models. For CelebA, it Pareto dominates or exhibits a more favourable fairness-accuracy tradeoff than the baselines. We argue that this is because sparisification makes intuitive sense within this framework, whereby samples that are sufficiently distant in terms of bias-status (and therefore likely to bias the representations) may as well be dropped and allow attention to be focussed over the remaining samples that help learn debiased and meaningful representations. \n\nSecondly, attention sparsification schemes typically exhibit a tradeoff between computational complexity and model performance (in this case taking both accuracy and fairness together) as sparisifation sacrifices the capacity of the attention mechanism for speed. SparseFARE, however, exhibits higher performance than the baselines while being faster as well and so again doesn\u2019t exhibit the normal tradeoff we observe in the literature.\n\nHence we believe that since sparsity, in this setting, is able to offer improvements in all these areas simultaneously, it\u2019s implementation cost is justified. \n\n[2] Yao-Hung Hubert Tsai, Martin Q Ma, Han Zhao, Kun Zhang, Louis-Philippe Morency, and Ruslan Salakhutdinov. Conditional contrastive learning: Removing undesirable information in self- supervised representations. arXiv preprint arXiv:2106.02866, 2021c."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8894/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700622428784,
                "cdate": 1700622428784,
                "tmdate": 1700624680733,
                "mdate": 1700624680733,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TBsK6Ums3K",
                "forum": "PI6yaLXz3C",
                "replyto": "stCpirKAbS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8894/Reviewer_Nbdh"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8894/Reviewer_Nbdh"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal (1/2)"
                    },
                    "comment": {
                        "value": "**Notation in section 2 for matrix U is overloaded**\n\nThe authors claim that $U$ is just a data specification of an abstract $U$. However, while it is explicitly mentioned in the paper that $U \\in \\mathbb{R}^{n \\times d_m}$, in its specified form it is a $n \\times n$ matrix. Furthermore, in the actual FAIR a $W_V$ matrix is not used as it leads to collapse. If anything, it seems that the actualized $U$ serves directly as $V$. As a result, the connection to the self-attention formulation is only loose and serves as an inspiration for FARE formulation. Many of these details need to be clarified further, and this part needs heavy rewriting for that.\n\n**There are missing citations, for example [1], on page 5 for the derivation of attention as kernel-based similarity, for example**\n\nTo the reviewer\u2019s perspective, [1] is exactly the inspiration for FARE here, as [1] provides with the way to explain attention as a kernelized non-linear similarity score, which is exactly what the authors deploy here. In other words, \u201cthe use of the kernel density estimator approach to estimate the joint and marginal densities\u201d is central to both cases.\n\n**There is an incorrect math statement in section 3.1 that $\\phi(g(y))$  is used to estimate $\\mathbb{E} \\phi(g(y))$**\n\nWhat is meant here in the original review is that **one can only say that $\\phi(g(y))$  can be used to estimate $\\mathbb{E} \\phi(g(y))$**. The opposite as the authors claim in the paper does not make mathematical/statistical sense. One quantity is stochastic, $\\phi(g(y))$, and the other is not, $\\mathbb{E} \\phi(g(y))$. One can use (via Monte Carlo)  $\\phi(g(y))$ to stochstically approximate $\\mathbb{E} \\phi(g(y))$, the other way around does not make sense since  $\\phi(g(y))$ is not even a constant. The justification here needs to be reconsidered."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8894/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674183308,
                "cdate": 1700674183308,
                "tmdate": 1700674183308,
                "mdate": 1700674183308,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TphYmXBPbP",
                "forum": "PI6yaLXz3C",
                "replyto": "TBsK6Ums3K",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8894/Reviewer_Nbdh"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8894/Reviewer_Nbdh"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal (2/2)"
                    },
                    "comment": {
                        "value": "**What are the differences between FAREContrast and the objective in [2]**\n\nPlease provide in exact latex the two losses and elucidate on the mathematical differences between the two formulations of the loss. As of now, when we abstract away the particularities of instantiating a similarity score (which we may nonetheless allow to depend of $z$), I am not able to spot any differences between the two.\n\n**Is sparseFARE sufficiently well motivated and does its induced performance benefit justify the implementation cost?**\n\nAccording to the results provided in the rebuttal, it is not true that \u201cfor CelebA, it Pareto dominates or exhibits a more favourable fairness-accuracy tradeoff than the baselines\u201d, as the SimCLR baseline outperforms all methods in the iid accuracy. The claim needs to be softened and furthermore the reviewer believes that further empirical evidence needs to be provided for a paper which focuses on fairness of representations.\n\nIn addition, the bias removal metric needs also to be clarified and described in the paper.\n\nOverall, for the reasons explained above, the reviewer chooses to maintain their original assessment.\n\nThe paper can improve significantly with better and correct writing describing the method, better attribution to relevant literature, and more empirical evidence justifying the additional incurring cost of SparseFARE."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8894/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700675168723,
                "cdate": 1700675168723,
                "tmdate": 1700675168723,
                "mdate": 1700675168723,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "UZxXCRCVUc",
            "forum": "PI6yaLXz3C",
            "replyto": "PI6yaLXz3C",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8894/Reviewer_nzY3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8894/Reviewer_nzY3"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to learn fair feature representations through contrastive learning. \nSimilar to [1], the authors adopt a learning scheme that assigns weights to data p\bairs according to the similarity of sensitive attributes. This is based on the assumption that the samples with similar sensitive attributes will serve as 'bias-reducing samples', which is beneficial \bfor learning fair representations. The proposed method, FARE, for estimating the (conditional) similarity between the anchor and the negative samples utilizes attention-based weights instead of kernel-based weights [1]. The authors also propose an additional method, SparseFARE, that further sparsifies the attention map by discarding \u2018extreme bias-causing\u2019 samples. \bHowever, the experiments section seems incomplete, as the comparison with baselines is carried out solely on a synthetic dataset, and some important details about the experimental set-up are not provided."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* While existing works in fair contrastive learning often assume binary sensitive attribute setting, the two proposed approaches can be applied to settings with high-dimensional and continuous sensitive attributes.\n* When the only available data is the batch of triplets $\\set{(x_{i}, y_{i}, z_{i})}_{i=1}^{b}$, the conditional sampling procedure in the Fair-InfoNCE objective [2] can be addressed through the proposed attention-based approaches. \n* The kernel-based method assumes a pre-defined kernel for calculating similarity, but attention-based methods learn similarity adaptively from the task, which alleviates the need for such an assumption. \n* Attention-based methods can lead to improved computational complexity ($O(b^2)$ or $O(b\\log{b})$) compared to the kernel-based methods ($O(b^3)$)."
                },
                "weaknesses": {
                    "value": "* The comparison with baselines is carried out solely on a synthetic dataset, ColorMnist [1]. Since this work is about learning fair representations, it seems necessary to consider experiments on fairness datasets (e.g., COMPAS, Adult), which are commonly used in the fairness literature, and to employ fairness criteria (e.g., Demographic Parity, Equalized Odds) for comprehensive assessment. Plotting a Pareto-frontier curve is an effective way to compare, especially when considering the accuracy-fairness trade-off.\n* Some important details for the proposed method such as the model architecture, batch size, and hyperparameter selection are not provided. For clarity and to ensure the paper is self-contained, it would be better to describe the specific procedures used.\n* Table 1 shows the result for CCLK [1] when using Cosine kernel, but [1] also provides a result for CCLK when Laplacian kernel is applied, showing Top-1 Accuracy of $85.0 \\pm 0.9$ and MSE of $72.8 \\pm 13.2$. Then, I'm not sure whether FARE indeed alleviates a significantly larger amount of bias compared to the baseline methods.\n* Given that the performance gain doesn\u2019t seem to be significant, it is not yet clear to me the benefits of the attention-based approach compared to the kernel-based approach. The kernel-based method relies on choosing an appropriate kernel, whereas the attention-based method focuses on training the model using data. However, it seems that more justification is required for the proposed method. It would be beneficial to include additional intuitive explanations on why attention-based methods are more effective than kernel-based methods for calculating the similarity of sensitive attributes, along with experimental results to support this. For instance, in Adult dataset, if \u2018age\u2019 is selected as the sensitive attribute, one could consider showing experimentally that the attention score tends to be higher when two individuals have similar ages, whereas this may not be the case with kernel-based methods.\n* Minor suggestions\n    * (p.4) \u201c~ Fair-InfoNCE objective in Eqn. 1\u201d \u2192 \u201c~ Fair-InfoNCE objective in Eqn. 1.\u201d\n    * (p.5) \u201cGiven 6 and the kernel density estimators in 7,\u201d \u2192 \u201cGiven Eqn. 6 and the kernel density estimators in Eqn. 7,\u201d\n    * (p.8) \u201cHence we only consider need to consider ~\u201d \u2192 \u201cHence we only consider ~\u201d\n    * (p.14) Consider adding 'Eqn.' for consistency."
                },
                "questions": {
                    "value": "* In Table 1, the result for SpareFARE appears to use the adjacent bucket scheme, but it differs from the result in Table 2 of the appendix. Which is correct?\n* In Eqn. (12), does FARE use a feature map associated with the Cosine kernel for $\\phi$?\n\n\n[1]: Tsai et al. Conditional Contrastive Learning with Kernel. ICLR, 2022.\n\n[2]: Tsai et al. Conditional Contrastive Learning for Improving Fairness in Self-Supervised Learning. Arxiv, 2021."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8894/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699363860310,
            "cdate": 1699363860310,
            "tmdate": 1699637118697,
            "mdate": 1699637118697,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EtZvA8yku2",
                "forum": "PI6yaLXz3C",
                "replyto": "UZxXCRCVUc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8894/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8894/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to comments"
                    },
                    "comment": {
                        "value": "**Need for further empirical results and experimental details**\n\nPlease see the general comment for all reviewers addressing these points.\n\n**Considering the performance of the Laplacian kernel, does FARE alleviate significantly more bias? If the performance gain isn\u2019t significant, what are the benefits of the attention-based approach compared to the kernel-based approach?**\n\n**Answer**\nIt\u2019s true that when implementing CCLK with a Laplacian kernel, the baseline model achieves bias removal close to SparseFARE. We note that our method nonetheless provides 3 simultaneous benefits over the Laplace method in particular, and over the baselines more generally:\nUsing the Laplacian method can indeed remove similar bias to our proposed models, but it does so at the expense of accuracy, shown by the drop in accuracy of 1.4%. Our SparseFARE does not suffer from this tradeoff, producing the same high bias removal with high accuracy concurrently. In this sense, we find that sparseFARE still Pareto dominates CCLK and the other baseline models.\nAn additional benefit of our approach is ease of implementation. In order to discover the best performing kernel baseline method, the user would have to exhaustively search over candidate kernels, each with their own hyperparameters, where there is in general little intuitive guidance in terms of knowing prior to the problem which kernel is suitable. Our approach does not need to consider a host of similarity metrics since the attention mechanism can adaptively learn it given the task, and so in this sense our model is easier to implement and fine-tune. \nLastly, we also refer to the benefits in computational complexity of our approach in reducing cubic computational complexity to $O(b^2)$ or $O(b log b)$. \n\nOverall, we agree that there probably will exist situations in which a pre-defined kernel performs close to our proposed method in terms of debiasing. In such situations we note that our methods can and do indeed match that debiasing performance without sacrificing accuracy, are faster to implement and fine-tune, and less computationally complex.\n\n**What are additional intuitive explanations for why attention is more effective than kernel for calculating similarity of sensitive attributes?**\n\n**Answer**\n\nAn alternate way of seeing the benefit of attention is that it is trained in an end-to-end manner alongside the encoder whereas any given kernel is simply pre-specified. In this way, the attention mechanism helps to focus attention on samples that learn meaningful representations. For example, suppose in a contrastive step that in a given batch there are n samples with highly similar sensitive attributes to the anchor. A kernel would indiscriminately assign all these n samples high weights irrespective of those samples\u2019 effect on the loss. Attention, by taking in information from the loss, is adapted to not only measure similarity over protected attributes, but be adapted by information coming from the gradients to better focus on samples that help minimize the loss and thereby learn meaningful representations.\n\n**In Table 1, the result for SpareFARE appears to use the adjacent bucket scheme, but it differs from the result in Table 2 of the appendix. Which is correct?**\n\n**Answer**\n\nWe thank the reviewer for picking this up and we apologize for the oversight. The original number in the main body is correct - we\u2019ve made the correction.\n\n**In Eqn. (12), does FARE use a feature map associated with the Cosine kernel for $\\phi$ ?**\n\n**Answer**\n\n$\\phi$ is the feature map associated with the exponentiation of the cosine kernel, which is what we need to aim for to estimate the similarity score in the objective. You can refer to equation 3 on page 3 to see the definition."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8894/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700621736690,
                "cdate": 1700621736690,
                "tmdate": 1700625753976,
                "mdate": 1700625753976,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cTrDLQsYSo",
                "forum": "PI6yaLXz3C",
                "replyto": "UZxXCRCVUc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8894/Reviewer_nzY3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8894/Reviewer_nzY3"
                ],
                "content": {
                    "title": {
                        "value": "Reply for author response"
                    },
                    "comment": {
                        "value": "Thank you for the replies. I have some comments on Figure 3 and the new results on CelebA dataset.\n\n**Regarding Figure 3 in Appendix C.2:**\n\n* The blue curve in Figure 3 does not show an appropriate trade-off between fairness and accuracy, but rather shows an improvement in accuracy as fairness improves. Typically, accuracy decreases as fairness improves (e.g., Figure 6 of [3]).\n* This might be because the figure was plotted with a model that hadn't been fully trained yet.\nNamely, the authors chose to plot using a single model during its training, interrupted at 25, 50, 75, and 100 epochs. \n* However, I think that it makes sense to use different models, each with distinct hyperparameters, after they have reached convergence.\nThen, the author\u2019s comments based on Figure 3 are not convincing.\n\n**On the new results with the CelebA dataset:**\n* In the additional experimental results on CelebA dataset, the authors used \u2018Attractive' as the target attribute; however, since \u2018Attractive\u2019 is subjective and controversial, it is recommended to use one of the other 40 attributes in the dataset for discussion of fairness. \n* For instance, a setting that classifies hair color while incorporating gender as a sensitive attribute can be considered, as reviewer Nbdh suggested.\n\n[3]: Zhang et al. Fairness-aware Contrastive Learning with Partially Annotated Sensitive Attributes. ICLR, 2023."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8894/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700653281961,
                "cdate": 1700653281961,
                "tmdate": 1700689908290,
                "mdate": 1700689908290,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]