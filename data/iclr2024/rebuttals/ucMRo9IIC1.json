[
    {
        "title": "Image Hijacks: Adversarial Images can Control Generative Models at Runtime"
    },
    {
        "review": {
            "id": "oiCepj86dJ",
            "forum": "ucMRo9IIC1",
            "replyto": "ucMRo9IIC1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2001/Reviewer_dgQe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2001/Reviewer_dgQe"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies adversarial images (referred to as image hijacks) in the context of attacking large vision-language models (VLMs). Specifically, the authors explore three types of attacks, i.e., specific string attack, leak context attack, and jailbreak attack, which target three different undesirable behaviours of VLMs. Three different image constraints are considered: Lp norm, stationary patch, and moving patch."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- \"Adversarial images on VLMs\" is an interesting and promising topic. \n- The paper is well written, including sufficient example visualizations and clearly described technical details.\n- Real-world experiments with human studies are conducted, involving both the tests with drawing and pasting tapes."
                },
                "weaknesses": {
                    "value": "- The experimental studies in this paper are limited. Only the ideal, white-box setting is considered. The authors should follow recent attempts (e.g., the GCG paper) to explore more realistic attack settings, such as query-based and transfer-based attacks. Moreover, although the authors envision three different undesired behaviors, the implemented three attacks are indeed following the same attack goal which is to output a pre-defined text.  More diverse attack goals should be explored.\n\n- Considering this paper studies a really hot topic, it should highlight its specific contributions, especially in terms of technical novelty, compared to other (concurrent) similar studies. In the current version, only the differences in attack settings are briefly mentioned in Section 5.\n\n- The reviewer does not get the idea of splitting the dataset into train, val, and test sets. Is the perturbation vector universal, i.e., it is trained over a lot of original images and once trained can be directly applied to any testing images? If this is the case, it should be explicitly mentioned in the paper. Otherwise, it is confusing that creating adversarial images requires image (or model) training. In addition, why didn\u2019t the authors explore the common setting of image-specific attacks?\n\n- Since the baseline GCG attack was also designed to achieve \u201cexact match\u201d, it is reasonable to compare with it also on the rest two types of attacks: specific string and leak context.\n\n- Figure 4 takes an unreasonably large space (i.e., one page) given the fact those three considered types of constraints are well-known in the literature."
                },
                "questions": {
                    "value": "See the above weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2001/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697790102797,
            "cdate": 1697790102797,
            "tmdate": 1699636131777,
            "mdate": 1699636131777,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "l1qWfWeHtS",
                "forum": "ucMRo9IIC1",
                "replyto": "oiCepj86dJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2001/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2001/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer dgQe"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their detailed review. Below, we respond to each of the concerns they raised. **Could the reviewer please let us know if this addressed their concerns?**\n\n>\u201cit should highlight its specific contributions, especially in terms of technical novelty, compared to other (concurrent) similar studies\u201d\n\nWe have added Table 3, which highlights our specific contributions, in Appendix C.\n\nIn our [OpenReview comment to all reviewers](https://openreview.net/forum?id=ucMRo9IIC1&noteId=8Eu4wFSlNR), we give a description of further changes to increase the novelty of our work. In summary, we draw your attention to following changes:\n\n\n1. We have moved the prompt matching algorithm from the Appendix to Section 5. This is an **entirely nove**l adversarial attack method that is different to anything presented by Qi et al. and Carlini et al. The prompt matching algorithm we introduce allows for the creation of adversarial images that induce similar model behaviors to textual prompts. This means we can create attacks that are **hard to encapsulate in training datasets**. \n2. Since our initial submission, we have added a more comprehensive comparison to text-based adversarial attacks. Because Qi et al. and Carlini et al. are restricted to jailbreak attacks, they only show that an existing text vulnerability (jailbreaks) can also be done with images. We have added results with a state-of-the-art text-based attack on the specific string and leak-context attacks. We find that in this broader setting, state-of-the-art text attacks fail; for example, text attacks get 0% success rate on the leak context attack. Thus, our study is the first to show **new image attacks that were impossible with just text**.\n3. As you noted, all related works released after May 28, 2023 are concurrent as per [ICLR guidelines](https://iclr.cc/Conferences/2024/ReviewerGuide). \n\n>\u201cThe implemented three attacks are indeed following the same attack goal which is to output a pre-defined text.\u201d\n\nIt is false that our three attacks are all outputting a predefined text. For each of the three different attacks, the output is defined in a different way.\n* The specific string attack is a predefined text. The output of the model only depends on this text.\n* In the leak context attack, the model's output depends on the user's input. The attack is only successful if the user's input appears in the output.\n* In the jailbreak attack, the attack depends on the model's RLHF safety training. The definition of a successful attack is if the output violates the model's safety training. It does not depend on any predefined text.\n\n\n>\u201cOnly the ideal, white-box setting is considered\u201d\n\nWe agree this is a limitation of our work, and we have added a discussion about it to Section 7. Nevertheless, we see our work being broadly relevant in two ways. First, it shows the existence of vulnerabilities in VLMs, even if the field has yet to develop black-box attacks that can find these vulnerabilities. Second, many apps are currently being developed with open-source models, and we expect this trend only to increase in the future. Our work is directly relevant to any app built using an open-source VLM.\n\n>\u201cThe reviewer does not get the idea of splitting the dataset into train, val, and test sets\u201d\n\nA single adversarial image is trained on a dataset of textual prompts to ensure that the image induces the desired model behavior **no matter the user\u2019s input text**. We take a dataset of possible user inputs and split it into training, val, and test sets. We have modified Section 2 to help make this more clear. \n\n>\u201cSince the baseline GCG attack was also designed to achieve \u201cexact match\u201d, it is reasonable to compare with it also on the rest two types of attacks: specific string and leak context.\u201d\n\n**We have added GCG baseline numbers for all attack types**. We find GCG underperforms image adversaries significantly. In particular, the GCG attack is unable to get any success with the leak context attack, compared to an optimal success rate of 96% for image based attacks. This finding is a novel contribution of our paper: **image-based attacks open up the possibility for new attacks that are impossible with existing text-based attacks**.\n\n> \u201cfigure 4 takes an unreasonably large space\u201d\n\nWe have reduced figure 4\u2019s size and moved it to the Appendix."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2001/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700596889772,
                "cdate": 1700596889772,
                "tmdate": 1700599837508,
                "mdate": 1700599837508,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vT5TBL2MVH",
                "forum": "ucMRo9IIC1",
                "replyto": "oiCepj86dJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2001/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2001/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer dgQe - New Black-Box Transfer Experiments"
                    },
                    "comment": {
                        "value": "**We have added black-box transferability experiments to Appendix E**. We try two settings: directly inputting our original attacks into new models, and also training on an ensemble of models to try improving transferability. We get 0% success rate in both cases, establishing preliminary negative results for black-box transferability. (As with any negative empirical result, it will be important for future work to continue investigating this question.) \n\nPlease see our [top-level comment](https://openreview.net/forum?id=ucMRo9IIC1&noteId=e7y7rRoDly) and Appendix E for more details."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2001/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700712155772,
                "cdate": 1700712155772,
                "tmdate": 1700712155772,
                "mdate": 1700712155772,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xBpRyu8KLr",
            "forum": "ucMRo9IIC1",
            "replyto": "ucMRo9IIC1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2001/Reviewer_DvbG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2001/Reviewer_DvbG"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the security problems of Vision-Language Models (VLMs), and propose to add unnoticeable perturbations onto the image inputs, to mislead the model have various types of adversarial behaviors."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The proposed attacks absolutely make sense, and demonstrate the weakness, the security risks of the VLMs."
                },
                "weaknesses": {
                    "value": "The greatest concern is that the paper does not study the transferability behavior of the proposed attacks to close-source VLMs. Although the authors mention that the jailbreak attack in [Zou 2023] did similar experiments to transfer the attacks from open-source LLMs to close-source LLMs, it can not ensure the VLM attacks can also have the ability to transfer. If this part of experiment is not provided, it would dramatically limit the significant of the contribution of this paper. \n\nBeyond the study of transferability, I found the methodology is not significantly novel, compared to existing attack methods such as Jailbreak attacks in LLMs and image adversarial examples. Therefore, without the transferability study, this work seems solely a white-box attack paper without significant novelty, because people already know that the DNN models are vulnerable to adversarial attacks."
                },
                "questions": {
                    "value": "Plz see the weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2001/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698689121448,
            "cdate": 1698689121448,
            "tmdate": 1699636131681,
            "mdate": 1699636131681,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tCEqubbpEz",
                "forum": "ucMRo9IIC1",
                "replyto": "xBpRyu8KLr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2001/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2001/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer DvbG"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their review, and respond to each of the concerns they raised below: \n\n>\u201cthe methodology is not significantly novel\u201d\n\nPlease see our [comment above to all reviewers](https://openreview.net/forum?id=ucMRo9IIC1&noteId=8Eu4wFSlNR), which discusses the technical contribution and novelty of the paper. In summary, we draw your attention to following changes:\n\n1. All the related work mentioned by reviewers was released after May 28, 2023. Per [ICLR guidelines](https://iclr.cc/Conferences/2024/ReviewerGuide), these papers are concurrent with our own.\n2. We have moved the prompt matching algorithm from the Appendix to Section 5. This is an **entirely nove**l adversarial attack method that is different to anything presented by Qi et al. and Carlini et al. The prompt matching algorithm we introduce allows for the creation of adversarial images that induce similar model behaviors to textual prompts. This means we can create attacks that are **hard to encapsulate in training datasets**. \n3. Since our initial submission, we have added a more comprehensive comparison to text-based adversarial attacks. Because Qi et al. and Carlini et al. are restricted to jailbreak attacks, they only show that an existing text vulnerability (jailbreaks) can also be done with images. We have added results with a state-of-the-art text-based attack on the specific string and leak-context attacks. We find that in this broader setting, state-of-the-art text attacks fail; for example, text attacks get 0% success rate on the leak context attack. Thus, our study is the first to show **new image attacks that were impossible with just text**.\n\n>\u201cpaper does not study the transferability behavior of the proposed attacks to close-source VLMs\u201d\n\nWe agree this is a limitation of our work, and we have added a discussion about it to Section 7. Nevertheless, we see our work being broadly relevant in two ways. First, it shows the existence of vulnerabilities in VLMs, even if the field has yet to develop black-box attacks that can find these vulnerabilities. Second, many apps are currently being developed with open-source models, and we expect this trend only to increase in the future. Our work is directly relevant to any app built using an open-source VLM.\n\nNote also that GPT-4V (mentioned by NiWH) was released after ICLR\u2019s abstract submission deadline.\n\n**Have we addressed the reviewer's concerns?**"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2001/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700596579897,
                "cdate": 1700596579897,
                "tmdate": 1700599580200,
                "mdate": 1700599580200,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cIkIQcw2mI",
                "forum": "ucMRo9IIC1",
                "replyto": "xBpRyu8KLr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2001/Reviewer_DvbG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2001/Reviewer_DvbG"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response. Due to the lack of transferability experiment, I would keep my original rating. \n\nOther concerns still remains. For example, the revised version has a weird structure. If the major novelty is about \"prompt matching\", this should be introduced in more details in early sections, instead of in the experiment section. Besides, I hope the authors could try avoiding provide irrelevant information which are not asked in this review."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2001/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700601460821,
                "cdate": 1700601460821,
                "tmdate": 1700601460821,
                "mdate": 1700601460821,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "i9d9zO6Gz5",
                "forum": "ucMRo9IIC1",
                "replyto": "xBpRyu8KLr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2001/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2001/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer DvbG Official Comment - New Black-Box transfer experiments"
                    },
                    "comment": {
                        "value": "**We have added black-box transferability experiments to Appendix E**. We try two settings: directly inputting our original attacks into new models, and also training on an ensemble of models to try improving transferability. We get 0% success rate in both cases, establishing preliminary negative results for black-box transferability. (As with any negative empirical result, it will be important for future work to continue investigating this question.) \n\nPlease see our [top-level comment](https://openreview.net/forum?id=ucMRo9IIC1&noteId=e7y7rRoDly) and Appendix E for more details."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2001/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700712066300,
                "cdate": 1700712066300,
                "tmdate": 1700712128580,
                "mdate": 1700712128580,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "o9HmbR0thG",
                "forum": "ucMRo9IIC1",
                "replyto": "xBpRyu8KLr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2001/Reviewer_DvbG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2001/Reviewer_DvbG"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the additional result provided. I really appreciate authors' effort to provide the result on the transferabilty study. \n\nFrom my perspective, it is critical for the attack to succeed in black box models, which is also agreed by other reviewers. Based on the result provided by the author, it demonstrates the possibility, although the attack is not achieved eventually. Therefore, a lot of more interesting studies can be done to improve the paper, such as trying to raise the perturbation budget or modify the attack objective.\n\nAs a conclusion, I hesitate that this study is completed on the current stage, and I hope the authors can provide a more comprehensive study on the transferability in the future revision."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2001/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700713098709,
                "cdate": 1700713098709,
                "tmdate": 1700713098709,
                "mdate": 1700713098709,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YJ1qGexgX8",
            "forum": "ucMRo9IIC1",
            "replyto": "ucMRo9IIC1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2001/Reviewer_NiWH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2001/Reviewer_NiWH"
            ],
            "content": {
                "summary": {
                    "value": "The paper delves into the security aspects of vision-language models (VLMs) and their susceptibility to adversarial attacks via the image channel. It introduces \"image hijacks,\" adversarial images that can manipulate generative models in real time. Using a method called Behavior Matching, the authors showcase three attack types: specific string attacks, leak context attacks, and jailbreak attacks. In their evaluation, these attacks have a success rate exceeding 90% on leading VLMs, with the attacks being automated and necessitating only minor image alterations. They underscore the security vulnerabilities of foundational models, hinting that countering image hijacks might be as formidable as defending against adversarial examples in image categorization."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Problem Motivation: The paper studies a highly pertinent and timely issue, highlighting the security vulnerabilities of VLMs, which are increasingly being used in various applications.\n\n2. Methodology: The behavior-matching method for creating image hijacks is sound and intuitive. The three types of attacks presented provide a wide understanding of the potential threats.\n\n3. Evaluating the Evaluation: The evaluation is highlighted with alarming results, with all attack types achieving a success rate above 90% against LLaVA. This supports the paper's claims about the vulnerabilities of open-sourced VLMs."
                },
                "weaknesses": {
                    "value": "1. Technical Contribution: Limited novelty due to similarities with prior works (e.g., Qi et al., Carlini et al.).\n    \n2. Problem Formulation: Restricted impact concerning closed-sourced VLMs or those not directly interfacing the image channel with the hidden space.\n    \n3. Transferability of Attacks: Insufficient discussion on attack applicability to unknown VLMs accessible only via APIs (balckbox attacks, which can be a great contribution to set the difference to existing efforts).\n    \n4. Lack of Defense Discussion: No mention of potential defenses or mitigation strategies against the proposed attacks."
                },
                "questions": {
                    "value": "- Technical Contribution and Novelty: While your paper introduces the specific string attack and leak context attack, which were not covered by Qi et al. and Carlini et al., the attack process appears to be highly similar to these prior works. Could you elaborate on the distinct technical innovations that differentiate your methods from these existing studies? Additionally, are there areas where you foresee further improvements or refinements to enhance the technical novelty of your approach?\n\n- Problem Formulation: Considering the limited applicability of your problem formulation to closed-sourced VLMs or those VLMs that do not directly interface the image channel with the hidden space, how do you envision the broader relevance of your proposed attacks? Are there specific scenarios or VLM architectures where your attacks would be particularly effective?\n\n- Transferability of Attacks: Could you expand on the transferability of your attacks to unknown VLMs, especially those only accessible via APIs, such as GPT-4V? Given that GPT-4V's technical report has already highlighted potential risks associated with the image channel and proposed baseline defenses, how do you anticipate your attacks would perform in such contexts?\n\n- Lack of Defense Discussion: Why was there an omission of defenses or potential mitigation strategies against the proposed attacks in your paper? A discussion on potential countermeasures would enhance the paper's depth and practical relevance. Do you have insights or preliminary findings on how one might defend against the attacks you've introduced?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2001/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698768553136,
            "cdate": 1698768553136,
            "tmdate": 1699636131615,
            "mdate": 1699636131615,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nQ5VnXnRQd",
                "forum": "ucMRo9IIC1",
                "replyto": "YJ1qGexgX8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2001/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2001/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer NiWH"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their detailed review, and respond to each of their questions below.\n\n>\u201cTechnical Contribution and Novelty\u201d\n\nPlease see our [comment above to all reviewers](https://openreview.net/forum?id=ucMRo9IIC1&noteId=8Eu4wFSlNR), which discusses the technical contribution and novelty of the paper. In summary, we draw your attention to following changes and comparisons to Qi et al. and Carlini et al.: \n\n1. The works by Qi et al. and Carlini et al. were released after May 28, 2023. Per [ICLR guidelines](https://iclr.cc/Conferences/2024/ReviewerGuide), these papers are concurrent with our own.\n2. We have moved the prompt matching algorithm from the Appendix to Section 5. This is an **entirely novel** adversarial attack method that is different to anything presented by Qi et al. and Carlini et al. The prompt matching algorithm we introduce allows for the creation of adversarial images that induce similar model behaviors to textual prompts. This means we can create attacks that are **hard to encapsulate in training datasets**. \n2. Since our initial submission, we have added a more comprehensive comparison to text-based adversarial attacks. Because Qi et al. and Carlini et al. are restricted to jailbreak attacks, they only show that an existing text vulnerability (jailbreaks) can also be done with images. We have added results with a state-of-the-art text-based attack on the specific string and leak-context attacks. We find that in this broader setting, state-of-the-art text attacks fail; for example, text attacks get 0% success rate on the leak context attack. Thus, our study is the first to show **new image attacks that were impossible with just text**.\n\n>\u201cLimited applicability of your problem formulation to closed-sourced VLMs\u201d\n\nWe see our work being broadly relevant in two ways. First, it shows the existence of vulnerabilities in VLMs, even if the field has yet to develop black-box attacks that can find these vulnerabilities. Second, many apps are currently being developed with open-source models, and we expect this trend only to increase in the future. Our work is directly relevant to any app built using an open-source VLM.\n\nOn internal testing, we found our attacks were in no way limited to the LLaVA series of VLMs. We were also able to apply attacks to the BLIP-2 and InstructBLIP models. This is especially notable because the BLIP-2 model uses an encoder-decoder language model, different to the decoder-only language model used by LLaVA.\n\n>\u201cTransferability of Attacks\u201d\n\nWe have added a paragraph **discussing this limitation in Section 7** of the paper. We agree with the reviewer that transferring our attacks to closed-source models would improve the impact of the paper. Upon further investigation we find that our adversarial images do not transfer to closed source models such as GPT4-V (that could be in part due to OpenAI uses defenses, such as image preprocessing, that we are unaware of and did not train our attacks to be robust to). We note that GPT4-V was only released publicly on September 25th, two days after the ICLR abstract submission deadline. We are however excited about future work that could build on our method to generate transferable attacks.\n\n>\u201cLack of Defense Discussion\u201d\n\nAdversarial defenses are a particularly fraught topic, where defenses are commonly created but quickly broken with minor tweaks to the attacking algorithm (see, for example, [\u201cObfuscated Gradients Give a False Sense of Security.\u201d](https://arxiv.org/abs/1802.00420)) Because one must be very careful when proposing defense algorithms, we leave this topic to future work. Still, we agree that a preliminary discussion of defenses is a valuable addition to our paper. **We have added a new paragraph discussing this topic in Section 7**.\n\n**Have we addressed the reviewer's concerns?**"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2001/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700596294790,
                "cdate": 1700596294790,
                "tmdate": 1700598695003,
                "mdate": 1700598695003,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YpHIH2m9br",
                "forum": "ucMRo9IIC1",
                "replyto": "YJ1qGexgX8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2001/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2001/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer NiWH - New Black-Box Transfer Experiments"
                    },
                    "comment": {
                        "value": "**We have added black-box transferability experiments to Appendix E**. We try two settings: directly inputting our original attacks into new models, and also training on an ensemble of models to try improving transferability. We get 0% success rate in both cases, establishing preliminary negative results for black-box transferability. (As with any negative empirical result, it will be important for future work to continue investigating this question.) \n\nPlease see our [top-level comment](https://openreview.net/forum?id=ucMRo9IIC1&noteId=e7y7rRoDly) and Appendix E for more details."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2001/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700711875094,
                "cdate": 1700711875094,
                "tmdate": 1700712100178,
                "mdate": 1700712100178,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "klCuK9gm0f",
            "forum": "ucMRo9IIC1",
            "replyto": "ucMRo9IIC1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2001/Reviewer_C2M9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2001/Reviewer_C2M9"
            ],
            "content": {
                "summary": {
                    "value": "This paper is well-organized and easy to follow. This idea is simple yet effective.\nThe authors introduce the concept of image hijacks \u2013 adversarial images that manipulate the behavior of VLMs during inference time. They propose a behavior matching algorithm to train these models in a more robust manner against user input. The authors have developed three types of image hijacks: specific string attacks, jailbreak attacks, and leak-context attacks. The optimized perturbation is both imperceptible and effective. \nThe experiments are comprehensive, and the overall ASR is high, demonstrating the effectiveness of these methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ The paper presents a rather interesting approach. Utilizing an image to replace the prompt templates of jailbreak attacks seems reasonable. As large multi-modal models continue to develop, such attack methods introduce a new area of adversarial attacks that can be better optimized using computer vision gradient information. I found the idea and the overall paper to be quite enjoyable."
                },
                "weaknesses": {
                    "value": "- This paper is well-organized and easy to follow, but there is one important baseline missing: \"Visual Adversarial Examples Jailbreak Aligned Large Language Models.\" This paper demonstrates that by using a preset adversarial sample image, a safely aligned LLM can be successfully jailbroken during subsequent model risk assessments, leading the LLM to generate harmful content. The primary attack method involves optimizing the adversarial sample image to create a specific mapping relationship between the image and malicious text.\n\n- I suggest further research on using a single image to match a series of jailbreak attack prompt templates, such as the \"Do Anything Now\" (DAN) series.\n\n- In my opinion, the patch-level attack or l-p norm attack may not be particularly meaningful. They are simply different methods for generating adversarial examples. What truly matters is the effectiveness of the mapping between adversarial examples and various hijack risks, such as specific string attacks, jailbreak attacks, and leak-context attacks."
                },
                "questions": {
                    "value": "Refer to weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "This paper investigates the use of images to hijack large language models, which could potentially lead to the generation of toxic content in the resulting output. However, the paper overall does not present any ethical issues, and the proposed method appears to work effectively."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2001/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698820422385,
            "cdate": 1698820422385,
            "tmdate": 1699636131534,
            "mdate": 1699636131534,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WPHgXhiMH2",
                "forum": "ucMRo9IIC1",
                "replyto": "klCuK9gm0f",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2001/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2001/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer C2M9"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their detailed and insightful comments. Below, we summarize what we identify as the crucial issues raised in the review, and describe how we have addressed each. **Does this fully address the reviewer\u2019s concerns?**\n\n> \u201cI suggest further research on using a single image to match a series of jailbreak attack prompt templates, such as the \"Do Anything Now\" (DAN) series.\u201d\n\nWe added a new section of the paper, Section 5, that creates images matching arbitrary text prompts through the novel prompt matching algorithm. Naively, creating images that elicit the same model behavior as text prompts can be done by training the embeddings of images to match the embeddings of a text prompt. We find this is impossible to do because of the modality gap in multi-modal embedding spaces [1]. Instead, we develop the novel prompt matching algorithm. For details of this algorithm, see the new section 5 of the paper.\n\nBecause we are already able to achieve high jailbreak rates with our prior attack methods, we use our prompt matching algorithm to conduct a new disinformation attack, editing the model\u2019s factual recall. In our experiments, we get the model to believe the Eiffel Tower has been moved to Rome, _without training on any questions specifically chosen to be about the Eiffel Tower or Rome_.\n\n> \u201cone important baseline missing\u201d\n\nThe method outlined by Qi et al. is concurrent work and very similar to our own method. The algorithm used is essentially the same as our own, besides using a different training set for their jailbreak attack. We accordingly omit the baseline comparison as both methods would achieve the same results if using the same training dataset and hyperparameters. We note however that we explore a far wider range of attacks than Qi et al. who only explore jailbreaking. We show a critical fact they do not: that image adversaries can **fully control the output of VLMs**.\n\nWe also note the [official ICLR policy](https://iclr.cc/Conferences/2024/ReviewerGuide): \u201c**if a paper was published (i.e., at a peer-reviewed venue) on or after May 28, 2023, authors are not required to compare their own work to that paper**\u201d. Qi et al.\u2019s work was first arXived Jun 22 (and has never appeared in a peer-reviewed venue).\n\n> \u201cpatch-level attack or l-p norm attack may not be particularly meaningful\u201d\n\nWe believe it is important to include the study under these constraints, to illustrate how these attacks could be deployed under credible threat models. For example, an attacker may only be able to place an image on a website and want to attack a VLM that analyzes screenshots of a user's display. Under this threat model, Image Hijacks are only of concern if they can be trained as patches on otherwise unperturbed images.\n\n[1] Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation. Learning Weixin Liang*, Yuhui Zhang*, Yongchan Kwon*, Serena Yeung, James Zou. NeurIPS (2022)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2001/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700595936189,
                "cdate": 1700595936189,
                "tmdate": 1700599386107,
                "mdate": 1700599386107,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kLxBSMZvnh",
                "forum": "ucMRo9IIC1",
                "replyto": "klCuK9gm0f",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2001/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2001/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer C2M9 - New Black-Box Transfer Experiments"
                    },
                    "comment": {
                        "value": "**We have added black-box transferability experiments to Appendix E**. We try two settings: directly inputting our original attacks into new models, and also training on an ensemble of models to try improving transferability. We get 0% success rate in both cases, establishing preliminary negative results for black-box transferability. (As with any negative empirical result, it will be important for future work to continue investigating this question.) \n\nPlease see our [top-level comment](https://openreview.net/forum?id=ucMRo9IIC1&noteId=e7y7rRoDly) and Appendix E for more details."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2001/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700711797293,
                "cdate": 1700711797293,
                "tmdate": 1700711896008,
                "mdate": 1700711896008,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]