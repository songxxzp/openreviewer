[
    {
        "title": "On the Hidden Waves of Image"
    },
    {
        "review": {
            "id": "y2hObsiiWu",
            "forum": "ISrxxvXJQO",
            "replyto": "ISrxxvXJQO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3126/Reviewer_Y4BM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3126/Reviewer_Y4BM"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces the concept of \"hidden waves,\" a phenomenon that enables the successful reconstruction of images through a series of one-way wave equations with hidden and adaptable speeds. To compute the initial conditions for each image, a visual encoder is employed based on the original image. The paper extends the existing framework of FINOLA by making two key enhancements: generalizing it and relaxing the local constraints. These relaxations not only enhance the image reconstruction performance but also lead to the observation of specific phenomena under certain conditions within the representation. They performed rigorous experiments to gain understanding of this perspective."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Through mathematical enhancements and experiments, they develop a deep understanding of FINOLA, which is a general wave representation of an image.\n- The section comparing \"hidden wave representation\" with image autoregression or transformations is well-written and provides clear insights into this encoding.\n- They conducted numerous experiments to validate the improvements they implemented."
                },
                "weaknesses": {
                    "value": "- Experimental design\n\nDrawing conclusions from the current results is uncertain due to the limited number of data points and the lack of significant differences. For instance, the interpretation of Table 3 remains unclear. Should we consider choosing initial condition positions different from the center? Overall, experiments were conducted from various perspectives, they do not yield meaningful insights or conclusions.\n\n- The phenomenon is undoubtedly intriguing and interesting, it remains unclear how this method can be applied to future research or practical applications. The paper would be improved by including discussions on potential future perspectives."
                },
                "questions": {
                    "value": "- Were all images resized to the same aspect ratio?  The aspect ratio can impact wave speed, and it may be more natural to preserve the aspect ratio from the original images.\n- Conventional image compression methods like DCT transform the image into a linear composition of orthogonal basis. This is simple and provides a straightforward understanding of each coefficient. The paper effectively describes the differences, but what are the main strengths and weaknesses of this method?\n- Is the selection of hyperparameters for training this method not very sensitive? How long does the training process take?\n\nMinor comments:\n- Use ` for opening quotation mark in latex. Currently ' ... '.\n- There are some missing brackets for citations. For example, ... known as FINOLA Chen et al. (2023)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3126/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3126/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3126/Reviewer_Y4BM"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3126/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698563070192,
            "cdate": 1698563070192,
            "tmdate": 1699636259615,
            "mdate": 1699636259615,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aSTuxgrkMq",
                "forum": "ISrxxvXJQO",
                "replyto": "y2hObsiiWu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3126/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3126/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' Rebuttal (part 1)"
                    },
                    "comment": {
                        "value": "Thank you for dedicating your time and effort to provide feedback on our work. Below, we answer the questions that have been raised.\n\n---\n\n**[Weakness 1] Experimental design: Drawing conclusions from the current results is uncertain due to the limited number of data points and the lack of significant differences. For instance, the interpretation of Table 3 remains unclear. Should we consider choosing initial condition positions different from the center? Overall, experiments were conducted from various perspectives, they do not yield meaningful insights or conclusions.**\n\nThanks for this constructive suggestion. We will reorganize the experiment section and showcase two key conclusions and three main ablations as follows:\n\n***Conclusion I: Hidden-Wave Enhances Mathematical Decription.*** \nHidden-Wave demonstrates a superior mathematical description by employing one-way wave equations, resulting in more accurate reconstructions compared to the two PDEs in FINOLA. Figure 4 in the paper supports this conclusion.\n\n***Conclusion II: Hidden-Wave Outperforms Conventional Methods.***\n\n*Comparison with discrete cosine transform (DCT):*\nThe table below showcases the superiority of Hidden-Wave over DCT. DCT operates per 8x8 image block, retaining the top-left $K$ coefficients (in zig-zag manner), with the rest set to zero. Compared to DCT with various $K$ values (1, 3, 6, 10), Hidden-Wave consistently achieves higher PSNR with a smaller latent size. \n\n|Method|Latent Size&darr;|PSNR&uarr;|\n|---|---|---|\n|DCT (top-left 1) | 3072 (32x32x3) | 20.6 |\n| **Hidden Wave (our)** | **2048 (1024x2)** | **24.8** |\n||||\n|DCT (top-left 3)| 9216 (32x32x9) | 23.5|\n| **Hidden Wave (our)** | **8192 (1024x8)** | **27.1** |\n||||\n|DCT (top-left 6)| 18432 (32x32x18) | 25.6|\n| **Hidden Wave (our)** | **16384 (2048x8)** | **27.8** |\n||||\n|DCT (top-left 10)| 30720 (32x32x30) | 27.5|\n| **Hidden Wave (our)** | **16384 (2048x8 mix)** | **28.9** |\n\nNotably, in the last row, the term *\"mix\"* denotes placing 8 initial conditions at different positions, as opposed to overlapping at the center. \n\n*Comparison with discrete wavelet transform (DWT):* \nWe compare Hidden-Wave with DWT in the table below. Three scales are chosen for wavelet decomposition. The comparisons are organized into three groups: (a) using only the LL subband at the coarsest scale (scale 3), (b) using all subbands (LL, LH, HL, HH) at the coarsest level, and (c) using all subbands at the finer scale (scale 2). Hidden-Wave achieves higher PSNR with a smaller latent size. \n\n|Method|Latent Size&darr;|PSNR&uarr;|\n|---|---|---|\n|DWT (scale-3 LL) | 3888 | 21.5 |\n| **Hidden Wave (our)** | **2048 (1024x2)** | **24.8** |\n||||\n|DWT (scale-3 LL+LH+HL+HH)| 15552 | 24.3|\n| **Hidden Wave (our)** | **8192 (1024x8)** | **27.1** |\n||||\n|DWT (scale-2 LL+LH+HL+HH)| 55953 | 28.7|\n| **Hidden Wave (our)** | **16384 (2048x8 mix)** | **28.9** |\n\n\n***Ablation I: Hidden-Wave Consistency across Feature resolutions and Image Sizes.*** Table 1 in the paper supports the claim that Hidden-Wave consistently performs well across multiple feature resolutions and image sizes.\n\n***Ablation II: Complex-valued Wave Speeds Improve Accuracy.*** Table 2 in the paper indicates that complex-valued wave speeds yield better accuracy compared to real-valued wave speeds.\n\n***Ablation III: Scattering Initial Conditions Enhance Performance.*** The table below demonstrates that scattering initial condition positions on a circle consistently outperforms overlapping at the center.\n\n|Position|\\#Init Conditions ($M$)|\\#Channels ($C$)|PSNR&uarr;|\n|---|---|---|---|\n|Overlapping at Center | 4 | 1024 | 26.1 |\n|Scattering on a Circle | 4 | 1024 | **26.7** |\n||||\n|Overlapping at Center | 8 | 1024 | 27.1 |\n|Scattering on a Circle | 8 | 1024 | **28.0** |\n||||\n|Overlapping at Center | 8 | 2048 | 27.8 |\n|Scattering on a Circle | 8 | 2048 | **28.9** |\n\nThis reorganization highlights the key conclusions and ablations, providing a clearer and more structured presentation of the experimental findings.\n\n---\n\n**[Weakness 2]: The phenomenon is undoubtedly intriguing and interesting, it remains unclear how this method can be applied to future research or practical applications. The paper would be improved by including discussions on potential future perspectives.**\n\nExcellent point! We will include discussions on potential future perspectives, encompassing (a) understanding the relationship between multiple initial conditions, (b) exploring the distribution of real/noise images in the latent space of Hidden-Wave, (c) investigating the application of Hidden-Wave in self-supervised learning, and (d) exploring the potential use of Hidden-Wave in image compression."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3126/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700463752214,
                "cdate": 1700463752214,
                "tmdate": 1700463752214,
                "mdate": 1700463752214,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "u5yo75uQxt",
            "forum": "ISrxxvXJQO",
            "replyto": "ISrxxvXJQO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3126/Reviewer_Zqsi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3126/Reviewer_Zqsi"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies a new finding about the mathematical properties of realistic images, i.e., that they can be recovered well from a set of specific solutions to one-way wave equations with the help of an encoder that generates initial conditions for the wave equations and a simple decoder that maps the wave equation solutions back to an image. The paper extends prior work of Chen et al., FINOLA, by interpreting their autoregressive process as the discretization of the one-way wave equation and by considering the sum of multiple such FINOLA solutions as an input to the decoder network. Numerical results illustrate that the reconstruction of the superposition of FINOLAs yields a significantly better PSNR than a single one, e.g., improving from 24.8 to 27.8."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is well written and the continuous interpretation of FINOLA that implies a solution to a wave equation is very interesting. In particular, the fundamental type of question investigated here, i.e., if certain wave equations are a common mathematical principle that allows reconstructing any realistic image well is intriguing and has large potential if answered positively. In this sense, I found the paper very interesting to read."
                },
                "weaknesses": {
                    "value": "Unfortunately, I also see several weaknesses in the presented work.\n- Contribution: It is stated in section 2, that the prior work FINOLA is extended in two ways: a) generalizing FINOLA to a set of one-way wave equations and b) relaxing the local constraints. Yet, a) is merely an interpretation, and b) is done by summing over multiple FINOLA solutions. I consider this to be rather simple with the increase in PSNR being very natural due to the larger latent space of the resulting model. \n- Insights on whether wave equations are really a common joint principle in natural images: There is no comparison to other autoencoders, no motivation on why the wave equation could be of particular importance to images, and no illustration/interpretation of the latent space encodings $q_i$ or the corresponding solutions $z_i$ of the autoregressive FINOLA features. In particular, a $256 \\times 256 \\times 3$ color image is represented, e.g. with a $8 \\times 1024$ latent space variable and subsequently decoded (using the multi-path FINOLA) with a PSNR of 27.1. But how would approaches perform that use a latent space of similar dimension and do not exploit any autoregressive (or wave-equation-based) operation in their decoder? In order to believe that wave equations or FINOLA are a fundamental \"underlying mathematical property shared by images\", the proposed approach would have to perform significantly better than competing approaches. A truncated PCA, keeping the largest patch-wise DCT (or wavelet) coefficients of an image, and training a standard convolutional autoencoder would be natural very classical baselines. Currently, I am not convinced that multi-path FINOLA / wave equations are the fundamental ingredient that makes the autoencoder work exceptionally well."
                },
                "questions": {
                    "value": "- As stated under 'Weaknesses' above, did you compare to any other encoding-decoding techniques that make the hypothesis, that wave equations are fundamental to all images more credible? The supplement compares to DCT and wavelet coding, but seemingly in terms of general aspects only, not in terms of reconstruction quality. \n- The cited work of FINOLA by Chen et al. does not have a journal, conference or ArXiv ID. Is it published? Please complete the bibliography entry as this citation is of utmost importance for your work."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3126/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698610493987,
            "cdate": 1698610493987,
            "tmdate": 1699636259514,
            "mdate": 1699636259514,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AwrXtqjmbn",
                "forum": "ISrxxvXJQO",
                "replyto": "u5yo75uQxt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3126/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3126/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' Rebuttal (part 1)"
                    },
                    "comment": {
                        "value": "Thank you for dedicating your time and effort to provide feedback on our work. Below, we answer the questions that have been raised.\n\n---\n\n**[Weakness 1]: Contribution: It is stated in section 2, that the prior work FINOLA is extended in two ways: a) generalizing FINOLA to a set of one-way wave equations and b) relaxing the local constraints. Yet, a) is merely an interpretation, and b) is done by summing over multiple FINOLA solutions. I consider this to be rather simple with the increase in PSNR being very natural due to the larger latent space of the resulting model.**\n\nThank you for this comment. Let us clarify the contribution of Hidden-Wave over FINOLA as follows:\n\n***I. Improved Mathematical Description:*** Hidden-Wave (one-way wave equations) enhances the mathematical description of images by generalizing FINOLA. Unlike FINOLA, which connects the partial derivatives of the feature map $\\mathbfit{z}(x, y)$ along the $x$ and $y$ axes using the current feature values, Hidden-Wave directly links these derivatives through one-way wave equations:\n\n|Method|PDEs|\n|---|---|\n|FINOLA| $\\frac{\\partial \\mathbfit{z}}{\\partial x}=\\mathbfit{A}\\mathbfit{z}_n, \\frac{\\partial \\mathbfit{z}}{\\partial y}=\\mathbfit{B}\\mathbfit{z}_n$ |\n| **Hidden Wave (our)** | $\\frac{\\partial \\mathbfit{z}}{\\partial x}=\\mathbfit{A}\\mathbfit{B}^{-1}\\frac{\\partial \\mathbfit{z}}{\\partial y}$ |\n\nThis generalization is *NOT* merely interpretative. We further demonstrate that a superior solution of wave equations can be *achieved* by summing multiple special solutions of FINOLA, validating that, for ***the same feature map size***, one-way wave equations offer a more accurate reconstruction than the two PDEs in FINOLA. This core insight underscores the paper's contribution:\n\n*\"Mathematically, the feature map governed by one-way wave equations is more accurate than the one governed by the two PDEs in FINOLA.\"*\n\n***II. Non-Trivial Summation over Multiple FINOLA:*** Summing over multiple FINOLA provides a simple yet elegant empirical validation of the generalized wave equations' optimality compared to the two PDEs in FINOLA. The feature map after summation stays within the solution space of wave equations through linearity, but departs from the solution space of FINOLA.\n\n***III. Larger Latent Space:*** \nThis observation is invaluable! It sheds light on why multiple FINOLA paths effortlessly navigate the solution space of wave equations for a more optimal solution. Encoding more initial conditions simplifies the approach toward the optimal solution within the wave equation space.\n\nMoreover, multiple FINOLA paths provide a *parameter-efficient* means to adjust compression rate and reconstruction quality. In contrast to vanilla FINOLA, which achieves increased reconstruction quality by enlarging the number of channels and the sizes of matrices $\\mathbfit{A}$ and $\\mathbfit{B}$, multiple FINOLA achieves this by adding paths (or initial conditions) for the same number of wave equations and parameters in matrices $\\mathbfit{A}$ and $\\mathbfit{B}$. The table below compares Hidden-Wave with FINOLA across multiple latent sizes, ranging from 512 to 4096. Both methods achieve higher PSNR by increasing the latent size differently (by increasing paths in Hidden-Wave vs. increasing channels in FINOLA). Although Hidden-Wave is slightly behind FINOLA in terms of PSNR, it maintains a constant size for matrices $\\mathbfit{A}$ and $\\mathbfit{B}$, which is 256 times smaller than FINOLA at latent size 4096.\n\n|Method|Latent Size|DIM of $\\mathbfit{A}$, $\\mathbfit{B}$&darr;|PSNR&uarr;|\n|---|---|---|---|\n|FINOLA | 512 | 512x512| **22.0** |\n| **Hidden Wave (our)** | 512 (256x2) | **256x256 (x0.250)**| 21.8 |\n||||\n|FINOLA| 1024 | 1024x1024| **23.4**|\n| **Hidden Wave (our)** | 1024 (256x4) | **256x256 (x0.063)** | 22.9 |\n||||\n|FINOLA| 2048 | 2048x2048| **24.8**|\n| **Hidden Wave (our)** | 2048 (256x8) | **256x256 (x0.016)** | 24.4 |\n||||\n|FINOLA| 4096 | 4096x4096| **25.9**|\n| **Hidden Wave (our)** | 4096 (256x16) | **256x256  (x0.004)** | 25.4 |"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3126/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700383480923,
                "cdate": 1700383480923,
                "tmdate": 1700383480923,
                "mdate": 1700383480923,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "y0O1gAZTPP",
                "forum": "ISrxxvXJQO",
                "replyto": "an4AKstB7q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3126/Reviewer_Zqsi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3126/Reviewer_Zqsi"
                ],
                "content": {
                    "title": {
                        "value": "Short remark on diagonalization"
                    },
                    "comment": {
                        "value": "In the discussion of [Q1], it is of course still not fully clear (or mathematically guaranteed) that $AB^{-1}$ is diagonalizable (even over $\\mathbb{C}$), right? This just happens to be the case because the set of matrices that can not be diagonalized is so small/thin, that a non-diagonalizable matrix never occurs as a result of a learning process?"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3126/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700664550696,
                "cdate": 1700664550696,
                "tmdate": 1700664550696,
                "mdate": 1700664550696,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CV4D939RUE",
                "forum": "ISrxxvXJQO",
                "replyto": "xoTKW18dX8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3126/Reviewer_Zqsi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3126/Reviewer_Zqsi"
                ],
                "content": {
                    "title": {
                        "value": "Interesting!"
                    },
                    "comment": {
                        "value": "Thanks for the additional experiments! They are very encouraging! I will consider raising my score. Yet, there are quite some changes to be made to the paper to incorporate the new experiments into a more complete story (for which some literature on how much autoencoders can typically compress images could be cited as well). All in all, this seems to be a major revision for me, which should be reviewed again before publication. Yet, I find the general idea very interesting and encouraging and I like the overall work!\n\nAs a side note: A non-FINOLA-based solution to the wave equation would have been a further interesting test. In terms of the final algorithm, it still feels like the sum of something existing/working (FINOLA) does a significant part of the job. Of course, this test is out of scope now (specifically now that I am posting this comment quite late - sorry for that)."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3126/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700665280392,
                "cdate": 1700665280392,
                "tmdate": 1700665280392,
                "mdate": 1700665280392,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7WiiqNKcjY",
                "forum": "ISrxxvXJQO",
                "replyto": "u5yo75uQxt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3126/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3126/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3126/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3126/Reviewers",
                    "ICLR.cc/2024/Conference/Submission3126/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission3126/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3126/Authors"
                ],
                "content": {
                    "title": {
                        "value": "The Revised Paper has been Submitted."
                    },
                    "comment": {
                        "value": "Dear Reviewer Zqsi,\n\nWe are pleased to inform you that we have submitted the revised version of our paper as promised in our previous communication. In this revision, we have extensively reworked the experimental section, incorporating all experiments and discussions addressed during the rebuttal. Additionally, we have provided further clarification on the diagonalization of $\\mathbfit{A}\\mathbfit{B}^{-1}$ in Section 2.4. Furthermore, we have included visualizations and a comprehensive discussion on future work in the appendix.\n\nWe greatly appreciate the invaluable feedback you provided, which has been instrumental in enhancing the quality of our work.\n\nSincerely,\n\n-authors"
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3126/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700734257703,
                "cdate": 1700734257703,
                "tmdate": 1700734276584,
                "mdate": 1700734276584,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dpd7rhpJ9r",
            "forum": "ISrxxvXJQO",
            "replyto": "ISrxxvXJQO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3126/Reviewer_kuax"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3126/Reviewer_kuax"
            ],
            "content": {
                "summary": {
                    "value": "This article studies how to use wave equations in a hidden space of an image to recover it. The wave equations can be solved using first-order autoregressive generative model. The main contribution is to make two significant extensions of existing works to recover diverse images of high-resolution, one based on diagonalization of wave equations, the other based on using several first-order autoregressive models with shared weights."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The diagonalization idea is interesting as it simplifies the way to solve wavelet equations. As a whole, this model defines a powerful way to represent natural images with limited number of parameters."
                },
                "weaknesses": {
                    "value": "A major issue is the lack of mathematical clarity in presenting your model. As a consequence, certain central idea such as diagonalization does not seem to be correct."
                },
                "questions": {
                    "value": "-\tIn your model, A,B matrices defined in eq 2 are real-valued matrices and learnable. How do you guarantee that AB^-1 is diagonalizable such that the eigenvalues are all real-valued? (the Lambda diagonal matrix in eq 4 contains only real values along diagonal). In general, one could only assume that AB^-1 is diagonalizable such that V and Lambda are complex-valued (if AB^-1 is not symmetric). \n-\tIt is not very clear in Fig 1 which part is trainable. Do you also train the encoder and decoder to recover input images ? If the encoder is not trained, how is it defined? Also what is your training loss? \n-\tYour decoder is linear, would that make the recovered images loss details (such as sharp edges) in images? This is not very clear from PNSR metrices. It would be better to add some visual examples."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3126/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698657127189,
            "cdate": 1698657127189,
            "tmdate": 1699636259352,
            "mdate": 1699636259352,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "an4AKstB7q",
                "forum": "ISrxxvXJQO",
                "replyto": "dpd7rhpJ9r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3126/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3126/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' Rebuttal (part 1)"
                    },
                    "comment": {
                        "value": "Thank you for dedicating your time and effort to provide feedback on our work. Below, we answer the questions that have been raised.\n\n---\n\n**[Q1]: In your model, A,B matrices defined in eq 2 are real-valued matrices and learnable. How do you guarantee that AB^-1 is diagonalizable such that the eigenvalues are all real-valued? (the Lambda diagonal matrix in eq 4 contains only real values along diagonal). In general, one could only assume that AB^-1 is diagonalizable such that V and Lambda are complex-valued (if AB^-1 is not symmetric).**\n\nThank you for your valuable feedback. Allow us to provide further clarification regarding the diagonalization process. In equation 4, it's crucial to note that we do *NOT* impose any constraints on the eigenvalues, allowing them to be complex-valued in the diagonalization of $\\mathbfit{A}\\mathbfit{B}^{-1}$. In other words, the wave speed can take complex values. It's important to emphasize that $\\mathbfit{\\Lambda}$ and $\\mathbfit{V}$ are *Not* explicitly trainable; instead, they are computed from the trainable matrices $\\mathbfit{A}$ and $\\mathbfit{B}$ through diagonalization post-training. We meticulously examined the eigenvalues in $\\mathbfit{\\Lambda}$ and eigenvectors in $\\mathbfit{V}$ across multiple models after training, confirming their complex nature.\n\nIn Section 2.4 of the paper, we also discuss an alternative approach wherein we constrain $\\mathbfit{\\Lambda}$ and $\\mathbfit{V}$ to be real-valued by constraining matrices $\\mathbfit{A}$ and $\\mathbfit{B}$ as follows:\n\n$\\mathbfit{A}=\\mathbfit{P}\\mathbfit{H}_x, \\quad \\mathbfit{B}=\\mathbfit{P}\\mathbfit{H}_y$\n\nwhere $\\mathbfit{P}$ is the shared projection matrix for $\\mathbfit{A}$ and $\\mathbfit{B}$, and $\\mathbfit{H}_x$ and $\\mathbfit{H}_y$ are two real-valued diagonal matrices that are learnable. Consequently, the wave speed $\\mathbfit{\\Lambda} = \\mathbfit{H}_x\\mathbfit{H}_y^{-1}$ is also real-valued.\n\nHowever, it's noteworthy that enforcing real values for the wave speed leads to a performance drop in reconstruction quality. Table 2 (or the table below) compares the reconstruction PSNR between complex and real speeds, showing a slight decrease from 26.1 to 24.9.\n\n|Speed Value|\\#Channels $C$|\\#Solutions $M$|PSNR|\n|---|---|---|---|\n|Complex Value| 1024|4|**26.1**|\n|Real Value| 1024|4|24.9|\n\n---\n\n**[Q2]: It is not very clear in Fig 1 which part is trainable. Do you also train the encoder and decoder to recover input images ? If the encoder is not trained, how is it defined? Also what is your training loss?**\n\n***Trainable Components:*** The trainable elements in our architecture encompass both the encoder and decoder. The matrices $\\mathbfit{A}$ and $\\mathbfit{B}$ for the multi-path FINOLA are also trainable. Throughout training, the entire network is optimized end-to-end. It's essential to note that the wave speed $\\mathbfit{\\Lambda}$ and the coefficient matrices $\\mathbfit{H}_x$ and $\\mathbfit{H}_y$ are not explicitly trainable. Instead, they are computed post-training through the diagonalization process using the trainable matrices $\\mathbfit{A}$ and $\\mathbfit{B}$.\n\n***Training of Encoder and Decoder*** Indeed, both the encoder and decoder undergo training to facilitate the reconstruction of input images.\n\n***Training Loss:*** The training process employs mean square error over image pixels as the loss function, ensuring the end-to-end optimization of the entire network."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3126/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700434576760,
                "cdate": 1700434576760,
                "tmdate": 1700434576760,
                "mdate": 1700434576760,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eE9Z5NmksK",
                "forum": "ISrxxvXJQO",
                "replyto": "fWlG7afPI3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3126/Reviewer_kuax"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3126/Reviewer_kuax"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your answers. I shall maintain my score for the following 2 reasons, \n- The notation in the article was not mathematically rigorous enough for me to understand the model. An improvement is needed. I still do not understand why xi(x,y) is in R^C (defined on top of page 4). \n- I do not know how to motivate the study of real-valued eigenvalues lambda_k in Table 2, as you are assuming that A B^-1 is diagonalizable in your model. It is not clear to me whether this assumption still holds when you consider real-valued eigenvalues."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3126/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700660170195,
                "cdate": 1700660170195,
                "tmdate": 1700660170195,
                "mdate": 1700660170195,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oUetxZUimQ",
                "forum": "ISrxxvXJQO",
                "replyto": "dpd7rhpJ9r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3126/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission3126/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3126/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3126/Reviewers",
                    "ICLR.cc/2024/Conference/Submission3126/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission3126/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3126/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Additional Mathematical Clarification (response to Reviewer kuax)"
                    },
                    "comment": {
                        "value": "**[Second Round Q1]: why $\\mathbfit{\\zeta}(x,y) \\in \\mathbb{R}^C$ (defined on top of page 4)**\n\nThank you for pointing out the notation discrepancy and expressing concerns about the mathematical rigor in our paper.\n\nThe notation stating $\\mathbfit{\\zeta}(x, y) \\in \\mathbb{R}^C$ on top of page 4 was an oversight. We acknowledge this error and wish to clarify that $\\mathbfit{\\zeta}(x, y)$ is correctly defined as a complex-valued vector, denoted as $\\mathbfit{\\zeta}(x, y) \\in \\mathbb{C}^C$. This vector results from a linear projection of the real-valued feature map $\\mathbfit{z}(x, y)$ using the inverse of the complex-valued eigenvalue matrix $\\mathbfit{V}^{-1}$, as expressed by $\\mathbfit{\\zeta} = \\mathbfit{V}^{-1}\\mathbfit{z}$. Notably, $\\mathbfit{V}^{-1}$ is itself a complex-valued matrix, as indicated by the relationship $\\mathbfit{A}\\mathbfit{B}^{-1} = \\mathbfit{V}\\mathbfit{\\Lambda}\\mathbfit{V}^{-1}$.\n\nWe apologize for any confusion caused by this oversight and assure you that we are taking steps to enhance the clarity and accuracy of the mathematical expressions in our paper. If you have further questions or specific areas that require clarification, please feel free to let us know. Your feedback is crucial in improving the precision of our work.\n\n---\n\n**[Second Round Q2]: I do not know how to motivate the study of real-valued eigenvalues lambda_k in Table 2, as you are assuming that A B^-1 is diagonalizable in your model. It is not clear to me whether this assumption still holds when you consider real-valued eigenvalues.**\n\n***Motivation for Studying Real-Valued Eigenvalues:***\nThe motivation behind our study of real-valued eigenvalues, specifically $\\lambda_k$ in Table 2, stems from the question *\"how the number type (real or complex) of wave speeds impacts reconstruction accuracy?\"* By default, wave speeds are complex-valued, as previously discussed. To explore this, we conducted this ablation study, transitioning the number type from complex to real. The results of this investigation revealed a slight degradation in performance.\n\n***Diagonalizable Assumption for Real-Valued Eigenvalues:***\nAddressing your concern about the assumption of diagonalizability when considering real-valued eigenvalues, we affirm that ***the diagonalizable assumption holds in our model***. We constrain the FINOLA matrices $\\mathbfit{A}$ and $\\mathbfit{B}$ as follows:\n\n$\\mathbfit{A}=\\mathbfit{P}\\mathbfit{H}_x, \\quad \\mathbfit{B}=\\mathbfit{P}\\mathbfit{H}_y$\n\nwhere $\\mathbfit{P}$ is a real-valued matrix, and $\\mathbfit{H}_x$ and $\\mathbfit{H}_y$ are real-valued diagonal matrices, all of which are learnable. This results in $\\mathbfit{A}\\mathbfit{B}^{-1} = \\mathbfit{P}\\mathbfit{H}_x\\mathbfit{H}_y^{-1}\\mathbfit{P}^{-1}$. Importantly, the eigenvalues $\\mathbfit{\\Lambda} = \\mathbfit{H}_x\\mathbfit{H}_y^{-1}$ and eigenvector $\\mathbfit{P}$ are explicitly real-valued and learnable.\n\nWe want to highlight that the only assumption in this context is that the projection matrix $\\mathbfit{P}$ is invertible or full rank. Our experiments confirm that the learned $\\mathbfit{P}$ matrix is indeed invertible."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3126/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700681084417,
                "cdate": 1700681084417,
                "tmdate": 1700681782654,
                "mdate": 1700681782654,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pirUObTYXT",
            "forum": "ISrxxvXJQO",
            "replyto": "ISrxxvXJQO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3126/Reviewer_nr9w"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3126/Reviewer_nr9w"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes an extension of FINOLA to reconstruct images using one-way wave equations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The theoretical intuition behind using multiple-path FINOLA is discussed thoroughly. They were also able to show that FINOLA is a special case of the method proposed in this paper. The quality seems to be consistently going up in the part of the graph showed to us."
                },
                "weaknesses": {
                    "value": "In figure 4, it is not clear if there is plateau for the reconstruction quality in terms of M & C. The curves seem to be increasing linearly, and we do not see a slowdown"
                },
                "questions": {
                    "value": "See weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3126/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3126/Reviewer_nr9w",
                        "ICLR.cc/2024/Conference/Submission3126/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3126/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699116598264,
            "cdate": 1699116598264,
            "tmdate": 1700420425050,
            "mdate": 1700420425050,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pNO9yerINE",
                "forum": "ISrxxvXJQO",
                "replyto": "pirUObTYXT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3126/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3126/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' Rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for dedicating your time and effort to provide feedback on our work. Below, we answer the questions that have been raised.\n\n---\n\n**[Weakness 1]: In figure 4, if there is plateau for the reconstruction quality in terms of M & C**\n\nThanks for this insightful suggestion! We conducted additional experiments to explore the impact of increasing the number of special solutions (or the number of FINOLA paths $M$) to 16. We evaluated for three choices of the number of wave equations (or the number of channels $C$), specifically 128, 256, and 512. The PSNR results are presented in the last column of the table below. In comparison to the rate of change observed over smaller $M$ values (1\u21922, 2\u21924, 4\u21928), the PSNR increase from $M=8$ to $M=16$ slows down, indicating the onset of a plateau.\n\n***Table: Reconstruction PSNR for different number of channels ($C$) and special solutions ($M$)***\n|Channels|**M=1**|**M=2**|**M=4**|**M=8**|**M=16**|\n|---|---|---|---|---|---|\n|**C=512**|  22.0|23.2|24.5|25.8|**26.3**|\n|**C=256**|  20.6|21.8|22.9|24.4|**25.5**|\n|**C=128**| 19.3|20.4|21.5|22.5| **23.1**|"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3126/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700420227639,
                "cdate": 1700420227639,
                "tmdate": 1700420227639,
                "mdate": 1700420227639,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kABh0sTEtH",
                "forum": "ISrxxvXJQO",
                "replyto": "pNO9yerINE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3126/Reviewer_nr9w"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3126/Reviewer_nr9w"
                ],
                "content": {
                    "comment": {
                        "value": "I am so sorry about the previous submitted review. It looks an unfinished version of the review was submitted. Please check the updated review. But, your comment has already answered my question. Thank you."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3126/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700420510448,
                "cdate": 1700420510448,
                "tmdate": 1700420510448,
                "mdate": 1700420510448,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]