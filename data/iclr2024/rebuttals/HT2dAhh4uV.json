[
    {
        "title": "Learning to Compose: Improving Object Centric Learning by Injecting Compositionality"
    },
    {
        "review": {
            "id": "eMmXvOukWV",
            "forum": "HT2dAhh4uV",
            "replyto": "HT2dAhh4uV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5301/Reviewer_H2Bo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5301/Reviewer_H2Bo"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on the compositional ability in object-centric learning. Existing works mostly rely on auto-encoding training paradigm and may sacrifice the object disentanglement. While this work explicitly introduces an object composition path in addition to the original reconstruction objective, and employs generative prior to validate the rendered object compositions. The experiments show that the proposed method achieves better resutls in object disentanglement and enhances the robustness to hyper-parameters."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well motivated. Compositionality is a vital property in object-centric perception and generation. The authors conclude the potential weakness in existing auto-encoding based object-centric learning methods and introduce an object composition path for explicit optimization on composition.\n2. The method is simple and intuitive. It uses generative prior to help validate the rendered object composition from mixed slot representations and guides the object disentanglement and composition learning.\n3. The experiments show the improvement on object-centric benchmarks. And the proposed method is robust to the number of slots, which is a significant hyper-parameter in object-centric learning and difficult to determine."
                },
                "weaknesses": {
                    "value": "1. Most experiments are conducted on the synthetic objects, more experiments on realistic complex scenes, e.g., COCO, are desired.\n2. More compared methods should be included, e.g., DINOSAURE [1].\n3. There are some recent works focusing on the binding ability of the slots to specific object types or properties, e.g., [2]. Is it possible to employ the binding ability to enhance the interpretablity of slot mixing and further enahnce the composition ability?\n\n[1] Seitzer et al. Bridging the gap to real-world object-centric learning. ICLR 2023.\n[2] Jia et al. Improving Object-centric Learning with Query Optimization. ICLR 2023."
                },
                "questions": {
                    "value": "See weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5301/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698719587456,
            "cdate": 1698719587456,
            "tmdate": 1699636530917,
            "mdate": 1699636530917,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bpa4tWGXLO",
                "forum": "HT2dAhh4uV",
                "replyto": "eMmXvOukWV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5301/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5301/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to Reviewer H2Bo (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the positive comments. Below we respond to the individual questions.\n\n> **Q1.**  Most experiments are conducted on synthetic objects, more experiments on realistic complex scenes, e.g., COCO, are desired. \n\n**A1.** We appreciate the valuable comments. First, we would like to kindly note that learning object-centric representation in complex real-world images is widely perceived as challenging, and existing works rely on frozen pre-trained encoders and decoders to alleviate the problem. Since freezing the image encoder with a shallow slot attention module limits the opportunity to investigate the true impact of the compositional objective, we follow the standard settings that employ the synthetic data for evaluation.\n\nNonetheless, we agree with the reviewer that the synthetic scenes are insufficient to demonstrate the scalability of our compositional objective to complex scenes. Following the reviewer\u2019s suggestion, we conducted preliminary experiments on the BDD100K dataset, which contains real-world driving scenes with many objects (cars, signs, buildings, trees, pedestrians, etc.) as well as complex interactions among objects and with the environment (occlusion, shadow, lighting, reflection, etc.). Following the prior work [1], we employed the pre-trained DINOv2 [2] as an image encoder and Stable Diffusion [3] as the generative prior.  We then apply warm-up training for the one-shot decoder for 140K steps with an auto-encoding objective, and apply our compositional objective for the next 100K steps. \n\nThe results are summarized in Section B.7 in the Appendix (Figures 10 and 11). Based on visualization of the slot attention map (Figure 10), we observe that our method captures composable object instances not only from the foreground such as cars, buses, and trucks, but also from static environments such as road signs, trees, buildings, sidewalks, etc. In contrast, learning with only diffusion loss often separates an object into multiple slots, or contains multiple objects into a single slot. When composing two images by mixing their slots (Figure 11), we observe that our method successfully generates realistic scenes, modeling complex correlations among objects and environments. It appropriately adapts the appearance of newly added/removed objects, their shadow, reflections in the front glass and hood, and sometimes even global illumination change caused by removing the sun. Our method also generates more accurate compositions compared to the baseline, which often fails to adhere to the addition or removal of the objects. \n\nTo summarize, our preliminary experiments showed promising results that the proposed compositional objective can scale to complex scenes and model complex correlations among objects. We believe that incorporating the compositional objective in self-supervised learning can greatly enhance object-centric learning in complex scenes through end-to-end training with the deep encoder, which we plan to investigate in future work.\n\n[1] Jiang et al., Object-centric slot diffusion, NeurIPS 2023.  \n\n[2] Oquab et al., DINOv2: Learning robust visual features without supervision, arXiv preprint 2023.  \n\n[3] Rombach et al., High resolution image synthesis with latent diffusion models, CVPR 2022."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5301/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700686447309,
                "cdate": 1700686447309,
                "tmdate": 1700686447309,
                "mdate": 1700686447309,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GHdzVSuAWK",
                "forum": "HT2dAhh4uV",
                "replyto": "eMmXvOukWV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5301/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5301/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to Reviewer H2Bo (2/2)"
                    },
                    "comment": {
                        "value": "> **Q2.**  Comparison to DINOSAUR. \n\n**A2.** We appreciate the comment. As suggested by the reviewer, we included the DINOSAUR as an additional baseline and reported the comparison results in the table below. Although it performs reasonably well in CLEVRTex and PTR datasets, it fails dramatically in MutiShapeNet and Super-CLEVR datasets; we observe that it is because DINOSAUR encodes the entire foreground objects into a single slot in these datasets. Such failures are attributed to the frozen image encoder, which fails to generalize to domains unseen during the pre-training. It shows that learning the image encoder end-to-end with object-centric representation is important for building a robust and general object-centric learning framework. \n\nWe would also like to note that the main contribution of the DINOSAUR is employing the pre-trained features in object-centric learning, which is orthogonal to our contribution and hence can be combined together. In Section B.7 in the Appendix, we provided results on a real-world dataset, where we incorporated the compositional loss on top of the pre-trained DINO.  \n\n\n| **CLVERTex** | **FG-ARI (\u2191)** | **mIoU (\u2191)** | **mBO (\u2191)** |\n|--------------|:--------------:|:------------:|:-----------:|\n|   DINOSAUR   |      70.67     |     48.20     |    50.03    |\n|    SLATE+    |      71.29     |     52.04    |    52.17    |\n|      LSD     |      76.44     |     72.32    |    72.44    |\n|     Ours     |    **93.06**   |   **74.82**  |  **75.36**  |\n\n\n| **MultiShapeNet** | **FG-ARI (\u2191)** | **mIoU (\u2191)** | **mBO (\u2191)** |\n|-------------------|:--------------:|:------------:|:-----------:|\n|      DINOSAUR     |      1.99      |     12.53    |     20.30    |\n|       SLATE+      |      70.44     |     15.55    |    15.64    |\n|        LSD        |      67.72     |     15.39    |    15.46    |\n|        Ours       |    **89.80**    |   **59.21**  |   **59.40**  |\n\n| **PTR**  | **FG-ARI (\u2191)** | **mIoU (\u2191)** | **mBO (\u2191)** |\n|----------|:--------------:|:------------:|:-----------:|\n| DINOSAUR |      58.23     |     23.20     |    26.48    |\n|  SLATE+  |    **91.25**   |     14.10     |    14.22    |\n|    LSD   |      61.10      |     10.18    |    10.33    |\n|   Ours   |      90.65     |   **40.89**  |  **41.45**  |\n\n\n| **SuperCLEVR** | **FG-ARI (\u2191)** | **mIoU (\u2191)** | **mBO (\u2191)** |\n|----------------|:--------------:|:------------:|:-----------:|\n|    DINOSAUR    |      2.92      |      5.70     |    8.93    |\n|     SLATE+     |      43.73     |     29.12    |    29.49    |\n|       LSD      |      54.79     |     14.12    |    14.43    |\n|      Ours      |    **63.08**   |   **47.17**  |  **48.03**  |  \n\n---\n> **Q3.**  There are some recent works focusing on the binding ability of the slots to specific object types or properties, *e.g.*, \u201c Improving Object-centric Learning with Query Optimization\u201d. Is it possible to employ the binding ability to enhance the interpretability of slot mixing and further enhance the composition ability?\n\n**A3.** We appreciate the insightful comment. Indeed, our work investigates a general problem of learning compositional representation and employs simple mixing strategies for generality.  We believe that incorporating the binding ability of the slots can enhance the mixing strategy by reducing the search space for the valid slot combination, and improving the interpretability of the composite representation which is useful especially in manipulation tasks."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5301/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700686787949,
                "cdate": 1700686787949,
                "tmdate": 1700686872994,
                "mdate": 1700686872994,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "aB1fyYZ9bM",
            "forum": "HT2dAhh4uV",
            "replyto": "HT2dAhh4uV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5301/Reviewer_f9fX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5301/Reviewer_f9fX"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a generative objective to regularize the learned representation of a Slot Attention model. In particular, the generative objective ensures that decoding a composition of slots from different images results in a realistic image. The additional regularizer trades off reconstruction quality for better compositionality of the learned representation."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The authors identify a weakness in the standard reconstruction objective of Slot Attention and similar papers: the reconstruction objective does not explicitly encourage compositionality of the learned representation. This observation calls for a generative objective that ensures that composition of slots can be decoded into realistic images. Instead of implementing this with a GAN discriminator, which would require an additional component, the authors follow the approach of Poole et al. (2022) where the diffusion model is reused as a generative prior.\n\nThe idea of using a cheap surrogate one-shot decoder to approximate an expensive denoising process is quite clever and surely helps speeding up training.\n\nThe qualitative results on compositional generation are convincing. I would like to see if the method would work on real-world images like the ones in COCO."
                },
                "weaknesses": {
                    "value": "In the proposed method, the number of losses, regularizers, and training tricks to balance is high. I would appreciate a more thorough ablation study where each component is isolated and tested. The combinations in table 2 are valid but do not cover all possibilities.\n\nI would appreciate additional evaluation metrics that are not based on segmentation. Learning a good object-centric representation means much more than achieving good segmentation. It is important to show that the learned slots are useful for downstream tasks such as counting, tracking, property prediction, etc. This way the claim made in the introduction that \"[the method] significantly boosts the overall quality of object-centric representations\" can be supported. My rating on \"soundness\" would be higher if the authors had included such experiments."
                },
                "questions": {
                    "value": "May I recommend a bar plot for Figure 3? The current plot based on colored Xs is quite hard to read. Moreover, a bar plot would allow to show the standard deviation of the results over multiple independent runs, which is important to strengthen the claims made in the text.\n\nCan you provide non-segmentation based evaluation metrics? They can be the same as in Jiang et al. (2023) for example.\n\nIn Jiang et al. (2023) the compositional generation appear to be much stronger that what is shown in Figure 4. It seems to me that for the proposed method the authors chose a source image where all objects are well separated and therefore the composition looks good. Whereas the segmentation in LSD and SLATE+ on the same image is poor and therefore the composed image looks bad. Can you clarify in the caption if the examples were cherry-picked? Can you find another image where all 3 methods segment correctly the objects that you want to compose? This way the comparison would be fairer.\n\nTypo: \"is not contrained to\""
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5301/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698790982302,
            "cdate": 1698790982302,
            "tmdate": 1699636530810,
            "mdate": 1699636530810,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DCTpD7T30P",
                "forum": "HT2dAhh4uV",
                "replyto": "aB1fyYZ9bM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5301/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5301/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to Reviewer f9fX (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the positive comments. Below we respond to the individual questions.\n\n> **Q1.** In the proposed method, the number of losses, regularizers, and training tricks to balance is high. I would appreciate a more thorough ablation study where each component is isolated and tested. The combinations in table 2 are valid but do not cover all possibilities. \n\n**A1.** We appreciate the valuable comment. Following the reviewer\u2019s suggestion, we report the ablation study on all valid combinations of the components in the table below. We observe that the overall trends remain the same: considering the composition of two images for object-centric learning, either via composition loss or regularization, is helpful, and adding the slot initialization also improves the performance as well as enabling stable training and faster convergence. Overall, all components of our method contribute to the improvement, while employing composition loss and regularization are the most critical. We appreciate the comment and will include the results in the paper.\n\n| **L_prior** | **L_Reg** | **Share slot init** | **FG-ARI** |  **mIoU** |  **mBO**  |\n|:-----------:|:---------:|:-------------------:|:----------:|:---------:|:---------:|\n|      X      |     X     |          X          |    42.48   |   52.26   |   52.41   |\n|      \u2714      |     X     |          X          |    65.76   |   67.62   |   67.62   |\n|      \u2714      |     X     |          \u2714          |    70.29   |   69.08   |   69.28   |\n|      X      |     \u2714     |          \u2714          |    65.26   |   58.81   |   58.99   |\n|      X      |     \u2714     |          X          |    57.58   |    53.50   |    53.70   |\n|      \u2714      |     \u2714     |          X          |  **88.72** |   73.24   |   73.63   |\n|      \u2714      |     \u2714     |          \u2714          |    88.15   | **75.30** | **75.64** |\n\n---\n\n>**Q2.** Can you provide non-segmentation based evaluation metrics? They can be the same as in Jiang et al. (2023) for example. \n\n**A2.** We appreciate the valuable comment. Following the reviewer\u2019s suggestion, we follow the evaluation protocol of Jiang et al. (2023) and evaluate the quality of the learned representation by predicting the object properties from the slots using object-wise annotations in CLEVRTex, PTR, and Super-CLEVR datasets (we could not include the MultiShapeNet dataset due to missing object labels). We report the results in Section B.6 in the Appendix and the tables below.\n\nOverall, we observe that our method consistently outperforms the baselines in all datasets and properties, showing that the learned representations capture the characteristics of the objects well. We found that the prediction error in Super-CLEVR dataset tends to be higher than the other datasets, since it contains many small and occluded objects. We appreciate the comment and will include the results in the paper.  \n\n| **CLEVRTex** | **SLATE+** | **LSD** |  **Ours**  |\n|:------------:|:----------:|:-------:|:----------:|\n| Position (\u2193) |   0.18   |  0.16 | **0.10** |\n|   Shape (\u2191)  |    78.72   |  85.07  |  **88.86** |\n| Material (\u2191) |    67.99   |  82.33  |  **84.29** |\n\n\n|    **PTR**   | **SLATE+** | **LSD** |  **Ours**  |\n|:------------:|:----------:|:-------:|:----------:|\n| Position (\u2193) |   0.22   |  0.60 | **0.14** |\n| Shape (\u2191) |    88.21   |   75.80  |   **90.00**   |\n\n\n| **Super-CLEVR** | **SLATE+** | **LSD** |  **Ours**  |\n|:--------------:|:----------:|:-------:|:----------:|\n|  Position (\u2193)  |   0.54   |  0.44 | **0.43** |\n|    Shape (\u2191)   |    76.28   |   76.50  |  **80.67** |\n|  Material (\u2191)  |    68.43   |  69.24  |  **71.31** |"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5301/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700687221504,
                "cdate": 1700687221504,
                "tmdate": 1700687221504,
                "mdate": 1700687221504,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PWbScIklyj",
                "forum": "HT2dAhh4uV",
                "replyto": "aB1fyYZ9bM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5301/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5301/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to Reviewer f9fX (2/2)"
                    },
                    "comment": {
                        "value": "> **Q3.** May I recommend a bar plot for Figure 3? The current plot based on colored Xs is quite hard to read. Moreover, a bar plot would allow to show the standard deviation of the results over multiple independent runs, which is important to strengthen the claims made in the text.\n\n**A3.** We appreciate the comment. We will replace Figure 3 with a bar or box plot to present the standard deviations of each run more clearly. \n\n---\n\n> **Q4.**  In composition generation (Figure 4), please specify whether the outputs are cherry-picked. If so, please provide cases where all three methods reasonably work for fair comparison. \n\n**A4.** Figure 4 illustrates the common failure cases of the baselines, which are caused by encoding an object into multiple slots (*e.g.*, separating an object into multiple parts). For a more comprehensive understanding of the baselines, we also present additional results in Section B.5 (Figure 9) in the Appendix. In this figure, we present composite generation results similar to Figure 4 but for the examples where the baselines capture an object into a slot successfully. It shows that despite the reasonable slot attention map, the composite images produced by the baselines often exhibit unsatisfactory quality by distorting objects or altering their appearances due to the addition or removal of the objects. It shows that the object-centric representation learned in the baselines is generally not as compositional as our method."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5301/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700687334210,
                "cdate": 1700687334210,
                "tmdate": 1700687334210,
                "mdate": 1700687334210,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ThN0XYiQqO",
            "forum": "HT2dAhh4uV",
            "replyto": "HT2dAhh4uV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5301/Reviewer_6966"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5301/Reviewer_6966"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a scheme for encouraging compositionality in slot-based Object-Centric Learning (OCL) models by introducing a compositional objective alongside the standard auto-encoding reconstruction loss. In particular, they enforce that mixing subsets of slots from two separate scenes should result in a \"valid\" image, as determined by a generative prior which is trained through the auto-encoding process - this is accomplished by using a diffusion model as the decoder. To enable fast-decoding and cheaper evaluation of gradients for the encoder through the compositional path, they also train a transformer-decoder on the autoencoding tasks.\n\nThey perform extensive experiments and ablations, demonstrating the efficacy of their method against other state-of-the art methods, both in terms of performance and robustness to hyperparameters such as slot-count and decoder size (which are sensitive in many OCL methods). To the best of my knowledge, the idea of encouraging compositionality of slots through a valid-mixing objective is novel and forms a valuable contribution to the OCL community."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper's methodology is well described with helpful figures, and the experiments are comprehensive. Notably, the experiments on parameter robustness in sec 5.2, as well as the qualitative results shown alongside experiments with slot mixing strategy in the appendices, provide a compelling case for the benefits of this method against other SoTA methods, beyond the mere improvement of performance."
                },
                "weaknesses": {
                    "value": "There are no major weaknesses with the methodology or evaluations. The presentation is clear in most places, but there are many grammatical mistakes. It is recommended that these be fixed through the use of automated grammar checkers or proof-reading by a native speaker.\n\nSome minor notes on clarification:\n* On the first reading, it was not entirely clear how the one-shot decoder and diffusion decoders were being trained (i.e. whether one, or both, were being used in the auto-encoding path) - this is made clear at the start of sec 3.3 \"in auto-encoding path [...] two different decoders [...] are trained\", but it would be good to make it more explicit in prior paragraphs when the relevant decoders are introduced and their training is discussed.\n* Related to the above, it would be good to make it clear in the figure that *both* decoders are trained/used on the auto-encoding branches, for example by putting \"(both) Decoder\" on the branches"
                },
                "questions": {
                    "value": "Overall the work is very interesting and comprehensively explained, and I was left only with minor questions:\n1) *Why background slots do not conflict*: Could the authors explain whether / why, if not, background slots conflict when sampling slots with the random mixing strategy? In this case 1) is there any reason to expect canonical slot ordering, such that two background slots are unlikely to be sampled or 2) any reason why if two background slots are sampled, this does not lead to invalid scenes?\n2) *On the relative contributions of Regularization+Shared Slot Initializations (Row 2) and the compositional prior (Row 1) in Table 2*: It is Interesting that Row 2, is roughly as effective as Row 1. Was there a significant difference in the qualitative behaviour of the models in these two settings; i.e. did Row 1 yield \"better\" slots, but lead to worse metrics for some other reason - or can we conclude that the bias towards compositional slot representations in both of these settings was comparably strong? (which would be somewhat surprising!). Perhaps the decoder receives a stronger learning signal in Row 2, and in Row 1 the generative prior acting by itself is constrained because the decoders don't learn as well, and so provide a weaker compositional signal?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5301/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5301/Reviewer_6966",
                        "ICLR.cc/2024/Conference/Submission5301/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5301/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698826288322,
            "cdate": 1698826288322,
            "tmdate": 1700099237401,
            "mdate": 1700099237401,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YEdULOeY18",
                "forum": "HT2dAhh4uV",
                "replyto": "ThN0XYiQqO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5301/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5301/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to Reviewer 6966"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the positive comments. Below we respond to the individual questions.\n\n>**Q1.** Suggestions to improve presentation\n\n**A1.** We appreciate the valuable comment. We will modify Figure 1 and the first paragraph of the Section 3 to clarify that we are learning two decoders simultaneously.  \n\n---\n>**Q2.**  Why background slots do not conflict: Could the authors explain whether / why, if not, background slots conflict when sampling slots with the random mixing strategy? In this case 1) is there any reason to expect canonical slot ordering, such that two background slots are unlikely to be sampled or 2) any reason why if two background slots are sampled, this does not lead to invalid scenes?\n\n**A2.** We appreciate the comment. In the random mixing strategy, each slot is randomly and independently initialized in each image, and combined into a composite representation by randomly sampling slots from two images. Due to this randomness, there is a chance that mixed slots (*i.e.*, composite representation) lead to ***invalid*** combinations, such as the ones composed only with the objects without background. To avoid this, we share the slot initialization between two images, and mix the slots ***exclusively*** according to their slot indices (*e.g.*, we can sample i-th slot either from the first or second images, but NOT from both). Formally, let $I_1$ and $I_2$ be a random partition of slot indices  *i.e.*, $I_1\\\\cup I_2=\\\\{1,...,N\\\\}, I_1\\\\cap I_2=\\\\emptyset$. Then we construct the composite slot by $\\\\mathbf{S}^c=\\\\mathbf{S}^1_{I_1}\\\\cup \\\\mathbf{S}^2_{I_2}$, where $\\\\mathbf{S}^1$ and $\\\\mathbf{S}^2$ are slots extracted from the first and the second images, respectively. Since the slots are initialized identically between two images and the slots extracted from the identical initialization never appear twice in the composite representation, the model can learn to associate certain slot initialization to a specific part of a scene, such that the composite representation is ensured to be always valid. Indeed, we observed that the model learns to associate the background to a certain slot, as shown in Figure 6 in the appendix. We appreciate the comment and will revise the paper to clarify this point.  \n\n---\n>**Q3.**  In ablation study (Table 2), why and how Regularization+Shared Slot Initializations is comparable to using only compositional prior? \n\n**A3.** We would like to first note that both the regularization and compositional loss (generative prior) contribute to enhancing the compositionality of the representation since they are computed from the composition of two images and provide learning signals orthogonal to the auto-encoding path. We observe that the two losses also promote complementary behaviors to the model; The composition loss provides a direct learning signal that the composite image should be realistic, promoting the compositionality of the slot representation. However, we observed that it sometimes captures loose object boundaries especially when the background is monotonic, since they are highly correlated hence compositional. On the other hand, the regularization loss encourages the part of the image captured by a slot to be consistent before and after composition, without considering the realism of the composite image. Combining these two, our method learns to capture compositional components of a scene to a slot while maintaining the consistent association between the slot and a scene, leading to the highest performance."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5301/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700686303464,
                "cdate": 1700686303464,
                "tmdate": 1700686303464,
                "mdate": 1700686303464,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gOoTO8qUec",
            "forum": "HT2dAhh4uV",
            "replyto": "HT2dAhh4uV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5301/Reviewer_Cp4U"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5301/Reviewer_Cp4U"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new training objective that encourages learning object-centric representation by considering mixtures over objects across images, and empirically explores it in conjunction with a slot attention architecture over CLEVR-like data."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- **Evaluation**: Multiple types of experiments are conducted, including quantitative/qualitative unsupervised segmentation, ablation studies, and analysis of robustness. The approach is shown to be robust to factors such as number of slots, as well as settings of the encoder and decoder, aspects which slot attention methods are usually sensitive to.\n- **Research Context**: The authors do a good job in providing the relevant research context as well as model\u2019s preliminaries, pointing to the limitations of some of the prior works.\n- **Presentation**: The writing quality is good and the paper is clear, well-organized and easy to follow. The first parts of the model section are particularly clear, and both the diagrams and visualizations help understanding the approach and its behavior. The supplementary is also good, providing implementation details and additional quantitative and qualitative results."
                },
                "weaknesses": {
                    "value": "- **Synthetic Data & Scalability**: The experiments are performed over synthetic data only. I recommend exploring scalability to real-world data too. This is especially important since it is unclear to me whether such an approach would scale well to more diverse real-world data, where there are correlations between object occurrence as well as appearance. I don\u2019t know for sure but it might be the case that for realistic data the loss could damage the model\u2019s learning compared to standard auto-encoding, by encouraging it to unlearn important object correlations. For instance, even for the images at figure 1, the lighting of the mixed objects does not fit the context, potentially making the model less aware of the impact of lighting conditions on the object\u2019s appearance. For this reason I believe it is really critical to study this objective in the context of realistic data too.\n- **Compositional Synthesis**: The results about compositional synthesis aren't the most compelling. I would like to see object mixing in cases that involve more interesting interactions among the objects (like reflection, more substantial occlusions, and matching of the appearance between the object and its environment). It does perform better than the baselines so that\u2019s still an improvment.\n- **Novelty & Related Works**: The specific proposed idea is novel, but object mixing as a form of regularization isn\u2019t. For instance,  it would be good to cite and discuss the \u201cObject Discovery with a Copy-Pasting GAN\u201d paper as a related prior work that also uses mixes over image to encourage object discovery."
                },
                "questions": {
                    "value": "- **Baselines**: It would be good to also compare to a vanilla slot attention as a baseline.\n- **Slot Initialization* I didn\u2019t fully understand the intuition why a shared slot initialization should help avoiding bad mixes of objects. It seems to me that learning good combinations of objects could be achieved instead by not sharing the initialization scheme between them and letting them interact across the two images to coordinate valid object compositions. If possible please explain the initialization point further.\n- **Diffusion**: It would be helpful to make it clearer how the denoising diffusion and auto-encoding fit together in the proposed framework. \n- **Super-CLEVR**: The ARI performance drop on super-clevr is particularly large, even though multiShapenet and PTR also include objects with subparts, and the latter even includes more complicated backgrounds. Do you have an intuition of why is that the case? \n- \u201cit decompose\u201d -> \u201cit decomposes\u201d\n- \u201cemployed auto-encoding framework\u201d -> \u201cemploy an auto-encoding framework\u201d\n- \u201cas architectural/algorithmic bias\u201d -> \u201cas an architectural/algorithmic bias\u201d\n- \u201catttention\u201d -> \u201cattention\u201d\n\nOverall, while the paper is well-made, I'm in between 5 and 6 due to mentioned weaknesses above. But I'll be glad to raise my score if results will be presented over realistic data."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5301/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699483189592,
            "cdate": 1699483189592,
            "tmdate": 1699636530604,
            "mdate": 1699636530604,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1JwTLsJzYi",
                "forum": "HT2dAhh4uV",
                "replyto": "gOoTO8qUec",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5301/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5301/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to Reviewer Cp4U (1/3)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the positive comments. Below we respond to the individual questions.\n\n> **Q1.** \"The experiments are performed over synthetic data only. I recommend exploring scalability to real-world data too.\", \"Please provide compositional generation results including interesting interactions among the objects\".\n\n**A1.** We appreciate the valuable comments. First, we would like to kindly note that learning object-centric representation in complex real-world images is widely perceived as challenging, and existing works rely on frozen pre-trained encoders and decoders to alleviate the problem. Since freezing the image encoder with a shallow slot attention module limits the opportunity to investigate the true impact of the compositional objective, we follow the standard settings that employ the synthetic data for evaluation.\n\nNonetheless, we agree with the reviewer that the synthetic scenes are insufficient to demonstrate the scalability of our compositional objective to complex scenes. Following the reviewer\u2019s suggestion, we conducted preliminary experiments on the BDD100K dataset, which contains real-world driving scenes with many objects (cars, signs, buildings, trees, pedestrians, etc.) as well as complex interactions among objects and with the environment (occlusion, shadow, lighting, reflection, etc.). Following the prior work [1], we employed the pre-trained DINOv2 [2] as an image encoder and Stable Diffusion [3] as the generative prior.  We then apply warm-up training for the one-shot decoder for 140K steps with an auto-encoding objective, and apply our compositional objective for the next 100K steps. \n\nThe results are summarized in Section B.7 in the Appendix (Figures 10 and 11). Based on visualization of the slot attention map (Figure 10), we observe that our method captures composable object instances not only from the foreground such as cars, buses, and trucks, but also from static environments such as road signs, trees, buildings, sidewalks, etc. In contrast, learning with only diffusion loss often separates an object into multiple slots, or contains multiple objects into a single slot. When composing two images by mixing their slots (Figure 11), we observe that our method successfully generates realistic scenes, modeling complex correlations among objects and environments. It appropriately adapts the appearance of newly added/removed objects, their shadow, reflections in the front glass and hood, and sometimes even global illumination change caused by removing the sun. Our method also generates more accurate compositions compared to the baseline, which often fails to adhere to the addition or removal of the objects. \n\nTo summarize, our preliminary experiments showed promising results that the proposed compositional objective can scale to complex scenes and model complex correlations among objects. We believe that incorporating the compositional objective in self-supervised learning can greatly enhance object-centric learning in complex scenes through end-to-end training with the deep encoder, which we plan to investigate in future work.\n\n[1] Jiang et al., Object-centric slot diffusion, NeurIPS 2023.  \n\n[2] Oquab et al., DINOv2: Learning robust visual features without supervision, arXiv preprint 2023.  \n\n[3] Rombach et al., High resolution image synthesis with latent diffusion models, CVPR 2022.  \n\n---\n> **Q2.** It would be good to cite and discuss the \u201cObject Discovery with a Copy-Pasting GAN\u201d paper as a related prior work that also uses mixes over image to encourage object discovery.\n\n**A2.** We appreciate the valuable comment on the related work. We agree with the reviewers that Copy-Pasting GAN (CP-GAN) shares similar motivation to ours in a spirit of discovering object masks by maximizing the realism of a copy-and-pasted image. Here we outline two major differences as follows. Firstly, our work focuses on learning compositional representation, which can be further employed in downstream tasks or compositional generation tasks, while the CP-GAN focuses on discovering object masks. Secondly, CP-GAN simulates the composition of objects by a simple copy-and-paste on RGB space, while our method combines two scenes using the object (slot) representation and employ powerful diffusion decoder to render the composite scene. We will include these discussions in the paper."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5301/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700684544457,
                "cdate": 1700684544457,
                "tmdate": 1700685923416,
                "mdate": 1700685923416,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zYbOeOwpk1",
                "forum": "HT2dAhh4uV",
                "replyto": "gOoTO8qUec",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5301/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5301/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to Reviewer Cp4U (2/3)"
                    },
                    "comment": {
                        "value": "> **Q3.** It would be good to also compare to a vanilla slot attention as a baseline.\n\n**A3.** We appreciate the comment.  As suggested by the reviewer, we additionally conducted experiments with vanilla slot attention. The results are summarized in the tables below. Overall, the performance of the vanilla slot attention is considerably lower than the other baselines and ours except in the PTR dataset where it achieves slightly better mIoU and mBO than ours. We observe that it is because the slot attention tends to capture tight object boundaries while missing some object parts (*e.g.*, legs of tables), which is favored in mIoU and mBO but results in much lower FG-ARI scores. In all other datasets (CLEVRTex, MultiShapeNet, and Super-CLEVR), however, the vanilla slot attention fails to capture meaningful object masks, often suffering from position bias or leveraging simple color cues due to weak pixel mixture decoder as commonly reported in the literature. We appreciate the comment and will add the results to the main paper. \n\n|        CLEVRTex      | FG-ARI (\u2191) | mIoU (\u2191) | mBO (\u2191) |\n|------------------------|:----------:|:--------:|:-------:|\n| Vanilla Slot Attention |    34.75   |   13.63  |  14.69  |\n|         SLATE+         |    71.29   |   52.04  |  52.17  |\n|           LSD          |    76.44   |   72.32  |  72.44  |\n|          Ours          |    **93.06**   |  **74.82**  |  **75.36**  |\n\n\n|   MultiShapeNet   | FG-ARI (\u2191) |  mIoU (\u2191) |  mBO (\u2191) |\n|------------------------|:----------:|:---------:|:--------:|\n| Vanilla Slot Attention |    69.26   |   16.49   |   16.6   |\n|         SLATE+         |    70.44   |   15.55   |   15.64  |\n|           LSD          |    67.72   |   15.39   |   15.46  |\n|          Ours          |  **89.8**  | **59.21** | **59.4** |\n\n\n|           PTR           | FG-ARI (\u2191) |  mIoU (\u2191) |  mBO (\u2191)  |\n|------------------------|:----------:|:---------:|:---------:|\n| Vanilla Slot Attention |    56.8    | **50.64** | **51.02** |\n|         SLATE+         |  **91.25** |    14.1   |   14.22   |\n|           LSD          |    61.1    |   10.18   |   10.33   |\n|          Ours          |    90.65   |   40.89   |   41.45   |\n\n\n|    Super-CLEVR    | FG-ARI (\u2191) |  mIoU (\u2191) |  mBO (\u2191)  |\n|------------------------|:----------:|:---------:|:---------:|\n| Vanilla Slot Attention |    26.92   |   13.63   |   14.69   |\n|         SLATE+         |    43.73   |   29.12   |   29.49   |\n|           LSD          |    54.79   |   14.12   |   14.43   |\n|          Ours          |  **63.08** | **47.17** | **48.03** |  \n---\n> **Q4.** Please elaborate more on why sharing slot initialization helps avoiding bad mixing of objects?\n\n**A4.** We appreciate the comment. In the random mixing strategy, each slot is randomly and independently initialized in each image, and combined into a composite representation by randomly sampling slots from two images. Due to this randomness, there is a chance that mixed slots (*i.e.*, composite representation) lead to ***invalid*** combinations, such as the ones composed only with the objects without background. To avoid this, we share the slot initialization between two images, and mix the slots ***exclusively*** according to their slot indices (*e.g.*, we can sample i-th slot either from the first or second images, but NOT from both). Formally, let $I_1$ and $I_2$ be a random partition of slot indices *i.e.*, $I_1\\cup I_2=\\\\{ 1,...,N\\\\}, I_1\\cap I_2=\\\\emptyset$. Then we construct the composite slot by $\\\\mathbf{S}^c=\\\\mathbf{S}^1_{I_1}\\\\cup \\\\mathbf{S}^2_{I_2}$, where $\\\\mathbf{S}^1$ and $\\\\mathbf{S}^2$ are slots extracted from the first and the second images, respectively. Since the slots are initialized identically between two images and the slots extracted from the identical initialization never appear twice in the composite representation, the model can learn to associate certain slot initialization to a specific part of a scene, such that the composite representation is ensured to be always valid. Indeed, we observed that the model learns to associate the background to a certain slot, as shown in Figure 6 in the appendix. We appreciate the comment and will revise the paper to clarify this point."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5301/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700685473260,
                "cdate": 1700685473260,
                "tmdate": 1700685473260,
                "mdate": 1700685473260,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3WmBi7qeHS",
                "forum": "HT2dAhh4uV",
                "replyto": "gOoTO8qUec",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5301/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5301/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to Reviewer Cp4U (3/3)"
                    },
                    "comment": {
                        "value": "> **Q5.** Please clarify how the denoising diffusion and auto-encoding fit together in the proposed framework.\n \n**A5.** Our diffusion decoder is trained with auto-encoding loss (Eq.(4) in the paper) while also used to compute the composition loss (Eq.(6)). In the auto-encoding path, the diffusion decoder learns the marginal distribution of data, while in the composition path, it is used to evaluate the likelihood of the composite image. We apply the stop gradient to the diffusion decoder in the composition path to ensure that it learns with only the true data distribution through the auto-encoding path. We will clarify the paper to make it more clear.  \n\n--- \n> **Q6.** Why does the ARI performance on Super-CLEVR drop a lot compared to other datasets? Please provide an intuition on this. \n\n**A6.** The ground-truth object masks of the Super-CLEVR dataset are much more fine-grained than the other datasets. Since the resolution of the attention map in our method and baselines is much coarser than the mask, we observe that it generally degrades the FG-ARI score for all baselines (*e.g.*, a small error in a low-resolution attention map can lead to a bigger penalty), especially when the objects are occluded by the others (*e.g.*, occluded by the spoke of a bike)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5301/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700685981669,
                "cdate": 1700685981669,
                "tmdate": 1700685981669,
                "mdate": 1700685981669,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]