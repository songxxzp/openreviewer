[
    {
        "title": "Prediction Risk and Estimation Risk of the Ridgeless Least Squares Estimator under General Assumptions on Regression Errors"
    },
    {
        "review": {
            "id": "3C6gLgxPuZ",
            "forum": "7ArYyAmDGQ",
            "replyto": "7ArYyAmDGQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4383/Reviewer_zjX2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4383/Reviewer_zjX2"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the prediction risk and estimation risk of the ridgeless least squares estimator. The main contribution is that the i.i.d. assumption is dropped in the theoretical analysis. The critical assumption is left-spherical symmetry for the distribution of the design matrix. Under those assumptions, the authors derived an accurate formula for the prediction error for ridgeless LSE with the high dimensional model and finite data set. Some numerical experiments show that the numerical results agree with the theoretical findings."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The rigorous evaluation of the prediction and estimation errors is presented for the high-dimensional model using finite samples.\n- The authors introduced the left-spherical symmetry as a critical assumption in the theoretical analysis."
                },
                "weaknesses": {
                    "value": "- The non-i.i.d. noise seems a minor extension of existing works.\n\n- The left-spherical symmetry is an interesting assumption to analyze the ridgeless LSE, the relationship between the left-spherical symmetry and the double descent phenomenon is not sufficiently investigated. Surely, the left-spherical symmetry is useful to derive the explicit expression of the risk. However, the interpretation or meaning of the assumption of the double descent is not sufficiently elucidated.\n\n- In numerical experiments, only the models that agree to the assumption for Theorem 3.2, and 3.3 are used. The readers may be interested in how much the theoretical analysis matches numerical experiments. In other words, the authors could investigate how robust is the theoretical findings to the violation of the assumption."
                },
                "questions": {
                    "value": "- Please make clear the technical difficulty of dealing with the non-i.i.d. noise assumption. \n\n- The left-spherical symmetry is an interesting assumption to analyze the ridgeless LSE; the relationship between the left-spherical symmetry and the double descent phenomenon is not sufficiently investigated. Surely, the left-spherical symmetry is useful to derive the explicit expression of the risk. However, the interpretation or meaning of the assumption of the double descent is not sufficiently elucidated. Is it possible to provide a more detailed description of the relationship between the left-spherical symmetry and the double descent phenomenon? \n\n- In numerical experiments, only the models that agree to the assumptions for Theorem 3.2 and 3,3 are used. The readers may be interested in how much the theoretical analysis matches numerical experiments. In other words, the authors could investigate how robust the theoretical findings are to violating the assumption. Is it possible to add numerical experiments for checking the robustness of the theoretical findings to the violation of the assumption?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4383/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698135430870,
            "cdate": 1698135430870,
            "tmdate": 1699636411269,
            "mdate": 1699636411269,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bgwVENE3cS",
                "forum": "7ArYyAmDGQ",
                "replyto": "3C6gLgxPuZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4383/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4383/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewers for their time and valuable feedback. During the discussion period, we hope to hear more questions or comments from the reviewers for further discussion to strengthen the paper.\n\n> **the technical difficulty of dealing with the non-i.i.d. noise assumption and the role of the left-spherical symmetry assumption**\n\nWe would like to emphasize our contributions that the non-i.i.d. noise assumption itself cannot explain the double descent phenomenon and the left-spherical symmetry (LSS) plays an important role for factoring out $\\mathrm{Tr}((X^\\top X)^\\dagger\\Sigma)$ term from the expected variance in the non-i.i.d. noise setting. This trace term explains the double descent phenomenon.\n- To further elaborate this, we rewrite the expected variance as $\\mathbb{E}\\_X[\\mathrm{Var}_\\Sigma(\\hat\\beta\\mid X)]=c^\\top b$,        where $a=\\lambda((X^\\top X)^\\dagger \\Sigma), b=\\lambda(\\Omega)$, and $c=\\mathbb{E}_X[\\Gamma^\\top a]$.\n- For the i.i.d. noise with $\\Omega=\\sigma^2 I$ (i.e., $\\mathbb{E}[\\varepsilon_i\\varepsilon_j]=\\sigma^21_{i=j}$), the vector $b=\\sigma^2 \\mathbf{1}$ is parallel to $\\mathbf{1}$; and thus we have $c^\\top b= (c^\\top \\mathbf{1})\\sigma^2$. Here, we have $c^\\top \\mathbf{1}=\\mathbb{E}\\_X[a^\\top \\Gamma \\mathbf{1}]=\\mathbb{E}\\_X[a^\\top \\mathbf{1}]=\\mathbb{E}\\_X[\\mathrm{Tr}((X^\\top X)^\\dagger \\Sigma)]$ since $\\Gamma \\mathbf{1}=\\mathbf{1}$ ($\\Gamma$ is a doubly stochastic matrix). \n- For the non-i.i.d. noise setting (with a general $\\Omega$), the vector $b$ is not (necessarily) parallel to $\\mathbf{1}$, but with the LSS assumption, $c$ is now parallel to $\\mathbf{1}$; and thus we can similarly factorize the expected variance as $c^\\top b=\\bar c (1^\\top b)=\\frac1n\\mathbb{E}\\_X[\\mathrm{Tr}((X^\\top X)^\\dagger \\Sigma)] \\mathrm{Tr}(\\Omega)$ for any positive definite $\\Omega$ where $c=\\bar c\\mathbf{1}$ and $\\bar c=\\mathbb{E}\\_X[\\frac1n \\sum\\_i a\\_i]=\\frac1n \\mathbb{E}\\_X[\\mathrm{Tr}((X^\\top X)^\\dagger \\Sigma)]$.\n- To achieve the factorization $c^\\top b=\\frac1n\\mathbb{E}\\_X[\\mathrm{Tr}((X^\\top X)^\\dagger \\Sigma)] \\mathrm{Tr}(\\Omega)$ for any positive definite $\\Omega$ (for any $b$ with $b_i>0$), it is **necessary for $c$ to be parallel to $\\mathbf{1}$**. And this may not be achieved without the LSS  assumption in the non-i.i.d. noise setting.\n\n> **the left-spherical symmetry and the double descent phenomenon**\n\nThe left-spherical symmetry makes each eigenvalue of $(X^\\top X)^\\dagger\\Sigma$ have an **equal influence** on the risk.\n\n- The double descent phenomenon can be explained by the trace term $\\mathrm{Tr}((X^\\top X)^\\dagger\\Sigma)$ (especially by large eigenvalues) from the expected variance under the left-spherical symmetry (LSS) assumption.\n- This is because when $p\\approx n$ there are many small eigenvalues of $X^\\top X$ near 0 and they highly increase $\\mathrm{Tr}((X^\\top X)^\\dagger\\Sigma)$ and the expected variance, but in the overparameterized regime $p\\gg n$, the eigenvalues of $X^\\top X$ are distant from 0, and thus the variance becomes much smaller.\n- Under the LSS assumption, the expected variance is factorized as $\\frac1n\\mathbb{E}\\_X[\\mathrm{Tr}((X^\\top X)^\\dagger \\Sigma)] \\mathrm{Tr}(\\Omega)$ where each eigenvalue of $(X^\\top X)^\\dagger \\Sigma$ is weighted with the same $\\mathrm{Tr}(\\Omega)$. \n- However, this is not the case without the LSS assumption. In other words, without the LSS assumption, the small eigenvalues of $X^\\top X$ near 0 may not play the similar role as they do under the LSS assumption."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4383/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699974774961,
                "cdate": 1699974774961,
                "tmdate": 1699974774961,
                "mdate": 1699974774961,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1Q2xOvMLLz",
            "forum": "7ArYyAmDGQ",
            "replyto": "7ArYyAmDGQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4383/Reviewer_jNnY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4383/Reviewer_jNnY"
            ],
            "content": {
                "summary": {
                    "value": "This paper delves into the assessment of prediction risk and estimation risk, expanding the scope to accommodate more general regression error assumptions. It underscores the advantages of overparameterization in the context of finite samples, revealing that the inclusion of a substantial number of seemingly inconsequential parameters relative to the sample size can effectively mitigate both types of risk.\n\nDespite the paper's technical nature and the wealth of analytical content, some aspects warrant further attention. Notably, main results such as the ones presented in Theorems 3.2, 3.3, and Corollary 4.1 lack comprehensive elucidation, leaving it unclear how these outcomes relate to the core assertion regarding the benefits of overparameterization or unimportant parameters. Moreover, there are instances of imprecise writing, such as the reference issue in Section 4.2, where \"we can obtain a similar result with 4.1\" appears to be a misreference.\n\n\n====\n\nI acknowledge that I have considered the authors' response, yet after careful deliberation, I have chosen to maintain the current score."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper seems to be technically sound."
                },
                "weaknesses": {
                    "value": "main results such as the ones presented in Theorems 3.2, 3.3, and Corollary 4.1 lack comprehensive elucidation, leaving it unclear how these outcomes relate to the core assertion regarding the benefits of overparameterization or unimportant parameters."
                },
                "questions": {
                    "value": "See \"weakness\"."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4383/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4383/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4383/Reviewer_jNnY"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4383/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698785733496,
            "cdate": 1698785733496,
            "tmdate": 1700719380840,
            "mdate": 1700719380840,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YvuYqPlnrv",
                "forum": "7ArYyAmDGQ",
                "replyto": "1Q2xOvMLLz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4383/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4383/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "First of all, thank you for pointing out a typo in \"we can obtain a similar result with 4.1\". \nWe will replace the problematic first sentence in Section 4.2 with the following:\n\"For the bias component of prediction risk, we can attain a result comparable to those presented in Section 4.1 as follows:''\n\nWe now move to your main concern, namely lack of comprehensive elucidation.\nIn Theorem 3.1, we factorize the variance component of $\\mathbb{E}\\_X[\\mathrm{Var}\\_\\Sigma(\\hat\\beta\\mid X)]$ into the product of the two terms:\nspecifically,\n    \\begin{align*}\n        \\mathbb{E}\\_X[\\mathrm{Var}_\\Sigma(\\hat\\beta\\mid X)]\n        &=\\frac{1}{n} \\mathrm{Tr}(\\Omega)\\mathbb{E}\\_X[\\mathrm{Tr}((X^\\top X)^{\\dagger}\\Sigma)],\n    \\end{align*}\nwhere on the one hand, the first term $\\frac{1}{n} \\mathrm{Tr}(\\Omega)$ remains bounded even if both $n$ and $p$ are very large and \non the other hand, the second term $\\mathbb{E}\\_X[\\mathrm{Tr}((X^\\top X)^{\\dagger}\\Sigma)]$ tends to be smaller if $p/n$ gets larger. \nIn other words, we isolate the effect of non-spherical $\\Omega$ via $\\mathrm{Tr}(\\Omega)$.\nWe obtain an even cleaner result for estimation risk in Theorem 3.2, where we have that\n    \\begin{align*}\n        \\mathbb{E}\\_X[\\mathrm{Var}(\\hat\\beta\\mid X)]\n        &=\\frac{1}{np}\\mathrm{Tr}(\\Omega)\\mathbb{E}\\_X[\\mathrm{Tr}(\\Lambda^{\\dagger})],\n    \\end{align*}\n   where \n    $XX^\\top/p= U\\Lambda U^\\top$ for some orthogonal matrix $U\\in O(n)$.  \nIt can be easily seen that $\\mathbb{E}\\_X[\\mathrm{Tr}(\\Lambda^{\\dagger})]$ is of order $n$, thus implying that the variance component of estimation risk\nwill become smaller as $p/n$ gets larger. \nWe would like to emphasize that both results are non-trivial and Assumption 2 (that is, left-spherical design matrix $X$) plays a key role in proving them.\nIn summary, Theorems 3.2 and 3.3 imply that the variance components of both risks will be smaller if $p/n$ is larger, thereby preserving the essential feature of benign overfitting.\nFurthermore, we need to analyze the bias components to fully characterize benign overfitting. This task is done in Corollaries 4.1 and 4.2.\nNotice that Assumptions 4.3 and 4.4 restrict that both weighted and unweighted $L_2$ norms of $\\beta$ are bounded. As we focus on the case that $p$ is large, the $L_2$ boundedness of $\\beta$ requires that a large number of elements of $\\beta$ be ``close to zero'' in some sense (unimportant parameters as we describe in the abstract). We hope that our explanations clarify the contributions of our main theoretical results and intend to implement appropriate revisions in the paper to enhance its overall exposition."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4383/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700119298573,
                "cdate": 1700119298573,
                "tmdate": 1700119298573,
                "mdate": 1700119298573,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "95lRdMZNko",
            "forum": "7ArYyAmDGQ",
            "replyto": "7ArYyAmDGQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4383/Reviewer_xuqE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4383/Reviewer_xuqE"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates the prediction risk and the estimation risk of ridgeless least squares estimator in an overparametrized regime where the number of samples $n$ is less than the number of variables $p$. The main interest of this work is that it addresses non i.i.d. regression errors. Notably the expected value of the estimator variance at finite $n<p$ is found to depend on the sum of the variances of the regression errors, ignoring the correlations between regression errors. As the bias of the estimator is independent of the regression errors, the prediction risk and the estimation risk exhibit the same behavior."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The article is well motivated. Removing the i.i.d. condition on the regression errors is indeed interesting for studying data such as time series.\n\n- The presentation is sufficiently clear although the nature of the theoretical findings could be better explained (see Weaknesses).\n\n- The theoretical results are confirmed by experiments."
                },
                "weaknesses": {
                    "value": "- It seems that the main theorems do not give direct access to the relations between the deterministic parameters underlying the data generating process and the learning risks, except in the asymptotic regime of $n,p\\to\\infty$. If that is the case, the nature of the contributions should be made clearer to stress that point.\n\n- It appears that while allowing dependences between regression errors, the proposed analysis in the ridgeless overparametrized setting shows that the performance stays the same whether or not the regression errors are independent,  as long as the sum of their variances is unchanged. The limitations of this work and the possible extensions should be better discussed in that regard. For instance, what would be the main technical difficulties to extend the analysis to ridge regularization and underparametrized regime, and would the dependences between regression errors have an impact on the learning performance in those settings?"
                },
                "questions": {
                    "value": "See Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4383/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4383/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4383/Reviewer_xuqE"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4383/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698944987825,
            "cdate": 1698944987825,
            "tmdate": 1699636410976,
            "mdate": 1699636410976,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qoBFkrxEfO",
                "forum": "7ArYyAmDGQ",
                "replyto": "95lRdMZNko",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4383/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4383/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewers for their time and valuable feedback. During the discussion period, we hope to hear more questions or comments from the reviewers for further discussion to strengthen the paper.\n\n> It seems that the main theorems do not give direct access to the relations between the deterministic parameters underlying the data generating process and the learning risks, except in the asymptotic regime of $n,p\\rightarrow \\infty$. If that is the case, the nature of the contributions should be made clearer to stress that point.\n\nYour comment that ``the main theorems do not give direct access to the relations between the deterministic parameters underlying the data generating process and the learning risks'' seems to indicate that the $\\beta$ parameter is random in our setting. Please correct us if there are any misunderstandings.\nThe assumption of the $\\beta$ parameter is widely adopted in the literature. \nFor example, see \nDobriban and Wager (2018, Annals of Statistics); \nRichards, Mourtada, and Rosasco (2021, AISTATS);\nLi, Xie, and Wang (2021, ICML);\nChen, Zeng, Yang, and Sun (2023, ICML) among others.\n\nThe random $\\beta$ parameter assumption facilitates a clear and concise exposition of the bias components of prediction and estimation risks. Importantly, our main results are concerned with the variance components, which do not require the random $\\beta$ assumption at all.\nWe will mention in the revised version of the paper that it is a topic for future research to consider the fixed $\\beta$ parameter. This would require more careful analysis because where the intricate interplay among $\\beta, \\Sigma$ and $\\Omega$ has to be dealt with."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4383/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700020933463,
                "cdate": 1700020933463,
                "tmdate": 1700020933463,
                "mdate": 1700020933463,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Dqjap6OLhk",
                "forum": "7ArYyAmDGQ",
                "replyto": "95lRdMZNko",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4383/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4383/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the comment. We will revise our manuscript with the following discussions.\n\n> **Our Focus on Overparameterized model and More Discussions on the Ridge Regularization**\n\n- One of our main goal is to understand the ability of deep neural networks and how overparameterization plays a role in the \"double descent'' or \"benign overfitting'' phenomena. This is why we focus on the overparameterized regime. There is not so much difficulties to extend our analysis to the underparameterized regime. The expected variance result is symmetric with respect to the line $\\gamma=1$ (Fig 3, \"logarithmic\" horizontal axis), i.e., the curve for $\\gamma'=n/p>1$ is the same as the curve for $\\gamma=p/n>1$. Anyway, the underparameterized regime is just not our focus.\n\n- On the other hand, it is not trivial to extend our analysis to the ridge regularization, but we can obtain a similar result with a little modification of the proof (Theorem 3.2). We summarize the modifications as follows:\n    - For the ridge regression, we have $\\hat\\beta_\\mu =(X^\\top X+\\mu I)^{-1}X^\\top y = A_\\mu y$ where $A_\\mu = (X^\\top X+\\mu I)^{-1}X^\\top$.\n    - Thus, we have the expected variance $\\mathrm{Var}\\_\\Sigma(\\hat\\beta\\_\\mu\\mid X)=\\mathrm{Tr}(A_\\mu\\Omega A_\\mu^\\top \\Sigma)=||SA_\\mu T||\\_F^2$ and $a_\\mu(X)=\\lambda(A_\\mu A_\\mu^\\top \\Sigma)$. \n    - Here, $a_\\mu(OX)=a_\\mu(X)$ still holds since $A_\\mu A_\\mu^\\top = (X^\\top X+\\mu I)^{-1}X^\\top X(X^\\top X+\\mu I)^{-1}$ is the same for $X$ and $OX$.\n    - Using the notation $A_\\mu(X)=(X^\\top X+\\mu I)^{-1} X^\\top $, we have $SA_\\mu(OX)=SA_\\mu(X)O^\\top$, and thus $\\Gamma(OX)_{ij}=\\langle Ov^{(i)}, u^{(j)}\\rangle^2$.\n    - We can conclude that \n$\\mathbb{E}\\_X[\\mathrm{Var}\\_\\Sigma(\\hat\\beta_\\mu\\mid X)]=\\mathbb{E}\\_X[a_\\mu(X)^\\top \\frac1n Jb]=\\frac1n \\sum_{i,j}\\mathbb{E}\\_X[(a\\_\\mu(X))_i]b_j=\\frac1n \\mathbb{E}\\_X[\\mathrm{Tr}(A\\_\\mu A\\_\\mu^\\top \\Sigma)]\\mathrm{Tr}(\\Omega)$"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4383/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700141118760,
                "cdate": 1700141118760,
                "tmdate": 1700141118760,
                "mdate": 1700141118760,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rqHc1x3why",
                "forum": "7ArYyAmDGQ",
                "replyto": "qoBFkrxEfO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4383/Reviewer_xuqE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4383/Reviewer_xuqE"
                ],
                "content": {
                    "title": {
                        "value": "Reply"
                    },
                    "comment": {
                        "value": "Actually here I meant to say that due to the presence of the data matrix $X$, the proposed expressions in Theorem 3.2 and Theorem 3.3 do not allow to retrieve the learning risks solely from $\\Sigma, \\Omega, n, p$, which are the parameters underlying the data generating process."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4383/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700695553948,
                "cdate": 1700695553948,
                "tmdate": 1700695553948,
                "mdate": 1700695553948,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "W29evoKCIn",
                "forum": "7ArYyAmDGQ",
                "replyto": "Dqjap6OLhk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4383/Reviewer_xuqE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4383/Reviewer_xuqE"
                ],
                "content": {
                    "title": {
                        "value": "Reply"
                    },
                    "comment": {
                        "value": "The double descent curve occurs when we go from the overparametrized regime to the underparametrized regime. If there is absolutely no additional technical difficulty, I do not really see why the results in the overparametrized regime should not be included to provide a better overview of the double descent curve and the benign overfitting phenomenon. \n\nAccording to the new results for the ridge regularization given by the authors, the expected variance is still the same for independent and dependent errors, as long as the sum of their variances is unchanged. The interest of considering dependent errors is thus still not reflected in the obtained results."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4383/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700696522936,
                "cdate": 1700696522936,
                "tmdate": 1700696522936,
                "mdate": 1700696522936,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]