[
    {
        "title": "Sub-token ViT Embedding via Stochastic Resonance Transformers"
    },
    {
        "review": {
            "id": "2pzHJwJ745",
            "forum": "etGEIggjWS",
            "replyto": "etGEIggjWS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1011/Reviewer_3VWw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1011/Reviewer_3VWw"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a \"Stochastic Resonance Transformer\" (SRT) method that improves the performance of pre-trained Vision Transformers (ViTs) in downstream tasks. SRT achieves this by applying controlled perturbations to input images (i.e., sub-token spatial translations) and super-resolve features of pre-trained ViTs, capturing more of the local fine-grained structures that might be neglected by tokenization."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Clarity of Presentation: The paper is well-written and easily understandable. It effectively conveys the proposed approach and its rationale.\nSimplicity and Effectiveness: The simplicity of SRT is a notable strength. Despite its simplicity, it has demonstrated high effectiveness in five vision tasks, which is a valuable contribution to the field.\nGeneralization Ability: SRT can be applied at any layer and on any task without fine-tuning."
                },
                "weaknesses": {
                    "value": "The theoretical guarantee is missing"
                },
                "questions": {
                    "value": "1. Can SRT be applied to models beyond Transformers? It would be of interest to see empirical results exploring its applicability to other models such as ResNets or MLPs. \n2. It is important to understand the hyper-parameters involved and perform in-depth analysis when noise level can be effective or harmful. Since stochastic resonance may not be suitable for all cases, how to determine the optimal noise levels in practical applications?\n3. It would be helpful to analyze the noise distribution and perform experiments using other augmentation strategies.\n4. Why not conduct experiments on the main task in computer vision such as image classification, semantic segmentation, and object detection?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1011/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697597497464,
            "cdate": 1697597497464,
            "tmdate": 1699636027033,
            "mdate": 1699636027033,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "t8r81HTmY1",
                "forum": "etGEIggjWS",
                "replyto": "2pzHJwJ745",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1011/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1011/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response to the initial review"
                    },
                    "comment": {
                        "value": "**Q1:** *Can SRT be applied to models beyond Transformers? It would be of interest to see empirical results exploring its applicability to other models such as ResNets or MLPs.*\n\n**R1:** As stated in the paper, 'It is worth noting that Stochastic Resonance is not limited to ViT architectures nor to spatial quantization. It can be applied to architectures like CNN as well as to other forms of quantization, such as sale or domain size. However, our emphasis in this paper is on ViTs that mostly use non-overlapping tokens, making them particularly suited to our approach.' (Section 4.2)\n\nAs suggested by the reviewer, we applied SRT to ResNet on classification problem on Cifar, the results are in the Appendix A, where we improve classification accuracy by an average of 5.87%.\n\n**Supervised classification on Cifar-10**\nArchitecture | ResNet20 | ResNet32 | ResNet56\n| ----------- | ----------- | ----------- | ----------- |\n| Accuracy | 91.95 | 92.68 | 93.50 |\n| Accuracy w/ SRT | **92.41** | **93.14** | **93.87** |\n| Relative Error Reduced | 5.6% | 6.3% | 5.7% |\n \n**Q2:** *It is important to understand the hyper-parameters involved and perform in-depth analysis when noise level can be effective or harmful. Since stochastic resonance may not be suitable for all cases, how to determine the optimal noise levels in practical applications?*\n\n**R2:** That's a great question, and admittedly an ongoing effort to study how to best determine them outside of empirical trials. In our experiments, we noticed that using an augmentation level close to one-fourth of the patch size works effectively for various tasks. We look forward to future research shedding light on this question.\n\n**Q3:** *It would be helpful to analyze the noise distribution and perform experiments using other augmentation strategies.*\n\n**R3:** We appreciate the reviewer's input but may need further clarification on the term 'noise distribution' that the reviewer is referring to. If the reference is to artificial noise added as augmentation for stochastic resonance, then indeed, various options are available, as long as they are group transformations which guarantees an inverse transformation. However, we specifically chose translation, as it avoids introducing interpolation artifacts, unlike rotation, and provides finer control over the ensemble (pixel displacement in rotation depends on its distance from the center). Importantly, for transformers with positional encodings, flipping is not a viable option since the embeddings are not designed to be invariant.\n\nIf the term 'noise' pertains to stochastic noise in the features, assuming the ensembled features represent a \"denoised\" signal, we measured it by the Euclidean distance between the ensembled feature and the feature from a single forward pass, aggregated as a histogram. We show this noise distribution in Fig. 4 in the revised manuscript's Appendix.\n\n**Q4:** *Why not conduct experiments on the main task in computer vision such as image classification, semantic segmentation, and object detection?*\n\n**R4:** We focus on zero-shot tasks to highlight the capabilities of SRT without any fine-tuning. However, as explained in the paper, SRT can be used for any layer in any Vision Transformer (VIT). In response to the suggestion, in addition to the classification results presented in Q1, we present results for semantic segmentation in the Appendix B. SRT consistently improves mIoU on all three pre-trained ViTs, by as much as 1.7% in relative improvement. A discussion is provided in the Appendix:\n\n**Semantic segmentation by DinoV2 with linear head on ADE20K**\nArchitecture | baseline | d=1 | d=2 | d=3\n| ----------- | ----------- | ----------- | ----------- | ----------- |\n| ViT-S/14 | 44.24 | 44.44 | 44.57 | **44.64** |\n| ViT-B/14 | 47.28 | 47.63 | 47.85 | **47.98** |\n| ViT-L/14 | 47.49 | 48/18 | 48.44 | **48.62** |"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1011/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700100403151,
                "cdate": 1700100403151,
                "tmdate": 1700110700092,
                "mdate": 1700110700092,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "J1kiiQQWvV",
                "forum": "etGEIggjWS",
                "replyto": "t8r81HTmY1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1011/Reviewer_3VWw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1011/Reviewer_3VWw"
                ],
                "content": {
                    "title": {
                        "value": "Reply to the authors"
                    },
                    "comment": {
                        "value": "By summaring the comments from other reveiwers and the reply from the authors, we have the following additional question.\n1. The problem statement regarding the exact nature of these artifacts lacks clarity in its definition. Please provide an in-depth characterization and analysis of the artifacts present in feature maps. For instance, it is crucial to determine which specific layers of the transformer these artifacts typically appear in, at which training stage they tend to manifest, and whether there is any correlation between the occurrence of artifacts and the size of the model.\n2. There have been other works [R1] developed for improving artifacts in the feature extraction process with pretrained models. Please analyze the relevance and differences between these works and the present study.\n[R1] Darcet T, Oquab M, Mairal J, et al. Vision Transformers Need Registers[J]. arXiv preprint arXiv:2309.16588, 2023."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1011/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700727487017,
                "cdate": 1700727487017,
                "tmdate": 1700727487017,
                "mdate": 1700727487017,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gzM4k589tn",
                "forum": "etGEIggjWS",
                "replyto": "2pzHJwJ745",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1011/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1011/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Nature of the artifacts, additional related work"
                    },
                    "comment": {
                        "value": "Thank you for bringing up the questions.\n\n**1.** To delve further into the 'exact nature,' we have included an additional theoretical analysis in Appendix E, please also see our response to Reviewer wDbQ. The artifact we're tackling is related to tokenization, so it should be present in every layer of ViT. However, as the features go deeper, they contain more high-level information and the artifact potentially affects the downstream task more significantly. We are delighted to add extra visualizations for the feature map at different layers in Figure 7 to bring more insights to the question.\n\n**2.** Reference [R1] is a concurrent ICLR submission (can be found by paper title search), and its Arxiv version was released after the paper submission deadline, so we could not discuss it in our initial manuscript. However, we're happy to offer additional insights:\n\n[R1] describes a phenomenon that when trained on specific tasks (such as contrastive, visual-language, etc.), some tokens in the \"image background\", considered \"uninformative\" for the pre-training task, get repurposed, possibly incurring decreased performance on dense prediction tasks. The proposed solution involves adding extra tokens to compensate for the repurposed tokens, ensuring that local ViT tokens represent local features of the image.\n\nAlthough what our work proposes is an ensembling method, which differs from [R1], two papers are indeed relevant. Please see the upper left side of Figure 1 in our paper. When CLIP is applied to the bicycle image, there are purple 'blobs' in the background, possibly corresponding to the 'repurposed tokens' described by [R1]. Instead of adding tokens, SRT reduces this artifact through ensembling. In the ensembled feature map (labeled as 'ours'), these purple blobs are smoothed out and eliminated. [R1] therefore provides a potential explanation for why SRT shows a significant improvement in performance for dense prediction tasks like depth prediction. Thanks to the reviewer for bringing this paper up and we're open to incorporating a detailed discussion in our manuscript.\n\nWe hope this response answers the reviewer's questions. If there are further concerns, we are more than willing to provide additional explanations."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1011/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733761472,
                "cdate": 1700733761472,
                "tmdate": 1700734605016,
                "mdate": 1700734605016,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lw5M0cRB4d",
            "forum": "etGEIggjWS",
            "replyto": "etGEIggjWS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1011/Reviewer_j1cP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1011/Reviewer_j1cP"
            ],
            "content": {
                "summary": {
                    "value": "In this work, a sub-token VIT embedding based method is proposed to increase the resolution of the intermediate feature maps from VIT. The method is called Stochastic Resonance Transformer (SRT). A set of perturbations(only translations for now) are applied to the input image. These set of perturbed images are then fed into VIT to get a series of token embeddings. These features are then upsampled to higher resolution and aligned using the inverse of the applied perturbations. Statistical aggregation, including mean and median, along the perturbation dimension, produces fine-grained feature representations. This method is evaluated on multiple CV tasks like object segmentation, depth estimation and unsupervised saliency segmentation."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This work introduce an interesting finding that super-resolution on the token embedding can help improve the VIT's capability\n- The effectiveness of this method has been verified in multiple CV tasks."
                },
                "weaknesses": {
                    "value": "- Perturbations are translation only. Translations are one of the perturbations, and can be replaced by a simple convolution kernel. If translation based perturbation works, then it's possible that other complex perturbations like rotation should also work. Also, since the translation works, it seems it can be replaced by an convolution network, followed by some deconvolution kernels in the upsampling stage, with some conv layers to do the aggregation. Then it will become a learnable model instead of translations only. In this way the model can not only simulate the translation, but also other non-linear perturbations.\n- Did not compare with image super-resolution method. Image super resolution is an well-studied area, and it's an natural thought to try some of them and see if the generated super-resolution features can help to improve the down-stream tasks."
                },
                "questions": {
                    "value": "- Is it possible to try other image super resolution models and see how the performance looks like? Theoretically, if the simple resizing works, then other super resolution should work better. \n- Is it possible to use some convolution kernels to replace the predefined perturbation type?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1011/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698736043498,
            "cdate": 1698736043498,
            "tmdate": 1699636026941,
            "mdate": 1699636026941,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fuUsknuCaI",
                "forum": "etGEIggjWS",
                "replyto": "lw5M0cRB4d",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1011/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1011/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response to the initial review"
                    },
                    "comment": {
                        "value": "**W1:** *Perturbations are translation only. Translations are one of the perturbations, and can be replaced by a simple convolution kernel. If translation based perturbation works, then it's possible that other complex perturbations like rotation should also work. Also, since the translation works, it seems it can be replaced by an convolution network, followed by some deconvolution kernels in the upsampling stage, with some conv layers to do the aggregation. Then it will become a learnable model instead of translations only. In this way the model can not only simulate the translation, but also other non-linear perturbations.*\n\n**R1:** The reviewer is correct that we could replace the translated pillbox kernel we use (although we do not explicitly mention) with any other kernel undergoing any other group (invertible) transformation. Those would give rise to generalized forms of convolution such as Fourier-Mellin etc., which indeed could further improve our method if the sampling of the group is adapted to the signal. We simply use the most obvious choice (pillbox = constant) of kernel and of group (translation) for simplicity. \n\nMoreover, from an engineering perspective, employing translation as a group transformation offers several advantages:\nAssuming pixel displacements in the translation matrix, one can map every feature to discrete pixel locations, which avoids interpolation artifacts commonly associated with other transformations, i.e., rotation, scaling.\nTranslation does not change the size for objects in the image, resulting in more stable features than scaling.\nUnlike rotation, where pixel displacement varies based on distance from the center, pixel displacement of each patch can be directly controlled by specifying translation components in the matrix. \nWe appreciate the reviewer's valuable suggestions for future work. However, it's essential to note that in this study, translation performs well and is validated by improvement in five different tasks. \n\n\n**W2:** *Did not compare with image super-resolution method. Image super resolution is an well-studied area, and it's an natural thought to try some of them and see if the generated super-resolution features can help to improve the down-stream tasks.*\n\n\n**R2:** (Please also see the \"general message\".) We wish to emphasize that ours is an ensembling method, not a super-resolution method in the traditional sense: We capture multiple samples (feature maps) from the original signal (image), which span a sigma-algebra strictly containing that of a single sample, from which one cannot retrieve the information lost simply by post-processing. We could use an analogy with acoustic processing where super-resolution would be akin to noise reduction, whereas stochastic resonance is threshold reduction. There may be a misunderstanding in the term \u201csuper-resolution\u201d, in the sense that we do not increase the size, but enhance the detail. We will revise the text to make this clear.   \n\n\n**Q1:** *Is it possible to try other image super resolution models and see how the performance looks like? Theoretically, if the simple resizing works, then other super resolution should work better.*\n\n**R3:** Regarding super-resolution, please see the [general message]. \n\nIn practice, the use of image super-resolution on the features is not feasible because, unlike the standard 3 channels (RGB), low-resolution features typically have many more channels (e.g., 256 for ViT/S or 768 for ViT/B). Existing super-resolution methods are not designed to handle inputs with such a high number of channels. We acknowledge the reviewer's concern and conducted experiments by resizing, some results are highlighted below (for a full table, see Table 7 in the revised paper). Overall simple resizing is detrimental to the task. This may be because there is no spatial smoothness constraint that is imposed on ViT architecture or training process, so a simple resizing operation introduces out-of-distribution features to the network.\n\n**Feature interpolation on monocular depth estimation with NYU-V2 and DinoV2**\n\nArchitecture | Method | RMSE | RMSE_log | AbsRel | SqRel\n ----------- | ----------- | ----------- | ----------- | ----------- | ----------- \n| ViT-S/14 | baseline | **0.336** | **0.114** | **0.080** | **0.048** |\n| | interpolation | 0.573 | 0.178 | 0.146 | 0.125 |\n\n\n**Q4:** *Is it possible to use some convolution kernels to replace the predefined perturbation type?*\n\n**R3:** In theory, perturbations are not restricted, as long as they adhere to group transformations, and translation can indeed be defined using convolution. However, in practice, finding more effective kernels remains an open problem, requiring further research. This paper concentrates on zero-shot tasks, where no additional training is performed. Nevertheless, we acknowledge that learned augmentations (so long as they are invertible) could be an interesting idea for certain specific tasks."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1011/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700099255975,
                "cdate": 1700099255975,
                "tmdate": 1700605164451,
                "mdate": 1700605164451,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IDpGU2F2Oa",
                "forum": "etGEIggjWS",
                "replyto": "fuUsknuCaI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1011/Reviewer_j1cP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1011/Reviewer_j1cP"
                ],
                "content": {
                    "title": {
                        "value": "response to rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for the rebuttal. It's surprising to see the simple resizing is not working here. Questions\n- What's the interpolation method used here? The most common one is the bicubic one.\n- For model based super-resolution solution, it's still possible to apply to the feature maps here, we can either train a gray-scale image super-resolution model (what's used in medical image super-resolution), or do super-resolution with each feature map (copy to RGB channels)\nAt this time I tend to keep my original rating"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1011/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700639658414,
                "cdate": 1700639658414,
                "tmdate": 1700639658414,
                "mdate": 1700639658414,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aWCV3e6zen",
                "forum": "etGEIggjWS",
                "replyto": "lw5M0cRB4d",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1011/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1011/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Interpolation, super-resolution"
                    },
                    "comment": {
                        "value": "**Q:** *What's the interpolation method used here? The most common one is the bicubic one.*\n\n**R:** We are reporting results using bilinear interpolation. As suggested, bicubic interpolation also yields following similar inferior results as bilinear interpolation, as shown by the following table:\n\nArchitecture | Method | RMSE | RMSE_log | AbsRel | SqRel\n ----------- | ----------- | ----------- | ----------- | ----------- | ----------- \n| ViT-S/14 | baseline | **0.336** | **0.114** | **0.080** | **0.048** |\n| | Bilinear interpolation | 0.573 | 0.178 | 0.146 | 0.125 |\n| | Bicubic interpolation | 0.572 | 0.178 | 0.146 | 0.124 |\n\nPlease also see Fig. 4 in the updated Appendix for a detailed discussion.\n\n**Q:** *For model based super-resolution solution, it's still possible to apply to the feature maps here, we can either train a gray-scale image super-resolution model (what's used in medical image super-resolution), or do super-resolution with each feature map (copy to RGB channels) At this time I tend to keep my original rating*\n\n**R:** If the reviewer is suggesting, for a $[h,w,768]$ feature map, map each $h\\times w\\times 1$ channel to a gray-scale image, run through a super-resolution network, and output a fine-grained feature, e.g. $[h\\times 16, w\\times 16, 768]$, this is infeasible since:\nSuper-resolution (SR) networks are domain-specific, hence applying SR networks pre-trained on, for instance, RGB/gray-scale natural images, would generalize poorly to high-dimensional feature maps. Indeed, as the reviewer rightfully pointed out, a proper comparison would require training a super-resolution network for each dimension of the feature map. This is well beyond the scope of our paper and comparisons since our method is zero-shot and does not require any training. \n\nPlease also refer to *A general message to the reviewers and AC* for a detailed discussion about super-resolution. If there is related work that we are currently unaware of, we would appreciate it if the reviewer could point to the exact literature, so that we can compare/respond accordingly."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1011/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700679375313,
                "cdate": 1700679375313,
                "tmdate": 1700698617948,
                "mdate": 1700698617948,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3XvUsEQBsU",
            "forum": "etGEIggjWS",
            "replyto": "etGEIggjWS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1011/Reviewer_wDbQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1011/Reviewer_wDbQ"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents the problem of quantisation artefacts in ViT features as a byproduct of the standard tokenisation procedure of partitioning images into non-overlapping tokens. As a remedy, the authors propose resolving patch contributions by perturbing the input by stochastic translation of the input image and subsequent aggregation in the embedding space to extract sub-token resolution features. \n\nThe authors directly refer to their method as a form of stochastic resonance. In classical stochastic resonance, thresholded boundaries are softened through the addition of stochastic noise, and the idea the paper seems to propose is that discrete boundaries of the partitioning can be super-resolved via stochastic perturbation with translations to allow neighbouring tokens to share information when aggregated. The goal seems to be to preserve finer grained spatial details and hence reduce the impact of the general partitioning in classical ViT tokenisation. The approach is reminiscent of classical dithering. The paper also mentions that the approach could easily be extended to other perturbations or augmentations, and is not necessarily \n\nApplications of the method is demonstrated, but largely limited to post-hoc or few-shot modelling approaches where features are further processed for dense prediction tasks; including upscaled feature visualisations using PCA, experiments on video object segmentation, and monocular depth prediction. Two non-dense downstream tasks are also demonstrated. \n\nIn summary, this reviewer find certain parts of the paper interesting and novel, particularly as a ensembling distillation method. However the methodology is murky at points, and the presented problem statement and applications somewhat undersells what this reviewer consider to be the strong points of the method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The proposed method is intuitive and based on well-established principles in signal processing, while being remarkably simple to apply, similar in form to an augmentation and ensembling scheme.\n- The method is essentially post-hoc and architecture agnostic as it only requires super-resolving features extracted the final layer, which could then be leveraged in potential downstream tasks, notably dense prediction tasks which require higher resolution feature attributions.\n- Aside from the outlined method using translation for super-resolving features, the method itself could be formulated as general augmentation-ensembling method, and is touched upon in the paper. This further makes the method potentially interesting for self-supervision and fine tuning.\n- The authors outline a distillation scheme that improves performance in certain downstream tasks. This posits the method as similar to augmentation for an ensemble based fine tuning tasks, which seems apt for further investigation. This distillation process could potentially be used in improved fine tuning for general ViT modelling approaches.\n- The paper seems to want to discuss what this reviewer considers an important and often overlooked limitation inherent in the canonical ViT architecture, where uniform partitioning might not align well with the spatial semantic content in the image."
                },
                "weaknesses": {
                    "value": "- ~~The problem the paper seeks to tackle, while somewhat intuitively reasoned, seems insufficiently motivated. The quantisation artefacts are presented as a result of discrete partitioning into uniform square patches, but the exact nature of these artefacts as well as their effect on predictions are hardly discussed or touched upon. This makes the problem statement more ambiguous than necessary.~~ **Edit:** *The last revision addresses this with a formalisation of the context of SRT.*\n- While the upscaled feature visualisations show improvement on the low resolution visualisations of the base model, ~~simpler interpolation methods exist for this express purpose. It is also not clear if these new feature maps can be said to be faithful as interpretations of the importance of the features with regard to the predictions.~~ **Edit**: *The authors expand their experiments with alternative interpolation methods, however the main concern on how useful the visualisations are as attributions for model predictions is not clear. The authors revise the manuscript and does no longer claim this as one of the main contributions of the work.*\n- ~~While the authors mention the conceptual overlap with Amir et al. (2021), this connection is not touched on to any further extent in the paper, except for a discussion on computational complexity. It seems natural to contrast against the results in this work since, by the authors own admission, there is an inherent similarity between approaches.~~ *The authors expands the link in their revised work with additional results.*\n- Methodology for non-dense downstream tasks (particularly image retrieval) seems unclear, and hardly reproducible. Several questions remain unanswered for practitioners wanting to reproduce the results in the paper. ~~Additionally, there seems to be little to no mention of input resolutions used for the modelling.~~ **Edit:** *Note on resolution was addressed. While the authors seek to address gaps in methodology by releasing their code, more exposition in the manuscript would be ideal, but is further addressed in the last revision*.\n\nWe also detail some minor weaknesses:\n- ~~The link to stochastic resonance, while somewhat clear to this reviewer, is a little opaque. While this analogy could provide an intuitive understanding of the method, it is imperative to clarify the link and show the extent to which quantisation artefacts inhibit ViTs. Stochastic resonance typically involves the enhancement of weak signals in the presence of noise. In this context, techniques are applied for extracting super-resolved features by treating the discrete boundaries imposed by the partitioning in the tokeniser as the thresholds. In this reviewers opinion, the link should be emphasised and clarified to improve the overall contribution of the paper.~~ *The authors agree that this is an issue, and have substantially revised their manuscript.* \n- While the authors highlight the computational limitations of ensembling, at times the wording in the article seems to point to computational benefits, e.g. \"Practical implementations demonstrate efficient execution on even laptop GPUs\". ~~At the same time, the study is limited to ViT-S16 capacities due to computational considerations. While we wholeheartedly agree that not all studies need to be extrapolated to models with huge memory footprints, the limitation to small architectures strikes the author as a little too restrictive, and using a base model (ViT-B16) would have been more convincing.~~ *The authors included larger models in subtasks and expand their results for segmentation in the revised paper.*\n- While the method is novel, the current applications seem moderately niched.\n\n**Summary from rebuttal:** *Several of the concerns were addressed in the last revision, and while certain doubts still linger on the potential impact of the method given the role of new tokenisation methods to alleviate quantisation artefacts, the method is novel with promising initial results.*"
                },
                "questions": {
                    "value": "- ~~What is the precise nature of the artefacts the authors wish to remedy? Are the authors arguing that the low resolution PCA / attention maps exhibits these artefacts by virtue of having low resolution?~~ *This was addressed in the last revision.*\n- Have the authors considered adding metrics for evaluating the faithfulness of the attributions in the visualisations of the features? **Edit:** *The authors downplay the contribution of the visualisations in their contributions, addressing this concern.*\n- ~~Why were contrastive comparisons to Amir et al. (2021) (in terms of results) omitted?~~ *The authors diligently expand the discussion and experiments in the revision. The exposition from the rebuttal could be meaningfully be appended to the manuscript for a more nuanced read.*\n- ~~Exactly which embeddings were used for the KNN in the image retrieval task? The ensemble of class tokens? Pooled features? This is very unclear.~~ *The authors address this in their revision, however the methodology for non-dense prediction tasks is still unclear.*\n- ~~The metrics for results in table 4, are they mAP? What does the columns 1-6 refer to?~~ *The authors address this in the revision.*\n\nAn additional question was added in discussion:\n- ~~The scores with salient segmentation using TokenCut omit comparisons with post processing using the bilateral solver proposed in the original paper. The reasoning behind this choice seems unclear [...]~~ *The authors addressed this by expanding their results.*"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1011/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1011/Reviewer_wDbQ",
                        "ICLR.cc/2024/Conference/Submission1011/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1011/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698929880927,
            "cdate": 1698929880927,
            "tmdate": 1700726589877,
            "mdate": 1700726589877,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tLWD4PZLcR",
                "forum": "etGEIggjWS",
                "replyto": "3XvUsEQBsU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1011/Reviewer_wDbQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1011/Reviewer_wDbQ"
                ],
                "content": {
                    "title": {
                        "value": "On salient region segmentation"
                    },
                    "comment": {
                        "value": "Some additional concerns; in reviewing the source material it seems that the scores with salient segmentation using TokenCut omit comparisons with post processing using the bilateral solver proposed in the original paper. The reasoning behind this choice seems unclear, and we would appreciate an elaboration, given that the goal of upscaling is very much aligned with the authors proposed method.\n \n> TokenCut demands substantial memory resources when applied to a larger number of ViT tokens\n\n> Notably, this improvement is constrained by the model architecture, as TokenCut operates at the coarse segmentation level of ViT tokens. Given that SRT has the capability to provide finer-grained features (directly applying TokenCut at this level is computationally impractical due to its $\\mathcal O(n^2)$ complexity, where $n$ is the number of tokens)\n\nExtracting the lowest two eigenvalues can be done efficiently with `torch.lobpcg`. This significantly reduces computational overhead, and we are uncertain as to the validity of the complexity estimate provided by the authors. \n\nWe kindly ask that the authors address these concerns in their rebuttal."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1011/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700044456395,
                "cdate": 1700044456395,
                "tmdate": 1700044604109,
                "mdate": 1700044604109,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pRDEMae6oO",
                "forum": "etGEIggjWS",
                "replyto": "3XvUsEQBsU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1011/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1011/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response to the initial review"
                    },
                    "comment": {
                        "value": "**W1:** *insufficiently motivated ... the exact nature of these artefacts*\n\n**R1:** We are not sure about the meaning of the term \u201cexact nature\u201d, but for imaging data the two most salient phenomena are occlusions and scale. Neither are respected by spatial quantization: occlusions are blended in samples that straddle the boundary, depriving the representation of precious information about the topology of the scene. Scale is also critically lost since the same spatial quantization of the image may correspond to any portion of an object, depending on its distance. This makes it difficult for the learned representation since it is forced to learn that the same object can span anywhere from a single sample to the entire image. \n\nIn practice, the shortcomings of quantization schema are evident in DINO-based methods for segmentation, e.g. Tokencut, which only creates low-resolution segmentation due to quantization, and requires post-processing and refinement of pixel-level segmentation maps. \n\nThis seems to be sufficient motivation to us, but if there are other aspects of spatial quantization that we missed we are certainly open to elaborate. We are more than happy to revise the paper to make the motivation clear to the readers.\n\n\n**W2:** *simpler interpolation methods exist... not clear if these new feature maps can be said to be faithful...*\n\n**R2:** We display the feature map only as a way of visualizing the outcome of SRT, which is a feature map that is strictly more informative than the original quantized map. It is difficult to determine whether feature maps are  \u201cfaithful\u201d (and to what degree of \u201cfaithfulness\u201d), so we measure it by the improvement on downstream tasks relative to the baseline. This is particularly the reason for conducting our experiments under zero-shot settings: Given that the encoding and decoding layers are frozen, features that are not faithful to the original would disrupt the outcome. On the contrary, our experiments show that the ensembled features yield consistent improvement for a number of downstream tasks including video object segmentation, depth prediction, unsupervised saliency segmentation, image retrieval and unsupervised object discovery; while it is difficult to define \u201cfaithfulness\u201d, experimental results imply that the ensembled features are not \u201cunfaithful\u201d. \n\nMoreover, please refer to the Appendix of the revised manuscript for a comparison with (bilinear) interpolation. We compare SRT and simple interpolation to features from single forward passes. It is evident that SRT makes adjustments to the feature map at semantic boundaries, whereas simple interpolation displays a less meaningful grid pattern. We hope that this provides further support for the 'faithfulness' concern raised by the reviewer.\n\n**W3:** *conceptual overlap with Amir et al. (2021)...*\n\n**R3:** Indeed, Amir et al. is only conceptually related and direct comparisons cannot be made: We could compare Amir et al. to our result prior to aggregation, but that would defeat the purpose because our goal is to retain the same dimension of the original representation. Amir et al. requires making modifications to the forward pass by using a stride smaller than the quantization domain to produce overlapping tokens that ultimately yields 2x increase in spatial resolution (but with 4x more tokens); whereas, our method does not require any changes to the forward pass and produces 16x increase in signal resolution. To match the same granularity, Amir et al. would need to produce 256x more tokens (65536x more compute), which makes it computationally infeasible. Nevertheless, we reproduced the results of Amir et al. and tested their 2x super-resolved ViT-S/16 feature maps on video object segmentation (Table 1, main paper) following protocol specified in DINO. Their features resulted in worse performance than the baseline pretrained model, whereas with just 1 pixel of displacement, SRT improves over the baseline (Fig. 3, main paper). \n\n**W4:** *Methodology for non-dense downstream tasks unclear...*\n\n**R4**: Reproducibility will be facilitated by open-sourcing our code which we will do upon completion of the review process. In the meantime, We have added details about input resolutions as requested by the reviewer in our revised manuscript. For evaluation on retrieval, we follow protocols used in DINO and use the original resolution from the datasets. \n\n**W5:** *The link to stochastic resonance...*\n\n**R5:** This is a great suggestion. We will emphasize as suggested\n\n**W6:** *the study is limited to ViT-S16*\n\n**R6:** SRT is not limited to ViT-S/16 capacities. We do show results for ViT-B/16 in Table 1 of the main paper. Additionally, in Table 2, we show results on VIT-B/14. In fact, we have no restriction in network capacity as we model the operation of inverse translation and average pooling into a recursive mean. So, SRT increases negligible memory footprint; however, as with all ensembling methods, it does scale in time."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1011/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700096735980,
                "cdate": 1700096735980,
                "tmdate": 1700710175778,
                "mdate": 1700710175778,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Pb6OVQcEmi",
                "forum": "etGEIggjWS",
                "replyto": "3XvUsEQBsU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1011/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1011/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response to the initial review (continued)"
                    },
                    "comment": {
                        "value": "**W7:** *While the method is novel, the current applications seem moderately niched.*\n\n**R7:** Respectfully, we disagree: In the main paper, we demonstrated five diverse applications of SRT: video object segmentation, depth prediction, unsupervised saliency segmentation, image retrieval, and unsupervised object discovery. We have additionally added classification and semantic segmentation in Table 5 and 6 of the Appendix, respectively, for a total of seven tasks covering a broad range of computer vision applications. SRT shows promise for all of them. \n \nWe acknowledge the reviewer's perspective, as our emphasis on SRT's ability to achieve fine-grained feature maps may have been more pronounced. In response to this feedback, we plan to revise the paper to give greater emphasis to the practical applications of SRT.\n\n**Q1\uff1a** *What is the precise nature of the artefacts the authors wish to remedy?*\n\n**R8:** Spatial quantization causes information loss about key phenomenologies such as occlusion and scale, which we expect to impact performance in geometric and semantic inference tasks. Therefore, we expect that the artifacts of quantization will be manifest in all downstream visual tasks, and we test this assumption on different vision tasks.\n\n**Q2\uff1a** *Have the authors considered adding metrics for evaluating the faithfulness of the attributions in the visualisations of the features?*\n\n**R9:** Visualization is only for illustrative purposes, since the underlying representation is high dimensional. We use downstream metrics to evaluate the faithfulness of the representation to the content of the scene, despite quantization of the image. If there are other metrics thats evaluate the faithfulness we are also happy to include in the revised paper.\n\n**Q3\uff1a** *Why were contrastive comparisons to Amir et al. (2021) (in terms of results) omitted?*\n\n**R10:** (Also see R8) Amir's approach is conceptually related, but in reality, their original work increases feature size by 2 times (equivalent to 4 times the number of tokens). In contrast, our method achieves a 16 times denser feature map. Even when adhering to their original settings, we encounter hardware limitations on most tasks. Interestingly, in Table 1, simply increasing the number of patches to overlapping patches is detrimental. We hypothesize that this practice alters the forward pass of the Vision Transformer (ViT), generating features outside the domain for which the original ViT is trained. In contrast, SRT does not modify the forward pass.\n\nMethod | F&J-Mean | J-Mean | J-Recall | F-Mean | F-Recall\n----------- | ----------- | ----------- | ----------- | ----------- | ----------- |\nBaseline (DINO-ViT-S/16) | 0.617 | 0.602 | 0.740 | 0.634 | 0.764\n+Overlapping Tokens  (Amir et al.)  | 0.591 | 0.577 | 0.706 | 0.605 | 0.741 |\nSRT | **0.642** | **0.632** | **0.783** | **0.653** | **0.819** |\n\n**Q4\uff1a** *Exactly which embeddings were used for the KNN in the image retrieval task? The ensemble of class tokens? Pooled features? This is very unclear.*\n\n**R11** We follow the protocol of the original DINO paper, which applies retrieval on the class token. Note that in order to obtain ensemble class token with SRT, we do not naively average the class token of augmented image, but ensemble PRIOR to the attention mechanism in the last layer. In this way the final class token is computed from the ensemble SRT feature. We have revised the paper to add the details as requested by the reviewer.\n\n\n**Q5\uff1a** *The metrics for results in table 4, are they mAP? What does the columns 1-6 refer to?*\n\n**R12** Yes mAP. 1-6 refer to noise level in SRT. We have revised the paper to make it clear to the readers.\n\n**Update:** As suggested by the reviewer, we will include a discussion on *W3/R3* in the paper."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1011/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700096813759,
                "cdate": 1700096813759,
                "tmdate": 1700710390696,
                "mdate": 1700710390696,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rzXfBn2AKn",
                "forum": "etGEIggjWS",
                "replyto": "3XvUsEQBsU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1011/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1011/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response to concerns about running TokenCut for saliency region segmentation"
                    },
                    "comment": {
                        "value": "**Reviewer:** *\"we are uncertain as to the validity of the complexity estimate provided by the authors.\"*\n\n**Our response regarding computational constraints:**\nWe have confirmed the $O(n^2)$ complexity with the original authors of TokenCut through email. There might be a misunderstanding, but this complexity arises not from eigenvalue computation but due to TokenCut's use of graphcut techniques, necessitating the construction of a fully connected graph among the patches (implemented by Numpy on CPU). Hence, like other graphcut-based methods, the computational feasibility of TokenCut depends largely on the size and granularity of the features. In the case of ViT-B architectures, the feature dimensions increase from 256 to 768. The hardware constraint we faced upon paper submission was from the CPU and RAM, not the GPU. ViT-B/8 has 4x more tokens. To build a complete graph on ViT-B/8, one would create 16x more graph connections each requiring 3x more memory. To the best of our ability (and the limitations of our compute), we have tried to conduct experiments with TokenCut on our ViT-B/16 and ViT-B/8 features but were only able to run it on ViT-S/16 (Table 3 in the main text). We have clarified this point in the revised paper (Sec. 3.4).  \n\nUpon rebuttal, with a hardware update, we managed to resolve the issue and successfully ran ViT-B/16 with TokenCut, the results are as follows and we are happy to include the results in the revision:\n\nDatasets | Method | maxF | IoU | Accuracy |\n----------- | ----------- | ----------- | ----------- | ----------- |\nECSSD | Baseline | 80.3 | 71.0 | 91.5 |\nECSSD  | SRT | **81.8** | **72.6** | **92.2** |\nDUTS | Baseline | 66.4 | 56.7 | 89.5 |\nDUTS  | SRT | **68.8** | **58.3** | **90.6** |\nDUTS-OMRON | Baseline | 56.7 | 50.5 | 85.4 |\nDUTS-OMRON  | SRT | **58.0** | **51.6** | **86.1** |\n\nIt is worth noting that, even with updated hardware, directly applying TokenCut to the fine-grained features is still computationally infeasible, since the number of tokens increases from $(h/16) \\times (w/16)$ to $h \\times w$ (256 times more), thus the computational resources required to handle the fully-connected graph becomes intractable. As stated in the paper, \"we anticipate that future research will develop methods to leverage SRT\u2019s high-resolution embeddings effectively.\"\n\n___\n**Reviewer:** *\"the scores with salient segmentation using TokenCut omit comparisons with post processing\"*\n\n**Our response regarding postprocessing:** \nIt's important to emphasize that our goal is to utilize TokenCut as a metric for comparing the original ViT feature with the ensemble SRT feature, rather than enhancing TokenCut itself. While post-processing techniques like Bilateral Solver or CRF refine segmentation boundaries, they don't directly address the comparison. As stated in the paper 'We execute TokenCut without any post-processing, such as Conditional Random Fields (CRF), to assess the raw quality of ViT embeddings.' (Sect. 3.4) Furthermore, our experiments are conducted fairly, using pooled features that share the same resolution as the original features employed by TokenCut.\nNevertheless, as requested by the reviewer, below we provide comparison of TokenCut with Bilateral Solver and ViT-S/16:\n\nDatasets | Method | maxF | IoU | Accuracy |\n----------- | ----------- | ----------- | ----------- | ----------- |\nECSSD | Baseline | 87.4 | **77.2** | 93.4 |\nECSSD  | SRT | **88.4** | 77.0 | **93.6** |\nDUTS | Baseline | 75.5 | **62.4** | 91.4 |\nDUTS  | SRT | **76.5** | **62.4** | **91.7** |\nDUTS-OMRON | Baseline | 69.7 | 61.8 | 89.7 |\nDUTS-OMRON  | SRT | **70.6** | **62.4** | **89.9** |\n\nWe hope this answers the reviewer's question. If there are further concerns, we are more than willing to provide additional explanations."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1011/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700112865651,
                "cdate": 1700112865651,
                "tmdate": 1700174501654,
                "mdate": 1700174501654,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TfbyQCO4tM",
                "forum": "etGEIggjWS",
                "replyto": "3XvUsEQBsU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1011/Reviewer_wDbQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1011/Reviewer_wDbQ"
                ],
                "content": {
                    "comment": {
                        "value": "> We are not sure about the meaning of the term \u201cexact nature\u201d [...] \n\n> Spatial quantization causes information loss [...]\n\nThe exact nature of the term exact nature is, in this reviewers mind, semantically unambiguous. Specifically, this reviewer was referring to a formal exposition, preferably in more formal mathematical terms to heighten the clarity of the work. The discussion of the spatial quantisation and the link to stochastic resonance requires a clear exposition of the artefacts you discuss, with a level of formality. As you may have noticed, this reviewer is not in lack of understanding, but asking for specifications which could heighten the readability of the paper. \n\n*This may have been addressed in the revision*, however it seems that reviewers currently cannot see the revision history. This reviewer have voiced their concerns in the appropriate channels to see if if this is, for some reason, intentional, or a hiccup in the system.\n\n> It is difficult to determine whether feature maps are \u201cfaithful\u201d (and to what degree of \u201cfaithfulness\u201d) [...]\n\n> Visualization is only for illustrative purposes, [...]\n\nSeveral metrics for faithfulness of model attributions exist, e.g. *\"A Comparative Study of Faithfulness Metrics for Model Interpretability Methods\"* (Chan et al. 2022). As you are claim these attributions as a leading contribution (\"yields a versatile visualization tool\"), it would be diligent to address this, or simply adjust the significance of this precise contribution. \n\n> Amir et al. is only conceptually related and direct comparisons cannot be made [...]\n\n> Amir's approach is conceptually related, but in reality, their original work increases feature size [...]\n\nIt seems that a overarching contrastive comparison can indeed be made by virtue of the discussion included by the authors. This discussion in your response is important for readers of your paper, and precisely the reason for our bringing light to the matter. Can the authors be compelled to expand their related work, or include the discussion in the appendix?\n\n> Reproducibility will be facilitated by open-sourcing our code [...]\n\nWhile your dedication to sharing your code is admirable, it is no replacement for detailing your methodology, since a practitioner starting from scratch to reproduce your results will potentially avoid the artefacts that could occur due to a specific implementation.\n\n> R5: This is a great suggestion. We will emphasize as suggested\n\nWe are happy to have positively contributed to your work. Ideally this ties in with our concern on the nature of quantisation artefacts. \n\n> SRT is not limited to ViT-S/16 capacities\n\nApologies if the comment seemed to imply that your method could not be applied for other capacities. Specifically, we meant to address base capacities in the segmentation results, which now has been dutifully addressed in a later comment.\n\n> Respectfully, we disagree: [...] (on niched applications)\n\nit is fine that the authors disagree, but other methods to handle quantisation artefacts exist, notably recent works on adaptable tokenisation, e.g. *\"Vision Transformers with Mixed-Resolution Tokenization\"* (Ronen et al. 2023) which tackles the same issue by changing the tokenisation method in-place. While this by no means diminishes the authors work, since the approaches are clearly different, we note that such methods have more direct applications. A discussion on alternative approaches to tackle the issues of uniform square spatial quantisation would serve to add context to the work and ongoing research in the field. Particularly since it is also can be applied in conjunction with SRT.\n\n> Note that in order to obtain ensemble class token with SRT, we do not naively average the class token of augmented image, but ensemble PRIOR to the attention mechanism in the last layer\n\nThis ties in with our concerns about the methodology of the work in previous replies. While this could perhaps be made clear by surveying the code, this reviewer suggests that *it is imperative that the methodology is made explicit in the paper*. While the authors are naturally concerned with space constraints, a full exposition in the appendix is not only preferable, but necessary.\n\n**On a general note**: We note that the authors have made efforts to improve their work, and we are happy to see clarification in the presentation of results and inclusion of more contrastive experiments for their approach. Our remaining concerns are related to the methodology, reproducibility and context for readers to ascertain the overall value of the contribution.\n\n**Update**: To clarify our position, at this time this reviewer tends to keep their original rating."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1011/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700300038470,
                "cdate": 1700300038470,
                "tmdate": 1700667693380,
                "mdate": 1700667693380,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SrXn0W2NWH",
                "forum": "etGEIggjWS",
                "replyto": "3XvUsEQBsU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1011/Reviewer_wDbQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1011/Reviewer_wDbQ"
                ],
                "content": {
                    "title": {
                        "value": "Brief comment on computational constraints"
                    },
                    "comment": {
                        "value": "> Our response regarding computational constraints [...]\n\nWe are glad to see our constructive feedback prompted extra results for your work.\n\nAs a side note, there is no compelling reason for using NumPy to apply TokenCut. The full graph is essentially constructed by taking cosine similarity, thresholding it, and computing the Laplacian $L = D - A$. The classic normalised graph cut is essentially performed using the Fiedler vector (the eigenvector corresponding to the second lowest magnitude eigenvalue). All this is possible to apply on GPU without issue (PyTorch has all the tools necessary), and while the theoretical complexity might be high, it is on the same level as self-attention operators (less so since it does not require Q,K,V projections)."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1011/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700301620466,
                "cdate": 1700301620466,
                "tmdate": 1700301783928,
                "mdate": 1700301783928,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uhDaz8kJNE",
                "forum": "etGEIggjWS",
                "replyto": "ndb9ucVbII",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1011/Reviewer_wDbQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1011/Reviewer_wDbQ"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for detailed response"
                    },
                    "comment": {
                        "value": "The last revision of the manuscript is well formulated, and looks to formally link stochastic resonance operators to the method and overall presents the framework in a more coherent and cohesive manner. This addresses a central concern for this reviewer, and raising the recommendation to a 6 as a result. Pardon the brevity of the comment due to time constraints."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1011/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700725772250,
                "cdate": 1700725772250,
                "tmdate": 1700725772250,
                "mdate": 1700725772250,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]