[
    {
        "title": "Interpretable and Generalizable Graph Neural Networks via Subgraph Multilinear Extension"
    },
    {
        "review": {
            "id": "Dhxdxii12O",
            "forum": "dVq2StlcnY",
            "replyto": "dVq2StlcnY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7185/Reviewer_uKEu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7185/Reviewer_uKEu"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors introduce a self-interpretable graph neural network rooted in the concept of subgraph multilinear extension. The method is elaborated from a perspective of counterfactual fidelity, and the authors put forward both linear and sampling-based graph neural networks (GNNs) for subgraph extraction. The experimental results suggest that the proposed method is effective in achieving its intended goals."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed method is well-supported by theoretical foundations. The authors provide an insightful explanation of the causal subgraph from a fidelity perspective and derive a method that aligns well with this theoretical underpinning.\n\n2. The authors offer a theoretical framework for improving graph interpretability and shed light on why existing methods often fall short in this regard. They also articulate how they aim to meet this criterion and present experimental results that appear to support their proposition."
                },
                "weaknesses": {
                    "value": "The authors conduct multiple comparisons between the proposed method and GSAT throughout the paper. However, it appears that the primary distinction lies in the sampling process, with the proposed method incorporating a higher number of samples than previously employed in GSAT. It would be valuable for the authors to emphasize this key differentiator and provide a more detailed discussion regarding the impact of increased sampling on the method's performance and results. This would enable a clearer understanding of the specific advantages of their approach over GSAT."
                },
                "questions": {
                    "value": "For figure 2(b) and 2(c), what is the distance metric specifically used? Will there be any difference in the results using different metrics?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7185/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7185/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7185/Reviewer_uKEu"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7185/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698806959155,
            "cdate": 1698806959155,
            "tmdate": 1699636852893,
            "mdate": 1699636852893,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9Dk54pCvMA",
                "forum": "dVq2StlcnY",
                "replyto": "Dhxdxii12O",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7185/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7185/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer uKEu [1/1]"
                    },
                    "comment": {
                        "value": "Thank you for your insightful comments on our work. Please see our responses to your comments and suggestions below.\n\n> W1.1. It appears that the primary distinction lies in the sampling process, with the proposed method incorporating a higher number of samples than previously employed in GSAT. \n\nWe\u2019d like to clarify that, technically, there are two key innovations in GMT compared to GSAT:\n- The first one is the accurate estimation of SubMT, either through the linearized variant (GMT-LIN) or the multiple random subgraph sampling variant (GMT-sam). Note that the weighted message passing scheme in GSAT cannot accept discrete input, and will suffer from the biases from both the approximation of SubMT and the Gumbel softmax trick[1,2]. GMT-sam overcomes the two drawbacks in GSAT simultaneously with the random subgraph sampling scheme implemented via straight-through estimator.\n- The second is the efficient approximation of the neural subgraph multilinear extension, which is typically non-trivial as it requires the MCMC sampling. We thus develop multiple subgraph decoding strategies to efficiently decode from the neural subgraph multlinear extension, and reduce the inference latency competitively to that of a vanilla GNN. The second innovation is critical to the deployment of GMT-sam in many realistic applications.\nWe have revised our manuscript to highlight the key differentiators.\n\n> W1.2. It would be valuable for the authors to emphasize this key differentiator and provide a more detailed discussion regarding the impact of increased sampling on the method's performance and results. \n\nThank you for the constructive suggestion! We have revised our manuscript to include the following discussion:\n- Theoretically, as shown in Theorem 5.1, increasing the sampling grounds in GMT-sam could result in a more accurate approximation of the SubMT. However, it can also bring drawbacks to the optimization with the increased sampling.\n- On the one hand, it requires unnecessary computational overhead with a large number of sampling. From Figure 3 (b) and (c), it can be found that the performance will saturate after a sufficient number of samplings such as 80 or 100. Then, it is unnecessary to increase the sampling rounds, since a smaller number of sampling rounds is already sufficient to guide the model to find the causal subgraph for the task.\n- On the other hand, in some cases where the task is too simple or too complicated, as discussed in Appendix E.2, a large number of sampling rounds at the beginning can lead the model more easily fit to noises, spurious correlations, or even a trivial solution of the GSAT objective. In this case, we find that sampling only once could benefit the optimization similarly to the warmup strategy. \n\n> Q1. For Figures 2(b) and 2(c), what is the distance metric specifically used? Will there be any difference in the results using different metrics?\n\nThroughout the paper, our discussion and experiments adopt the total variation distance as the distance metric. Following your suggestion, we also conducted extensive experiments with KL divergence and JSD divergence. As shown in Figures 8,9,10,11 in the revised version, GMT achieves even more improvements up to 10 times larger than that of GSAT when measured in KL or JSD divergence. The results align with those using the total variation distance and demonstrate the generality of our theory and method.\n\nPlease let us know if you have any further questions. We\u2019d sincerely appreciate it if you could take the above responses into consideration when making the final evaluation of our work!\n\n**References**\n\n[1] Categorical reparameterization with gumbel-softmax, ICLR 2017.\n\n[2] The concrete distribution: A continuous relaxation of discrete random variables, ICLR 2017."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7185/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700332632738,
                "cdate": 1700332632738,
                "tmdate": 1700332632738,
                "mdate": 1700332632738,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ze1yuq2fdZ",
                "forum": "dVq2StlcnY",
                "replyto": "Dhxdxii12O",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7185/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7185/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A summary of response to Reviewer uKEu"
                    },
                    "comment": {
                        "value": "Dear Reviewer uKEu,\n\nWe once again thank you for your valuable time and constructive comments in reviewing our work. We have replied to each of the weaknesses/questions raised in your review. To summarize,\n- We discussed more clearly the techical contributions of GMT-sam compared to GSAT, and how to properly tune the sampling rounds;\n- We evaluated the counterfactual fidelity with other distance metrics such as KL divergence, and found GMT-sam can achieve even better results;\n\nPlease let us know if there are any of your concerns have not been addressed in our response. We would sincerely appreciate it if you jointly could take our response into consideration when making the final evaluation of our work!"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7185/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700464831518,
                "cdate": 1700464831518,
                "tmdate": 1700464831518,
                "mdate": 1700464831518,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TJhtz6Re5i",
                "forum": "dVq2StlcnY",
                "replyto": "Dhxdxii12O",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7185/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7185/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A gentle reminder for the closing rebuttal window"
                    },
                    "comment": {
                        "value": "Dear Reviewer uKEu,\n\nWe would like to remind you that the rebuttal will be closed within 48 hours. To allow us sufficient time to discuss with you any of your remaining concerns, we would appreciate it if you could take some time to read our rebuttal and give us some feedback. Thank you very much."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7185/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700570054265,
                "cdate": 1700570054265,
                "tmdate": 1700570054265,
                "mdate": 1700570054265,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IoOyvIC1Mt",
                "forum": "dVq2StlcnY",
                "replyto": "Ze1yuq2fdZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7185/Reviewer_uKEu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7185/Reviewer_uKEu"
                ],
                "content": {
                    "title": {
                        "value": "Another concern for the baseline results"
                    },
                    "comment": {
                        "value": "I appreciate the authors' response, which has addressed several of my previous questions.\n\nHowever, upon revisiting the original GSAT paper, I have a new concern. It appears that the previous baseline results (from GNNEXPLAINER to DIR in Table 1) used for comparison in this paper are identical to those reported in GSAT. Nevertheless, the reported results of GSAT in some cases exhibit deviations from those in its original paper. Moreover, in certain datasets, the deviations are even more substantial (e.g., SPURIOUS-MOTIF and MNIST-75SP). Could the authors provide an explanation for why the reported results of the previous baselines are consistent with those in GSAT while there are deviations in the GSAT paper?\n\nThank you!"
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7185/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700596881532,
                "cdate": 1700596881532,
                "tmdate": 1700596881532,
                "mdate": 1700596881532,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "erUnoJ5Mdz",
                "forum": "dVq2StlcnY",
                "replyto": "Dhxdxii12O",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7185/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7185/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Explanation for the baseline results"
                    },
                    "comment": {
                        "value": "Thanks for your reply and careful observation. We are happy to learn that your previous concerns have been addressed. We are confident to address the remaining concern. Please find our answer in the below:\n- The main reason for the performance deviations of GSAT from the original work is that the original GSAT performances are obtained via a **mathematically incorrect implementation**, as acknowledged by the authors ([link](https://github.com/Graph-COM/GSAT/issues/10)). Nevertheless, as explained by the authors, in their follow-up work LRI, they found that mathematically correct implementation could have a better performance. \n- **To ensure a mathematically consistent and fair comparison, we implement all methods strictly following the mathematically correct formulations in all experimental settings**. Nevertheless, GMT-sam can still outperform GSAT in most of the settings regardless of which form GSAT is implemented. We showcase via spurious-motif datasets:\n\n|                       | Generalization  |                 |                 | Interpretation  |                 |                 |\n|-----------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|\n|                       | spmotif-0.5     | spmotif-0.7     | spmotif-0.9     | spmotif-0.5     | spmotif-0.7     | spmotif-0.9     |\n| GSAT (math correct)   | 47.45+-5.87     | 43.57+-3.05     | 45.39+-5.02     | 74.49+-4.46     | 72.95+-6.40     | 65.25+-4.42     |\n| GSAT (math incorrect) | 52.74+-4.08     | 49.12+-3.29     | 44.22+-5.57     | 78.45+-3.12     | 74.07+-5.28     | 71.97+-4.41     |\n| GMT-sam               | **60.09+-5.57** | **54.34+-4.04** | **55.83+-5.68** | **85.50+-2.40** | **84.67+-2.38** | **73.49+-5.33** |\n\n\n- As for the other baselines, they do not have the issue, while having a significant performance gap from GSAT. Therefore we directly adopt the reported results in the paper of GSAT.\n\n\nPlease let us know any of your remaining concerns. Thank you!"
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7185/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700621855409,
                "cdate": 1700621855409,
                "tmdate": 1700622027420,
                "mdate": 1700622027420,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "c6o3iHgLgt",
                "forum": "dVq2StlcnY",
                "replyto": "erUnoJ5Mdz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7185/Reviewer_uKEu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7185/Reviewer_uKEu"
                ],
                "content": {
                    "comment": {
                        "value": "Thank the authors for their explanation. I will take into consideration their experiments in the final decision."
                    }
                },
                "number": 28,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7185/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700697801668,
                "cdate": 1700697801668,
                "tmdate": 1700697801668,
                "mdate": 1700697801668,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yFH4rsVqw0",
            "forum": "dVq2StlcnY",
            "replyto": "dVq2StlcnY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7185/Reviewer_pmQk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7185/Reviewer_pmQk"
            ],
            "content": {
                "summary": {
                    "value": "The paper seeks a theoretical understanding of the representational properties and limitations of interpretable GNNs (XGNNs). The paper identifies the ability to approximate the so-called Subgraph Multilinear Extension as the key distribution for interpretable subgraph learning. Building on this observation, they propose the GMT model which implements such an approximation. The paper experimentally demonstrates the superiority of GMT in interpretation and generalization."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is overall very well-written and the ideas are neatly structured. The authors are able to devise a theoretical framework for characterizing the expressivity of XGNNs, and shows that the existing XGNNs fail to approximate the subgraph multilinear extension function. The proposed XGNN admits a natural design in light of the above deficiency. The experimental protocol is quite thorough and the conducted experiments adequately test the hypothesis."
                },
                "weaknesses": {
                    "value": "The prediction performance of the proposed model does not see too much improvement over the state-of-the-art. There is not much discussion over the time complexity of GMT-SAM version, vis-a-vis the GMT-LIN version."
                },
                "questions": {
                    "value": "1. Page 4: \"Note that $f_c$ is a GNN defined only for discrete graph-structured inputs (i.e., $\u03b1 \\in [0, 1]^m$)\". Do you mean all Boolean m-length vectors, because otherwise this is not really discrete. \n\n2. Page 4: \"which implicitly assumes the random graph data model (Erdos & Renyi, 1984). Def. 3.1 can also be generalized to other graph models with the corresponding parameterization ... \". Can you please comment on this further if this admits such a straightforward generalization to other models than ER, such SBMs or graphons as you suggested?\n\n3. What is $k$ in Eq. 9? Does it mean an application of k-layers of linear message-passing layers? In continuation, in Eq 11, what is the intuition behind the Hadamard operation between $\\hat{A}$ and $A^{k-1}$? Section 5.1 needs to be written more clearly."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7185/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698834234608,
            "cdate": 1698834234608,
            "tmdate": 1699636852791,
            "mdate": 1699636852791,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7Dt6KFXS74",
                "forum": "dVq2StlcnY",
                "replyto": "yFH4rsVqw0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7185/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7185/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer pmQk [1/2]"
                    },
                    "comment": {
                        "value": "Thank you for your constructive comments. We hope our responses to your comments and suggestions below could make you more confident in supporting our work!\n\n> W1.1. The prediction performance of the proposed model does not see too much improvement over the state-of-the-art. \n\nWe\u2019d like to clarify that the generalization performance of the network also depends on the objective itself. In this work, we focus on the graph information bottleneck objective as GSAT[1] that can generalize under certain distribution shifts (i.e., FIIF shifts)[2]. \n- For real-world datasets, it\u2019s unknown for the types of distribution shifts, thus the improvements to generalization performance may be limited.\n- For spurious motif datasets, they are generated exactly following the FIIF shifts, hence **it\u2019s expected that the graph information bottleneck shall hold stable and high generalization performance on spurious motif datasets**. From the results it can be found that GSAT fails to do so as GSAT can not reliably approximate SubMT, while GMT yields a stable and good generalization performance, improving GSAT up to 10% in prediction performance.\n\n> W1.2. There is not much discussion over the time complexity of GMT-sam version, vis-a-vis the GMT-LIN version.\n\nWe have supplemented the discussion regarding the time complexity of GMT variants as well as GSAT in the revised version. Our discussion focuses on vanilla GNNs implemented in message passing scheme and one could generalize to other more complicated GNNs by correspondingly incorporating their message passing complexity.\n- Let $E$ be the number of edges for the graph. As GMT-LIN differs only in the number of weighted message passing rounds from GSAT, and has the same number of total message passing rounds, hence GMT-LIN and GSAT have the same time complexity as $O(E)$ for each epoch, or for inference.\n\nWhen comparing GMT-sam to GMT-LIN and GSAT,\n- During training, GMT-sam needs to process $k$ rounds of random subgraph sampling, resulting in $O(kE)$ time complexity;\n- During inference, GMT-sam with normal subgraph decoding methods requires the same complexity as GMT-LIN and GSAT, as $O(E)$. \nWhen with special decoding strategy such as setting part of the attention entries to $1$ or $0$,  GMT-sam additionally needs to sort the attention weights, and requires $O(E+E\\log E)$ time complexity;\n\nIn the table below, we benchmarked the real training/inference time of GSAT, GMT-LIN and GMT-sam in different datasets, where each entry demonstrates the time in seconds for one epoch. We benchmark the latency of GSAT, GMT-LIN and GMT-sam based on GIN, PNA and EGNN on different scales of datasets. The sampling rounds of GMT-sam are set to 20 for PNA on MNIST-75sp, 10 for EGNN, and 100 to other setups. \n|                  | BA_2Motifs |             | MNIST-75sp     |              | ActsTrack  |\n|------------------|------------|-------------|--------------|--------------|------------|\n| Training         | GIN        | PNA         | GIN          | PNA          | EGNN       |\n| GSAT             | 0.70+-0.12 | 1.00+-0.13  | 41.28+-0.61  | 80.98+-10.55 | 3.57+-1.41 |\n| GMT-LIN          | 0.68+-0.12 | 1.02+-0.15  | 41.12+-0.69  | 81.11+-10.44 | 3.69+-0.93 |\n| GMT-sam          | 6.25+-0.48 | 17.03+-0.91 | 136.60+-1.21 | 280.77+-4.00 | 5.38+-0.59 |\n| Inference        |            |             |              |              |            |\n| GSAT             | 0.07+-0.05 | 0.11+-0.12  | 18.69+-0.35  | 24.40+-2.06  | 0.84+-0.38 |\n| GMT-LIN          | 0.08+-0.07 | 0.07+-0.01  | 18.72+-0.41  | 23.81+-1.89  | 0.80+-0.21 |\n| GMT-sam (normal) | 0.05+-0.01 | 0.12+-0.01  | 18.72+-0.35  | 18.01+-1.47  | 0.50+-0.13 |\n| GMT-sam (sort)   | 0.07+-0.01 | 0.21+-0.06  | 19.07+-0.55  | 18.69+-3.35  | 0.54+-0.10 |\n\nFrom the table, it can be found that, \n- Although GMT-sam takes a longer time for training, but the absolute values are not high even for the largest dataset MNIST-75sp. When compared to other intrinsic interpretable methods, GMT-sam consumes a similar training time around 6 hours on MNIST-75sp as DIR. \n- As for inference, GMT-sam enjoys a similar latency as others, aligned with our discussion."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7185/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700332489866,
                "cdate": 1700332489866,
                "tmdate": 1700332489866,
                "mdate": 1700332489866,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gJ0BZHGspo",
                "forum": "dVq2StlcnY",
                "replyto": "yFH4rsVqw0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7185/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7185/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer pmQk [2/2]"
                    },
                    "comment": {
                        "value": "> Q1. Do you mean all Boolean m-length vectors, because otherwise this is not really discrete.\n\nYes, we have revised the description to $\\boldsymbol{\\alpha}\\in\\\\{0,1\\\\}^m$ to avoid any confusion.\n\n> Q2. Can you please comment on this further if this admits such a straightforward generalization to other models than ER, such SBMs or graphons as you suggested?\n\nOne of the key differences between ER and the other two graph models SBMs and Graphons is that, the latter considers the generation of the graph in a blockwise manner. Within each block, the edges have a similar probability of occurring among nodes in the block. The subgraph multilinear can therefore be extended to a hierarchical variant to adhere to the blockwise property.\n\n> Q3.1. What is $k$ in Eq 9? Does it mean an application of k-layers of linear message-passing layers?\n\nYes, $k$ in Eq 9 refers to the $k$ layers of message passing. We have supplemented the explanation to Eq 9 in our revised version.\n\n> Q3.2. What is the intuition behind the Hadamard operation in Eq 11? Section 5.1 needs to be written more clearly.\n\nAs the non-linearity of $k>1$ weighted message passing in Eq 9 is the main cause for the approximation failure, the key idea for adopting the Hardamoard operation is to inject more \u201clinearity\u201d in the message passing by reducing the weighted message passing rounds to $1$. We revised Section 5.1 accordingly to improve its clarity.\n\nPlease let us know if you have any further questions. We\u2019d be grateful if you could take the above responses into consideration when making the final evaluation of our work!\n\n**References**\n\n[1] Interpretable and Generalizable Graph Learning via Stochastic Attention Mechanism, ICML 2022.\n\n[2] Learning Causally Invariant Representations for Out-of-Distribution Generalization on Graphs, NeurIPS 2022."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7185/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700332558470,
                "cdate": 1700332558470,
                "tmdate": 1700332558470,
                "mdate": 1700332558470,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fwaWbED483",
                "forum": "dVq2StlcnY",
                "replyto": "gJ0BZHGspo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7185/Reviewer_pmQk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7185/Reviewer_pmQk"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the reply. I intend to maintain my rating for the paper."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7185/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700422900860,
                "cdate": 1700422900860,
                "tmdate": 1700422900860,
                "mdate": 1700422900860,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3hrBERvHtJ",
            "forum": "dVq2StlcnY",
            "replyto": "dVq2StlcnY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7185/Reviewer_gapk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7185/Reviewer_gapk"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on investigating theoretical foundations for interpretable graph neural networks (XGNN). To study the expressive power of interpretable GNNs, the authors propose a framework that formulates interpretable subgraph learning with the multilinear extension of the subgraph predictivity (subMT). To obtain a more accurate approximation of SubMT, the authors design an XGNN architecture (GMT). The superior empirical results on several graph classification benchmarks support the theoretical findings."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. It's interesting to see some works focusing on better understanding the theoretical foundation for the expressive power of XGNN. The motivation to consider the connection between interpretable subgraph learning and multilinear extension is also sound.\n2. There are some empirical results to directly support the theoretical findings/claims, which have some merits. For example, the ablation studies demonstrate a better performance on counterfactual fidelity which supports the claims in Sec 4.2.\n3. The overall empirical performance of the proposed method seems significantly better than those of baselines."
                },
                "weaknesses": {
                    "value": "1. For Proposition 3.3, in my understanding, it basically says with > 2-layer linear GNN, Eq (6) cannot approximate SubMT. Do you provide any proof of that? Besides, I am curious if we have any empirical findings regarding this. \n2. For interpretation performance comparison, the authors provide more baseline methods for GIN while PNA only has GSAT. I wonder what the reason is for this. Could you provide more baselines with PNA as the backbone?"
                },
                "questions": {
                    "value": "See the above questions in weaknesses. \nMinors:\n1. Table 4 is before Table 2 and 3. It would be better to fix this."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7185/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7185/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7185/Reviewer_gapk"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7185/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699094570374,
            "cdate": 1699094570374,
            "tmdate": 1699636852682,
            "mdate": 1699636852682,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "78bfljP6Pf",
                "forum": "dVq2StlcnY",
                "replyto": "3hrBERvHtJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7185/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7185/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer gapk [1/1]"
                    },
                    "comment": {
                        "value": "Thank you for your insightful comments on our work! Please find our detailed responses to your comments and suggestions below.\n\n> W1.1. Do you provide any proof and empirical findings for Proposition 3.3?\n\nThank you for pointing out the missed proof of Proposition 3.3. We have supplemented the proof in the revised version. The idea to prove for linear GNN is intuitive:\na. First, one could write the outcomes of $f\\_c(A)=\\rho(A^kXW)$ as a summation in $A^k\\_{i,j}v\\_j$, with $v\\_j$ is the j-th entry of the vector $v=XW$. \n\nb. Given the linearity of expectations, the comparison between $E[f_c(A)]$ and $f_c(E[A])$ now turns into the comparison between $E[A^k_{i,j}v_j]$ and $(E[A_{i,j}])^kv_j$. \n\nc. Since $A_ij$ is drawn from the Bernoulli distribution, with the expectation as $\\widehat{A}\\_{i,j}$, it suffices to know that $E[A^k\\_{i,j}v\\_j]=1^k\\widehat{A}_{i,j}+0^k(1-\\widehat{A}\\_{i,j})=\\widehat{A}\\_{i,j}$, while $(E[A\\_{i,j}])^k=\\widehat{A}\\_{i,j}^k$. Then, we know that $E[f\\_c(A)] \\neq f\\_c(E[A])$.\n\n> W1.2. Do you provide any empirical findings for Proposition 3.3?\n\nYes. We have conducted experiments to measure the total variation distance between the simulated SubMT, i.e., $E[f_c(A)]$, and the previous weighted message passing, i.e., $f_c(E[A])$.  Shown as in Fig. 14 to 17 in the revised version, we evaluate the SubMT approximation gaps of GSAT implemented in GIN and SGC on BA\\_2Motifs and Mutag respectively. To fully verify Proposition 3.3, we range the number of layers of GIN and SGC from 1 to 5. It can be found that the results are well aligned with Proposition 3.3:\n- When the number of layers is 1, the SubMT approximation gap is generically the smallest, because of more ``linearity'' in the network.\n-  While along with the growing number of GNN layers, the network becomes more ``unlinear'' such that the SubMT approximation gap will be larger.\n\n> W2.1. For interpretation performance comparison, why do the authors provide more baseline methods for GIN while PNA only has GSAT?\n\nWe follow the GSAT experimental setting to conduct the comparison [1]. Since it has been theoretically shown that post-hoc methods are suboptimal in generating the explanation and sensitive to the pre-trained models. The empirical results with GIN can already verify the results.\n\nWith the results, and given the fact that GSAT has achieved significant improvements over the previous baselines, we thus compare with the other baselines only based on the GIN backbone.\n\n> W2.2. Could you provide more interpretation baselines with PNA as the backbone?\n\nYes. We have conducted experiments with post-hoc explanation methods based on the PNA backbone. Specifically, we selected two representative post-hoc methods GNNExplainer and PGExplainer, and one representative intrinsic interpretable baseline DIR. The results are given in the table below. It can be found that most of the baselines still significantly underperform GSAT and GMT. One exception is that DIR obtains highly competitive (though unstable) interpretation results in spurious motif datasets, nevertheless, the generalization performance of DIR remains highly degenerated (53.03\u00b18.05 on spmotif_0.9).\n|         | BA_2Motifs     | Mutag          | MNIST-75sp     | spmotif_0.5    | spmotif_0.7    | spmotif_0.9    |\n|---------|----------------|----------------|----------------|----------------|----------------|----------------|\n| GNNExp  | 54.14\u00b13.30     | 73.10\u00b17.44     | 53.91\u00b12.67     | 59.40\u00b13.88     | 56.20\u00b16.30     | 57.39\u00b15.95     |\n| PGE     | 48.80\u00b114.58    | 76.02\u00b17.37     | 56.61\u00b13.38     | 59.46\u00b11.57     | 59.65\u00b11.19     | 60.57\u00b10.85     |\n| DIR     | 72.33\u00b123.87    | 87.57\u00b127.87    | 43.12\u00b110.07    | 85.90\u00b12.24     | 83.13\u00b14.26     | 85.10\u00b14.15     |\n| GSAT    | 89.35\u00b15.41     | 99.00\u00b10.37     | 85.72\u00b11.10     | 79.84\u00b13.21     | 79.76\u00b13.66     | 80.70\u00b15.45     |\n| GMT-lin | 95.79\u00b17.30     | 99.58\u00b10.17     | 85.02\u00b11.03     | 80.19\u00b12.22     | 84.74\u00b11.82     | 85.08\u00b13.85     |\n| GMT-sam | **99.60\u00b10.48** | **99.89\u00b10.05** | **87.34\u00b11.79** | **88.27\u00b11.71** | **86.58\u00b11.89** | **85.26\u00b11.92** |\n\n\n> Q1. Positions of Table 2-4.\n\nWe have fixed it in the revised version.\n\nPlease let us know if you have any further questions. We\u2019d be grateful if you could take the above responses into consideration when making the final evaluation of our work!\n\n**References**\n\n[1] Interpretable and Generalizable Graph Learning via Stochastic Attention Mechanism, ICML 2022."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7185/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700332338766,
                "cdate": 1700332338766,
                "tmdate": 1700332354555,
                "mdate": 1700332354555,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4xkQGHrzDv",
                "forum": "dVq2StlcnY",
                "replyto": "3hrBERvHtJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7185/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7185/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A summary of response to Reviewer gapk"
                    },
                    "comment": {
                        "value": "Dear Reviewer gapk,\n\nWe again thank you for your valuable time and constructive comments in reviewing our work. We have responded to each of your concerns. In summary,\n- We provided both theoretical and empirical evidence for Proposition 3.3;\n- We conducted extensive experiments with more interpretation baselines using PNA, where the results demonstrate the consistent improvements of GMT over previous methods;\n- We also revised and updated our manuscript following your suggestions;\n\nPlease let us know if you have any further questions. We\u2019d sincerely appreciate it if you could take our previous responses into consideration when making the final evaluation of our work!"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7185/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700465040869,
                "cdate": 1700465040869,
                "tmdate": 1700465040869,
                "mdate": 1700465040869,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kgHUa18YGZ",
                "forum": "dVq2StlcnY",
                "replyto": "3hrBERvHtJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7185/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7185/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A gentle reminder for the closing rebuttal window"
                    },
                    "comment": {
                        "value": "Dear Reviewer gapk,\n\nWe would like to remind you that the rebuttal will be closed very soon. To allow us sufficient time to discuss with you any of your remaining concerns, we would appreciate it if you could take some time to read our rebuttal and give us some feedback. Thank you very much."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7185/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700569988375,
                "cdate": 1700569988375,
                "tmdate": 1700569988375,
                "mdate": 1700569988375,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aycYINnbbi",
                "forum": "dVq2StlcnY",
                "replyto": "3hrBERvHtJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7185/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7185/Authors"
                ],
                "content": {
                    "title": {
                        "value": "The rebuttal window will be closed very soon"
                    },
                    "comment": {
                        "value": "Dear Reviewer gapk,\n\nWe would like to remind you that the rebuttal will be closed in **less than 24 hours**. To allow us sufficient time to discuss with you your concerns about our work, we would appreciate it if you could take some time to read our rebuttal and give us some feedback. Thank you very much!"
                    }
                },
                "number": 27,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7185/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700659823320,
                "cdate": 1700659823320,
                "tmdate": 1700659860977,
                "mdate": 1700659860977,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Yz6eu9KtAj",
                "forum": "dVq2StlcnY",
                "replyto": "3hrBERvHtJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7185/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7185/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We're looking forward to your feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer gapk,\n\nWe would like to remind you that the rebuttal will be closed in **less than 8 hours**. To allow us a last chance to discuss with you any of your remaining concerns about our work, we would appreciate it if you could take some time to read our rebuttal and give us some feedback. Thank you very much!"
                    }
                },
                "number": 31,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7185/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700711853940,
                "cdate": 1700711853940,
                "tmdate": 1700711853940,
                "mdate": 1700711853940,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gkXKwbV6lY",
                "forum": "dVq2StlcnY",
                "replyto": "78bfljP6Pf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7185/Reviewer_gapk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7185/Reviewer_gapk"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the detailed rebuttal. The response seems sound to me especially the proof and the empirical evidence for Proposition 3.3. I keep my score as 6."
                    }
                },
                "number": 32,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7185/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700712953181,
                "cdate": 1700712953181,
                "tmdate": 1700712953181,
                "mdate": 1700712953181,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MX0XfCd4yR",
            "forum": "dVq2StlcnY",
            "replyto": "dVq2StlcnY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7185/Reviewer_FFuz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7185/Reviewer_FFuz"
            ],
            "content": {
                "summary": {
                    "value": "The authors present a framework for GNN explanation through the len of multilinear extension. To improve the expressiveness they proposed a new method, namely GMT. One variant of GMT is to make sure the GNN is fully linear. Another neural version is to simply re-train a GNN classifier with a frozen subgraph extractor."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper offered some interesting insights into interpretable GNNs and proposed some solutions. The experiments on the proposed methods are extensive and shown pretty good results."
                },
                "weaknesses": {
                    "value": "First of all, I find the paper hard to read and follow. The presentation and writing need some good work to make the paper more readable and clear. While the authors seem to had a hard time squeeze the paper below the page limit, they also over cite even by a very conservative standard. The citation list is almost the same length as the paper itself. \nThe experiment set-up is actually quite complicated as detailed in appendix E. The authors also searched extensively for different setups (stated in F2). It is unclear how sensitive the result is, wrt. warm-up strategy and a bunch of sampling strategies in Appendix E2 (page 32)."
                },
                "questions": {
                    "value": "Is there any reason or hypothesis that both SubMT (Figure 2) and GMT (Figure 6/7 in Appendix) perform better on Mutag but less well on BA-2Motifs?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7185/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699332114864,
            "cdate": 1699332114864,
            "tmdate": 1699636852573,
            "mdate": 1699636852573,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0Kcbmk1ugs",
                "forum": "dVq2StlcnY",
                "replyto": "MX0XfCd4yR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7185/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7185/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer FFuz [1/3]"
                    },
                    "comment": {
                        "value": "We appreciate your time and efforts in reviewing our paper. Please find our responses below to your concerns.\n\n> W1.1. The presentation and writing need some good work to make the paper more readable and clear.\n\nWe are committed to making our paper clearer and would appreciate it if you could help us improve the writing by pointing to us the specific places where we should make it clearer.  For your other comments in W1, we have tried to address some of your concerns in W1 in our responses W1.2 and W1.3 below.\n\n> W1.2. The citation list is almost the same length as the paper itself.\n\nWe\u2019d like to clarify that, for a new and crucial topic like the expressivity of interpretable GNNs, it is usually unavoidable to have connections with many other topics and applications. As discussed in our paper, the expressivity of interpretable GNNs has a close relation with faithful interpretation and reliable generalization and has many applications such as interpretable scientific discovery. **We aim to provide an integrated context by citing the most related papers for readers to better understand the position of our work in the literature**. \n\nIn fact, we also provide a most concise context in the main paper, and leave the complete version in the appendix for interested readers who are not very familiar with the relevant literature. Our practice of citation follows the convention of the literature such as [1,2,3]. Nevertheless, we are open to any suggestions you feel would improve the readability of our paper."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7185/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700332073399,
                "cdate": 1700332073399,
                "tmdate": 1700332177721,
                "mdate": 1700332177721,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eUpuk9kzPO",
                "forum": "dVq2StlcnY",
                "replyto": "MX0XfCd4yR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7185/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7185/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer FFuz [2/3]"
                    },
                    "comment": {
                        "value": "> W1.3. The experiment set-up is actually quite complicated as detailed in appendix E.\n\nWe\u2019d like to clarify that **we did not extensively search all combinations** of the implementation options as given in Appendix E. We chose to provide the complete list of the implementation choices that we have tried in Appendix E, in order to provide an integrated context to readers of what has been carried out in the research. We believe the discussion of successes and failures in implementation could help readers better understand and improve our work in the future.\n\nIn fact, **it does not require an exhaustive search of all combinations** of the implementation options as given in Appendix E, as GMT separates the training into two stages: i) subgraph extractor training; ii) classifier training:\n\ni) The warmup strategies and the hyperparameters in Appendix F.2 are mainly used in training the subgraph extractor. In the experiments, we follow a trial-and-error setting. \n- Since warmup strategies are mainly designed to avoid the trivial solution, it can be inspected with a few runs from the validation performance whether the original training without warmup will stuck in the trivial solutions. \n- We will only use the warmup strategies together (i.e., a larger initial $r$, a larger penalty score, and single subgraph sampling) if we observe a performance degeneration in the validation set. Otherwise, we will stick to the original receipt. \n- In fact, we do not use warmups for most of the setups. Only in BA-2Motifs and MNIST-75sp with GIN, and in Tau3Mu with EGNN, we need the warmups. We have supplemented the details in Appendix E and F to avoid any misunderstandings.\n\nii) The sampling strategies are mainly used for the training of classifiers based on the subgraph extractor from stage i). **The hyperparameter tuning for classifier training is independent of stage i)**. One could simply select the best subgraph extractor based on the validation performance, and train the classifier onto the selected subgraph extractor. We consider the following 9 decoding strategies from the initialization, architecture, and attention perspectives:\n- Initialization: \"new\" refers to that the classifier is initialized from scratch; \"old\" refers to that the classifier is initialized from the subgraph extractor;\n- Architecture: \"mul\" refers to the default message passing architecture; \"lin\" refers to the GMT-lin architecture;\n- Attention: \"normal\" refers to the default weighted message passing scheme; \"min0\" refers to setting the minimum p% attention scores directly to 0; \"max0\" refers to setting the maximum p% attention scores directly to 1; \"min0max1\" refers to setting the maximum p% attention scores directly to 1 while setting the minimum (1-p)% attention scores directly to 0;\n\n|                |              |           | Generalization  |                 |                 | Interpretation  |                 |                 |\n|----------------|--------------|-----------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|\n| Initialization | Architecture | Attention | spmotif-0.5     | spmotif-0.7     | spmotif-0.9     | spmotif-0.5     | spmotif-0.7     | spmotif-0.9     |\n|                |              | GSAT      | 47.45+-5.87     | 43.57+-3.05     | 45.39+-5.02     | 74.49+-4.46     | 72.95+-6.40     | 65.25+-4.42     |\n| new            | mul          | min0      | **60.09+-5.57** | 54.34+-4.04     | **55.83+-5.68** | 85.50+-2.40     | **84.67+-2.38** | 73.49+-5.33     |\n| old            | mul          | min0      | 58.83+-7.22     | **55.04+-4.73** | 55.77+-5.97     | **85.52+-2.41** | 84.65+-2.42     | 73.49+-5.33     |\n| new            | mul          | max1      | 44.49+-2.65     | 49.77+-2.31     | 50.22+-2.79     | 85.50+-2.39     | 84.66+-2.37     | 73.50+-5.31     |\n| old            | mul          | max1      | 45.91+-2.86     | 49.11+-3.04     | 50.30+-2.07     | 85.49+-2.39     | 84.64+-2.39     | 73.50+-5.35     |\n| old            | mul          | min0max1  | 51.21+-6.46     | 50.91+-6.50     | 53.13+-4.46     | **85.52+-2.41** | 84.66+-2.43     | 73.49+-5.34     |\n| new            | mul          | normal    | 47.69+-5.72     | 44.12+-5.44     | 40.69+-4.84     | 84.69+-2.40     | 80.08+-5.37     | 73.48+-5.34     |\n| old            | mul          | normal    | 45.36+-2.65     | 44.25+-5.41     | 43.43+-5.44     | 83.52+-3.41     | 80.07+-5.35     | 73.49+-5.36     |\n| new            | lin          | normal    | 43.54+-5.02     | 47.59+-4.78     | 46.53+-3.27     | 85.47+-2.39     | 80.07+-5.37     | **73.52+-5.34** |\n| old            | lin          | normal    | 46.18+-3.03     | 46.42+-5.63     | 49.00+-3.34     | 83.51+-3.39     | 80.09+-5.34     | 73.46+-5.35     |"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7185/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700332100204,
                "cdate": 1700332100204,
                "tmdate": 1700332100204,
                "mdate": 1700332100204,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QeVQwi2gel",
                "forum": "dVq2StlcnY",
                "replyto": "MX0XfCd4yR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7185/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7185/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer FFuz [3/3]"
                    },
                    "comment": {
                        "value": "The table above shows the generalization and interpretation performance of GMT-sam in the spurious motif datasets, denoted as \"spmotif\" with different levels of spurious correlations. It can be found that **GMT-sam is generically robust to the different choices of the decoding scheme and leads to improvements with most setups**.\n\n\n> Q1. Why GMT perform less better on BA_2Motifs than Mutag?\n\nFirst, we\u2019d like to clarify that the SubMT results in Figures 2/6/7 all correspond to GMT-sam implementations. \nAlthough GMT already achieves two times higher counterfactual fidelity, we note that the empirical performance of GMT can be affected by the optimization of the network. As we have discussed there exists a trivial solution to the graph information bottleneck objective in GSAT, and multi-sampling strategies in GMT can more easily lead the optimization to the trivial solution in simple tasks like BA_2Motifs.\n\nTo better demonstrate the theoretical advance of SubMT and avoid the side effects of the optimization, we rerun the counterfactual fidelity experiments with GMT using warmup strategies. As shown in the revised Figures 2, 6, and 7, GMT can achieve even higher counterfactual fidelity than GSAT where GMT achieves 3 times larger counterfactual fidelity compared to GSAT on both BA_2Motifs and Mutag, as GMT approximates the SubMT better. \n\nPlease let us know if there are any outstanding questions, and we\u2019d sincerely appreciate it if you could jointly consider our responses above when making a final evaluation of our work.\n\n## References\n\n[1] How Neural Networks Extrapolate: From Feedforward to Graph Neural Networks, ICLR 2021.\n\n[2] In Search of Lost Domain Generalization, ICLR 2021.\n\n[3] Interpretable Geometric Deep Learning via Learnable Randomness Injection, ICLR 2023."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7185/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700332141054,
                "cdate": 1700332141054,
                "tmdate": 1700332141054,
                "mdate": 1700332141054,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8dplG2xw8x",
                "forum": "dVq2StlcnY",
                "replyto": "MX0XfCd4yR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7185/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7185/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A summary of response to Reviewer FFuz"
                    },
                    "comment": {
                        "value": "Dear Reviewer FFuz,\n\nWe once again appreciate your time and efforts in reviewing our paper. We have revised our manuscript to improve its clarity following your suggestions. In summary,\n- We discussed more clearly for the experimental setup and clarified that the hyperparameter search of GMT-sam has a similar complexity as previous works;\n- Supplementary to our ablation study, we also tested the sensitivity of GMT-sam regarding different subgraph sampling/decoding schemes, and found GMT-sam is generically robust to different;\n- We also discussed the use of warmup strategies more clearly that they mainly helped with escaping from the trivial solution in our experiments. With warmups, GMT-sam could better approximate SubMT and achieve better counterfactual fidelity.\n\nWe would appreciate it if you could take a look at our responses and let us know if any of your remaining concerns are not addressed, and we will try our best to address them."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7185/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700464477639,
                "cdate": 1700464477639,
                "tmdate": 1700464477639,
                "mdate": 1700464477639,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JJQUfjaYsJ",
                "forum": "dVq2StlcnY",
                "replyto": "MX0XfCd4yR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7185/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7185/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A gentle reminder for the closing rebuttal window"
                    },
                    "comment": {
                        "value": "Dear Reviewer FFuz,\n\nWe would like to remind you that the rebuttal will be closed **within 48 hours**. To allow us sufficient time to discuss with you your concerns about our work, we would appreciate it if you could take some time to read our rebuttal and give us some feedback. Thank you very much."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7185/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700569529134,
                "cdate": 1700569529134,
                "tmdate": 1700569529134,
                "mdate": 1700569529134,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tXiFEKIz6D",
                "forum": "dVq2StlcnY",
                "replyto": "MX0XfCd4yR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7185/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7185/Authors"
                ],
                "content": {
                    "title": {
                        "value": "The rebuttal window will be closed very soon"
                    },
                    "comment": {
                        "value": "Dear Reviewer FFuz,\n\nWe would like to remind you that the rebuttal will be closed in **less than 24 hours**. To allow us sufficient time to discuss with you your concerns about our work, we would appreciate it if you could take some time to read our rebuttal and give us some feedback. Thank you very much!"
                    }
                },
                "number": 26,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7185/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700659767983,
                "cdate": 1700659767983,
                "tmdate": 1700659871858,
                "mdate": 1700659871858,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vAY0erEN3p",
                "forum": "dVq2StlcnY",
                "replyto": "MX0XfCd4yR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7185/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7185/Authors"
                ],
                "content": {
                    "title": {
                        "value": "The rebuttal window is closing very soon"
                    },
                    "comment": {
                        "value": "Dear Reviewer FFuz,\n\nWe would like to remind you that the rebuttal will be closed in **less than 8 hours**. To allow us a last chance to discuss with you any of your remaining concerns about our work, we would appreciate it if you could take some time to read our rebuttal and give us some feedback. Thank you very much!"
                    }
                },
                "number": 30,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7185/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700711801570,
                "cdate": 1700711801570,
                "tmdate": 1700711801570,
                "mdate": 1700711801570,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]