[
    {
        "title": "Coordinate-Aware Modulation for Neural Fields"
    },
    {
        "review": {
            "id": "KAkMJJxODP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3192/Reviewer_DCsj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3192/Reviewer_DCsj"
            ],
            "forum": "4UiLqimGm5",
            "replyto": "4UiLqimGm5",
            "content": {
                "summary": {
                    "value": "This main contribution of this paper is to use a grid-based approach to provide a scale and bias for the features generated at each layer of an implicit neural network, an approach used to encode any kind of signal. This is in contrast to the typical approach where the grid-based approach provides an input to the implicit neural network. In other words, the proposed approach is: a) use the input coordinate to recover the scale and bias for all layers of the MLP, b) recursively apply each layer of the network, normalize features, apply scale and bias for the appropriate layer. The authors also discuss which subset of input coordinates should be used to define the grid. Other contributions of the paper include the use of the aforementioned feature normalization in implicit neural representation and a comparison to various benchmarks on image encoding and generalization, novel view synthesis in static and dynamic NERF and video compression."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Originality:\n* As far as I know, use a grid-based approach for scale/bias in implicit neural representation is new.\n\nQuality:\n* Evaluation is performed on several different tasks, with good results, thus I believe the strength of the method is demonstrated.\n* The baselines used are generally competitive and recent.\n\nClarity:\n* The paper is well written.\n\nSignificance:\n* good results on multiple tasks, state-of-the-art in some."
                },
                "weaknesses": {
                    "value": "Quality:\n* I find the use of FFN (Tancik, 2020) as the only baseline in the image task disappointing. While not the most significant experiment of the paper, I think the use of more recent baselines and in particular of other grid-based approaches, for example at least instant-NGP, would make the comparison on images more significant. I also want to add that I find the baselines in the video experiment to be adequate, and arguably the video experiment is more important.\n\nClarity:\n* I might have missed it, but for me, the paper does not sufficiently discuss/explain the reasoning behind the choice of coordinates that are used in the grid to select a scale and bias. For example, the scale/bias depends on the pixel coordinates only for a picture. As far as I understand, scale/bias are thus the same for each channel. But for the video, the scale/bias depend on pixel coordinates, time and channel. I do not understand why the channel becomes important for the video. I noticed the ablation study (Table 6), but it only covers the NERF experiments. This ablation study also does not discuss making the scale/bias depend on both direction and time.\n* I do not understand what is the variance represented in Figure 8. Is it the variance between the elements of the input of the last layer of the MLP? \n* I find the notation in equation 1 and subsequent equations a bit confusing. Both $\\gamma_n$ and $\\beta_n$ takes as input the full batch $X$. This suggests that any element of the vectors $\\gamma$ and $\\beta$ may depend on the full batch $X$. I suspect this is not the case due to how grid based approaches typically work but I am not sure. Would it make sense to change the notation to $\\gamma(X_n;\\Gamma)$ or  $\\gamma(X^{(n)};\\Gamma)$, to be closer to the notation already used in equation 4?"
                },
                "questions": {
                    "value": "* On page 8, the paper states that HM decodes at 10fps using a GPU. I was very surprised, because, as far as I know, HM does not use a GPU. I also could not find a mention of this fact in (Hu et al., 2023). While it is common to compare decoding speed on different hardware (CPU for traditional codec, GPU for ML methods), I think it is misleading to state that HM uses GPU. Could you please comment on/clarify this?\n* Could you please comment on the choice of coordinates used in the grid to define scale/bias?\n* Could you please provide further explanation about Figure 8?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3192/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3192/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3192/Reviewer_DCsj"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3192/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697446481202,
            "cdate": 1697446481202,
            "tmdate": 1700671187353,
            "mdate": 1700671187353,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8v9p8ejPjf",
                "forum": "4UiLqimGm5",
                "replyto": "KAkMJJxODP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3192/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3192/Authors"
                ],
                "content": {
                    "title": {
                        "value": "(1/2) Thank you for all the invaluable comments and would like to respond individually."
                    },
                    "comment": {
                        "value": "We sincerely appreciate all insightful comments and efforts in reviewing our manuscript. All the reviewers acknowledged the innovative and impactful approach proposed in our paper, supported by extensive experiments. We intend to respond to each of your comments one by one, offering thorough explanations and clarifications as required.\n\n**[W1] More baseline (instant-NGP) for image tasks**\n\nFollowing the reviewer's suggestion, we assessed I-NGP for our image tasks. While I-NGP demonstrated impressive results in Natural image regression, highlighting its superiority in overfitting, it fell short in terms of image generalization. In contrast, CAM consistently shows high performance in both tasks, demonstrating its overall effectiveness. We will include this result in the upcoming draft.\n\n| Method | #Params | Natural (Reg.) | Text (Reg.) | Natural (Gen.) | Text (Gen.) |\n|:------:|---------|:--------------:|:-----------:|:--------------:|:-----------:|\n|  I-NGP | 237K    |      32.98     |    41.94    |      26.11     |    32.37    |\n| FFN    | 263K    |      30.30     |    34.44    |      27.48     |    30.04    |\n| + CAM  | 266K    |      32.21     |    50.17    |      28.19     |    33.09    |\n\n**[W2-1, Q2] Clarification of the choice for modulation parameters**\n\nCAM is applied not based on the signal's property but on the input coordinates. Although video signals contain spatial components, frame-wise methods such as FFNeRV only take time as the input coordinate to represent each corresponding video frame. This has been demonstrated as an efficient solution in terms of accuracy and training/inference speed, as opposed to incorporating both spatial and time coordinates (pixel-wise methods) in video representation literature. In our experiments, we adopted this more practical and applicable baseline and used the input time coordinate to determine the modulation parameters, showing a significant improvement over the baseline. Given that the time coordinate is the only option to determine the modulation parameters in this context, our ablation study presented in Table 6 did not include the video experiment. We would like to claim that this result highlights the wide and general application of CAM.\n\n**[W2-2] CAM based on both direction and time.**\n\nAs described in Section 3.0, our approach involves assigning priority to input coordinates (when input coordinates include various components) for modulation parameters to avoid the curse of dimensionality. The prioritization is arranged in the following order: time, view direction, and space. This specific ordering is based on experimental results from tasks that involve several coordinate components, such as NeRF and dynamic NeRF, with the results detailed in Table 6. Furthermore, we have conducted additional evaluations of CAM regarding both direction and time coordinates in the following table. Although it increases performance compared to the baseline, using a single prioritized component demonstrates the most efficient result. This result will be included in the updated version.\n| S | D | T | #Params |  PSNR |\n|:-:|---|:-:|:-------:|:-----:|\n|   |   |   |   0.6M  | 32.22 |\n| V |   |   |  13.1M  | 32.57 |\n|   | V |   | 0.6M    | 32.44 |\n|   | V | V | 0.6M    | 32.49 |\n|   |   | V | 0.6M    | 33.78 |\n\n**[W3, Q3] Description of the variance in Figure 8**\n\nThe variance in Figure 8 denotes the mean of the variance of all pixels at the same channel. Formally, the variance $v$ of $H\\times W$ pixel-wise $C$-channel features $X \\in \\mathbb{R}^{C\\times H\\times W}$ can be expressed as follows, \n\n$v = {1\\over C}\\sum_{ch=1}^{C} var^{(H,W)}(X_{ch}),$\n\nwhere $var^{(H,W)}(\\cdot):\\mathbb{R}^{H\\times W} \\rightarrow \\mathbb{R}$ computes the variance of $H\\times W$ values and $X_{ch}\\in\\mathbb{R}^{H\\times W}$ is features at the channel $ch$. We would like to clarify the information in the revised draft."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3192/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700499433084,
                "cdate": 1700499433084,
                "tmdate": 1700499433084,
                "mdate": 1700499433084,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jlsv8M3qTB",
                "forum": "4UiLqimGm5",
                "replyto": "KAkMJJxODP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3192/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3192/Authors"
                ],
                "content": {
                    "title": {
                        "value": "(2/2)"
                    },
                    "comment": {
                        "value": "**[W4] Clarification of notation**\n\nIn our paper, we have described the modulating parameter of MLP's intermediate features as $\\gamma(X;\\Gamma)$ (and $\\beta(X;B)$), where the function $\\gamma(\\cdot;\\Gamma):\\mathbb{R}^{N\\times D} \\rightarrow \\mathbb{R}^{N}$ and the input coordinate $X\\in\\mathbb{R}^{N\\times D}$, with the dimension of the input coordinates $D$. However, we overlooked specifying $\\gamma_n(X;\\Gamma)$ as the scale value for batch $n$ in Equations 1 and 2, even though we precisely mention 'the scalars $\\gamma_n(X; \\Gamma)$ and $B_n(X; B)$ denote the scale and shift value, respectively, for ray $n$' in Equation 4.\n\nThe notation $X^{(\\cdot)}$ denotes the subset of input coordinates based on our proposed priority. For example, this could be the view-direction coordinate $X^{(\\phi, \\theta)}$ in NeRFs or the time coordinate $X^{(t)}$ in dynamic NeRFs. Given that image and video representations involve only spatial and time coordinates, respectively, we use the complete input coordinate by denoting it as $X$.\nWe would like to revise and provide clarity on these aspects in the revised version of our draft.\n\n**[Q1] GPU usage of HM**\n\nWe extend our sincere apologies for the incorrect statement that 'HM decodes at 10fps using a GPU'. Upon the reviewer's correct observation, we realize our misinterpretation of the report by Hu et al.[1] and acknowledge that HM natively supports only CPU usage, barring any specialized implementation. This error would be rectified in the revised version of our draft.\n\n[1] Hu, Z., Xu, D., Lu, G., Jiang, W., Wang, W., & Liu, S. (2022). FVC: An end-to-end framework towards deep video compression in feature space. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(4), 4569-4585."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3192/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700499781335,
                "cdate": 1700499781335,
                "tmdate": 1700499781335,
                "mdate": 1700499781335,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iBljTCOuRI",
                "forum": "4UiLqimGm5",
                "replyto": "jlsv8M3qTB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3192/Reviewer_DCsj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3192/Reviewer_DCsj"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the update"
                    },
                    "comment": {
                        "value": "Thank you for the update. I think my comments have all been addressed by the author. I think the paper looks better now. I will update my score.\n\n**W1**\n\nThank you for the additional experiment. The results look good.\n\n**W2**\nThank you for the clarification and additional experiment. The results look quite nice and convincing.\n\n**W3**\nVery clear, thank you.\n\n**W4**\nThank you for the explanation.\n\n**Q1**\nThank you for the clarification"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3192/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700671146728,
                "cdate": 1700671146728,
                "tmdate": 1700671146728,
                "mdate": 1700671146728,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "oAsCHnrvEG",
            "forum": "4UiLqimGm5",
            "replyto": "4UiLqimGm5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3192/Reviewer_7EJV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3192/Reviewer_7EJV"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes combining grid input and MLP structure at intermediate features level for Neural fields application. This naturally extends MLP-only or grid-only methods, which despite its competitive performance have had downsides, such as not being able to represent high-frequency content or being computationally intensive."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- The biggest strength of CAM is its simplicity and its plug-and-play nature. I believe this will have much far-reaching impact in the Neural fields literature, compared to other highly sophisticated & implementation-heavy frameworks designed to maximize PSNR value at all costs. Similar to how widely Batch/layer-normalization has been used by the entire field.\n- Extensive experimentation on diverse tasks and against different baselines add to the credibility of the work."
                },
                "weaknesses": {
                    "value": "- While there are no specific weaknesses to point out, I don't think Figure 1 or Figure 2 convey the idea that well. Figure 1 probably will be better served by displaying more detailed mechanism (exammple of x, example of the values for \\Gamma, etc.).\n- Also giving a brief description of what functional form \\Gamma and B take would be informative for readers."
                },
                "questions": {
                    "value": "Don't have specific questions"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3192/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3192/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3192/Reviewer_7EJV"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3192/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698613507467,
            "cdate": 1698613507467,
            "tmdate": 1699636267054,
            "mdate": 1699636267054,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yO2yYdDZJm",
                "forum": "4UiLqimGm5",
                "replyto": "oAsCHnrvEG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3192/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3192/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for all the invaluable comments and would like to respond individually."
                    },
                    "comment": {
                        "value": "We sincerely appreciate all insightful comments and efforts in reviewing our manuscript. All the reviewers acknowledged the innovative and impactful approach proposed in our paper, supported by extensive experiments. We intend to respond to each of your comments one by one, offering thorough explanations and clarifications as required.\n\n**[W1] More informative figure**\nWe appreciate the reviewer's thoughtful comment on our main figure. We revised Figure 1, as shown in [Link](https://raw.githubusercontent.com/anony3192/anony3192/main/figure1_rev.png). We would like to include this figure in the updated draft.\n\n**[W2] Brief description of the functional form $\\Gamma$ and $B$**\n\n$\\Gamma$ and $B$ are grid structures to represent the scale and shift factors, where each grid is trained as a function of coordinates with infinite resolution, outputting coordinate-corresponding components.\nThe output in infinite resolution is aggregated by nearby features in the grid based on the distance between the coordinates of the input and neighboring features. We would like to include this in the appendix of the upcoming draft."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3192/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700498764042,
                "cdate": 1700498764042,
                "tmdate": 1700498764042,
                "mdate": 1700498764042,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FdD7DJnqNg",
            "forum": "4UiLqimGm5",
            "replyto": "4UiLqimGm5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3192/Reviewer_kCkz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3192/Reviewer_kCkz"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduced a coordinate-aware modulation module that combines MLP features and grid representations for neural fields. Unlike the popular methods that chain the features, this new method not only preserves the strengths of MLP but also mitigates the bias problem by leveraging grid-based representation. The authors conducted experiments on tasks in multiple domains and the results demonstrate its capability of modeling high-frequency components and advantages over prevalent neural field features."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The motivation of the paper was clearly stated \n- The proposed approach is simple yet effective\n- The paper is well structured and the idea is easy to follow \n- Experiments are comprehensive. It covers various domains such as images, videos, etc."
                },
                "weaknesses": {
                    "value": "- The numbers of the baseline models seem to be from the authors' own implementation, which makes it less appealing"
                },
                "questions": {
                    "value": "- Could you answer the first question I posted in the \"Weaknesses\" section?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3192/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698881136603,
            "cdate": 1698881136603,
            "tmdate": 1699636266988,
            "mdate": 1699636266988,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5TRFSq2ZqY",
                "forum": "4UiLqimGm5",
                "replyto": "FdD7DJnqNg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3192/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3192/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for all the invaluable comments and would like to respond."
                    },
                    "comment": {
                        "value": "We sincerely appreciate all insightful comments and efforts in reviewing our manuscript. All the reviewers acknowledged the innovative and impactful approach proposed in our paper, supported by extensive experiments. We intend to respond to your comment, offering thorough explanations and clarifications as required.\n\n**[W1, Q1] Baseline model with our own implementation**\n\nFor our experiments, we utilized the official codes for all baseline methods, including FFNeRV, NerfAcc, Mip-NeRF, and Mip-NeRF 360, and reported the values in the original papers. The only exception was FFN, which was originally developed in Jax a few years back. Due to its older environment, we opted for a Pytorch implementation to simplify the experimental process. We followed all the original settings of FFN, and successfully reproduced the reported value (1.9 PSNR higher for the Natural dataset and 0.4 lower for the Text dataset). It's important to note that, apart from the integration of CAM, all other configurations in our experiment were exactly the same. This consistency underscores the effectiveness of CAM in a clear and unbiased manner. If we understand your question correctly, we would like to include the reported values in the original paper and clarify this."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3192/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700498600273,
                "cdate": 1700498600273,
                "tmdate": 1700498600273,
                "mdate": 1700498600273,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mwBsN9SkoN",
            "forum": "4UiLqimGm5",
            "replyto": "4UiLqimGm5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3192/Reviewer_oV38"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3192/Reviewer_oV38"
            ],
            "content": {
                "summary": {
                    "value": "Authors propose a new architecture for neural fields, i.e mapping low-dimensional input co-ordinates to the signal values, called CAM (co-ordinate aware modulation). The main idea is to modulate intermediate features using scale and shift parameters which are inferred from the low-dimensional input co-ordinates. Authors show that, while regular a regular MLP shows heavy spectral bias, and just grid representation is computationally very expensive, CAM can mitigate the spectral bias learning high frequency components, while also being compact. In addition, CAM facilitates adding normalization layers which improves training stability. Authors empirically show that CAM achieves competitive results image representation, novel view synthesis, and video representation tasks, while being fast and very stable to train."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ Authors are tackling a very relevant problem, with wide interest to practitioners. \n+ Paper is well written and easy to follow. \n+ Claims in the paper are sounds. I particularly like that the argument about spectral bias and not learning high frequency components is verified empirically in Section 4.3 \n+ Experiments are sound and covers a wide range of tasks. Results are strong with performance comparable or exceeding the state of the art."
                },
                "weaknesses": {
                    "value": "+ While the paper is generally strong, I believe that it lacks certain references which can put the work in a better context. There is a long history of using feature modulation in deep learning. A good example is [FiLM](https://arxiv.org/abs/1709.07871). This is also used for image generation/reconstruction tasks like in [generation](https://arxiv.org/abs/1810.01365), [denoising](https://arxiv.org/pdf/2107.12815.pdf), [image restoration](https://arxiv.org/abs/1904.08118) and [style transfer](https://arxiv.org/abs/1705.06830). Adding these references, and including a discussion around it can put feature modulation in a better context. \n\n+ Can authors include inference speed/inference memory requirements to put regular MLP methods, grid based method, and CAM in prospective? \n\n+ The choice of not using the all lower dimensional inputs to infer the scale and shift parameters, but a subset based on the problem is interesting. Have you conducted ablation studies that this is in some way beneficial? \n\nA bit tangential but:\n+ Do you think CAM can benefit decoder MLP for a triplane based representation as well? It would be cool to see some experiments and demonstrate the generality here. \n+ In addition, I think text to 3D is one another domain where the speed and training stability of CAM can benefit quite a lot. If authors can demonstrate a couple of results comparing the training stability and speed using CAM augmented MLP in DreamFusion, that would be a great additon to the paper."
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3192/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698953912807,
            "cdate": 1698953912807,
            "tmdate": 1699636266900,
            "mdate": 1699636266900,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WBvBTrAUfT",
                "forum": "4UiLqimGm5",
                "replyto": "mwBsN9SkoN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3192/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3192/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for all the invaluable comments and would like to respond individually."
                    },
                    "comment": {
                        "value": "We sincerely appreciate all insightful comments and efforts in reviewing our manuscript. All the reviewers acknowledged the innovative and impactful approach proposed in our paper, supported by extensive experiments. We intend to respond to each of your comments one by one, offering thorough explanations and clarifications as required.\n\n\n**[W1] More references**\n\nWe are grateful for the reviewer's insightful suggestion and would like to include the mentioned references for a more thorough discussion in our work.\n\n\n\n**[W2] Inference speed and memory**\n\nWe report the inference speed and GPU memory requirements of the MLP-based method (NerfAcc), grid-based method (K-Planes), and CAM, evaluated on the 'Mic' scene. K-Planes requires small memory while showing slow inference. CAM reduces the original NerfAcc's speed when testing chunk size is small. However, increasing the testing chunk size reduces the speed gap between using CAM and not using it. Intriguingly, CAM even lowers memory usage under these conditions. We interpret that CAM facilitates a more effectively trained occupancy grid and helps bypass volume sampling, offsetting the additional computational demands introduced by CAM itself.\n\n|  Method  | Test chunk |  PSNR | Inf. FPS | Inf. Mem. |\n|:--------:|:----------:|:-----:|:--------:|:---------:|\n| K-Planes |      -     | 34.10 |   0.25   |   3.8 GB  |\n|  NerfAcc |    1024    | 33.77 |   0.51   |   4.4 GB  |\n|   +CAM   |    1024    | 36.03 |   0.26   |   4.7 GB  |\n|  NerfAcc |    4096    | 33.77 |   1.19   |  10.5 GB  |\n|   +CAM   |    4096    | 36.03 |   0.67   |   8.8 GB  |\n|  NerfAcc |    8192    | 33.77 |   1.45   |  19.5 GB  |\n|   +CAM   |    8192    | 36.03 |   1.01   |  16.4 GB  |\n\n\n\n**[W3] Ablation on the choice for modulation parameters**\n\nAs described in Section 3.0, our approach involves assigning priority to input coordinates for modulation parameters to avoid the curse of dimensionality. The prioritization is arranged in the following order: time, view direction, and space. This specific ordering is based on experimental results from tasks that involve several coordinate components, such as NeRF and dynamic NeRF, with the results detailed in Table 6. Furthermore, we have conducted additional evaluations of CAM regarding both direction and time coordinates in the following table. Although it increases performance compared to the baseline, using a single prioritized component demonstrates the most efficient result.\n| S | D | T | #Params |  PSNR |\n|:-:|---|:-:|:-------:|:-----:|\n|   |   |   |   0.6M  | 32.22 |\n| V |   |   |  13.1M  | 32.57 |\n|   | V |   | 0.6M    | 32.44 |\n|   | V | V | 0.6M    | 32.49 |\n|   |   | V | 0.6M    | 33.78 |\n\n\n\n**[W4] CAM on tri-plane representation**\n\nWhen applied CAM to a tri-plane method (K-Planes), there is no significant improvement observed (from 34.10 to 34.12 in the 'Mic' scene). This is because general grid-based methods avoid spectral bias by utilizing grids with high resolution and large channels, where the decoder MLP primarily serves to aggregate these grid features. This is the reason for the substantial memory demands of grid-based methods. In contrast, CAM represents a novel approach to mitigate spectral bias, achieving this with notably compact parameters.\n\n\n\n**[W5] Application for DreamFusion**\n\nWe have applied CAM to the official DreamFusion codebase, observing some impact on the outcome, as shown in Figure [Link](https://raw.githubusercontent.com/anony3192/anony3192/main/figure.png). However, due to time limitations, we were unable to conduct a more in-depth exploration. Exploring this area further, we believe, would constitute an interesting direction for future research."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3192/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700498407400,
                "cdate": 1700498407400,
                "tmdate": 1700498407400,
                "mdate": 1700498407400,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]