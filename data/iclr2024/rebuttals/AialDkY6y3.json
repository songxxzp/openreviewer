[
    {
        "title": "Deep Graph Predictions using Dirac-Bianconi Graph Neural Networks"
    },
    {
        "review": {
            "id": "ty95moL913",
            "forum": "AialDkY6y3",
            "replyto": "AialDkY6y3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4015/Reviewer_3z7c"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4015/Reviewer_3z7c"
            ],
            "content": {
                "summary": {
                    "value": "Edit: I have increased my score in light of your responses.\n\nIt is has been noted that the message passing operations in standard GNNS are essentially equivalent to heat propagation over the network. This can lead to desirable smoothing on relatively shallow networks. However, letting heat propagation proceed for too long leads to oversmoothing and loss of information. Therefore, the authors instead proposed an alternative approach based on the Dirac-Biaconi equation.\n\nThe authors cite as an example GNNs aiming to predict the stability of power grids. In this setting 13 layers are needed. It is also to treat nodes and edges as equally important.\n\nThe author utilized the Dirac-Bianconi operator which was previously introduced (in the graph setting) by Bianconi in 2021. They derive message passing type operators which pass information from edges to nodes and vice versa. From there they derive graph dynamics and a layerwise update rule which discretizes the time domain and adds in learnable weight matrices.\n\nNetwork architecture consists of linear layer, then use DB layers with skip connections followed by a final MLP. They demonstrate good results compared to a couple of baselines in limited experiments."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Going beyond the vanilla MPNN approach and avoiding related oversmoothing problems is important and the authors provide an interesting setting, optimizing power grids, for needing to go deeper than standard DNNs. The approach is both well motivated from physics and novel in the context of GNNs.\n\nThe paper is generally well-written and well-explained."
                },
                "weaknesses": {
                    "value": "The alternating procedure of DB and linear layers could be more clearly explained.  What does a linear layer mean in this context and why are they needed\n\nNeither of the datasets considered inlcude non-trivial edge features. Perhaps you could have flux along an edge in a powergrid or something?\n\nNot clear how the baseline methods where chosen? Are these also methods meant to address oversmoothing and long range dependencies? Are they at or near s-o-t-a?\n\nx' and e'_{i,j} are not defined in equations 6 and 7. I am assuming they mean the output of delta_{DB}(x, e), but this should be made more clear\n\nThe setup assumes a symmetry on the edge features e_{j,i} = e_{i,j}\n\nThe leap from 8 to 9 should be made more clear. This appears to solving \\partial_t - \\partial_{DB} \\pm \\beta =0, or something but this should be made explicit. \n\nTable 2 is interesting, that one DBGNN layer = 3 GCN layers, but it would be more convincing if you also showed results with DBGNN outpeforming GCN in addition to other baselines"
                },
                "questions": {
                    "value": "Is there any connection by the DB equation and the wave equation?\n\nIs there any way to use the DB operator to define Riesz transforms ($\\partial_{x_i}\\Delta^{-1/2}$ in the Euclidean setting)?\n\nWhere does the network include non-linearitiees? Why do these not induce loss of energy? \n\nWhat is the computational complexity of DBGNN vs other methods?\n\nPaper could be improved via more thorough experimentation and also analysis of / references too the theoretical properties of the DB operator."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4015/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4015/Reviewer_3z7c",
                        "ICLR.cc/2024/Conference/Submission4015/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4015/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698344403566,
            "cdate": 1698344403566,
            "tmdate": 1700334630035,
            "mdate": 1700334630035,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YUr8Owsdwu",
                "forum": "AialDkY6y3",
                "replyto": "ty95moL913",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4015/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4015/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer 3z7c - Part 1"
                    },
                    "comment": {
                        "value": "```\nThe alternating procedure of DB and linear layers could be more clearly explained. What does a linear layer mean in this context and why are they needed\n```\n\nLinear layer simply refers to a learnable linear weight matrix of an appropriate dimension. We use them here for skip connections, that is, after running the DB dynamics for some time we allow the DBGNN to mix the features into the hidden dimensions again. This is expected to improve the ability of the system to go deep. We will expand on this feature further in the manuscript.\n\nDoes this address the reviewers concerns, or were they also asking for clarification of other aspects of the DBGNN?\n\n```\nNeither of the datasets considered inlcude non-trivial edge features. Perhaps you could have flux along an edge in a powergrid or something?\n```\n\nWe experimented with using the power flow on the edges as additional input features, but it did not improve the performance. See Reply to all Reviewers - Further Datasets.\n\n```\nNot clear how the baseline methods where chosen? Are these also methods meant to address oversmoothing and long range dependencies? Are they at or near s-o-t-a?\n```\n\nWe have chosen the best performing models on the datasets as baselines, so we consider them state of the art. ARMANet and TAGNet, which perform best for the power grid task, share some features with architectures designed specifically to avoid oversmoothing, and are iterated quite deeply to achieve their performance. The DBGNN layer that beats them here is comparatively simpler, at least from a dynamics point of view.\n\n```\nx' and e'{i,j} are not defined in equations 6 and 7. I am assuming they mean the output of delta{DB}(x, e), but this should be made more clear\n```\n\nThank you for pointing this out, you are right, we will clarify this point.\n\n```\nIs there any connection by the DB equation and the wave equation?\n```\n\nThis is an excellent question. The Dirac equation is a wave equation, however, it includes an overall factor $i$ in front of the right hand side. As we omit this factor, the equation we use is not directly a wave equation. Instead, we have exponentially growing and shrinking modes. We will clarify this point in the manuscript.\n\n```\nThe leap from 8 to 9 should be made more clear. This appears to solving \\partial_t - \\partial_{DB} \\pm \\beta =0, or something but this should be made explicit.\n```\n\nEquations 8 and 9 together are the Bianconi's topological Dirac equation written out explicitly in coordinates. We are not solving anything, edges and nodes evolve at the same time. We will clarify this point.\n\n```\nTable 2 is interesting, that one DBGNN layer = 3 GCN layers, but it would be more convincing if you also showed results with DBGNN outpeforming GCN in addition to other baselines\n```\n\nThe 3 layer GCN is the state of the art for this task, see Reply to all Reviewers - Baselines for binding affinity. We are investigating further Datasets to strengthen the results of the paper, see Reply to all Reviewers - Further Datasets.\n\n```\nIs there any way to use the DB operator to define Riesz transforms (in the Euclidean setting)? \n```\n\nThis is an interesting question, we do not know. Generally speaking, Dirac operators tend to require internal spaces and do not directly operate on real valued functions. We expect that any connection would be subtle.\n\n```\nWhere does the network include non-linearitiees? Why do these not induce loss of energy?\n```\n\nWe include a ReLu after every step of the dynamics. It is unclear to us why a non-linearity should be expected to induce a loss of Dirichlet energy. We will update the section on Dirichlet energy to better represent typical conditions in the layer, in which case we do see a gradual decline, see Reply to all Reviewers - Dirichlet Energy.\n\n```\nWhat is the computational complexity of DBGNN vs other methods?\n```\n\nThe time complexity of the forward pass is the same as that of Message Passing Neural Networks. The main difference to these, is the fact that we have the edges as full updating states of the layer iteration. This does not increase the computational complexity. We will include a small note in the manuscript to this effect."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4015/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700330784350,
                "cdate": 1700330784350,
                "tmdate": 1700330784350,
                "mdate": 1700330784350,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WwcfWxepe0",
                "forum": "AialDkY6y3",
                "replyto": "YUr8Owsdwu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4015/Reviewer_3z7c"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4015/Reviewer_3z7c"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for answering my questions"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4015/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700679832834,
                "cdate": 1700679832834,
                "tmdate": 1700679832834,
                "mdate": 1700679832834,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "z1iZveUml5",
            "forum": "AialDkY6y3",
            "replyto": "AialDkY6y3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4015/Reviewer_NtG7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4015/Reviewer_NtG7"
            ],
            "content": {
                "summary": {
                    "value": "Summary:\nThe paper presents a new graph neural network layer called the Dirac-Bianconi Graph Neural Network (DBGNN). The layer is inspired by the topological Dirac equation on graphs proposed by Bianconi (2021). In DBGNN, the edges and nodes are treated on an equal footing, with features attached to both and the Dirac operator mixes edge and node features. Additionally, DBGNN avoids over-smoothing problem that affects traditional GNNs like GCNs. \n\nThe DBGNN layer implements a discretized, generalized version of the Dirac-Bianconi equation. Multiple DBGNN layers with shared weights are stacked to enable propagation across longer distances in the graph.\n\nThe DBGNN is evaluated on two tasks: i) Predicting power grid stability. (ii) Predicting protein-ligand binding affinity \nIn (i) it outperforms previous approaches, especially for out-of-distribution generalization. While in (ii), DBGNN achieves par with deeper GCNs.\n\nIn summary, this work presents a new GNN layer that can be helpful for long range tasks in graphs with some demonstration shown in the paper."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Strengths:  \n(i) well motivated combination of ideas from physics and GNNs. The Dirac equation is a natural model for directional propagation on graphs.  \n(ii) good empirical results on two relevant graph tasks. Outperforms prior models on power grids.   \n(iii) experiments analyzing model dynamics and over-smoothing."
                },
                "weaknesses": {
                    "value": "Weaknesses and questions:  \ni) Limited ablation studies. It can be made further clear for how much performance gain comes from Dirac structure vs other enhancements.  \nii) the paper does not exploit edge features on binding affinity task. It may be unsure if Dirac structure helps here.  \niii) although one of the experimental task is related to long range dependency, it is not sure for the second task. A robust study on some long range dataset benchmark would be required to verify holistically the claims of the model on long range tasks as is mentioned in multiple instances in the paper. The paper would be strengthened by a more in-depth analysis of what long-range effects the model is capturing in this domain.  \niv) For binding affinity task, did you try using edge features? Does the Dirac structure help in that case?"
                },
                "questions": {
                    "value": "above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4015/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698492982789,
            "cdate": 1698492982789,
            "tmdate": 1699636363999,
            "mdate": 1699636363999,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Qy3gjCAlcC",
                "forum": "AialDkY6y3",
                "replyto": "z1iZveUml5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4015/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4015/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer NtG7"
                    },
                    "comment": {
                        "value": "```\ni) Limited ablation studies. It can be made further clear for how much performance gain comes from Dirac structure vs other enhancements.\n```\n\nIt is unclear to us what the reviewer views as important other enhancements. DBGNN is a relatively simple layer. It is the iterated application of equations (11) and (12), which are the direct analogies of the Dirac Bianconi equation, followed by a skip connection. We have added a direct comparison to a simple MPNN to clarify this, and we now show that the equivalent MPNN does not have comparable dynamics or long range feature propagation (Reply to all Reviewers - Evidence for long range Capabilities).\n\nA more detailed study on how performance varies with various hyperparameters would, of course, be highly interesting. We also believe the layer to be highly compatible with the vast array of architectural enhancements that have been explored for MPNNs and GCNs. We think such broad questions are beyond the scope of this initial paper, though, and we decided to focus our experimental resources elsewhere.\n\n```\nii) the paper does not exploit edge features on binding affinity task. It may be unsure if Dirac structure helps here.\n```\n\nIt is important to note that in the absence of edge input features, we still have a non-trivial hidden edge dynamics. Thus, the way the nodal information propagates is shaped by the Dirac structure in this case, too. We will clarify this point in the paper.\n\nThus we consider the competitive performance on this task a strength of the layer. It shows that the underlying dynamics is interesting. We assume that useful edge features can only help to get higher performances. We are also exploring further datasets, see Reply to all Reviewers.\n\n```\niii) although one of the experimental task is related to long range dependency, it is not sure for the second task. A robust study on some long range dataset benchmark would be required to verify holistically the claims of the model on long range tasks as is mentioned in multiple instances in the paper. The paper would be strengthened by a more in-depth analysis of what long-range effects the model is capturing in this domain.\n```\n\nWe agree that a much broader study would be needed. We have extended the analysis beyond the Dirichlet energy by including example trajectories of a randomly initialized layer. See Reply to all Reviewers - Evidence for long-range capabilities.\n\nWe also would like to point out that the power grid task is known to require long-range capabilities (Ringsquandl et al. (doi: 10.1145/3459637.3482464)), and that the state-of-the-art models (ARMANet and TAGNet) are quite deep. Together with the evidence from randomly initialized layers, we consider the conclusion reasonable that DBGNNs are good for long-range dependencies.\n\nAt the same time, we agree that a study of DBGNN on a long-range dataset benchmark would strengthen the paper considerably, and we are working towards that goal. See also Reply to all Reviewers - Further Datasets.\n\n```\niv) For binding affinity task, did you try using edge features? Does the Dirac structure help in that case?\n```\n\nSee Reply to all reviewers (Baselines for binding affinity)."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4015/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700330603252,
                "cdate": 1700330603252,
                "tmdate": 1700330603252,
                "mdate": 1700330603252,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xiwwizZDCd",
            "forum": "AialDkY6y3",
            "replyto": "AialDkY6y3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4015/Reviewer_VKy2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4015/Reviewer_VKy2"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces the Dirac-Bianconi Graph Neural Network (DBGNN) based on Bianconi's topological Dirac equation for graph-based network dynamics. DBGNN preserves long-range information propagation and treats edges and nodes equally. This approach is beneficial when edges convey physical properties. The paper demonstrates competitive performance in molecular property prediction and superior performance in predicting the dynamic stability of power grids. In the case of power grids, DBGNN exhibits robust out-of-distribution generalization, indicating learned structural relations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The development of the Dirac-Bianconi Graph Neural Network (DBGNN) based on a modified generalized Dirac-Bianconi equation represents an innovative approach. The incorporation of learnable weights, nonlinearity, and multiple time steps for evolving features offers a unique solution to address long-range dependencies in graph neural networks. The integration of fundamental quantum equations into GNN research is relatively rare, demonstrating a level of innovation. The paper upholds a high standard of quality in its methodology, showcasing the effectiveness of the DBGNN architecture in challenging tasks, including power grid analysis and molecular property prediction. The model's performance underscores its robustness and practicality. The competitive performance and out-of-distribution generalization observed in power grid tasks, along with enhanced molecular property prediction, demonstrate the practical relevance of this work."
                },
                "weaknesses": {
                    "value": "The theoretical part of this paper lacks an analysis of the time complexity of the Dirac-Bianconi Equation. The paper does not provide a detailed theoretical explanation of the integration of the Dirac-Bianconi equation with Graph Neural Networks (GNNs). It focuses on the properties of the Dirac-Bianconi equation itself, dedicating substantial space to this, but does not elaborate on the theoretical foundation for its fusion with GNNs. \nThe paper applies the DBGNN model to two vastly different tasks, power grid analysis and molecular property prediction, without tailoring the model for the specifics of each task. This lack of task-specific optimization could limit the model's performance and applicability. The experimental setup in section 4.1.2 appears to have some shortcomings in terms of training parameters and code implementation. This raises concerns about the reproducibility and robustness of the experiments, which are essential in scientific research.\nThe paper's comparison with other models, particularly using only GCN with three layers, is not comprehensive and may not represent a fair benchmark. Comparing models with varying numbers of layers and different architectural complexities would provide a more meaningful evaluation. Lack of clear improvement in accuracy with the use of the Dirac-Bianconi equation within GNNs could be seen as a weakness in the paper's claims regarding its advantages."
                },
                "questions": {
                    "value": "Can a more detailed theoretical explanation be provided regarding why the Dirac-Bianconi equation was chosen for integration with GNNs and how they are combined to enhance graph representation capabilities?\nGiven the vastly different nature of power grid analysis and molecular property prediction, is there a plan to introduce task-specific optimizations within the model to leverage the unique characteristics of each task?\nCould more detailed experimental settings and parameter choices be provided to ensure experiment reproducibility while also considering the robustness of the model?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4015/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698672530999,
            "cdate": 1698672530999,
            "tmdate": 1699636363918,
            "mdate": 1699636363918,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZGNtdmlWcM",
                "forum": "AialDkY6y3",
                "replyto": "xiwwizZDCd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4015/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4015/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer VKy2"
                    },
                    "comment": {
                        "value": "```\n[The paper] does not elaborate on the theoretical foundation for its fusion with GNNs.\n```\n\nMany GCN methods are equivalent to dynamical systems on networks. This has been elaborated in the literature we cite in the introduction. Given this close equivalence it is unclear to us what theoretical foundation for the fusion the Reviewer had in mind. We are happy to elaborate if the question is clarified.\n\n```\nThe theoretical part of this paper lacks an analysis of the time complexity of the Dirac-Bianconi Equation.\n```\n\nThe time complexity of the forward pass is the same as that of Message Passing Neural Networks. The main difference to these is the fact that we have the edges as full updating states of the layer iteration. This does not increase the computational complexity. We have included a small note in the manuscript to this effect.\n\n```\nThe paper applies the DBGNN model to two vastly different tasks, power grid analysis and molecular property prediction, without tailoring the model for the specifics of each task. This lack of task-specific optimization could limit the model's performance and applicability.\n```\n\nWe agree that the performance of DBGNN could be further improved, but it already shows competitive and superior performance in comparison to the baselines. We consider it as a strength that the layer as presented is relatively simple but already exhibits interesting performance. Adding optimizations should therefore be possible, but we consider it beyond the scope of this paper, which focuses on the properties and performance of the core layer.\n\n```\nThe experimental setup in section 4.1.2 appears to have some shortcomings in terms of training parameters and code implementation.\n\nCould more detailed experimental settings and parameter choices be provided to ensure experiment reproducibility while also considering the robustness of the model?\n```\n\nDetails of the hyperparameters and used software are provided in the appendix. Furthermore, we will provide all code to make the results fully reproducible. We would be glad to add more details if Reviewer VKy2 elaborates on the type of missing information.\n\nWe also tried different hyperparameters and show some examplary results of different configurations below:\n\n\n| learning rate | dropout_n | dropout_e | hidden dimension n| hidden dimension e | tr20ev20  |\n| --- | ---| ---| --- | ---- | ---|\n| 3.6e-4 | 6.1e-2 | 0| 500| 10 |84.79|\n| 1.3e-3 | 5.5e-2 | 0| 500| 10 |85.96|\n| 1.3e-3 | 6e-2 | 0| 500| 10 |85.73|\n| 2.2e-3 | 2e-2 | 0| 300| 10 |84.13|\n| 4.6e-4 | 5.5e-2 | 0| 250| 250 |84.35|\n| 2e-4 | 1.3e-2 | 1.3e-2| 300| 100 |80.87|"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4015/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700330516363,
                "cdate": 1700330516363,
                "tmdate": 1700330516363,
                "mdate": 1700330516363,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TuGifTUf0Q",
            "forum": "AialDkY6y3",
            "replyto": "AialDkY6y3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4015/Reviewer_Bj5J"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4015/Reviewer_Bj5J"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new Graph Neural Network, the Dirac-Bianconi Graph Neural Network, derived from an Euler discretization of the generalized Dirac-Bianci equation on a network. While the graph Laplacian operator-based GNNs cause over-smoothing, the proposed method is designed to capture long-range interactions between nodes. This paper confirms that the proposed method does not cause over-smoothing evaluated using Dirichlet Energy. Also, this paper applies the proposed method to estimating power grid stability and predicting binding affinity and compares its prediction accuracy with existing methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The proposed method improves the accuracy of power grid stability prediction compared to existing methods, especially for the out-of-distribution problem setting.\n* Numerical results show that the over-smoothing evaluated using Dirichlet Energy can be alleviated for the model trained on real data.\n* The paper is well-written, and the derivation of the proposed model is carefully described, making it accessible to readers unfamiliar with the Dirac-Bianconi operator."
                },
                "weaknesses": {
                    "value": "* If I understand correctly, the Dirac-Bianci operator is called the boundary and co-boundary operators in the simplicial complex theory on graphs. Existing studies propose GNNs that use or extend them (e.g., [1--4]. Also, [5] does not use (co-)boundary operators but extends GNNs on simplicial complexes.) Therefore, it is debatable whether the proposed method has (theoretical and experimental) novelty and significance in terms of using the Dirac-Bianci operator.\n* This paper claimed that one of the advantages of the proposed method is that it solves over-smoothing. However, since there exist models that tackle over-smoothing, such as GCNII [6] and DRew [7], I have a question about whether the proposed method is superior to them.\n* In the task of binding affinity prediction, this paper argues that the proposed method with ten steps is comparable with 3-layer GCN. However, the deeper the model is the greater the computational complexity and memory usage. Since existing models overcome the over-smoothing, as I mentioned above, more is needed for a deep model to be comparable in performance to a shallow GNN.\n* One of the advantages of the proposed method is that both node and edge features can be used equally. However, in the numerical experiments on real data, only node features are available to GNNs (in the binding affinity prediction task, edge features are used for graph construction but not as features). Therefore, it is unclear whether the proposed method can take advantage of this feature in real data.\n\n- [1] https://openreview.net/forum?id=vbPsD-BhOZ\n- [2] https://proceedings.mlr.press/v139/bodnar21a\n- [3] https://openreview.net/forum?id=ScfRNWkpec\n- [4] https://proceedings.mlr.press/v139/roddenberry21a.html\n- [5] https://openreview.net/forum?id=nPCt39DVIfk\n- [6] https://proceedings.mlr.press/v119/chen20v.html\n- [7] https://openreview.net/forum?id=WEgjbJ6IDN"
                },
                "questions": {
                    "value": "* How were the hyperparameters chosen? In particular, the numerical experiments use networks with 48 and 10 steps in total, respectively. However, it is yet to be known whether these hyperparameters are optimal. That is, a model with fewer steps might be sufficient. I suggest conducting ablation studies to see the sensitivity of performances to the number of steps.\n* In Figure 5(a), the Dirichlet energy of untrained DBGNN is almost constant regardless of the number of steps. Is this expected from the theory?\n\nMinor Comments\n- P4, Section 2: $\\partial_{db}$ -> $\\partial_{DB}$\n- Section A.3: pyTorch -> PyTorch"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N.A."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4015/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698745993278,
            "cdate": 1698745993278,
            "tmdate": 1699636363812,
            "mdate": 1699636363812,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YDPiR5FWuL",
                "forum": "AialDkY6y3",
                "replyto": "TuGifTUf0Q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4015/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4015/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer Bj5J - Part 1"
                    },
                    "comment": {
                        "value": "```\nIf I understand correctly, the Dirac-Bianci operator is called the boundary and co-boundary operators in the simplicial complex theory on graphs. Existing studies propose GNNs that use or extend them (e.g., [1--4]. Also, [5] does not use (co-)boundary operators but extends GNNs on simplicial complexes.) Therefore, it is debatable whether the proposed method has (theoretical and experimental) novelty and significance in terms of using the Dirac-Bianci operator.\n```\n\nWe thank the reviewer for highlighting the connection to this part of the literature. The comment was extremely helpful. We agree that the boundary operator in itself is not new, and we cite Loyd et al. as an early work on using the boundary operator in the context of topological data analysis on simplicial complexes.\n\nThe key point of Bianconi's Dirac equation is the combination of the boundary operator, and having the edge degree of freedoms as full participants in the dynamics with an appropriate mass term. This causes a spectral gap, and interesting propagation on the network. The work \\[1\\] on sheaf diffusion for example, while considering a very similar topological set up as we do, does not treat the edge space as possessing its own dynamics, and thus cannot induce analogous dynamics.\n\nThe topological set up is of the general form considered in \\[2\\], which we will note in the manuscript. However, in \\[2\\] the focus is on simplicial complexes, and in their applications to graphs, the set up we use is not considered (we have boundary and coboundary at the same time, but no up/down/Hodge Laplacians. The set ups of \\[2\\] did not feature the combination of a boundary/coboundary operator). In a further literature review based on the input of the Referee we identified ( arxiv.org/abs/2012.06010 ) as one example where a similar set up appears, and we will cite this as a predecessor.\n\nOverall we believe our approach is closer to, e.g. the use of Kuramoto oscillators and other physically inspired dynamical equations that are not diffusive, rather than approaches inspired from algebraic topology. The close relationship to these works is of course highly relevant and of great theoretical interest. We will rework the theory section to make this clear.\n\n```\nThis paper claimed that one of the advantages of the proposed method is that it solves over-smoothing. However, since there exist models that tackle over-smoothing, such as GCNII [6] and DRew [7], I have a question about whether the proposed method is superior to them.\n```\n\nThe motivation to explore the use of the DB equation was twofold, we wanted to be able to deal with edge features and to use a non-diffusive dynamical equation that would enable the deep predictions needed for power grid analysis. In our view, it is highly interesting that the DB equation possesses these qualities without having to be specifically engineered for it.\n\nWe agree that it would be interesting to understand in more detail how it compares to other approaches that were designed to handle oversmoothing. In the power grid context, deep GNNs such as ARMANets and TAGNets are known to have the best performance. These share some characteristics with GCNII. DBGNN is a relatively simple layer, that does not make use of these strategies, yet has competitive performance in this dataset at least.\n\nIt would be highly interesting to see whether these strategies, as well as those used in DRew, can be used to further improve the performance of DBGNN. We will add this as a research question to the outlook, but consider it out of scope for this paper.\n\n```\nthis paper argues that the proposed method with ten steps is comparable with 3-layer GCN [...] to be comparable in performance to a shallow GNN.\n\nOne of the advantages of the proposed method is that both node and edge features can be used equally. However, in the numerical experiments on real data, only node features are available to GNNs (in the binding affinity prediction task, edge features are used for graph construction but not as features). Therefore, it is unclear whether the proposed method can take advantage of this feature in real data.\n```\n\nWe have expanded on the baseline comparison, and agree that more diverse datasets would be desirable, see the reply to all reviewers.\n\n```\nHow were the hyperparameters chosen? In particular, the numerical experiments use networks with 48 and 10 steps in total, respectively. However, it is yet to be known whether these hyperparameters are optimal. That is, a model with fewer steps might be sufficient. I suggest conducting ablation studies to see the sensitivity of performances to the number of steps.\n```\n\nWe conducted unstructured hyperparameter studies testing different number of steps. We believe that there is more potential when conducting more hyperparameter studies, but even with our small studies, we could already outperform current baselines."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4015/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700330374222,
                "cdate": 1700330374222,
                "tmdate": 1700330374222,
                "mdate": 1700330374222,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mb48bYzGAF",
                "forum": "AialDkY6y3",
                "replyto": "y1MFr2pwM9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4015/Reviewer_Bj5J"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4015/Reviewer_Bj5J"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors' Comments"
                    },
                    "comment": {
                        "value": "I thank the authors for answering my review comments. Here, I respond to the authors' comments one by one.\n\n### Comparison with GNNs inspired by algebraic topology theory on graphs\n\n> The key point of Bianconi's Dirac equation is the combination of the boundary operator, and having the edge degree of freedoms as full participants in the dynamics with an appropriate mass term. This causes a spectral gap, and interesting propagation on the network. The work [1] on sheaf diffusion for example, while considering a very similar topological set up as we do, does not treat the edge space as possessing its own dynamics, and thus cannot induce analogous dynamics.\n\nI understand that the proposed method differs from [1] in that it can incorporate edge features.\n\n------------------------\n\n> The topological set up is of the general form considered in [2], which we will note in the manuscript. However, in [2] the focus is on simplicial complexes, and in their applications to graphs, the set up we use is not considered (we have boundary and coboundary at the same time, but no up/down/Hodge Laplacians. The set ups of [2] did not feature the combination of a boundary/coboundary operator). \n\nI understand that Message Passing Simplicial Network (MPSN) proposed in [2] is a general form of message passings between simplicial objects. Also, Simplicial Isomorphism Network (SIN), a specific realization of MPSN, does not use boundary and co-boundary operators.\n\n### Comparison with GNNs for over-smoothing\n\n> We agree that it would be interesting to understand in more detail how it compares to other approaches that were designed to handle oversmoothing. In the power grid context, deep GNNs such as ARMANets and TAGNets are known to have the best performance. These share some characteristics with GCNII. DBGNN is a relatively simple layer, that does not make use of these strategies, yet has competitive performance in this dataset at least.\n\nCertainly, ARMANet has initial residuals as in GCNII, as shown in Eq. (14) of [A]. However, GCNII has other characteristics, such as the identity mapping and hyperparameter tuning of $\\beta_l$, which also help to mitigate the over-smoothing problem. Regarding TAGNet, looking at [B], especially Eq. (5), it is not easy to think that TAGNet has GCNII's characteristics in common. In addition, although the power grid task in Section 4.1 is a problem of estimating the dynamic system since it is a node prediction task on a graph, any GNN that provides embedding for each node can be applied. Therefore, I think the fact that the task is a power grid estimation may not imply that we do not have to compare the proposed model with the SOTA models for over-smoothing to claim that it solves the over-smoothing problem.\n\n[A] https://arxiv.org/abs/1901.01343\n[B] https://arxiv.org/abs/1710.10370\n\n### Baselines for binding affinity\n\n> The surprising fact that 3 layers might be sufficient is probably an indicator that long-range dependencies may not be relevant in that case. Furthermore, the dataset does not contain edge features. Those factors probably limit the chances of DBGNN to show advantages over GCN.\n\nI thank the authors for clarifying the motivation for choosing 3-layer MLP as a baseline. However, if my understanding is correct, the model proposed as an answer to the research question of whether long-range interactions can be adequately captured. This explanation raises whether predicting binding affinity is an appropriate dataset for this research question. I want to confirm what the authors want to claim from this task.\n\n### Constant Dirichlet energy\n\n> In the previously submitted version, the weights were initialized too small. With larger weights, the growing and shrinking modes start to mix, and we obtain a more complex behavior of the Dirichlet Energy, as might be expected. We will swap the figure for one with weights drawn from a wider distribution.\n\nOK. I look forward to the additional experiment results using the wider weight distributions."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4015/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700646147060,
                "cdate": 1700646147060,
                "tmdate": 1700646147060,
                "mdate": 1700646147060,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YVaQP9pjoA",
                "forum": "AialDkY6y3",
                "replyto": "WJzmH8iqZ9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4015/Reviewer_Bj5J"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4015/Reviewer_Bj5J"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for the detailed responses. I understand what the authors intended to mean. I will consider the evaluation based on the authors' comments."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4015/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700714040213,
                "cdate": 1700714040213,
                "tmdate": 1700714040213,
                "mdate": 1700714040213,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "EDy8S9zkFd",
            "forum": "AialDkY6y3",
            "replyto": "AialDkY6y3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4015/Reviewer_YvSy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4015/Reviewer_YvSy"
            ],
            "content": {
                "summary": {
                    "value": "Deviating from the traditional Laplacian based GNNs, on this paper the authors propose Dirac-Bianconi Graph Neural Network (DBGNN) which are based on the topological Dirac equation on the graph. The major advantage of the proposed method is that it does not lead to over-smoothing of the node features when a large number of layers are stacked.\n\n\nThe paper is difficult to read at times. Especially for readers not having a background in topological data analysis. The paper is well motivated but lacks insights when using Dirac-Bianconi operators. Although Figure 5 demonstrate that the proposed method enabling heterogeneous representations even after 500 layers, giving more insights will help improving the paper."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The work is well motivated and the main advantage seems to avoid over-smoothing and also use input edge features in learning representations.\n\n2. Figure 5 is a good demonstration of avoiding over-smoothing.\n\n3. Slight performance improvement on power grid data."
                },
                "weaknesses": {
                    "value": "1. In abstract, you mention \"we expect DBGNN to be useful in contexts where edges encode more than mere logical connectivity, but have physical properties as well\". Also the introduction motivates for the same. However, the datasets used in the evaluation do not consider any useful edge information. Including datasets with useful input edge features will enrich the paper.\n\n2. In Table 2, it is shown that only 1 layer of DBGNN suffices. This is deviating from the story that stacking more layers helps in complex (hard) datasets.\n\nI am willing to raise my score significantly if the story of the paper is in line with the findings and more insights with toy examples are presented."
                },
                "questions": {
                    "value": "1. Why do we need at least 13 layer GNNs for power grids? Is it true for all type of GNNs? Some powerful GNNs might need fewer than 13 layers.\n\n2. Please clarify using the notation $e_{ij}$ = $ - e_{ji}$.\n\n3. What is the performance of the proposed method on node classification tasks on citation networks such as Cora, Pubmed, etc?\n\n4. The Dirichlet energy is constant over 500 steps in Figure 5? Why? Is it just for this experiment? It does not seem to change a t all.."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4015/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699036830901,
            "cdate": 1699036830901,
            "tmdate": 1699636363734,
            "mdate": 1699636363734,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZfwxMLb3gg",
                "forum": "AialDkY6y3",
                "replyto": "EDy8S9zkFd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4015/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4015/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer YvSy"
                    },
                    "comment": {
                        "value": "```\nHowever, the datasets used in the evaluation do not consider any useful edge information. \n```\n\nWe agree. See also Reply to All Reviewers - Further Datasets. We will clarify that even in the absence of edge features, the DB equations lead to a very different behavior than standard MPNNs, see Reply to All Reviewers - Evidence for long-range capabilities of DBGNN.\n\n```\n it is shown that only 1 layer of DBGNN suffices\n```\n\nThis is probably a property of the dataset. If 1 layer with 12 steps already considers sufficiently large subgraphs, more layer will not be necessary.\n\n```\nWhy do we need at least 13 layer GNNs for power grids? Is it true for all type of GNNs? Some powerful GNNs might need fewer than 13 layers.\n```\n\nRingsquandl et al. (doi: 10.1145/3459637.3482464) state that the unique structure of power grids leads to the need at least 13 layers for GNNs. Even that might not be sufficient for all tasks: the behavior of power grids shows profound non-locality. A failure in one part of the grid can cause a subsequent failure at a distance comparable to the system size (see e.g. <https://www.science.org/doi/10.1126/science.aan3184>). For example, large system splits and blackouts in Southern Europe have been caused by line failures in Northern Germany. The specific task we investigate is not known to be quite so non-local, but features the same physics.\n\n```\nPlease clarify using the notation e_ij = - e_ji .\n```\n\nSee reply to all Reviewers - Edge notation.\n\n```\nWhat is the performance of the proposed method on node classification tasks on citation networks such as Cora, Pubmed, etc?\n```\n\nWe do not expect that DBGNN has an advantage over existing methods on citation networks. Large topological patterns are not known to be relevant in this case, while close neighborhoods play a more significant role. See also Reply to All Reviewers - Further Datasets.\n\n```\nThe Dirichlet energy is constant over 500 steps in Figure 5? Why? Is it just for this experiment? It does not seem to change a t all..\n```\n\nSee Reply to all reviewers - Constant Dirichlet energy.\n\n```\nI am willing to raise my score significantly if the story of the paper is in line with the findings and more insights with toy examples are presented.\n```\n\nWe are more than happy to provide further support for the story line of the paper. We will add a visualization of the long range response of the layer to a localized signal. See Reply to all Reviewers.  If the reviewer has further toy examples that they would find informative, we would be more than happy to implement them.\n\nAs the revision of the manuscript will still take some days we have already included the new visualization in a supplementary note."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4015/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700330256243,
                "cdate": 1700330256243,
                "tmdate": 1700330256243,
                "mdate": 1700330256243,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]