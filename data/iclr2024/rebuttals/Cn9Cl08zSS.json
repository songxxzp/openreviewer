[
    {
        "title": "Language Guided Interpretable Image Recognition via Manifold Alignment"
    },
    {
        "review": {
            "id": "XViMNsxv0l",
            "forum": "Cn9Cl08zSS",
            "replyto": "Cn9Cl08zSS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2416/Reviewer_CUNM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2416/Reviewer_CUNM"
            ],
            "content": {
                "summary": {
                    "value": "Inspired by prototype theory, this paper proposes a two-stream model that learns semantic concepts from both visual and linguistic modalities. A manifold alignment method is applied to map the features from the two streams into a common subspace. Based on the mixed features in the subspace, a Manifold Alignment based Prototypical Part Network is proposed to achieve concept learning and interpretable image recognition. The paper conducts experiments on the CUB-200-2011 and Oxford Flowers, and the results show that the proposed MA-ProtoPNet outperforms the baseline model in terms of both accuracy and interpretability."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The mathematical foundation of manifold alignment is solid, with clear formulas and algorithms that are well supported.\n2. The paper actively seeks methods to evaluate model's interpretability and designs different quantitative metrics for various types of datasets (object part localization error for CUB and Pointing Game protocol for Flower-102).\n3. From the empirical experiments, the proposed MA-ProtoPNet shows performance improvements over the baseline model in terms of both accuracy and interpretability."
                },
                "weaknesses": {
                    "value": "1. The paper claims that the proposed method achieves semantic concept learning; however, the method does not make efficient use of linguistic modality and is only used to generate a better hybrid feature for the prototypical part network. It lacks innovation in generating explanations that are easier for human to understand.\n2. The design of manifold alignment is entirely based on the assumption that \"the vision and language streams reside on different, yet semantically highly correlated manifolds.\" However, there has been no prior validation or clarification for why this assumption always holds true. This assumption seems to be not that intuitive.\n3. In the experiments, compared methods are limited and outdated (ProtoPNet [NeurIPS 2019] and ProtoPShare [KDD 2021]), and it seems that ProtoPShare has not been cited correctly (in Table 2).\n4. The ablation study is not sufficient as the paper only investigates the impact of the projection matrix on accuracy and interpretability. More detailed analysis of the manifold alignment module is expected. Additionally, the selection of important hyperparameters (such as \u03b1) is not cross-validated. This involves determining the contributions of visual and language modalities to the final hybrid features, which the paper does not explain in the main content but only mentions in the supplementary materials."
                },
                "questions": {
                    "value": "1. Please clarify why the assumption that \"the vision and language streams reside on different, yet semantically highly correlated manifolds\" always holds true.\n2. Please include more discussion on the ablation study and the impact of important hyperparameters (such as \u03b1) on the results."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2416/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698663961891,
            "cdate": 1698663961891,
            "tmdate": 1699636176875,
            "mdate": 1699636176875,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "hLNQeQ2KeQ",
            "forum": "Cn9Cl08zSS",
            "replyto": "Cn9Cl08zSS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2416/Reviewer_LvbZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2416/Reviewer_LvbZ"
            ],
            "content": {
                "summary": {
                    "value": "The authors present a method for joint vision-language representation in fine-grained recognition tasks. The idea is to allow for better explainability of the decision of a black-box neural network. Based on vision (for example, ResNet) and language (Bert) representations, two (linear) mappings to the common manifold are estimated. In this manifold, concept assignments can be made to image regions. This is done by applying a standard prototypical part network. The method is evaluated on two well-known benchmark datasets for fine-grained recognition."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The primary strength of the paper comes from the application of vision-language representations for fine-grained recognition. I also like the idea of opening the doors for the interpretability of results by semantically meaningful concepts. However, this aspect is not sufficiently demonstrated in the paper. Finally, assuming that linear embeddings into the joint manifold are reasonable, a mathematically rigorous derivation of the optimization criteria is given."
                },
                "weaknesses": {
                    "value": "There are two major criticisms of the paper:\n1. the manifold alignment method assumes that linear mapping can arrive at the expected properties of image regions and concepts in the joint manifold. For me, it is not clear why this shall be possible. The correspondence is measured by cosine-similarity and a K-NN decision for which K is not analyzed at all. \n2. experimental results lack support for the effectiveness of the method. The baseline methods are far from the state-of-the-art of existing models (93%+ ob CUB200) on such data sets. Even concerning explainability, existing work is not considered; for example, \n- Simon et al.: Generalized orderless pooling performs implicit salient matching.\nInternational Conference on Computer Vision (ICCV). 2017.\n- Simon et al.: The Whole Is More Than Its Parts? From Explicit to Implicit Pose Normalization.\nIEEE TPAMI. 42 (3). 2020\n\nIn addition, I cannot see an evaluation of explainability that supports the claim by the authors. What are the benefits of this method compared to manually assigning concepts to different feature maps of a DNN? For the flowers data set, evaluation has been done empirically. \n\nFinally, if I did not completely misunderstand the paper, where does the language modality come from? Is this part of the applied ProtoNet? I would have expected an analysis of the joint manifold, for example, showing some images and language/concepts that are close to each other."
                },
                "questions": {
                    "value": "There exists work in the area of manifold alignment; for example, \n- Zhao et al.: RLEG: Vision-Language Representation Learning with Diffusion-based Embedding Generation. ICML 2023. \n. Li et al.: Learning Visually Aligned Semantic Graph for Cross-Modal Manifold Matching. ICIP 2019.\nWhy did you not consider such ideas? What do you expect would be the benefits of your alignment?\n\nWhy do you not achieve state-of-the-art on those benchmark datasets? Even for existing work no longer being state-of-the-art, the explainability of the results could be worth a comparison. \n\nWhat is the advantage of your interpretation (Fig 3) over the alpha-pooling exemplar-based explanation by Simon et al. (see TPAMI paper from above)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2416/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698767777348,
            "cdate": 1698767777348,
            "tmdate": 1699636176801,
            "mdate": 1699636176801,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "CUtsgrzhLQ",
            "forum": "Cn9Cl08zSS",
            "replyto": "Cn9Cl08zSS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2416/Reviewer_76Fr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2416/Reviewer_76Fr"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to leverage both visual and text modalities to learn more effective semantic concepts for interpretable image recognition. To this end, it constructs a two-stream model consisting of one visual encoding stream and one language encoding stream. In particular, it performs manifold alignment between the visual feature manifold and the language feature manifold by learning the project matrices by Cayley transform on the Stiefel manifold. As a result, it can learn the semantic concepts incorporating both the visual and language information by the proposed manifold alignment based ProtoPNet.\n\nOverall, the paper is motivated well and organized well. The method is technically sound."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is motivated well.\n2. The method is technically sound."
                },
                "weaknesses": {
                    "value": "1. I wonder is it possible to validate that incorporating the language can indeed improve the quality of the learned semantic concepts. For example, compare the learned semantic concepts both quantitatively and qualitatively between your method and the baseline using only the visual information.\n2. The method learns the semantic correspondence between two modalities in an unsupervised way by simply minimizing the across-modality pairwise distance of the points  that within k nearest neighbors between each other in the common space and maximizing the points that outside the range of k nearest neighbors. However, how to guarantee the correctness of the initial k nearest cross-modality neighbors for a sample? I surmise that the model cannot rectify the wrongly identified k nearest neighbors automatically."
                },
                "questions": {
                    "value": "Check the weaknesses above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2416/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699198503208,
            "cdate": 1699198503208,
            "tmdate": 1699636176735,
            "mdate": 1699636176735,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]