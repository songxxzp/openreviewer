[
    {
        "title": "($\\texttt{PEEP}$) $\\textbf{P}$redicting $\\textbf{E}$nzym$\\textbf{e}$ $\\textbf{P}$romiscuity with its Molecule Mate \u2013 an Attentive Metric Learning Solution"
    },
    {
        "review": {
            "id": "hZ1pG9fdtm",
            "forum": "760br3YEtY",
            "replyto": "760br3YEtY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6470/Reviewer_QFnk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6470/Reviewer_QFnk"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel ML algorithm, PEEP, to predict enzyme promiscuity, which integrates biology priors of protein functionality to regularize the model learning."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) This work introduces PEEP, a metric learning method, designed to identify promiscuous enzymes that can be utilized in subsequent protein engineering processes.\n\n2)  OThe PEEP framework integrates biology-aware designs to enhance the learning of protein functionality. (1) the integration of cognate ligands' embeddings ; (2) the incorporation of an attentive module to identify crucial residues for protein functionality; and (3) the introduction of an EC-aware training objective to bolster the metric learning capability.\n\n3) code and dataset are available."
                },
                "weaknesses": {
                    "value": "1. The technique contribution of this work is not very high since it mainly use MoCo in the protein domain.It also mixes up some biological efforts to improve performance.\n\n2. In the experiments, the dataset may be very small. e.g., very small number of proteins."
                },
                "questions": {
                    "value": "1. The technique contribution of this work is not very high since it mainly use MoCo in the protein domain.It also mixes up some biological efforts to improve performance."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6470/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698362372433,
            "cdate": 1698362372433,
            "tmdate": 1699636724164,
            "mdate": 1699636724164,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7DW4BINb9Z",
                "forum": "760br3YEtY",
                "replyto": "hZ1pG9fdtm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6470/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6470/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer QFnk"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer for providing the valuable feedback. We have provided responses to your questions and we hope that they can address your concerns. \n\n**Weakness 1: Technique Contributions?**\nWe respectfully disagree. While we do utilize MoCo in the protein domain, our key innovations lie in integrating biological insights into various facets of our approach. These aspects are not explicitly considered in previous machine learning algorithms for protein functional annotation. In our framework, we have leveraged the attention mechanism to capture residue importance, introduced a hierarchical loss term customized for functional annotations, and integrated ligand embeddings to enrich the representations. These elements collectively contribute to our framework's efficacy. Our detailed experimental results (summarized below) underscore that these incorporations, rather than using MoCo alone, are instrumental in achieving the performance enhancements we have observed.\n\n| Method | Rec. | Prec. | F-1 |\n| :-: | :-: | :-: | :-: |\n| CLEAN | 0.187 | 0.261 | 0.204 |\n| CLEAN + MoCo | 0.186 | 0.271 | 0.202 | \n| Ours | 0.217 | 0.339 | 0.241 |\n\n**Weakness 2: Sizes of datasets?**\nWe would like to clarify that the datasets utilized in our experiments are not characterized as small. Specifically, for the training sets, we worked with a substantial number of protein sequences, with counts of 8K/10K/220K when employing different identity cutoffs of 10%/30%/100%, respectively. As for the testing set, which includes the Price, New, and CATH subsets, the respective sequence counts are 149/392/98. It is important to highlight that these dataset sizes align with those commonly encountered in the existing literature and are considered challenging, as demonstrated in the recent study by Yu et al. (2023)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6470/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700603611731,
                "cdate": 1700603611731,
                "tmdate": 1700603611731,
                "mdate": 1700603611731,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rrFUDLXvly",
                "forum": "760br3YEtY",
                "replyto": "hZ1pG9fdtm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6470/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6470/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer QFnk,\n\nWe thank Reviewer QFnk for spending time to review our work and providing valuable feedbacks. We have provided responses to your question regarding the technical contribution. As the rebuttal deadline is drawing near, we hope you could take a look at our responses and see if they have addressed your concerns. Thank you again for your time and effort. \n\nBest,\n\nAuthors"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6470/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700722145758,
                "cdate": 1700722145758,
                "tmdate": 1700722145758,
                "mdate": 1700722145758,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "B0Q1jlQPJD",
            "forum": "760br3YEtY",
            "replyto": "760br3YEtY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6470/Reviewer_nNj8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6470/Reviewer_nNj8"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on a specific bioinformatic problem, named function annotation of proteins. Previous studies like CLEAN have achieved good performance, yet they also suffer from generation ability. To achieve this problem, this paper proposes a novel metric learning method. First, at the input level, SMILE representation is used to preserve the prior of proteins. At the objective, they propose a\nmetric learning objective that captures the hierarchical nature of EC numbers to appropriately weight\ndissimilarity at each EC level. Three datasets and extensive experiments have varified the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed metric-based method seems simple but effective, the fusion of prior information in function annotation is an interesting idea.\n\n2. The strong performance when given a few training data and strong generalization ability.\n\n3. The detailed ablation experiments verified the modules on the Price dataset."
                },
                "weaknesses": {
                    "value": "1. The paper might lack key explanations, such as references to EC in the second paragraph.\n\n2. The differences between the CLEAN method and the proposed method need further discussion. I did not see a concrete motivation for CLEAN."
                },
                "questions": {
                    "value": "How to use the transformer layer to achieve \"facilitate learning functional residues associated with an enzyme function\"?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6470/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698761430608,
            "cdate": 1698761430608,
            "tmdate": 1699636724021,
            "mdate": 1699636724021,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tTHB3VJHcW",
                "forum": "760br3YEtY",
                "replyto": "B0Q1jlQPJD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6470/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6470/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer nNj8"
                    },
                    "comment": {
                        "value": "We genuinely thank the reviewer for saying that our idea is interesting. We have provided answers to your questions in detail. \n\n**Weakness 1: Lacking explanation?**\nThank you for your valuable feedback. In the manuscript, we have included a paragraph explaining the EC number in Section 3.1. In response to your suggestion, we have also incorporated an introductory mention of the EC number in the second paragraph of the revised draft to provide additional context.\n\n**Weakness 2: Difference between CLEAN and PEEP.**\nThe primary disparity between our approach and CLEAN lies in our incorporation of several techniques that account for biological priors across various dimensions: (1) at the input level, we fuse the representation of ligands to provide substrate-scope information; (2) at the model level, we adopt attention mechanism to learn functional residues; (3) at the objective level, we propose a metric learning objective to capture hierarchical nature of EC numbers. These enhancements collectively set our method apart from CLEAN and underline the motivations behind our approach.\n\n\n**Question 1: How to use attention modules to learn functional residues?**\nThe transformer layer was chosen to replace global average pooling used by CLEAN based on the insight that the residues responsible for endowing an enzyme function make up a small amount of the sequence, thus, global average pooling will significantly weaken this signal. By using attention, we instead learn a weighted pooling that weights residues, and their importance, towards enzyme function.\n\nIn Figure 2 and 4, we have provided evidence of the concordance between these attention values and the predicted significance of residues. As such, these introduced modules play a crucial role in enhancing the learning process for functional residues."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6470/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700603556429,
                "cdate": 1700603556429,
                "tmdate": 1700603556429,
                "mdate": 1700603556429,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6zelPo8c7r",
            "forum": "760br3YEtY",
            "replyto": "760br3YEtY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6470/Reviewer_sx29"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6470/Reviewer_sx29"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a ML algorithm, PEEP, to predict the function of enzymes.  The method integrates three different aspects into its pipeline: 1) It utilizes the EC number hierarchy to define different levels of similarity. 2) It uses self-attention to capture residues at the active pockets in binding to ligands. 3) It fuses the information of a protein\u2019s ligands (their SMILES representation) with its own sequence representation by traversing the substrates and the products involved in different reactions. The method is validated on three public benchmarks and shows performance improvements in F-1 scores. It also generalizes to unseen protein sequences with unseen functionalities."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The authors demonstrate deep domain knowledge to find three different aspects in protein structure prediction that can be added to improve performance.\n\nDetailed experimental studies have been carried out with all different kinds of optimizations, attention mechanisms, and regularizations.\n\nAblation studies demonstrate that all three factors are needed to improve prediction. However, it is interesting that the performance becomes worse by adding EC metric netween the 4th and 5th rows of Table 2."
                },
                "weaknesses": {
                    "value": "The ideas do not seem to be that novel. The methods make use of the available data in interesting ways but there is limited ML innovation. \n\nIt is difficult to understand the significance of the performance improvement, especially when all the values are so low (e.g., on the CATH dataset)."
                },
                "questions": {
                    "value": "Please refer to the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6470/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698895095249,
            "cdate": 1698895095249,
            "tmdate": 1699636723910,
            "mdate": 1699636723910,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Fo0yCkwYVr",
                "forum": "760br3YEtY",
                "replyto": "6zelPo8c7r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6470/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6470/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer sx29"
                    },
                    "comment": {
                        "value": "We thank the reviewer for acknowledging the domain knowledge embedded in our designs. We have provided answers to your questions below in detail and hope they can address your concerns. \n\n**Weakness 1: Novelty of our framework?**\nWhile some of our techniques resemble some prevalent ones, it is also important to note that our proposed method has been recognized as novel by Reviewer nNj8 and QFnk. The novelty of our method primarily stems from its design, which is driven by specific challenges in the biological domain. This includes three key innovations: (1) the functions of proteins are highly correlated with their active sites; (2) the functional annotations have hierarchical structures; and (3) embeddings of ligands can be leveraged as additional sources of information for functional annotations. These elements, not explicitly considered in prior work, are central to our innovative approach. We have incorporated an attention mechanism to capture residue importance, introduced a hierarchical loss term customized for functional annotations, and integrated ligand embeddings to enrich the representations. Empirical evidence supports the effectiveness of these integrations in enhancing functional annotation performance. \n\n**Weakness 2: Significance of the performance improvement?**\nWe performed an additional analysis to assess the significance of the performance improvements on the Price-149 and CATH datasets, both of which were subjected to three separate experiments under a 10% sequence identity cutoff. Our results revealed error bars of approximately 0.007 and 0.011 for Price-149 and CATH, respectively, demonstrating the statistical significance of the observed improvements. \n\n**Weakness 3: Performance Comparison in Table 2.**\nThank you for your observation! It is worth noting that when comparing the results between the 4th and 5th rows of Table 2, we observe improvements in four metrics out of six. As such, we continue to believe that the combination of all techniques remains the most favorable choice."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6470/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700603492113,
                "cdate": 1700603492113,
                "tmdate": 1700603492113,
                "mdate": 1700603492113,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TpmAZPS1Pa",
                "forum": "760br3YEtY",
                "replyto": "Fo0yCkwYVr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6470/Reviewer_sx29"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6470/Reviewer_sx29"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response"
                    },
                    "comment": {
                        "value": "I thank the authors for their response.\nI will maintain my score."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6470/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700626795629,
                "cdate": 1700626795629,
                "tmdate": 1700626795629,
                "mdate": 1700626795629,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IzJ1J0w1aK",
            "forum": "760br3YEtY",
            "replyto": "760br3YEtY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6470/Reviewer_siYE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6470/Reviewer_siYE"
            ],
            "content": {
                "summary": {
                    "value": "The authors focus on the problem of predicting an enzyme's function given it's amino acid sequence, with a focus on situations where the sequence is quite remote from any enzyme with known function or where we seek to make predictions for a novel enzyme function that had not previously appeared in the training data. They use a number of techniques for improving models, such as triple-loss-based contrastive learning, using pre-trained protein and small molecule embeddings, a loss function that accounts for the label hierarchy, and an attention-based pooling technique."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The intro/background is accessible, comprehensive, and well written.\n\nThe problem that the paper approaches is important for the life science community and the paper achieves reasonable performance improvements.\n\nThe authors are careful to construct train-test splits that probe a model's ability to do meaningful extrapolation, both in terms of protein sequence and protein function.\n\nThe paper provides a significant number of ablations probing the impact of various design choices."
                },
                "weaknesses": {
                    "value": "Despite the paper's title, it doesn't really model enzyme promiscuity, just enzyme function. Promiscuity is the tendency of an enzyme to accept many substrates. There is no evaluation setup that focuses on enzymes with annotated activity on multiple substrates. In part this is because Swissprot, the training data, only contains annotations for enzymes' natural function, while there are many other reactions that these enzymes could catalyze in the lab.\n\nThe paper boasts about incorporating 'biology priors', but the actual solutions aren't particularly novel or biology-specific. For example, the authors use a learned attention head to pool per-residue embeddings into a per-sequence embedding. The motivation about attention as focusing on active site residues is tenuous and post-hoc. Attention-based pooling is common these days. Similarly, another 'biology' detail is that the labels are hierarchical in nature, and the loss function is adjusted such that the similarity of 2 proteins' embeddings reflects the degree of their similarity in terms of the hierarchy of function. There's nothing biology-specific about training models with hierarchical labels.\n\nIt's unclear if the benchmarking setup is fair to baseline models. See below."
                },
                "questions": {
                    "value": "I'm confused by the comparison to baseline models. When you compare to proteinfer, for example, do you retrain it on the same train-test split as your model? If not, how is the comparison fair?\n\nI'm confused by the motivation for changing the clustering threshold for de-duplicating the training data. To me, the key quantity when assessing the difficulty of an extrapolation task is the distance between evaluation examples and training examples. How does this distance change as you vary the training threshold? \n\nCan you elaborate on the concept of 'promiscuity'? In what sense are you tackling the promiscuity problem?\n\nI was confused by the statement in the intro that 'only 570K (\u223c 0.3%) sequences have been manually annotated with computational methods that bridge the sequence-annotation gap.' Surely there are far more sequences in uniprot with computationally-derived functional annotations. Do you mean that there are 570K sequences with human-curated annotations (i.e., Swissprot)? \n\nI found fig 4 unsatisfactory, since the distributions seem to overlap so much. Is there a way to quantitatively evaluate this separation, such as a precision/recall/f1 for classifying residues as occurring in the active site?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6470/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699552200359,
            "cdate": 1699552200359,
            "tmdate": 1699636723806,
            "mdate": 1699636723806,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HJE6s7bivY",
                "forum": "760br3YEtY",
                "replyto": "IzJ1J0w1aK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6470/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6470/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer siYE"
                    },
                    "comment": {
                        "value": "We appreciate the efforts made by the reviewer for providing the detailed and valuable comments. We have provided responses in detail and we hope they can address your concerns.  \n\n**Weakness 1: Concept of Promiscuity**\nWe appreciate and agree with your comments. Your understanding of enzyme promiscuity is correct. While we have proteins with multiple EC numbers in our testing datasets, we are not directly tackling the promiscuity problem. We have updated the title of the paper and also the draft to make it purely about enzyme function and not enzyme promiscuity.\n\n**Weakness 2: Motivation of using attention?**\nWe would like to clarify that our motivation for deploying attention mechanisms is based on specific scientific reasoning and not post-hoc. In the context of protein analysis, it is well-established that not all residues contribute equally to the protein's function. Traditional methods that assign uniform weights to all residues often overlook the nuanced contributions of key functional residues.\nOur choice of attention mechanisms is driven by their ability to dynamically emphasize these functionally significant residues. This is not merely following a common trend, but a strategy to enhance model accuracy. These attention modules allow our model to adaptively focus on residues that are crucial for the protein's activity, thereby offering a more precise representation of functional sites compared to uniform weighting. \nEmpirical evidence supports the effectiveness of attention mechanisms. In our experiments, deploying the attention mechanism consistently demonstrated superior performance. This improvement is attributable to the model's ability to learn and prioritize residues that are more relevant to the protein's function.\nThus, our adoption of attention mechanisms is a design that aims at addressing the specific challenges in protein function prediction. \n\n\n**Weakness 3: Hierarchical label training is not biology-specific**\nWe appreciate the reviewer's point that hierarchical labels are not exclusively used in biological contexts. However, we demonstrate that EC numbers\u2019s ontology is well-suited for the hierarchical label strategy and we demonstrate that it leads to improved performance. Thus, demonstrating the use of domain knowledge via hierarchical labeling is beneficial in prediction protein function from sequence. An insight that could benefit the protein machine learning community.\n\nHere are a few examples on how combining hierarchical labels with domain knowledge is beneficial: \n- For example, [r1] incorporates class hierarchy from WordNet [r2] as a source of domain knowledge to improve the performance of visual classification and [r3] uses label hierarchy to improve concept classification. Our technique applies this concept to the domain of functional annotation. By adjusting the loss function to reflect protein similarities based on their functional hierarchy, our model effectively integrates domain-specific knowledge.\n\nIn summary, while hierarchical labeling is a technique used across various fields, its implementation in our model is specifically tailored to address the complexities of biological data. This approach aligns with established practices in the literature, demonstrating our model's effective absorption and utilization of domain-specific knowledge.\n\n\n[r1] Integrating Domain Knowledge: Using Hierarchies to Improve Deep Classifiers\n\n[r2] WordNet: A Lexical Database for English\n\n[r3] Domain-Specific Knowledge Acquisition and Classification using WordNet"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6470/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700603340909,
                "cdate": 1700603340909,
                "tmdate": 1700603340909,
                "mdate": 1700603340909,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YThJ4T3zFB",
                "forum": "760br3YEtY",
                "replyto": "IzJ1J0w1aK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6470/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6470/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer siYE (cont'd)"
                    },
                    "comment": {
                        "value": "**Question 1: The fairness of comparison with baselines.**\nIn our study, we have ensured a fair comparison with baseline methods. For the training phase, we have used identical training datasets to (re-)train all models. For the evaluation, all the models including baselines, are evaluated on identical sets of test data. This ensures the fairness of our comparison. We have incorporated this benchmarking setup description into the draft to enhance its clarity and presentation.\n\n**Question 2: Motivation for de-duplicating training data.** \nThe motivation of using different clustering thresholds is to decrease the similarity between the training and the testing set and evaluate the model\u2019s ability to generalize. This is in line with your understanding on the importance of the difference between training and testing examples in changing the difficulty of extrapolation tasks. \nIn response to your inquiries, we conducted an assessment of relative Levenshtein distances (i.e., divided by the length of the sequence) between sequences in the training and testing sets. For every sequence in the testing set, we average the distance between it and 5 samples from the training set with the smallest Levenshtein distances. The table presented below illustrates that as the thresholds decrease, the distances between these sequences tend to increase. This indicates that adjusting the clustering threshold effectively enhances the distinctiveness of the training and testing data. \n\n| Clustering Threshold | Average Relative Levenshtein Distance | \n| :-: | :-: | \n| 100% | 0.615 | \n| 30% | 0.713 | \n| 10% | 0.721 |  \n\n\n**Question 3: Clarifying the statement in introduction.**\nThank you for pointing out this ambiguity in our statement. Yes, this number refers to the number of sequences with experimental annotation in UniProt SwissProt. We will clarify that this refers to SwissProt in the main text. \n\n**Question 4: Quantitative evaluation of active sites detection.** \nFollowing your suggestion, we have established another evaluation protocol to judge the performance of active sites detection, using the accuracy and the precision at residue levels. Our method achieves an average accuracy of 48% and an average precision of 73% on the New-392 dataset. We have also benchmarked ProteInfer, using the class activation mapping (CAM) technique to identify functional localization, on New-392, which shows an average accuracy of 49% and an average precision of 54%. Note that both methods are trained on the same split of data (10% identity), and the comparisons are conducted on test samples that are in the training set, a prerequisite required by ProteInfer. Our results exhibit much higher precision while having the same level of accuracy. We have included these new results in the revision."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6470/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700603435589,
                "cdate": 1700603435589,
                "tmdate": 1700603435589,
                "mdate": 1700603435589,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7Wd1FQyWSi",
                "forum": "760br3YEtY",
                "replyto": "IzJ1J0w1aK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6470/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6470/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer siYE,\n\nWe extend our gratitude to Reviewer siYE for dedicating time to review our work and for the valuable constructive comments provided. We have meticulously addressed your questions in a point-by-point fashion. As the rebuttal deadline is drawing near, we hope you could take a look at our responses and see if they have addressed your concerns. Thank you again for your time and effort. \n\nBest,\n\nAuthors"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6470/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700722033637,
                "cdate": 1700722033637,
                "tmdate": 1700722044178,
                "mdate": 1700722044178,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "XeqRGSn1QD",
            "forum": "760br3YEtY",
            "replyto": "760br3YEtY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6470/Reviewer_fk1t"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6470/Reviewer_fk1t"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes PEEP, a metric learning framework for protein functionality. The framework uses established techniques, such as ESM2 embeddings and Momentum Contrast as the backbone of the model. On top of these, the paper applies three types of algorithmic insights: using the ligands as additional information, leveraging a self-attentive mechanism to identify key residues, and a modified objective focused on the hierarchy of EC labels. PEEP outperforms relevant baselines on Price-149, New-392 and CATH and ablation studies show the relevance of the proposed changes."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is well written and motivated. The final architecture is backed by good empirical performance, and ablation studies are thorough, providing necessary insight into the importance of the proposed modifications."
                },
                "weaknesses": {
                    "value": "I encourage the authors to revise the writing in \"To meet the goal, we customize a self-attention mechanism (Figure 1, c) to model the residue importance within protein sequences\u201d as it currently reads as though the self-attention mechanism is newly-proposed, while it seems to be a standard setup. \n\nWhile the paper is well-motivated, it seems that a significant part of it studies how some well-known techniques fit together in the context of the chosen task. The lack of novelty hinders from a higher rating at the moment unfortunately."
                },
                "questions": {
                    "value": "In the paper, it is stated that \"PEEP randomly samples one from the ligands\u2019 SMILE embedding and integrates it with the protein\u2019s sequence representation\u201d. It would be useful to provide additional information on the distribution of ligands per protein and the sensitivity of the random choice with respect to the results.\n\nMoreover, in 3.3, two methods are described for doing the fusion \u2014 how does the performance compare/which one is used?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6470/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6470/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6470/Reviewer_fk1t"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6470/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699571419663,
            "cdate": 1699571419663,
            "tmdate": 1699636723691,
            "mdate": 1699636723691,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "imMuohr2lb",
                "forum": "760br3YEtY",
                "replyto": "XeqRGSn1QD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6470/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6470/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer fk1t"
                    },
                    "comment": {
                        "value": "We greatly appreciate the reviewer's recognition of the quality of our paper's writing and its empirical performance. We have prepared detailed responses to your comments, and we hope that they can address your concerns. \n\n**Weakness 1: revision of the writing?**\nThank you for your insightful feedback. We have revised the manuscript and adjusted the sentence to enhance clarity, addressing your suggestion as follows: \u201cTo meet the goal, we customize a self-attention mechanism (Figure 1, c) to model the residue importance within protein sequences,\u201d has been modified to \u201cTo meet the goal, we deploy a self-attention mechanism (Figure 1, c) to model the residue importance within protein sequences,\u201d.\n\n**Weakness 2: lack of novelty?**\nWhile some of our techniques resemble some prevalent ones, it is also important to note that our proposed method has been recognized as novel by Reviewer nNj8 and QFnk. The novelty of our method primarily stems from its design, which is driven by specific challenges in the biological domain, including three key aspects: (1) the functions of proteins are highly correlated with their active sites; (2) the functional annotations have hierarchical structures; and (3) embeddings of ligands can be leveraged as additional sources of information for functional annotations. These elements, not explicitly considered in prior work, are central to our innovative approach. We have incorporated an attention mechanism to capture residue importance, introduced a hierarchical loss term customized for functional annotations, and integrated ligand embeddings to enrich the representations. Empirical evidence supports the effectiveness of these integrations in enhancing functional annotation performance. \n\n**Question 1: more information about ligands?**\nThank you for your feedback. To address your concern, we have included a histogram illustrating the distribution of ligands associated with proteins in our training dataset in Appendix. We can see most EC numbers have less than 10 associated ligands. \n\nWe would like to emphasize that our random selection process is performed iteratively at each training step, ensuring that all ligands are utilized in the training procedure. We have also conducted multiple experiments specifically on the Price-149 dataset, employing a sequence identity cut-off of 10%. These experiments reveal a negligible error bar of 0.007, underscoring that the random selection process introduces only minor fluctuations in the results.\n\n**Question 2: methods for fusion?**\nWe have chosen the second option, which involves fusing protein representations with zero vectors. For a detailed performance comparison between the two methods, you can refer to Appendix A3.2. To make it more convenient, we have included the results in the table below:\n\n| Strategy | Rec. | Prec. | F-1 |\n| :------: | :-----: | :-----: | :--: |\n| Fuse Zero Vectors | 0.211 | 0.319 | 0.240 | \n| Fuse Every | 0.171 | 0.251 | 0.196 |"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6470/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700603281918,
                "cdate": 1700603281918,
                "tmdate": 1700603281918,
                "mdate": 1700603281918,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "B689SPefco",
                "forum": "760br3YEtY",
                "replyto": "imMuohr2lb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6470/Reviewer_fk1t"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6470/Reviewer_fk1t"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to thank the authors for their response. I will maintain my score, recommending acceptance of the paper."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6470/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700739638533,
                "cdate": 1700739638533,
                "tmdate": 1700739638533,
                "mdate": 1700739638533,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]