[
    {
        "title": "Generative Adversarial Inverse Multiagent Learning"
    },
    {
        "review": {
            "id": "9uG83ji1iD",
            "forum": "JzvIWvC9MG",
            "replyto": "JzvIWvC9MG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8714/Reviewer_zp62"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8714/Reviewer_zp62"
            ],
            "content": {
                "summary": {
                    "value": "Game theory provides a structured approach to predicting outcomes of interactions between rational agents. Inverse game theory deals with situations where the players' behavioral models are unknown and aims to deduce the payoff functions that explain observed actions as equilibria. This paper presents a new approach for solving inverse equilibrium problems in a range of games, using generative adversarial optimization to match game-theoretic models to observed data and make predictions, as exemplified by modeling the Spanish electricity market."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The authors study an interesting and challenging problem of inverse MARL. The theoretical results are nice; they are simple yet relevant and impactful. \nThe experiments on an electricity market are well thought and designed."
                },
                "weaknesses": {
                    "value": "The readability can be improved. I think Section 3 never mentions that it is for the one-shot game setting. \nThere is no methodological contributions. All presented algorithms are simple extension of known algorithms. (On the other hand we should not invent/propose algorithms just for the sake of proposing them)."
                },
                "questions": {
                    "value": "Minor remark/question: what is the meaning of that weird symbol in Theorems 3.2, 4.1, 5.2? I assume it means of the same order as (but I don't think this is standard notation.)\nIn Theorem 3.2, it is a little bit surprising that the optimal is obtained by averaging prior solutions instead of the last one. What is the intuition behind averaging (which includes initial solutions that can be of very low quality)? I assume the proof is correct."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8714/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698707500473,
            "cdate": 1698707500473,
            "tmdate": 1699637093332,
            "mdate": 1699637093332,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ITR3qFZuyg",
                "forum": "JzvIWvC9MG",
                "replyto": "9uG83ji1iD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8714/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8714/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your review!\n\n### Weaknesses\n\n> **W1)**  The readability can be improved. I think Section 3 never mentions that it is for the one-shot game setting. There is no methodological contributions. \n\n**Response to W1)**: In the second paragraph of Section 3, we define an inverse game as a tuple comprising a one-shot parametric game whose parameter values are missing, together with a Nash equilibrium action profile.\n    \n> **W2)** All presented algorithms are simple extension of known algorithms. (On the other hand we should not invent/propose algorithms just for the sake of proposing them).\n    \n**Response to W2)** As you mention, we have not developed any groundbreaking methodologies, but have simply introduced a novel mathematical characterization of a longstanding problem. Perhaps the strength of our paper lies in its simplicity. \n\n### Questions \n    \n> (Q1) Minor remark/question: what is the meaning of that weird symbol in Theorems 3.2, 4.1, 5.2? I assume it means of the same order as (but I don't think this is standard notation.) \n\nWe believe you are referring to the notation $\\eta_{\\boldsymbol{y}}^{(t)} \\asymp \\varepsilon^4$. This notation is equivalent to big theta notation, i.e., $\\eta_{\\boldsymbol{y}}^{(t)} \\in \\Theta(\\varepsilon^4)$. An example of the use of this notation can be found in [1]. Regardless, we will add a footnote to clarify. \n        \n> (Q2) In Theorem 3.2, it is a little bit surprising that the optimal is obtained by averaging prior solutions instead of the last one. What is the intuition behind averaging (which includes initial solutions that can be of very low quality)? I assume the proof is correct.\n\n**Response to Question**: Simple gradient descent ascent methods are not guaranteed to converge in last-iterates in min-max optimization problems, and can even diverge (see, for instance, Mertikopoulos et al. [2]). A common remedy is to average the iterates. As we mention in the sentence preceeding Theorem 3.2, we can instead obtain last-iterate convergence using extragradient descent ascent or optimistic gradient descent ascent.  \n    \n**References**\n\n[1] Daskalakis, Constantinos, Dylan J. Foster, and Noah Golowich. \"Independent policy gradient methods for competitive reinforcement learning.\" Advances in neural information processing systems 33 (2020): 5527-5540.\n    \n[2] Panayotis Mertikopoulos, Christos Papadimitriou, and Georgios Piliouras. Cycles in adversarial regularized learning. In Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 2703\u20132717, 2018."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8714/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700237936932,
                "cdate": 1700237936932,
                "tmdate": 1700337225236,
                "mdate": 1700337225236,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wgF4Csqwyj",
            "forum": "JzvIWvC9MG",
            "replyto": "JzvIWvC9MG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8714/Reviewer_F6LX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8714/Reviewer_F6LX"
            ],
            "content": {
                "summary": {
                    "value": "This paper formalizes the problem of inverse game-theory (determining\nequilibrium strategies and associated game structure from historical play) as a\nnovel min-max optimization problem and solves it using a primal-dual gradient\nmethod. This technique is deployed on a practical and interesting application\ndomain of market pricing dynamics."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The simple formulation of the set of inverse Nash equilibria (NE) as a min-max\ngame is elegant and appears to be original. If it is indeed original, for this\nalone, the paper merits publication and should be highlighted.\n\nThe paper overall is well written and showcases immediate applications of the\nproposed solution to an important and practical domain as a proof-of-concept. I\nbelieve these results are significant and will be impactful."
                },
                "weaknesses": {
                    "value": "As an easily rectified issue, Figure 1 could have been better represented by\nplotting residuals over time or, by subsampling the data, plotting mean\nresiduals with error bars.\n\nAs a minor complaint, I do not prefer the language of \"generative-adversarial\"\n(especially not in terms of a \"discriminator\"), even if this is the closest\nanalogy familiar to machine learning practitioners: This is a standard min-max\noptimization problem that need not be wed to the ML setting."
                },
                "questions": {
                    "value": "Remark 1 is indeed interesting, but it is not obvious. Did I miss an associated\nproof or example?\n\nWhy was the proof of Theorem 3.2 omitted? Was it rephrased to appear as Theorem\n6.1 in the supplementary material?  Establishing the convergence rates of\nvarious algorithms is not my expertise, but the results seem reasonable,\nespecially given assumptions of convexity and Lipshitz smoothness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "10: strong accept, should be highlighted at the conference"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8714/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8714/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8714/Reviewer_F6LX"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8714/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698802254432,
            "cdate": 1698802254432,
            "tmdate": 1699637093206,
            "mdate": 1699637093206,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "z3Mn5ajioK",
                "forum": "JzvIWvC9MG",
                "replyto": "wgF4Csqwyj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8714/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8714/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your kind and encouraging review, it means a lot to us!\n\n### Weaknesses\n\n>**W1)** As an easily rectified issue, Figure 1 could have been better represented by plotting residuals over time or, by subsampling the data, plotting mean residuals with error bars.\n\n**Response to W1)**: This is a great suggestion. Thank you. We will indeed plot residuals over time and error bars over the five seeds for which we ran our experiments.\n\n>**W2)** As a minor complaint, I do not prefer the language of \"generative-adversarial\" (especially not in terms of a \"discriminator\"), even if this is the closest analogy familiar to machine learning practitioners: This is a standard min-max optimization problem that need not be wed to the ML setting.\n\n**Response to W2)**: \n\nThank you for this feedback. The \"generative\" language is meant to allude to the generative model fitting problem within the larger technical development of our approach. That said, the discriminator language is not quite right. We will give further thought to this concern, perhaps by replacing \"generative-adversarial\" simply by \"min-max\". \n\n### Questions\n\n>**Q1**: Remark 1 is indeed interesting, but it is not obvious. Did I miss an associated proof or example?\n\n**Response to Q1**: This remark follows from a folklore theorem, which states that set of solutions to convex-concave min-max optimization problems (i.e., min-max problems in which the objective is convex-concave, and the constraints are non-empty, compact, and convex) are convex. This fact can be seen as a corollary of the set of Nash equilibria being convex in zero-sum, potential, and monotone games. You can find a proof in Nau et al [1]; for completeness we also provide a proof here. \n    \nConsider a convex-concave min-max optimization problem $\\min_{\\mathbf{x} \\in \\mathcal{X}} \\max_{\\mathbf{y} \\in \\mathcal{Y}} f(\\mathbf{x},\\mathbf{y})$ where $f: \\mathcal{X} \\times \\mathcal{Y} \\to \\mathbb{R}$ is convex-concave and $\\mathcal{X}, \\mathcal{Y}$ are non-empty, compact, and convex sets. Let $V(\\mathbf{x}) \\doteq \\max_{\\mathbf{y} \\in \\mathcal{Y}} f(\\mathbf{x},\\mathbf{y})$. By Danskin's theorem [2], $V$ is convex since it is the maximum of a set of convex functions. Hence, by Theorem 2.6 of Rockafeller and Wets [3], the set of solutions $\\arg \\min_{\\mathbf{x} \\in \\mathcal{X}} V(\\mathbf{x}) = \\arg\\min_{\\mathbf{x} \\in \\mathcal{X}} \\max_{\\mathbf{y} \\in \\mathcal{Y}} f(\\mathbf{x},\\mathbf{y})$ is convex. \n\n>**Q2**: Why was the proof of Theorem 3.2 omitted? Was it rephrased to appear as Theorem 6.1 in the supplementary material? Establishing the convergence rates of various algorithms is not my expertise, but the results seem reasonable, especially given assumptions of convexity and Lipschitz smoothness.\n\n**Response to Q2**: We refer you to our common answer.\n\n**References**\n\n[1] Nau, Robert, Sabrina Gomez Canovas, and Pierre Hansen. \"On the geometry of Nash equilibria and correlated equilibria.\" International Journal of Game Theory 32 (2004): 443-453.\n\n[2] Danskin, John M. \"The theory of max-min, with applications.\" SIAM Journal on Applied Mathematics 14.4 (1966): 641-664.\n\n[3] R Tyrrell Rockafellar and Roger J-B Wets. Variational analysis, volume 317. Springer Science and Business Media,2009"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8714/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700237816236,
                "cdate": 1700237816236,
                "tmdate": 1700337204493,
                "mdate": 1700337204493,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TYtLCNVLDj",
            "forum": "JzvIWvC9MG",
            "replyto": "JzvIWvC9MG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8714/Reviewer_Wrco"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8714/Reviewer_Wrco"
            ],
            "content": {
                "summary": {
                    "value": "This paper studied inverse game theory to find parameters of the payoff functions of the game. Polynomial time and sample efficient algorithms are provided and claimed."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper formulate the inverse game as an generative-adversarial optimization problem and provide polynomial time algorithms."
                },
                "weaknesses": {
                    "value": "1. The proofs are not completed, e.g., I cannot find the proofs for Theorem 4.1 and Theorem 5.2.\n\n2. The presentation can be further improved, e.g., more intuitions about the assumptions and theorems."
                },
                "questions": {
                    "value": "1. See weaknesses.\n\n2. Can you further polish the paper? Some typos: for example, in the fifth line of the abstract, should it be \"to solve them\"?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8714/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8714/Reviewer_Wrco",
                        "ICLR.cc/2024/Conference/Submission8714/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8714/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698844452802,
            "cdate": 1698844452802,
            "tmdate": 1700668556240,
            "mdate": 1700668556240,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vyzSPjiULQ",
                "forum": "JzvIWvC9MG",
                "replyto": "TYtLCNVLDj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8714/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8714/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your review!\n\n### Weaknesses\n\n>**W1)** The proofs are not completed, e.g., I cannot find the proofs for Theorem 4.1 and Theorem 5.2.\n \n**Response to W1)**:  We refer you to our common answer.\n\n>**W2)** The presentation can be further improved, e.g., more intuitions about the assumptions and theorems.\n\n**Response to W2)**: We will try to add additional intuition to the main paper, beyond what already appears at the bottom of page 6 and top of page 7 (for example), respecting the space constraints. You can also already find some additional discussion in the appendix (see page 16).\n\n### Questions\n\n>**Q2)** Can you further polish the paper? Some typos: for example, in the fifth line of the abstract, should it be \"to solve them\"?\n\n**Response to Q2)**: We will do our best to thoroughly proofread the paper again to correct all grammatical, semantic, and syntactical errors in the camera-ready version."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8714/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700237546439,
                "cdate": 1700237546439,
                "tmdate": 1700337180018,
                "mdate": 1700337180018,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wGPpDQjsic",
                "forum": "JzvIWvC9MG",
                "replyto": "xn7Md2LKRx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8714/Reviewer_Wrco"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8714/Reviewer_Wrco"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response. Similar to reviewer WrBZ, the proofs are still a little hard for me to follow. I am sorry that I may not be able to check the detailed technique. However, I agree with the strengths pointed out by other reviewers. I have increased my rate to a 6."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8714/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668538293,
                "cdate": 1700668538293,
                "tmdate": 1700668538293,
                "mdate": 1700668538293,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qIFr0OtbT3",
            "forum": "JzvIWvC9MG",
            "replyto": "JzvIWvC9MG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8714/Reviewer_WrBZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8714/Reviewer_WrBZ"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a generative-adversarial (or min-max) characterization of the inverse game theory problem where a generator provides payoff parameters that minimize total regret, and a discriminator looks for action-profiles that maximize it. A min-max objective mimicking this two-player game is optimized to estimate the inverse equilibrium using gradient descent ascent algorithm (and other variations of it). It is further proposed that for games that satisfy certain assumptions guaranteeing convex-concavity of the objective, the algorithm converges in a number of iterations that are polynomial in the precision of the obtained inverse equilibrium. This formulation is further extended to give algorithms for multi-agent inverse reinforcement learning and multi-agent apprenticeship learning, accompanied by polynomial time (and space) convergence guarantees under appropriate assumptions. Experiments are conducted to identify categories of games for which the method is effective, and whether its usefulness goes beyond provided theoretical limits."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* An inverse game theoretic perspective to multi-agent inverse reinforcement learning is certainly a novel direction to approach the problem with. Backed by results in inverse game theory, this approach leads to algorithms with desirable convergence guarantees that prior work in multi-agent imitation learning does not provide.\n\n* The low restrictiveness of the assumptions made allow for the framework to be effective on a vast majority of markov games, leading to useful and efficient solutions on a wide variety of multi-agent problems.\n\n* While the paper focuses on the inverse nash equilibrium, the simplicity of the objective allows for easy extensions of the framework to alternative game theory solution concepts.\n\n* All presented algorithms are succinct and easy to understand. Sufficient mathematical background is provided as and when necessary."
                },
                "weaknesses": {
                    "value": "* It would be helpful to expand on the proofs of theorems 6.1, 6.2, and 6.3 in the supplementary material. I know that a reference has been provided, but a slight explanation of the cited result and how it relates to the theorem in question would be nice.\n\n* Although a comparison of the method has been shown with the ARIMA model on the spanish electricity market data, it would be beneficial to have a comparison with prior methods in inverse multi-agent reinforcement learning. Especially in terms of efficiency since it's one of the main points of the paper. The abstract says that the method outperforms other widely-used methods (plural), and we only get to see it being compared with one other model which is specific to time-series data. \n\n* Some comparison/contextualization with prior work in multiagent inverse reinforcement learning would also be helpful."
                },
                "questions": {
                    "value": "What does the term $\\psi(\\pi, \\rho; \\theta)$ in the \"Multiagent Apprenticeship Learning\" section expand to? Cannot seem to find a definition anywhere.\n\nAssuming that Algorithm 3 was used on the spanish electricity market data, how was the observation distribution specified?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8714/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8714/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8714/Reviewer_WrBZ"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8714/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699407882055,
            "cdate": 1699407882055,
            "tmdate": 1699637092964,
            "mdate": 1699637092964,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "018eGXE0Cb",
                "forum": "JzvIWvC9MG",
                "replyto": "qIFr0OtbT3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8714/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8714/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your review!\n\n### Weaknesses\n\n>**W1)** $\\bullet$ It would be helpful to expand on the proofs of theorems 6.1, 6.2, and 6.3 in the supplementary material. I know that a reference has been provided, but a slight explanation of the cited result and how it relates to the theorem in question would be nice.\n\n**Response to W1)**: We refer you to our common answer.\n\n>**W2)** $\\bullet$ Although a comparison of the method has been shown with the ARIMA model on the spanish electricity market data, it would be beneficial to have a comparison with prior methods in inverse multi-agent reinforcement learning. Especially in terms of efficiency since it's one of the main points of the paper. The abstract says that the method outperforms other widely-used methods (plural), and we only get to see it being compared with one other model which is specific to time-series data.\n \n**Response to W2)**: To our knowledge, all existing methods of inverse multiagent reinforment learning apply only to finite state and action Markov games, while our electricity market model is a continuous state and action Markov game. As a result, comparing to these methods would require potentially non-trivial extensions. Instead, we chose to compare our method to a statistical method, namely that of ARIMA, only. We will adjust our language to correct the plural.\n\n>**W3)** $\\bullet$ Some comparison/contextualization with prior work in multiagent inverse reinforcement learning would also be helpful.\n\n\n**Response to W3)**: A more extensive related works section is included in the third section of the appendix. There, we summarize related work not only in inverse multiagent reinforcement learning, but also in microeconomics, econometrics, and algorithmic game theory. Thank you for pointing out that a reference to this more detailed related work section is missing. We will be sure to correct this oversight in the camera-ready version. \n\n### Questions\n\n>**Q1) ** $\\bullet$ What does the term $\\psi(\\pi,\\rho;\\theta)$ in the \"Multiagent Apprenticeship Learning\" section expand to? Cannot seem to find a definition anywhere.\n\n**Response to Q1)**: $\\psi$ is the cumulative regret and is defined as $\\psi(\\pi, \\rho ; \\theta) \\doteq \\sum_{i \\in[n]} u_i\\left(\\rho_i, \\pi_{-i} ; \\pi\\right)-u_i(\\pi ; \\mathbf{\\theta})$. (See page 3, at the end of the paragraph starting with \"One-shot games\".) Note that this definition extends immediately to Markov games since any Markov game can be seen as a one-shot game in which the space of actions is taken to be the space of policies. This point is explained further in the sentences preceeding Corollary 1.\n\n>**Q2) ** $\\bullet$ Assuming that Algorithm 3 was used on the spanish electricity market data, how was the observation distribution specified?\n\n**Response to Q2)**: In the electricity market experiments, the Markov game consists of an electricity seller who sets prices in the day ahead market and spot market, and $n$ buyers who demand electricity. We use price for the day ahead market, prices for the spot market, and aggregate demand (i.e., the sum of the demand across all buyers) of electricity for every hour from 2015 to 2019 as our observation space. The observation distribution then consists of the history distribution associated with this Markov game, which is pushed forward through a function that outputs sampled price trajectories and the sum of each buyer's individual demand."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8714/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700237386755,
                "cdate": 1700237386755,
                "tmdate": 1700337163798,
                "mdate": 1700337163798,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uS6WNTVFi8",
                "forum": "JzvIWvC9MG",
                "replyto": "V1B3O5qiHv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8714/Reviewer_WrBZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8714/Reviewer_WrBZ"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer Response"
                    },
                    "comment": {
                        "value": "Thank you for your response. Good to see that almost all weaknesses have been acknowledged. Though I am still a little torn on the proofs, they're just hard to follow for someone who does not have a core game theory background, given the paper's title does not mention game theory at all. However, my lack of expertise in verifying those convergence proofs cannot be ruled out, so take this with a grain of salt. A more experienced reviewer may have a different opinion.\n\nAlso I hope you are aware that there is another paper with an almost identical title (https://arxiv.org/abs/1807.09936) which tackles the same problem albeit by extending single-agent GAIL to the multi-agent setting. Yours being a inverse game theoretic approach, the phrase \"Generative-Adversarial\" is a bit misleading to use for any min-max objective (another reviewer has also pointed this out)."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8714/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700622188652,
                "cdate": 1700622188652,
                "tmdate": 1700622188652,
                "mdate": 1700622188652,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]