[
    {
        "title": "Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM"
    },
    {
        "review": {
            "id": "455z5ZAsNK",
            "forum": "V01FPV3SNY",
            "replyto": "V01FPV3SNY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6818/Reviewer_Lpzg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6818/Reviewer_Lpzg"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on defending current LLMs against adversarial attacks (i.e., jailbreak attacks). The authors propose a method that requires multiple times of model inference. \n\nThe first inference is the conventional inference that takes the original prompt (e.g., harmful instruction and jailbreak prompt) as the input and collects the output. Subsequently, this paper randomly drops the words in the original prompt, inferences to get the output, and detect the harmfulness of this output. Through multiple times of such procedure, the harmfulness of this original prompt is determined by collecting these detection results.\n\nThis paper shows experimental results on one dataset and two models."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The topic of this paper is important in the field of LLM.\n2. The proposed method is intuitively reasonable that can defends adversarial attacks to an extent (e.g., the GCG attack)."
                },
                "weaknesses": {
                    "value": "1. **Lack of baseline comparisons.** This paper did not compare with a highly related baseline, that is detecting harmfulness based on the model output [1]. This baseline requires roughly $L_{in} + (L_{in}+L_{out})$ input cost and $L_{out}$ output cost, where the overall cost could be much smaller than this paper's method (if the $L_{out}$ is not too large). Besides, this baseline has a simple variation, where we can instruct the LLM to revise the output of first stage, which could also potentially improve the helpfulness and reduce harmfulness.\n2. The experiments are not comprehensive. There are only two small tables. Only two relatively small models, one dataset, open-source models are considered. Since such method is more appropriate for proprietary models, experiments on proprietary models are needed.\n3. The claim of \"such alignment checking is not robust\" (page 4, Robust Alignment Check Function) is not well-supported. What is the relationship between adversarial prompts [2] and such claim? In think this point is critical. If the authors cannot fully clarify the drawbacks of existing alignment checking methods, the motivation of this paper will seem to be weak.\n4. The authors approximate $AC(\\cdot)$ by only inspecting the existence of prefix in a pre-collected prefix set (e.g., \u201cI can not\u201d, \u201cI\u2019m\nsorry\u201d). However, is the approximation robust? It is unclear. Such prefixes may vary across different models, for example, some models may output \"as a helpful and harmless chatbot, my job is to ....\". Since there are so many potential prefixes, I do not think enumerating to construct a prefix set is a robust solution.\n5. **Computational Cost**. The current calculation manner of computational cost is not convincing: authors seem to compare the per-token cost, however, this method requires much larger token length. Through a rough calculation, this method requires $n*(1-p)=20*(1-0.3)=14$ times of input cost, which has not been revealed by the authors.\n\n[1] Alec Helbling, Mansi Phute, Matthew Hull, and Duen Horng Chau. Llm self defense: By self\nexamination, llms know they are being tricked. arXiv preprint arXiv:2308.07308, 2023.\n\n[2] Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial\nattacks on aligned language models. arXiv preprint arXiv:2307.15043, 2023."
                },
                "questions": {
                    "value": "see weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6818/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698047921770,
            "cdate": 1698047921770,
            "tmdate": 1699636788517,
            "mdate": 1699636788517,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TwFenY3Lik",
                "forum": "V01FPV3SNY",
                "replyto": "455z5ZAsNK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6818/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6818/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "# Response to Reviewer Lpzg (1/3)\nThank you for your valuable comments!\n\n**Q1: Addition of Baseline Method:**\n\n**A1:** Thank you for your suggestion! We have added two baseline methods: LLM Self Defense by Alec et al. [1] and Perplexity-Based Defenses by Jain et al. [2]. The results are as follows:\n\nWe first test the performance of [1]. For the obtained responses to malicious samples, if the harm filter model could not identify them as harmful, we considered the attack successful. As shown in the following table, we found that the Vicuna and Guanaco models we tested had poor performance in identifying harmful content, with ASRs of 22.7% and 52.0% after Self-Defense, respectively. Their BARs were also low (68.7% and 41.3%). Our examination of the results revealed many unexpected scenarios, including LLM not responding, not following the preset rules of outputting \"Yes, it is harmful.\" or \"No, it is harmless.\", or continuing unfinished content from the previous context instead of making a judgment on harm. This suggests that LLM Self-Defense might not be effective in scenarios where we can only use the target LLM model itself for defense. We also tried using GPT3.5 as the harm filter model, which showed higher rates of harmful content recognition. Using GPT3.5 reduced the ASRs on Vicuna and Guanaco to 8.0% and 8.7%, respectively, while GPT3.5 still resulted in lower BARs (around 90%). We believe this may be due to the current LLMs being overly cautious in detecting harmful content. We have updated these experimental results and analyses in Appendix F of our paper.\n\n| Model | BAR Original LLM  | BAR Self Defense | BAR GPT3.5 |BAR RA-LLM | ASR Original LLM  | ASR Self Defense | ASR GPT3.5 |ASR RA-LLM |\n| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |\n| Vicuna | 99.3% | 68.7%  | 90.0% | 98.7% | 98.7% | 22.7%  | 8.0% | 10.7% |\n| Guanaco | 95.3% | 41.3% | 87.3%  | 92.0% | 96.0% | 52.0% | 8.7% | 6.7% |\n\nRegarding perplexity-based defense [2], we have conducted some extra experiments in Appendix G. Here we also summarize the comparison results in the following table. We can observe that even though [2] achieves high BAR and effectively reduces the ASR of individual GCG attacks, this defense mechanism completely fails to detect handcrafted jailbreak prompts, presumably owing to the lower perplexity of these prompts, as they are manually written by humans. In contrast, our method effectively defends against both GCG adversarial prompts and handcrafted jailbreak prompts.\n\n\n| Attack | Model | BAR Original LLM  | BAR Perplexity Defense | BAR RA-LLM | ASR Original LLM  | ASR Perplexity Defense | ASR RA-LLM | \n| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |\n| Individual GCG| Vicuna-7B-chat-HF | 99.3% | 98.0% | 98.7% | 98.7% | 0% | 10.7% |\n| Individual GCG| Guanaco-7B-HF | 95.3% | 100% | 92.0% | 96.0% | 4% |  6.7% |\n| Handcrafted prompt|Vicuna-7B-chat-HF | 99.3% | 98.0% | 98.7% | 98.7% | 100% | 12.0% |\n|Handcrafted prompt|Guanaco-7B-HF |  95.3% | 100% | 92.0% | 94.7% | 100% |  9.3% |"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6818/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700330070877,
                "cdate": 1700330070877,
                "tmdate": 1700330070877,
                "mdate": 1700330070877,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "v1giWIXclW",
                "forum": "V01FPV3SNY",
                "replyto": "455z5ZAsNK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6818/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6818/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "# Response to Reviewer Lpzg (2/3)\n\n**Q2: Computational Cost:**\n\n**A2:** We apologize for any confusion caused. Note that the cost calculated in Section 4.5 of our paper is not for each token but the theoretical maximum cost per query. In our defense, we believe our design is already quite time-efficient for two main reasons:\n\n - Limited output length: we set the maximum generation length in Monte Carlo simulation to $t_{max} = 10$,  since the negative prefixes often appear at the start of LLM responses, allowing us to obtain effective defense without generating full responses. This has been discussed in our original submission Sections 3.3 and 4.5.\n - Early exit mechanism: during the Monte Carlo simulation, if the detected failure cases exceed our set threshold, RA-LLM can directly terminate the process early and mark the input as malicious. For instance, with $n = 20$ Monte Carlo trials and a threshold of $t = 0.2$, if $0.2 \\times 20 = 4$ aligned responses are already detected, RA-LLM will instantly mark it as malicious and reject the request. If the alignment check is passed for the first 17 simulations, RA-LLM will also instantly mark it as benign. This further reduces computational costs. \n\nWe also evaluated the actual time overhead of RA-LLM against the original LLM in our experiments. We tested 150 attack samples and recorded the normal inference time on Vicuna and Guanaco models and the time required by our RA-LLM. We report the extra time per data on average overhead in the following table where the values in parenthesis represent the percentage of additional time relative to the normal inference time. These Results show RA-LLM's extra time requirement is less than 20% of normal inference, and even in the worst case (completing all Monte Carlo simulations) - the extra time overhead is less than 45% of normal inference. Detailed settings and analysis of this experiment can be found in Appendix H.\n| Model | normal inference time  | RA-LLM extra time |  RA-LLM extra time (worst case)|\n| ---- | ---- | ---- | ---- |\n| Vicuna-7B-chat-HF |20.97s | 3.93s (18.7%) | 9.26s (44.1%)  |\n| Guanaco-7B-HF | 30.36s | 3.76s (12.4%) | 12.84s (42.3%)|\n\n**Q3: Experiments on Proprietary Models:**\n\n**A3:** Thank you for the suggestion and we agree that evaluating proprietary models such as ChatGPT would enhance our credibility. Technically speaking, our method is applicable to any aligned LLM model. However, in order to show the effectiveness of our defense, we first need to obtain valid alignment-breaking prompts. While the adversarial prompts for those open-sourced models can be obtained by directly running the GCG attack, regrettably, we were unable to obtain enough successful adversarial prompts that are effective against GPT models (maybe due to that the authors of the GCG had previously disclosed this to OpenAI). Consequently, we were unable to perform our defense experiments on GPT models. Nevertheless, we further conducted experiments using hand-crafted jailbreak prompts and tested them on GPT3.5 and reported the results in the following table, which show that on GPT3.5, RA-LLM reduced ASR from 82.0% to 8.0%, without affecting BAR. We have also added these results to Section 4.3 of our paper.\n\n|Model | BAR Original LLM  | BAR RA-LLM | ASR Original LLM  | ASR RA-LLM | ASR reduce |\n| :---- | :---- | :---- | :---- | :---- | :---- |\n|Vicuna-7B-chat-HF| 99.3% | 98.7% | 98.7% |12.0% | 86.7% |\n|Guanaco-7B-HF | 95.3% | 92.0% | 94.7% |  9.3% | 85.4% |\n|GPT-3.5-turbo-0613| 99.3% | 99.3% | 82.0% |8.0% | 74.0% |"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6818/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700330176366,
                "cdate": 1700330176366,
                "tmdate": 1700330176366,
                "mdate": 1700330176366,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fUZJjf1PeD",
                "forum": "V01FPV3SNY",
                "replyto": "455z5ZAsNK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6818/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6818/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "# Response to Reviewer Lpzg (3/3)\n**Q4: Claim of \"Such Alignment Checking is Not Robust\":**\n\n**A4:** We are sorry for the confusion. We would like to clarify that the Alignment Checking here is used to formalize the alignment capabilities of LLMs. Specifically, $\\text{AC}(f(\\mathbf{x}))=\\text{Fail}$ represents that the response is aligned by the LLM, thus containing aligned text such as \u201cI cannot answer this\u2026\u201d rather than fulfilling the user instruction. Our claim that \u201csuch alignment checking is not robust\u201d refers to that the existing alignment capabilities are not robust. For example, the success of the GCG attack has shown that attaching a particular adversarial prompt to the malicious question could bypass the alignment mechanism and elicit the corresponding helpful response instead of providing aligned text and refusing to answer. Therefore, the Alignment Checking would not return \u201cFail\u201d, even though the user instruction is malicious, which demonstrates it is not robust to the adversarial prompt.\n\n**Q5: Approximation of  $AC(\\cdot)$:**\n\n**A5:** We agree that the current prefix checking is only an approximation to the actual  $AC(\\cdot)$ function. In our paper, this approximation design follows from the GCG attack, which also utilizes such a prefix checking for their attack evaluations. Clearly, it is possible to have the cases that you mentioned. To make sure our result is indeed valid, we manually verified all our experimental outcomes and found no such cases in our current experimental results.\n\nAlthough we agree that such a metric design cannot be totally accurate and we cannot guarantee whether such cases would happen or not in other models/cases that we didn\u2019t test here, we want to emphasize that this metric does not always favor our RA-LLM. In fact, from a defender\u2019s perspective, this design might also have the chance to underestimate our defense capability.  For example, assuming there is a malicious input and the LLM model\u2019s alignment response is outside our prefix set. In such a case, RA-LLM may give it a Pass (since it cannot be detected by the prefix set) and output the original response (a rejection that is not detected by the prefix set). Our current evaluation pipeline would consider that as a defense failure since RA-LLM gives a Pass for a malicious question, however, the actual outcome is still a rejection response. Hence, if we employed an ideal $AC(\\cdot)$, the ASR on RA-LLM could even be lower.\n\nWe have also considered how to improve this implementation of $AC(\\cdot)$ to be more accurate in the future. In Appendix E of our paper, we discussed some possible improvement directions. We will leave finding a better approximation for $AC(\\cdot)$ as our future work.\n\n[1] Alec Helbling, Mansi Phute, Matthew Hull, and Duen Horng Chau. Llm self defense: By self examination, llms know they are being tricked. arXiv preprint arXiv:2308.07308, 2023.\n\n[2] Jain, Neel, et al. \"Baseline defenses for adversarial attacks against aligned language models.\" arXiv preprint arXiv:2309.00614 (2023)."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6818/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700330242177,
                "cdate": 1700330242177,
                "tmdate": 1700330242177,
                "mdate": 1700330242177,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RtxYPGbra3",
                "forum": "V01FPV3SNY",
                "replyto": "455z5ZAsNK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6818/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6818/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow up with reviewer Lpzg"
                    },
                    "comment": {
                        "value": "We really appreciate the reviewer Lpzg for the constructive comments, which significantly enhance the quality of our work. We would like to ask if there are any additional comments regarding our response, as the discussion phase is nearing its end. We are more than happy to address them. Additionally, we would appreciate if you could consider updating your scores if our rebuttal has satisfactorily addressed the concerns. Thanks again for your time and effort!"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6818/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700586525802,
                "cdate": 1700586525802,
                "tmdate": 1700586525802,
                "mdate": 1700586525802,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FIxSUanwUE",
            "forum": "V01FPV3SNY",
            "replyto": "V01FPV3SNY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6818/Reviewer_Nds5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6818/Reviewer_Nds5"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a method to defend alignment-breaking attack by perturbing the input prompt to see whether the request is rejected by an aligned LLM, which is interesting. Experiments on both attack dataset and QA datasets verify the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Defending the alignment-breaking attack for LLMs is a very important research direction to protect LLMs from being misused.\n\n2. The proposed method seems to be quite effective according to the reported experimental results.\n\n3. The proposed method is very easy to implement."
                },
                "weaknesses": {
                    "value": "1. I wonder whether it is enough to have only one dataset for ASR and BAR evaluation.\n\n2. The size of the experimental dataset seems to be small.\n\n3. This paper does not consider the adaptive attack scenario."
                },
                "questions": {
                    "value": "I wonder whether the proposed method can make some false positive errors."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6818/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6818/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6818/Reviewer_Nds5"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6818/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698840187529,
            "cdate": 1698840187529,
            "tmdate": 1699636788388,
            "mdate": 1699636788388,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bKnHTeZr5O",
                "forum": "V01FPV3SNY",
                "replyto": "FIxSUanwUE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6818/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6818/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "# Response to Reviewer Nds5 (1/2)\nThanks for the constructive comments!\n\n**Q1: Use of Only One Dataset:**\n\n**A1:** We would like to clarify that our experiments involved three types of data from two different datasets. For Harmful Behavior Attack and Harmful String Attack, we used Harmful Behaviors and Harmful Strings from the AdvBench dataset, respectively, to calculate the ASR. For the BAR, we utilized the MS MARCO dataset. Additionally, we validated the effectiveness of RA-LLM using hand-crafted jailbreak prompts sourced from the internet. Currently, as far as we know, AdvBench is the only dataset available for such tasks. We are open to including more datasets if they become available in the future.\n\nAs an additional measure, we included an extra benign dataset [1] derived from chatlogs.net, which records user interactions with ChatGPT. We first filtered out the initial user inputs from each conversation session and randomly selected 150 entries, ensuring these did not contain potentially harmful or non-English content. Experiments showed that on the Vicuna model, the BAR was 95.3% for the original model and 91.3% for the model incorporating RA-LLM. This demonstrates that RA-LLM still maintains good performance on this dataset. We are open to including more datasets and conducting further experiments should they become available in the future.\n\n**Q2: Small Size of Experimental Dataset:**\n\n**A2:** Thank you for your suggestion! We have added more experimental data to enhance the credibility of our results. Specifically, in Section 4.2, and 4.3 of our paper, we increased the benign input samples and the attack samples for both the Vicuna and Guanaco models by 50%. We report the results under GCG attacks and handcrafted jailbreak prompt attacks with more data samples in the following two tables, respectively.  The results in Sections 4.2 and 4.3 of our paper have been updated accordingly. It is evident that the results with the additional data are consistent with the original findings, further validating the effectiveness of our method. Additionally, the number of test data used in our experiments has now exceeded the quantity evaluated in the original paper proposing the attack method by GCG.\n\nResults under GCG attacks:\n|Attack|Model | BAR Original LLM  | BAR RA-LLM | ASR Original LLM  | ASR RA-LLM | ASR reduce |\n| :---- | :---- | :---- | :---- | :---- | :---- | :---- |\n|Individual|Vicuna-7B-chat-HF| 99.3% | 98.7% | 98.7% |10.7% | 88.0% |\n|Individual|Guanaco-7B-HF | 95.3% | 92.0% | 96.0% |  6.7% | 89.3% |\n|Transfer|Vicuna-7B-chat-HF| 99.3% | 98.7% | 83.3% |11.3% | 71.0% |\n|Transfer|Guanaco-7B-HF | 95.3% | 92.0% | 78.7% |  8.7% | 70.0% |\n\n\nResults under handcrafted prompt attacks:\n|Model | BAR Original LLM  | BAR RA-LLM | ASR Original LLM  | ASR RA-LLM | ASR reduce |\n| :---- | :---- | :---- | :---- | :---- | :---- |\n|Vicuna-7B-chat-HF| 99.3% | 98.7% | 98.7% |12.0% | 86.7% |\n|Guanaco-7B-HF | 95.3% | 92.0% | 94.7% |  9.3% | 85.4% |\n|GPT-3.5-turbo-0613| 99.3% | 99.3% | 82.0% |8.0% | 74.0% |\n\n**Q3: Adaptive Attacks:**\n\n**A3:**: We have considered a potential adaptive attack method in Appendix C of our paper. Since our method randomly drops a slight portion of tokens from the input, one may also utilize this design choice and simply try increasing the length of the adversarial prompts (e.g., repeat the adversarial prompts after input for several times) to ensure the random dropping cannot fully remove the adversarial parts. According to our experimental results, even if the attackers are aware of our design and opt for longer repetitive adversarial prompts, our method remains effective in thwarting their efforts.\n\nAnother intuitive adaptive attack is to utilize the GCG attack upon our RA-LLM model. However, since our proposed RA-LLM decides the response based on Monte Carlo experiments, and the final response is not relevant to the Monte Carlo random dropping (either return the original response or rejection to answer), it is very challenging to directly backpropagate and perform the GCG attack. We also plan to explore potentially viable methods of adaptive attacks in our future research. Thank you for your suggestion!\n\n\n[1] https://www.kaggle.com/datasets/noahpersaud/89k-chatgpt-conversations/data"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6818/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700329755532,
                "cdate": 1700329755532,
                "tmdate": 1700329846325,
                "mdate": 1700329846325,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fRJbLLfRIE",
                "forum": "V01FPV3SNY",
                "replyto": "FIxSUanwUE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6818/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6818/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "# Response to Reviewer Nds5 (2/2)\n\n**Q4: False Positive Errors:**\n\n**A4:** Thank you for your insight! In our experiments, we indeed encountered some false positive cases after random dropping. However, the chances of having those false positive cases are very rare. These bad cases primarily resulted from the random discarding of tokens, making the original input challenging to understand, as discussed in the \"The Practical Choice of $t$\u201d section on page 6 of our paper.\n\nTo mitigate such instances, we set the hyperparameter $t$ to prevent benign inputs from being misjudged as malicious. Specifically, in a Monte Carlo experiment, if the proportion of randomly discarded samples judged as malicious is less than the threshold $t$, the input will not be misclassified as malicious. \nTo ensure that our final defense is not suffering from false positive errors, we also report the BAR metric, representing the proportion of benign samples that pass the model alignment check. In fact, $1\u2212\\text{BAR}$ corresponds to the RA-LLM\u2019s False Positive Rate (FPR). We observed that the FPRs of RA-LLM on Vicuna and Guanaco models are 1.3% and 8%, respectively. The higher FPR on the Guanaco model is due to its inherently higher FPR (4.7%). We believe that sacrificing a small FPR to enhance the model's robustness against jailbreak attacks is worthwhile."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6818/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700329880820,
                "cdate": 1700329880820,
                "tmdate": 1700329880820,
                "mdate": 1700329880820,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "obFf2U07kl",
                "forum": "V01FPV3SNY",
                "replyto": "FIxSUanwUE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6818/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6818/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow up with reviewer Nds5"
                    },
                    "comment": {
                        "value": "We really appreciate the reviewer Nds5 for the constructive comments, which significantly enhance the quality of our work. We would like to ask if there are any additional comments regarding our response, as the discussion phase is nearing its end. We are more than happy to address them. Additionally, we would appreciate if you could consider updating your scores if our rebuttal has satisfactorily addressed the concerns. Thanks again for your time and effort!"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6818/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700586492176,
                "cdate": 1700586492176,
                "tmdate": 1700586492176,
                "mdate": 1700586492176,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IcBdfRdtX9",
            "forum": "V01FPV3SNY",
            "replyto": "V01FPV3SNY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6818/Reviewer_8tCQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6818/Reviewer_8tCQ"
            ],
            "content": {
                "summary": {
                    "value": "The authors present a Robustly Aligned LLM (RA-LLM) as a countermeasure to jailbreaking attacks. The primary methodology involves randomly removing tokens from the prompt and assessing the failure rate under aligned LLMs."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The underlying principle of RA-LLM is evident: the strategic removal of tokens from the prompt has the potential to neutralize the adversarial prefix, thereby mitigating the effectiveness of the attack.\n\n2. The introduced methodology demonstrates substantial robustness when tested on Vicuna-7B and Guanaco-7B."
                },
                "weaknesses": {
                    "value": "1. The concept of partially erasing the prompt as a defensive measure against jailbreak attacks has been previously explored, as evidenced by concurrent work [1]. It would be beneficial if the authors delved deeper into this method to enhance its defensive capabilities. Furthermore, it might be worth comparing the RA-LLM's performance with the perplexity-based defense [2], which has also demonstrated commendable robustness.\n\n2. The experimental evaluations appear to be limited to open-source LLMs. Is it feasible for the RA-LLM to be effective on GPT3.5/4? Comprehensive experimental results on GPT3.5/4 would enhance the study's credibility.\n\n3. In assessing computational costs, the authors have focused on financial implications rather than time expenses. The reviewer posits that time cost is of paramount importance, as it directly relates to the model's efficiency.\n\n\n[1] Aounon et al. Certifying llm safety against adversarial prompting.\n[2] Jain et al. Baseline defenses for adversarial attacks against aligned language models"
                },
                "questions": {
                    "value": "See weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6818/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6818/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6818/Reviewer_8tCQ"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6818/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699198900203,
            "cdate": 1699198900203,
            "tmdate": 1699714715835,
            "mdate": 1699714715835,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yNR1i19CA8",
                "forum": "V01FPV3SNY",
                "replyto": "IcBdfRdtX9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6818/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6818/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "# Response to Reviewer 8tCQ (1/2)\nThank you for your valuable feedback! \n\n**Q1: Comparison with concurrent work:**\n\n**A1:** Thank you for pointing out those concurrent works! We have updated our manuscript to discuss them ([1] [2]) in the Related Work section and added some experiments in Appendix G. \n\nNote that although [1] and our RA-LLM both explore the concept of partially erasing the prompt, the goal and the actual defense mechanisms are different. Specifically, [1] focused on providing a verifiable safety guarantee by enumerating all possible partially erased input and using a safety filter to identify the harmfulness of **input** content. In contrast, our defense aims to provide a practical design by examining the model's **response** (alignment check) to the partially erased input. Therefore, we believe [1] and our work focuses on different aspects here. Of course, combining the input inspection mechanism from [1] with our response check mechanism may further improve the defense effectiveness and we leave it as our future work.\nRegarding perplexity-based defense [2], we have conducted some extra experiments in Appendix G. Here we also summarize the comparison results in the following table. We can observe that even though [2] achieves high BAR and effectively reduces the ASR of individual GCG attacks, this defense mechanism completely fails to detect handcrafted jailbreak prompts, presumably owing to the lower perplexity of these prompts, as they are manually written by humans. In contrast, our method effectively defends against both GCG adversarial prompts and handcrafted jailbreak prompts.\n\n| Attack |Model | BAR Original LLM  | BAR Perplexity Defense | BAR RA-LLM | ASR Original LLM  | ASR Perplexity Defense | ASR RA-LLM |\n| :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- |\n|Individual GCG| Vicuna-7B-chat-HF| 99.3% | 98.0% | 98.7% | 98.7% | 0% | 10.7% |\n|Individual GCG| Guanaco-7B-HF | 95.3% | 100% | 92.0% | 96.0% | 4% |  6.7% |\n|Handcrafted prompt| Vicuna-7B-chat-HF| 99.3% | 98.0% | 98.7% | 98.7% | 100% | 12.0% |\n|Handcrafted prompt| Guanaco-7B-HF | 95.3% | 100% | 92.0% | 94.7% | 100% |  9.3% |\n\n**Q2: Effectiveness of RA-LLM on GPT3.5/4:**\n\n**A2:** Technically speaking, our method is applicable to any aligned LLM models. However, in order to show the effectiveness of our defense, we first need to obtain valid alignment-breaking prompts. While the adversarial prompts for those open-sourced models can be obtained by directly running the GCG attack, regrettably, we were unable to obtain enough successful adversarial prompts that are effective against GPT models (maybe due to that the authors of the GCG had previously disclosed this to OpenAI). Consequently, we were unable to perform our defense experiments on GPT models. Yet we agree that results on GPT3.5/4 would enhance our credibility. Thus, we further conducted experiments using hand-crafted jailbreak prompts and tested them on GPT3.5 and reported the results in the following table, which show that on GPT3.5, RA-LLM reduced ASR from 82.0% to 8.0%, without affecting BAR. We have also added these results to Section 4.3 of our paper. For GPT4, we found that the Handcrafted Jailbreak prompts used in our experiments are now ineffective, thus precluding related experiments. However, it is important to note that for models already possessing robust safety alignment, RA-LLM does not diminish their security. This is because, in the case of a malicious input, the worst outcome with RA-LLM is that the model generates a normal response without interfering with the content of the response. Should any viable data become available in the future, we are very willing to further supplement our experiments.\n\n\n\n|Model | BAR Original LLM  | BAR RA-LLM | ASR Original LLM  | ASR RA-LLM | ASR reduce |\n| :---- | :---- | :---- | :---- | :---- | :---- |\n|Vicuna-7B-chat-HF| 99.3% | 98.7% | 98.7% |12.0% | 86.7% |\n|Guanaco-7B-HF | 95.3% | 92.0% | 94.7% |  9.3% | 85.4% |\n|GPT-3.5-turbo-0613| 99.3% |  99.3% | 82.0% | 8.0% | 74.0% |"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6818/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700329371414,
                "cdate": 1700329371414,
                "tmdate": 1700329371414,
                "mdate": 1700329371414,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7fQ79YyMA5",
                "forum": "V01FPV3SNY",
                "replyto": "IcBdfRdtX9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6818/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6818/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "# Response to Reviewer 8tCQ (2/2)\n**Q3: Time Cost of RA-LLM:**\n\n**A3:** Thank you for your suggestion! We agree that time cost is equally crucial and we believe our design is already quite time-efficient for two main reasons:\n\n - Limited output length: we set the maximum generation length in Monte Carlo simulation to $t_{max} = 10$,  since the negative prefixes often appear at the start of LLM responses, allowing us to obtain effective defense without generating full responses. This has been discussed in our original submission Sections 3.3 and 4.5.\n - Early exit mechanism: during the Monte Carlo simulation, if the detected failure cases exceed our set threshold, RA-LLM can directly terminate the process early and mark the input as malicious. For instance, with $n = 20$ Monte Carlo trials and a threshold of $t = 0.2$, if $0.2 \\times 20 = 4$ aligned responses are already detected, RA-LLM will instantly mark it as malicious and reject the request. If the alignment check is passed for the first 17 simulations, RA-LLM will also instantly mark it as benign. This further reduces computational costs.  \n\n\n\nWe also evaluated the actual time overhead of RA-LLM against the original LLM in our experiments. We tested 150 attack samples and recorded the normal inference time on Vicuna and Guanaco models and the time required by our RA-LLM. We report the extra time per data on average overhead in the following table where the values in parenthesis represent the percentage of additional time relative to the normal inference time. These results show RA-LLM's extra time requirement is less than 20% of normal inference, and even in the worst case (completing all Monte Carlo simulations) - the extra time overhead is less than 45% of normal inference. Detailed settings and analysis of this experiment can be found in Appendix H.\n\n| Model | normal inference time  | RA-LLM extra time |  RA-LLM extra time (worst case)|\n| ---- | ---- | ---- | ---- |\n| Vicuna-7B-chat-HF |20.97s | 3.93s (18.7%) | 9.26s (44.1%)  |\n| Guanaco-7B-HF | 30.36s | 3.76s (12.4%) | 12.84s (42.3%)|\n\n[1] Aounon et al. Certifying llm safety against adversarial prompting. \n\n[2] Jain et al. Baseline defenses for adversarial attacks against aligned language models"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6818/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700329494090,
                "cdate": 1700329494090,
                "tmdate": 1700330277356,
                "mdate": 1700330277356,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BUYPLOj168",
                "forum": "V01FPV3SNY",
                "replyto": "IcBdfRdtX9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6818/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6818/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow up with reviewer 8tCQ"
                    },
                    "comment": {
                        "value": "We really appreciate the reviewer 8tCQ for the constructive comments, which significantly enhance the quality of our work. We would like to ask if there are any additional comments regarding our response, as the discussion phase is nearing its end. We are more than happy to address them. Additionally, we would appreciate if you could consider updating your scores if our rebuttal has satisfactorily addressed the concerns. Thanks again for your time and effort!"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6818/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700586436011,
                "cdate": 1700586436011,
                "tmdate": 1700586436011,
                "mdate": 1700586436011,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]