[
    {
        "title": "Compositional VLM: Composing Visual Entities and Relationships in Large Language Models Via Communicative Decoding"
    },
    {
        "review": {
            "id": "O7kuEjKGnv",
            "forum": "PHGxChm1l5",
            "replyto": "PHGxChm1l5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6693/Reviewer_fA94"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6693/Reviewer_fA94"
            ],
            "content": {
                "summary": {
                    "value": "Current vision-language models (VLMs) lack this ability due to their \"bag-of-words\" approach and inability to accurately represent visual entities and their relationships. The proposed Compositional VLM addresses this by guiding the model to explicitly compose visual entities and relationships and enabling dynamic communication between the visual detection system and the language system. This is achieved through the introduction of communication tokens, which guide the detection network to propose relevant visual regions based on the generated sentence. These regions are then integrated into the language generation process. This iterative communication between vision and language continues until a complete sentence is generated. This approach effectively bridges the gap between visual perception and language models, significantly outperforming other VLMs in compositional reasoning benchmarks. It also performs well in traditional vision-language tasks like referring expression comprehension and visual question answering."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. **Dataset Collection**: The paper collect new object level bounding box to image-text pairs ensures that the data is not just vast but also well-curated, enhancing the model's training and performance.\n\n2. **Reasonable Model Design**: The introduction of the Compositional VLM, with the token design framwork and the interation between visual detector LLM, displays a systematic approach towards bridging the gap between visual entities and their textual descriptions.\n\n3. **Comprehensive Experiments**: With the grounded image-text pairs, the paper offers a detailed experimental setup, validating the model across various compositional reasoning benchmarks.\n\n3. **Strong Performance**: The Compositional VLM showcases impressive results, especially when compared to previous vision-language models."
                },
                "weaknesses": {
                    "value": "1. **Fundamental Oversimplification of Compositionality:** The Compositional VLM framework, though innovative, may not truly capture the essence of disentangling objects and their relationships within images. Instead of delving deep into the inherent complexities of this challenge, the method leans towards gathering object-text-image paired datasets and reinforcing their connections. This approach, while seemingly effective, might only be a surface-level solution rather than addressing the root of the compositional problem.\n\n2. **Scalability Concerns**: The model's heavy reliance on precise associations between text captions and visual entities raises questions about its scalability. Can it consistently perform well in diverse or ambiguous scenarios, or will it be constrained by the specificity of its training data?\n\n3. **Rigid Token Implementation**: The manual nature of token positioning (with <obj> around the object and <box> after them), suggests a lack of flexibility in the model. This rigidity could hamper the model's adaptability, especially when faced with varied or unforeseen testing scenarios. For example, if we have a \"a man between two dog\", how to deal with the <box> for multiple instance of dog.\n\n4. **Operational Inefficiency**: The necessity for pre-parsing sentences and manually inserting tokens, even during testing, indicates potential operational bottlenecks. This could impede real-time applications and demands additional preprocessing steps, detracting from the model's overall efficiency."
                },
                "questions": {
                    "value": "1. When integrating the object detection model within the auto-regressive language model, what would be the time complexity for each individual inference? Would it be very slow?\n\n2. Given that the proposed Compositional VLM necessitates text parsing even during inference, how resilient is the model to inaccuracies or ambiguities in token placement? For instance, the phrase \"a black dog\" could be parsed as \"a <obj>black dog<\\obj>\" or \"a black <obj>dog<\\obj>\". How does the model handle such variations?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Responsible research practice (e.g., human subjects, data release)"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "New collected dataset, may require ethic review."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6693/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6693/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6693/Reviewer_fA94"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6693/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697957034151,
            "cdate": 1697957034151,
            "tmdate": 1700634102710,
            "mdate": 1700634102710,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4KxqdllOc1",
                "forum": "PHGxChm1l5",
                "replyto": "O7kuEjKGnv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6693/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6693/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer fA94 (part1)"
                    },
                    "comment": {
                        "value": "We thank you for your time and valuable comments. Below we answer the main concerns raised in the review and would be happy to provide further clarification if suitable.\n    \n> **Q1. What would be the time complexity for each individual inference? Would it be very slow?**\n\nIn our implementation, we use the YOLOX detection head as our object detection model, which is very lightweight compared to the auto-regressive language model and CLIP vision encoder. The number of parameters for the object detection model is negligible compared to the whole model.\n    \n|  | #Params | Latency |\n|:---:|:---:|:---:|\n| Object detector | 3.4M | 2.2 ms |\n| Vision encoder + LLM | 1,717.8M | 103.9 ms |\n| Percentage | 0.2% | 2.1% |\n\n\n\n    \nWe also measure the latency for the object detector and the vision encoder + LLM, separately. From the table above, the object detector takes only 2.1% of the total latency. Since the object detection model is not activated every time during training and inference, it will take less than 2.1\\% time when we generate a sequence of tokens.\n\n> **Q2. How resilient the model is to inaccuracies or ambiguities in token placement.**\n\nOur model is resilient to inaccuracies or ambiguities in token placement. We evaluate the token placement accuracy and our model's resilience to inaccurate token placement on RefCOCOg/RefCOCO+/RefCOCO datasets.\n* Token placement accuracy measures how accurate *\\<obj\\>* and *\\</obj\\>* is placed compared to the ground truth. We handcraft several templates to contain the referring expression into a sentence. For example, one template is \"I can see {expression} in the image\". If both *\\<obj\\>* and *\\</obj\\>* are placed correctly, we count it as a true positive in overall accuracy. The placement accuracy for *\\<obj\\>* and *\\</obj\\>* are also separately evaluated.\n* Localization accuracy measures the correctness of the predicted bounding box using either ground truth placement or predicted placement for *\\<obj\\>* and *\\</obj\\>*.\n\n|  | RefCOCOg val | RefCOCO+ val | RefCOCO val |\n|---|:---:|:---:|:---:|\n| **Token Placement Accuracy** |  |  |  |\n| Overall | 13.21 | 47.43 | 44.30 |\n| <obj> | 99.84 | 99.84 | 99.85 |\n| </obj> | 13.21 | 47.43 | 44.30 |\n| **Localization Accuracy** |  |  |  |\n| Predicted Placement | 58.74 | 45.77 | 45.71 |\n| Ground Truth Placement | 60.87 (+2.13) | 47.62 (+1.85) | 48.19 (+2.48) |\n\n\nFor token placement accuracy, the results show that the overall accuracy is not bad except on the RefCOCOg dataset, which mostly consists of long expressions. Therefore, it is harder for our model to correctly identify the whole expression. We further analyze the placement accuracy for *\\<obj\\>* and *\\</obj\\>* separately, and we can find that our model can generate *\\<obj\\>* in the right place in most cases, while the generation of *\\</obj\\>* is not always correct. We empirically find that our model often fails in cases such as \"an adult giraffe scratching its back with its horn\". It will be parsed as \"*<obj>*an adult giraffe scratching its back*</obj>* with its horn\" or \"*<obj>*an adult giraffe*</obj>* scratching its back with its horn\". \n\nFrom the results of localization accuracy, we can see that the accuracy of using predicted token placement and the accuracy of using ground truth token placement is close. It indicates that our model is robust to inaccurate or ambiguous token placement.\n\n> **Q3. Fundamental oversimplification of compositionality.**\n\nBringing compositionality to VLM is a challenging problem, and we are the very first step to handle this challenge. The good results in representative compositional reasoning benchmarks show the effectiveness of our method in improving the compositional reasoning ability of VLMs, and we acknowledge that there are some limitations in our method such as not coping much with object-attribute compositionality and spatial event compositionality, which could be crucial future directions.\n\n> **Q4. Rigid token implementation. If we have a \"a man between two dog\", how to deal with the <box> for multiple instances of dog?**\n\nOur model can deal with the situation that there are multiple objects in an image, thanks to our detector (i.e., a YOLO-style model) can localize them simultaneously. \n    \nTaking the text \"*a man between two dogs*\" as an example, a *<visual>* token is automatically generated after the word \"dogs\". Then, the object detector takes this token as input and outputs two boxes, each for one dog. All the features of predicted boxes will be sent back to LLMs, using one *<box>* for each detected box. Thus, the updated text would be \"*a man between two \\<obj\\>dog\\</obj\\><visual><box><box>*\". This gives our model the maximum flexibility to handle various scenarios."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6693/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700122723724,
                "cdate": 1700122723724,
                "tmdate": 1700122723724,
                "mdate": 1700122723724,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LFvvRJIVfq",
                "forum": "PHGxChm1l5",
                "replyto": "grVPcgGDDT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6693/Reviewer_fA94"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6693/Reviewer_fA94"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "Thank you for the detailed clarification, especially regarding the experiments with token replacement. This addresses my concerns about the accuracy of token placement effectively. However, I believe the approach might be overly simplistic. The claim that this is the 'very first step to handle this challenge' seems overstated, since the compositionality in VLM is prevailing, and has been repeated in brought up in the field."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6693/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700449215375,
                "cdate": 1700449215375,
                "tmdate": 1700449215375,
                "mdate": 1700449215375,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GlTrAkDrYn",
                "forum": "PHGxChm1l5",
                "replyto": "O7kuEjKGnv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6693/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6693/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Followup (part2)"
                    },
                    "comment": {
                        "value": "**Training data**\n* **Diverse data**. Some previous works such as [a,b,d] rely on rule-based templates to create specific training data. MosaiCLIP [a] uses a scene graph generator to generate the scene graph of the image, then construct captions based on this scene graph information using simple templates. SyViC [b] utilized synthetic data generated by a simulation platform. They then compiled a list of sentences, including a caption prefix, object enumeration, pairwise positional relations between objects, scene description, and action and clothing descriptions for each human. The sentences were concatenated to form a complete dense caption for the image. TSVLC [d] first parses the text into components such as nouns, verbs, etc., and replaces one word in that text using either pre-defined word lists or LLM prediction to form new captions. Our method relies on raw caption text and can be applied to any large-scale image-text pair datasets using an automatic labeling pipeline, so we have the maximum diversity in data.\n* **Realistic data**. SyViC [b] relies on synthetic data, which may induce a domain gap and lack of variety in content compared to using real-world image-text data. Our method can use data from any image-text data source.\n* **Faithful data**. Faithful data means that the text should be related to the image and there are no hallucinations in data or introduced during the data preprocessing. DAC [c] relies on LLM to generate more captions for a given image and its ground truth caption by asking LLM \"What should I expect to see in an image of \\{caption\\}?\". Some facts that generated by LLM are likely to be hallucinated and unrelated to the image since the LLM cannot see the image directly. Therefore, unfaithful data is generated for training which will harm the model. Unlike DAC, we keep the raw caption, and the grounding process will only ground the objects that are in the image to the textual description in the text, doing our best to reduce hallucinations.\n\nSince only TSVLC releases their pre-trained weight, we choose to compare the compositional reasoning ability of CLIP, TSVLC, and our model using the ARO Top-1/5 metric to further illustrate the importance of diverse, realistic, and faithful data.\n\n| Method | ARO Top-1 | ARO Top-5 |\n|---|---|---|\n| CLIP | 6.93 | 21.12 |\n| TSVLC | 8.19 | 22.78 |\n| **Ours** | **32.46** | **55.70** |\n\nThe results show that although TSVLC can improve the compositional reasoning ability for CLIP, our method can perform much better, thanks to the diverse, realistic, and faithful training data we have.\n\nWe thank for the Reviewer pointing out the overclaim issue, and we have already toned down this statement in our revision. Nevertheless, our method has significant differences from previous works as analyzed above. We test our method on various datasets and tasks including ARO, Cola, HICO-DET, RefCOCO, and VQAv2, and get significant performance improvement, showing the effectiveness of our method. The simple but effective nature of our method is also helpful when scaling up the model or pre-training data, leading to even better performance and wider application for much larger LLMs.\n\n> **Reference:**\n\n[a] Coarse-to-Fine Contrastive Learning in Image-Text-Graph Space for Improved Vision-Language Compositionality. ICCV 2023.\n\n[b] Going Beyond Nouns With Vision & Language Models Using Synthetic Data. ICCV 2023.\n\n[c] Dense and Aligned Captions (DAC) Promote Compositional Reasoning in VL Models. NeurIPS 2023.\n\n[d] Teaching Structured Vision & Language Concepts to Vision & Language Models. CVPR 2023."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6693/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700601085206,
                "cdate": 1700601085206,
                "tmdate": 1700603263163,
                "mdate": 1700603263163,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bddtgfXWz7",
                "forum": "PHGxChm1l5",
                "replyto": "GlTrAkDrYn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6693/Reviewer_fA94"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6693/Reviewer_fA94"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "I value the authors' efforts in introducing new related work and comparisons. However, I maintain that the current solution, while an improvement, is somewhat simplistic and lacks a fundamental understanding of the problem.\n\nIn recognition of their efforts, I've increased my score to 8, but this does not imply the paper is flawless."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6693/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634082663,
                "cdate": 1700634082663,
                "tmdate": 1700634082663,
                "mdate": 1700634082663,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pfnqWjkjoV",
            "forum": "PHGxChm1l5",
            "replyto": "PHGxChm1l5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6693/Reviewer_Aw71"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6693/Reviewer_Aw71"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents Compositional VLM, a vision-language architecture allowing the language model (LLM) to communicate with the vision encoder(s) to generate its output iteratively. Several new tokens, such as <box>, <prebox>, <visual>, and <previsual>, are introduced to facilitate this communication between the two components working on different modalities. The goal is to improve the ability of VLMs to capture the compositional structure found in vision-language tasks. The model is pre-trained on a large corpus of grounded image-text pairs from sources such as COCO, CC3M, CC12M, Visual Genome (Krishna et al., 2017), SBU, and LAION400M. The model is thoroughly evaluated on many tasks, such as compositional visual understanding, referring expression comprehension, VQA, and human-object interaction. The quantitative performance shows the benefit of improved communication between vision-language modules and outperforms considered VLM baselines on public benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ The paper addresses an important topic for VLMs. Imbuing VLMs with compositional visual understanding is an important direction of research, and the proposed approach is an interesting mechanism to achieve it.\n+ The bidirectional communication between the vision and language modules is interesting and is an important component that needs to be introduced and explored in VLM research. Existing works do not have iterative communication, and relying only on global features for alignment/pre-training is insufficient to exhibit compositional understanding.\n+ The quantitative performance is strong and shows consistent gains over the considered baseline VLMs on several tasks and benchmarks.\n+ The paper is well-written and the idea is simple and presented intuitively, making it easy to follow and understand."
                },
                "weaknesses": {
                    "value": "- My primary concern is about the pre-training data and how to interpret the results. The pre-training dataset consists of text-image pairs from COCO, CC3M, CC12M, Visual Genome (Krishna et al., 2017), SBU, and LAION400M. The evaluation datasets share considerable overlap with the pre-training data. For example, COLA is composed of data from GQA (derived from Visual Genome), CLEVR, and LVIS (also includes COCO). Similarly, ARO is based on Visual Genome, and HICO-DET is based on Flickr (with objects limited to those from COCO). I understand that the training data is essentially the same as BLIP-2 and that it is common for VLMs to be trained on these datasets. The question does remain: given this overlap, how well does the model generalize? The generalization can be quantified based on two factors - tasks and domains. From the task perspective, the generalization of the proposed approach seems to be limited, going by the performance of VQA. There are no convincing arguments for the generalization beyond training data. How about the performance on PACO[1]? It does not have a **significant** overlap with the pre-training datasets and should provide some measure of generalization.\n- There are very few qualitative results presented. There are 4 examples in the supplementary, but they are very limited beyond that. By reading the paper, it is hard to understand when/where/why the model fails. For example, is there a reason why the model has a higher performance on the \"rare\" class from HICO-DET? It would be good to understand the success and failure modes and have a discussion on the approach's qualitative performance as opposed to the purely quantitative take.\n- There are no ablations presented. What does the choice of encoders/pre-training data/tokens have on the model?\n- On that note, how sensitive is the model to the template used for prompting?\n\nReferences:\n[1] Ramanathan, Vignesh, et al. \"Paco: Parts and attributes of common objects.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023."
                },
                "questions": {
                    "value": "My major concerns are the generalization capabilities and the lack of qualitative results and ablation studies, detailed in the weaknesses section.\n\n--- Post-rebuttal update ---\nI am raising my score after the authors' response."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6693/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6693/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6693/Reviewer_Aw71"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6693/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698682537511,
            "cdate": 1698682537511,
            "tmdate": 1700584612513,
            "mdate": 1700584612513,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ugFJSw51H1",
                "forum": "PHGxChm1l5",
                "replyto": "pfnqWjkjoV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6693/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6693/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Aw71"
                    },
                    "comment": {
                        "value": "We thank you for your time and valuable comments. Below we answer the main concerns raised in the review and would be happy to provide further clarification if suitable.\n\n> **Q1. Generalization capabilities.**\n\nWe thank the reviewer for recommending a related benchmark PACO [a] that is more suitable to provide some measure of generalization. We evaluate the AR@1 metric on all L1 queries under the zero-shot instance detection setting.\n\n| Model | Type | PACO L1 AR@1 |\n|---|---|---|\n| MDETR R101 [b] | Open-vocabulary detector | 4.9 |\n| Detic Swin-B [c] | Open-vocabulary detector | 5.9 |\n| KOSMOS-2 | VLM | 8.2 |\n| Compositional VLM 1.4B | VLM | 9.4 |\n\nThe results show that our method can generalize well to other image domains and harder tasks. Please see G3 for a detailed explanation of the results.\n\n\n> **Q2. There are very few qualitative results presented..**\n\nThanks. We have added more qualitative results and discussion in Appendix A.5 in our revision.\n\n> **Q3. There are no ablations presented. What does the choice of encoders/pre-training data/tokens have on the model?**\n\nThanks for your advice. We have added an ablation study on communication tokens in G1.\n\n\n| Setting | <previsual> | <prebox> | <visual> | <box> | ARO Top-1 | Cola |\n|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| 1 | x | x | x | x | 29.60 | 38.10 |\n| 2 | x | x | \u2713 | \u2713 | 30.08 | 34.76 |\n| 3 | \u2713 | x | \u2713 | x | 32.26 | 36.19 |\n| 4 | \u2713 | \u2713 | \u2713 | \u2713 | **32.46** | **44.29** |\n\nThe ablation study results demonstrate the necessity and effectiveness of the proposed communication tokens. Please refer to G1 for detailed explanation.\n\nFor the choice of encoder, we follow previous works such as BLIP-2 [d] and LLaVA [e] to use pre-trained CLIP ViT-L [f] as our vision encoder, as the pre-trained CLIP ViT is proved to be a powerful model in various downstream tasks. For the choice of pre-training data, the only concurrent public large-scale grounded image-text dataset is from KOSMOS-2, but KOSMOS-2 only releases a 20 million subset of its full 90 million data. As the data quantity is important for multimodal pre-training, we choose to create our own large-scale grounded image-text dataset based on the widely used BLIP-2 pre-training data. Our pre-training dataset consists of over 97 million image-text pairs, comparable to KOSMOS-2's full pre-training data.\n\nDue to the short period for rebuttal and the time-consuming nature of multimodal pre-training, it is hard for us to conduct an ablation study on encoders and pre-training data during the rebuttal period. We leave the additional ablation study on different encoders and different sets of pre-training data in our future work and we are happy to have more discussion with the Reviewer if needed.\n\n> **Q4. How sensitive is the model to the template used for prompting?**\n\nOur model is robust to the template used for prompting. We evaluate the performance of using different prompts in RefCOCOg/RefCOCO+/RefCOCO benchmarks.\n\n| Prompt | RefCOCOg val | RefCOCO+ val | RefCOCO val |\n|---|:---:|:---:|:---:|\n| Locate the object: <obj>{text}</obj><visual> | 60.64 | 46.83 | 47.50 |\n| Locate: <obj>{text}</obj><visual> | 60.89 | 46.97 | 47.63 |\n| Find: <obj>{text}</obj><visual> | 60.89 | 47.20 | 47.60 |\n| <obj>{text}</obj><visual> | 60.87 | 47.62 | 48.19 |\n\nThe evaluation results do not differ much when using different prompts, indicating that our model is robust to the prompt template.\n\n> **Reference:**\n\n[a] PACO: Parts and Attributes of Common Objects. CVPR 2023.\n\n[b] MDETR - Modulated Detection for End-to-End Multi-Modal Understanding. ICCV 2021.\n\n[c] Detecting Twenty-thousand Classes using Image-level Supervision. ECCV 2022.\n\n[d] BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models. ICML 2023.\n\n[e] Visual Instruction Tuning. NeurIPS 2023.\n\n[f] Learning Transferable Visual Models From Natural Language Supervision. PMLR 2021."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6693/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700122599108,
                "cdate": 1700122599108,
                "tmdate": 1700194210372,
                "mdate": 1700194210372,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DRQcE8X6As",
                "forum": "PHGxChm1l5",
                "replyto": "ugFJSw51H1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6693/Reviewer_Aw71"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6693/Reviewer_Aw71"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "Thanks for your response with the additional comparison. It does mostly address my concerns. I am raising my score to 6."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6693/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700584527926,
                "cdate": 1700584527926,
                "tmdate": 1700584527926,
                "mdate": 1700584527926,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "scciMc6z1e",
            "forum": "PHGxChm1l5",
            "replyto": "PHGxChm1l5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6693/Reviewer_onDF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6693/Reviewer_onDF"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces Compositional VLM, a new Large Vision-language Model that can compose visually grounded concepts and relationships within a given text input. It achieves this by employing a set of special tokens to manipulate the interactions between the LLM and a visual object detection network. This bi-directional communication of vision-to-language and language-to-vison occurs through multiple iterations as an output sentence is generated. The proposed model outperformed prior works across various compositional reasoning tasks and vision-language tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The object-centric approach presented in this paper is interesting as I believe visual objects and words are of the same level of abstraction of representation. I also appreciate the idea of multi-step interactions between an LLM and a visual object detection network, much like older multi-step reasoning techniques in the joint vision-language understanding literature.\n\n- The experiments demonstrate significant improvements over the existing works across a number of downstream tasks and benchmarks."
                },
                "weaknesses": {
                    "value": "- The presentation of the method is brief, making it hard to understand. It is not clear how the communication tokens are generated. I recommend providing a pseudo algorithm to describe the method and making the input/output for each step more readable.\n\n- The proposed pre-training procedure seems to incorporate a lot of engineering tricks and data processing techniques from KOSMOS-2, without adequate attribution to previous works. The paper also lacks justifications for the selection of sub-components in the proposed method, making it hard to interpret the results. \n\n- Pretrained data and downstream tasks (i.e., RefCOCO/RefCOCO+, VQA v2) share the same pool of visual data taken from COCO and Visual Genome. This raises concerns about the validity of the reported performance on these downstream tasks."
                },
                "questions": {
                    "value": "-\tWhy did you opt to create your own dataset of similar scale for pretraining, rather than utilizing the data from KOSMOS-2?\n-\tCould you provide further analyses of the contributions of these communication tokens?\n-\tOn the task REFERRING EXPRESSION COMPREHENSION: the proposed model outperforms the KOSMOS-2 model, but with a small gap. While the grounding of vision-language in KOSMOS-2 was reasonably good compared to the proposed model, it exhibited significantly poorer performance in the previous three tasks. Can you provide insights into these differences?\n\n-\tCan you clarify the large margins between KOSMOS-2 and the proposed method in Table 1? What would be the main contributor to this significant performance leap?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6693/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698796804975,
            "cdate": 1698796804975,
            "tmdate": 1699636767838,
            "mdate": 1699636767838,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ylQALdjmtC",
                "forum": "PHGxChm1l5",
                "replyto": "scciMc6z1e",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6693/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6693/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer onDF (part1)"
                    },
                    "comment": {
                        "value": "We thank you for your time and valuable comments. Below we answer the main concerns raised in the review and would be happy to provide further clarification if suitable.\n\n> **Q1. Why do you create your own dataset rather than using data from KOSMOS-2.**\n\nKOSMOS-2 has made their data public, but they have only released a **subset** of it, about 20 million image-text pairs instead of the full dataset with 90 million image-text pairs. A large-scale and high-quality dataset is essential for pre-training, therefore, we turned to create our own dataset which has a similar scale compared to KOSMOS-2. The dataset we create consists of over 97 million image-text pairs. We will make our full dataset public in the future.\n\n> **Q2. Could you provide further analyses of the contributions of the communication tokens?.**\n\nCommunication tokens are designed to enable bidirectional communication between the vision module and the language module. They can summarize the compositionality information within text to guide the vision module to detect the desired objects, and can also bring the visual feature of the object back to the language module to guide the LLM to notice the localized visual feature, thus helping it to better generate more faithful and related tokens thereafter. We have added an ablation study on communication tokens in G1.\n\n| Setting | <previsual> | <prebox> | <visual> | <box> | ARO Top-1 | Cola |\n|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| 1 | x | x | x | x | 29.60 | 38.10 |\n| 2 | x | x | \u2713 | \u2713 | 30.08 | 34.76 |\n| 3 | \u2713 | x | \u2713 | x | 32.26 | 36.19 |\n| 4 | \u2713 | \u2713 | \u2713 | \u2713 | **32.46** | **44.29** |\n\nThe ablation study results demonstrate the necessity and effectiveness of the proposed communication tokens. Please refer to G1 for detailed explanation."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6693/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700122350244,
                "cdate": 1700122350244,
                "tmdate": 1700122371032,
                "mdate": 1700122371032,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Xj76hhm53k",
                "forum": "PHGxChm1l5",
                "replyto": "scciMc6z1e",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6693/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6693/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer onDF (part2)"
                    },
                    "comment": {
                        "value": "> **Q3. The proposed model outperforms the KOSMOS-2 model in referring expression comprehension tasks but with a small gap. While the grounding of vision-language in KOSMOS-2 was reasonably good compared to the proposed model, it exhibited significantly poorer performance in the previous three tasks. Can you provide insights into these differences?**\n\nThe performance gap is small in referring expression comprehension tasks because KOSMOS-2 and our model both adopt a similar manner for testing. This gap is large in the previous three tasks because the communication tokens in our method can facilitate our model to understand object relations and focus on a localized ROI to generate more faithful object predictions. The deeper analysis is as below:\n\n1. For **referring expression comprehension** tasks, both of us use a format of *\\<obj\\>*{text}*\\</obj\\>* as the prompt to the model, and then enable the model to output location information. KOSMOS-2 outputs some location tokens to represent the location, while our model outputs the bounding box through the object detector. The advantages of communicative decoding and the bidirectional communication between the vision module and the language module are not fully exploited in this task, thus the performance gap between KOSMOS-2 and ours is reasonably small.\n\n2. The **HICO-DET** benchmark asks the model to detect all human-object interactions in the image. Many images contain more than one human-object interaction, that is, there are multiple objects corresponding to the same textual description that need to be identified correctly. We empirically find that compared to KOSMOS-2, our model will make it easier to detect more correct objects corresponding to the same correct textual description in the image. Therefore, our model can find out the more complete human-object interactions within an image. This is thanks to our YOLO-like object detector that can naturally identify all correct objects in the image. The input image will be encoded by the vision encoder to become a map of 16x16 patches. When performing the object detection, each patch will be concatenated with the hidden state of *\\<previsual\\>*/*\\<visual\\>* from LLM, and then fed to the object detector to get a score indicating the probability that this patch contains the center of the object, and also get a regressed bounding box coordinates indicating the left-top and right-bottom coordinates of the object if exists. Each patch can produce a probability and object bounding box individually, thus it is natural and convenient for our model to identify multiple objects.\n\n3. **ARO** requires that given entity A and relation, the model should predict entity B to which the relation is applied. It requires the model to understand where is entity A and what's the relation and use this information to infer the entity that A is applying the relation to. The design of *\\<previsual\\>*/*\\<prebox\\>* communication tokens can handle this relation information well. By using the information of entity A and the relation, *\\<previsual\\>*/*\\<prebox\\>* communication tokens can guide the LLM to focus on the potential regions for entity B, so it would be much easier to correctly predict entity B. **Cola** benchmark also require the model to understand both the objects and their relation, thus *\\<previsual\\>*/*\\<prebox\\>* communication tokens can help in this task. Since KOSMOS-2 lacks communication tokens and explicit guidance for LLM to focus on a specific region, it does not perform well in such tasks.\n\n> **Q4. Can you clarify the large margins between KOSMOS-2 and the proposed method in Table 1? What would be the main contributor to this significant performance leap?**\n\nThe large margins between KOSMOS-2 and our proposed method in Table 1 are mainly because our model has adopted a novel bidirectional communication between the vision module and language model via communication tokens, which brings a large improvement in compositional reasoning ability. Our ablation study especially shows the importance of the proposed communication tokens for compositional tasks (please refer to G2). KOSMOS-2 does not have this kind of design and thus performs much poorer in these tasks.\n    \n> **Q5. The presentation of the method is brief, making it hard to understand. It is not clear how the communication tokens are generated. I recommend providing a pseudo algorithm to describe the method and making the input/output for each step more readable.**\n\nCommunicate tokens are automatically generated during the auto-regressive generation process. We have added an explanation of the communication token generation in G1 and a pseudo algorithm in Appendix A.1 in our revision."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6693/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700122435065,
                "cdate": 1700122435065,
                "tmdate": 1700602764388,
                "mdate": 1700602764388,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uP8MpdKeve",
                "forum": "PHGxChm1l5",
                "replyto": "scciMc6z1e",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6693/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6693/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer onDF (part3)"
                    },
                    "comment": {
                        "value": "> **Q6. No adequate attribution to previous works.**\n\nThanks for your reminder. During our data-creating process, we have borrowed some techniques from KOSMOS-2 including using spaCy to parse the text and use *\\<obj\\>\\</obj\\>* to enclose an object. We also have some differences from KOSMOS-2. For example, we use GroundingDINO to directly ground the textual description in the text to the object in the image. In contrast, KOSMOS-2 first parses the sentence to extract all noun chunks. Then, they eliminate certain abstract noun chunks that are hard to recognize such as \"time\" and \"love\". Finally, they use GLIP to obtain the bounding boxes for these noun chunks. In comparison, our data-creating method is more simple but still effective. We have revised to give more adequate attribution to previous works in our revision.\n\n> **Q7. Lack of justifications for the selection of sub-components in the proposed method.**\n    \nThanks for your reminder. We have added the ablation study on communication tokens. Detailed results and analysis are shown in G2, which indicates the effectiveness and necessity of these tokens.\n\n> **Q8. Pretrained data and downstream tasks (i.e., RefCOCO/RefCOCO+, VQA v2) share the same pool of visual data taken from COCO and Visual Genome.**\n\nIt is worth noting that in KOSMOS-2's instruction tuning stage, LLaVA-Instruct [b] data is used to fine-tune the model, and the images in LLaVA-Instruct data are from COCO. Therefore, KOSMOS-2's training data and downstream tasks also share the same pool of visual data from COCO. The visual data overlap concern is not unique to our model.\n\nTo prove the generalization of our model, we further evaluate our model on PACO [a] as recommended by reviewer Aw71.\n\n| Model | Type | PACO L1 AR@1 |\n|---|---|---|\n| MDETR R101 [b] | Open-vocabulary detector | 4.9 |\n| Detic Swin-B [c] | Open-vocabulary detector | 5.9 |\n| KOSMOS-2 | VLM | 8.2 |\n| Compositional VLM 1.4B | VLM | 9.4 |\n\nOur model continues to perform well in this dataset, indicating our model's good generalization to other image domains. The dataset description and result analysis for the PACO dataset can be found in G3.\n\n> **Reference:**\n    \n[a] PACO: Parts and Attributes of Common Objects. CVPR 2023.\n    \n[b] MDETR - Modulated Detection for End-to-End Multi-Modal Understanding. ICCV 2021.\n    \n[c] Detecting Twenty-thousand Classes using Image-level Supervision. ECCV 2022."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6693/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700122492471,
                "cdate": 1700122492471,
                "tmdate": 1700194185821,
                "mdate": 1700194185821,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]