[
    {
        "title": "Avalon's Game of Thoughts: Battle Against Deception through Recursive Contemplation"
    },
    {
        "review": {
            "id": "ojhOwFgMO0",
            "forum": "fvhJu0FODp",
            "replyto": "fvhJu0FODp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission118/Reviewer_p3aL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission118/Reviewer_p3aL"
            ],
            "content": {
                "summary": {
                    "value": "The paper explores the challenges LLMs face in processing deceptive information, utilizing the game Avalon for evaluation. It demonstrates that incorporating human-like cognitive patterns improves LLMs' performance in the game. The authors introduce \"ReCon\", a method that merges \"formulation and refinement contemplation\" with \"first- and second-order perspective transitions\" for LLM prompts. In practice, this involves a phase in which the LLM is asked to contemplate internally from a first-order (its own) perspective, and then to contemplate again from a second-order (other players') perspective. This method's efficacy is validated using ChatGPT and Claude in the Avalon game."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The results are interesting, the paper is well written, and the analysis is reasonable. The work is also novel, I believe, and it improves our understanding about how to make LLMs safer."
                },
                "weaknesses": {
                    "value": "My main comment is that I would like to see how the proposed approach may be applied in case of attempts to extort PII from LLMs or jailbreaking outside of the Avalon environment. It should be fairly straightforward to assess this. Have the authors tried it or can they comment on it at least? \n\nOther comments:\n\n* The paper argues that ReCon is superior to CoT and its variants that lack either contemplation or perspective transition. I think that the results are pretty convincing, although I want to see some confidence intervals for the estimates in Figure 4. One inconsistency I noted, which the authors also mention, is the differing performance between ReCon without refinement contemplation and ReCon without formulation contemplation when applied to Claude versus ChatGPT. Some insight into this discrepancy would be useful. It might also be beneficial to contrast paired strategies, such as ReCon with and without refinement contemplation, to reinforce the findings. Have the authors tried this approach? \n\n* Another question is regarding the scalability of LLMs: Would CoT match ReCon's performance when applied to larger LLMs? \n\n* The number of games used to generate Figure 4 is unclear. It's unclear whether Figure 4's results are derived from the 20 full Avalon games mentioned in the 4.2 Setup, especially since multiple strategies were simultaneously deployed for a single side (good/evil) in this context so that's likely not the case. Clarification on this, along with the inclusion of confidence intervals for the results mentioned above, would provide readers with a clearer understanding of the robustness of the findings.\n\n* The statement \"we use ReCon for the good side\" seems inconsistent and should likely read \"we use CoT for the good side,\" correct? The paper also contains a few typographical errors that need rectification.\n\n* It would be beneficial for the authors to disclose the specific prompts used for GPT-4 across the various evaluation metrics. Furthermore, it's worth pondering whether the results would remain consistent if humans, rather than CoT, played the game. Lastly, why omitting Claude's performance across all evaluation metrics."
                },
                "questions": {
                    "value": "Mentioned above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission118/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698107877733,
            "cdate": 1698107877733,
            "tmdate": 1699635937192,
            "mdate": 1699635937192,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ymv58rKY5C",
                "forum": "fvhJu0FODp",
                "replyto": "ojhOwFgMO0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission118/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission118/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal to Reviewer p3aL"
                    },
                    "comment": {
                        "value": "Thank you for your careful review and constructive suggestions, which have helped us improve our manuscript. We are pleased to address your concern as follows.\n\n> Q1: The paper argues that ReCon is superior to CoT and its variants that lack either contemplation or perspective transition. I think that the results are pretty convincing, although I want to see some confidence intervals for the estimates in Figure 4.\n\nWe are grateful for your suggestion and have addressed your concern in `Global Q2`. For more details, please see `Global Q2` in the [Global Responses](https://openreview.net/forum?id=fvhJu0FODp&noteId=Y7twEtlQGy).\n\n\n\n---\n\n\n\n> Q2: One inconsistency I noted, which the authors also mention, is the differing performance between ReCon without refinement contemplation and ReCon without formulation contemplation when applied to Claude versus ChatGPT. Some insight into this discrepancy would be useful.\n\nThank you for pointing out this aspect of our study. The discrepancy you noticed in Figure 4 could likely be **due to the different abilities of LLMs to hide internal thoughts from their spoken outputs**. Our empirical analysis of the game logs suggests that ChatGPT, particularly GPT-4, is more adept at concealing internal thoughts from spoken outputs than Claude.\n\nAs a result, for hiding internal thoughts in spoken content, **the refinement contemplation based on GPT-4 may be more effective than the formulation process used in ReCon with ChatGPT. Conversely, the formulation contemplation is more critical for ReCon when implemented with Claude.** Thus, in Figure 4, the relative magnitude of the performance without refinement contemplation (the green bin) compared to the performance without formulation contemplation (the red bin) is different between ChatGPT and Claude.\n\n\n\n---\n\n\n\n> Q3: It might also be beneficial to contrast paired strategies, such as ReCon with and without refinement contemplation, to reinforce the findings. Have the authors tried this approach?\n\nThank you for your suggestion. Yes, we have carried out the ablation study you referred to, as depicted in Figure 4 (comparing the purple bin with the green bin) and in Figure 5(b) of our paper.\n\n\n\n---\n\n\n\n> Q4: Another question is regarding the scalability of LLMs: Would CoT match ReCon's performance when applied to larger LLMs?\n\nWe appreciate your interest in the scalability of large language models. We agree that this is an important and relevant question to investigate.\n\nOur opinion is that in certain situations, particularly those involving deceptive cognitive processes like in the Avalon game, **CoT would not be as effective as ReCon**. This is because an LLM agent using CoT is akin to a thoughtful person whose thoughts are fully exposed and lacks the ability to take on different perspectives. In contrast, an LLM agent with ReCon can be likened to a thoughtful person who can separate internal thoughts from spoken content and possesses the capability of perspective-taking, leading to more comprehensive thinking. Even with human intelligence as it stands, in environments characterized by deceptions and complex logic, a person thinking with ReCon is likely to have an advantage over one using CoT. Therefore, in the foreseeable future, even if LLMs reach human-level intelligence, ReCon is expected to outperform CoT in scenarios involving deceptions and complex logic."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission118/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700235620375,
                "cdate": 1700235620375,
                "tmdate": 1700235620375,
                "mdate": 1700235620375,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WtgyZeJKZj",
                "forum": "fvhJu0FODp",
                "replyto": "ojhOwFgMO0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission118/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission118/Authors"
                ],
                "content": {
                    "title": {
                        "value": "(Continued) Rebuttal to Reviewer p3aL"
                    },
                    "comment": {
                        "value": "> Q5: The number of games used to generate Figure 4 is unclear. It's unclear whether Figure 4's results are derived from the 20 full Avalon games mentioned in the 4.2 Setup, especially since multiple strategies were simultaneously deployed for a single side (good/evil) in this context so that's likely not the case. Clarification on this, along with the inclusion of confidence intervals for the results mentioned above, would provide readers with a clearer understanding of the robustness of the findings.\n\nWe apologize for the lack of clarity in our paper regarding the number of games used for Figure 4. **To clarify, we conducted 20 games for each model shown in Figure 4, maintaining this number consistently across all experiments except for Claude, which involved around 40 games.** The rationale behind conducting a higher number of games for Claude is detailed in `Global Q2` of the [global responses](https://openreview.net/forum?id=fvhJu0FODp&noteId=Y7twEtlQGy). Additionally, regarding your suggestion about confidence intervals, we have included statistical tests in `Global Q2` of the [global responses](https://openreview.net/forum?id=fvhJu0FODp&noteId=Y7twEtlQGy). Thank you again for your constructive advice.\n\n\n\n---\n\n\n\n> Q6: The statement \"we use ReCon for the good side\" seems inconsistent and should likely read \"we use CoT for the good side,\" correct? The paper also contains a few typographical errors that need rectification.\n\nWe apologize for the confusion caused by the lack of clarity in our paper. Indeed, it was our intention to use the ReCon method for the good side throughout our experiments. \n\nWe have tried to test the good side using the CoT method against the evil side with ReCon. However, given the evil side's advantage (per [Avalon statistics](https://www.proavalon.com/statistics)), the good side consistently lost, leading to uniform results. Thus, **for more discriminative outcomes, we used ReCon on the good side when assessing methods for the evil side.**\n\nWe will make this explanation clear in our revised paper.\n\n\n\n---\n\n\n\n> Q7: It would be beneficial for the authors to disclose the specific prompts used for GPT-4 across the various evaluation metrics. \n\nThank you for raising this meaningful question. For full transparency, we have provided all the complete prompts used with GPT-4 for evaluation in [this anonymous link](https://anonymous.4open.science/r/ICLR_Avalon_Rebuttal_ID_118/gpt4_evaluation_prompts.pdf) for your reference.\n\n\n\n---\n\n\n\n> Q8: It's worth pondering whether the results would remain consistent if humans, rather than CoT, played the game.\n\nThank you for your constructive question. Regarding the comparison with human players, we first acknowledge that current LLMs, even those utilizing the ReCon method, may not yet match the performance level of expert human players. This is a recognition of the inherent limitations of current LLM. Additionally, the challenge is further compounded by the lack of a comprehensive Avalon game dataset tailored for fine-tuning LLMs, which limits our ability to enhance their capabilities in this context. However, our paper demonstrates significant improvements when using ReCon in the Avalon game, a finding we believe is substantial compared to the CoT method. \n\nAnother concern when considering conducting human experiments is that Avalon is a complex strategic game. **The gap in performance between novice and experienced players is substantial. In contrast, GPT-4's performance is more consistent, making our experimental results more transparent and reproducible.** However, you make an excellent point; conducting experiments with human players to verify the consistency of our results is crucial, and this will be an important focus of our future work.\n\n\n\n---\n\n\n\n> Q9:  Why omitting Claude's performance across all evaluation metrics.\n\nRegarding the exclusion of Claude's performance, **our paper primarily focuses on ChatGPT as the representative model**. Consequently, apart from the initial experiment in Figure 4, we carried out additional analysis experiments specifically on ChatGPT. Moreover, **since our ReCon designs proved consistently effective for both ChatGPT and Claude in Figure 4, it's likely that the extra analysis on ChatGPT could mirror similar results for Claude.** Nonetheless, we acknowledge the value of including Claude's performance across all evaluation metrics and plan to thoroughly analyze Claude's performance in our future research.\n\n---\n\nThank you once again for your valuable feedback and advice. We look forward to further discussion to refine our work."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission118/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700235743617,
                "cdate": 1700235743617,
                "tmdate": 1700726888583,
                "mdate": 1700726888583,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MUmwucVQBR",
            "forum": "fvhJu0FODp",
            "replyto": "fvhJu0FODp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission118/Reviewer_48Jp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission118/Reviewer_48Jp"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents an LLM prompting setup termed \"ReCon\" consisting of\n\n1. one prompt from the perspective of the agentic-role the LLM is intended to assume, consisting of a description of the scenario, the task description and task-hints (information about the game the prompt is evaluated, what to pay attention to, generalized action recommendations)\n2. one prompt to guide the LLM through a THINK-SPEAK pattern similar to ReACT, with nudging language specifically guiding the \"THINK\" inference towards the goal\n3. one prompt restating the role, potentially updated scenario and asking the model to critique the \"SPEAK\" content (but not the THINK content) from the viewpoint and goals of the other roles, again with specific nudges (similar to Self-Ask)\n4. one prompt asking the LLM to act as a critique of a player+role, restating the updated situation and asking the model to offer possible improvements guided by hints\n\nThis prompt template is avaluated on the role playing game \"Avalon: The Resistance\" which is described in detail in the appendix and evaluated by having OpenAI GPT-4 and Anthropic Claude play $n=20$ games of one of four ablations (Recon, Recon wo/ ReACT aka prompt , ReCon w/o Self-ask  aka prompt 3, CoT) and evaluated across 6 metrics based on asking GPT-4 to compare rank the two (ranking primpt not included).\nA comparison on LLama 2 was attempted but not completed due to formatting-output issues."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "I feel very mean and mean no offense but I cannot point to any particular unconditional strength in the main axes of consideration.\nIn an effort to highlight positives:\n\n1. The complete paper is very pretty and visually appealing\n2. based on the related work the authors are very familiar with the literature.\n3. The method *does* win against the baseline in the task proposed"
                },
                "weaknesses": {
                    "value": "1. If the point is to present Avalon as a new benchmark for langauge game playing that as the paper claims \"goes beyond a language game and a game of thought\", then that benchmark would have to be evaluated with *all* currently relevant baseline methods, as well as studied and discussed in detail to justify the claims: *why* is it a game of thought while e.g. the other games discussed in Serrino et al aren't?\n2. If the point is to contribute ReCon, then what is missing is an evaluation on HumanEval, or some other task except the one it appears to be specifically constructed for. E.g., how would one adapt this to a negotiation game, a market game or another situation in which the roles are not as clearly defined etc?\n\nOn top of this, the methodology of the evaluation is not strong enough to support anything.\nThis is currently par for the course and I am sorry that the authors encounter me as a reviewer, but very basic scientific standards currently disqualify most of the established evaluation procedures:\n\n- [recent leaks of basically all large LLM providers system prompts](https://gist.github.com/cedrickchee/9390389d755e574cca24a2b42aaa7d47) show the large possible confounding nature of using blackbox APIs for any form of evaluation, beyond the unknown other methods and filters that might be employed. The authors acknowledge the data and version they use, but this makes replicability impossible, even without the fact that the non-zero temperature and sparse MoE inference at these APIs makes the evaluation indeterministic\n- $n=20$ appears very low to me and I would like to see a suitable statistical significance test together with the motivation for choosing and interpreting it (i.e., choosing an appropriate correcting like [this one](https://en.wikipedia.org/wiki/Bonferroni_correction), or justifying a less conservative one)\n- I am also confused why the authors didn't use simply use [guidance](https://github.com/guidance-ai/guidance) for their LLamav2 experiments, since they seem otherwise very tuned into the LLM world.  However, Occams *and* Hanlons razor tell me to assume simple unawareness. If the experiments succeed with the help of Guidance, I'd be curious to see the updated results\n- That tuned-in-ness brings me to another critique: the described method is basically a specialized application of [Tree of Thoughts](https://arxiv.org/abs/2305.10601). While it might be a concurrent work, the authors also do not adequately put their method into context with ReACT and self-ask as I have done in my summary. What would need to be done is to create a matrix or table comparing all extant/widely used prompt techniques and comparing them while highlighting only *unquestionable* differences\n- The room for this could be gained by removing a lot of the unsubstantiated bombast from the introduction and section 2. While I appreciate the aesthetics, I don't think quoting the three body problem, discussing AGIs or making grand claims about CoT not causing the Morgana-role to disclose its thoughts *despite that being in the role description* is suitable for an otherwise quite thinly supported paper. Without more *stats* and *quantifiable information* in the appendix, I also think we do not need to see more individual examples of impressive chats in LLM papers. If e.g. Fig c) shows consistent behaviour, what I would *expect* to see is a section in the appendix detailing \n  1. a definition of hidden thought deception that is operationalizable, maybe even with python code\n  2. An experimental setup that evaluates this operationalized definition, including all prompts etc. used\n  3. a statistical evaluation\n  4. a discussion of possible confounders and alternative explanations\n- meanwhile, the authors *don't even give the complete prompt in the appendix*. This is the room where you do not need to worry about brevity dear authors, please just add the original prompts, the prompts used to perform the GPT-4 ranking, basically *everything* that could be used to evaluate, judge and replicate the paper\n- finally, these are nitpicks, but it is meant to illustrate why I can't even call the writing clear or pleasant:\n    - In the introduction, beyond lacking any justification for claiming a \"game of thought\"...\n    - ...how exactly did your findings (which I assume refer to your method and the experiments) inspire your framework?\n    - In the appendix, Park et al. 2023 is not a methodological paper, it is a critical evaluation of deception in Machine learning\n    - In Appendix g, the Assasin prompt reads \"The evil tema is close to losing; you must guess who Merlin is. losing; you must guess who Merlin is\". Is this an LLM artifact or a copying error?\n  this is a sampling of the general feeling of sloppiness/lack of attention to detail that permeates the paper.\n\n---\n\nIf the paper decides whether to present a new benchmark or a method as its focus, then it can be salvaged:\n\n1. For the method, evaluate on standard benchmarks and add your avalon benchmark as a bonus. Properly compare against ToT and other baselines.U\n2. For the benchmark, use more open source models and create a detailed story, supported by suitable experiments, to demonstrate *why* this is a \"game of thought\" as opposed to  a \"language game\", and throw in your method as the bonus\n\nFor both, use guidance to create a controlled experiment using LLama or the recent mystral etc. finetunes, use proper statistical measurements and report things in a manner that can actually be compared (e.g. the table highlighting differences) and reproduced (all exact prompts!)"
                },
                "questions": {
                    "value": "I tried to be very constructive and complete in my weaknesses section, hence I do not have questions that would meaningfully change my opinion without those issues addressed"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission118/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698693931933,
            "cdate": 1698693931933,
            "tmdate": 1699635937110,
            "mdate": 1699635937110,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LxXZfd1ycM",
                "forum": "fvhJu0FODp",
                "replyto": "MUmwucVQBR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission118/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission118/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Before Rebuttal:"
                    },
                    "comment": {
                        "value": "Although you mentioned in your reviews, \"I am sorry that the authors encounter me as a reviewer,\" we actually feel quite fortunate to have you as our reviewer because of your detailed reviews, insightful comments, and constructive suggestions. To match the energy and time you have devoted to reviewing, we have thoroughly answered all your questions and comments. We hope our detailed and sincere responses can change your opinion of our paper. Thank you once again for your review."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission118/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700679341068,
                "cdate": 1700679341068,
                "tmdate": 1700679341068,
                "mdate": 1700679341068,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VXYXyiFskO",
                "forum": "fvhJu0FODp",
                "replyto": "MUmwucVQBR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission118/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission118/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal to Reviewer 48Jp"
                    },
                    "comment": {
                        "value": "> Q1: In summary section: The paper presents an LLM prompting setup termed \"ReCon\" consisting of: 1. one prompt ...; 2. one prompt ...; 3. one prompt ...; 4. one prompt ...\n\nThank you for your detailed summary. However, we beg to differ that **the main contributions of our paper are not focusing on the four prompts**, but **the cognitive insights related to the theory of mind [1] and communicative theory [2], such as perspective-taking and recursive thinking**, which is geneneralizable to other fields or applications.\n\n[1] Yuan, Luyao, et al. \"In situ bidirectional human-robot value alignment.\" *Science robotics* 7.68 (2022): eabm4183.\n\n[2] Tomasello, Michael. *Origins of human communication*. MIT press, 2010.\n\n\n---\n\n> Q2: The paper is to present benchmark or method?\n\n**The primary focus of our paper is to present the method ReCon.** However, the motivation of the proposal of ReCon is derived from the future usage of AI agents in deceptive environments, and the Avalon game is a suitable testbed for the setting. **The logic of ReCon and Avalon game is similar to the Cicero method and the Diplomacy game in [1], and the AlphaGo method and the game of Go in [2].**\n\n[1] Meta Fundamental AI Research Diplomacy Team (FAIR)\u2020, et al. \"Human-level play in the game of Diplomacy by combining language models with strategic reasoning.\" *Science* 378.6624 (2022).\n\n[2] Silver, David, et al. \"Mastering the game of Go with deep neural networks and tree search.\" *nature* 529.7587 (2016).\n\n\n\n---\n\n\n\n> Q3: Evaluation on more tasks such as HumanEval, a negotiation game, or a market game, etc.\n\nThank you for your constructive suggestion. Due to time constraints during the rebuttal period, it's not feasible for us to apply our method to additional tasks. However, a method called SimToM, introduced in a recent paper [1] (submitted to ArXiv approximately one month after our submission), employs techniques similar to ours, such as perspective-taking and a think-twice strategy. This method has been tested on ToMI [2] and BigTOM [3] tasks, demonstrating the effectiveness of these techniques. **While our ReCon framework differs somewhat from SimToM, the outcomes achieved with SimToM could, to some extent, suggest the potential of ReCon for generalization to other tasks.**\n\n[1] Wilf, Alex et al. \u201cThink Twice: Perspective-Taking Improves Large Language Models' Theory-of-Mind Capabilities.\u201d (2023).\n\n[2] Le, Matthew, Y-Lan Boureau, and Maximilian Nickel. \"Revisiting the evaluation of theory of mind through question answering.\" *Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)*. 2019.\n\n[3] Gandhi, Kanishk, et al. \"Understanding social reasoning in language models with language models.\" *arXiv preprint arXiv:2306.15448* (2023).\n\n\n\n---\n\n\n\n> Q3: The reliability and replicability of using blackbox APIs for evaluation.\n\nThank you for raising this important issue.\n\nAs for reliability, please see `Global Q3` in our [Global Responses](https://openreview.net/forum?id=fvhJu0FODp&noteId=j338DnUQOY), where we have carried out human surveys to assess the reliability of GPT-4's evaluation.\n\nTo ensure the replicability as much as possible, we employ zero temperature for GPT-4 for evaluation. In addition, given the sufficient large number of evaluated samples, i.e., more than 2k, randomness can be largely dicreased due to the law of large numbers.\n\n\n\n---\n\n\n\n> Q4: n=20 seems too low; would like to see a suitable statistical significance test \n\nThank you for your insightful feedback. We address the issue regarding $n=20$ in `Global Q1`, and a statistical significance test is detailed in `Global Q2`. For more information, please refer to our [Global Responses](https://openreview.net/forum?id=fvhJu0FODp&noteId=Y7twEtlQGy). We greatly appreciate your input."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission118/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700679425438,
                "cdate": 1700679425438,
                "tmdate": 1700683393076,
                "mdate": 1700683393076,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZVorGlZlpT",
                "forum": "fvhJu0FODp",
                "replyto": "MUmwucVQBR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission118/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission118/Authors"
                ],
                "content": {
                    "title": {
                        "value": "(Continued and to Be Continued) Rebuttal to Reviewer 48Jp"
                    },
                    "comment": {
                        "value": "> Q5: Use \"guidance\" for LLaMA v2 expriments\n\nThank you for your feedback. We initially overlooked including \"guidance\" in our LLaMA v2 experiments, but following your suggestion, we've updated our code to integrate it while keeping most prompts the same.\n\nIn our tests, we used LLaMA2-Chat (7b, 13b, 70b), `vicuna-13b-v1.5-16k`, and `Mistral-7B-Instruct-v0.1` for playing the Avalon game. The LLaMA2-Chat models struggled with Avalon tasks due to their limited 2k-token context window. The `vicuna-13b-v1.5-16k` and `Mistral-7B-Instruct-v0.1` models, with their larger context windows of 16k and 8k tokens, could play the game but faced issues with format mismatches and repeated responses, as detailed below:\n\n- **Format mismatches.** Format mismatches occur in some stages of our ReCon process, particularly during the first-think-then-speak phase. Initially, we generated the thinking and speaking contents by creating a JSON file, following the steps in the [guidance's tutorial](https://github.com/guidance-ai/guidance#guidance-acceleration):\n\n  ```python\n  with guidance.assistant():\n      lm += f\"\"\"\n  ```json\n  {{\n      \"thinking content\": \"THINK: {guidance.gen('think_content', suffix='\"', max_tokens=500)}\",\n      \"speaking content\": \"SPEAK: {guidance.gen('speak_content', suffix='\"', max_tokens=500)}\"\n  }}```\"\"\"\n      think_content, speak_content = lm['think_content'], lm['speak_content']\n  ```\n\n  Occasionally, a `KeyError` occurs when `think_content` or `speak_content` is missing in `lm`, hindering the completion of Avalon games due to frequent thinking and speaking actions. We speculate that this may stem from the LLMs' inconsistent adherence to format requirements. This conclusion is aligned with Sec. 5.5.\n\n- **Repeated responses.** To address the issue of mismatched format mentioned earlier, we modified our code as follows:\n\n  ```python\n  with guidance.assistant():\n      lm += f'''\"thinking content\": \"THINK: {guidance.gen('think_content', suffix='\"', max_tokens=500)}\"'''\n      lm += f'''\"speaking content based on the thinking content (not repeating other's previous speaking)\": \"SPEAK: {guidance.gen('speak_content', suffix='\"', max_tokens=500)}\"'''\n      think_content, speak_content = lm['think_content'], lm['speak_content']\n  ```\n\n  This revision effectively prevents the `KeyError` mentioned previously. Yet, we observe that agents frequently repeat the speaking content of other agents, as provided in the given context. For instance:\n\n  ```\n  Player 1: \n  \n  I propose the team player 2, player 1. Based on my considerations, I have decided to propose a mission team consisting of myself and Player 2. Player 2 has been actively involved in the discussions and has shown a willingness to collaborate with others. They have also been supportive of the team and have not been suspect of any wrongdoing. I believe that with their skills and my leadership, we can successfully complete the mission. However, I must also be cautious about how my proposal might be perceived by other players, especially those who may be suspect of being evil. I will make sure to provide cover for my teammate if they are suspect of being evil, without revealing my own identity or Morgana's identity to the other players.\n  \n  ---\n  \n  Player 2: \n  \n  Dear fellow players, I have carefully considered the proposed team players for this mission. Based on my analysis, I believe that player 2 and player 1 are suitable candidates for the team. Both players have been activey involved in the discussion and have shown a willingness to collaborate with others. They have also been supportive of the team and have not been suspect of any wrongdoing.\n  \n  However, I must also be cautious about how my proposal might be perceived by other players, especially those who may be suspect of being evil. I will make sure to provide cover for my teammates if they are suspect of being evil, without revealing my own identity or Morgana's identity to the other players.\n  \n  ---\n  \n  Player 3:\n  \n  Dear fellow players, I have carefully considered the proposed team players for this mission. Based on my analysis, I believe that player 2 and player 1 are suitable candidates for the team. Both players have been active and supportive of the team, and they have not been suspect of any wrongdoing.\n  \n  However, I must also be cautious about how my proposal might be perceived by other players, especially those who may be suspect of being evil. I will make sure to provide cover for my teammates if they are suspect of being evil, without revealing my own identity or Morgana's identity to the other players.\n  ```\n\n  We suspect that this issue of repetitive responses might be attributed to the limited capabilities of the LLMs we are using within the context of Avalon.\n\n**Owing to the aforementioned issues, despite our keen interest in providing experimental results on open-source LLMs using \"guidance\", we find ourselves unable to do so at this time.** We will leave this to future work."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission118/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700683179198,
                "cdate": 1700683179198,
                "tmdate": 1700683415593,
                "mdate": 1700683415593,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vaxX7rZHf3",
                "forum": "fvhJu0FODp",
                "replyto": "ZVorGlZlpT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission118/Reviewer_48Jp"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission118/Reviewer_48Jp"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors. Thank you very much for spending the time to explore using uidance. In the meantime, I also learned of llama.cpps [GBNF](https://github.com/ggerganov/llama.cpp/blob/master/grammars/README.md) funtionality which might further be suitable for your setup, however, I want to explicitly acknowledge the effort put into addressing this part of my review and think this is truly a \"best effort\" in using Open Source LLMs."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission118/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700683775928,
                "cdate": 1700683775928,
                "tmdate": 1700683775928,
                "mdate": 1700683775928,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nflNnEWEY7",
                "forum": "fvhJu0FODp",
                "replyto": "VXYXyiFskO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission118/Reviewer_48Jp"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission118/Reviewer_48Jp"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors.\n\nThank you for the effort in addressing the concerns about sample size and black box evaluations. For me, these mainly assuage the methodological concerns I had.\n\nHowever, my concerns about the contribution and clarity of presentation stand. In order to claim \n\n>However, we beg to differ that the main contributions of our paper are not focusing on the four prompts, but the cognitive insights related to the theory of mind [1] and communicative theory [2], such as perspective-taking and recursive thinking, which is generalizable  to other fields or applications.\n\none needs to address at the very least\n\n- the similarity to Tree of thought/lack of contextualization and differentiation\n- demonstrate applicability and benefits beyond the simultaneously presented benchmark...\n- *or* motivate further why performance in this benchmark cannot be expected to be a (quoting the paper from memory) \"mere language game, but a game of thought\" and thus make ReCon unique\n- perform further experiments isolating the effect of the proposed Theory of mind and communicative theory. E.g. [Ullman 2023](https://arxiv.org/abs/2302.08399) showed showed LLMs \"ToM\" to be exceedingly brittle against small alterations and https://arxiv.org/abs/2305.15068 as well as https://arxiv.org/abs/2305.14763 develop specific benchmarks for ToM, which can probably improve an evaluation of ReCon and support this main claim\n\nThis is beyond ones perspective on the \"mumbling of incantations\" and debates on prompt engineering vs. \"techniques of thoughts\" as raised in the discussion with reviewer zjpH (although I am with them on in their skepticism/hostility towards LLMs) and simply on the solidity of the claims being made.\n\nWhile I acknowledge your effort in solidifying your evaluation of GPT and Claude, for me these core issues of the paper remain unaddressed. Please correct me if I have missed a response or alteration of the paper in which you could debunk this perception."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission118/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700684689234,
                "cdate": 1700684689234,
                "tmdate": 1700684689234,
                "mdate": 1700684689234,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8UH62n7UdO",
                "forum": "fvhJu0FODp",
                "replyto": "LxXZfd1ycM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission118/Reviewer_48Jp"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission118/Reviewer_48Jp"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Authors, as the final comment for now, I am glad you feel the review was constructive. I have acknowledges some responses as having assuaged my concerns and replied to others, however since the core problems remain unaddressed I will retain my score for now, modulo additional comments and discussion with the other reviewers. If you feel  my remaining concerns are unfounded, please do not hesitate in arguing for why."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission118/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700684812392,
                "cdate": 1700684812392,
                "tmdate": 1700684812392,
                "mdate": 1700684812392,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "I9jNRF4wke",
                "forum": "fvhJu0FODp",
                "replyto": "MUmwucVQBR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission118/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission118/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer 48Jp (https://openreview.net/forum?id=fvhJu0FODp&noteId=nflNnEWEY7)"
                    },
                    "comment": {
                        "value": "Dear Reviewer 48Jp:\n\nThank you for your response and additional insightful questions. We have provided detailed answers to your further queries below. We trust that our responses will address and clarify any doubts you may have regarding our work.\n\n\n---\n\n\n\n> Comparison with Tree of Thoughts (ToT)\n\nWe understand your concern about the similarity between ToT and our ReCon due to the iterative thinking pattern. However, we believe that our ReCon has some significant differences from ToT, and cannot be viewed as a specific case of ToT, due to the following reasons:\n\n|                           | Tree of Thoughts (ToT)                                       | ReCon (Ours)                                                 |\n| ------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |\n| Core Motivation           | ToT is designed to facilitate exploration through coherent text units (\"thoughts\") as intermediate steps in **problem-solving**. It's aimed at tasks like the Game of 24, Creative Writing, and Mini Crosswords, which **do not involve misinformation or deception**. These tasks also **do not occur in communicative contexts**. | ReCon is developed to **manage deceptions and misinformation in communicative environments**, where there may not be a definitive \"correct\" answer, emphasizing the nuances of communication. |\n| Algorithmic Focus         | ToT focuses on **constructing complex reasoning trees**, with looking ahead or backtracking, to address problems. | ReCon's approach differs as it does not rely on complex reasoning trees. Instead, **it prioritizes Theory of Mind** (such as understanding and speculating about others' thoughts and identities), **and communicative skills** (such as expressing opinions while concealing its own identity when necessary). |\n| Nature of Reasoning Trace | ToT's reasoning trace comprises a series of **decomposed, intermediate thought steps** aimed at **problem resolution**. | ReCon's reasoning trace involves **the original and revised versions of thoughts and spoken contents**, tailored for **enhanced Theory of Mind and communicative skills**. |\n\n---\n\n\n\n> motivate further why performance in this benchmark cannot be expected to be a (quoting the paper from memory) \"mere language game, but a game of thought\" and thus make ReCon unique\n\nThe Avalon game, on which ReCon is tested, inherently involves a significant number of deceptions. As highlighted on [proavalon.com](https://www.proavalon.com/), a renowned platform for playing Avalon online, the game is defined as \"a game of deception\", where \"a small band of revolutionaries must use logic and deduction in order to ferret out the spies who have infiltrated their ranks and are sabotaging the cell's presumably heroic acts of rebellion against government tyranny.\" **This characterization of the Avalon game solidly backs our stance that it is not just \"mere language game, but a game of thought\".** We also suggest participating in an Avalon game when possible, as it will likely solidify your agreement with our claim.\n\n---\n\n\n\n> perform further experiments isolating the effect of the proposed Theory of mind and communicative theory.\n\nThank you for your insightful comments and suggestions.\n\nWe acknowledge that current LLMs' Theory of Mind (ToM) capabilities are brittle. However, in this work, LLMs serve primarily as a tool to bridge the ToM framework with natural language responses. In other words, **we're explicitly integrating ToM into LLM-as-agent, demonstrating enhanced performance over LLMs alone**. The benchmarks you've recommended **mainly assess the ToM abilities of various LLMs, whereas our work's focus isn't on developing stronger LLMs, but on their integration with ToM.**\n\nAlthough the benchmarks you suggest may not be the best fit for testing our methodology, we believe it's important to distinctly showcasing the impacts of Theory of Mind and our communicative approach. Understanding your preference against more individual examples, **we have provided a comprehensive set of 372 instances (which represents a substantial quantity and precludes the possibility of selective cherry-picking) in the Excel file of this [anonymous link](https://anonymous.4open.science/r/ICLR_Avalon_Rebuttal_ID_118/ReCon_thinking_and_speaking_logs.xlsx). These examples display ReCon's thought and speech processes, where the thinking content demonstrates the efficacy of ReCon's Theory of Mind, and the speaking content illustrates the effectiveness of ReCon's communication skills.** \n\nAdditionally, **Figures 4 and 6** in our paper, highlighting \"\"**ReCon w/o First-Order Perspective Transition**\" and \"**ReCon w/o Second-Order Perspective Transition**,\" demonstrate ReCon's ToM effectiveness by isolating the first-order and second-order ToM elements from ReCon, respectively."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission118/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700721775393,
                "cdate": 1700721775393,
                "tmdate": 1700722003307,
                "mdate": 1700722003307,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Id97yLhTQy",
                "forum": "fvhJu0FODp",
                "replyto": "MUmwucVQBR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission118/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission118/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Replies to Additional Comments from Reviewer 48Jp's Initial Review (https://openreview.net/forum?id=fvhJu0FODp&noteId=MUmwucVQBR)"
                    },
                    "comment": {
                        "value": "> Q7: the authors don't even give the complete prompt in the appendix.\n\nApologies for the previous lack of detail. For your convenience, we have provided the full prompts of ReCon in this [anonymous link](https://anonymous.4open.science/r/ICLR_Avalon_Rebuttal_ID_118/Avalon_game_prompt.pdf). Additionally, the prompts used for GPT-4 evaluation are available at another [anonymous link](https://anonymous.4open.science/r/ICLR_Avalon_Rebuttal_ID_118/gpt4_evaluation_prompts.pdf).\n\n---\n\n> Q8: In the appendix, Park et al. 2023 is not a methodological paper, it is a critical evaluation of deception in Machine learning\n\nThank you for your thorough review. However, there seems to be a possible misunderstanding. In our related work section, **we have cited two papers authored by researchers with the surname Park: Park et al. (2023a) [1] and Park et al. (2023b) [2]**. While the latter [2] critically evaluates deception in machine learning, the former [1] is a methodological paper that introduces \"generative agents\". **It appears there might have been a mix-up between these two works during review.**\n\n[1] Park, Joon Sung, et al. \"Generative agents: Interactive simulacra of human behavior.\" *Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology*. 2023.\n\n[2] Park, Peter S., et al. \"AI deception: A survey of examples, risks, and potential solutions.\" *arXiv preprint arXiv:2308.14752* (2023).\n\n---\n\n\n\n> Q9: In Appendix g, the Assasin prompt reads \"The evil tema is close to losing; you must guess who Merlin is. losing; you must guess who Merlin is\". Is this an LLM artifact or a copying error? this is a sampling of the general feeling of sloppiness/lack of attention to detail that permeates the paper.\n\nThank you for pointing out this issue. We acknowledge the copying error and apologize for the typo, which has been corrected in our revised manuscript. \n\nWhile we regret this oversight, we respectfully disagree that it reflects a broader lack of attention to detail throughout our paper. We have approached our work with diligence and care, as evidenced by the positive remarks on writing and presentation from most reviewers, and as further demonstrated in our detailed rebuttal response. Aside from this isolated instance, our paper contains few typographical or grammatical errors. \n\nNevertheless, we sincerely apologize for this mistake and appreciate your understanding."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission118/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700729467116,
                "cdate": 1700729467116,
                "tmdate": 1700729467116,
                "mdate": 1700729467116,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9EO0tidiMv",
                "forum": "fvhJu0FODp",
                "replyto": "Id97yLhTQy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission118/Reviewer_48Jp"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission118/Reviewer_48Jp"
                ],
                "content": {
                    "comment": {
                        "value": "Q7: thank you for providing the complete prompt\nQ8: Indeed, this was an unfortunate mixup as the two references are above each other, apologies for this.\nQ9: I appreciate the refutation."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission118/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700734140251,
                "cdate": 1700734140251,
                "tmdate": 1700734140251,
                "mdate": 1700734140251,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gzDAZv8HTK",
                "forum": "fvhJu0FODp",
                "replyto": "I9jNRF4wke",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission118/Reviewer_48Jp"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission118/Reviewer_48Jp"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors. You have responded adequately to most of the other concerns I have raised and I think the paper has grown stronger from it, however, on this, I think we are in a loop and my concerns are of a manner that you won't be able to still them in rebuttal.\n\nOur divergence lies in the fact that I do not care about the *words* Theory of Mind or \"game of deception\", I want *numerics* giving meaning to them.\n\nThat is, while you outline three claimed differences between ToT and your method, there is not a clear formal distinction. Paraphrasing your argument, you claim that one distinction between ToT and ReCon is the imperfect information and ambiguity setting. Well, [MCTS performs very well in imperfect information](https://www.frontiersin.org/articles/10.3389/frai.2023.1014561/full) and theory of mind and imperfect information involves reasoning (as in, exploring a search tree of possible deductions) through various scenarios using assumptions - which just means taking statements as a fact without clear evidence. Thus, ToT can be seen as a superset of your method, modulo the exact wording of the prompt - which, if you want to claim a distinction from mere prompt engineering (\"arcane incantations\") should not matter that much. The problem is that Ullman 2023 shows that it *does* matter, which implies to me again that the claim to be focused on ToM would require you to perform similar ablations and investigations in that matter.\n\nFurther, the previous claims of ToM and deception being central to your paper are kind of in conflict with statements like\n\n>The benchmarks you've recommended mainly assess the ToM abilities of various LLMs, whereas our work's focus isn't on developing stronger LLMs, but on their integration with ToM.\n\nWhat *is* the central point of the paper? \n\n- Showcasing good performance of the LLM on a new benchmark using your new prompt technique? => Then you need more diverse benchmarks and ablations.\n- Highlight and study the importance of ToM in Avalon => then you probably should integrate deeper with the ToM literature and perform specific ablations and at least one ToM specific benchmark with ReCon\n- Study deception in LLMs and how to make it explicit/deal with it? => then this is not really clearly articulated in the paper, the method isnt' really evaluated in this manner (except the sucess rate on avalon which might just be an artifact of the prompt technique and system prompts sabotaging the agens ability to generate falsehoods coherently)\n\nFinally, neither of\n\n> This characterization of the Avalon game solidly backs our stance that it is not just \"mere language game, but a game of thought\".\nand \n> We also suggest participating in an Avalon game when possible, as it will likely solidify your agreement with our claim.\n\nare scientific arguments. If you ask the average person, computer science is a mix of black magic and boring minutia, and \n\n1. I find myself both unable to summon eldritch forces *and* highly stimulated after over a decade in the field\n2. as aggregated anecdotal evidence, nothing of this matters unless suitably operationalized.\n\nIFF you present an operationalization on what it *means* to  be a game of though as opposed to a language game (e.g. by grounding it in imperfect information and the need to perform deduction and bluffing from meta-game awareness, as e.g. done in [Deepminds study of stratego](https://www.science.org/stoken/author-tokens/ST-887/full#supplementary-materials) or by isolating the language aspect from the reasoning aspect, then your argument would fly.\n\nAlternatively, you might be able to side step this by showing alignment with a reasoning algorithm in a latent state, by learning interpretable mappings from the latent state of game states observed by the LLMs into predicting belief states a game tree and then showing it behaves like a causal inference or otherwise suitable algorithm.\n\nOr there might be other ways of making your point in a more empirical manner.\n\nBut in the absence of this the words \"this paper is about ToM, deception and a game of thought\"  do not mean anything, because they are too fuzzy concepts and too weakly supported in their construction. That is at least my current stance, and in the remaining time I don't think you will be able to perform the additional work to remedy this. However I wish you the best of luck to further refine and sharpen the paper, and hope that the angle I sketch above (sharpen the distinction between ToT and ReCon by performing focused studies that leverage an analogy on the difference of MCTS in MDPs and POMPDs) might prove helpful in this."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission118/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700735458585,
                "cdate": 1700735458585,
                "tmdate": 1700735458585,
                "mdate": 1700735458585,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nuTsjMq1Om",
            "forum": "fvhJu0FODp",
            "replyto": "fvhJu0FODp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission118/Reviewer_zjpH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission118/Reviewer_zjpH"
            ],
            "content": {
                "summary": {
                    "value": "The paper attempts to study and improve LLMs' ability to produce situationally appropriate responses in a strategic environment requiring deception and the ability to detect deception. LLMs are prompted to play roles in the social-deduction game Avalon. The experimental intervention is a new prompting technique ReCon, which explicitly prompts agents to contemplate the roles of other players based on the game history, and then explicitly prompts them to contemplate how other players will perceive them. The outputs are then evaluated in two ways: (1) by playing complete games of Avalon to see whether ReCon-prompted agents perform better, and (2) having GPT-4 classify outputs as successful or not along six dimensions."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The prompt design is clever and the prompts make explicit ideas that are implicit in human thought processes about social roles."
                },
                "weaknesses": {
                    "value": "It is hard to see what is generalizable here. LLMs are fundamentally linguistic; any higher-order cognitive processes are emergent properties.  The kind of prompt engineering illustrated here yields no insight into those processes. Instead, it is akin to mumbling different incantations at a mysterious creature to see how it responds. In what sense have the LLMs understood or responded to the ReCon prompts? Unclear. Do they have a usable theory of mind about other agents in the game? Unclear. These linguistic prompt manipulations are inherently fragile. They depend enormously on the current capabilities of the specific LLMs studied; there is no good reason to think that these approaches will transfer to other future LLMs with different training sets, architectures, or fine-tuned guardrails.\n\nThe use of ChatGPT to perform multi-dimensional evaluation needs to be validated against human evaluation."
                },
                "questions": {
                    "value": "none"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Potentially harmful insights, methodologies and applications"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "The authors note that these techniques require eliciting deceptive behavior from LLMs and could be used in the future for deception. I do not think that there are serious ethical problems with the research, but someone who works specifically on the ethics of AI safety should review this to ensure that it is in line with professional ethical standards."
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission118/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission118/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission118/Reviewer_zjpH"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission118/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698767657357,
            "cdate": 1698767657357,
            "tmdate": 1699635937027,
            "mdate": 1699635937027,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mzHw8Esai5",
                "forum": "fvhJu0FODp",
                "replyto": "nuTsjMq1Om",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission118/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission118/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal to Reviewer zjpH"
                    },
                    "comment": {
                        "value": "We want to express our thanks to you for your reviews. Although there might be some misunderstandings, we are pleased to address your concern in the following response.\n\n> Q1: It is hard to see what is generalizable here.\n\nWe appreciate the feedback and understand the concerns regarding generalizability. However, we emphasize that our proposed ReCon is **not limited to specific prompts in the Avalon game**. Instead, **it focuses on emulating human-like recursive thinking and perspective-taking**. As pointed out by reviewer zjpH himself/herself, ReCon \"makes explicit ideas that are implicit in human thought processes about social roles.\" **This human-like thought processes on LLMs are the generalizable aspect of ReCon**, which is particularly effective in complex scenarios involving logic and deception.\n\n**Furthermore, testing on a single, challenging task that current methods struggle with is a standard research practice and does not indicate non-generalizability.** For instance, Cicero [12] demonstrated exceptional performance exclusively in the game of Diplomacy, yet its designs are considered generalizable. Similarly, generative agents [13], tested solely in a Sims-inspired interactive environment, yielded generalizable insights. Beyond LLM agents, in the realm of RL agents, notable studies like AlphaGo [1] and AlphaStar [2] focused on specific tasks\u2014Go and StarCraft II, respectively\u2014yet the techniques developed in these works have been proven to be generalizable.\n\n\n\n[1] Silver, David, et al. \"Mastering the game of Go with deep neural networks and tree search.\" *nature* 529.7587 (2016).\n\n[2] Vinyals, Oriol, et al. \"Grandmaster level in StarCraft II using multi-agent reinforcement learning.\" *Nature* 575.7782 (2019)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission118/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700240766578,
                "cdate": 1700240766578,
                "tmdate": 1700240766578,
                "mdate": 1700240766578,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "h2KFGdlG2h",
                "forum": "fvhJu0FODp",
                "replyto": "nuTsjMq1Om",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission118/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission118/Authors"
                ],
                "content": {
                    "title": {
                        "value": "(Continued) Rebuttal to Reviewer zjpH"
                    },
                    "comment": {
                        "value": "> Q4: These linguistic prompt manipulations are inherently fragile. They depend enormously on the current capabilities of the specific LLMs studied.\n\nIn our previous discussion, we emphasized that possessing a capability does not automatically lead to realizing its full potential. Thus, **there are primarily two types of work concerning LLMs: (1) expanding the boundaries of LLMs' emergent capabilities (such as [8-10]), and (2) unlocking the full potential of these capabilities (such as [1-7] and ReCon). Both approaches are critically important.** To draw an analogy, the first approach is similar to enhancing a child's IQ, while the second is akin to teaching the child advanced thinking strategies to maximize the use of their IQ. It would be incorrect to dismiss the value of these thinking strategies simply because they are contingent on the child's IQ. Similarly, it's unjustified to label the thinking process methods for LLMs as \"inherently fragile\" and dismiss them as irrelevant just because they depend on the current abilities of LLMs. Rather, **as the limits of LLMs' emergent capabilities significantly expand, the importance of fully unlocking the potential of LLMs may increasingly come to the forefront [1-7, 11].**\n\n\n\n[1] Wei, Jason, et al. \"Chain-of-thought prompting elicits reasoning in large language models.\" *Advances in Neural Information Processing Systems* 35 (2022): 24824-24837.\n\n[2] Kojima, Takeshi, et al. \"Large language models are zero-shot reasoners.\" *Advances in neural information processing systems* 35 (2022): 22199-22213.\n\n[3] Yao, Shunyu, et al. \"Tree of thoughts: Deliberate problem solving with large language models.\" *arXiv preprint arXiv:2305.10601* (2023).\n\n[4] Besta, Maciej, et al. \"Graph of thoughts: Solving elaborate problems with large language models.\" *arXiv preprint arXiv:2308.09687* (2023).\n\n[5] Shinn, Noah, et al. \"Reflexion: Language agents with verbal reinforcement learning.\" *Thirty-seventh Conference on Neural Information Processing Systems*. 2023.\n\n[6] Yao, Shunyu, et al. \"React: Synergizing reasoning and acting in language models.\" *arXiv preprint arXiv:2210.03629* (2022).\n\n[7] Wang, Guanzhi, et al. \"Voyager: An open-ended embodied agent with large language models.\" *arXiv preprint arXiv:2305.16291* (2023).\n\n[8] Brown, Tom, et al. \"Language models are few-shot learners.\" *Advances in neural information processing systems* 33 (2020): 1877-1901.\n\n[9] Touvron, Hugo, et al. \"Llama: Open and efficient foundation language models.\" *arXiv preprint arXiv:2302.13971* (2023).\n\n[10] Touvron, Hugo, et al. \"Llama 2: Open foundation and fine-tuned chat models.\" *arXiv preprint arXiv:2307.09288* (2023).\n\n[11] Wang, Lei, et al. \"A survey on large language model based autonomous agents.\" *arXiv preprint arXiv:2308.11432* (2023).\n\n\n\n\n\n---\n\n\n\n> Q5: There is no good reason to think that these approaches will transfer to other future LLMs with different training sets, architectures, or fine-tuned guardrails.\n\nIn our study, we evaluated ReCon on **two distinct LLMs, ChatGPT and Claude**, which differ in \"training sets, architectures, and fine-tuned guardrails.\" We applied **identical thinking processes and prompts** to both. ReCon's superior outcomes with ChatGPT and Claude, as depicted in Figure 4 of our paper, suggest that **ReCon possesses a certain level of generalizability across various LLMs**.\n\nRegarding \"future LLMs,\" the thought processes in ReCon, such as recursive thinking and perspective-taking, are also useful in human intelligence. Therefore, **we have reason to believe that in the foreseeable future, even if LLMs approach or match human intelligence, the ReCon method we propose will remain effective**. The only scenario where ReCon might not work on future LLMs is if the chosen future LLMs have inferior generative abilities."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission118/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700240985106,
                "cdate": 1700240985106,
                "tmdate": 1700241919640,
                "mdate": 1700241919640,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "psrJAxANUl",
                "forum": "fvhJu0FODp",
                "replyto": "nuTsjMq1Om",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission118/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission118/Authors"
                ],
                "content": {
                    "title": {
                        "value": "(Continued) Rebuttal to Reviewer zjpH"
                    },
                    "comment": {
                        "value": "> Q6: The use of ChatGPT to perform multi-dimensional evaluation needs to be validated against human evaluation.\n\nWe appreciate your valuable suggestions. In line with your recommendations, we are currently undertaking a human agreement evaluation akin to the approach used by AlpacaEval (details found [here](https://github.com/tatsu-lab/alpaca_eval#evaluators)). We aim to publish the results promptly.\n\n----\n\nWe extend our heartfelt gratitude once more for your review. We anticipate further dialogue and collaboration, and are open to any more thoughts you may have to help refine our work."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission118/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700241906357,
                "cdate": 1700241906357,
                "tmdate": 1700241941008,
                "mdate": 1700241941008,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "w6Bu9LSunW",
            "forum": "fvhJu0FODp",
            "replyto": "fvhJu0FODp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission118/Reviewer_WArX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission118/Reviewer_WArX"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies whether LLMs can deceive and act strategically in deceptive environments, using the Avalon game as a testbed. The authors propose a method called Recursive Contemplation (ReCon) to enhance LLMs\u2019 ability to identify and counteract deceptive information, based on theory of mind methods. The authors show that ReCon is able to aid LLMs to discern and maneuver around deceptive information."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well-motivated: studying the truthfulness and deceptive abilities of LLMs is certainly very important.\n2. The dataset can be a nice contribution.\n3. The paper is well-written."
                },
                "weaknesses": {
                    "value": "1. Mainly GPT-4 is used for evaluation so I'm not sure how reliable the evaluation is - there should be at least reliable human evaluation.\n2. There are only 20 games so not sure how reliable how the results are.\n3. 4.2 MULTI-DIMENSIONAL EVALUATION is only done with chatgpt so it's not clear how reliable and reproducible the results will continue to be.\n4. I also don't really understand 4.2 MULTI-DIMENSIONAL EVALUATION - why are existing metrics from (Li et al., 2023b; Bang\net al., 2023) being used here? Aren't we trying to study the performance of LLMs on avalon game? Again, I believe human evaluation would be the gold standard here.\n5. 4.3 QUALITATIVE ANALYSES is quite interesting, but the results are very anecdotal to be reliable. 'ReCon\u2019s Proficiency in Detecting Misinformation' -> this is quite cool and would be a very nice result, but there are only anecdotes to support it and not a complete quantitiative evaluation. Can an actual dataset be collected with misinfo, true info labeled, and ReCon vs other baselines rigorously benchmarked on it? Similarly for 'The efficacy of ReCon in information', another cool finding, but needs much more extensive comparisons and evaluation to be reliable. 'Lai et al., Werewolf among us: Multimodal resources for modeling persuasion behaviors in social deduction games' could be a useful resource here."
                },
                "questions": {
                    "value": "1. Explain why the evaluation with GPT-4 makes sense and why human eval is not needed, or provide human eval if necessary.\n2. Explain 4.2 MULTI-DIMENSIONAL EVALUATION, including why only chatgpt was used or provide results with open-source LMs, explain the evaluation metrics and why human eval is not needed, or provide human eval if necessary.\n3. Additional results on 4.3 QUALITATIVE ANALYSES as appropriate, including whether these can be evaluation rigorously."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Privacy, security and safety"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "This paper dives deep into generation, detection, and mitigation of misinformation in LLMs. While I believe the authors have mostly addressed the ethical impacts of their work I will defer the final judgement to the ethics expert reviewers."
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission118/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698810376414,
            "cdate": 1698810376414,
            "tmdate": 1699635936919,
            "mdate": 1699635936919,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "M63DUgjNKz",
                "forum": "fvhJu0FODp",
                "replyto": "w6Bu9LSunW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission118/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission118/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal to Reviewer WArX"
                    },
                    "comment": {
                        "value": "We are grateful for your thorough review and helpful recommendations, which have contributed significantly to the enhancement of our manuscript.\n\n----\n\n> Q1: There are only 20 games, so I'm not sure how reliable the results are.\n\nThank you for your valuable question. We've addressed the rationale behind selecting 20 Avalon game rounds in `Global Q1` and have provided a statistical test of our results in `Global Q2`. For more information, please see our [Global Responses](https://openreview.net/forum?id=fvhJu0FODp&noteId=Y7twEtlQGy). We appreciate your interest.\n\n---\n\n> Q2: Explain why the evaluation with GPT-4 makes sense and why human eval is not needed, or provide human eval if necessary.\n\nThank you for your insightful question. We agree that incorporating human evaluation alongside GPT-4 assessment is beneficial. Consequently, we are now conducting a human agreement evaluation similar to the method employed by AlpacaEval (further information available [here](https://github.com/tatsu-lab/alpaca_eval#evaluators)). We intend to release the findings as soon as possible.\n\n\n\n---\n\n\n> Q3: 4.2 MULTI-DIMENSIONAL EVALUATION is only done with ChatGPT, so it's not clear how reliable and reproducible the results will continue to be.\n\nRegarding **reliability**, automated evaluation of LLMs is widely recognized and popular [1]. In our evaluation using GPT-4, we analyzed **over 2300 LLM responses**, which we believe enhances the evaluation's reliability. Nevertheless, as mentioned in our response to `Q2`, we acknowledge the value of adding human evaluation to GPT-4 assessment for greater reliability. Therefore, we are conducting a human agreement evaluation using a method similar to AlpacaEval (more details [here](https://github.com/tatsu-lab/alpaca_eval#evaluators)), and we plan to publish these results promptly.\n\nIn terms of **reproducibility**, while OpenAI's API did not offer deterministic output during our experiments, we maintained reproducibility by **setting the GPT-4 evaluator's temperature to 0 and documenting the GPT-4 version in our paper's \"Reproducibility Statement.\"** Additionally, evaluating **over 2300 LLM responses** significantly reduces randomness **under the law of large numbers**. Hence, we consider our GPT-4 evaluation to be reproducible.\n\n[1] Chang, Yupeng, et al. \"A survey on evaluation of large language models.\" *arXiv preprint arXiv:2307.03109* (2023).\n\n---\n\n> Q5: 4.3 QUALITATIVE ANALYSES is quite interesting, but the results are very anecdotal to be reliable. 'ReCon\u2019s Proficiency in Detecting Misinformation' -> this is quite cool and would be a very nice result, but there are only anecdotes to support it and not a complete quantitiative evaluation. Can an actual dataset be collected with misinfo, true info labeled, and ReCon vs other baselines rigorously benchmarked on it? Similarly for 'The efficacy of ReCon in information', another cool finding, but needs much more extensive comparisons and evaluation to be reliable. 'Lai et al., Werewolf among us: Multimodal resources for modeling persuasion behaviors in social deduction games' could be a useful resource here.\n\nThank you for recognizing the interest in our qualitative analysis, and we're happy to address your question about it below.\n\nRegarding the \"anecdotal\" aspect of our results, **to bolster the reliability of our qualitative analysis, we've compiled extensive data on how LLMs think and speak using ReCon in an Excel file, accessible at this [anonymous link](https://anonymous.4open.science/r/ICLR_Avalon_Rebuttal_ID_118/ReCon_thinking_and_speaking_logs.xlsx)**. Additionally, upon acceptance of our paper, we plan to open-source our codes, inviting you to test them and assess the reliability of our qualitative results yourself.\n\nWe also value your suggestion about compiling an actual dataset. Due to time constraints in this rebuttal phase, we haven\u2019t been able to gather such a dataset and benchmark different methods on it, but we will consider incorporating this into our future work. Thank you once again for your constructive suggestion.\n\n---\n\n> Q6: Explain 4.2 MULTI-DIMENSIONAL EVALUATION, including why only chatgpt was used or provide results with open-source LMs, explain the evaluation metrics and why human eval is not needed, or provide human eval if necessary.\n\nThank you for your inquiry about our evaluation approach.\n\nRegarding the lack of evaluation with open-source LMs, as detailed in Section 5.5 of our paper, LLaMA2-70b-chat, a prominent open-source LM, often struggles to consistently produce the structured output formats our study requires, like generating outputs suitable for regular expression matching (e.g., [approve]). This issue poses challenges in employing open-source LMs for playing the Avalon game.\n\nFor the question on human evaluation, we invite you to refer to our response to Q2 for more information.\n\n---\n\nWe thank you again for your expertise and attention, and are ready to address any further questions or concerns."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission118/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700285138373,
                "cdate": 1700285138373,
                "tmdate": 1700285138373,
                "mdate": 1700285138373,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QdbN27CjUu",
                "forum": "fvhJu0FODp",
                "replyto": "M63DUgjNKz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission118/Reviewer_WArX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission118/Reviewer_WArX"
                ],
                "content": {
                    "title": {
                        "value": "thank you for your response"
                    },
                    "comment": {
                        "value": "Thank you authors for your responses. I am glad you agree with my concerns regarding reliability of automatic language model evaluation, the need for more human evaluation, and the need for more concrete quantitative analysis of the claims made in the paper. Since you acknowledge that these are directions you are currently working towards, I hope to see them in the improved version of the paper, and I keep my current score."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission118/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700714616587,
                "cdate": 1700714616587,
                "tmdate": 1700714616587,
                "mdate": 1700714616587,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qjJCq7H47g",
                "forum": "fvhJu0FODp",
                "replyto": "w6Bu9LSunW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission118/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission118/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer WArX"
                    },
                    "comment": {
                        "value": "Thank you for your response. We have completed the human evaluation assessing the reliability of automatic language model evaluation. For our results, please see **Appendix D.1 in the revised manuscript** or **refer to [Global Q3](https://openreview.net/forum?id=fvhJu0FODp&noteId=j338DnUQOY)**. We appreciate your attention to this matter.\n\nWe trust that our comprehensive response has addressed your concerns regarding our work and will positively influence your evaluation. Thank you for your consideration."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission118/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700723918493,
                "cdate": 1700723918493,
                "tmdate": 1700724001535,
                "mdate": 1700724001535,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]