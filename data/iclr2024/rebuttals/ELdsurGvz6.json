[
    {
        "title": "Partitioning-Guided K-Means: Extreme Empty Cluster Resolution for Extreme Model Compression"
    },
    {
        "review": {
            "id": "7qTtQlxWS5",
            "forum": "ELdsurGvz6",
            "replyto": "ELdsurGvz6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1303/Reviewer_uYXA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1303/Reviewer_uYXA"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes Partitioning-Guided k-means (PG k-means), a novel k-means algorithm for quantizing neural network weights. It aims to improve Iterative Product Quantization (iPQ) with Quant-Noise, which suffers from empty clusters that degrade accuracy.\n\nPG k-means has 3 main components: Partitioning-guided pre-assignment - Minimizes initial empty clusters by recursively partitioning weights.\nPartitioning-guided cluster fine-tuning - Resolves empty clusters during k-means by cautiously splitting large clusters. Dense weights consolidation (optional) - Consolidates dense clusters to prevent erroneous separation. PG k-means consistently outperforms iPQ with Quant-Noise for extreme compression of RoBERTa on GLUE benchmarks, demonstrating its viability for quantization-aware training. The proposed techniques significantly reduce empty clusters and improve accuracy."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "PG k-means consistently outperforms iPQ with Quant-Noise for extreme compression of RoBERTa on GLUE benchmarks, demonstrating its viability for quantization-aware training. The proposed techniques significantly reduce empty clusters and improve accuracy."
                },
                "weaknesses": {
                    "value": "At the beginning of the introduction, the authors claimed their object is to compress the large language models (LLMs). However, I have not seen any evidences showing that their method is beneficial for LLM compression. Specifically, all the explorations and experiments are conducted with RoBERTa networks. Those BERT-architecture models only have 1 billion parameters, which can not be called LLM for the recent community. And the architecture of RoBERTa is hugely different than the architecture of those real LLMs such as GPT, OPT, and LLAMA. \n\nThe scope is limited, the authors choose to modify or improve Iterative Product Quantization (iPQ) with Quant-Noise (Fan et al.,\n2020), as it is the state-of-the-art in this area. In the area of network (non-uniformed) quantization, there are many works can outperform Iterative Product Quantization (iPQ) with Quant-Noise. The foundation of developing their method based on iPQ with Quant-Noise is not solid."
                },
                "questions": {
                    "value": "Refer to weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1303/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1303/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1303/Reviewer_uYXA"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1303/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697656773533,
            "cdate": 1697656773533,
            "tmdate": 1699636057578,
            "mdate": 1699636057578,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "n3rEDnk7OQ",
            "forum": "ELdsurGvz6",
            "replyto": "ELdsurGvz6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1303/Reviewer_rfms"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1303/Reviewer_rfms"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents Partitioning-Guided k-means (PG k-means) as a new approach to address the issue of inference quality degradation caused by empty clusters in Iterative Product Quantization (iPQ) with Quant-Noise. The proposed PG k-means method comprises three key components to resolve the prevalent empty cluster problem. The PG k-means offers practical solutions to minimize empty clusters, leverage improved empty cluster resolution, and enhance overall accuracy for extreme model compression in language modeling tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The proposed PG k-means method reduces the number of empty clusters in iPQ with Quant-Noise, decreases the iterations required for empty cluster resolution, and improves the overall model accuracy by up to 12% when applied to RoBERTa on various tasks in the GLUE benchmark. The effectiveness of PG k-means is compared to iPQ with Quant-Noise, revealing consistently superior results on tasks such as MNLI, RTE, and QNLI. The findings suggest that PG k-means is a solution for extreme model compression."
                },
                "weaknesses": {
                    "value": "I have some concerns as follows.\n\nThis paper falls in the scope of PQ-based model compression [1-3]. Previous work has typically focused on quantizing model parameters for compression purposes [4-5], it is unsurprising that optimizing the k-means clustering inside can improve model performance.\n\n[1] Product quantization for nearest neighbor search. TPAMI 2010.  \n[2] VQ-GNN: A Universal Framework to Scale up Graph Neural Networks using Vector Quantization. NeurIPS 2021.  \n[3] Quantized convolutional neural networks for mobile devices. CVPR 2016.  \n[4] Dynamic Dual Gating Neural Networks. ICCV 2021.  \n[5] FPGM-Filter Pruning via Geometric Median for Deep Convolutional Neural Networks Acceleration. CVPR 2019."
                },
                "questions": {
                    "value": "Please see the weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1303/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698643693936,
            "cdate": 1698643693936,
            "tmdate": 1699636057505,
            "mdate": 1699636057505,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "2OFn5oLY9C",
            "forum": "ELdsurGvz6",
            "replyto": "ELdsurGvz6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1303/Reviewer_5ufE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1303/Reviewer_5ufE"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on the extreme model compression with product quantization. The authors first reveal the empty cluster issue in Iterative Product Quantization (iPQ) method proposed in 2020, and they posit that the presence of empty clusters often leads to noteworthy losses in inference quality. To solve this problem, Partitioning-Guided k-means (PG k-means) is proposed with three contributions: \n\n- a pre-assignment strategy that ensures non-empty initial centroids.\n\n- a partitioning method of populous clusters into new sub-clusters when empty clusters arise during k-means iterations.\n\n- an optional optimization step that consolidates dense clusters of weights to ensure that they map to a single centroid after quantization completes."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The proposed method about solving empty cluster problem is reasonable and is easy to follow.\nDetailed algorithm implementation is given."
                },
                "weaknesses": {
                    "value": "I cannot agree with the authors about the empty cluster problem to cause the loss in model quality. In popular k-means implementation, there are mechanisms to make sure that the clusters are non-empty, such as the k-means++ initialization and the reassignment method used in scikit-learn KMeans. I believe the k-means method is not correctly used if lots of clusters are empty.\n\nIf the iPQ with Quant-Noise can resolve empty clusters with more iterations, then the proposed method only speedup the k-means optimization step. From this viewpoint, the necessity of the proposed method is questionable.\n\nThe proposed method is validated on a single RoBERTa model. The three tasks (MNLI, RTE, QNLI) of GLUE are also not enough.\n\nThe baseline results of iPQ with Quant-Noise seems to be much lower than reported in the original paper. In iPQ, the accuracy on MNLI is 83.6 with 12.6x compression and 82.5 with 34.3x compression, which outperforms the proposed PG k-means method.\n\nThe proposed method should be validated on other models such as EfficientNet-B3 to fairly compare with iPQ. Comparison with other quantization methods, such as binary/ternary quantization methods, should be discussed.\n\n1. https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/cluster/_kmeans.py"
                },
                "questions": {
                    "value": "When calculating the model size of the original model, what data format is used (Fp32 or fp16)?\nWhat are the percentages when Partitioning-Guided Pre-assignment method is used without Partitioning-Guided Cluster Fine-tuning?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1303/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698680449663,
            "cdate": 1698680449663,
            "tmdate": 1699636057425,
            "mdate": 1699636057425,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]