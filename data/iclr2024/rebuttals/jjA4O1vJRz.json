[
    {
        "title": "LLM Augmented LLMs: Expanding Capabilities through Composition"
    },
    {
        "review": {
            "id": "xMdkPuM7bS",
            "forum": "jjA4O1vJRz",
            "replyto": "jjA4O1vJRz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6139/Reviewer_AR8D"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6139/Reviewer_AR8D"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes CALM to compose a general LLM with specialized models with a cross-attention module. By only fine-tuning the cross-attention on a small amount of data, the composed system is able to enable new capabilities for compositional tasks that neither of the two models can handle independently. The authors conduct experiments on tasks like math reasoning on low-resource language and code understanding and generation to verify the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Though similar ideas are tested to be effective for building multi-modal models (e..g. reuse image encoder outputs for an LLM to enable multi-modal capabilities), this paper extends the application to new domains and scenarios.\n- The method is simple and effective."
                },
                "weaknesses": {
                    "value": "- Lack of important details, e.g. it is unclear how many the size of these models, the training steps, and hyperparameters. And how large is the training data $D_c$?\n- It would be helpful to add a discussion on the composition with existing methods, such as routing and tool-using. If we treat the specialized models as external tools and prompt the general LLM to first call these models and then process the returned outputs, is it a kind of composition in the text space rather than representations?"
                },
                "questions": {
                    "value": "Please see above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6139/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6139/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6139/Reviewer_AR8D"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6139/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698413294379,
            "cdate": 1698413294379,
            "tmdate": 1699636665445,
            "mdate": 1699636665445,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "f8Or5r977e",
                "forum": "jjA4O1vJRz",
                "replyto": "xMdkPuM7bS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6139/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6139/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer AR8D (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their feedback. We are glad to see that the reviewer understands the usability of our work with respect to past approaches and appreciates the simplicity of our approach. We address their questions and comments below:\n\n\n> It would be helpful to add a discussion on the composition with existing methods, such as routing and tool-using\n\nUsing models as tools and performing composition in the text space is definitely a very interesting direction. We have now added a discussion on this line of work in **Section 2**.\n\nUpon the reviewer's suggestion, **we conduct a experiment to compare with a routing approach in our key-value setup**. Particularly, we consider a much harder version of the task where the final **key-value arithmetic question is presented in the form of natural language text**: \"What is the value when <key_1> is added to the difference of <key_2> and <key_3>\"? In order to solve this task, a system would first need to parse the question in a simpler form, \"<key_1> + <key_2> - <key_3>\", then perform substitution of keys to values, and finally perform the arithmetic. We evaluate CALM on this hard task and compare with a skyline mA-to-mB routing performance by taking a product of mA's substitution performance on this task with the arithmetic performance of mB. We report the numbers here:\n\n|                            |      mA     | mB     |  CALM  |  Routing (best case) |\n|----------------------------|:-------------:|-------------:|-------:|-------:|\n| KV-Substitution                 |     98.1      |     0.0      |   92.9  | -- |\n| Numeric Arithmetic             |     4.2                |     73.7      |   72.0 | -- |\n| KV-Arithmetic (Natural Language Hard)            |     0.0  |     0.0            |     **36.7**       |     23.6  |\n\nWe observe that **CALM outperforms the skyline routing performance by around 13 absolute accuracy points**.\\\nThis experiment demonstrates that CALM is much more effective for scenarios when **back-and-forth computation** needs be performed. In this case, the anchor model first needs to process the given input, which then requires the augmenting model's key-value mapping, and finally back to the anchor model for the arithmetic computation. Evidently, composition through representations allows this back-and-forth computation as well as the flexibility of employing either model (or some combination, thereof) at each decoding step.\n\nFurther, in comparison to a framework such as **Toolformer** (Schick et al. (2023)) where a model is taught to call external tools during training, we believe **CALM presents several advantages**. The former requires the creation of a separate dataset to teach the model to use external tools (here, models), which in turn requires a large amount of prompt engineering and manual effort, before additional training. This is in contrast to CALM where we reuse the existing dataset for training and do not require any manual intervention. Moreover, composing via representations might also have the advantage of utilizing internal model computations, and hence might be more effective.\n\nPerforming more targeted comparisons, towards this end, might be useful to compare the efficacy and training efficiency of the two approaches. While, setting up this setup and performing comparison with models as tools is beyond the scope of this rebuttal, we hope to include them in future versions of this work. Finally, we believe that both composition in the text space and that through internal representations have their individual strengths and might evolve to be useful across different use cases."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6139/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700461510787,
                "cdate": 1700461510787,
                "tmdate": 1700511369084,
                "mdate": 1700511369084,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cZPYhuBzXO",
            "forum": "jjA4O1vJRz",
            "replyto": "jjA4O1vJRz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6139/Reviewer_6aS2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6139/Reviewer_6aS2"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes CALM, composition to augment language models. CALM is designed to compose LLMs with different capabilities. In particular, the paper focuses on composing an anchor model with a domain-specific augmenting model to enable new capabilities. CALM introduces simple linear transformations to extract features from the augmenting model and insert them into the anchor model with cross-attention modules. In the experiments, the paper introduces three different applications of CALM, in the domains of key-value arithmetic, low-resource language inclusivity, and code understanding and generation. The experimental results demonstrate the composed LLMs can perform new challenging tasks and obtain better results than the models without composition."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- The idea of LLM composition is interesting. A great number of LLMs are designed with diverse capabilities. Combining LLMs with different capabilities and deriving new capabilities are valuable research questions. In section 4.2, after combining the model with KV-substitution skill and the model with numeric-arithmetic skill, the combined model achieves zero-shot KV-arithmetic inference. In comparison, the underlying models fail to do KV-Arithmetic inference, demonstrating the emergence of \"new skills\".\n- The authors conduct extensive experiments on diverse domains ranging from language inclusivity to code generation. The results show that CALM successfully combines the capabilities of the anchor and augmenting models."
                },
                "weaknesses": {
                    "value": "- Except for the key-value arithmetic case, the composition between low-resource languages and English, and the composition of coding and language capabilities are similar to the works of efficient cross-modal LLMs. For example, many studies have explored how to efficiently connect image encoders to LLMs, which finally achieves a \"combined skill\" like image-grounded language generation. I believe the relation between CALM and these related cross-modal models should be discussed. Besides, I would expect the combined models to have more non-trivial combined skills ( key-value arithmetic) beyond conditional generation (machine translation or code-to-text generation).\n- Introduction mentions that directly training LLMs is computationally expensive but it seems that the experiments do not involve whether CALM is more computationally efficient than directly training the anchor model."
                },
                "questions": {
                    "value": "Many studies have explored how to efficiently connect image encoders to LLMs, which finally achieves a \"combined skill\" like image-to-text generation. What is the difference between CALM and the efficient cross-modal LLM methods?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6139/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6139/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6139/Reviewer_6aS2"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6139/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698903295860,
            "cdate": 1698903295860,
            "tmdate": 1699636665327,
            "mdate": 1699636665327,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "chPy3yu6wf",
                "forum": "jjA4O1vJRz",
                "replyto": "cZPYhuBzXO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6139/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6139/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 6aS2"
                    },
                    "comment": {
                        "value": ">  Many studies have explored how to efficiently connect image encoders to LLMs, which finally achieves a \"combined skill\" like image-to-text generation. What is the difference between CALM and the efficient cross-modal LLM methods? [...] I believe the relation between CALM and these related cross-modal models should be discussed.\n\nWe have now expanded our discussion with respect to this body of past work in Section 2.\n\n\n> I would expect the combined models to have more non-trivial combined skills ( key-value arithmetic) beyond conditional generation (machine translation or code-to-text generation)\n\nIn Table 3 we show CALM outperforms other models on MGSM tasks on NTL which we believe is a good evidence of combined skill (math reasoning in NTL languages). Similarly for code, we showcase superior performance for code completion in Table 4. We would be delighted to consider other potential scenarios and tasks to demonstrate non-trivial combined skills if the reviewer has some particular suggestions in this regard.\n\n\n> the experiments do not involve whether CALM is more computationally efficient than directly training the anchor model.\n\nThe total training cost of training CALM is significantly lesser than training the anchor model.\nFor all experiments reported in our work, CALM was trained only for 5-7% of the data as full mB training, and hence **training CALM is up to 10 times cheaper than training the anchor model**.\n\nUpon the reviewer\u2019s suggestion, we now include a detailed discussion on training overhead with CALM in **Appendix C**.\\\nFor the reviewer\u2019s reference, we find that:\n\nIf we assume the number of parameters in mA to be 10% of those in mB and the amount of data required to train CALM is 5% of mB training.\n\nAssuming a linear scaling factor of training cost with model parameters and data:\\\n**Cost of training CALM ~5% of the cost for training mB**\\\n**Net cost of training mA + CALM ~15% of the cost for training mB**\n\n***\n\nWe would be happy to discuss any further questions about the work and would appreciate support for acceptance of the paper."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6139/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700464339811,
                "cdate": 1700464339811,
                "tmdate": 1700470440881,
                "mdate": 1700470440881,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PNN7i68A1u",
                "forum": "jjA4O1vJRz",
                "replyto": "chPy3yu6wf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6139/Reviewer_6aS2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6139/Reviewer_6aS2"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for addressing my comments."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6139/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700683467843,
                "cdate": 1700683467843,
                "tmdate": 1700683467843,
                "mdate": 1700683467843,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nioxcZoSI5",
            "forum": "jjA4O1vJRz",
            "replyto": "jjA4O1vJRz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6139/Reviewer_QMa2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6139/Reviewer_QMa2"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces the CALM framework, which aims to enhance an existing LLM by incorporating a smaller, specialized one. This framework combines two pretrained LLMs while keeping their weights intact and utilizes learnable linear projections and self-attention to map intermediate representations from one model to the other. CALM demonstrates superior performance compared to both the anchor and augmenting models across three diverse tasks, including arithmetic reasoning, low-resource language translation, and code generation, which are not part of direct training but require a combination of the capabilities of both models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Comprehensive Experiments: The authors conduct extensive experiments that span three widely used application domains. The choices of training and evaluation tasks are reasonable and effectively show the composition of two LLMs.\n\n2. Performance Improvement: The experimental results consistently demonstrate performance improvements across all domains. Notably, the arithmetic reasoning experiment on synthetic datasets shows an impressive performance gain in comparison to both the anchor and augmenting models. The ability of CALM to tackle a novel task not previously feasible for either model is particularly intriguing.\n\n3. Usefulness: The paper presents a concept with potential real-world applications. It enhances model reusability and eliminates the need to scale up existing models for injecting new knowledge.\n\n4. Clarity: The paper is well-written and easy to follow."
                },
                "weaknesses": {
                    "value": "1. Lack of Comparison to Existing Methods: While the idea of combining models with different skill sets has been explored in related work, such as ensembling hidden representations of two models, the paper lacks empirical results for such comparisons. Authors are recommended to consider providing a comparison of CALM with existing methods for ensembling model representations. Additionally, could the authors clarify \"LoRA assumes access to the full underlying domain data during pretraining\"? It is assumed that the same training data are used for both LoRA and CALM in Table 5.\n\n2. Missing Information on Overhead: The paper does not provide information on the additional parameters introduced and their ratio to the anchor and augmenting models, which is important to understand the system's overhead. It would also be helpful to support the claim that \"The composed model achieves similar performance for a tiny fraction of the training cost\" with evidence, such as the number of FLOPs and memory requirements. Since the framework requires backpropagation through the initial layers of both models, I assume that the training cost should be comparable to directly training $m_B$.\n\n3. The paper should include model configurations and training details to enhance reproducibility.\n\n4. Some notations are used before being formally introduced, leading to potential confusion. For instance, notation $\\mathbf{C}$ is introduced in Section 3.2 after being used from the beginning of Section 3. Additionally, both $t_{\\{A, B\\}}$ and $C$ denote task sets, which can be confusing and should be clarified or distinguished."
                },
                "questions": {
                    "value": "1.  Given that CALM requires the model weights access and running both forward and backward passes, keeping the weights frozen isn't necessarily required. It would be interesting to investigate whether updating an anchor, augmenting, or both models could enhance performance. \n2. It would be also interesting to explore the interactions between $m_B$ attending to $m_A$ in comparison to the current direction, to determine whether the capabilities of \"generic\" models can be effectively transferred to a \"specialized\" model, especially given that $m_A$ is more efficient to run.\n3. Table 3 shows more noticeable performance gains in high-resource languages compared to low-resource ones. I wonder if this might imply that the composition strengthens the skills of pretrained models thanks to extra parameters, rather than teaching them entirely new skills."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6139/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6139/Reviewer_QMa2",
                        "ICLR.cc/2024/Conference/Submission6139/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6139/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699400548709,
            "cdate": 1699400548709,
            "tmdate": 1700932206454,
            "mdate": 1700932206454,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OXOhVXPncy",
                "forum": "jjA4O1vJRz",
                "replyto": "nioxcZoSI5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6139/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6139/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer QMa2 (1/3)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their careful analysis of our paper and the extremely insightful comments. We are glad to that they find our work useful and appreciate the comprehensiveness of our experiments. We really appreciate the clarity of the questions raised by them. We address the reviewer's comments below:\n\n\n> The paper does not provide information on the additional parameters introduced and their ratio to the anchor and augmenting models, which is important to understand the system's overhead\n\nDue to the proprietary nature of our models, we are unable to provide specific details regarding the models. However, for the experimental setting considered in our paper, the additional parameters introduced for composition are **less than 2% of the number of parameters in the anchor model**.\n\nUpon the reviewers\u2019 suggestion, we include a detailed computation of the expected overhead assuming generic transformer models in **Appendix C.1**. We include a brief of the same here for the reviewer\u2019s reference:\n\nExtending from the notations in Section 3.1, let's say the two models mA and mB have NA and NB number of standard transformer layers, respectively, with each layer of an output dimensionality of DA and DB. As mentioned, we choose n = nA = nB number of layers to perform the composition.\n```\n# Parameters for each f_proj layer = (DA * DB)\n# Parameters for each f_cross layer = (3 * DB**2)\n# Total parameters introduced during composition = n * (DA * DB + 3 * DB**2)\n# Parameters in mB = NB * (VB*DB + 3 * DB**2 + 2*DB*DB*KB)\n```\nwhere, VB and KB depict the vocabulary size and hidden dimension multiplication factor, respectively.\n\n%age of new parameters added = (\\# Total parameters introduced during composition\\*100) / (\\# Parameters in mB)\n\nLet's consider some standard transformer configurations to understand the parameter overhead.\\\nAs an example, we consider the layer configurations of the BERT models: BERT-small (mA) and BERT-large (mB).\\\nIn this case: NA = 4, DA = 512, NB = 24, DB = 1024, VB = 30K, KB = 4. Assuming that we select all layers of mA, n = 4.\n\nHence,\n```\n# Total parameters introduced during composition = 4  * (512 * 1024 + 3 *1024**2) \u2248  1.5x107 \u2248 15M\n# Parameters in mB = 24 * (30K*1024 + 3 * 1024**2 + 2*1024*1024*4) \u2248 1B\n%age of new parameters added = 15M * 100 / 1B = 1.5%\n```\nHence,\\\n**Total parameters introduced during composition ~1.5% of parameters in mB**\n\n\n> Since the framework requires backpropagation through the initial layers of both models, I assume that the training cost should be comparable to directly training mB. \n\nThe reviewer is correct that the training FLOPs are comparable for both mB and CALM for a fixed set of examples since the gradients are computed over mB during backpropagation. However, the training cost of CALM is significantly smaller than the anchor model because it is trained over a very small fraction of the total training examples and iterations as full anchor model training. \n\nFor all experiments reported in our work, CALM was trained only for 5-7% of the data as full mB training, and hence **training CALM is up to 10 times cheaper than training the anchor model**.\n\nWe now include a detailed discussion on training overhead with CALM in **Appendix C.2**. We include salient parts of the details here for the reviewer\u2019s reference:\n\nFirstly, as discussed above, the additional number of parameters introduced during composition is <2% of the number of parameters of mB\u2014hence, a negligible addition in training cost per example.\\\nFurther, since only ~5-7% of the total mB fine-tuning data is required to train CALM, the training cost of CALM is minimal with respect to training cost of training the entire anchor model.\\\nMoreover, since our experiments consider an mA that has 5-20% of parameters as mB, even the net cost of training mA and CALM is significantly lesser than training mB. \n\nLet\u2019s say that the number of parameters in mA is 10% of those in mB and the amount of data required to train CALM is 5% of mB training.\n\nAssuming a linear scaling factor of training cost with model parameters and data:\\\n**Cost of training CALM ~5% of the cost for training mB**\\\n**Net cost of training mA + CALM ~15% of the cost for training mB**"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6139/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700319996168,
                "cdate": 1700319996168,
                "tmdate": 1700464385943,
                "mdate": 1700464385943,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pYpsXH5lIh",
                "forum": "jjA4O1vJRz",
                "replyto": "nioxcZoSI5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6139/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6139/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer QMa2 (2/3)"
                    },
                    "comment": {
                        "value": "> While the idea of combining models with different skill sets has been explored in related work, such as ensembling hidden representations of two models, the paper lacks empirical results for such comparisons\n\n\nThe existing methods for ensembling model representations might not be directly comparable with CALM for the following reasons (a) The augmenting model(s) mA and anchor model mB can be of different size and their representation dimension can differ, which are not suitable for representation ensembling models. (b) The ensemble models work best when the original models are well aligned i.e. they are derived from the same base model, CALM is applicable for any set of models.\n\n\nOn the reviewer\u2019s suggestion, we have now expanded our evaluations with other baselines such as efficient fine-tuning approaches like LoRA for all our tasks and setups (NTL and Code). We report these comparisons in Table 5 and present a snippet here:\n\n\n|                            |      CALM     |  LoRA  |\n|----------------------------|:-------------:|-------:|\n| HumanEval [pass@1]                 |     22.5      |   18.3 |\n| MBPP [pass@1]                       |     32.2      |   28.7 |\n| FLORES-200 [#( >mB)]                    |     175       |     82 |\n| GSM 8K (LRL) [#( >mB)]                    |      20       |     15 |\n\n\nWe observe CALM outperforms LoRA in most of the tasks across NTL and code tasks.\n\n\n\n\n> Table 3 shows more noticeable performance gains in high-resource languages compared to low-resource ones. I wonder if this might imply that the composition strengthens the skills of pretrained models thanks to extra parameters, rather than teaching them entirely new skills.\n\nThat's an important question and the set of ablation experiments in the paper specifically address this question.\nIn order to see whether the performance gains through composition occur due to the utilization of knowledge from mA or from the additional parameters, we replaced mA with randomly initialized versions.\n\n|                            | CALM | Vanilla mA | Random mA |\n|----------------------------|------|------------|-----------|\n| FLORES-200 [avg. acc.]   | 60.5 | 59.2       | 58.8      |\n| #(>mB)                     | 175  | 115        | 43        |\n| GSM 8K (LRL) [avg. acc.]          | 21.4 | 19.0       | 17.8      |\n| #(>mB)                    | 20   | 15         | 9         |\n| GSM 8K (HRL) [avg. acc.]          | 33.1 | 29.7       | 28.5      |\n| #(>mB)                     | 11   | 8          | 4         |\n\nTable 5 (a snippet shown above) shows the corresponding results. We observed that both \"vanilla\" (mA replaced with a vanilla pre-trained checkpoint) and \"random\" (a randomly set of mA weights) show significant decrement in performance. For all these cases, the number of additional parameters introduced remains the same. Hence, **the difference in performance is reflective of the utility of mA through composition**.\n\nMoreover, the rank for LoRA is chosen such that the same number of parameters are added. The inferior performance therein further supports our claims that composition with mA is useful.\n\n\n> Some notations are used before being formally introduced, leading to potential confusion.\n\nWe thank the reviewer for their careful observation and for pointing this out. We have now updated the notations and have made the surrounding descriptions more clear."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6139/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700320744241,
                "cdate": 1700320744241,
                "tmdate": 1700321528467,
                "mdate": 1700321528467,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "462w3f0aGY",
                "forum": "jjA4O1vJRz",
                "replyto": "nioxcZoSI5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6139/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6139/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer QMa2 (3/3)"
                    },
                    "comment": {
                        "value": ">  Could the authors clarify \"LoRA assumes access to the full underlying domain data during pretraining\"? It is assumed that the same training data are used for both LoRA and CALM in Table 5.\n\n\nApologies for the confusion. Here we are alluding to a potential proprietary situation where the full set of downstream domain data is unavailable at the time of adaptation, but the entire domain data has been parametrically condensed in an augmenting model that is available to us. We have observed that training CALM over an available data subset suffices in augmenting the desired set of knowledge to an anchor model. However, this is not possible with LoRA since the parameters are only exposed to the data subset available during training.\n\nAs an example, our key-value arithmetic setup involves a set of 25K keys. We performed CALM training only over a held-in set of 5K key-value pairs and evaluated over the remaining 20K held-out keys (Table 1).\n\nWe have now made this more clear in the paper.\n\n\n> Given that CALM requires the model weights access and running both forward and backward passes, keeping the weights frozen isn't necessarily required. It would be interesting to investigate whether updating an anchor, augmenting, or both models could enhance performance\n\nWe agree. It would be interesting to investigate whether updating the models during composition could enhance performance. It is possible that updating the augmenting models during this process leads to better alignment with the anchor model.\n\nHowever, in this work, we inherently assume a setup where the underlying models are given upfront and updating their parameters is not permitted. This setup is of practical importance in several scenarios such as when serving a proprietary large model to downstream customers/users who need to utilize the model for their custom knowledge.\n\nFurther, keeping the given models unchanged ensures control for catastrophic forgetting (as evidenced by our results on GSM-8K and CodeXGLUE). Changing model parameters can lead to undesirable and intractable changes.\n\nA setting where we update only very few layers in the two models could be interesting to explore and we can investigate this going forward. Since this exploration requires large experimental bandwidth, we include it as a part of our future plans.\n\n> It would be also interesting to explore the interactions between mB attending to mA in comparison to the current direction, to determine whether the capabilities of \"generic\" models can be effectively transferred to a \"specialized\" model, especially given that mA is more efficient to run.\n\nThat's an interesting suggestion. However, an implicit directionality is assumed in how we define augmenting and anchor models.\nBy definition, the anchor model has coherent text generation attributes (along with other higher level capabilities such as reasoning) but misses some specialized skills (such as domain knowledge) that, in turn, is present in the augmenting models. Once the relevant skills are augmented through composition, decoding from the same anchor model follows the logical ordering in the framework.\n\nReversing this ordering might be useful for certain settings (for instance where we wish to restore specialized model's performance on a task that it has forgotten) and investigating such scenarios could be very interesting, however beyond the scope of this rebuttal.\n\n***\n\nWe would be happy to discuss any further questions about the work, and would appreciate an appropriate increase in the score if the reviewer\u2019s concerns are adequately addressed."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6139/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700321027020,
                "cdate": 1700321027020,
                "tmdate": 1700470361054,
                "mdate": 1700470361054,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zsCw4T0Ub9",
            "forum": "jjA4O1vJRz",
            "replyto": "jjA4O1vJRz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6139/Reviewer_kQBq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6139/Reviewer_kQBq"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new approach, CALM, to compose an anchor model and a domain-specific augmenting model by introducing a small number of trainable parameters over both models\u2019 intermediate layer representations. This approach combines the capabilities of the anchor model and the augmenting model, and therefore, is able to address new challenging tasks that cannot be solved by either model alone. The authors use three tasks to demonstrate the advantages of CALM:\n1. solving arithmetic expressions containing keys: the anchor model is trained to do arithmetic over integers, and the augmenting model is trained to memorize string-to-integer key-value mappings.\n2. Translation and math-word problem-solving in low-resource languages: the anchor model is a pretrained PaLM-2 S model and the augmenting model is trained on low-resource languages.\n3. Code completion, text-to-code generation, and code-to-text generation: the anchor model is a pretrained PaLM-2 S model and the augmenting model is pretrained on codes."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Strengths\n1. The new approach enables new capabilities in the composed version that the original models cannot achieve.\n2. CALM does not modify the parameters of the original model, avoiding the catastrophic forgetting that is prevalent in conventional approaches.\n3. CALM has a flexible architecture that makes it possible to compose more than one augmenting model with an anchor model."
                },
                "weaknesses": {
                    "value": "1. The first task, key-value arithmetic, seems rather arbitrary. A simple encoder-decoder model is expected to solve this problem, leaving the readers the question of why they bother to use the composition of two models.\n2. For the second task, machine translation in low-resource languages, when the anchor model is trained on the low-resource languages, its performance is higher than the composed approach, which again raises the question of why you need to compose two models instead of fine-tuning the anchor model.\n3. The paper misses the details of parameterization/hyperparameter selection. For example, the authors did not write how the layers of the anchor and the augmenting models are selected."
                },
                "questions": {
                    "value": "1. It is not clear to me how the projection function works. Does it project each selected layer of A to all selected layers of B? What is the definition of HBj? Does HA\u2295Bj contain the information from all HA or only from a specific layer HAi?\n2. Why does CALM on the KV-Arithmetic task have a higher score than mB on the  Numeric-Arithmetic task? Should mB trained on the Numeric-Arithmetic task be the upper bound?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6139/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6139/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6139/Reviewer_kQBq"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6139/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699640705017,
            "cdate": 1699640705017,
            "tmdate": 1699640705017,
            "mdate": 1699640705017,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HKd5jQneYI",
                "forum": "jjA4O1vJRz",
                "replyto": "zsCw4T0Ub9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6139/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6139/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer kQBq (1/2)"
                    },
                    "comment": {
                        "value": "> The first task, key-value arithmetic, seems rather arbitrary. A simple encoder-decoder model is expected to solve this problem, leaving the readers the question of why they bother to use the composition of two models. \n\nWhile encoder-decoder models are capable of solving arithmetic tasks, our key-value arithmetic setup is **rather unique** where it aims to evaluate whether expert knowledge of one model can be transferred into general capabilities of an anchor model.\n\nIn this setup, all key assignments are stored in an expert model mA (var1=4, var2=5). While mB has the capability to do arithmetic (4+5=9), it is incapable of performing arithmetic on keys (var1+var2=?). The CALM framework aims to transfer the key-assignment knowledge from mA into mB and demonstrates that the composed model is able to perform arithmetic on keys (var1+var2=9). \n\nBelow we will explain the exact setup of our task-\n(i) All key assignments, {var1: 4, var2: 5, ..., varN: X}, are stored in mA,\n(ii) We perform CALM training with mA and mB on a certain number of **held-in keys**, {var1, var2, ..., varH}, where H << N. As mentioned in Section 4.1, we use 20% of the keys stored in mA during CALM training.\n(iii) We evaluate key-value arithmetic performance on **held-out keys not seen during CALM training**: {varH+1, ..., varN}.\n\nHappy to discuss more if we misunderstood your question or if you would like further clarification.\n\n> For the second task, machine translation in low-resource languages, when the anchor model is trained on the low-resource languages, its performance is higher than the composed approach, which again raises the question of why you need to compose two models instead of fine-tuning the anchor model\n\nComposition through CALM has significant advantages over fine-tuning:\n\n(i) CALM provides an easy and efficient way of adding specific knowledge to a general model and expanding capabilities of an anchor model. We have added a discussion in Appendix C discussing the cost of training CALM v/s that of fine-tuning the anchor model. We find that training through **CALM is up to 10 times cheaper than direct fine-tuning**. Fine-tuning of a large model, as considered in our experiments, is expensive. **For most of our experiments, we see comparable or better performance with composition for a fraction of the cost**.\n\n(ii) While fine-tuning performance for the low-resource translation task exceeds that of CALM, **Table 3 provides evidence of forgetting due to fine-tuning**. We see that fine-tuning led to regression on numeric arithmetic on both high and low-resource languages (performing equal or lower than the base anchor model) while CALM still performs well, exceeding both fine-tuned and base versions. We provide a summarized snippet of the results below for the reviewer's reference:\n\n|                            |      Base anchor     | CALM     |  Fine-tuned  |\n|----------------------------|:-------------:|-------------:|-------:|\n| FLORES-200                 |     41.7      |     46.1      |   **48.2**  |\n| GSM-8K (LRLs avg.)             |     22.5                |     **25.0**      |   22.6 |\n| GSM-8K (HRLs avg.)            |     28.0              |     **31.8**       |     25.6  |\n\nWe also see similar findings for the code setup where the fine-tuned model on code (mostly involving code as output) shows large regression on the code-to-text tasks since they involve generating text output:\n\n|                            |      Base anchor     | CALM     |  Fine-tuned  |\n|----------------------------|:-------------:|-------------:|-------:|\n| HumanEval (code completion)                 |    16.4      |    22.5      |   **24.3**  |\n| CodeXGLUE (code-to-text; avg. over 6 languages)             |        32.33            |     **32.76**      |   28.98 |\n\n\n> The paper misses the details of parameterization/hyperparameter selection. For example, the authors did not write how the layers of the anchor and the augmenting models are selected.\n\nOur framework only involves one hyperparameter of n, specifying the number of layers used for composition in both models. For some value of n, the output representation from every (NA/n)th layer in the augmenting model is used for composition with every (NB/n)th layer in the anchor model  (where NA and NB are the number of layers in the two models, respectively). \n\nFor all our experiments, we set the value of (NA/n) as 4. We have now specified this hyperparameter choice in Section 4.\n\n\n> How the projection function works. [...] What is the definition of DBj? Does DA\u2295Bj contain the information from all DA or only from a specific layer DAi?\n\nThe projection is a simple learnable linear layer that maps representations from mA to the dimensionality of mB."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6139/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700509408366,
                "cdate": 1700509408366,
                "tmdate": 1700509408366,
                "mdate": 1700509408366,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]