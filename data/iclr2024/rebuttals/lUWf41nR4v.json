[
    {
        "title": "Addressing Long-Horizon Tasks by Integrating Program Synthesis and State Machines"
    },
    {
        "review": {
            "id": "OYGvue3sMR",
            "forum": "lUWf41nR4v",
            "replyto": "lUWf41nR4v",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission309/Reviewer_bA3y"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission309/Reviewer_bA3y"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the problem of solving long-horizon tasks through programmatic RL. Specifically, the authors first learn the program embedding space for sampling the mode programs, and then learn a mode transition function to compose the programs towards the final task goal. Experiments are performed on several tabular tasks to validate the idea."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper presents an interesting idea that utilizes state machine representations and program generation for solving long-horizon tasks; \n\nThe performance is on par or better than other baselines from the evaluation perspective; \n\nThe paper is well-written and easily read."
                },
                "weaknesses": {
                    "value": "The framework utilizes 3 stage design where the learning of the program embeddings and the mode transition function are separated, which could lead to cascading errors;  \n\nThe absence of strict constraints to guarantee compatibility between generated programs could result in execution incompatibilities; \n\nThe experiments only consider simple tabular tasks, and only a few baselines are compared. Demonstrating the effectiveness of the framework on more complex tasks would strengthen the paper."
                },
                "questions": {
                    "value": "In Sec. 4.2.3, the most compatible program is selected by inserting the newly generated program into a randomly chosen program sequence and ranking the returns of the execution, would this design guarantee compatibility? If not, what\u2019s the failure ratio of the case that the generated program cannot be executed due to the compatibility issue? \n\nIt seems the method does not perform very ideally in the INF-DOORKEY and INF-HARVESTER, could authors provide additional explanations on this? Including more analysis of the failure modes would also help readers understand the limitations of the work; \n\nSome related works also explore state machines for high-level transition abstraction in RL, e.g., integrating symbolic planning and skill policies [1][2][3], please consider discussing or comparing; \n\nReference: \n[1] LEAGUE: Guided Skill Learning and Abstraction for Long-Horizon Manipulation, RA-L 2023;   \n[2] Leveraging approximate symbolic models for reinforcement learning via skill diversity, ICML 2022; \n[3] PEORL: Integrating Symbolic Planning and Hierarchical Reinforcement Learning for Robust Decision-Making, IJCAI 2018."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission309/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission309/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission309/Reviewer_bA3y"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission309/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698095298237,
            "cdate": 1698095298237,
            "tmdate": 1699635957621,
            "mdate": 1699635957621,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cclunoLZRV",
                "forum": "lUWf41nR4v",
                "replyto": "OYGvue3sMR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission309/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission309/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer bA3y (1/3)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the thorough and constructive comments. Please find the response to your questions below.\n\n### Responses to Questions\n\n> The framework utilizes 3 stage design where the learning of the program embeddings and the mode transition function are separated, which could lead to cascading errors.\n\nWe thank the reviewer for raising this concern. In the following, we discuss how error propagation may happen from Stage 1 (i.e., learning a program embedding space) to Stage 2 (i.e., retrieving mode programs), and from Stage 2 to Stage 3 (i.e., constructing the policy by learning a transition function with the mode programs).\n\n**Stage 1 $\\rightarrow$ Stage 2**: Stage 1 learns a program embedding space that continuously parameterizes diverse programs. Once the reconstruction loss is optimized, the program decoder can ideally decode all the training programs given corresponding program embeddings. In fact, the experiments show that with a sufficient embedding size, we can achieve 100% reconstruction accuracy on the entire training program set. Moreover, it can generate novel programs by interpolating in the learned program embedding space, as demonstrated in Trivedi et al., 2021. We believe the cascading error issue would not happen between Stage 1 and Stage 2, i.e., the training programs would not be \"dropped/lost\" in Stage 1.\n\n**Stage 2 $\\rightarrow$ Stage 3**: Stage 2 retrieves a set of programs as modes of a state machine, and Stage 3 learns a transition function based on the retrieved mode programs. Therefore, as pointed out by the reviewer, if none of the programs retrieved from Stage 2 can solve a part of a task, Stage 3 cannot recover from such failure. To maximally avoid this issue of missing programs in Stage 2, this work focuses on designing search methods that can reliably retrieve all the essential programs in Stage 2. Specifically, we propose a CEM-based search method considering a set of programs' effectiveness, diversity, and compatibility. The ablation studies compare various methods for program retrieval and verify the superior performance of our proposed search method. \n\n> The absence of strict constraints to guarantee compatibility between generated programs could result in execution incompatibilities. In Sec. 4.2.3, the most compatible program is selected by inserting the newly generated program into a randomly chosen program sequence and ranking the returns of the execution, would this design guarantee compatibility? If not, what\u2019s the failure ratio of the case that the generated program cannot be executed due to the compatibility issue?\n\nWe thank the reviewer for raising this question. Each decoded program is guaranteed to be executable because a syntax mask is applied to the decoder during the program decoding process proposed by Trivedi et al., 2021 to ensure syntactic correctness. That said, we can safely (i.e., it does not \u201cfail\u201d) execute any programs in any order. \n\nWe would like to clarify that we introduced the compatibility bonus to account for task performance instead of executability. For example, given a set of retrieved programs $p_1$ and $p_2$, the compatibility bonus tends to retrieve the third mode program $p_3$ if executing some orders (e.g., $p_1 \\rightarrow p_3 \\rightarrow p_2$,  $p_2 \\rightarrow p_3$) of these three programs yields satisfactory rewards.\n\n> The experiments only consider simple tabular tasks, and only a few baselines are compared. Demonstrating the effectiveness of the framework on more complex tasks would strengthen the paper.\n\n**Baselines**: We believe that our comparisons to the baselines are sufficient. HPRL (Liu et al., 2023) is the current state-of-the-art method published at ICML 2023 (July 2023). We have revised the paper to include comparisons to the Moore machine network framework proposed by Koul et al., 2019, and state machine policies inspired by Inala et al., 2020. We would appreciate it if the reviewer could be specific about which methods we should compare against.\n\n**Tasks**: We would like to emphasize that the tasks in the Karel domain characterize various properties that resemble hard RL problems, such as long horizons (up to tens of thousands of time steps),  sparse rewards, deceptive rewards, etc. Nevertheless, we agree with the reviewer that extending our work to a variety of domains, e.g., ViZDoom (Kempka et al., 2016), 2D Minecraft (Sun et al., 2020), gym-minigrid (Chevalier-Boisvert et al., 2023), robot arm manipulation (Liang et al., 2023, Wang et al., 2023), is a promising research direction."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission309/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700666665127,
                "cdate": 1700666665127,
                "tmdate": 1700666982890,
                "mdate": 1700666982890,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CRrWA5fU9z",
                "forum": "lUWf41nR4v",
                "replyto": "OYGvue3sMR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission309/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission309/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer bA3y (2/3)"
                    },
                    "comment": {
                        "value": "> It seems the method does not perform very ideally in the INF-DOORKEY and INF-HARVESTER, could authors provide additional explanations on this? Including more analysis of the failure modes would also help readers understand the limitations of the work.\n\nWe thank the reviewer for raising this question. \n\n**Inf-Harvester**: We noted that in the Inf-Harvester task, there is a drastic difference between retrieving programs and learning the transition function. Specifically, when retrieving mode programs, it seldom starts executing program candidates from a state with a small number of markers, and therefore, the retrieved programs modes mainly are specialized in collecting \u201cdense\u201d markers; however, when learning the transition function, after executing a few programs, the state contains only a small number of markers, and no program mode is specialized in such a scenario. \n\n**Inf-DoorKey**: Our proposed POMP achieves the second-best performance in the Inf-DoorKey task, with an average reward of 0.91, outperforming HPRL (0.15) and LEAPS (0.1) by a large margin. DRL achieves the best performance (0.97). The Inf-DoorKey environment consists of four small chambers (i.e., one 3x3, two 2x3, and one 2x2) connected to each other. Therefore, each stage of Inf-Doorkey requires only moderate exploration, allowing DRL to achieve good performance. \n\n> Some related works also explore state machines for high-level transition abstraction in RL, e.g., integrating symbolic planning and skill policies [1][2][3], please consider discussing or comparing. Reference: \n\n> [1] LEAGUE: Guided Skill Learning and Abstraction for Long-Horizon Manipulation, RA-L 2023;\n\n> [2] Leveraging approximate symbolic models for reinforcement learning via skill diversity, ICML 2022; \n\n> [3] PEORL: Integrating Symbolic Planning and Hierarchical Reinforcement Learning for Robust Decision-Making, IJCAI 2018.\n\nWe thank the reviewer for pointing out these relevant works on symbolic planning, and we have revised the paper to discuss them. The major difference between POMP and Cheng and Xu, 2023 and Lin et al., 2022 is the interpretability of the skill or option. In POMP, each learned skill is represented by a human-readable program. On the other hand, neural networks used by Cheng and Xu, 2023 and tabular approaches used by Lin et al., 2022 are used to learn the skill policies. In Yang et al., 2018, the option set is assumed as input without learning and cannot be directly compared with Cheng and Xu, 2023, Lin et al., 2022 and POMP.\n\nAnother difference between the proposed POMP framework and Cheng and Xu, 2023, Lin et al., 2022, and Yang et al., 2018 is whether the high-level transition abstraction is provided as input. In Cheng and Xu, 2023, a library of skill operators is taken as input and serves as the basis for skill learning. In Lin et al., 2022, the set of \u201clandmarks\u201d is taken as input to decompose the task into different combinations of subgoals. In PEORL, the option set is taken as input, and each option has a 1-1 mapping with each transition in the high-level planning. On the other hand, the proposed POMP framework utilized the set of retrieved programs as  modes, which is conducted based on the reward from the target task without any guidance from framework input."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission309/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700666858355,
                "cdate": 1700666858355,
                "tmdate": 1700666959157,
                "mdate": 1700666959157,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "J51Fze4xNv",
                "forum": "lUWf41nR4v",
                "replyto": "OYGvue3sMR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission309/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission309/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer bA3y (3/3)"
                    },
                    "comment": {
                        "value": "### References\n\n- Liu et al. \u201cHierarchical Programmatic Reinforcement Learning via Learning to Compose Programs\u201d ICML 2023 \n- Inala et al. \u201dSynthesizing Programmatic Policies that Inductively Generalize\u201d ICLR 2020 \n- Trivedi et al. \u201cLearning to Synthesize Programs as Interpretable and Generalizable Policies\u201d NeurIPS 2021 \n- Koul et al. \"Learning Finite State Representations of Recurrent Policy Networks\" in ICLR 2019 \n- Kempka et al. \"Vizdoom: A doom-based ai research platform for visual reinforcement learning\" IEEE Conference on Computational Intelligence and Games 2016 \n- Sun et al. \u2018Program Guided Agent\u201d in ICLR 2020 \n- Chevalier-Boisvert, et al. \u201cMinigrid & Miniworld: Modular & Customizable Reinforcement Learning Environments for Goal-Oriented Tasks\u2019\u201d arxiv 2023 \n- Liang et al. \u201cCode as Policies: Language Model Programs for Embodied Control\u201d ICRA 2023 \n- Wang et al. \u201cDemo2Code: From Summarizing Demonstrations to Synthesizing Code via Extended Chain-of-Thought\u201d arxiv 2023 \n- Cheng and Xu \"LEAGUE: Guided Skill Learning and Abstraction for Long-Horizon Manipulation\" RA-L 2023\n- Lin et al. \"Leveraging Approximate Symbolic Models for Reinforcement Learning via Skill Diversity\" ICML 2022\n- Yang et al. \"PEORL: Integrating Symbolic Planning and Hierarchical Reinforcement Learning for Robust Decision-Making\" IJCAI 2018\n\n### Conclusion\n\nWe are incredibly grateful to the reviewer for the detailed and constructive review. We believe our responses address the concerns raised by the reviewer. Please kindly let us know if there are any further concerns or missing experimental results that potentially prevent you from accepting this submission. We would be more than happy to address them if time allows. Thank you very much for all your detailed feedback and the time you put into helping us to improve our submission."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission309/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700666974101,
                "cdate": 1700666974101,
                "tmdate": 1700667109327,
                "mdate": 1700667109327,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "B0eiIWoDDP",
            "forum": "lUWf41nR4v",
            "replyto": "lUWf41nR4v",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission309/Reviewer_dxYr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission309/Reviewer_dxYr"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a programmatic RL framework which is both interpretable and capable of inductive generalisation. This is achieved by a three steps process. First the program embedding space is pre-trained using a VAE. This embedding space is then searched using CEM  to obtain programs which are then used to construct a finite state machine in the third step. The transitions between modes is also learned using RL. The primary technical contribution of this work lies particularly in how the programs are retrieved to ensure that they are able to obtain rewards (effective), provide new capabilities to the model (diverse) and also work well when applied in sequence (compatible)."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "## Originality\nThis work combines existing ideas in a unique way to achieve the proposed framework. The main originality of this work lies in the construction of the evaluation function which influences how the programs are obtained from the embedding space. Explicitly enforcing sampling of programs which are compatible with the previous and subsequent programs appears original and useful to the RL setting.\n\n## Quality\nThis work clearly motivates each step in the pipeline and each component of the evaluation function. The figures of this paper are particularly high quality and helpful in presenting the pipeline. Overall the experimental design and setup is appropriate to evaluate the hypothesis of the work and the extension of the benchmarks is clearly able to determine the relative performance of the baseline algorithms and POMP, as well as the effect of the various ablations on POMP. Experimental results are interpreted fairly and presented clearly.\n\n## Clarity\nThe work is written well and figures are legible and clear. An extremely minor point, in Figure 6a marker ticks aren't used but in Figure 6b they are. Consistency on this would be nice and if possible adding the ticks to 6a would be useful. Once again, I will note that the explanatory figures are particularly useful. The general consistency between them is makes them work together particularly well and presenting the ideas.\n\n## Significance\nInterpretability and hierarchical policies are both well established concepts and are important in RL. Thus, the prospect of having a hierarchical and interpretable model is definitely significant. I also agree that in most hierarchical frameworks it is usually the case that some clarity in how a policy achieves a certain goal is lost at the more detailed level. There is, however, a greater degree of clarity at the level of the hierarchical policy which transitions through the state machine or chooses programs/options/value functions. Thus, the proposed pipeline as it is presented would be clearly significant to the field for achieving an interpretable policy across both levels of the policy abstraction. Additionally, the new benchmarks appear challenging and increase the significance of the work. Overall, I can see this work leading to future work in programmatic RL, as well as being of practical use."
                },
                "weaknesses": {
                    "value": "## Quality\nMy primary concern for this work is the depth to which the proposed pipeline is considered. I think that all explicit claims in this work are accurate, however I think that the pipeline is not challenged or considered fully. For example, the compatibility regulariser ensures that programs are compatible to the previous and subsequent programs. This does not seem to ensure compatibility across multiple programs. There is almost a markov assumption being implicitly used at the level of the programs. If this is true, it should be stated explicitly as it is an important point. It makes the comparison to Skill Machines less valid and also makes the pipeline resemble an Options framework far more. Moreover, skill machine are primarily used to encode long-horizon tasks where transitions between FSM states corresponds to a meaningful (usually semantic) change in the environment. However, since all all programs here are being sampled from a pre-trained embedding space which is not conditioned on the state or context of the environment, it seems all programs are only usable in the same environment, making them more monolithic in their own right. On the point of this resembling Options, how inaccurate would it be to characterise POMP as being interpretable Options since there is still a high-level neural network being used to pick programs.\n\nGiven the above my score is quite low to begin with, but I also acknowledge that some of my criticisms could also be due to me missing a subtle distinction between POMP and other works. So my confidence will also remain relatively low to start. I am certainly willing to increase both following a discussion on the above if it is shown that I have indeed not appreciated a point fully. I also reiterate, the work as proposed seems very significant, I am just not certain POMP goes all the way to achieving what is proposed.\n\nFinally, I am concerned with the baselines which are being compared against. They seem to perform extremely poorly on the benchmarks, except in the cases where they work well and then they beat POMP. For example, DRL on the INF-DOORKEY domain and HPRL on INF_HARVESTER. While it beating POMP is not a problem, I would like to know what about this one particular domain made DRL work and better. Similarly in the ablation study, why are CEM and CEM+diversity capped at 0.5 performance. Why is the performance of CLEAN HOUSE and SNAKE particularly bad for both. Relatively little insight or discussion is given on these points. As a consequence it is difficult to get a good idea of the strength and weaknesses of POMP."
                },
                "questions": {
                    "value": "I have asked a number of questions above in the weaknesses section which I think would aid my understanding greatly if answered. However, in line with some of what was mentioned there I would like to ask if the authors have a sense of the recursive optimality of POMP? This would also help contextualise or ground POMP in the rest of the hierarchical RL literature."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission309/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission309/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission309/Reviewer_dxYr"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission309/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698751827896,
            "cdate": 1698751827896,
            "tmdate": 1699635957518,
            "mdate": 1699635957518,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "laadohRmTs",
                "forum": "lUWf41nR4v",
                "replyto": "B0eiIWoDDP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission309/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission309/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dxYr (1/4)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the thorough and constructive comments. Please find the response to your questions below.\n\n### Responses to Questions\n\n> An extremely minor point, in Figure 6a marker ticks aren't used but in Figure 6b they are. Consistency on this would be nice and if possible adding the ticks to 6a would be useful. \n\nWe thank the reviewer for this suggestion. We have fixed it in the revised paper.\n\n> I think that the pipeline is not challenged or considered fully. For example, the compatibility regulariser ensures that programs are compatible to the previous and subsequent programs. This does not seem to ensure compatibility across multiple programs. \n\nWe would like to emphasize that our proposed compatibility regulariser **ensures compatibility across the entire program sequence**. For example, say during the search phase, we have found 3 mode programs, $p_1$, $p_2$, $p_3$, and we are evaluating a candidate for the 4th program, $p_4$. At this iteration, if the program sequence $[p_2, p_1, p_4, p_3]$ was sampled, we will compute the execution reward by executing this entire program sequence, resulting in the compatibility score considering all $p_1$, $p_2$, $p_3$, not just the program executed right before and after $p_4$. We revised the paper to make this clear.\n\nAs shown in Table 2, by taking this compatibility that considers entire program sequences described in Section 4.2.3 into consideration, CEM+diversity+compatibility outperforms other baselines that ignore the compatibility issues. This result highlights the effectiveness of the designed compatibility regulariser.\n\n> There is almost a markov assumption being implicitly used at the level of the programs. If this is true, it should be stated explicitly as it is an important point. It makes the comparison to Skill Machines less valid and also makes the pipeline resemble an Options framework far more.\n\nWe would like to emphasize that we explicitly discuss this in the problem formulation section (Section 3) in the original paper. Specifically, it is stated that \u201cthe tasks considered in this work can be formulated as finite-horizon discounted Markov Decision Processes (MDPs).\u201d\n\nWe fully agree with the reviewer that our proposed Program Machine Policy shares the same spirit and some ideas with hierarchical reinforcement learning frameworks and semi-markov decision processes (SMDPs) if we view the transition function as a \u201chigh-level\u201d policy and the set of mode programs as \u201clow-level\u201d policies or skills. \n\nWhile most HRL frameworks either pre-define and pre-learned low-level policies, or jointly learn the high-level and low-level policies from scratch, our proposed framework first retrieves a set of effective, diverse, and compatible modes, and then learns the mode transition function.\n\nWe have revised the paper and included a discussion on HRL (Section A). We would appreciate it if the reviewer could suggest more related HRL and SMDP papers if our discussion misses anything.\n\n> Moreover, skill machine are primarily used to encode long-horizon tasks where transitions between FSM states corresponds to a meaningful (usually semantic) change in the environment. However, since all all programs here are being sampled from a pre-trained embedding space which is not conditioned on the state or context of the environment, it seems all programs are only usable in the same environment, making them more monolithic in their own right.\n\nWe thank the reviewer for raising this question. Following Trivedi et al., 2021 and Liu et al., 2023, we learn the pre-trained program embedding space using randomly generated task-agnostic programs. Once a program embedding space of a domain (e.g., the Karel domain) has been learned, it can be used for solving any tasks in this domain. In fact, **we used the precisely same program embedding space to produce policies for all the tasks in the Karel, Karel-Hard, and Karel-Long problem sets**. \n\nSince we construct such a program embedding space from randomly generating a program dataset, the majority of the generated programs do not capture meaningful behaviors. Therefore, devising efficient search methods that can quickly identify effective programs is critical, which is the main focus of LEAPS and HRPL. The search method proposed in our work, on the other hand, not only aims to obtain effective programs but is also designed to retrieve a set of effective, diverse, and compatible programs. For example, Figure 7 illustrates how the proposed CEM+diversity can avoid searching over paths similar to those previously explored, which drastically increases the searching efficiency."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission309/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700666159122,
                "cdate": 1700666159122,
                "tmdate": 1700666526340,
                "mdate": 1700666526340,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gHPW2pYY2f",
            "forum": "lUWf41nR4v",
            "replyto": "lUWf41nR4v",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission309/Reviewer_HDvE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission309/Reviewer_HDvE"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a system that combines program synthesis and finite state machines to generate policies for solving reinforcement learning problems. The authors built on the previous work of LEAPS, where one trains a latent space of programs that is used as a search space for policies. The LEAPS space is used to generate a set of diverse behaviors (modes), and later, a reinforcement learning algorithm is trained to learn how to transition from one mode to the next. \n\nThe system is evaluated on novel Karel tasks, which includes the need of repetitions."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The problem of learning policies with programmatic representations is an important one and this paper makes a contribution in this line of research. I also enjoyed the idea of mixing synthesis in the Karel language with finite state machines. Although I find it a little odd the dichotomy used in the paper: programmatic and FSM. I find it odd because a FSM is also a program, but in a language different from the Karel language. Perhaps the main contribution of this work is to show how to combine the inductive biases of two languages, where one is used to learn small functions and the other is used to combine such functions."
                },
                "weaknesses": {
                    "value": "Although I like many of the ideas in the paper and I also found it very easy to read and understand, I have several important issues with the current submission. \n\n**Interpretability Motivation**\n\nThe main motivation of the work is around interpretability. The paper states already in the abstract the separation between FSM and programmatic solutions, where the former allows for repetitive behaviors and the latter for interpretability. The paper specifically cites the work of Inala et al. as being difficult to be interpreted by human users. Similarly to how I can accept that programs in the Karel language can be interpretable, I can also accept that the FSM learned with Inala et al.'s system to also be interpretable. As far as I know, no paper has actually evaluated the interpretability of these systems, so the argument that FSM aren't interpretable is wrong to me. \n\nEven if we accept that the policies the system of Inala et al. generates aren't interpretable and the programs in the Karel language are, I still find it hard to accept that the policies POMP encodes are interpretable. The choices of which mode to execute next are provided by a neural network, which arguably is hard to interpret. While it is easy to accept that the modes are interpretable (although no evidence for it was provided), I would accept that the POMP policies are interpretable only after seeing some strong evidence of the fact. \n\n**Empirical Methodology**\n\nI apologize if I missed this in the paper, but is the process of learning modes accounted for in the curves shown in Figure 6a and in the numbers shown in Table 2? And is the learning of modes performed in the target task? I ask this question because, due to the compatible constrain, learning modes is essentially learning a solution to the problem because it already tries to combine programs such that they can maximize the expected reward. I might have also missed this information, but it isn't clear the number of steps used to train each system. \n\nIt is disappointing that POMP was not evaluated on the original Karel problems from LEAPS and HPRL. I was particularly interested in seeing POMP performance in the original DoorKey problem. Since both CEM and CEM+diversity get the reward of 0.50 with 0.0 standard deviation, I can only assume that the system learns how to get the key, but it doesn't open the door. Due to the way POMP is trained, it would not find a mode that knows how to act once the agent picks up the key. I suspect POMP would take much longer to reach the same reward of LEAPS and HPRL. I would like to see results of POMP on the original set of problems, to see how it compares with LEAPS and HPRL.\n\nSince I was not convinced with the interpretability argument against FSM and in favor of POMP, the experiments miss a baseline that learns FSMs. If not Inala et al.'s method, then the method by Koul et al. (Learning Finite State Representations Recurrent Policy Networks).\n\nI also missed a baseline that searches directly in the space of programs, like in genetic programming. For example, Aleixo & Lelis (Show Me the Way! Bilevel Search for Synthesizing Programmatic Strategies) use a two-level search with simulated annealing to search for programmatic policies. How would this method compare? \n\nWhat about NDS by Verma et al. (Programmatically interpretable reinforcement learning), can it be used to find programs for Karel?"
                },
                "questions": {
                    "value": "1. Is the process of learning modes accounted for in the curves shown in Figure 6a and in the numbers shown in Table 2? \n\n2. Is the learning of modes performed in the target task?\n\n3. How does POMP perform in the original DoorKey domain? \n\n4. Why not consider other baselines such as NDS and Simulated Annealing?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission309/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698902903394,
            "cdate": 1698902903394,
            "tmdate": 1699635957389,
            "mdate": 1699635957389,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CsICDcGJAx",
                "forum": "lUWf41nR4v",
                "replyto": "gHPW2pYY2f",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission309/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission309/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer HDvE (1/3)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the thorough and constructive comments. Please find the response to your questions below.\n\n### Responses to Questions\n\n> The paper specifically cites the work of Inala et al. as being difficult to be interpreted by human users. Similarly to how I can accept that programs in the Karel language can be interpretable, I can also accept that the FSM learned with Inala et al.'s system to also be interpretable. As far as I know, no paper has actually evaluated the interpretability of these systems, so the argument that FSM aren't interpretable is wrong to me.\n\nWe thank the reviewer for pointing that out. We agree with the reviewer that symbolic programs used in Inala et al., 2020's systems are more interpretable than neural network policies. However, we still believe that such symbolic mode programs describing the relationship among variables, which are presented in Figure 16 to Figure 21 in Inala et al., 2020 paper, are less interpretable than our programs written in a formal language. Trivedi et al., 2021 demonstrated that users with basic programming skills can interpret, edit, and improve the programs synthesized by LEAPS, which highlights the interpretability of DSL programs. \n\nAs suggested by the reviewer, we have revised the paper and toned down our statement. Instead of stating that the FSMs used in Inala et al., 2020 are \"difficult to be interpreted by human users,\" we now argue such FSMs \"are less interpretable (compared to DSL programs).\"\n\n> Even if we accept that the policies the system of Inala et al. generates aren't interpretable and the programs in the Karel language are, I still find it hard to accept that the policies POMP encodes are interpretable. The choices of which mode to execute next are provided by a neural network, which arguably is hard to interpret. While it is easy to accept that the modes are interpretable (although no evidence for it was provided), I would accept that the POMP policies are interpretable only after seeing some strong evidence of the fact.\n\nWe agree with the reviewer that employing the neural network transition function makes our proposed program machine policies not entirely interpretably. However, the program machine policies still enjoy \"local interpretibility.\" That said, once the transition function chooses each mode program, human users can read and understand the following execution of the program. To reflect this point raised by the reviewer, we have revised the paper to emphasize that the policies produced by our proposed framework are \"locally interpretable.\" We appreciate the reviewer for helping us clarify this.\n\nTo further improve the interpretability of the proposed program machine policies, we have conducted additional experiments to extract Moore machines from learned transition functions using the Moore machine network framework proposed by Koul et al., 2019. By synthesizing the corresponding Moore machine from a learned transition function, we can explicitly display the learned mode transition mechanism as a state machine and thus make the overall framework more interpretable. The extracted results on \"Farmer,\" \"Inf-Doorkey,\" and \"Inf-Harvester\" are shown in Section E of the revised paper.\n\n> Is the process of learning modes accounted for in the curves shown in Figure 6a and in the numbers shown in Table 2? And is the learning of modes performed in the target task? I ask this question because, due to the compatible constrain, learning modes is essentially learning a solution to the problem because it already tries to combine programs such that they can maximize the expected reward. I might have also missed this information, but it isn't clear the number of steps used to train each system.\n\nWe thank the reviewer for raising the questions.\n- Yes, timesteps of learning modes are accounted for in the curves shown in Figure 6a. The detail about how returns in Figure 6a are calculated is addressed in Section C of the revised paper.\n- Yes, the mode program retrieving process introduced in Section 4.2.3 is performed on each target task to ensure that the retrieved mode programs are diverse and compatible for solving the given task."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission309/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700665888976,
                "cdate": 1700665888976,
                "tmdate": 1700665963942,
                "mdate": 1700665963942,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IvFjzFHkSv",
            "forum": "lUWf41nR4v",
            "replyto": "lUWf41nR4v",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission309/Reviewer_NAAH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission309/Reviewer_NAAH"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes to solve long-horizon tasks by synthesizing programs to accomplish the task. Since existing methods for synthesizing programs struggle with long-horizon tasks, this work proposes to synthesize simple programs and then learn to compose these programs. Specifically, this work takes an approach involving three steps:\n(1) An embedding space over programs is learned.\n(2) A set of diverse and reusable programs is retrieved by searching over the embedding space.\n(3) A function is learned to compose these programs: i.e., to choose which programs to execute one after another.\n\nThis work evaluates its proposed approach on a benchmark of Karel tasks, involving picking and placing objects in a grid world and finds that the proposed approach performs favorably compared to prior approaches for program synthesis."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This work investigates an interesting and novel method for solving RL tasks by generating and composing programs. The results indicate improved performance over prior approaches and could be interesting to the RL community."
                },
                "weaknesses": {
                    "value": "I initially lean toward rejection due to two primary concerns:\n\n(1) *Unclear presentation and missing details.*\n- Several key pieces of information are not clearly described in the main text of the paper, which makes it difficult to understand the exact proposed approach and to reproduce the results:\n- How are the programs over which the embedding space is learned generated? Are they simply randomly sampled from the Karel DSL? These programs form such a crucial building block for the method that it's worth discussing how they're generated here, and how they might be generated in other domains.\n- How are the rewards defined in the CEM optimization in the retrieval stage? It seems like one would need many different reward functions to ensure good diversity and coverage in the selected modes. Is it task-specific? This crucial information does not appear to be clearly explained anywhere.\n- How is the transition function f learned? Is this learned via standard RL, where choosing the modes is the action?\n- How do state machine policies differ from other notions of hierarchical reinforcement learning? It seems like the modes can simply be interpreted as \"skills\" or formally options in a SMDP framework.\n- Details about the state are missing from the main body of the work, which makes it difficult to understand the task. From what I gather from the Appendix, a N x M array is used as the state for a N x M grid world for the deep RL baseline -- is this the same state that the proposed approach uses for selecting which mode to switch to?\n- The deep RL baseline seems to be surprisingly weak, given that the task is just grid world pick and place. Is there a reason behind this? Is it a result of the state representation? Does e.g., using a different state representation of just the agent's (x, y) coordinates work better?\n\n(2) *Concerns about the generality of the proposed approach.*\n- While the details for how the entire space of programs is generated is missing, it seems difficult to imagine being able to do this well for arbitrary domains. In general, writing programs that can easily compose is challenging, particularly when they have inputs and outputs, whereas this domain is particularly simple and the programs do not need to take arguments or return values. Consequently, I'm concerned that the proposed approach may not be able to be applied to other less toy domains. Discussion about this would be very helpful.\n- It also seems generally difficult to span a set of useful behaviors needed in the programs. How can this be achieved without ending up with an intractable space of programs to search over?"
                },
                "questions": {
                    "value": "Please see the many questions in the previous section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission309/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698907810873,
            "cdate": 1698907810873,
            "tmdate": 1699635957315,
            "mdate": 1699635957315,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SpumJkwelR",
                "forum": "lUWf41nR4v",
                "replyto": "IvFjzFHkSv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission309/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission309/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer NAAH (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the thorough and constructive comments. Please find the response to your questions below.\n\n### Responses to Questions\n\n> Unclear presentation and missing details\n\nWe sincerely thank the reviewer for pointing these out. We have thoroughly revised the paper to include the missing details and make the existing information easier to find.\n\n> How are the programs over which the embedding space is learned generated? Are they simply randomly sampled from the Karel DSL? These programs form such a crucial building block for the method that it's worth discussing how they're generated here, and how they might be generated in other domains.\n\nAs described in Section 4.1, we follow the training procedure proposed by LEAPS (Trivedi et al., 2021), which generates 50000 programs that are randomly sampled according to the Karel DSL to train the encoder-decoder model. Further details about the program dataset generation and the training of the encoder-decoder model can be found in Section F.1 of the revised paper. \n\nTo generate programs in another domain, we can simply randomly sample programs according to the DSL in that domain \"for free\" and then learn a program embedding space.\n\n> How are the rewards defined in the CEM optimization in the retrieval stage? It seems like one would need many different reward functions to ensure good diversity and coverage in the selected modes. Is it task-specific? This crucial information does not appear to be clearly explained anywhere.\n\nThe reward used by POMP for CEM optimization, as shown in Equation 1 and Figure 3, is defined as: \n\n$R\\_{\\rho} = \\sum\\_{i=1}^{|\\Psi\\_{\\text{before}}|} \\gamma ^ {i-1} \\sum\\_{t=0}^{T^i} \\gamma^t \\mathbb{E}\\_{(s\\_t,a\\_t) \\sim \\text{EXEC} (\\Psi\\_{\\text{before}}[i])}[r\\_{t}] + \\gamma^{|\\Psi\\_{\\text{before}}|}\\sum\\_{t=0}^{T} \\gamma^t \\mathbb{E}\\_{(s\\_t,a\\_t) \\sim \\text{EXEC} (\\rho\\_z)}[r\\_{t}]  + \\gamma^{|\\Psi\\_{\\text{before}}|+1}\\sum\\_{i=1}^{|\\Psi\\_{\\text{after}}|} \\gamma^{i-1} \\sum\\_{t=0}^{T^i} \\gamma^t \\mathbb{E}\\_{(s\\_t,a\\_t) \\sim \\text{EXEC} (\\Psi\\_{\\text{after}}[i])}[r\\_{t}].$\n\nNote that this reward function is not task-specific, and we use this reward to retrieve the set of mode programs for POMP across all tasks discussed in Section 5.1.\n\n> How is the transition function f learned? Is this learned via standard RL, where choosing the modes is the action?\n\nAs stated in Section 4.3 and Section F, the learning process of transition function $f$ is formulated as a standard RL problem. A state includes the current Karel grid and the previously executed mode represented by a one-hot vector, while an action chooses the next mode program, as correctly understood by the reviewer. We use the PPO algorithm to optimize the transition function $f$.\n\n> How do state machine policies differ from other notions of hierarchical reinforcement learning? It seems like the modes can simply be interpreted as \"skills\" or formally options in a SMDP framework.\n\nWe fully agree with the reviewer that our proposed Program Machine Policy shares the same spirit and some ideas with HRL frameworks and SMDP if we view the transition function as a \u201chigh-level\u201d policy and the set of mode programs as \u201clow-level\u201d policies or skills. \n\nWhile most HRL frameworks either pre-define and pre-learned low-level policies, or jointly learn the high-level and low-level policies from scratch, our proposed framework first retrieves a set of effective, diverse, and compatible modes, and then learns the mode transition function.\n\nWe have revised the paper and included a discussion on HRL (Section A). We would appreciate it if the reviewer could suggest more related HRL and SMDP papers if our discussion misses anything.\n\n> Details about the state are missing from the main body of the work, which makes it difficult to understand the task. From what I gather from the Appendix, a N x M array is used as the state for a N x M grid world for the deep RL baseline -- is this the same state that the proposed approach uses for selecting which mode to switch to?\n\nWe thank the reviewer for bringing up this issue. Yes, as described in Section 4.3 and Section F of the revised paper, both the DRL baseline and the mode transition function of POMP use a CNN to encode an input state of an N x M array, describing the current Karel grid, as shown in Figure 5 and Section H of the revised paper."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission309/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700665535486,
                "cdate": 1700665535486,
                "tmdate": 1700665535486,
                "mdate": 1700665535486,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]