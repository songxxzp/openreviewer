[
    {
        "title": "DRSM: De-Randomized Smoothing on Malware Classifier Providing Certified Robustness"
    },
    {
        "review": {
            "id": "SnhwWeM1WZ",
            "forum": "m7aPLHwsLr",
            "replyto": "m7aPLHwsLr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8147/Reviewer_EohB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8147/Reviewer_EohB"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose DRSM, a custom de-randomized smoothing algorithm for MalConv, an end-to-end convolutional neural network. DRSM works by dividing input malware in chunks, each of them containing a percentage of the input bytes. These are classified independently, and labels are provided through majority voting. Table 3 shows that, using different number of windows, DRSM is able to certify more than 40% of points, with a peak of almost 54%.\nTo empirically show their results, the authors also use state-of-the-art adversarial attacks against DRMS, highlighting that DRMS-12 to 24 are able to stop most of the proposed attacks.\nLastly, the authors also release their PACE dataset, by sharing URLs and SHAs of both malware and goodware programs."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper bridges an interesting gap between general machine learning robustness, and its application to complex domains like malware detection. \n2. The certification approach works by splitting malware in chunks, averging its predictions. This is doable thanks to the nature of the architecture. Also, this would have been much more difficult to do with a model relying on hand-crafted features.\n3. The PACE dataset is timely, since it is always difficult to get goodware samples."
                },
                "weaknesses": {
                    "value": "**No white-box evaluation.** The authors state that they compute attacks on the base model, thus framing them as white-box attacks (like Partial DOS; Shift, etc). However, these are evaluated as black-box transfer attack, and thus should be clarified on the paper.\n\n**Probable bad params for attacks.** The low success rates of attacks (especially GAMMA) might be due to a wrong initialisation. In the appendix, it is written that 200 as population size and query are used, but the number of queries for the GAMMA attack are computed as population_size * iterations. Also, the number of used sections is missing (which is a crucial point for the attack).\n\n**Dataset concerns.** While the release of a goodware dataset is for sure a great contribution, I am doubtful on the composition of such corpus. In particular, the sources might contain malware  or generic unwanted propgrams (Softonic is known to host plenty of installers and grayware that asks you to install other third-party programs). The authors should better clarify the origins of these data, or at least try to study the quality of the provided ground truth. Otherwise, the dataset might contain biases that reduce the fairness of the publication.\n\n**False statements.** The authors state that \"it is difficult to add more than 10% of content\". This statement is false, since adversarial malware attacks are automated through tools. Papers like [Demetrio et al. 2021a&b / Lucas et al. 2021] can increase the size more than 10% of the file size (Lucas et al. bound it to 5% just to not enlarge too much the input file).\nLastly, Header Modification is not proposed by Nisi et al. 2021, but it is contained inside the SecML Malware library, inspired by the paper (that states which fields are not used by the loader anymore).\n\n**Limitations and related work not addressed.** The paper does not discuss limitations of their methodology, by just saying that it is \n certification is a difficult problem to solve. Also, related work misses a preliminary (but unpublished) paper [1] that addressed the problem in the early months of 2023 (more than 6 moths ago). It would be better to mention the fact that preliminary work on certification for malware detection are already there.\n\n[1] Certified Robustness of Learning-based Static Malware Detectors - https://arxiv.org/pdf/2302.01757.pdf"
                },
                "questions": {
                    "value": "1. How you conducted the adversarial attacks? Which library did you use / how you obtained the code of attacks? Inside the provided material, it is not possible to test the attacks from Lucas et al.\n\n2. Can the authors provide better information on the quality of the collected goodware?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8147/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8147/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8147/Reviewer_EohB"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8147/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697729377991,
            "cdate": 1697729377991,
            "tmdate": 1700648094159,
            "mdate": 1700648094159,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vMHp6v0kba",
                "forum": "m7aPLHwsLr",
                "replyto": "SnhwWeM1WZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8147/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8147/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">>Thank you for your concise and constructive comments.\n\n>*\"No white-box evaluation. The authors state that they compute attacks on the base model, thus framing them as white-box attacks (like Partial DOS; Shift, etc). However, these are evaluated as black-box transfer attack, and thus should be clarified on the paper.\"*\n>>Thank you for pointing this out. We agree the current description might be confusing: Since the attacks require white-box access to the base models, they are indeed white-box. However, we agree they are in some sense transfer attacks since it is the base model rather than the DRSM itself being attacked. We have updated the **Section 7** for clarity. Let us know if you have further suggestions regarding presentations. We appreciate your feedback."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8147/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700249723864,
                "cdate": 1700249723864,
                "tmdate": 1700249723864,
                "mdate": 1700249723864,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ptm5kYQO4b",
                "forum": "m7aPLHwsLr",
                "replyto": "SnhwWeM1WZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8147/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8147/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">*\"Probable bad params for attacks. The low success rates of attacks (especially GAMMA) might be due to a wrong initialisation. In the appendix, it is written that 200 as population size and query are used, but the number of queries for the GAMMA attack are computed as population_size * iterations. Also, the number of used sections is missing (which is a crucial point for the attack).\"*\n>>We apologize for mentioning the query number as 200. We set the population size as 200, and ran it for 20 iterations. We have corrected it in the **Appendix A.4.8**. Also, for further confirmation, you can go to our provided code, and in the *line 68* of the `attack_malconv.py` file, you can find the initialization. It is -\n>>```\n>>attack = CGammaSectionsEvasionProblem(section_population, CEnd2EndWrapperPhi(net), population_size=200, penalty_regularizer=1e-12, iterations=20, threshold=0.5)\n>>```"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8147/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700249901603,
                "cdate": 1700249901603,
                "tmdate": 1700249901603,
                "mdate": 1700249901603,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1LGjzYl5dC",
                "forum": "m7aPLHwsLr",
                "replyto": "SnhwWeM1WZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8147/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8147/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">*\"Dataset concerns. While the release of a goodware dataset is for sure a great contribution, I am doubtful on the composition of such corpus. In particular, the sources might contain malware or generic unwanted propgrams (Softonic is known to host plenty of installers and grayware that asks you to install other third-party programs). The authors should better clarify the origins of these data, or at least try to study the quality of the provided ground truth. Otherwise, the dataset might contain biases that reduce the fairness of the publication.\"*\n>>Evaluating the quality of goodware data is a very good point, and we are actively trying to address this. We are in the process of getting access to the premium API of VirusTotal, and are planning to scan all the benign files to filter out any malicious file before the official release. Meanwhile, we are working on getting some preliminary statistics before the discussion ends."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8147/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700249969604,
                "cdate": 1700249969604,
                "tmdate": 1700249969604,
                "mdate": 1700249969604,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uXXYHTi33G",
                "forum": "m7aPLHwsLr",
                "replyto": "SnhwWeM1WZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8147/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8147/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">*\"False statements. The authors state that \"it is difficult to add more than 10% of content\". This statement is false, since adversarial malware attacks are automated through tools. Papers like [Demetrio et al. 2021a&b / Lucas et al. 2021] can increase the size more than 10% of the file size (Lucas et al. bound it to 5% just to not enlarge too much the input file).\"*\n>>In the last paragraph of subsection 6.2, we mentioned that \u2013 perturbing 200KB in 2MB file (=10%) is \u2018challenging\u2019 in a malware file. If that is the concern, we have rephrased it and toned it down to **\u2018considered as a sizeable modification\u2019**. If you have further suggestions, let us know. \n>\n>*\"Lastly, Header Modification is not proposed by Nisi et al. 2021, but it is contained inside the SecML Malware library, inspired by the paper (that states which fields are not used by the loader anymore).\"*\n>>We are aware of the fact that \u2013 Nisi et. al. 2021 did not explicitly propose the header field modification attack but we wanted to mention the most relevant paper to the attacks so that interested readers can study them. We have changed *\u2018proposed by\u2019* to *\u2018motivation from\u2019* in our text for Nisi et. al. paper. If you have further suggestions regarding presentations, we are more than happy to incorporate them."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8147/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700250121729,
                "cdate": 1700250121729,
                "tmdate": 1700250121729,
                "mdate": 1700250121729,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cRZLQLvDse",
                "forum": "m7aPLHwsLr",
                "replyto": "SnhwWeM1WZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8147/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8147/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">*\"Limitations and related work not addressed. The paper does not discuss limitations of their methodology, by just saying that it is certification is a difficult problem to solve.\n>>We wanted to put a \u2018Limitations\u2019 section before the \u2018Conclusion\u2019. However, due to the page constraint, we could not do that. So, we have added a \u2018Limitations\u2019 section in our **Appendix (A.6)** and referred to it in our \u2018Conclusion\u2019 section.\n>\n>Also, related work misses a preliminary (but unpublished) paper [1] that addressed the problem in the early months of 2023 (more than 6 moths ago). It would be better to mention the fact that preliminary work on certification for malware detection are already there.\n[1] Certified Robustness of Learning-based Static Malware Detectors - https://arxiv.org/pdf/2302.01757.pdf\"*\n>>Although ICLR policy tolerates such overlooks for preprints without peer reviews that are published after May 28 (https://iclr.cc/Conferences/2024/ReviewerGuide), we have discussed this recent paper in the \u2018Related Work\u2019 of our revision."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8147/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700250239986,
                "cdate": 1700250239986,
                "tmdate": 1700250239986,
                "mdate": 1700250239986,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wlOiUZYr1c",
                "forum": "m7aPLHwsLr",
                "replyto": "SnhwWeM1WZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8147/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8147/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">*\"How you conducted the adversarial attacks? Which library did you use / how you obtained the code of attacks? Inside the provided material, it is not possible to test the attacks from Lucas et al.\"*\n>>For most of the attacks we used the `secml-malware` Python library. Also, we built the DRSM framework on top of the secml-malware library so that it can be easily reproduced, extended, and evaluated against more attacks in future (mentioned in the footnote of page 2). \n>>\n>>We really appreciate your effort of checking our provided material and code. About Disp and IPR attack of Lucas et. al. \u2013 we collected the code implementation directly from the authors. Though we want to make everything of this work publicly available, we cannot do that for Disp and IPR attack. The white-box implementation of these attacks was kept private by the authors, and before access, we agreed to keep it private too."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8147/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700250342262,
                "cdate": 1700250342262,
                "tmdate": 1700250342262,
                "mdate": 1700250342262,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1cTnxBOTvc",
                "forum": "m7aPLHwsLr",
                "replyto": "vMHp6v0kba",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8147/Reviewer_EohB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8147/Reviewer_EohB"
                ],
                "content": {
                    "title": {
                        "value": "Transfer attacks are black-box attacks"
                    },
                    "comment": {
                        "value": "Thank you for the answer. But transfer attacks *are* black-box attacks.\nYou are optimizing samples against a believed-similar model, and then you hope they will be effective on the target. I read Section 7 and I think it should be clearly stated that attacks where computed on base malconv (through gradient-descent methods) and then later transfered to the real target, mimicking a blackbox attack.\nThen, it should be reported in limitation that currently, no state of the art gradient-based attacks have been defined."
                    }
                },
                "number": 31,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8147/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700472288491,
                "cdate": 1700472288491,
                "tmdate": 1700472288491,
                "mdate": 1700472288491,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Iq8lYC6hY2",
                "forum": "m7aPLHwsLr",
                "replyto": "Ptm5kYQO4b",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8147/Reviewer_EohB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8147/Reviewer_EohB"
                ],
                "content": {
                    "title": {
                        "value": "Still missing details about goodware used for the attack"
                    },
                    "comment": {
                        "value": "Thank you so much for this clarification, but what about the goodware used to manipulate the malware? How many sections? Which sections? I agree that there is not much space, but these details are important. Also because defenses should be evaulated at the best possible to avoid the same history on vision models [1,2,3].\n\n[1] Tramer, F., Carlini, N., Brendel, W., & Madry, A. (2020). On adaptive attacks to adversarial example defenses. Advances in neural information processing systems, 33, 1633-1645.\n[2] Athalye, A., Carlini, N., & Wagner, D. (2018, July). Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. In International conference on machine learning (pp. 274-283). PMLR.\n[3] Carlini, N., & Wagner, D. (2017, November). Adversarial examples are not easily detected: Bypassing ten detection methods. In Proceedings of the 10th ACM workshop on artificial intelligence and security (pp. 3-14)."
                    }
                },
                "number": 32,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8147/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700472610860,
                "cdate": 1700472610860,
                "tmdate": 1700472610860,
                "mdate": 1700472610860,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hMEvgberGs",
                "forum": "m7aPLHwsLr",
                "replyto": "uXXYHTi33G",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8147/Reviewer_EohB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8147/Reviewer_EohB"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the clarification."
                    },
                    "comment": {
                        "value": "Thank you for re-wording that sentence.\nStill, I would add a citation to the technique you have used from which paper.\nSo, if you have coded that attac by yourself, just write so. Otherwise, explicitly say where you took the attack implementation."
                    }
                },
                "number": 33,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8147/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700472714220,
                "cdate": 1700472714220,
                "tmdate": 1700472714220,
                "mdate": 1700472714220,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8qD5yf8iGY",
                "forum": "m7aPLHwsLr",
                "replyto": "wlOiUZYr1c",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8147/Reviewer_EohB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8147/Reviewer_EohB"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the reply on libraries"
                    },
                    "comment": {
                        "value": "Thank you for this reply, I would like the authors to add this statement to the paper.\nIt should be clear what is reproducible and what is not (and why)."
                    }
                },
                "number": 34,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8147/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700472874668,
                "cdate": 1700472874668,
                "tmdate": 1700472874668,
                "mdate": 1700472874668,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UM8xVJhynx",
                "forum": "m7aPLHwsLr",
                "replyto": "cRZLQLvDse",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8147/Reviewer_EohB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8147/Reviewer_EohB"
                ],
                "content": {
                    "title": {
                        "value": "I think it is possible to carve space for limitations"
                    },
                    "comment": {
                        "value": "Limitations are an important part of the paper, it helps reader understand where to go next. I totally understand the problems with space, but there are many tweaks you can do.\nFirst, remove bullet lists, they consume plenty of space. Same for paragraphs: you can use \\noindent to break the line without adding space. Then, all equations without numbers can go inline, etc."
                    }
                },
                "number": 35,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8147/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700473044727,
                "cdate": 1700473044727,
                "tmdate": 1700473044727,
                "mdate": 1700473044727,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "m8i4xOZ9v9",
                "forum": "m7aPLHwsLr",
                "replyto": "SnhwWeM1WZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8147/Reviewer_EohB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8147/Reviewer_EohB"
                ],
                "content": {
                    "title": {
                        "value": "Appreciated all the work during for rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for your work during this rebuttal period. \nI raised my score to ACCEPT (8)."
                    }
                },
                "number": 46,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8147/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700648157112,
                "cdate": 1700648157112,
                "tmdate": 1700648157112,
                "mdate": 1700648157112,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "P5ym5Tgp6Q",
            "forum": "m7aPLHwsLr",
            "replyto": "m7aPLHwsLr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8147/Reviewer_grdf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8147/Reviewer_grdf"
            ],
            "content": {
                "summary": {
                    "value": "This paper applies the de-randomized smoothing technique from the image classification domain to the domain of malware detection - proposing a window ablation scheme. Theoretical robustness is argued and certified and empirical robustness (the latter against a broad range of attacks) is tested empirically. A dataset of benign executables will also be made available to support future research."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This is a well written paper which clearly presents the core idea. The experiments are quite thorough and support the claims.\n\nMaking the dataset available for future research is a positive."
                },
                "weaknesses": {
                    "value": "The basic idea is quite simple, and this is not a contribution of major impact in the field. \n\nIt would have been good to see a description of de-randomized smoothing in the related work, as this is core to the idea.\n\nThere are a few grammatical issues throughout the paper - it needs a polish before publication, e.g.\n- \"We will use it as base classifiers\"\n- \"potentially attributing to the issue\"\n- \"convolution neural network\"\n- \"there have been a large amount of work\"\n- \"to some extents\"\n- \"though it has been believed as a robust model\"\n- \"However, it's worth highlighting..\" not sure However is right word.\n\nMinor issues:\n- X \\subset [0,N-1] is wrong. [0,N-1] is a real-valued continuous interval. I think it means X is the set {0, 1, ..., N-1}"
                },
                "questions": {
                    "value": "Would it be possible to indicate the values of \\Delta on the x-axis of figure 3? Everything has been in terms of that up to this point.\n\nOne issue would be with attacks which INSERT bytes. Section 3.2 seems to suggest this is possible (\"attacker can modify or add any bytes in a contiguous portion\"). Surely, if added at the start of the file, this can change the contents then of EVERY window, as all the bytes get shifted to the right. So adding bytes does not seem to be in the threat model that would give certified robustness.\n\nIn fact, the above comment may explain why the DOS Extension attack has such a big effect on the DRSM models in Figure 5."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8147/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697783994100,
            "cdate": 1697783994100,
            "tmdate": 1699637009845,
            "mdate": 1699637009845,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UaXPuqL6YH",
                "forum": "m7aPLHwsLr",
                "replyto": "P5ym5Tgp6Q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8147/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8147/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">*\"There are a few grammatical issues throughout the paper - it needs a polish before publication,\"*\n>>We really appreciate your effort in reading our paper thoroughly and pointing out the grammatical issues. We have corrected all of the mentioned ones. You can find them in blue text in the **updated version** of our submission. Let us know if you find anything else. We would love to have your feedback.\n\n>*\"It would have been good to see a description of de-randomized smoothing in the related work, as this is core to the idea.\"*\n>>While we understand that it would have added more clarity if we discussed de-randomized smoothing more in the \u2018Related Work\u2019, we could not do it due to space constraints. So, we have added a subsection in the **Appendix (A.2)** discussing the de-randomized smoothing in-depth, and have referred to this in our \u2018Related Work\u2019.\n\n>*\"X \\subset [0,N-1] is wrong. [0,N-1] is a real-valued continuous interval. I think it means X is the set {0, 1, ..., N-1}\"*\n>>Thank you for pointing this out. We have addressed this in our revised version."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8147/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700248938470,
                "cdate": 1700248938470,
                "tmdate": 1700249052978,
                "mdate": 1700249052978,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "M0vSsJuSIO",
                "forum": "m7aPLHwsLr",
                "replyto": "P5ym5Tgp6Q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8147/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8147/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">*\"Would it be possible to indicate the values of \\Delta on the x-axis of figure 3? Everything has been in terms of that up to this point.\"*\n>>We have added another figure **(Figure 10 in the Appendix A.4)** that shows the certified accuracy in terms of $\\Delta$. From this figure, it is interpretable that a smaller window size achieves higher certified accuracy. At the same time, we want to mention that \u2013 a smaller window allows attackers a smaller budget for perturbation. For example, if $\\Delta =2$, in DRSM-4, the attacker can perturb up to 511K bytes, whereas in DRSM-24, it is 82K bytes. So, it might not be fair to compare these models just depending on the $\\Delta$, and hence, we kept the Figure 3 showing accuracy with respect to perturbed bytes in the main text, and put the Figure 10 in the Appendix so that interested readers can see them and easily interpret the results. We would also love to have your feedback on this one."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8147/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700249165700,
                "cdate": 1700249165700,
                "tmdate": 1700249165700,
                "mdate": 1700249165700,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TkF0xImDjM",
                "forum": "m7aPLHwsLr",
                "replyto": "P5ym5Tgp6Q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8147/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8147/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">*\"One issue would be with attacks which INSERT bytes. Section 3.2 seems to suggest this is possible (\"attacker can modify or add any bytes in a contiguous portion\"). Surely, if added at the start of the file, this can change the contents then of EVERY window, as all the bytes get shifted to the right. So adding bytes does not seem to be in the threat model that would give certified robustness.\"*\n>\n>*\"In fact, the above comment may explain why the DOS Extension attack has such a big effect on the DRSM models in Figure 5.\"*\n>>\n>>This is a very good observation, and we are thankful for this. It might be the case that \u2013 the DOS extension attack has an impact on the rest of the windows to some extent, and thus the DRSM-4 and DRSM-8 are less robust to this attack. Since this attack still cannot directly perturb or alter bytes in other windows, it is partially aligned with our threat model. So, we have modified our **Table 4, subsection 3.2, and Appendix A.4.3**, accordingly. We have also added a subsection in the **Appendix (A.4.1)** discussing the probable reason for the higher ASR of DOS Extension Attack.  \nA future mitigation can be \u2013 extracting sections from the file and training different base classifiers on each of them (discussed in the **'Limitations' in Appendix A.6)**. Thus, attacks like DOS extension will not be able to impact other (or later) windows."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8147/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700249457120,
                "cdate": 1700249457120,
                "tmdate": 1700249457120,
                "mdate": 1700249457120,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0ISeOMckTm",
                "forum": "m7aPLHwsLr",
                "replyto": "TkF0xImDjM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8147/Reviewer_grdf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8147/Reviewer_grdf"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your responses. I acknowledge and appreciate the updates. That said, I am maintaining my original review score as I think this still represents a fair overall view of the paper."
                    }
                },
                "number": 30,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8147/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700429858188,
                "cdate": 1700429858188,
                "tmdate": 1700429858188,
                "mdate": 1700429858188,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zmFHsWyfFd",
            "forum": "m7aPLHwsLr",
            "replyto": "m7aPLHwsLr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8147/Reviewer_sYdT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8147/Reviewer_sYdT"
            ],
            "content": {
                "summary": {
                    "value": "This paper applies de-randomized smoothing to produce a classifier for malware detection (called DRSM) that is certifiably robust against patch attacks. The proposed classifier can be viewed as an ensemble of base classifiers, each of which operates on a distinct block of the input file. After collecting predictions for each block from the base classifiers, the prediction for the file as a whole is made by majority vote. This architecture admits a patch certificate that depends on the block size, maximum input length and the voting margin. The included experiments show that DRSM (with MalConv base classifiers) achieves a similar accuracy as vanilla MalConv, while producing patch certificates of order 100KB in size. Experiments examining empirical robustness to several attacks are also reported, which generally demonstrate improvements compared to vanilla MalConv and MalConv with non-negative weights. The paper also contributes a new dataset of benign executables, which is useful given the limited availability of publicly available benchmark datasets for malware."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. It\u2019s great to see a paper investigating certified robustness outside the vision domain, which has dominated the literature to date. The patch-like threat model seems well-motivated for malware, given it encompasses several existing attacks. \n\n2. Another strength of the work is its simplicity. DRSM is conceptually straightforward to implement and analyze, which could reduce barriers to adoption.\n\n3. I\u2019m pleased the authors have found a way to share a public dataset for malware analysis. The lack of public benchmark datasets is a major impediment for academic malware research. Having this dataset available will save researchers' time, and it should allow for better comparison between papers (which tend to use different datasets currently)."
                },
                "weaknesses": {
                    "value": "1. The paper can be seen as applying an existing method (de-randomized smoothing) to a new domain (malware). Although the authors claim that it is \u201cchallenging\u201d to adapt de-randomized smoothing for malware, I\u2019m not convinced. The proposed method is a form of structured ablation, originally studied by Levine & Feizi (2020) for 2-d inputs with homogeneous base classifiers. The modification from 2-d to 1-d inputs and from homogeneous to heterogeneous base classifiers seems straightforward. Moreover, the proposed method has appeared in prior work in a more general form by Hammoudeh & Lowd (2023).\n\n1. The paper claims to be \u201cfirst to offer certified robustness in the realm of static detection of malware executables\u201d. However there is prior work on this topic by Huang et al. (2023) which appeared on arXiv in January 2023. Their work considers a different threat model for malware: edit distance robustness rather than patch robustness.\n\n1. A characteristic feature of the malware domain is that inputs vary in length. However, it\u2019s not clear to me how the proposed classifier architecture handles this. As an example, consider a 100 KB malicious file and a classifier with a maximum input length of 2 MB. For $n$ in the range 4\u201320, the malicious file fits within a single block, meaning it is passed to a single base classifier, while the remaining $n - 1$ base classifiers receive padding as input. Assuming the base classifiers predict \u201cbenign\u201d for padding, the votes are $1$ \u201cmalicious\u201d and $n - 1$ \u201cbenign\u201d giving a prediction of \u201cbenign\u201d. I wonder if I\u2019m missing something here, because it seems the classifier is guaranteed to make false negative errors on small files, which are abundant according to Figure 6.\n\n1. I found the description of the threat model and certificate unclear. Section 3.2 states that the attacker is allowed to \u201cmodify or add any bytes in a contiguous portion\u201d. I understand that \u201cmodify\u201d means overwrite or replace, but it\u2019s not clear what \u201cadd\u201d means in this context. For instance, \u201cadd\u201d could mean \u201cinsert\u201d or \u201cappend\u201d, or it may mean \u201cincrement or decrement by some amount\u201d.  The mathematical description $x\u2019 = x + \\delta$ implies the original sequence $x$ is additively perturbed by $\\delta$ (which is undefined), but this seems to be at odds with the earlier description. Reading between the lines, my understanding is that the certificate covers a contiguous chunk of the original file being overwritten, which may include some bytes being appended to the end of the file. This should be precisely stated somewhere. The current definition of the certificate in Section 5 is in terms of ablated sequences \u2013 it would be helpful to translate this to the input space.\n\n1. It\u2019s great that the authors are planning to release the PACE dataset. However, the current description of the collection process is a bit light on detail. It would be helpful to describe how binaries were selected from the various sources. For instance, was there a preference for recent binaries? Are the binaries for a single platform (e.g., Windows x64) or multiple platforms? How do you know the binaries are benign?\n\nMinor points:\n1. Section 2 states that it\u2019s \u201csurprising\u201d MalConv is still considered state-of-the-art for malware detection on raw byte sequences, given it was released in 2018. The authors attribute this to limited availability of public data. However, I think the main reason is due to difficulties in scaling more complex models (such as transformers) to very long sequences, containing upwards of a million tokens. It\u2019s worth pointing out that the authors of MalConv have released a follow up model known as MalConv 2 (Raff et al., 2021).\n1. Section 3 states that the input vector fed into the network \u201chas to be of a fixed dimension\u201d. This is not true in general, and I don\u2019t believe it\u2019s true for MalConv. I believe it is possible to support arbitrary length inputs in modern frameworks such as PyTorch by specifying `None` for the size of the dimension.\n1. Section 3.1 states that models like EMBER and GBDT \u201ccan work only on feature vectors\u201d. I\u2019m a bit puzzled by this statement. When composed with their feature extractors, these models must be able to operate on raw binaries, otherwise they would be useless as malware detectors?\n1. Section 5 states that vision-oriented ablation techniques such as masking and block ablations are infeasible for byte sequences. However I can\u2019t see why these wouldn\u2019t work on 1d sequences? It seems feasible to mask bytes or ablate 1d blocks.\n1. Section 7 states MalConv NonNeg \u201chas been believed as a robust model for a long time\u201d. It would be good to include a citation for this claim.\n\n**References**\n\n- Huang et al., \u201cCertified robustness of learning-based static malware detectors,\u201d arXiv:2302.01757 (2023). https://arxiv.org/abs/2302.01757\n\n- Hammoudeh & Lowd, \u201cFeature Partition Aggregation: A Fast Certified Defense Against a Union of $\\ell_0$ Attacks,\u201d AdvML-Frontiers 2023. https://openreview.net/forum?id=NX5Nxrz6PV \n\n- Levine & Feizi, \u201c(De)Randomized Smoothing for Certifiable Defense against Patch Attacks,\u201d NeurIPS 2020. https://proceedings.neurips.cc/paper/2020/file/47ce0875420b2dbacfc5535f94e68433-Paper.pdf\n\n- Raff et al., \u201cClassifying Sequences of Extreme Length with Constant Memory Applied to Malware Detection,\u201d AAAI 2021. https://ojs.aaai.org/index.php/AAAI/article/view/17131/16938"
                },
                "questions": {
                    "value": "It would be great if the authors could comment on my feedback about:\n- novelty of DRSM (how does it differ from Levine & Feizi (2020) and Hammoudeh & Lowd (2023)?)\n- how DSRM operates on small files\n- the threat model"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8147/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8147/Reviewer_sYdT",
                        "ICLR.cc/2024/Conference/Submission8147/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8147/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698503494251,
            "cdate": 1698503494251,
            "tmdate": 1700636685749,
            "mdate": 1700636685749,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YY6NGuhxzG",
                "forum": "m7aPLHwsLr",
                "replyto": "zmFHsWyfFd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8147/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8147/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">>We really appreciate your effort in going into the details and for your constructive feedback. \n\n>*\"The paper can be seen as applying an existing method (de-randomized smoothing) to a new domain (malware). Although the authors claim that it is \u201cchallenging\u201d to adapt de-randomized smoothing for malware, I\u2019m not convinced. The proposed method is a form of structured ablation, originally studied by Levine & Feizi (2020) for 2-d inputs with homogeneous base classifiers. The modification from 2-d to 1-d inputs and from homogeneous to heterogeneous base classifiers seems straightforward. Moreover, the proposed method has appeared in prior work in a more general form by Hammoudeh & Lowd (2023).\"*\n>>We respectfully argue that the success of a \u2018general\u2019 method in a specific domain should not be taken for granted. An example will be the paper of Hammoudeh & Lowd (2023) mentioned in your review, which also resembles Levine & Feizi (2020): One contribution of Hammoudeh & Lowd (2023) is their study of partitioning strategies, in which they suggest using stridded input dimensions (pixels) is beneficial for vision tasks, potentially because information contained by adjacent pixels are redundant to some extent. In contrast, for the malware domain, due to its nature, some adjacent bytes are malicious only when they are combined, i.e., not considering adjacent bytes might end up representing a whole different instruction (or invalid instruction) that might not be malicious."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8147/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700250555227,
                "cdate": 1700250555227,
                "tmdate": 1700250555227,
                "mdate": 1700250555227,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ol8FRPTGTo",
                "forum": "m7aPLHwsLr",
                "replyto": "zmFHsWyfFd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8147/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8147/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">*\"The paper claims to be \u201cfirst to offer certified robustness in the realm of static detection of malware executables\u201d. However there is prior work on this topic by Huang et al. (2023) which appeared on arXiv in January 2023. Their work considers a different threat model for malware: edit distance robustness rather than patch robustness.\"*\n>>Although ICLR policy tolerates such overlooks for preprints without peer reviews that are published after May 28 (https://iclr.cc/Conferences/2024/ReviewerGuide), we have acknowledged and discussed this recent paper in the **\u2018Related Work\u2019 of our revision**. Additionally, we want to point out that \u2013 along with the different threat model (that you already mentioned), this paper adapts the randomized smoothing scheme, which differs from ours."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8147/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700250634573,
                "cdate": 1700250634573,
                "tmdate": 1700250634573,
                "mdate": 1700250634573,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GeAPVb77Ux",
                "forum": "m7aPLHwsLr",
                "replyto": "zmFHsWyfFd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8147/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8147/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">*\"A characteristic feature of the malware domain is that inputs vary in length. However, it\u2019s not clear to me how the proposed classifier architecture handles this. As an example, consider a 100 KB malicious file and a classifier with a maximum input length of 2 MB. For n in the range 4\u201320, the malicious file fits within a single block, meaning it is passed to a single base classifier, while the remaining n-1 base classifiers receive padding as input. Assuming the base classifiers predict \u201cbenign\u201d for padding, the votes are 1 \u201cmalicious\u201d and n\u22121 \u201cbenign\u201d giving a prediction of \u201cbenign\u201d. I wonder if I\u2019m missing something here, because it seems the classifier is guaranteed to make false negative errors on small files, which are abundant according to Figure 6.\"*\n>>This is a very good observation. To tackle this issue in our DRSM framework, we consider the votes (or predictions) from the base classifiers that get any input (except padding). We could consider all votes and solve this by adding an extra learnable layer (such as logistic regression) on all votes, but it would have hurt the non-differentiability property (eventually, certified robustness) of the whole framework."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8147/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700250714167,
                "cdate": 1700250714167,
                "tmdate": 1700250714167,
                "mdate": 1700250714167,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BFDrQtr4ol",
                "forum": "m7aPLHwsLr",
                "replyto": "zmFHsWyfFd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8147/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8147/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">*\"I found the description of the threat model and certificate unclear. Section 3.2 states that the attacker is allowed to \u201cmodify or add any bytes in a contiguous portion\u201d. I understand that \u201cmodify\u201d means overwrite or replace, but it\u2019s not clear what \u201cadd\u201d means in this context. For instance, \u201cadd\u201d could mean \u201cinsert\u201d or \u201cappend\u201d, or it may mean \u201cincrement or decrement by some amount\u201d. The mathematical description x\u2032=x+delta implies the original sequence x is additively perturbed by delta (which is undefined), but this seems to be at odds with the earlier description. Reading between the lines, my understanding is that the certificate covers a contiguous chunk of the original file being overwritten, which may include some bytes being appended to the end of the file. This should be precisely stated somewhere. The current definition of the certificate in Section 5 is in terms of ablated sequences \u2013 it would be helpful to translate this to the input space.\"*\n>>Thanks for your constructive feedback and asking for the clarification of the term \u2018add\u2019. We have rephrased a few sentences in the second paragraph of our threat model **(subsection 3.2)**. With the term \u2018add\u2019, we meant the attacker can \u2018insert\u2019 or \u2018append\u2019 any bytes; not \u2018increment / decrement\u2019. Additionally, we have mentioned that it has to be bounded in a contiguous portion. Also, we apologize if the mathematical description looked confusing. We have reformed that into a simple sentence now for better clarity. \nAdditionally, we have rephrased a few sentences in the **third paragraph of the threat model**, and mentioned what type of attack falls within our threat model. Let us know if you have further suggestions about the presentation. We would appreciate your feedback."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8147/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700250802047,
                "cdate": 1700250802047,
                "tmdate": 1700250802047,
                "mdate": 1700250802047,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WwnVer6z9W",
                "forum": "m7aPLHwsLr",
                "replyto": "zmFHsWyfFd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8147/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8147/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">*\"It\u2019s great that the authors are planning to release the PACE dataset. However, the current description of the collection process is a bit light on detail. It would be helpful to describe how binaries were selected from the various sources. For instance, was there a preference for recent binaries? Are the binaries for a single platform (e.g., Windows x64) or multiple platforms? How do you know the binaries are benign?\"*\n>>Thanks for appreciating our plan to release the PACE dataset. The sources of this dataset are given in **Table 2**, and we crawled these websites to download the benign files. We downloaded them in August, 2022 and there was no preference for recent binaries. Yes, the binaries are for a single platform (windows). Evaluating the quality of goodware data is a very good point, and we are actively trying to address this. We are in the process of getting access to the premium API of VirusTotal, and are planning to scan all the benign files to filter out any malicious file before the official release. Meanwhile, we are working on getting some preliminary statistics before the discussion ends."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8147/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700250901641,
                "cdate": 1700250901641,
                "tmdate": 1700250901641,
                "mdate": 1700250901641,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JCxa6MRQjD",
                "forum": "m7aPLHwsLr",
                "replyto": "zmFHsWyfFd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8147/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8147/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">*\"Section 2 states that it\u2019s \u201csurprising\u201d MalConv is still considered state-of-the-art for malware detection on raw byte sequences, given it was released in 2018. The authors attribute this to limited availability of public data. However, I think the main reason is due to difficulties in scaling more complex models (such as transformers) to very long sequences, containing upwards of a million tokens. It\u2019s worth pointing out that the authors of MalConv have released a follow up model known as MalConv 2 (Raff et al., 2021).\"*\n>>Thanks for pointing this out. Initially, we did not include this due to space constraints, but we have modified the **first paragraph in the 'Related Work'**, and discussed the MalConv 2 model.\n\n>*\"Section 3 states that the input vector fed into the network \u201chas to be of a fixed dimension\u201d. This is not true in general, and I don\u2019t believe it\u2019s true for MalConv. I believe it is possible to support arbitrary length inputs in modern frameworks such as PyTorch by specifying None for the size of the dimension.\"*\n>>With that sentence, we wanted to mention the original implementation of MalConv that was done for 2MB of file size.\n\n>*\"Section 3.1 states that models like EMBER and GBDT \u201ccan work only on feature vectors\u201d. I\u2019m a bit puzzled by this statement. When composed with their feature extractors, these models must be able to operate on raw binaries, otherwise they would be useless as malware detectors?\"*\n>>We are aware that EMBER, GBDT can work as malware detectors that require an extra feature extraction step unlike our work, and we apologize for the misunderstanding. We have rephrased that sentence in **subsection 3.1**.\n\n>*\"Section 5 states that vision-oriented ablation techniques such as masking and block ablations are infeasible for byte sequences. However I can\u2019t see why these wouldn\u2019t work on 1d sequences? It seems feasible to mask bytes or ablate 1d blocks.\"*\n>>An instruction is converted to multiple bytes that are usually contiguous, and masking a random byte might end up changing the meaning of an instruction or in the worst case, an invalid instruction. Block ablation was proposed for 2D input, it is not clear how it can be applied for 1D input. (We also update our manuscript with a recap of de-randomized smoothing in vision tasks in Appendix A.2.).  \n\n>*\"Section 7 states MalConv NonNeg \u201chas been believed as a robust model for a long time\u201d. It would be good to include a citation for this claim.\"*\n>>We see how this statement might be controversial. We have removed it from **section 7**."
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8147/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700251645818,
                "cdate": 1700251645818,
                "tmdate": 1700251645818,
                "mdate": 1700251645818,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pGYgiUlzCm",
                "forum": "m7aPLHwsLr",
                "replyto": "Ol8FRPTGTo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8147/Reviewer_sYdT"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8147/Reviewer_sYdT"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for pointing out the ICLR policy which excuses authors from citing preprints. It's great that you have decided to include a citation regardless."
                    }
                },
                "number": 26,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8147/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700315211121,
                "cdate": 1700315211121,
                "tmdate": 1700315211121,
                "mdate": 1700315211121,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Xd91iLFRiL",
                "forum": "m7aPLHwsLr",
                "replyto": "GeAPVb77Ux",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8147/Reviewer_sYdT"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8147/Reviewer_sYdT"
                ],
                "content": {
                    "comment": {
                        "value": "> To tackle this issue in our DRSM framework, we consider the votes (or predictions) from the base classifiers that get any input (except padding)\n\nJust to clarify, are you saying that DRSM does not incorporate votes from base classifiers that receive padding exclusively as input? Could you point out where this behavior is specified in the paper? Also, if votes from some classifiers are excluded dependent on the input, does the robustness certificate remain valid?"
                    }
                },
                "number": 27,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8147/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700315591304,
                "cdate": 1700315591304,
                "tmdate": 1700315591304,
                "mdate": 1700315591304,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oWMxmzZFuT",
                "forum": "m7aPLHwsLr",
                "replyto": "WwnVer6z9W",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8147/Reviewer_sYdT"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8147/Reviewer_sYdT"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for clarifying, this sounds like a great plan."
                    }
                },
                "number": 28,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8147/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700315642477,
                "cdate": 1700315642477,
                "tmdate": 1700315642477,
                "mdate": 1700315642477,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2J2mic3PFu",
                "forum": "m7aPLHwsLr",
                "replyto": "YY6NGuhxzG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8147/Reviewer_sYdT"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8147/Reviewer_sYdT"
                ],
                "content": {
                    "comment": {
                        "value": "I agree that there is value in studying a general method (de-randomized smoothing) in a new domain, and will take this into consideration when reconsidering my score."
                    }
                },
                "number": 29,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8147/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700317061914,
                "cdate": 1700317061914,
                "tmdate": 1700317061914,
                "mdate": 1700317061914,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tyaML0Ay09",
                "forum": "m7aPLHwsLr",
                "replyto": "caYVcPNCUn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8147/Reviewer_sYdT"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8147/Reviewer_sYdT"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks, the revision has addressed most of my concerns. I have decided to increase my score from 3 to 6."
                    }
                },
                "number": 45,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8147/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700636744646,
                "cdate": 1700636744646,
                "tmdate": 1700636744646,
                "mdate": 1700636744646,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Cz8XGzFkKA",
            "forum": "m7aPLHwsLr",
            "replyto": "m7aPLHwsLr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8147/Reviewer_5AC4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8147/Reviewer_5AC4"
            ],
            "content": {
                "summary": {
                    "value": "The author employs the de-randomized smoothing technique to develop a certified defense for malware detection. Furthermore, the author introduces a new dataset named PACE, comprising 15.5K recent benign raw executables from diverse sources. Experimental results validate the effectiveness of their approach."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well written and generally easy to follow.  \n2. A well-structured and clear presentation.\n3. I appreciate the author for providing a novel dataset."
                },
                "weaknesses": {
                    "value": "1.\tThe motivation behind the method somewhat contradicts intuition to a certain extent.\n2.\tThere is a lack of comparison with recent methods for defense adversarial attack."
                },
                "questions": {
                    "value": "1.The proposed \"window ablation\" strategy seems to be applicable only to scenarios where perturbations are clustered together (similar to patch attacks in the CV domain). However, many adversarial attack methods for malware disperse the inserted perturbations across various locations within the software. It remains unclear whether the proposed method would still be effective in such cases.\n\n2.The construction of many malware samples follows a piggyback approach, where the majority of the software consists of benign code, with only a small portion exhibiting malicious behavior. That\u2019s to say most of the ablated sequences will be given a benign label. The algorithm proposed by the author may result in false negatives for such malware samples.\n\n3.It is essential to provide a comparative analysis of the author's proposed defense method against existing adversarial example defense techniques for malware."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8147/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8147/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8147/Reviewer_5AC4"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8147/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698741824122,
            "cdate": 1698741824122,
            "tmdate": 1699637009618,
            "mdate": 1699637009618,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IUaTcXSm5B",
                "forum": "m7aPLHwsLr",
                "replyto": "Cz8XGzFkKA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8147/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8147/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">*\"The proposed \"window ablation\" strategy seems to be applicable only to scenarios where perturbations are clustered together (similar to patch attacks in the CV domain). However, many adversarial attack methods for malware disperse the inserted perturbations across various locations within the software. It remains unclear whether the proposed method would still be effective in such cases.\"*\n>>We really appreciate your concise review. \n>>\n>>Yes, you are right. The proposed method is \u2018theoretically\u2019 for the attacks that can modify or add in a contiguous region. And we also agree that there are many adversarial attacks that do not follow this and can modify/add in multiple regions. So, in this work, we actually evaluated our proposed model DRSM against such attacks, because we believe that \u2013 only \u2018theoretical robustness\u2019 is not enough for a security-critical application like malware detection. In **section 7 \u2018Empirical Robustness Evaluation\u2019**, we considered in total of 9 attacks where 5 of them can modify multiple regions in a file. In **Table 4**, we listed all these attacks with their short description and alignment with our threat model. For example, **Slack append, Header field modification, and Gamma attack** can perturb multiple regions in a malware file, and recently proposed **Disp and IPR** attacks can modify a file on the instruction level and are not limited to a certain region. \n>>\n>>We used these attacks to compare our DRSM with the baseline model \u2018MalConv\u2019 and its more robust variant \u2018MalConv (NonNeg)\u2019. The results are shown in **Figures 4 and 5**."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8147/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700247746028,
                "cdate": 1700247746028,
                "tmdate": 1700247746028,
                "mdate": 1700247746028,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eCVGPCrCWG",
                "forum": "m7aPLHwsLr",
                "replyto": "Cz8XGzFkKA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8147/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8147/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">*\"The construction of many malware samples follows a piggyback approach, where the majority of the software consists of benign code, with only a small portion exhibiting malicious behavior. That\u2019s to say most of the ablated sequences will be given a benign label. The algorithm proposed by the author may result in false negatives for such malware samples.\"*\n>>This is a very good observation, and it is true that most of the contents in a malware file are actually benign. This argument actually aligns with our result too. We found that \u2013 if we increase the number of windows (decreasing the length of ablated sequences), the probability of an ablated sequence getting classified as \u2018benign\u2019 gets higher because a smaller ablated sequence would cover less content, and hence, less/no malicious content. As a result, for the higher number of windows ($n$), DRSM models have less standard accuracy, and we showed this in Table 3 and discussed this in subsection 6.1. For example, **DRSM-4 has 98.18%** standard accuracy, whereas **DRSM-24 achieves 90.24%**. The goal of this paper was to \u2013 find a balance between standard accuracy and robustness."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8147/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700247888517,
                "cdate": 1700247888517,
                "tmdate": 1700247888517,
                "mdate": 1700247888517,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oC5ZknkW0f",
                "forum": "m7aPLHwsLr",
                "replyto": "Cz8XGzFkKA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8147/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8147/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">*\"It is essential to provide a comparative analysis of the author's proposed defense method against existing adversarial example defense techniques for malware.\"*\n>>The existing defenses for adversarial malware can be broadly divided into two \u2013 Non-negative classifier, and Adversarial training. We have already included the former one, **\u2018MalConv (NonNegative)\u2019**, in our work and compared it in terms of standard accuracy, certified accuracy, and empirical robustness in **Table 3 and Figures 4, 5 ( and Appendix A.3)**. We also want to emphasize that \u2013 recent work by Lucas et. al. [1] has already shown that \u2013 in most cases, the adversarially trained models (the second defense) do not provide good robustness against other attacks. Moreover, such defense compromises the standard accuracy too, for example, Lucas et. al. showed training the model adversarially on Kreuk-0.01 degraded the true positive rates to 84.4% ~ 90.1%.  We have discussed this in the second paragraph of **section 2 (Related Work)**. \n>>\n>>[1] Adversarial training for {Raw-Binary} malware classifiers. https://www.usenix.org/conference/usenixsecurity23/presentation/lucas"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8147/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700248429265,
                "cdate": 1700248429265,
                "tmdate": 1700248429265,
                "mdate": 1700248429265,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "D2lYVfEnyr",
            "forum": "m7aPLHwsLr",
            "replyto": "m7aPLHwsLr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8147/Reviewer_4y7b"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8147/Reviewer_4y7b"
            ],
            "content": {
                "summary": {
                    "value": "This work introduces a certified defense called DRSM (De-Randomized Smoothed MalConv) through a redesign of the de-randomized smoothing technique in the context of malware detection.\nMore specifically, they introduce a window ablation scheme that creates a series of ablated sequences by partitioning the input sequence into non-overlapping windows.\nExtensive experimentation involving 9 distinct empirical attacks of various types reveals that the proposed defense demonstrates empirical robustness when faced with a diverse range of attacks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The authors have gathered 15.5K recent benign raw executables from a variety of sources. These files will be released to the public as a dataset named PACE (Publicly Accessible Collection(s) of Executables). This dataset aims to address the shortage of publicly available benign datasets for research in malware detection and to provide future studies with more representative contemporary data."
                },
                "weaknesses": {
                    "value": "* Insufficient theoretical analysis of certified robustness when facing multiple malicious ablated sequences. Specifically, the authors assume that an attacker generates a byte perturbation of size $p$ and can modify a maximum of $\\lceil \\frac{p}{w} \\rceil + 1$ ablated sequences. However, if the attacker simultaneously inserts multiple adversarial code segments at different locations, how will this impact the certified robustness? The authors should offer a more in-depth theoretical analysis of this scenario. Furthermore, the authors should establish the relationship between the window size ($w$) and the resulting certified robustness. For example, does a smaller window size lead to improved certified robustness?\n\n* In my view, there appears to be a contradiction between Figures 1 and 2. As discussed in Section 5, malicious ablated sequences are expected to influence their respective base classifiers, and the predicted winning class should be \"benign.\" However, in Figure 1, the predicted winning class is labeled as \"malware,\". It confuses me. If this work indeed presents a more robust framework that prevents attackers from generating adversarial examples, then the winning class should ideally be \"benign.\" However, if the winning class is consistently benign, it suggests that the proposed framework might miss detecting certain malware instances, creating a contradiction.\n\n* The absence of comparisons with a broader range of real-world antivirus engines accessible via VirusTotal is notable. It would be beneficial, for instance, to determine how many antivirus engines effectively identify the malware and test cases as malicious.\n\n* Another deficiency is the absence of specific details regarding the process of compromising executable files. The authors should provide a more comprehensive explanation of how to generate adversarial examples within the problem-space[Ref-1], which encompasses defining a comprehensive set of constraints on available transformations, preserving semantics, ensuring robustness to preprocessing, and maintaining plausibility.\n\n* The rationale behind the design choice is unclear. Why have the authors chosen MalConv as the baseline classifier? There are numerous alternative models that can serve as the baseline classifier, such as LGBM, RF, and SVM. The authors should consider evaluating their framework with these alternative baseline classifiers.\n\n\nPierazzi, Fabio, et al. \"Intriguing properties of adversarial ml attacks in the problem space.\" 2020 IEEE symposium on security and privacy (SP). IEEE, 2020."
                },
                "questions": {
                    "value": "* If the winning class of Fig. 2 is \"benign\"?\n\n* Does a smaller window size lead to improved certified robustness?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8147/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698840080823,
            "cdate": 1698840080823,
            "tmdate": 1699637009511,
            "mdate": 1699637009511,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IyWRbXfRSx",
                "forum": "m7aPLHwsLr",
                "replyto": "D2lYVfEnyr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8147/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8147/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">*\"Insufficient theoretical analysis of certified robustness when facing multiple malicious ablated sequences. Specifically, the authors assume that an attacker generates a byte perturbation of size p and can modify a maximum of \u2308p/w\u2309+1 ablated sequences. However, if the attacker simultaneously inserts multiple adversarial code segments at different locations, how will this impact the certified robustness? The authors should offer a more in-depth theoretical analysis of this scenario.\"*\n>>Thanks for such a detailed response. We appreciate your effort in reading our paper thoroughly. \n>>\n>>As the de-randomized smoothing approach was borrowed from the computer vision domain, we followed the same threat model where the attacker can add a patch of a specific size (add or modify byte sequence of $p$ size, in our case). In our theoretical study, we wanted to keep it as similar as the CV domain. At the same time, we also understand that the malware domain does not work like that, and your point on \u2013 *\"the attacker simultaneously inserts multiple adversarial code segments at different locations\"* is totally valid and we agree with that. Therefore, we empirically evaluated the robustness of our DRSM approach against such attacks that can insert multiple adversarial code segments **(section 7)**. To be specific, we have included attacks, namely **Slack Apppend, Header Field Modification, Gamma,** where the attacker can include or modify at multiple places, and stronger attacks, namely **Disp, IPR,** where the attacker can even modify on the instruction level.  **Table 4** includes the list of attacks we experimented with in this paper where the column \u2018Threat Model\u2019 indicates their alignment with our threat model. In Figures 4 and 5, we showed the attack success rates for these attacks. Also, we have included how all these attacks work and were implemented in **Appendix A.4**."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8147/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700244895080,
                "cdate": 1700244895080,
                "tmdate": 1700244895080,
                "mdate": 1700244895080,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EZhYrVqXV8",
                "forum": "m7aPLHwsLr",
                "replyto": "D2lYVfEnyr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8147/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8147/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">*\"Furthermore, the authors should establish the relationship between the window size (w) and the resulting certified robustness. For example, does a smaller window size lead to improved certified robustness?\"*\n>>Thanks for asking about the relationship between window size and certified accuracy. Yes, a smaller window size leads to a better certified accuracy. And we discussed this in the **last paragraph of subsection 6.2**. We mentioned that \u2013 *\u201cBy analyzing Table 3, we can see that $n$ has a positive and negative correlation with certified and standard accuracy, respectively. While DRSM-24 provides the highest certified accuracy (53.97%), it has the lowest standard accuracy (90.24%).\u201d*  The Table 3 shows the certified accuracy for the same $\\Delta (=2)$ for each model variant, and notably, DRSM-24 (with the smallest window size) achieves the highest certified accuracy (53.97%) whereas DRSM-4 (with the highest window size) achieves the lowest certified accuracy (12.2%). Additionally, we have added another figure **(Figure 10 in the Appendix A.4)** that shows the certified accuracy in terms of $\\Delta$. From this figure, it is interpretable that smaller window size achieves higher certified accuracy. At the same time, we want to mention that \u2013 a smaller window allows attackers a smaller budget for perturbation. For example, if $\\Delta =2$, in DRSM-4, the attacker can perturb up to 511K bytes, whereas in DRSM-24, it is 82K bytes. So, it might not be fair to compare these models just depending on the $\\Delta$, and hence, we kept the Figure 3 showing accuracy with respect to perturbed bytes in the main text, and put the Figure 10 in the Appendix. We would appreciate your feedback too."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8147/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700245111294,
                "cdate": 1700245111294,
                "tmdate": 1700245111294,
                "mdate": 1700245111294,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WQ2jcIBcWL",
                "forum": "m7aPLHwsLr",
                "replyto": "D2lYVfEnyr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8147/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8147/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">*\"In my view, there appears to be a contradiction between Figures 1 and 2. As discussed in Section 5, malicious ablated sequences are expected to influence their respective base classifiers, and the predicted winning class should be \"benign.\" However, in Figure 1, the predicted winning class is labeled as \"malware,\". It confuses me. If this work indeed presents a more robust framework that prevents attackers from generating adversarial examples, then the winning class should ideally be \"benign.\" However, if the winning class is consistently benign, it suggests that the proposed framework might miss detecting certain malware instances, creating a contradiction.\"*\n>>*\u201cIf this work indeed presents a more robust framework that prevents attackers from generating adversarial examples, then the winning class should ideally be \"benign.\"\u201d* \u2013 we want to mention that \u2013 this work does not prevent attackers from generating adversarial examples. We proposed a framework (DRSM) on top of an already existing classifier that can improve its robustness for adversarial malware; our goal was not to stop malware authors from generating them. \n>>\n>>In Figure 1, we tried to show the fundamental difference between the original base classifier (MalConv) and our method (DRSM) with a toy example in a simple way. Here, for DRSM, the adversarial malware gets ablated into 3 non-overlapping windows (or sequences) and generates 3 different predictions. Since the perturbation impacted only one window (the middle portion in the file written with red font), DRSM gave the wrong prediction (benign) only for that one. But for the rest of the windows (the first and third portion in the file written with black font), DRSM correctly classifies them as \u2018malware\u2019. As a result, \u2018malware\u2019 (2) wins against \u2018benign\u2019 (1) in voting, and DRSM gives the final output as \u2018malware\u2019."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8147/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700245528866,
                "cdate": 1700245528866,
                "tmdate": 1700245528866,
                "mdate": 1700245528866,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WhYKDXrf7v",
                "forum": "m7aPLHwsLr",
                "replyto": "D2lYVfEnyr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8147/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8147/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">*\"The absence of comparisons with a broader range of real-world antivirus engines accessible via VirusTotal is notable. It would be beneficial, for instance, to determine how many antivirus engines effectively identify the malware and test cases as malicious.\"*\n\n>*\"Another deficiency is the absence of specific details regarding the process of compromising executable files. The authors should provide a more comprehensive explanation of how to generate adversarial examples within the problem-space[Ref-1], which encompasses defining a comprehensive set of constraints on available transformations, preserving semantics, ensuring robustness to preprocessing, and maintaining plausibility.\"*\n>>The main focus of this paper was to propose a better defense model for adversarial attacks, not the attacks themselves. Moreover, we evaluated our models against 9 different attacks and there is a page limitation. So, we just kept the gist of the attacks in our main paper. **Table 4** lists all 9 attacks with their short description, and alignment with our threat model and settings. Additionally, in **Appendix A.4**, we included details for all of these attacks and their implementation in this work. Also, all of these attacks have already been proposed in published prior works showing that they do not compromise executable files, and we cited them so that any specific details about them can be retrieved if necessary. And some of these attacks (such as Disp, IPR, GAMMA, etc.) had already been tested against VirsuTotal and were found successful. We also want to mention that -- Disp, IPR attack (Lucas et. al. (2021)) reported that they can evade VirusTotal in 49%-53%, whereas in DRSM, these attacks could evade 42% and 9.50% cases, respectively, even for our weakest model (DRSM-4)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8147/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700245662942,
                "cdate": 1700245662942,
                "tmdate": 1700247007263,
                "mdate": 1700247007263,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "d05ITaVdGq",
                "forum": "m7aPLHwsLr",
                "replyto": "D2lYVfEnyr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8147/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8147/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">*\"The rationale behind the design choice is unclear. Why have the authors chosen MalConv as the baseline classifier? There are numerous alternative models that can serve as the baseline classifier, such as LGBM, RF, and SVM. The authors should consider evaluating their framework with these alternative baseline classifiers.\"*\n\n>>We want to emphasize that CNN-based models like MalConv can take the whole raw bytes as input whereas models like LGBM, RF, or SVM require feature engineering (e.g., byte entropy), and they cannot take raw bytes as input. These features are known to be specifically targeted by adversaries to avoid detection, which makes \"'being able to take the whole raw binary\" attractive. However, this new capability introduces a risk of adversarial attacks too, which motivates us to position our defense for CNN-based models. As MalConv is one of the state-of-the-art CNN based static classifiers, it was a suitable choice to evaluate our framework. However, note that our window-ablation scheme is agnostic of the base classifier as it operates at the input level. As a result, it could be applied to any detector that consumes raw bytes."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8147/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700245960856,
                "cdate": 1700245960856,
                "tmdate": 1700247880609,
                "mdate": 1700247880609,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0ELJB6BEhd",
                "forum": "m7aPLHwsLr",
                "replyto": "D2lYVfEnyr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8147/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8147/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">*\"Is the winning class of Fig. 2 is \"benign\"?\"*\n>>Figure 2 is a generalized Figure where the input file can be anything (malware or benign). The green check stands for a correct prediction, whereas the red cross stands for a wrong prediction. At the end, the bars stand for the number of correct vs wrong predictions.\nFor example, let us assume that the input file is an adversarial malware, and the perturbed region of that malware falls into one of the ablated sequences of DRSM (shown with a red small block in ablated sequences). So, the base classifier misclassifies that sequence (shown with a red cross) but classifies the rest of the sequences correctly (shown with green checks). Hence, the winning class is still \u2018malware\u2019 (shown with the green bar in \u2018count class prediction\u2019). We apologize if the figure is confusing. We have added a small description in the caption. If there is any scope to make the presentation better, let us know. We would love your feedback."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8147/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700246254831,
                "cdate": 1700246254831,
                "tmdate": 1700246254831,
                "mdate": 1700246254831,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]