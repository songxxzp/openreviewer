[
    {
        "title": "Adversarial Supervision Makes Layout-to-Image Diffusion Models Thrive"
    },
    {
        "review": {
            "id": "nTMRRTgkkL",
            "forum": "EJPIzl7mgc",
            "replyto": "EJPIzl7mgc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3622/Reviewer_VHY8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3622/Reviewer_VHY8"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims for a better layout-to-image method. It applies a segmentation-based discriminator (Sushko et al., 2022) to the diffusion generator on the pixel-level alignment between the denoised image and the input layout. In addition, it proposes multistep unrolling, predicting the clean image at multiple timesteps and apply the segmenter-based discriminator. The experiments are on the classical segmentation datasets ADE20K and Cityscapes. The model exhibits comparable pixel level alignment and image fidelity. Different type of L2I synthesis adaptation models and segmenters are tested, to demonstrate its effectiveness."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed method seems straightforward and effective.\n2. The paper contains thorough ablation tests on different settings (different L2I models and segmenters) and different hyperparameters."
                },
                "weaknesses": {
                    "value": "1. As mentioned in the Failure Cases, when editing the attribute of one object, it could affect the other objects as well. It is claimed to be inherited from Stable Diffusion.\n2. Despite thorough ablation tests, the paper does not give any insights on the experiments. The text in the experiments only describes the results instead of analyzing the phenomenon. Thus, I think it shows limited contribution to the community. I suggest to delete the plain description of the results, as we can all see from the tables and figure captions, but add more analysis and insights of why it can work."
                },
                "questions": {
                    "value": "1. When using a frozen segmenter, mIoU is higher and FID is also higher. Why does it happen, any insight?\n2. It is mentioned in the limitations that editing one object may also affect others. However, in Figure 4, it shows better local controllability than ControlNet and FreestyleNet. Any insights why? Because of better pixel alignment? If that's the case, is better mIoU always means better local controllability? For example, can the one trained on frozen segmenter exhibits better local controllability?\n3. Why larger number of unrolling steps is always better? Have you tried any K>9? Do all steps contribute the similar gradient magnitude or some of them is more important? What if we omit some of the earlier steps?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3622/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3622/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3622/Reviewer_VHY8"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3622/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698101665055,
            "cdate": 1698101665055,
            "tmdate": 1700695665182,
            "mdate": 1700695665182,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BoOirwG65l",
                "forum": "EJPIzl7mgc",
                "replyto": "nTMRRTgkkL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3622/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3622/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer VHY8 (1/2)"
                    },
                    "comment": {
                        "value": "We genuinely thank the reviewer for positively assessing our work and providing constructive feedback. We would like to provide detailed answers in the following to further clarify the remaining concerns:\n\n> (W.1) As mentioned in the Failure Cases, when editing the attribute of one object, it could affect the other objects as well. It is claimed to be inherited from Stable Diffusion. \n\nWe believe that the attribute/concept binding issue, i.e., editing the attribute of one object could affect the other objects as well, is an inherited problem from Stable Diffusion (SD), as also been reported in prior SD based text-to-image works [1,2,3,4]. Using words to describe detailed changes at specific locations can be lengthy and inaccurate, therefore realizing such edit via text can be difficult. SD still cannot accomplish the task perfectly, either ignoring the text or mapping the text description to different objects. With the further scaling of the denoising UNet, e.g., SDXL [5], such issue might be mitigated but still exists as mentioned in their limitation section. One hypothesis for the binding issue lies in the pretrained text encoders, which tend to compress information into a fixed number of tokens. Employing inference time optimization [2,3] could resolve the problem to a certain extent. Nevertheless, we believe this is an interesting yet open research question. \n\n[1] Feng, Weixi, et al. \"Training-Free Structured Diffusion Guidance for Compositional Text-to-Image Synthesis.\" ICLR. 2022.\n\n[2] Chefer, Hila, et al. \"Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models.\" ACM Transactions on Graphics (TOG) 42.4 (2023): 1-10.\n\n[3] Li, Yumeng, et al. \"Divide \\& bind your attention for improved generative semantic nursing.\" arXiv preprint arXiv:2307.10864 (2023).\n\n[4] Rassin, Royi, et al. \"Linguistic Binding in Diffusion Models: Enhancing Attribute Correspondence through Attention Map Alignment.\" arXiv preprint arXiv:2306.08877 (2023).\n\n[5] Podell, Dustin, et al. \"Sdxl: Improving latent diffusion models for high-resolution image synthesis.\" arXiv preprint arXiv:2307.01952 (2023).\n\n> (W.2) Despite thorough ablation tests, the paper does not give any insights on the experiments. The text in the experiments only describes the results instead of analyzing the phenomenon. Suggestion to add more analysis and insights of why it can work.  \n\nWe thank the reviewer for pointing this out to help us further improve the manuscript quality. We will reiterate some of the insights presented in the Introduction and Method sections, and connect it better with the experiments. To provide a short summary of the important insights: ALDM improves upon the other methods from the training pipeline: the adversarial supervision explicitly leverages the label map condition; and the unrolling strategy can consider more sampling steps during the learning process, bridging the gap between training and the inference time sampling. The combination of both enables the model to well comply with the condition consistently over a time window, leading to the improvement on the alignment in the final result.  \n\n> (Q.1) When using a frozen segmenter, mIoU is higher and FID is also higher. Why does it happen, any insight?\n\nWhen using a frozen segmenter, the diversity of the generated images is very limited, as shown in Fig 6. in the Appendix. The generator (i.e., the diffusing UNet) tends to learn a mean mode to cheat the discriminator (a frozen segmenter in this case), leading to the mode collapse issue. Such mean class mode is essentially easier for the segmenter to classify, yielding high mIoU. Since the FID metric assesses both visual quality and diversity,  such phenomenon is reflected in the high FID score. We have clarified this better in the revision as well."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3622/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700140062829,
                "cdate": 1700140062829,
                "tmdate": 1700140062829,
                "mdate": 1700140062829,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hdpan3DNMa",
                "forum": "EJPIzl7mgc",
                "replyto": "Qy6bcuPIX4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3622/Reviewer_VHY8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3622/Reviewer_VHY8"
                ],
                "content": {
                    "title": {
                        "value": "Official comment for the authors"
                    },
                    "comment": {
                        "value": "Thanks for the additional experiments and the clarification. Most of my concerns have been addressed (except the analysis of different steps, and writing). Therefore I keep my original rating for weak acceptance, but increase the contribution score."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3622/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700695634277,
                "cdate": 1700695634277,
                "tmdate": 1700695634277,
                "mdate": 1700695634277,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "T1PMBBP8cO",
            "forum": "EJPIzl7mgc",
            "replyto": "EJPIzl7mgc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3622/Reviewer_rjfS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3622/Reviewer_rjfS"
            ],
            "content": {
                "summary": {
                    "value": "The paper \"Adversarial Supervision Makes Layout-to-Image Diffusion Models Thrive\" proposes to augment diffusion models with adversarial methods to achieve a model that is controlable from text and segmentation map layout simultaneously. In particular, the authors propose two additions to fine-tune a Stable Diffusion model: 1) a recent adversarial learning methods incorporating segmentation networks is applied to the outputs of the diffusion network, and 2) the diffusion process is unrolled, and the segmentation network is applied already to intermediate de-noised steps. It is shown in the paper that there's incremental value in both. The method is evaluated on two datasets (ADE20K and Cityscapes), and the proposed method is showns to be superior to several recent baselines (T2I-Adapter, FreestyleNet and ControlNet) qualitatively, as well as quantitatively. Finally, the method is applied on a domain generalization task, and is shown to perform best against the baselines there."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ the paper is well written, intuitive motivations are given, and it is well understandable.\n+ the proposed method is shown to work well. It shows both qualititatively better alignment and quantitative improvements.\n+ it also shows very good results on domain generalization."
                },
                "weaknesses": {
                    "value": "- it's unclear if careful tuning of a frozen segmentation network's impact on the total loss wouldn't be competitive to the proposed adversarial approach. I.e. in Table 3, the frozen UperNet achieves the best mIoU, but much worse FID - consistent with the hypothesis that the impact of the segmentation network is just too strong. Verying the impact of the segmentation loss relative to the diffusion loss while fine-tuning would clarify this.\n- The introduction gave a good intuition for the whole method. However, the combination of diffusion models with adversarial methods remains ad-hoc, and there's no theoretical justification for the validaty of the approach given. It should be either worked out, or at least added for future work that propoer understanding of diffusion-adversarial coupling should be investigated (to make these methods work best together).\n\nSmaller things:\n- Figure 1 lowest right image: this example shows that the method doesn't yet work perfectly. The top part of the truck is a building (see the windows and the 'roof structure'). It should be pointed out in the paper that some failed cases still persist (even if harder to see).\n- In \"Related Work\" you write \"more attention has been devoted to leveraging pretrained knowledge for the L2I task and using diffusion models\"; I think this is an important point, and should already be part of the motivation of the general method in the Introduction."
                },
                "questions": {
                    "value": "- why don't you train a diffusion model from scratch for the task of L2I and T2I simultaneously? Recent segmentation models are very powerful, and can produce the segmentation maps needed to augment the dataset (e.g. LAION-5B). I'd expect that to work best.\n- for the case of a frozen pre-trained segmenter: did you train a diffusion model plus an additional loss for the segmentation model? That is surprising, because I wouldn't have expected the diffusion model to collapse in such case (the same training method/loss is still there)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3622/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698671293681,
            "cdate": 1698671293681,
            "tmdate": 1699636317794,
            "mdate": 1699636317794,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WF0e6iFfWH",
                "forum": "EJPIzl7mgc",
                "replyto": "T1PMBBP8cO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3622/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3622/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer rjfS (1/2)"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer for the overall positive assessment and provided insightful feedback. We have revised the manuscript based on your constructive comments. In what follows, we provide our response to individual questions.\n\n> (Q. 1) Train a diffusion model from scratch for the task of L2I and T2I simultaneously. Recent segmentation models can produce the segmentation maps needed to augment the dataset (e.g. LAION-5B). \n\nWe agree that training a diffusion model for L2I and T2I jointly is a promising idea, and the recent segmentation models such as SAM have demonstrated impressive zero-shot performance, and in many cases can provide reliable label maps for training. Unfortunately, in our lab we have very limited computational resources, thus large-scale training, e.g., 256 A100 GPUs with 150000 hours reported for Stable Diffusion, is far beyond our computing capability. In contrast, fine-tuning only requires 2 A100 GPUs, which is affordable for us. As not having enough resources to train Stable Diffusion from scratch is not an uncommon status for many labs in the world, developing fine-tuning techniques is valuable as well. Nevertheless, we thank the reviewer's valuable suggestion, and we have incorporated this into the newly added \"Future Work\" discussion in Appendix E. \n\n> (Q. 2 & W. 1) (W.1) It's unclear if careful tuning of a frozen segmentation network's impact on the total loss wouldn't be competitive to the proposed adversarial approach. \n(Q. 2) for the case of a frozen pre-trained segmenter: did you train a diffusion model plus an additional loss for the segmentation model? \n\nOur initial assumption was the same, however, we couldn't manage to resolve the issue after tuning the hyperparameters. As shown below, when decreasing the weighting $\\lambda$ of the frozen segmenter loss, the mIoU drops heavily while the FID has minor improvement. The training loss is the same as formulated in Eq. (7), which is a combination of diffusion MSE loss and the segmenter loss. The only difference is for our proposed ALDM the segmenter as a discriminator is also updated with the diffusion model instead of being frozen. In the adversarial game, it is important that the discriminator and generator strike a balance to continuously improve each other. While using a frozen segmenter, the discriminator loses the chance to update itself, and the powerful generator (Stable Diffusion UNet) can freely find a cheating path, leading to the mode collapse issue, as observed in GANs [1]. The mean mode yields limited diversity thus high FID, and it's easier for the segmenter to classify the class mean mode thus the high mIoU. \n\n| Method| mIoU | FID|\n|--|--|--|\n| $$\\lambda=1e-2$$ |50.8|40.2|\n| $$\\lambda=1e-4$$ |39.2|39.8|\n| ALDM (Ours)|36.0|30.2|\n\n[1] Thanh-Tung, Hoang, and Truyen Tran. \"Catastrophic forgetting and mode collapse in GANs.\" 2020 International Joint Conference on Neural Networks (IJCNN). IEEE, 2020."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3622/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700139701381,
                "cdate": 1700139701381,
                "tmdate": 1700139807240,
                "mdate": 1700139807240,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jXpVAnf7We",
                "forum": "EJPIzl7mgc",
                "replyto": "cENhBOrohS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3622/Reviewer_rjfS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3622/Reviewer_rjfS"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the elaborate discussion on the frozen segmentation network, the discussion on the interaction of diffusion and adversarial methods, and adding Appendix E. \nThis settles most of my questions. I'll update my final score after discussion with the other reviewers."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3622/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700587532835,
                "cdate": 1700587532835,
                "tmdate": 1700587532835,
                "mdate": 1700587532835,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OFvPdmvWvh",
            "forum": "EJPIzl7mgc",
            "replyto": "EJPIzl7mgc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3622/Reviewer_d5qA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3622/Reviewer_d5qA"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to adopt a ControlNet architecture for better semantic image synthesis using an adversarial discriminator (as in OASIS) on the per-pixel label maps. Furthermore, a multistep unrolling mechanism is presented so that adversarial supervision takes into account several denoising steps to improve the signal at low noise levels."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper is well-structured and well-written.\n- Ideas and results are presented clearly.\n- Adapting a pre-trained diffusion model for semantic image synthesis is interesting and challenging."
                },
                "weaknesses": {
                    "value": "There are several issues with the presented work.\n\n- The technical contributions of this manuscript are limited. Adversarial supervision on the semantic maps is identical as in OASIS. Multistep unrolling is computationally very expensive (scales linearly and hence can take up x9 longer), and has a small effect on the performance.\n- The results (while improving over the diffusion baselines) are behind OASIS, a GAN from 2020.\n- The motivation to use a strong text-to-image model and adapt it for semantic image synthesis is flawed. The paper motivates it by enabling text-conditioned content and style transfer, but the semantic mask completely specifies the content. Hence, the application is reduced to text-guided style and color transfer. \n- Furthermore, the proposed model does not perform style transfer well. When changing to a snowy scene, the whole image and all objects are resampled. Local editing is also not possible. Instead, the whole image is affected when changing \"a red van\" to \"a burning van\". \n- A simple baseline combining OASIS and a state-of-the-art style transfer model should be considered.\n\nSide remarks:\n- Layout-to-image is usually referred to as a task that transforms a list of bounding boxes and class labels into an image [1,2]. This paper tackles semantic image synthesis where the input is a label mask (each pixel is labelled).\n- The paper states that Stable Diffusion based models do not comply well with layout input, but see [3,4]\n\n[1] Image Synthesis From Reconfigurable Layout and Style, https://arxiv.org/abs/1908.07500\n[2] LayoutDiffusion: Controllable Diffusion Model for Layout-to-image Generation, https://arxiv.org/abs/2303.17189\n[3] SpaText: Spatio-Textual Representation for Controllable Image Generation, https://arxiv.org/abs/2211.14305\n[4] ReCo: Region-Controlled Text-to-Image Generation, https://arxiv.org/abs/2211.15518"
                },
                "questions": {
                    "value": "-"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3622/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3622/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3622/Reviewer_d5qA"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3622/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698671367594,
            "cdate": 1698671367594,
            "tmdate": 1700654424194,
            "mdate": 1700654424194,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wiDmYlCt5c",
                "forum": "EJPIzl7mgc",
                "replyto": "OFvPdmvWvh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3622/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3622/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer d5qA  (1/2)"
                    },
                    "comment": {
                        "value": "We sincerely thank Reviewer d5qa's acknowledging the paper is well-written, the topic is interesting and challenging. We believe there is a misunderstanding on the motivation of our approach, and thus would like to reiterate the motivation, our technical contributions and placement of our approach in the scope of the semantic image synthesis work, both diffusion-based and GAN-based. We believe we point out an important issue in the current diffusion model pipeline and the proposed training strategy has proven its effectiveness on different diffusion model based approaches.  We next would like to address individual concerns: \n\n> (1)  The technical contributions of this manuscript are limited. \n\nTo the best of our knowledge, our adversarial supervision with the multistep unrolling strategy has not been proposed before for improving layout-to-image diffusion models. Our approach is indeed inspired by OASIS, which we have adequately acknowledged in the paper, i.e., cited it in multiple sections (introduction, related work, method, etc.) in the main paper.  However, incorporating the broad concept of discriminator loss into a completely different type of generative model, i.e., diffusion model, is by no means trivial. \n\nThe traditional diffusion model suffers from poor alignment due to the suboptimal training objective without explicit supervision based on the label map, and its sampling characteristics also make consistent adherence to the conditional layout with time difficult. Little attention has been paid to improve the original diffusion reconstruction loss. Therefore, the proposed adversarial supervision and multistep unrolling is a novel training design for diffusion models, to explicitly encourage consistent alignment to the label map over a time window, closing the gap with inference time sampling. **We believe our work focuses on the important yet underexplored aspects of diffusion models, i.e., the training strategy**, and we demonstrate the effectiveness of our proposal on different recent diffusion model-based methods, e.g., OFT (NeurIPS'23), T2I-Adapter (ArXiv'23-Mar) and SOTA ControlNet (ICCV'23 Best Paper) (see Table 1 and below). Especially, the multistep unrolling is  tailored for diffusion models, considering its sampling characteristic, which is significantly different from GANs.\n\n| Method| Cityscapes mIoU | ADE20K mIoU|\n|--|--|--|\n| OFT |48.9|24.1|\n| OFT + Ours |58.8|31.8|\n| T2I-Adapter|37.1|24.0|\n| T2I-Adapter + Ours|50.1|29.1|\n| ControlNet |55.2|30.4|\n| ControlNet + Ours |63.9|36.0|\n\nRegarding the computing overhead, as mentioned in the \"Implementation details\", we only apply the unrolling every 8 optimization steps, such that the additional overhead is manageable. It is important to mention that multistep unrolling is a training strategy that can largely improve the alignment with the label map, coming at no additional cost at inference time, which is more important for most applications.\n\n> (2) The results (while improving over the diffusion baselines) are behind OASIS, a GAN from 2020.\n\nOur goal is to improve text-to-image (T2I) diffusion models for semantic image synthesis.  We agree with the reviewer that this class of generative models underperforms GANs in terms of alignment, which we have shown in Table 8 in the Appendix. However, there are many benefits of using large-scale T2I diffusion models for this task, such as powerful text controllability. Our ALDM can naturally synthesize novel samples, which can be used for downstream applications such as improved domain generalization (see Table 4 and Fig. 5). In contrast, GAN alone can only synthesize samples resembling the trained domain. We believe our proposed training strategies can inspire the community and can be combined with advancing future diffusion model architecture designs, which are orthogonal to our work, leading to better results in the future."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3622/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700139091735,
                "cdate": 1700139091735,
                "tmdate": 1700139444245,
                "mdate": 1700139444245,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CAJtPaP0rl",
                "forum": "EJPIzl7mgc",
                "replyto": "tZx0RWKPmR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3622/Reviewer_d5qA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3622/Reviewer_d5qA"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Dear authors, thank you for the detailed response, additional analysis, and patience. I have read the updated version, replies, and other reviews. My main concerns have been addressed thus I have decided to raise my score and suggest acceptance. The work presents a valuable contribution to the community in improving the controllability of existing large-scale t2i models."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3622/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700654369376,
                "cdate": 1700654369376,
                "tmdate": 1700654369376,
                "mdate": 1700654369376,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "UJ7CP4jxwW",
            "forum": "EJPIzl7mgc",
            "replyto": "EJPIzl7mgc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3622/Reviewer_4k64"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3622/Reviewer_4k64"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to embed adversarial supervision into the training of a diffusion model conditioned on layout. By introducing adversarial supervision and the multi-step rolling strategy, the framework can get strong results, better than baseline methods like control net which does not explicitly use adversarial supervision. It also shows that the generated samples can boost the domain generalization for semantic segmentation tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is well-written and easy to understand, the proposed method is simple but effective\n- Adversarial supervision is not new though, it is the first time to be used in the context of diffusion model\n- The experimental results are strong and are better than the baseline methods."
                },
                "weaknesses": {
                    "value": "- The qualitative results seem to be much better than baseline methods, are they cherry-picked? A non-cherry picked results can better show that the proposed methods largely exceed the current baselines.\n- What is the difference between Control-Net + Adv Supervision and multi-step rolling in Table 1 compared with ALDM?\n- Can you show some outputs of the discriminator? Since it works in the latent space, we need to be more careful about what is happening inside."
                },
                "questions": {
                    "value": "In the `weaknesses`"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3622/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3622/Reviewer_4k64",
                        "ICLR.cc/2024/Conference/Submission3622/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3622/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698730714737,
            "cdate": 1698730714737,
            "tmdate": 1700164208734,
            "mdate": 1700164208734,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Q3YQAthwZO",
                "forum": "EJPIzl7mgc",
                "replyto": "UJ7CP4jxwW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3622/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3622/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the overall positive assessment, especially acknowledging the simplicity and effectiveness of our method. In what follows, we address the individual concerns in detail:\n\n> (1) The qualitative results seem to be much better than baseline methods, are they cherry-picked?\n\nWe selected representative examples to illustrate the problems in existing methods in the teaser figure, i.e., Fig 1. For other figures, we did not curate the results. We thank the reviewer to help us further improving the manuscript quality, and we have provided more comparisons in the Appendix. Besides, the quality comparison of random samples can also be seen in the extensive quantitative evaluation (e.g., Table 1), in which our method outperforms the other competitors. \n\n> (2) What is the difference between Control-Net + Adv Supervision and multi-step rolling in Table 1 compared with ALDM?\n\nThere is no difference between Control-Net + Adv Supervision + multi-step rolling and ALDM in other tables. In general, ALDM (adversarial supervision plus multi-step unrolling) can be seen as an improved training pipeline for L2I diffusion models, which can be combined with different adaptation architecture designs as shown in Table 1. In other tables, we use ControlNet as the default model combined with adversarial supervision and multistep unrolling, to represent ALDM.  We thank the reviewer for pointing this out, and we have clarified accordingly in the revision (on Page 7 after Table 1). \n\n> (3) Can you show some outputs of the discriminator?\n\nWe have added some discriminator predictions in Fig. 15. The discriminator takes the input of the decoder, which actually turns the latents into the pixel images, as mentioned in the \"Implementation details\". From Fig. 15 the discriminator can categorize some regions as \"fake\" class (in black), meanwhile, it is also fooled by some areas, where the discriminator produces reasonable segmentation predictions matched with ground truth labels. Therefore, the discriminator can provide useful feedback to the generator (diffusion model) that it should produce realistic results meanwhile complying with the given label map."
                    },
                    "title": {
                        "value": "Response to Reviewer 4k64"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3622/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700138436769,
                "cdate": 1700138436769,
                "tmdate": 1700138728555,
                "mdate": 1700138728555,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3TwhRVsQ7t",
                "forum": "EJPIzl7mgc",
                "replyto": "Q3YQAthwZO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3622/Reviewer_4k64"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3622/Reviewer_4k64"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the reply"
                    },
                    "comment": {
                        "value": "The response answered most of my questions, I will keep and score and raise up my confidence. I will also further decide my final score by discussing it with other reviewers."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3622/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700164185753,
                "cdate": 1700164185753,
                "tmdate": 1700164185753,
                "mdate": 1700164185753,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "z8bL8auo70",
                "forum": "EJPIzl7mgc",
                "replyto": "UJ7CP4jxwW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3622/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3622/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the reviewer's prompt reply"
                    },
                    "comment": {
                        "value": "We genuinely thank the reviewer for the prompt response. We are glad that our answers have improved your confidence that our work is above the acceptance bar. \n\nPlease let us know if there are any new concerns we could address to further improve the score during the subsequent discussion. We remain committed to addressing any further questions to ensure the continued improvement of our manuscript."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3622/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700211565332,
                "cdate": 1700211565332,
                "tmdate": 1700211592018,
                "mdate": 1700211592018,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]