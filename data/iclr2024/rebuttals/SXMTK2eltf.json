[
    {
        "title": "GPT-Driver: Learning to Drive with GPT"
    },
    {
        "review": {
            "id": "qV0neVjEi7",
            "forum": "SXMTK2eltf",
            "replyto": "SXMTK2eltf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4601/Reviewer_PY8W"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4601/Reviewer_PY8W"
            ],
            "content": {
                "summary": {
                    "value": "The author proposed a scheme utilizing ChatGPT for motion planning, where perception results (or ground truth) are used as inputs. With meticulous prompt design and finetuning through the API, the approach achieved commendable open-loop performance on the large-scale Nuscenes dataset. Additionally, experiments were designed to demonstrate the large language model's generalizability (few-shot) and interpretability (reasoning) for motion planning tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The overall text flows smoothly and is easy to understand.\n2. Seeing the large language model demonstrate impressive performance and few-shot capabilities in such a simple manner is astonishing. This can better motivate people to continue validating the exploration of large language models in the direction of motion planning.\n3. The author will open-source the code, and since the experiment is relatively simple, I believe there is a high probability that the experiment can be reproduced. The confidence level in the experiment's results is very high."
                },
                "weaknesses": {
                    "value": "1. While I personally appreciate the simplicity and effectiveness of this research, I do not believe it possesses sufficient novelty to be a paper for ICLR. The article simply utilizes the ChatGPT API for finetuning, which can be regarded as an application experiment report on motion planning using ChatGPT. Since it does not introduce any new modules or methodologies, I think its actual contribution to the field is quite limited.\n2. Motion planning ultimately needs to be validated in a closed-loop system, as the results from open-loop and closed-loop scenarios are not always aligned, as highlighted in [1]. In cases where perception results (or ground truth) are directly used as input, it is easy to integrate into a closed-loop system, and there are numerous readily available public datasets for closed-loop testing, such as Nuplan, Waymo Motion Dataset, MetaDrive, etc. If closed-loop results could be obtained, I believe it would significantly enhance the credibility and contribution of the paper.\n3. The descriptions in the RELATED WORKS section of the article are somewhat inaccurate. Dauner et al. (2023)[1] is actually a rule-based method.\n\n[1] Dauner, D., Hallgarten, M., Geiger, A., & Chitta, K. (2023). Parting with Misconceptions about Learning-based Vehicle Motion Planning. arXiv preprint arXiv:2306.07962."
                },
                "questions": {
                    "value": "1. I hope to see closed-loop results; please refer to the \"Weaknesses\" section for more details."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics review needed."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4601/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4601/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4601/Reviewer_PY8W"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4601/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698693149729,
            "cdate": 1698693149729,
            "tmdate": 1699636438999,
            "mdate": 1699636438999,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NsbVu5M0Ad",
                "forum": "SXMTK2eltf",
                "replyto": "qV0neVjEi7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4601/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4601/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the reviewer's feedback"
                    },
                    "comment": {
                        "value": "Thanks for the reviewer's feedback!\n\n**While I personally appreciate the simplicity and effectiveness of this research, I do not believe it possesses sufficient novelty to be a paper for ICLR. The article simply utilizes the ChatGPT API for finetuning, which can be regarded as an application experiment report on motion planning using ChatGPT. Since it does not introduce any new modules or methodologies, I think its actual contribution to the field is quite limited.**\n\nWe believe our method has sufficient novelty to be a paper for ICLR for the following reasons:\n\n(1) Our proposed motion planning as language modeling is a fundamental change to the current regression-based planning methods. As we mentioned in Section 3.2, motion planning as language modeling is **essentially transforming the typical regression problem into a coarse-to-fine region classification problem**, which overcomes the weaknesses of regression (e.g. sensitive to scale variation). Hence we argue that we did introduce new methodologies in our paper.\n\n(2) From the architecture's perspective, our approach **transforms the conventional MLP-based planner by introducing fine-tuned LLM as a motion planner**, which is a fundamental architectural change. Compared to the MLP-based planner, our approach can benefit from the LLM and demonstrates superior generalization ability (see the few-shot learning results) and interpretability, thus enabling a more robust and transparent decision-making process. \n\n(3) Our observation that **LLMs can perform low-level motion planning and estimate very accurate numerical coordinates** is also unique. While a lot of papers are working on leveraging LLMs for language-based task planning, we are the first to demonstrate that LLMs can also perform motion planning and accurate numerical predictions. We believe this observation also brings new insights and merits to the research community.\n\nOverall, the proposed method is simple and can be implemented in a few lines of code. Although the idea is intuitive, the learning objective, architecture, and observations are unique and completely different from existing approaches. Hence, we believe our method has sufficient technical contributions to be accepted to ICLR.\n\n**Motion planning ultimately needs to be validated in a closed-loop system, as the results from open-loop and closed-loop scenarios are not always aligned, as highlighted in [1]. In cases where perception results (or ground truth) are directly used as input, it is easy to integrate into a closed-loop system, and there are numerous readily available public datasets for closed-loop testing, such as Nuplan, Waymo Motion Dataset, MetaDrive, etc. If closed-loop results could be obtained, I believe it would significantly enhance the credibility and contribution of the paper.**\n\nWe appreciate the review's suggestion and we also agree with this point. We're now working on adapting GPT-Driver to close-loop evaluation on nuPlan. Since nuPlan is much larger than nuScenes, we are unable to get the close-loop performance in this short discussion phase. We will update the paper once we have finished the experiments.\n\n**The descriptions in the RELATED WORKS section of the article are somewhat inaccurate. Dauner et al. (2023)[1] is actually a rule-based method.**\n\nFrom our understanding, Dauner et al. (2023)[1] is a hybrid approach that leverages rule-based IDM to generate an initial trajectory and then applies a learning-based neural network to refine the trajectory. For better clarification, we have moved this literature to the rule-based method according to the review's feedback."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700612368216,
                "cdate": 1700612368216,
                "tmdate": 1700612416689,
                "mdate": 1700612416689,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rR1CrA0Z9M",
                "forum": "SXMTK2eltf",
                "replyto": "qV0neVjEi7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4601/Reviewer_PY8W"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4601/Reviewer_PY8W"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response"
                    },
                    "comment": {
                        "value": "Thank you for your comprehensive response to my review. I appreciate the effort you've made to address the concerns raised.\n\n\nWhile I acknowledge the novelty and simplicity of your approach in using ChatGPT for motion planning, I feel that the depth of the research is somewhat limited. The primary method seems to hinge on integrating ChatGPT API for in-context learning and fine-tuning. Although this is an innovative use of ChatGPT, I believe that for a submission to ICLR, more in-depth analysis and exploration are required to truly advance the field.\n\nHere are some directions that might add depth to your work:\n\n* Algorithmic Innovations: Beyond using the API, consider developing algorithmic improvements or adaptations specific to motion planning.\n* In-Depth Analysis: Explore the limitations and strengths of using LLMs in this context in more detail. How does the LLM handle edge cases or particularly complex driving scenarios? (i.e errors from upstream perception module or complex scenarios.)\n* Comparative Studies: It might be beneficial to compare the performance of your approach against traditional motion planning algorithms under various conditions.\n\n\nAdditionally, I strongly agree that results from closed-loop scenarios, especially on platforms like nuPlan, would be more indicative of the practical utility of your method. I am looking forward to seeing the closed-loop performance scores on nuPlan, as these would significantly enhance the credibility and real-world applicability of your research.\n\n\nOnce again, thank you for considering my feedback, and I am eager to see how this work evolves with these additional aspects."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700688333030,
                "cdate": 1700688333030,
                "tmdate": 1700688363668,
                "mdate": 1700688363668,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "u7x8X0IPRD",
                "forum": "SXMTK2eltf",
                "replyto": "qV0neVjEi7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4601/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4601/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your advice"
                    },
                    "comment": {
                        "value": "Thanks for your feedback. We sincerely appreciate your advice and will incorporate a more in-depth analysis into our paper.\n\nWe still want to emphasize again the impact of our paper: \n\n**Algorithmic Innovation:** It is not merely an innovative use of ChatGPT, but also sheds light on the potential of LLMs to perform accurate numerical reasoning, which has broader impacts on not only motion planning in autonomous driving but also planning and controls for general robotics. We believe the findings in this paper are very significant in autonomous driving and robotics, in the LLM era. \n\n**In-Depth Analysis:** We provided few-shot learning ablation study in the main text and that is another evidence that shows LLMs exhibits strong generalization to little training data. In addition, we have a comparison of using UniAD perception and/or ground-truth perception, and results (Table 1) indicate that our method is robust under imperfect perception outputs. \n\n**Comparative Studies:** We compare to many learning-based motion planning baselines (see Table 1). Due to lack of implementations of traditional motion planning methods, we can't compare to them. Due to time limit and an intensive CVPR submission, we didn't have enough time to re-implement traditional motion planning algorithms.\n\nOnce again, we thank the reviewer for the fruitful discussions. We are happy to address any concerns you still have during the discussion period. If the responses are sufficient, we kindly ask that you consider raising your score."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700691576170,
                "cdate": 1700691576170,
                "tmdate": 1700729633692,
                "mdate": 1700729633692,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ytnGs8ICKy",
            "forum": "SXMTK2eltf",
            "replyto": "SXMTK2eltf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4601/Reviewer_1ZwR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4601/Reviewer_1ZwR"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose some techniques for using ChatGPT3.5 to generate driving trajectories (as a list of coordinates) from a language representation of the world/ego state. The approach outperforms end-to-end learning-based approaches on a computer vision driving benchmark."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- How to best utilize LLMs for autonomous vehicles is an interesting and timely question that I think is of great interest to the community.\n- That the authors achieved such a huge improvement with fine tuning is a surprising insight (going from worst to best results), but potentially useful if it checks out."
                },
                "weaknesses": {
                    "value": "- The benchmark comparison seems a bit apples to oranges. You compare against end-to-end computer vision approaches from CVPR, but your approach assumes the object detections are given and only solves the planning part. Even if you use the detections from a competing CV method, it is unclear to me how strong this result is as planning is perhaps not the main focus of their approach. It would have been good to have some conventional planning stack as baseline as well.\n- Your portrayal of related work outside of CV/learning seems weak/dated with the most recently cited planning paper being from 2018. I quite frequently see conventional planning/optimization papers for autonomous driving at other venues (robotics, AI...). Check some recent surveys, I include one which does not use your terminology of \"rule-based\" for these either (not sure what you mean by that). [1] S. Teng et al., \"Motion Planning for Autonomous Driving: The State of the Art and Future Perspectives,\" in IEEE Transactions on Intelligent Vehicles, vol. 8, no. 6, pp. 3692-3711, June 2023, doi: 10.1109/TIV.2023.3274536.\n- This really is \"GPT\"-driver, it just uses the web APIs for ChatGPT for prompting and fine tuning. GPT is state of the art (at least if you had used 4.0) so is somewhat defensible, but it would have been interesting to see how this generalized across other LLMs.\n\nMinor questionable claims or presentation issues:\n\n- You write \"Albeit simple, these approaches attempt to simultaneously regress waypoints across different scales, e.g. coordinate values ranging from 0 to over 50, which generally results in imprecise coordinate estimations of the more distant waypoints\": This seems like it would be fixed by a simple rescaling, is this really a fundamental problem with IL approaches? This also does not mention RL-based approaches (see [1]) \n- Sec 3.2:  Does all of the IL approaches really use absolute value loss, that seems oddly specific? The discussion of how a number is encoded as a text string seems pretty obvious/trivial.\n- \"It is worth noting that these state-of-the-art planners heavily rely on multiple heterogeneous observations such as detections, predictions, occupancy grids, and maps, which makes their systems intricate and time consuming.\" You rely on their detections (maybe predictions, unclear) so this seems at least half misleading."
                },
                "questions": {
                    "value": "- What data did you use for fine tuning vs evaluation? I am not very familiar with this particular benchmark, but your trajectory prediction errors are surprisingly low. The optimal trajectory for driving is in reality sometimes ambigious, possibly multi-modal and an open research problem, so it seems a bit suspicious that you can get centimeter precision on trajectory prediction. \n- Can you clarify what prompts you used in your three-stage approach for training. Fig 3. seems to only include the prompt for the final stage?\n- Isn't your information about the obstacles both incomplete and technically cheating (acausal) in the simple prompting baseline, since you do not include obstacle velocities, but seemingly where they will be in the future? Care to comment on what this description really means, where they will stop / be x seconds? in the future?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4601/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4601/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4601/Reviewer_1ZwR"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4601/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698784586359,
            "cdate": 1698784586359,
            "tmdate": 1701046775981,
            "mdate": 1701046775981,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zyx0dZg4vN",
                "forum": "SXMTK2eltf",
                "replyto": "ytnGs8ICKy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4601/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4601/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the reviewer's feedback"
                    },
                    "comment": {
                        "value": "Thanks for the reviewer's feedback!\n\n**The benchmark comparison seems a bit apples to oranges. You compare against end-to-end computer vision approaches from CVPR, but your approach assumes the object detections are given and only solves the planning part. Even if you use the detections from a competing CV method, it is unclear to me how strong this result is as planning is perhaps not the main focus of their approach. It would have been good to have some conventional planning stack as baseline as well.**\n\nThe benchmark comparison is **clearly not apples to oranges** for the following reasons: \n\n(1) We are comparing against end-to-end driving approaches in CVPR mainly because these papers are the state-of-art approaches on the nuScenes motion planning benchmark. \n\n(2) Our approach assumes the object detections are given because our approach focuses on motion planning. As we mentioned in the experiments, we also leverage the same detection outputs as UniAD. Hence from the perspective of motion planners, our approach and UniAD have the same detection inputs, which is a fair comparison. \n\n(3) Planning is the main focus and the ultimate optimization goal of these end-to-end driving methods. The title of UniAD is \"Planning-oriented Autonomous Driving\", and all the detection and prediction modules are optimized for better planning. Hence, it is clear that motion planners in these baseline methods are strong enough for comparison. Our comparison is completely fair with these baselines.\n\n(4) We agree with the point that it's good to have conventional planning baselines. We didn't compare with conventional planning mainly because there are no published convention planning methods on the nuScenes motion planning benchmark. We will try reproducing some approaches on the nuScenes benchmark in the future.\n\n**Your portrayal of related work outside of CV/learning seems weak/dated with the most recently cited planning paper being from 2018. I quite frequently see conventional planning/optimization papers for autonomous driving at other venues (robotics, AI...). Check some recent surveys, I include one which does not use your terminology of \"rule-based\" for these either (not sure what you mean by that). [1] S. Teng et al., \"Motion Planning for Autonomous Driving: The State of the Art and Future Perspectives,\" in IEEE Transactions on Intelligent Vehicles, vol. 8, no. 6, pp. 3692-3711, June 2023, doi: 10.1109/TIV.2023.3274536.**\n\nLearning-based and rule-based planning are two mainstream motion planning approaches in the context of autonomous driving. In the survey paper \"Motion Planning for Autonomous Driving: The State of the Art and Future Perspectives\" you have provided, they focus on introducing planning methods based on imitation learning and reinforcement learning, which has already been covered in this paper. For other optimization-based planning approaches, we believe you're referring to control methods, which aim to output more low-level control signals to trace the trajectory and avoid collisions. Optimization-based control is actually a downstream task to motion planning and is out of the scope of this paper.\n\n**This really is \"GPT\"-driver, it just uses the web APIs for ChatGPT for prompting and fine tuning. GPT is state of the art (at least if you had used 4.0) so is somewhat defensible, but it would have been interesting to see how this generalized across other LLMs.**\n\nWe agree with this point. We are working on fine-tuning llama 2 for motion planning, and we will update the results soon.\n\n**You write \"Albeit simple, these approaches attempt to simultaneously regress waypoints across different scales, e.g. coordinate values ranging from 0 to over 50, which generally results in imprecise coordinate estimations of the more distant waypoints\": This seems like it would be fixed by a simple rescaling, is this really a fundamental problem with IL approaches? This also does not mention RL-based approaches (see [1])**\n\nRescaling mitigates the effects but the errors can be amplified when you rescale the regression outputs back to their real values. Hence it is an innate weakness of leveraging regression to resolve the imitation learning problem. \n\n**Sec 3.2: Does all of the IL approaches really use absolute value loss, that seems oddly specific? The discussion of how a number is encoded as a text string seems pretty obvious/trivial.**\n\nState-of-the-art methods (e.g., UniAD, ST-P3) on nuScenes rely on the regression of real-world coordinates. Our discussion reveals the essential difference between regression and language modeling in imitation learning for motion planning, and our language modeling approach is simple to implement and demonstrates promising results compared to those regression-based methods, which we believe is a major contribution and not trivial."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700607606457,
                "cdate": 1700607606457,
                "tmdate": 1700607606457,
                "mdate": 1700607606457,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tUKx7MDPD2",
                "forum": "SXMTK2eltf",
                "replyto": "ytnGs8ICKy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4601/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4601/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Cont'd"
                    },
                    "comment": {
                        "value": "**\"It is worth noting that these state-of-the-art planners heavily rely on multiple heterogeneous observations such as detections, predictions, occupancy grids, and maps, which makes their systems intricate and time consuming.\" You rely on their detections (maybe predictions, unclear) so this seems at least half misleading.**\n\nThese state-of-the-art planners, such as UniAD, take compact maps (H xW grids) and occupancy (H x W x T grids), in addition to perception and prediction, as inputs to their motion planner. Extracting those compact maps and occupancy grids makes their system intricate and time-consuming. By contrast, our approach only leverages text-based perception and prediction (instead of compact maps and occupancy grids) as inputs to GPT-Driver, which is more lightweight compared to these methods. We have revised this part for better clarification.\n\n**What data did you use for fine-tuning vs evaluation? I am not very familiar with this particular benchmark, but your trajectory prediction errors are surprisingly low. The optimal trajectory for driving is in reality sometimes ambiguous, possibly multi-modal and an open research problem, so it seems a bit suspicious that you can get centimeter precision on trajectory prediction.**\n\nAs we mentioned in Section 4.1, we leverage the training set of nuScenes for fine-tuning our GPT-Driver, and the validation set for evaluating our approach. This is a commonly adopted training and evaluation protocol on nuScenes and ensures a fair comparison with the baselines on the nuScenes dataset. Trajectory prediction errors are low compared to the baselines, demonstrating the effectiveness of our approach. Although trajectory prediction can be multi-modal, on the nuScenes benchmark we focus on the most likely mode/trajectory and computing the errors w.r.t human driving trajectory. \n\nThe results are fully validated on widely used benchmarks and we have followed the same training and evaluation protocol to ensure a fair comparison with baselines. We argue that the reviewer's current suspicion is not based on facts and thus not convincing. \n\n**Can you clarify what prompts you used in your three-stage approach for training. Fig 3. seems to only include the prompt for the final stage?**\n\nWe only fine-tune the GPT-Driver **once**. The prompting-reasoning-finetuning strategy means that we design proper prompts as inputs to the GPT-Driver, we enumerate the reasoning process as one of the learning targets, and we fine-tune the LLM using the prompts as inputs and the reasoning target as one of the outputs. Hence, Figure 2 and Figure 3 already include all the inputs and outputs to GPT-Driver during fine-tuning.\n\n**Isn't your information about the obstacles both incomplete and technically cheating (acausal) in the simple prompting baseline, since you do not include obstacle velocities, but seemingly where they will be in the future? Care to comment on what this description really means, where they will stop / be x seconds? in the future?**\n\nClearly, the information about obstacles **is not technically cheating**. As we mentioned, we transform **both detection and prediction** results into text descriptions. This means we describe not only the current location of the object but also the future moving trajectory (in 6 waypoint coordinates) of this object. In this paper, we simplify the prediction by describing the location where the object ends up in the 3rd second. Prediction, or motion forecasting, is a commonly adopted sub-task in the context of autonomous driving, Most state-of-the-art driving systems rely on a perception-prediction-planning paradigm where detection and prediction results are leveraged as inputs to the motion planners. Here we adopted the detection and prediction results from the state-of-the-art UniAD to ensure a fair comparison with this baseline approach."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700607786679,
                "cdate": 1700607786679,
                "tmdate": 1700609911350,
                "mdate": 1700609911350,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zx6XpoeQtp",
                "forum": "SXMTK2eltf",
                "replyto": "tUKx7MDPD2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4601/Reviewer_1ZwR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4601/Reviewer_1ZwR"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "I thank the authors for the clarifications which resolved some concerns. \n\nHowever, some concerns remain:\n\n**About the credibility of the results:**\nI still think it is a fair point to ask why you are proposing a pure planning approach but only evaluating it on a benchmark for end-to-end approaches, without comparing to any conventional planning approach. Being SOTA on nuScenes is not necessarily the same thing as being the best planning approach. While actual SOTA in AD is a bit murky due to private sector R&D, the major players at least started from a conventional, modular autonomy stack and I suspect there is still a lot of that in there. The public comment about the recent issues discovered with nuScenes benchmarking reinforces my concern that this is not a mature benchmark for planning, as is the question of the actual L2 metric used. I will await replies on these points before making my final decision.\n\n**Minor: About incomplete/acausal (\"cheating\") inputs:** \nThank you for the clarification that the \"moving to (x, y)\" input actually is from a prediction module and not GT. I still think this is incomplete data as you never seem to tell the planner exactly how many seconds into the future the x and y in \"moving to (x,y) represents (i.e. you cannot recover the object velocity without that). However, this can only make your results worse so it's not a big issue for the review.\n\nYour prompt just says: \n\"Inputs\n1. Perception & Prediction: Info about surrounding objects and their predicted movements.\"\n\n**Minor: Regarding AD related work and terminology:** (minor)\nYou seem to be right about the \"rule-based\" being accepted terminology for conventional motion planning approaches in the AD community, my bad. I still think the related work section for these approaches is rather weak as it ends in 2018 but as you are focusing on end-to-end approaches it is perhaps good enough. For future reference, even though the survey I posted focused more on learning-based approaches, they do provide a decent overview of local planning in Sec II. Optimization-based does not just mean control either, to consider vehicle kinematics and dynamics in a motion planning problem you often formulate it as an OCP and approximate it with e.g. lattice (graph) or tree planners using motion primitives.\n\nBefore adjusting my score I will await the outcome of the other discussions, and in particular your response to the issues raised in the public comment."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700678091435,
                "cdate": 1700678091435,
                "tmdate": 1700678091435,
                "mdate": 1700678091435,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ay67JsiudS",
                "forum": "SXMTK2eltf",
                "replyto": "ytnGs8ICKy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4601/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4601/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your feedback"
                    },
                    "comment": {
                        "value": "**I still think it is a fair point to ask why you are proposing a pure planning approach but only evaluating it on a benchmark for end-to-end approaches, without comparing it to any conventional planning approach.**\n\nnuScenes is a commonly used **motion planning** benchmark where numerous works have been using this benchmark to evaluate the planning quality of their approach. Again, we'd like to emphasize that we are comparing with the motion planners in these end-to-end driving approaches, instead of directly comparing with end-to-end approaches. We don't compare with the conventional planning approaches because there is no such method implemented or evaluated on nuScenes, but we will try to reproduce some conventional approaches according to your advice.\n\n**The major players at least started from a conventional, modular autonomy stack and I suspect there is still a lot of that in there**\n\nAs far as we know, nuScenes is one of the most popular benchmarks for modular autonomy stack (detection, prediction, planning), and we are actually starting from this modular autonomy stack (but not conventional because we're focusing on learning-based approaches). \n\n**The public comment about the recent issues discovered with nuScenes benchmarking reinforces my concern that this is not a mature benchmark for planning, as is the question of the actual L2 metric used.**\n\nWe will update the results for all metrics very soon. The differences in L2 calculation do not affect the conclusion though. Again, as we mentioned, nuScenes is a mature benchmark for open-loop evaluation of motion planning, which is agreed by most reviewers.\n\n**Minor: About incomplete/acausal (\"cheating\") inputs:**\n\nFirst, it is not an acausal (\"cheating\") input. As we mentioned in the response, we're taking the prediction module's outputs as inputs to the planner, which is a common practice in most existing works. Second, it is not an incomplete input, as we have already included the start location and the end location of an object in a 3-second prediction horizon (see prompts in Figure 2 for context).\n\n**Regarding AD related work and terminology**\n\nWe have included the most recent rule-based work (Dauner et al. 2023) in our paper. We have added the optimization-based approaches you mentioned in the paper. \n\nWe sincerely thank you for providing feedback to help us make this submission stronger. Please let us know if our responses address your concerns. We are more than happy to address any further questions during the discussion period. If the responses are sufficient, we kindly ask that you consider raising your score. Thank you so much! \n\nDauner, D., Hallgarten, M., Geiger, A., & Chitta, K. (2023). Parting with Misconceptions about Learning-based Vehicle Motion Planning. arXiv preprint arXiv:2306.07962."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700681320049,
                "cdate": 1700681320049,
                "tmdate": 1700681817345,
                "mdate": 1700681817345,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MmpuJA5iOt",
            "forum": "SXMTK2eltf",
            "replyto": "SXMTK2eltf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4601/Reviewer_FVv6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4601/Reviewer_FVv6"
            ],
            "content": {
                "summary": {
                    "value": "This work studied the application of LLM in motion planning for autonomous driving. In the proposed GPT-Driver framework, perception and prediction results (e.g., object types, coordinates, and predicted future coordinates) together with the ego states are converted into language tokens. Then, they are used to prompt an LLM to produce a planned trajectory alongside its decision-making process in natural language. In particular, the authors propose a fine-tuning scheme with auto-generated reasoning labels to fine-tune a GPT-3.5 model for the purpose of motion planning. The results show that the GPT-Driver outperforms existing learning-based motion planners in terms of imitating human drivers and performs on par with top methods in collision rate."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed method is simple, straightforward, and well-performing in the studied driving scenarios. \n2. It provides informative insights on the feasibility and performance of steering LLMs into motion planners producing numerical waypoints. It is particularly interesting and promising that the authors show that GPT-Driver can outperform existing approaches after few-shot fine-tuning."
                },
                "weaknesses": {
                    "value": "While the proposed GPT-Driver has demonstrated impressive performance, the paper lacks in-depth analysis to help the audience gain a deeper understanding of the model's performance and limitations:\n\n1. For example, an ablation study should be conducted to evaluate the benefit of having chain-of-thought reasoning in the LLM's output. While it is well-known that chain-of-thought reasoning boosts LLM's performance, it is worth evaluating its contribution to the motion planning task. \n\n2. Also, there should be an ablation study to evaluate the benefit of having the auto-generated chain-of-thought reasoning labels during fine-tuning. While the proposed method to auto-label the chain-of-thought reasoning through hypothetical ego-trajectory is sensible, it is not guaranteed to generate the ground-truth reasoning process (i.e., identifying the actual causal objects and their relations to the ego agents). The authors claimed that this strategy worked well in practice. I wonder how the authors evaluated the quality of the auto-generated labels and drew such a conclusion. There should be numerical results to examine the quality of the auto-generated labels, and an ablation study to validate that the fine-tuning process indeed benefits from the plausibly noisy and inaccurate reasoning labels. \n\n3. The GPT model is only prompted with a simplified textual description of the traffic scene, e.g., without maps, historical trajectories of the objects, or predicted future trajectories over multiple timesteps. It is quite surprising that the GPT model can surpass carefully designed learning-based planners by a large margin in L2 errors. It is rather counter-intuitive as the information currently missing (e.g., maps, historical contexts, future trajectories) is normally considered important for motion planning in autonomous driving. The authors should provide an in-depth analysis and pinpoint the scenarios where GPT-Driver has clear advantages against SOTA methods and cases where GPT-Driver suffers. Only showing the average L2 errors and collision rates could be misleading, as the average statistics highly depend on the data distribution."
                },
                "questions": {
                    "value": "1. Could the authors clarify how the hypothetical ego trajectory is generated? Whether an object is identified as critical seems to highly depend on the hypothetical ego trajectory. The authors should discuss how they designed the generation algorithm and adjusted the hyperparameters. \n\n2. Is the motion planning performance evaluated with the most likely output sequence? How stable and reliable is the GPT-Driver in generating sensible reasoning processes and trajectories? Is it able to account for multi-modality in driving behavior?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4601/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4601/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4601/Reviewer_FVv6"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4601/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698793305779,
            "cdate": 1698793305779,
            "tmdate": 1699636438841,
            "mdate": 1699636438841,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aeJo18yCN3",
                "forum": "SXMTK2eltf",
                "replyto": "MmpuJA5iOt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4601/Reviewer_FVv6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4601/Reviewer_FVv6"
                ],
                "content": {
                    "title": {
                        "value": "Awaiting Author's Response"
                    },
                    "comment": {
                        "value": "It is a friendly reminder that the authors have not uploaded their responses to my review comment yet. Note that, due to the time difference, I may not be able to engage in further discussion if the response is uploaded in the last minutes. \n\nIn addition to my original review comments, I agreed with the other reviewers and the public comment on the limitation of open-loop evaluation. I agree with the authors that nuScenes can serve as a mature *open-loop* planning benchmark. However, it does not mean that it is sufficient as a comprehensive benchmark for AD planning, due to the limitation of the open-loop evaluation scheme itself. I think the authors should either incorporate closed-loop evaluation on nuPlan as they promised or carefully interpret the current set of open-loop evaluation results and discuss the limitations, without overly exaggerating the model's performance. \n\nNote that, in one of their responses, the authors said, \"Since nuPlan is much larger than nuScenes, we are unable to get the close-loop performance in this short discussion phase. We will update the paper once we have finished the experiments.\" As I recall, the authors are not allowed to revise the paper after Nov. 22nd. If the authors think they are not able to include the closed-loop evaluation results in the discussion period, I would suggest the authors attempt to address these issues in another way. I cannot accurately re-evaluate the quality of the paper only based on the authors' promises."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700684915829,
                "cdate": 1700684915829,
                "tmdate": 1700684915829,
                "mdate": 1700684915829,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "r6G2XyPWna",
                "forum": "SXMTK2eltf",
                "replyto": "MmpuJA5iOt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4601/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4601/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your advice"
                    },
                    "comment": {
                        "value": "Thanks for your advice!\n\n**For example, an ablation study should be conducted to evaluate the benefit of having chain-of-thought reasoning in the LLM's output.**\n\nHere is the ablation study of removing chain-of-thoughts reasoning:\n\nMethod | L2   |      |      |      | Collision |      |      |      |\n|------|------|------|------|------|-----------|------|------|------|\nGPT-Driver  | 1s   | 2s   | 3s   | Avg. | 1s        | 2s   | 3s   | Avg. |\nw/ cot. reason | 0.21 | 0.43 | 0.79 | 0.48 | 0.16      | 0.27 | 0.63 | 0.35 |\nw/o cot. reason | 0.22 | 0.45 | 0.82 | 0.49 | 0.16      | 0.25 | 0.63 | 0.34 |\n\nRemoving chain-of-thoughts reasoning slightly harms the L2 but performs slightly better in the collision. We interpret this insignificant synergy as the LLM does not benefit from a joint language modeling of reasoning and trajectory. Since reasoning will account for most of the output tokens, simultaneously optimizing reasoning and trajectory will cause the LLM to pay less attention to trajectory prediction. Recently we found a cascaded design where the LLM first outputs reasoning and then uses reasoning to output trajectory works better.\n\n**Could the authors clarify how the hypothetical ego trajectory is generated? Whether an object is identified as critical seems to highly depend on the hypothetical ego trajectory. The authors should discuss how they designed the generation algorithm and adjusted the hyperparameters.**\n\nThe hypothetical ego trajectory is generated by calculating the future waypoint locations based on the current velocity and acceleration (assuming the vehicle maintains the current speed and drives without interference.) We will identify those objects that fall into a 5x3 m region of any waypoint as critical objects. This auto-generation process is essentially which objects will affect driving when we don't take any special actions.\n\n**Is the motion planning performance evaluated with the most likely output sequence? How stable and reliable is the GPT-Driver in generating sensible reasoning processes and trajectories? Is it able to account for multi-modality in driving behavior?**\n\nThe LLM only generates one sequence at a time. The outputs of LLMs are quite stable and very few outputs over the whole validation set contain invalid format. It is able to account for multi-modality when we turn up the temperatures of LLMs and perform multiple runs."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700724168118,
                "cdate": 1700724168118,
                "tmdate": 1700729524644,
                "mdate": 1700729524644,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ogLG7Hjl36",
            "forum": "SXMTK2eltf",
            "replyto": "SXMTK2eltf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4601/Reviewer_cPYB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4601/Reviewer_cPYB"
            ],
            "content": {
                "summary": {
                    "value": "The authors present a novel method for motion planning in the context of autonomous driving, where they propose to use GPT to both output the motion plan and to explain the reasoning behind it. They fine-tune the GPT model using textual representation of the surrounding context and the output motion plan, and show that the method compares very positively when compared to the existing state-of-the-art."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- A very relevant problem being evaluated.\n- Interesting and novel approach being proposed.\n- Promising experimental results."
                },
                "weaknesses": {
                    "value": "- The method does not seem to be very feasible for online execution.\n- The method seems to critically depend on the existing SOTA methods as its integral part, making the overall system quite complex.\n- The experimental section can be improved."
                },
                "questions": {
                    "value": "I found the work quite interesting, and the combination of the motion planner with GPT seems like a neat idea (although not that novel at this point). However, the method seems far from being actually applicable in the real world, which the authors don't really explore or address (beyond a very brief explanation in Section 4.6 that seems insufficient and handwavy). Moreover, the explanations of the method can be improved significantly, and the methodology itself seems quite complex and dependent on the existing SOTA methods.\nDetailed comments can be found below:\n- The authors should fix the format of the references. Instead of \"P3 (Sadat et al., 2020)\" they use \"P3 Sadat et al. (2020)\" throughout the work, which is incorrect and adds some confusion in several places.\n- Figure 1 is not referenced in the text.\n- The method assumes the existing strong method for perception and prediction, which seems like quite a large requirement. The input to GPT assumes detections and their predicted trajectories, which seems to add quite a lot of complexity (both from the training and inference standpoint).\n- Related to this, the authors don't really do an ablation study of the perception/prediction module, which would give an indication of how robust is GPT to this part of the methodology.\n- In eq (2) the authors say that the input to their model is a map, yet that is not the case as they don't provide the map to the model.\n- Later they say that their model can indeed take the map as an input, but given that they represent all inputs as a text it is far from clear how can that be done.\n- In Section 4.2 the authors say that other approaches depend on various heterogeneous inputs \"which makes their systems intricate and time-consuming\", yet the proposed method also depends on the same inputs since it depends on UniAD. So the authors are not being really honest in this case.\n- In Section 4.3 it is unclear if the authors use fully trained UniAD for generating input strings for their method, or if they use partially trained UniAD. This should be clarified.\n- Some sort of latency analysis should be provided, beyond just a handwavy explanation from Section 4.6. This is important for the practical application of their method and is something that the authors should explore."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4601/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698811990459,
            "cdate": 1698811990459,
            "tmdate": 1699636438749,
            "mdate": 1699636438749,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NtEmXZcvXp",
                "forum": "SXMTK2eltf",
                "replyto": "ogLG7Hjl36",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4601/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4601/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the reviewer's feedback"
                    },
                    "comment": {
                        "value": "Thanks for the reviewer's feedback!\n\n**The authors should fix the format of the references. Instead of \"P3 (Sadat et al., 2020)\" they use \"P3 Sadat et al. (2020)\" throughout the work, which is incorrect and adds some confusion in several places.**\n\nThanks for pointing out. We have updated the paper to fix the typos in citations.\n\n**Figure 1 is not referenced in the text.**\n\nThanks for pointing out. We have added the reference to Figure 1 in the updated version of this paper.\n\n**The method assumes the existing strong method for perception and prediction, which seems like quite a large requirement. The input to GPT assumes detections and their predicted trajectories, which seems to add quite a lot of complexity (both from the training and inference standpoint).**\n\nOur method does not assume **strong** perception and prediction methods. On the contrary, our method can adapt to **any** perception and prediction methods, and here we choose the perception and prediction modules in UniAD mainly for a fair comparison with this approach. \n\nPerception and prediction are indispensable for safe motion planning in the context of autonomous driving. In most state-of-the-art approaches (e.g. UniAD, ST-P3), their motion planners take perception and prediction data as inputs. Hence, it is not a large requirement for the GPT-Driver to take perception and prediction as inputs.\n\n**Related to this, the authors don't really do an ablation study of the perception/prediction module, which would give an indication of how robust is GPT to this part of the methodology.**\n\nWe have actually ablated the effects of perception and prediction in Table 5, where we use perfect and UniAD's perception and prediction results as inputs to GPT-Driver. As we mentioned in Section 4.2, when replacing the perfect perception and prediction with the learned ones, the planning performance only drops slightly, which indicates the robustness of our GPT-Driver to perception and prediction errors. We are also working on ablating more perception and prediction approaches.\n\n**In eq (2) the authors say that the input to their model is a map, yet that is not the case as they don't provide the map to the model.**\n\nWhat we want to present in Eq (2) is motion planning, in general, can take maps as input data. Since our approach currently doesn't rely on maps, we have revised this part for better clarification.\n\n**Later they say that their model can indeed take the map as an input, but given that they represent all inputs as a text it is far from clear how can that be done.**\n\nMaps in autonomous driving can be represented in vectorized format, where the compact representation can be expressed as a graph denoting the locations and connections of lanes. The vectorized map can be further transformed into texts describing the locations and connections of the lanes. Hence, our approach has the potential to incorporate vectorized map information.\n\n**In Section 4.2 the authors say that other approaches depend on various heterogeneous inputs \"which makes their systems intricate and time-consuming\", yet the proposed method also depends on the same inputs since it depends on UniAD. So the authors are not being really honest in this case.**\n\nOther approaches, such as UniAD, take compact maps (H xW grids) and occupancy (H x W x T grids), in addition to perception and prediction, as inputs to their motion planner.  Extracting those compact maps and occupancy grids makes their system intricate and time-consuming. By contrast, our approach only leverages text-based perception and prediction (instead of compact maps and occupancy grids) of UniAD as inputs to GPT-Driver. Hence, we believe our argument here is **correct** and **honest**. We have revised this part to make more clarifications.\n\n**In Section 4.3 it is unclear if the authors use fully trained UniAD for generating input strings for their method, or if they use partially trained UniAD. This should be clarified.**\n\nIn Section 4.3, since we are comparing the motion planning performance, the perception and prediction in both UniAD and our approach are fully trained, and only the motion planner in UniAD and GPT-Driver are trained with 1%, 10%, 50% data. We have added more clarifications in this section.\n\n**Some sort of latency analysis should be provided, beyond just a handwavy explanation from Section 4.6. This is important for the practical application of their method and is something that the authors should explore.**\n\nWe agree with this point. As we discussed in Section 4.6, the major problem here is that we are unable to obtain very accurate inference time through the OpenAI APIs. Hence the latency analysis is not feasible at this moment. We will keep track of the updates of OpenAI and see whether we could add this analysis in the future."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700603936239,
                "cdate": 1700603936239,
                "tmdate": 1700604365379,
                "mdate": 1700604365379,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9mSBZJJxqr",
                "forum": "SXMTK2eltf",
                "replyto": "NtEmXZcvXp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4601/Reviewer_cPYB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4601/Reviewer_cPYB"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "I would like to thank the authors for their detailed responses.\nThe responses do clarify many of my concerns. However, some of the bigger ones remain:\n- The latency question is still a major disadvantage and unknown of the proposed method. \n- When it comes to the map, while I do see how the map can be provided, it is still unclear if that would actually work well, especially given some recent evidence that LLMs have trouble with mathematical reasoning. More evaluation and evidence are needed beyond just pointing out how the map info can be provided. \n- Also, when it comes to dependence on the existing methods, it is still true that the proposed method also inherits their downsides.\n\nI definitely appreciate the proposed idea and the authors' clarifications and edits. However, looking at the feedback from the other authors and the ongoing discussions it seems that the best course of action would be to revise the work, address in detail all the comments and requests, and resubmit.\nStill, if the other reviewers agree on a positive outcome and champion the paper, I would not block such a decision."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700724861641,
                "cdate": 1700724861641,
                "tmdate": 1700724861641,
                "mdate": 1700724861641,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]