[
    {
        "title": "Symmetrization of Loss Functions for Robust Training of Neural Networks in the Presence of Noisy Labels and the Multi-class Unhinged Loss Function"
    },
    {
        "review": {
            "id": "2yvWeR1GPw",
            "forum": "MY7gMVioiX",
            "replyto": "MY7gMVioiX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4378/Reviewer_81Mq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4378/Reviewer_81Mq"
            ],
            "content": {
                "summary": {
                    "value": "This work introduces a very simple method for symmetrizing a loss function and explains it as a Dirichlet prior. It further demonstrates that the unhinged loss is the only convex symmetric loss function for multi-class classification. Additionally, it symmetrizes specific loss functions, such as SGR and SGCE, and conducts some preliminary experiments on CIFAR-10 with symmetric label noise."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- The symmetrization of loss functions is very straightforward.\n- The paper presents its concepts in a clear manner and is easy to follow."
                },
                "weaknesses": {
                    "value": "- The paper template or compilation environment may have issues, leading to the use of fonts that are not in line with the ICLR reference fonts, potentially not meeting submission requirements\n- The proposed symmetrization is equivalent to negative label smoothing, which resembles the work presented in [1]. The authors should provide a more in-depth discussion of the novelty of their approach, including its relationship to other existing works [2,3,4].\n- The unhinged loss mentioned by the authors is not novel enough [5, 6].\n- Although the method proposed by the authors is straightforward, the introduction of the negative part in the symmetrization of losses could potentially result in an unstable training process. I conducted some experiments based on code from [7], suggesting significant issues, including the cccurance of \"nan\" with CE and non-convergence of other losses. The authors should conduct additional experiments and provide further analysis on this matter.\n- The experimental section of the paper is rather limited, focusing solely on basic experiments conducted on CIFAR-10. This level of experimentation is insufficient to support the paper's acceptance.\n- The writting style is not very fluent, and a reorganization of the structure is recommended. For example, it is strongly advised to move the content from Section 6 to Section 4.\n\n\n\n[1] Wei J, Liu H, Liu T, et al. To Smooth or Not? When Label Smoothing Meets Noisy Labels[C]//International Conference on Machine Learning. PMLR, 2022: 23589-23614.\n\n[2] Zhou X, Liu X, Jiang J, et al. Asymmetric loss functions for learning with noisy labels[C]//International conference on machine learning. PMLR, 2021: 12846-12856.\n\n[3] Liu Y, Guo H. Peer loss functions: Learning from noisy labels without knowing noise rates[C]//International conference on machine learning. PMLR, 2020: 6226-6236.\n\n[4] Charoenphakdee N, Lee J, Sugiyama M. On symmetric losses for learning from corrupted labels[C]//International Conference on Machine Learning. PMLR, 2019: 961-970.\n\n[5] Van Rooyen B, Menon A, Williamson R C. Learning with symmetric label noise: The importance of being unhinged[J]. Advances in neural information processing systems, 2015, 28.\n\n[6] Long P M, Servedio R A. The perils of being unhinged: On the accuracy of classifiers minimizing a noise-robust convex loss[J]. Neural Computation, 2022, 34(6): 1488-1499.\n\n[7] Ma X, Huang H, Wang Y, et al. Normalized loss functions for deep learning with noisy labels[C]//International conference on machine learning. PMLR, 2020: 6543-6553."
                },
                "questions": {
                    "value": "Please see weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4378/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698633849421,
            "cdate": 1698633849421,
            "tmdate": 1699636410644,
            "mdate": 1699636410644,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PtkH4JaCvg",
                "forum": "MY7gMVioiX",
                "replyto": "2yvWeR1GPw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4378/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4378/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Link with negative label smoothing"
                    },
                    "comment": {
                        "value": "Thank you for your comments.\n\nLink with negative label smoothing: If we denote by $L^r(z,y)$ the loss obtained from smoothing the cross-entropy loss with rate $r$, the multi-class unhinged is the limiting loss function when smoothing goes to negative infinity. We have\n\n$lim_{r \\rightarrow -\\infty} \\frac{L^r(z,y)}{1-r}=$multi-class unhinged.\n\nAnother way to see this (following from the relation between label smoothing and noise correction) is that the multi-class unhinged is obtained as the limit when the probability of corruption $p$ goes to $1$ in the noise correction loss. When using a symmetric loss function, we do not need to know $p$ since robustness is obtained for any $p$. The multi-class unhinged is like choosing $p=1$ and is robust for any $p$. \n\nLink with peer loss function: When the prior probability of y is uniform, the expected peer loss (with CE) is equal to the expected multi-class unhinged over the corrupted distribution. For each sample, the peer loss requires to draw two other samples (the peer samples). These additional samples are not needed when using the symmetrization of a loss function however. \n\n-Numerical stability: As we explained, we could solve the problem of 'nan' (the loss is negatively unbounded) by adding a batch normalization layer to the final layer of the network. Another idea to increase even further numerical stability is to remove the learnable affine parameters in this last batch normalization layer (for example by setting 'affine=False' in pytorch)."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4378/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700694054700,
                "cdate": 1700694054700,
                "tmdate": 1700694054700,
                "mdate": 1700694054700,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "UvmLW9v1Co",
            "forum": "MY7gMVioiX",
            "replyto": "MY7gMVioiX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4378/Reviewer_FQCN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4378/Reviewer_FQCN"
            ],
            "content": {
                "summary": {
                    "value": "The manuscript considers loss functions for multi-class classification in the presence of uniform label noise. It is known that a certain notion of symmetry in the loss functions (commonly used losses satisfy the property) is central to being robust against uniform label noise --- i.e., the bayes optimal classifiers for the underlying clean distribution and the noisy distribution (where some labels are flipped) coincide. In this work, the authors make a simple observation that any loss function can be decomposed as a sum of a symmetric component and a label-insensitive component, that is unique up to constant factor. They apply this observation to the standard cross-entropy loss and derive a multi-class version of the so-called unhinged loss studied in Rooyen et al 2015, which is symmetric and therefore robust to (uniform) label noise. The authors show interesting properties of the loss function, and its connection to robustness of SGD-based optimization. \n\nOverall, the paper develops and presents a few key ideas of merit, but I felt a) the contributions are a bit short of a strong ML venue, b) the paper is a bit all over the place, and the core contributions/messages don't stand out clearly, c) the paper's organization and writing can be improved a lot, and the current version hampers clarity."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Rigorous development of results, several interesting properties proved and connections made.\n- A simple but powerful observation in Proposition 4.1, that leads to symmetrization results for CE loss and generalized CE loss, and derivations of new robust loss functions for multi-class classifications.\n- Connections to SGD-based optimization + early stopping of standard CE loss"
                },
                "weaknesses": {
                    "value": "- Applicability to (only) uniform label noise is somewhat limiting. What are the ideas in the paper that could be extended/adapted to more general noise models? \n- Lack of clarity in presentation/writing\n- Take-away messages are not clear. If unhinged multi-class loss function is indeed what practitioners should consider in some if not all scenarios, we need to see a lot more empirical support than the results in Section 8."
                },
                "questions": {
                    "value": "- Please respond to the points raised under 'weaknesses'\n- Section 5 is confusing. Are there $l_{Dir}(.,.)$ forms that are meaningful _and_ symmetric besides the one in Lemma 5.1?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4378/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4378/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4378/Reviewer_FQCN"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4378/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699288177881,
            "cdate": 1699288177881,
            "tmdate": 1699636410575,
            "mdate": 1699636410575,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nZnoPa4T44",
                "forum": "MY7gMVioiX",
                "replyto": "UvmLW9v1Co",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4378/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4378/Authors"
                ],
                "content": {
                    "title": {
                        "value": "More general noise models"
                    },
                    "comment": {
                        "value": "Thank you for your comments.\n\n-More general noise models: it is possible to extend to symmetry condition to a generalized symmetry condition by choosing a different distribution in the definition of symmetry. We want to point out first that symmetric loss functions can also exhibit robustness to other type of noise as shown by (Ghosh et al. 2017) . Indeed, noise tolerance under simple non uniform noise and class conditional noise are also obtained but under some more assumptions (the true risk for the optimal classifier must be 0). Also, if we do not know the noise model, we would have to learn it. This can be more costly than using a model free method like a symmetric loss function. \n\nIt is also possible to extend these ideas to regression by replacing the summation with an integral and choosing for example a normal distribution or a uniform distribution over a bounded interval. There is also a unique decomposition of any regression loss function as a sum of a term independent from the response variable y and a symmetric regression loss function. \n\n-Multi-class unhinged: We do not claim that the Multi-class unhinged would perform best in practice. We obtained better results with SGCE for example. However, it plays a central role in the theory of symmetric loss functions. Any symmetric loss function with strong enough regularization is approximately equivalent to the multi-class unhinged."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4378/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700692258695,
                "cdate": 1700692258695,
                "tmdate": 1700692258695,
                "mdate": 1700692258695,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ryZ7lpvExH",
            "forum": "MY7gMVioiX",
            "replyto": "MY7gMVioiX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4378/Reviewer_7oFd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4378/Reviewer_7oFd"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel symmetrization method for multi-class loss functions, leading to a general approach for constructing symmetric loss functions from non-symmetric ones. The authors demonstrate the effectiveness of their method by applying it to various loss functions, including cross-entropy loss, generalized cross-entropy loss, and multi-class unhinged loss. Additionally, they provide theoretical insights into the properties of the multi-class unhinged loss function, showing that it is the unique convex, non-trivial, non-increasing, multi-class symmetric loss function under the assumption of invariance to permutations. Experimental results on the CIFAR10 dataset validate the robustness of the proposed approach."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper proposes a novel symmetrization method for multi-class loss functions, which is original in both its concept and its implementation. Also, the paper is also well-written and easy to understand. Authors present a variety of experimental results that validate the effectiveness of the proposed approach."
                },
                "weaknesses": {
                    "value": "1. The paper presents experimental results on the CIFAR10 dataset, which is a relatively small dataset with simple label noise distributions. It would be more convincing to see results on a wider range of datasets, such as CIFAR100.\n\n2. The comparison of the proposed symmetrization method to other existing methods for constructing symmetric loss functions is not clear. For example, Table 1 is not well structured. It is hard to find which result is the original loss, which are existing methods, and which is loss after symmetrization. \n\n3. The proposed symmetrization method is only applicable to uniform label noise distributions while recent research is more focused on structured noise. Is there any approach to mitigate this issue such as using a different type of prior distribution?"
                },
                "questions": {
                    "value": "See weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4378/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699527962890,
            "cdate": 1699527962890,
            "tmdate": 1699636410501,
            "mdate": 1699636410501,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]