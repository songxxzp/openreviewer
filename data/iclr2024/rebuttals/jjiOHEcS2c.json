[
    {
        "title": "Self-Supervised High Dynamic Range Imaging with Multi-Exposure Images in Dynamic Scenes"
    },
    {
        "review": {
            "id": "m642qI3iQQ",
            "forum": "jjiOHEcS2c",
            "replyto": "jjiOHEcS2c",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2377/Reviewer_DQCr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2377/Reviewer_DQCr"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposed a self-supervised HDR reconstruction method from a triplet of LDR images with relative motion. The proposed method first prepares two intermediate components focusing on HDR structure and color, and then learns a neural network to estimate the final HDR under the supervision of both. The approach is superior to other self-supervised learning approaches and is comparable to supervised learning methods with regard to objective metrics and visual qualities."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The idea of using two-stage training to learn structure and color information first and then learn the final output is novel to me. The overall design of the method is reasonable.\n\n* experiments are extensive and demonstrate the compelling performance of the proposed method."
                },
                "weaknesses": {
                    "value": "* The proposed method is specific to 3 inputs. The 3 inputs should be informatively captured, especially the mid-exposed image, which should have texture and color of high fidelity as the reference. The requirement for the input is strict and can limit the application of the proposed method.\n\n*The authors should point readers to the appendix when appropriate, e.g., visualization of masks and selection of sigmas."
                },
                "questions": {
                    "value": "The structure-focused network was proposed to avoid errors caused by optical flow alignment, but it is integrated with optical flow alignment in training stage 2, which doesnt make sense to me. Although Tab. 6 shows improvement in general, I think it will hurt the performance in the cases when the alignment has more errors, whereas those cases can demonstrate the core value of the paper. The authors should elaborate on this and experiment on those cases to justify their statement when necessary.\n\nMisc:\n* HDR-VDP-2 metric is not as commonly used as PSNR/SSIM. The authors should briefly introduce what the value means, and if a bigger/smaller value indicates better image quality.\n* In Tab 3-6 the highest scores can be marked in bold."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2377/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2377/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2377/Reviewer_DQCr"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2377/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697931207339,
            "cdate": 1697931207339,
            "tmdate": 1699636171610,
            "mdate": 1699636171610,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KLCiJgILFO",
                "forum": "jjiOHEcS2c",
                "replyto": "m642qI3iQQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2377/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2377/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer DQCr"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the valuable comments and suggestions.\nWe appreciate the reviewer's questions and hope our responses could address the concerns.\n\n\n1. Limitation of Inputs\n\nWe agree with the reviewer's insight on method limitations. Moreover, most existing multi-exposure HDR reconstruction methods (including supervised and self-supervised ones) only focus on removing ghosting artifacts caused by misalignment between inputs, having this limitation as well. This work (SelfHDR) has taken a step toward more realistic self-supervised HDR imaging by deghosting, while our ongoing work is to further address this limitation.\n\n\nHere we share two possible solutions. On the one hand, we can combine HDR reconstruction with some self-supervised image restoration methods when considering more realistic shooting conditions. For example, there may exist noise in short-exposure images and blur in long-exposure images. In order to achieve a self-supervised algorithm, we can combine HDR reconstruction with some self-supervised denoising and debluring works to process input images for removing degradations. \n\nOn the other hand, an adaptive method may need to be explored to select a more appropriate image as a base frame. For example, when a mid-exposure image suffers more severe degradations than others, the method should adaptively take short-exposure or long-exposure images as a new base frame for HDR reconstruction. Thus, it can alleviate the shooting requirements for a mid-exposure image.\n\nWe will add this in Sec. E of the appendix in the revision.\n\n\n2. Pre-Alignment in Structure-Focused Network\n\n\n\nThe ultimate goal of the structure-focused network is to provide HDR supervision with good structure that is complementary to the color component. It should avoid ghosting results, rather than avoid errors caused by optical flow alignment. Moreover, different from naive merging operation (in Eqn. (5)) for generating color component, learning-based structure-focused network has the ability to handle misalignment problem, including the remaining misalignment after optical flow pre-alignment, as shown in Fig. A (l).\n\n\n\nBelow we further explain the effect of pre-alignment. On the one hand, significantly incorrect optical flow mainly appears in occlusion and overexposed areas from other frames. These areas have little valuable information, so the results mainly rely on the base frame, and pre-alignment or not has little impact on the results. Thus, it's not bad to perform pre-alignment in this case. On the other hand, when optical flow is roughly correct, pre-alignment reduces the processing burden of the structure-focused network that does not adopt explicit alignment. Thus, it naturally brings improvements.\n\n\n\nWe additionally test the structure-focused network on 74 training scenes with and without optical flow pre-alignment, respectively. First, the average results are shown in the following table. It can be seen that the pre-alignment manner has a significant improvement on average. Second, we compare the results between the two manners one by one. We find that only in 6 scenes, the results without pre-alignment are more than 0.1dB better than those with pre-alignment on PSNR-$u$. In the other 68 scenes, the pre-alignment manner always gives better or comparable results. Moreover, we also observe the characteristics of these 6 scenes. We find that their alignment is not worse than the other 68 scenes and is moderate in general. The experiments show that the above explanation is reasonable in most scenes.\n\n\n\n|  | PSNR-$u$ / SSIM-$u$ | PSNR-$l$ / SSIM-$l$ \n| :----:       |  :----:  | :----:  |\n| w/o Pre-Alignment        | 42.40 / 0.9809    | 40.02 / 0.9869   |\n| w/ Pre-Alignment       | 42.84 / 0.9814    | 41.48 / 0.9894   |\n\nWe will add this in Sec. B of the appendix in the revision.\n\n\n3. Paper Writing and Misc\n\nMany thanks. In the revision, we will add pointers to the contents of the appendix. We will also briefly introduce HDR-VDP-2 and mark the best results in Tables 3$\\sim$6 in bold."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2377/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700649327265,
                "cdate": 1700649327265,
                "tmdate": 1700649327265,
                "mdate": 1700649327265,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "v0Qf4I62kC",
            "forum": "jjiOHEcS2c",
            "replyto": "jjiOHEcS2c",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2377/Reviewer_TwnM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2377/Reviewer_TwnM"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents \"SelfHDR,\" a self-supervised High Dynamic Range (HDR) image reconstruction method. Traditional HDR reconstruction methods require ample ground-truth data, which can be difficult and expensive to collect. The novelty in SelfHDR comes from learning a reconstruction network under two complementary components, HDR color and structure, constructed from multi-exposure static images. They designed a detailed process to generate these components and used them to train the reconstruction network. Experimental results showed that SelfHDR surpassed state-of-the-art self-supervised methods and achieved comparable performance with supervised methods. Contributions include a novel solution to alleviate the need for extensive labeled data for HDR image reconstruction."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The originality of the paper is substantial as it proposes an innovative self-supervised method for HDR image reconstruction, eliminating the need for extensive labeled data, a factor that typically hinders HDR reconstruction methods. The quality of the research is commendable. To build this method, they analyzed HDR images thoroughly and isolated the primary components, color, and structure that could be derived from multi-exposure static images. The paper is coherent and clear in its exposition, and the authors elegantly illustrate the problem, their approach, and results. The significance of this work is considerable as it presents a promising alternative for HDR image reconstruction, which demonstrates competitiveness with supervised methods while minimizing data requirements."
                },
                "weaknesses": {
                    "value": "A potential weakness of the paper is the lack of a broader experimental validation. While they have tested the model on several datasets, the application in real-world scenarios, especially dealing with complex situations and variable lighting conditions, is not clearly examined. The paper could have also delved more into the limitations of the proposed method, such as the cases where the SelfHDR method may not provide optimal results. An exploration of the generalizability limitations of the method would have been appreciated."
                },
                "questions": {
                    "value": "- How sensitive is the SelfHDR method to the quality of the input images (for example, noise)? Would substantial noise or minor shifts in alignment between images drastically affect the performance of the method? This might be a bit hard to comment on, as the misalignments in the Kalantari dataset are already fixed.\n- Have the authors considered combining the SelfHDR method with other techniques (perhaps pre-processing or post-processing techniques) to enhance its performance? For example, use a pre-trained denoising network to process the low-exposure image and use a hallucination network such as Stable Diffusion to generate the content in the over-exposed regions.\n- Can the authors provide more insights into the cases where the SelfHDR method might fail or provide subpar results? For instance, are there certain types of scenes, color distributions, or specific types of exposure variations that may destabilize the method?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2377/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698207823289,
            "cdate": 1698207823289,
            "tmdate": 1699636170976,
            "mdate": 1699636170976,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4LocKcXLV5",
                "forum": "jjiOHEcS2c",
                "replyto": "v0Qf4I62kC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2377/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2377/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer TwnM"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the valuable comments and suggestions.\nWe appreciate the reviewer's questions and hope our responses could address the concerns.\n\n1. SelfHDR Sensitivity to Noise\n\nSelfHDR does not specifically deal with the noise problem. If the noise in the multi-exposure image is obvious, the result may also contain noise. Moreover, most existing multi-exposure HDR reconstruction methods (including supervised and self-supervised ones) only focus on removing ghosting artifacts caused by misalignment between inputs, having this limitation as well. SelfHDR has taken a step toward more realistic self-supervised HDR imaging by deghosting, while our ongoing work is to further address this limitation.\n\nNonetheless, perhaps benefiting from the combination of two complementary supervision, our method still performs better than other self-supervised methods (e.g., FSHDR) when testing noisy images, as shown in Fig. 4(b).\n\n\n2. SelfHDR Sensitivity to Misalignment\n\n\nThe robustness to misalignment depends on the HDR reconstruction networks we use. In Table 1, we use four HDR reconstruction networks (including CNN-based ones and Transformer-based ones) for experiments. From the results, the Transformer-based networks (i.e., HDR-Transformer and SCTNet) perform better. And the qualitative results testing on out-of-domain datasets (i.e., Sen et al. and Tursun et al. datasets) also show this trend.\n\nIn addition, benefiting from the self-supervised properties of SelfHDR, the best way to improve robustness may be to collect as much data as possible for training.\n\n\n\n3. Combining with Other Techniques\n\nFor noisy images, we have tried to deploy a pre-trained real-world denoising network, but it leads to over-smooth results, losing many details. This may be because the noise model and level of noisy images mismatch with the data (i.e., SIDD dataset) on which the denoising network was trained. \n\nWe also agree with the reviewer's insight that HDR reconstruction can be combined with some image restoration methods, especially when considering more realistic shooting conditions. Our ongoing work is aimed at this. Instead of using pre-trained networks, we think that it may be more appropriate to introduce some self-supervised image restoration works to process input images.\n\nFor over-exposed regions, we argue that it is difficult to generate realistic details with a hallucination network such as Stable Diffusion, as it can only imagine the information. Actually, the content of over-exposed regions can be preserved well in low-exposure images. It may be more reliable to search for the corresponding information in low-exposure images.\n\n\n4. Failed Cases \n\nIn addition to handling noisy images, here we give some other situations where SelfHDR may fail. First, when long-exposure images suffer from blur, SelfHDR may introduce blurry results. In future work, we can combine some self-supervised image restoration methods to process the blurry images. Second, when the scene irradiance changes drastically in shooting multi-exposure images, SelfHDR may fail, as the constructed color components may be inaccurate. And this situation is also difficult to handle for supervised methods.\n\n\nBesides, SelfHDR has no specific requirements for scene type and content color. In terms of generalization ability in different scenes, SelfHDR is similar to the corresponding supervised HDR reconstruction networks. And the best way to improve generalization ability is to collect more data and retrain the network.\n\n\nWe will add this and give some examples in Sec. E of the appendix in the revision."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2377/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700649199563,
                "cdate": 1700649199563,
                "tmdate": 1700649199563,
                "mdate": 1700649199563,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Rq1g1rlZmj",
                "forum": "jjiOHEcS2c",
                "replyto": "4LocKcXLV5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2377/Reviewer_TwnM"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2377/Reviewer_TwnM"
                ],
                "content": {
                    "title": {
                        "value": "Thank you!"
                    },
                    "comment": {
                        "value": "Thank you so much for addressing my concerns. I have no further questions and would like to stick to my initial positive opinion."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2377/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700661730756,
                "cdate": 1700661730756,
                "tmdate": 1700661730756,
                "mdate": 1700661730756,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xGIZaFbYbK",
            "forum": "jjiOHEcS2c",
            "replyto": "jjiOHEcS2c",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2377/Reviewer_nVjt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2377/Reviewer_nVjt"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a self-supervised HDR reconstruction method SelfHDR for dynamic scenes, which only requires input multi-exposure images during training. Specifically, SelfHDR decomposes latent ground-truth into constructible color and structure component supervisions. Experiments show that SelfHDR achieves comparable performance to supervised ones."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1.This work proposes a self-supervised HDR reconstruction method, addressing the problem of the difficult collection of paired HDR data.\n\n2.The idea of constructing color and structure supervision respectively is somewhat novel.\n\n3.This work achieves competitive results compared to supervised methods."
                },
                "weaknesses": {
                    "value": "1.Is the performance upper limit of this work limited by the upper limit of structure or color supervision? What designs correspond to solving this problem?\n\n2.This method may also be limited by the alignment method.\n\n3.Some visualizations of structure and color supervision can be given. \n\n4. Discussion with some traditional multiple exposure fusion methods, such as HDR plus?"
                },
                "questions": {
                    "value": "Please see the above weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2377/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2377/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2377/Reviewer_nVjt"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2377/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698507263549,
            "cdate": 1698507263549,
            "tmdate": 1699636170575,
            "mdate": 1699636170575,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NZbocWSRLU",
                "forum": "jjiOHEcS2c",
                "replyto": "xGIZaFbYbK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2377/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2377/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer nVjt"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the valuable comments and suggestions.\nWe appreciate the reviewer's questions and hope our responses could address the concerns.\n\n1. Performance Upper Limit\n\nThe performance upper limit of our work is indeed limited by the upper limit of structure or color supervision. Actually, the performance of existing supervised methods is also limited by the quality of ground truth (GT). Moreover, although the individual structure or color component is not sufficient as supervision, combining the two components as supervision achieves comparable results to supervised methods. As a self-supervised method, the performance has been enough good.\n\nBelow we describe in detail the combination of the two complementary components. Two strategies are presented to combine their advantages and discard their disadvantages. First, we use different loss terms for the two supervisions to learn their different information. We adopt L1 loss for the color component and VGG loss for the structure component. The purpose of the former is to learn color information, while the latter is to learn texture information. Second, we design masks to exclude ghosting areas in color components, thus avoiding adverse impacts from these areas.\n\n\n2. Limited by Alignment Method\n\nThe alignment approach mainly influences the color component. Nevertheless, the color component is not very demanding for the alignment approach, as its poorly aligned areas can be further supplemented by the structure component. Furthermore, the development of the current image alignment works is mature, they provide us with enough assistance to achieve comparable performance to supervised methods.\n\n\nWe will add this in Sec. E of the appendix in the revision.\n\n3. Visualizations of Structure and Color Supervision\n\nWe respectively visualize a color and structure supervision in Fig. A (g) and (l) of the appendix.\n\n\n4. Discussion with HDR+ \n\nTraditional methods to remove ghosting include rejecting misaligned areas, aligning input images, and using patch-based composite, as mentioned in the second paragraph of Sec. 1 (Introduction). Recently, deghosting methods based on deep learning perform more effectively than traditional ones. Thus, in this work, we adopt learning-based methods to explore self-supervised HDR reconstruction.\n\nIn particular, HDR+ takes bursts captured by the same exposure setting as input, while our method adopts multi-exposure images. Unlike multi-exposure images, bursts have limited dynamic range, which makes HDR+ only suitable for scenes with moderate dynamic range, rather than high dynamic range. In addition, HDR+ may only handle small misalignment between the input images, while our method can handle large motions."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2377/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700649049387,
                "cdate": 1700649049387,
                "tmdate": 1700649049387,
                "mdate": 1700649049387,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "G3oQdRWhjb",
            "forum": "jjiOHEcS2c",
            "replyto": "jjiOHEcS2c",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2377/Reviewer_Vk35"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2377/Reviewer_Vk35"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a self-supervised method to fuse multi-exposed images in dynamic scenes. It learns a reconstruction network under the supervision of two complementary components, including the color component and the structure component. The color component is estimated from aligned multi-exposure images. The structure one is generated through a structure-focused network that is supervised by the color component and an input image. These components construct a pseudo reference for training."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. It is a self-supervised method, which overcomes the need for supervised labeled data in supervised methods.\n2. Experiments show that our SelfHDR outperforms the state-of-the-art self-supervised methods, and achieves comparable performance to supervised ones.\n3. The way of decomposing an image into color and structure components is new."
                },
                "weaknesses": {
                    "value": "1. The drawback of existing self-supervised methods is that they construct pseudo-pairs for HDR reconstruction. The performance of these methods is unsatisfactory, as their simulated pairs still have gaps with real-world ones. The proposed self-supervised way is also realized by constructing a pseudo reference. Why does this method show advantages? The theoretical discussion and differentiation with existing self-supervised methods are not very sufficient.\n2. For dynamic scenes, the method relies on existing registration methods to obtain more accurate color components for registration.\n3. This method constructs a pseudo ground truth for self-supervision. The keys lie in the combination optimization (for color components), and mask-based weighting (for structure components). The construction process is simple and mainly based on the functions in Figure 1 which is set by prior.\n4. One of the contributions is the construction of a color component. However, it lacks the comparison of the overall results which can directly reflect this advantage in terms of color."
                },
                "questions": {
                    "value": "1. Does this decomposition way (decomposing an image into color and structure components) have a corresponding theoretical basis, and does it correspond to some existing image decomposition theory?\n2. \u201cRegardless of the ghosting areas, the rest can record the rough color value, and in which well-aligned ones can offer both good color and structure cues of HDR images\u201d. The experiment lacks the HDR results in the presence of alignment errors.\n3. The mask-based weighting does not consider pixel neighborhood relationships. Will it lead to artifacts?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2377/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2377/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2377/Reviewer_Vk35"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2377/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698638859880,
            "cdate": 1698638859880,
            "tmdate": 1699636170190,
            "mdate": 1699636170190,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1LdxJiBkTZ",
                "forum": "jjiOHEcS2c",
                "replyto": "G3oQdRWhjb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2377/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2377/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Vk35"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the valuable comments and suggestions.\nWe appreciate the reviewer's questions and hope our responses could address the concerns.\n\n1. Discussion with Existing Self-Supervised Methods\n\nExisting self-supervised methods (e.g., FSHDR (Prabhakar et al., 2021) and Nazarczuk et al. (Nazarczuk et al., 2022)) construct pseudo-inputs and a corresponding pseudo-target. For these methods, motion and illumination in input images are synthetic, and exhibit gaps with real-world ones.\n\nIn this work, we maintain the original inputs and only construct pseudo-targets for them. It does not destroy the real-world motion and illumination in the original inputs. Moreover, we construct two complementary supervisions to address the possible information missing problem in a single pseudo-target.\n\nWe will make it clear in the revision.\n\n2. Dependence on Registration Method\n\nThe color component indeed relies on the registration method. Nevertheless, it is not very demanding for the registration method, as its poorly aligned areas can be further supplemented by the structure component. Furthermore, the development of the current image registration works is mature, they provide us with enough assistance to achieve comparable performance to supervised methods.\n\n\nWe will add this in Sec. E of the appendix in the revision.\n\n\n3. More Discussion about Color Components\n\nThe color component is generated by weighting the aligned images, as shown in Eqn. (5). When some areas are aligned well, the corresponding areas in the color component are consistent with the ground-truth. When some areas are aligned poorly, the corresponding areas in the color component may have ghosting artifacts. For the former situation, we can directly take these well aligned areas as supervision of HDR reconstruction network. For the latter situation, we construct a mask in Eqn. (14) to exclude these poorly aligned areas when training HDR reconstruction network. And experiments in Table 4 show that the mask indeed plays a positive role.\n\n\nIn addition, we visualize a color component in Fig. A (g) of the appendix. It can be seen that its characteristics are consistent with the phenomenon we describe above.\n\n\n\n4. Decomposition Way\n\nOur decomposition way is a focus or emphasis on color and structure relatively, not an absolute separation. Nevertheless, it may have some similarities with Laplace decomposition. The color component is similar to the low-frequency part of  Laplace decomposition, while the structure component is similar to the high-frequency part of Laplace decomposition. \n\n5. Mask-Based Weighting\n\nFig. 1 is a commonly used weighting function for generating an HDR image from static multi-exposure images. In Fig. 1, the abscissa of the function figure is the medium-exposure image value, and the ordinate is the mask value. In other words, the mask value depends on the image value. As long as the images are aligned well, additional artifacts will not be introduced in the color component. Moreover, we visualize a mask in Fig. A (i) of the appendix. It can be seen the mask is highly relevant to the image value, and is not scattered and irregular."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2377/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700648847764,
                "cdate": 1700648847764,
                "tmdate": 1700648908265,
                "mdate": 1700648908265,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]