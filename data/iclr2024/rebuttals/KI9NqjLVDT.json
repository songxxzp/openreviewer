[
    {
        "title": "ReMasker: Imputing Tabular Data with Masked Autoencoding"
    },
    {
        "review": {
            "id": "vR4UfGFsFT",
            "forum": "KI9NqjLVDT",
            "replyto": "KI9NqjLVDT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4077/Reviewer_YQFA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4077/Reviewer_YQFA"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents an imputation method, ReMasker, that leverages a remasking autoencoder with a Transformer-based architecture. A significant amount of experimentation is included to validate the proposed procedure."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The proposed method provides a simpler heuristic than competing SOTA methods such as HyperImpute, while displaying similar or slightly improved accuracy. \n\nThe experiments are relatively thorough and show strong results for the MCAR setting in particular."
                },
                "weaknesses": {
                    "value": "The methodology is not novel, as masking is a common practice for imputation algorithms and masked autoencoders with Transformers are used for other tasks. However, the authors do not claim novelty and I agree this is likely the first work to consider a masked autoencoder with Transformers for imputation specifically. \n\nThe claims that ReMasker generalizes to MNAR settings is not well justified:\n\n* It\u2019s not clear how we could properly learn a missingness-invariant representation in the MNAR setting. Under MNAR, the missing data values fundamentally depend on the missingness mechanism, so recovering a missingness-invariant representation would require external information.\n\n* The experimental condition for MNAR (masking via a Bernoulli random variable with fixed mean) is not representative of many MNAR settings. For example, cases where values across possibly many variables are masked due to censoring of one or more variables (e.g. $X_2$, $X_5$, $X_6$ are missing when $X_5 < 0$). The missing values often follow a completely different distribution than the observed values, which cannot be recovered using the observed data alone.\n\nIt is not clear that the results are practically different from HyperImpute, and perhaps other methods. The included bar plots are hard to read, so any assessment is difficult. The relative simplicity of ReMasker compared to HyperImpute is mentioned as one advantage, although the much simpler ICE appears competitive in almost all scenarios.\n\nWhile the theoretical justification is appropriately included in the Discussion section, the justification is a conjecture with minimal validation. In practice, the missingness mechanism is different for $m^+$ and $m^-$, so it is not clear that representation will be invariant in general. Further experimentation is needed."
                },
                "questions": {
                    "value": "Is there a better way to represent the results visually? It is very difficult to read the plots (e.g. Figure 2), and in particular to the confidence intervals.\n\nWhile it is clearly stated that imputation for downstream tasks is not part of the scope of the paper, can any more discussion be provided? Considering imputation is almost always followed by downstream tasks, I believe more justification for this decision is needed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4077/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4077/Reviewer_YQFA",
                        "ICLR.cc/2024/Conference/Submission4077/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4077/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698256407542,
            "cdate": 1698256407542,
            "tmdate": 1700491475564,
            "mdate": 1700491475564,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SBFfIB82pq",
                "forum": "KI9NqjLVDT",
                "replyto": "vR4UfGFsFT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4077/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4077/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the valuable feedback on improving this paper! Please find below our response to the reviewer\u2019s questions.\n\n> The methodology is not novel, as masking is a common practice for imputation algorithms and masked autoencoders with Transformers are used for other tasks. However, the authors do not claim novelty and I agree this is likely the first work to consider a masked autoencoder with Transformers for imputation specifically.\n\nWhile both Transformer and masked autoencoding (MAE) have been used in prior work, this work adapts the MAE approach (with Transformer as the backbone) in a nuanced yet novel way: besides the missing values in the given dataset, we randomly select and \u201cre-mask\u201d another set of values, optimize the autoencoder with the objective of reconstructing this re-masked set, and then apply the trained autoencoder to predict the missing values. This resulting re-masking approach serves as a strong baseline for developing and evaluating future imputation methods.\n\n> The claims that ReMasker generalizes to MNAR settings is not well justified:\n> * It\u2019s not clear how we could properly learn a missingness-invariant representation in the MNAR setting. Under MNAR, the missing data values fundamentally depend on the missingness mechanism, so recovering a missingness-invariant representation would require external information. \n> * The experimental condition for MNAR (masking via a Bernoulli random variable with fixed mean) is not representative of many MNAR settings. For example, cases where values across possibly many variables are masked due to censoring of one or more variables (e.g. X_2, X_5, X_6 are missing when X_5<0). The missing values often follow a completely different distribution than the observed values, which cannot be recovered using the observed data alone.\n\nWe thank the reviewer for the insightful comments. In general, it is impossible to identify the missingness distribution in MNAR without domain-specific assumptions or constraints (Ma & Zhang, 2021). That is why most prior work (e.g., Yong et al., 2019; Yoon and Sull, 2020; Jarrett et al., 2022) focuses on MCAR and MAR. For completeness, following the setting of Jarrett et al., (2022), we also evaluate ReMasker under one specific MNAR setting and show that it also partially generalizes to such settings. We acknowledge that there are MNAR settings (such as the ones suggested by the reviewer) where access to external knowledge is essential. We have revised the statement (**Section 4**) to more accurately reflect our work\u2019s focus on MAR and MCAR and its limitations with respect to MNAR. \n\nFurther, we have added a detailed discussion about how ReMasker\u2019s performance is influenced by the missingness mechanism (**Section 5 Q2**). Intuitively, ReMasker encourages learning representations invariant to re-masked values and then leverages such representations to impute missing values. Thus, while ReMasker takes effect as long as the distributions of re-masked and missing values are correlated, its performance may vary with the disparity between the two distributions. In MACR, the distributions of re-masked and missing values are highly similar; in MAR, the distribution of re-masked values is biased towards the observable values; in MNAR, there is even more disparity between the two distributions. Thus, ReMasker\u2019s performance tends to vary as MCAR > MAR > MNAR."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4077/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700068770336,
                "cdate": 1700068770336,
                "tmdate": 1700068770336,
                "mdate": 1700068770336,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "J2ETLE6IhX",
                "forum": "KI9NqjLVDT",
                "replyto": "SBFfIB82pq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4077/Reviewer_YQFA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4077/Reviewer_YQFA"
                ],
                "content": {
                    "comment": {
                        "value": "> While both Transformer and masked autoencoding (MAE) have been used in prior work, this work adapts the MAE approach (with Transformer as the backbone) in a nuanced yet novel way: besides the missing values in the given dataset, we randomly select and \u201cre-mask\u201d another set of values, optimize the autoencoder with the objective of reconstructing this re-masked set, and then apply the trained autoencoder to predict the missing values. This resulting re-masking approach serves as a strong baseline for developing and evaluating future imputation methods.\n\nUnfortunately I still have concerns about novelty. For instance, masking observed data has been previously studied for missing data problems, for example Gondara and Wang, 2018, where they refer to masking as stochastic corruption.\n\n> We have revised the statement (Section 4) to more accurately reflect our work\u2019s focus on MAR and MCAR and its limitations with respect to MNAR.\n\nI believe the revised exposition for the MNAR setting in particular is much more reasonable and a significant improvement."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4077/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700491064752,
                "cdate": 1700491064752,
                "tmdate": 1700491064752,
                "mdate": 1700491064752,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jJ62h5RYjv",
                "forum": "KI9NqjLVDT",
                "replyto": "a4Jyin0AnI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4077/Reviewer_YQFA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4077/Reviewer_YQFA"
                ],
                "content": {
                    "comment": {
                        "value": "> To better visualize the improvement of ReMasker over other baselines, we have summarized the performance of ReMasker and baselines (measured and ranked in terms of RMSE) on 12 benchmark datasets under MAR with 0.3 missingness ratio in Table 10 (Appendix B3).\n\nThank you, this addition is greatly appreciated.\n\n> The key observation is that ReMasker consistently outperforms other baselines across most datasets (ranked top 1 or 2).\n\nI acknowledge that ReMasker is consistently the best performing algorithm in your experiments. However, based on the provided standard deviation estimates, the improvement is not significant. It is not clear whether the improvement is practically significant for downstream tasks that leverage the imputed data.\n\n> Compared with HyperImpute, ReMasker is a much simpler and more modular imputer; therefore, it may serve as a strong baseline to develop or evaluate future imputation methods or it can be integrated into an ensemble imputer such as HyperImpute.\n\nI agree with this observation, and for this reason in particular, I believe ReMasker is a potentially valuable contribution.\n\n> To further validate the masking-invariant properties of ReMasker, we measure the CKA similarity (Kornblith et al., 2019) between the latent representations (i.e., the output of ReMasker\u2019s encoder) of inputs under the masks of $m^+$ and $m^-$, with results shown in Figure 6 (Appendix B4). Observe that the CKA measures between the inputs under the two masks steadily increase with the training epochs, which empirically corroborates the analysis in Section 5 Q1 that ReMasker tends to learn representations that are invariant to various missingness.\n\nThis addition is greatly appreciated, and I agree it does add evidence about the paper's claims. Can you clarify how the missingness mechanisms for $m^+$ and $m^-$ differ in this experiment?\n\n> We thank the reviewer for the question. Following the line of work on imputing tabular data (e.g., Mattei & Frellsen, 2018; Yong et al., 2019; Nazabal et al., 2020; Yoon and Sull, 2020; Jarrett et al., 2022), we do not consider imputing missing values as a means to obtain data for known downstream tasks. Under those settings, the imputation is fundamentally entangled with the downstream task, typically requiring joint training to optimize the downstream task and the imputation task simultaneously. Due to the potential conflict between the objectives of the two tasks (i.e., data fidelity versus downstream-task performance), it is not straightforward to evaluate the performance of imputation methods directly. We thus focus on the imputation task with data fidelity as the main metric.\n\nThis is reasonable. I apologize that my original question was relatively vague, I would like to provide some additional context. You compare your results with algorithms such as MICE, a multiple imputation framework that was not originally intended for SOTA imputation accuracy necessarily, but rather the ability to measure the added uncertainty from missing data in downstream *inferential* tasks. Based on your justification, I believe the focus of the paper is reasonable, but although I encourage the authors to consider the value of ReMasker with a focus on improved uncertainty estimation in downstream tasks, possibly as part of future research."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4077/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700491078101,
                "cdate": 1700491078101,
                "tmdate": 1700491078101,
                "mdate": 1700491078101,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uhspCuQOxJ",
                "forum": "KI9NqjLVDT",
                "replyto": "a4Jyin0AnI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4077/Reviewer_YQFA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4077/Reviewer_YQFA"
                ],
                "content": {
                    "title": {
                        "value": "Thank you"
                    },
                    "comment": {
                        "value": "I would like to thank the authors for taking the time to reply to the various comments from myself and my fellow reviewers. I believe the revised submission is an improvement over the previous version. In particular, the improved exposition surrounding MNAR and the addition of appendices B1-B4.\n\nI still have significant concerns about novelty and improvement over other baselines, but I agree with the authors that ReMasker serves as a simple baseline for future work or as part of future ensemble methods. \n\nFor those reasons, I will update my score to a 5."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4077/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700491459193,
                "cdate": 1700491459193,
                "tmdate": 1700491459193,
                "mdate": 1700491459193,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "s89cvPTsU7",
            "forum": "KI9NqjLVDT",
            "replyto": "KI9NqjLVDT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4077/Reviewer_Fz3c"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4077/Reviewer_Fz3c"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose ReMasker, a method for imputing missing values in tabular data based on masked auto encoders (MAE). Specifically, given a dataset, they mask some of the features and train a model to impute these features. The approach does not require all features to be present in training, so a dataset with missing values can be used for training. The authors provide extensive empirical study and a theoretical justification."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I think the authors propose an elegant solution to a problem many practitioners encounter. The paper is well written and the empirical results are convincing. \nIn general, for an approach such as the one presented, it is difficult to provide a theoretical analysis. I appreciate the theoretical justification in Section 5, but I would have liked it to have a more prominent position in the paper."
                },
                "weaknesses": {
                    "value": "The authors fail to mention and discuss multiple previous works. Specifically, transformers have been previously been applied to tabular data (Somepalli et al. 2021, Ar\u0131k et al. 2020,  Huang et al 2020). Specific to Huang et al, the authors cite this paper for supporting their model choice, but fail to mention it in the related works. The MAE work by Majmundar et al. (2022) is also not discussed.\n\n\nSomepalli et al. 2021: SAINT: Improved Neural Networks for Tabular Data via Row Attention and Contrastive Pre-Training\nAr\u0131k et al. 2020: TabNet: Attentive Interpretable Tabular Learning\nHuang et al 2020: TabTransformer: Tabular Data Modeling Using Contextual Embeddings\nMajmundar et al. 2022: MET: Masked Encoding for Tabular Data"
                },
                "questions": {
                    "value": "I have one minor question:\n- On page 1, you state that discriminative imputers are hindered by the requirement of specifying a functional form. It seems to me that your architecture is also a choice of \"functional form\". Am I wrong?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4077/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4077/Reviewer_Fz3c",
                        "ICLR.cc/2024/Conference/Submission4077/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4077/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698660479522,
            "cdate": 1698660479522,
            "tmdate": 1700474781671,
            "mdate": 1700474781671,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HJOZMXVHcJ",
                "forum": "KI9NqjLVDT",
                "replyto": "s89cvPTsU7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4077/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4077/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the valuable feedback on improving this paper! Please find below our response to the reviewer\u2019s questions.\n\n> The authors fail to mention and discuss multiple previous works. Specifically, transformers have been previously been applied to tabular data (Somepalli et al. 2021, Ar\u0131k et al. 2020, Huang et al 2020). Specific to Huang et al, the authors cite this paper for supporting their model choice, but fail to mention it in the related works. The MAE work by Majmundar et al. (2022) is also not discussed.\n\nWe thank the reviewer for pointing out the missing references. We have included a discussion of these works (**Section 2**). While SAINT, TabNet, and TabTransformer also use Transformer as the backbone to model tabular data, the main idea of ReMasker is to adapt the masked autoencoding (MAE) approach to imputing missing values of tabular data: besides the missing values in the given dataset, randomly select and \u201cre-mask\u201d another set of values, optimize the autoencoder with the objective of reconstructing this re-masked set, and then apply the trained autoencoder to predict the missing values. Meanwhile, although MET also employs the MAE approach, its primary focus is on the representation learning of tabular data, operating under the assumption of data completeness and applying MAE in a relatively direct manner. In contrast, ReMasker, with its emphasis on imputing missing values, inherently assumes incomplete data and adapts the MAE technique in a nuanced yet novel way.\n\n> On page 1, you state that discriminative imputers are hindered by the requirement of specifying a functional form. It seems to me that your architecture is also a choice of \"functional form\". Am I wrong?\n\nWe thank the reviewer for the question. We would like to clarify that the functional form refers to the conditional distributions of missing values. Specifically, the discriminative methods often impute missing values by modeling their conditional distributions on the basis of observable values, therefore these methods are hindered by the requirement of specifying the proper functional forms about the conditional distributions.\n\nAgain, we thank the reviewer for the valuable feedback. Please let us know if there are any other questions or suggestions.\n\nBest,\n\nAuthors"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4077/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700068291856,
                "cdate": 1700068291856,
                "tmdate": 1700068291856,
                "mdate": 1700068291856,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zvX8QZnK1G",
                "forum": "KI9NqjLVDT",
                "replyto": "HJOZMXVHcJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4077/Reviewer_Fz3c"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4077/Reviewer_Fz3c"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your Updates"
                    },
                    "comment": {
                        "value": "I thank the authors for updating their paper with a more detailed discussion of existing works. Since I believe that the proposed method is valuable to the community, and after reading your answer to all reviewers, I would like to increase my score to 6."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4077/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700474766365,
                "cdate": 1700474766365,
                "tmdate": 1700474766365,
                "mdate": 1700474766365,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0t3yfhKnWD",
            "forum": "KI9NqjLVDT",
            "replyto": "KI9NqjLVDT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4077/Reviewer_w2Hd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4077/Reviewer_w2Hd"
            ],
            "content": {
                "summary": {
                    "value": "The paper Remasker: Imputing Tabular Data with Masked Autoencoding presents a novel algorithm for missing data imputation in tabular datasets using the masked autoencoding framework. The algorithm operates in two stages: a first stage where extra variables are masked and the autoencoder is optimized to reconstruct the re-masked variables, and a second stage where the trained model is used to predict the missing values. Authors evaluate the method on a wide range of state-of-the-art methods in several tabular datasets, showing competitive performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is well-written and grammatically correct with no evident typos. Every section is very well outlined with clear paragraphs describing the content of the manuscript. \n\nThe proposed method, namely the Remasker method, is easy to understand for experts and non-experts in the field. The idea is intuitive yet effective, showing promising results on how we can leverage the promises of transformer-based architecture for missing data imputation. \n\nThe evaluation of the method is quite thorough, where standard tabular datasets are tested under different state-of-the-art methods. This reveals that the authors are aware of the most recent advances in missing data imputation. Besides, different metrics and ablation studies are performed to motivate the performance of the method further. Another precious point is that the authors do comment on the limitations of their proposed method, which is something that is sometimes overlooked."
                },
                "weaknesses": {
                    "value": "I have a few concerns and questions that I'd like the authors to address:\n\n1. While I understand the primary motivation behind using a transformer-based architecture in this work, I would appreciate a more in-depth discussion of why the authors chose to use masked autoencoders over other approaches, like deep generative models. It's crucial to outline the advantages and disadvantages of this choice. Additionally, it would be beneficial to address questions like the computational cost of training the Remasker method and its ability to generate new data, which is a common concern in the missing data imputation field.\n2. The argument presented in Section 4.1 about why Remasker generalizes different missing data assumptions could be strengthened. While transformer-based architectures have proven their superior performance in various tasks, a more comprehensive analysis of why transformers excel in missing data imputation would enhance the paper's quality and its contribution to both the missing data imputation and transformer-based research fields.\n3. The manuscript appears to be densely packed in terms of space. Figures and tables are positioned very closely to the text, which can make the paper seem cluttered and condensed. Consider the possibility of making some tables smaller or relocating them to the Appendix (e.g., Table 1), and possibly reducing the size of certain figures (e.g., Figure 2 with fewer datasets). This issue can be addressed in the camera-ready version if the paper is accepted."
                },
                "questions": {
                    "value": "Some questions I have are the following:\n\n1. I'd like to understand the rationale behind using a deep encoder and a shallow decoder when describing the decoder. While the Ablation Study in Section 4.3 provides some insight, could you provide a more abstract explanation for this choice? Is there a specific reason why you believe the first fitting stage of the Remasker benefits from a more complex encoder?\n2. Some figures display results for different baselines (e.g., Figure 3 RMSE), and while they may appear distinct, the values on the y-axis are actually quite close. Could you elaborate on how these slight differences influence the final results? For instance, in Figure 3, Remasker outperforms other methods by only a narrow margin in terms of RMSE and AUROC. How does this marginal improvement make Remasker a more beneficial choice compared to other methods that utilize deep generative models to learn the data distribution?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4077/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4077/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4077/Reviewer_w2Hd"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4077/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698755596477,
            "cdate": 1698755596477,
            "tmdate": 1699636371918,
            "mdate": 1699636371918,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WRLta9tytc",
                "forum": "KI9NqjLVDT",
                "replyto": "0t3yfhKnWD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4077/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4077/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the valuable feedback on improving this paper! Please find below our response to the reviewer\u2019s questions.\n\n> While I understand the primary motivation behind using a transformer-based architecture in this work, I would appreciate a more in-depth discussion of why the authors chose to use masked autoencoders over other approaches, like deep generative models. It's crucial to outline the advantages and disadvantages of this choice. Additionally, it would be beneficial to address questions like the computational cost of training the Remasker method and its ability to generate new data, which is a common concern in the missing data imputation field.\n\nOur use of masked autoencoding (MAE) is motivated by its effectiveness of modeling image and text data (e.g., Delvin et al., 2019; He et al., 2021). This work demonstrates that MAE also represents a promising approach for imputing tabular data. Compared with alternative approaches (e.g., deep generative models), MAE has the following advantages: 1) it does not require complete data during training or the knowledge about the underlying missingness mechanism; 2) without suffering the difficulties of adversarial training (Goodfellow et al., 2014) or training through variational bounds (Zhao et al., 2022), it is easier to optimize; 3) it works effectively even under a high ratio of missing data (e.g., up to 70%), which corroborates the findings in previous work (e.g. He et al, 2021) that MAE accurately reconstructs missing parts of an image, even when over half of the image is masked.\n\nTo evaluate its computational cost, we further compare the running time of ReMasker (under the default setting of Table 7) and HyperImpute on datasets with over 1,000 records, with results summarized as follows (**Appendix B1**). Observe that compared with HyperImpute, ReMasker is not only more scalable but also less sensitive to data size. \n\n| | compression |  wine |  spam | credit | bike | obesity | california |\n| :----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: |\n| **HyperImpute** | 154.9s | 297.7s | 185.8s | 39.9s | 259.9s | 196.1s | 195.8s |\n| **ReMasker** | 72.9s | 73.8s | 74.1s | 74.0s | 66.9s | 66.8s |66.9s |\n\nTo evaluate its ability to generate new data, we run the following evaluation (**Appendix B2**). We halve each dataset into two subsets $D$ and $D'$ (both with 30% missing value missing under MAR). We train ReMasker on $D$ and then apply it to impute the missing values in $D'$. The results are shown in the table below. Note that compared with the setting where ReMasker is trained and applied on the same dataset (Figure 2), ReMasker performs comparably well under the setting that it is applied on a new dataset.\n\n| | compression |  wine |  spam | credit | bike | obesity | california |\n| :----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: |\n| **RMSE** $\\downarrow$ | 0.117 | 0.120 | 0.052 | 0.251 | 0.108 | 0.197 | 0.068 |\n| **WD** $\\downarrow$ | 0.047 | 0.033 | 0.009 | 0.131 | 0.035 | 0.059 | 0.020 |\n| **AUROC** $\\uparrow$ |  N/A | 0.784 | 0.098 | 0.921 | 0.968 | 0.937 | N/A |"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4077/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700067847245,
                "cdate": 1700067847245,
                "tmdate": 1700067847245,
                "mdate": 1700067847245,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "39i0YgnbOi",
                "forum": "KI9NqjLVDT",
                "replyto": "Lbyup3jIrK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4077/Reviewer_w2Hd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4077/Reviewer_w2Hd"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the comments"
                    },
                    "comment": {
                        "value": "I express my gratitude to the authors for addressing my comments and conducting additional experiments to enhance the paper's quality. Their efforts not only addressed my queries but also those raised by other reviewers. I am confident that incorporating the feedback from the reviews will significantly enhance the paper's readability, particularly in terms of improving the plots and providing the necessary justifications that were lacking in the initial version of the manuscript.\n\nUpon reviewing the comments from other reviewers, it appears that the paper could benefit from a more in-depth exploration of other works utilizing transformers for tabular data, as highlighted by reviewer Fz3c. I concur with reviewer YQF tA's observation that certain results suggest a near-identical performance between Hyperimpute and the proposed method. However, I believe this similarity could be advantageous for researchers in the missing data imputation field who are familiar with alternative methods such as Deep Generative Models but have yet to encounter a paper employing transformers for this task. Although some justifications were lacking in the main manuscript, and there is room for improvement in the visual representation of results, I appreciate the authors' efforts and find the proposed method intriguing enough to merit consideration for acceptance.\n\nI will maintain my initial score but aim to collaborate with other reviewers and the AC to reach a consensus during the discussion period. Once again, I would like to thank the authors for their work and fast response to the reviews."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4077/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700134686313,
                "cdate": 1700134686313,
                "tmdate": 1700134686313,
                "mdate": 1700134686313,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]