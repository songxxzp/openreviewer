[
    {
        "title": "AUC-CL: A Batchsize-Robust Framework for Self-Supervised Contrastive Representation Learning"
    },
    {
        "review": {
            "id": "zRpLyeGhgM",
            "forum": "YgMdDQB09U",
            "replyto": "YgMdDQB09U",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4089/Reviewer_QVtp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4089/Reviewer_QVtp"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes AUC-CL, a new batch-size robust framework for self-supervised contrastive representation learning. At first, the authors point out the limitations of existing NT-Xent objectives (i.e., performance is heavily influenced by batch size). Then, the authors theoretically analyze why SimCLR loss is sensitive to batch size from the gradient perspective. Finally, the author provides the theoretical analysis to demonstrate the convergence guarantee of AUC-CL. Although the relative performance is significant, the absolute accuracy is not competitive. Besides, the reasons why AUC-CL is batch-size robust are not clear, which seems to be an important property."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is well-organized and easy to follow\n\nThe experimental results demonstrate the effectiveness of the proposed AUC-CL."
                },
                "weaknesses": {
                    "value": "1. As stated in the summary, the motivations behind using the AUC framework in combination with contrastive learning, and why this is helpful for batch size robustness, are not clear."
                },
                "questions": {
                    "value": "As I have reviewed this submission in the other venue, and most of the concerns have been addressed, I have no further questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4089/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4089/Reviewer_QVtp",
                        "ICLR.cc/2024/Conference/Submission4089/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4089/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698026311541,
            "cdate": 1698026311541,
            "tmdate": 1700270418689,
            "mdate": 1700270418689,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xKV06afJqI",
                "forum": "YgMdDQB09U",
                "replyto": "zRpLyeGhgM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4089/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4089/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Although the relative performance is significant, the absolute accuracy is not competitive.**\n\nWe express our appreciation to the reviewer for their favorable review and comments. We direct the reviewer to our primary response, where the ongoing training of ResNet-50 on ImageNet is elaborated. Furthermore, we earnestly request the reviewer to extend their evaluation beyond larger architectures and batch sizes, emphasizing the distinctive value of our contribution to the field of contrastive learning literature.\n\n**The motivations behind using the AUC framework in combination with contrastive learning, and why this is helpful for batch size robustness, are not clear.**\n\nWe thank the reviewer for this critique and clarify further. The AUC framework fits perfectly for our application for the following reasons.\n\n1. AUC is formulated for binary classification problems wherein, the objective of the network is to enhance the prediction scores for \u201cpositive\u201d samples in comparison to the \u201cnegatives\u201d (Definition 3.1 in manuscript). Thus by virtue of its construction, it aligns seamlessly for our application in contrastive learning wherein due to the lack of labels, we are compelled to enforce separation amongst classes through a binary objective (positives=augmentations of the same sample, negatives=augmentations of different samples).\n\n2. AUC was originally devised to address the imbalance of classes whereby accuracy as a metric may lead to misleading evaluation of the network. A classic example of this phenomenon is often cited with a dataset containing 100 samples, 99 of which are of the \u201cpositive\u201d class and a network that predicts every sample as a \u201cpositive\u201d will therefore have attained a 99\\% accuracy. This aspect of the function resonates well with our context as for one image in our batch of samples, the remaining images are considered to be \u201cnegative\u201d.\n\n\nThe function exhibits robustness to variations in batch size, attributed to unbiased stochastic estimators providing gradient estimates that remain unbiased for the entire batch, as theoretically demonstrated in our manuscript (section A.1). Notably, this characteristic isn't exclusive to our formulation, as other works, such as [1, 2, 3, 4, 5, 6], also integrate unbiased estimators in their loss functions. Our function's superior performance is evident, and we attribute this to its inherent nature. The decline in performance observed in other formulations with smaller batch sizes is linked to increased noise in the batch, where a smaller batch tends to have a noisier set of similarity scores. Notably, our function introduces an adaptive margin term ($A_3$), a unique component in our formulation. This term proves particularly relevant for handling noise in stochastic samples. For instance, in cases where the sim$(i, j)$ term is artificially high due to noise, the term sim$(i, i)$ is strategically adjusted to compensate for the noise, improving class clustering in the feature space and mitigating the effects of inherent noise.\n\n[1] Grill, Jean-Bastien, et al. \"Bootstrap your own latent-a new approach to self-supervised learning.\" Advances in neural information processing systems 33 (2020): 21271-21284.\n\n[2] Zbontar, Jure, et al. \"Barlow twins: Self-supervised learning via redundancy reduction.\" International Conference on Machine Learning. PMLR, 2021.\n\n[3] Caron, Mathilde, et al. \"Emerging properties in self-supervised vision transformers.\" Proceedings of the IEEE/CVF international conference on computer vision. 2021.\n\n[4] Zhang, Shaofeng, et al. \"Zero-cl: Instance and feature decorrelation for negative-free symmetric contrastive learning.\" International Conference on Learning Representations. 2021.\n\n[5] Ermolov, Aleksandr, et al. \"Whitening for self-supervised representation learning.\" International Conference on Machine Learning. PMLR, 2021.\n\n[6] Zhang, Shaofeng, et al. \"Align representations with base: A new approach to self-supervised learning.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4089/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700263562117,
                "cdate": 1700263562117,
                "tmdate": 1700264152084,
                "mdate": 1700264152084,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UvCo6M71q2",
                "forum": "YgMdDQB09U",
                "replyto": "xKV06afJqI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4089/Reviewer_QVtp"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4089/Reviewer_QVtp"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response."
                    },
                    "comment": {
                        "value": "I appreciate the authors' detailed responses, which clarify the advantages and motivation of the proposed AUC-CL. Therefore, I would like to change my score to 6."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4089/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700270401596,
                "cdate": 1700270401596,
                "tmdate": 1700270401596,
                "mdate": 1700270401596,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cYe58tltJK",
            "forum": "YgMdDQB09U",
            "replyto": "YgMdDQB09U",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4089/Reviewer_pifE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4089/Reviewer_pifE"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes AUC-Contrastive Learning, which incorporates the contrastive objective within the AUC-maximization framework. Since it maintains unbiased stochastic gradients, it is more robust to batch sizes compared to the standard contrastive loss. It empirically shows that the method with a batch size of 256 outperforms or is on par with several state-of-the-art methods using larger batch sizes."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The method is novel, and theoretically sound. It is great that there is clear link between the AUC-maximization and robustness to batch size.\n\n- The empirical results are promising. The proposed method with a small batch size can outperform or is on par with state-of-the-art methods with large batch size, which is beneficial for the efficiency of computation power."
                },
                "weaknesses": {
                    "value": "- Some baselines are missing in the experiments. There are other methods for self-supervised representation learning, such as MAE[1] and MAGE[2]. Some of them have better performance than the contrastive learning methods. Although this paper focuses on improving contrastive representation learning, it is still necessary to compare with non-contrastive representation learning methods. \n\n- Some contrastive learning methods which do not need negative samples, e.g. BYOL and DINO, also shows some robustness to batch size. I think it is necessary to list their performance with smaller batch size (e.g., 256, the same as the proposed method) in Table 1.\n\n- Why does the paper use ViT-S on ImageNet in the experiment? As far as I aware, most of the representation learning methods use ViT-B or ViT-L for ImageNet evaluation. Also, do all methods converge at 400 epochs for ResNet-50 and 300 epochs for ViT-S? If the baseline methods can reach the same performance as the proposed method with longer epochs and larger architecture, then I don't think the paper can claim the outperformance over them.\n\n[1] He et al. Masked Autoencoders Are Scalable Vision Learners. CVPR 2022. \n\n[2] Li et al. MAGE: MAsked Generative Encoder to Unify Representation Learning and Image Synthesis. CVPR 2023."
                },
                "questions": {
                    "value": "See Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4089/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698436469703,
            "cdate": 1698436469703,
            "tmdate": 1699636373618,
            "mdate": 1699636373618,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vyhxjektve",
                "forum": "YgMdDQB09U",
                "replyto": "cYe58tltJK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4089/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4089/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Some baselines are missing in the experiments. There are other methods for self-supervised representation learning, such as MAE and MAGE. Some of them have better performance than the contrastive learning methods. Although this paper focuses on improving contrastive representation learning, it is still necessary to compare with non-contrastive representation learning methods.**\n\nWe express appreciation to the reviewer for their valuable feedback and recommendations. We would like to direct the reviewer's attention to the comprehensive comparison in our manuscript, where we assess non-contrastive methods such as BYOL [1], Barlow Twins [2], DINO [3], Zero-CL [4], W-MSE [5], and ARB [6] across various architectures and datasets. Given the common practice of employing ResNet-50 in Self-Supervised Learning (SSL), our comparisons are specifically tailored to include well-known methods using this backbone. Notably, the methods MAE and MAGE, which utilize ViT-H and ViT-L backbones with prohibitively high batch sizes (2048-4096), deviate from this common practice and necessitate industrial-grade equipment for efficient execution. We kindly hope that the reviewer considers the detailed comparisons in Tables 1, 2, and 3 in our manuscript, as these directly address the raised concern and provide insights into our method's performance against recent state-of-the-art techniques.\n\n**Why does the paper use ViT-S on ImageNet in the experiment? As far as I aware, most of the representation learning methods use ViT-B or ViT-L for ImageNet evaluation.**\n\nWe respectfully disagree with the reviewer as a substantial number of self-supervised learning methods predominantly utilize ResNet-50 on the ImageNet dataset. Notable recent methods, referenced in [1, 2, 3, 4, 5, 6], form a part of our comparisons in Table 1. A more contemporary trend in Self-Supervised Learning (SSL) involves incorporating the Vision Transformer (ViT), exemplified in DINO (Table 2). It is noteworthy that this comparison only includes a subset of methods compared to those utilizing ResNet-50, signifying a recency in the adoption of the ViT architecture.\nAdditionally we would like to mention that incorporating the ViT-B and ViT-L architectures mandate large GPUs for training in a reasonable amount of time which is an infeasible task for us considering our constraints on resources.\n\n**Some contrastive learning methods which do not need negative samples, e.g. BYOL and DINO, also shows some robustness to batch size. I think it is necessary to list their performance with smaller batch size (e.g., 256, the same as the proposed method) in Table 1.**\n\nWe appreciate the reviewer's suggestion in this regard. The following are some of the linear evaluation results upon pretraining for 100 epochs on ImageNet using ResNet-50. We also include our results for 100 epoch pretraining here.\n\n|Method\t|Batch Size\t|Linear Acc@1|\n|:----|:----:|:----:|\n|DINO\t|256\t\t|68.2|\n|BYOL\t|256\t\t|62.4|\n|VicReg\t|256\t\t|63.0|\n|Barlow Twins\t|256\t|62.9|\n|Ours |256 |**69.5**| \n\nKindly note that the numbers are borrowed from the popular benchmark of https://docs.lightly.ai/self-supervised-learning/getting_started/benchmarks.html#imagenet in the interest of time. We shall duly incorporate these in our Table 1.\n\n\n[1] Grill, Jean-Bastien, et al. \"Bootstrap your own latent-a new approach to self-supervised learning.\" Advances in neural information processing systems 33 (2020): 21271-21284.\n\n[2] Zbontar, Jure, et al. \"Barlow twins: Self-supervised learning via redundancy reduction.\" International Conference on Machine Learning. PMLR, 2021.\n\n[3] Caron, Mathilde, et al. \"Emerging properties in self-supervised vision transformers.\" Proceedings of the IEEE/CVF international conference on computer vision. 2021.\n\n[4] Zhang, Shaofeng, et al. \"Zero-cl: Instance and feature decorrelation for negative-free symmetric contrastive learning.\" International Conference on Learning Representations. 2021.\n\n[5]  Ermolov, Aleksandr, et al. \"Whitening for self-supervised representation learning.\" International Conference on Machine Learning. PMLR, 2021.\n\n[6] Zhang, Shaofeng, et al. \"Align representations with base: A new approach to self-supervised learning.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4089/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700262961376,
                "cdate": 1700262961376,
                "tmdate": 1700334073037,
                "mdate": 1700334073037,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nVZJhTGaIj",
                "forum": "YgMdDQB09U",
                "replyto": "cYe58tltJK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4089/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4089/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Do all methods converge at 400 epochs for ResNet-50 and 300 epochs for ViT-S? If the baseline methods can reach the same performance as the proposed method with longer epochs and larger architecture, then I don't think the paper can claim the outperformance over them.**\n\nTo enhance convergence, our method is trained for 650 epochs using the ResNet-50 architecture on ImageNet, with detailed results outlined in the common response section. We only claim state of the art performance for the widely considered architectures of ResNet-50, ResNet-18 (smaller datasets) and ViT-S, in keeping with the common convention. Additionally, It is crucial to emphasize that our contribution is not geared towards surpassing State-of-the-Art (SOTA) performances but rather proposing a novel contrastive learning approach that significantly reduces computational requirements. We kindly hope that the reviewer acknowledges this distinction and considers our method's unique merits."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4089/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700265262414,
                "cdate": 1700265262414,
                "tmdate": 1700334119624,
                "mdate": 1700334119624,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1PiB6GTjKk",
                "forum": "YgMdDQB09U",
                "replyto": "cYe58tltJK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4089/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4089/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Gentle reminder"
                    },
                    "comment": {
                        "value": "Dear reviewer pifE,\n\nWe express our gratitude for your dedicated efforts in reviewing and providing valuable suggestions. Despite the prolonged discussion period, we have yet to receive feedback on your response. Kindly note that we have concluded the pretraining of Resnet-50, as outlined in the main response, and have incorporated the necessary updates into the PDF (appendix, Table 6), including minor formatting adjustments. We would really appreciate it if you can let us know if our responses and the longer pretraining results resolve your concerns. Additionally, please inform us of any lingering concerns you may have.\n\nBest regards, Authors"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4089/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700680082330,
                "cdate": 1700680082330,
                "tmdate": 1700680082330,
                "mdate": 1700680082330,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nP7FartQI8",
                "forum": "YgMdDQB09U",
                "replyto": "1PiB6GTjKk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4089/Reviewer_pifE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4089/Reviewer_pifE"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response"
                    },
                    "comment": {
                        "value": "The authors' responses address most of my concerns, while I'd like to clarify that for the ViT-S question I was referring to that most of the representation learning methods use ViT-B or ViT-L for ImageNet evaluation with ViT backbones (besides ResNet backbones). I still think including larger architectures and longer training epochs will enhance the evaluation of this paper. I acknowledge the results with longer epochs in the common response and I think that should be a crucial part for the evaluation. \n\nNevertheless, I appreciate the novelty of the proposed method and understand the limitation of computation resources, so I keep my rating as positive."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4089/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700687963926,
                "cdate": 1700687963926,
                "tmdate": 1700687963926,
                "mdate": 1700687963926,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "U9ABYlB50Q",
            "forum": "YgMdDQB09U",
            "replyto": "YgMdDQB09U",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4089/Reviewer_nw59"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4089/Reviewer_nw59"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces AUC-Contrastive Learning as a method to enable self-supervised learning even with a small batch size. This proposed method optimizes the model by making binary predictions on positive and negative samples, aiming to maximize the AUC score. Despite using a significantly smaller batch size, the method shows improved performance over existing contrastive approaches."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper presents a contrastive learning technique based on the AUC score, effectively approximating a deterministic score. Its simplicity makes it easy to implement.\n\n- Unlike conventional contrastive learning-based self-supervised approaches, the proposed method demonstrates strong robustness to batch size variations while also showing performance enhancements.\n\n- Through extensive experiments, such as those involving various architectures, smaller datasets, and few-shot transfer, the paper proves the effectiveness of the method across different settings."
                },
                "weaknesses": {
                    "value": "- The paper is generally not reader-friendly and lacks clarity. In several sections, terms are not defined or are vaguely explained. Specific issues include:\n    - On the second page, the terms B and T are not defined, making comprehension difficult.\n    - The introduction should provide a full explanation of AUC.\n    - On the third page, the term N is not explained.\n    - Figure 1 is distant from its related equation, making it hard to understand terms like A_1,2,3.\n    - In Eq. 8, there's a lack of clarity on how 'a' and 'b' are replaced.\n    - The algorithm section doesn\u2019t exactly mention which loss function is being optimized.\n    - The paper introduces each method name without specifying which previous work it refers to.\n\n- The experimental results appear to be unfair, and several state-of-the-art methods are not reported.\n    - In Table 1, many methods didn't use the multi-crop strategy, but the proposed method did. This gives the proposed method an unjust \n benefit as it would've seen many more images per epoch, leading to potentially improved results.\n    - In Table 2, recent methods like DINO are not mentioned at all. DINO reported performance metrics (KNN: 72.8, Linear: 76.1 in 300 epochs) that are higher than the proposed method. The paper's claim that it outperforms several state-of-the-art methods is misleading, and a comprehensive comparison with all state-of-the-art methods is required."
                },
                "questions": {
                    "value": "There needs to be a clearer explanation regarding how a(w) and b(w) are replaced in Equation 7 and what 'a' and 'b' represent in Equation 8."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4089/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4089/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4089/Reviewer_nw59"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4089/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698575276367,
            "cdate": 1698575276367,
            "tmdate": 1699636373536,
            "mdate": 1699636373536,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zT3B2DMUwx",
                "forum": "YgMdDQB09U",
                "replyto": "U9ABYlB50Q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4089/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4089/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We are grateful to the reviewer for providing the critique which we aim to address and strengthen our contribution.\n\n**On the second page, the terms B and T are not defined, making comprehension difficult.**\n\nThe terms $B$ and $T$ refer to the batch size of the training and the number of training steps. We merely use the common convention in referring to these quantities, in keeping with the larger context of the introduction. We shall update the manuscript in order to clarify further.\n\n**The introduction should provide a full explanation of AUC.**\n\nWe chose to incorporate the complete definition and the methods used to optimize the metric in the method section with the intent of simplifying the introduction and motivations to the readers and avoid mathematical definitions and derivations in the early phases of the paper. We kindly direct the reviewer to section 3.2 for a deeper explanation of the AUC metric.\n\n**On the third page, the term N is not explained.**\n\nWe were merely using the Big-O notation for time and space complexity wherein the term $N$ refers to the number of samples.\n\n**Figure 1 is distant from its related equation, making it hard to understand terms like A_1,2,3.**\n\nWe apologize for the inconvenience in this regard. Figure 1 refers to the Equation 8 (next page). We shall clarify the reference in Section 3.3 which contains our main equation.\n\n**In Eq. 8, there's a lack of clarity on how 'a' and 'b' are replaced.**\n\nIn Equation 8, the parameters $a$ and $b$ undergo optimization by the network, akin to AUC maximization (referenced as Equation 4 in [1]). Notably, we opt to fix $a$ and optimize $b$, a decision grounded in empirical considerations. This choice is made to enhance the stability of the training process, leveraging the network's greater ease of optimization, as articulated in the manuscript.\n\n**The algorithm section doesn\u2019t exactly mention which loss function is being optimized.**\n\nWe thank the reviewer for pointing out this oversight to us. The algorithm optimizes Equation 8, which we have now corrected in the manuscript.\n\n**The paper introduces each method name without specifying which previous work it refers to.**\n\nWe express gratitude to the reviewer for their observation. Citations to the methods in Tables 2 and 3 are embedded in the manuscript's text. To enhance clarity, we have revised these citations, now incorporating the names of the methods.\n\n**In Table 1, many methods didn't use the multi-crop strategy, but the proposed method did. This gives the proposed method an unjust benefit as it would've seen many more images per epoch, leading to potentially improved results.**\n\nWe express our gratitude to the reviewer for highlighting this observation. In the subsequent analysis, we aim to showcase our method's performance without employing the multi-crop strategy. Specifically, we compare our approach against DINO, omitting the multi-crop strategy, utilizing the ViT-S backbone, and pretraining on ImageNet for 100 epochs with a batch size of 128. Notably, DINO employed a batch size of 1024. The results are extracted from Table 8 in DINO and Table 6 in our manuscript.\n\n|Method\t|Batch Size\t|Linear Acc@1|\n|:----|:----:|:----:|\n|DINO\t|1024\t|67.8|\n|MoCo-v3 |512\t\t|62.3|\n|Ours\t|256\t\t|**67.9**|\n\nAdditionally, we kindly direct the reviewer's attention to the results presented in the manuscript, excluding Tables 1, 2, and 4. These results are obtained without the use of the multi-crop augmentation strategy, following the standard augmentation method [2]. We compare against all methods listed in the main ImageNet comparisons in these tables. We assert that our method consistently demonstrates state-of-the-art performance without relying on the multi-crop strategy, as evident in multiple manuscript results. Furthermore, in the common response table, we present the outcome of pretraining ResNet-50 on ImageNet using our method for 650 epochs with the multi-crop strategy. Notably, the other figures in the table align with accepted work from DINO ([3], Table 2), encompassing contributions with and without the use of the multi-crop strategy.\n\n\n[1] Yuan, Zhuoning, et al. \"Large-scale robust deep auc maximization: A new surrogate loss and empirical studies on medical image classification.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021.\n\n[2] Chen, Ting, et al. \"A simple framework for contrastive learning of visual representations.\" International conference on machine learning. PMLR, 2020.\n\n[3] Caron, Mathilde, et al. \"Emerging properties in self-supervised vision transformers.\" Proceedings of the IEEE/CVF international conference on computer vision. 2021."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4089/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700262009247,
                "cdate": 1700262009247,
                "tmdate": 1700262329884,
                "mdate": 1700262329884,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pNMwIlBCwQ",
                "forum": "YgMdDQB09U",
                "replyto": "U9ABYlB50Q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4089/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4089/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**In Table 2, recent methods like DINO are not mentioned at all. DINO reported performance metrics (KNN: 72.8, Linear: 76.1 in 300 epochs) that are higher than the proposed method. The paper's claim that it outperforms several state-of-the-art methods is misleading, and a comprehensive comparison with all state-of-the-art methods is required.**\n\nWe express gratitude to the reviewer for this feedback and we will add and clarify this result in revision. It is pertinent to note that extensive efforts have been made to encompass well-known state-of-the-art methods in our comparisons, as demonstrated in Table 1, where DINO is included. We outperform DINO in Tables 1 and 3. Additionally, we hope the reviewer can kindly also consider our newer results mentioned in the common response section."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4089/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700262045323,
                "cdate": 1700262045323,
                "tmdate": 1700276665875,
                "mdate": 1700276665875,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "G0fEA8XgGp",
                "forum": "YgMdDQB09U",
                "replyto": "U9ABYlB50Q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4089/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4089/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Gentle reminder"
                    },
                    "comment": {
                        "value": "Dear reviewer nw59,\n\nWe express our gratitude for your dedicated efforts in reviewing and providing valuable suggestions. Despite the prolonged discussion period, we have yet to receive feedback on your response. Kindly note that we have concluded the pretraining of Resnet-50, as outlined in the main response, and have incorporated the necessary updates into the PDF (appendix, Table 6), including the suggested adjustments to method names and references. We would really appreciate it if you can let us know if our response resolves your concerns. Additionally, please inform us of any lingering concerns you may have.\n\nBest regards, Authors"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4089/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700678741367,
                "cdate": 1700678741367,
                "tmdate": 1700679824205,
                "mdate": 1700679824205,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OhKQMnIh80",
            "forum": "YgMdDQB09U",
            "replyto": "YgMdDQB09U",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4089/Reviewer_vuAt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4089/Reviewer_vuAt"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces AUC-CL, a new approach to self-supervised contrastive learning that seeks to mitigate the dependencies on large batch sizes required by many existing contrastive methods. By integrating the contrastive objective within the AUC-maximization framework, the method prioritizes improving the binary prediction differences between positive and negative samples. This unique approach offers unbiased stochastic gradients during optimization, suggesting a higher resilience to batch size variations. Experimental results indicate that the AUC-Contrastive Learning, even with smaller batch sizes like 256, can rival or surpass performances of other methods requiring considerably larger batch sizes."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper addresses an important issue in contrast learning: model performance is particularly sensitive to batch size, preferring large batch size.\n\n2. The theoretical proof of the design in this paper is very detailed."
                },
                "weaknesses": {
                    "value": "1. There are non-contrastive learning SSL methods (e.g., SimSiam) that are also less sensitive to batch size. Considering those methods, the novelty of this paper might be a concern.\n\n2. There appears to be a conflict between the experimental results in the table and the figure."
                },
                "questions": {
                    "value": "1. The contrastive learning methods (e.g., SimCLR and MoCo) are sensitive to batch size and prefer larger batch size. However, there are some self-supervised leanring methods such as SimSiam, which do not have the concept of \"positive\" and \"negative\" pairs and are more robust to smaller batch sizes. Considering those methods, the contribution of this paper is diminished.\n\n2. In Table 3, the method proposed in this paper outperforms SimSiam in Cifar-10 dataset, with a 3.1% higher accuracy. However, if we take a look at the Figure 3, the accuracy gap between them is very small, especially in late training stage (after 400 epochs). Could you explain why there appears to be a conflict between these two results?\n\n3. As shown in Figure 3, the accuracy advantage of the proposed method gradually disappear as training proceeds, so it actually just converges faster at early training stage. Moreover, according to the convergence trend in Figure 3, if we train the model for 1,000 epochs for complete convergence as stated in Table 3, the proposed method will not even have an advantage convergence speed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4089/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4089/Reviewer_vuAt",
                        "ICLR.cc/2024/Conference/Submission4089/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4089/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698790911999,
            "cdate": 1698790911999,
            "tmdate": 1700710413604,
            "mdate": 1700710413604,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6xqgXqhHwX",
                "forum": "YgMdDQB09U",
                "replyto": "OhKQMnIh80",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4089/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4089/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**The contrastive learning methods (e.g., SimCLR and MoCo) are sensitive to batch size and prefer larger batch size. However, there are some self-supervised leanring methods such as SimSiam, which do not have the concept of \"positive\" and \"negative\" pairs and are more robust to smaller batch sizes. Considering those methods, the contribution of this paper is diminished.**\n\nWe express our appreciation to the reviewer for their insightful feedback. In response, we respectfully direct the reviewer's attention to our comparative analysis, presented in Tables 1 and 3, which contrasts our method, SimSiam, with several others claiming robustness to batch size. Notably, these methods, namely BYOL [1], Barlow Twins [2], Zero-CL [3], W-MSE [4], ARB [5] and DINO [6], are all negative-free techniques. Our work demonstrates superior performance on the specified datasets at a significantly smaller batch size, showcasing heightened robustness.\n\nIt is imperative to highlight that while these methods are negative-free, our noteworthy advantage lies in achieving **superior performance with a considerably reduced batch size**. This distinction enables us to execute training on limited computational resources, underscoring the unique contribution of our work. Furthermore, we would like to emphasize that contrastive learning, incorporating negatives, remains an active and evolving research domain despite the advancements in non-contrastive methods. Some exemplary and recent approaches in this area include [7, 8, 9].\n\n**In Table 3, the method proposed in this paper outperforms SimSiam in Cifar-10 dataset, with a 3.1% higher accuracy. However, if we take a look at the Figure 3, the accuracy gap between them is very small, especially in late training stage (after 400 epochs). Could you explain why there appears to be a conflict between these two results?**\n\nWe are afraid that the reviewer may be mistaken in this evaluation. Table 3 lists the *linear evaluation* results after pretraining for 500 epochs using a batch size of 128, while Figure 3 lists the *k-NN* accuracy (Acc @1) for the mentioned methods.\nIt is crucial to highlight that, despite the close proximity in k-NN accuracy, the features derived from our pretraining method exhibit a greater informativeness for downstream tasks, as is evinced by the linear evaluation performance given in Table 3.\n\n\n[1] Grill, Jean-Bastien, et al. \"Bootstrap your own latent-a new approach to self-supervised learning.\" Advances in neural information processing systems 33 (2020): 21271-21284.\n\n[2] Zbontar, Jure, et al. \"Barlow twins: Self-supervised learning via redundancy reduction.\" International Conference on Machine Learning. PMLR, 2021.\n\n[3] Zhang, Shaofeng, et al. \"Zero-cl: Instance and feature decorrelation for negative-free symmetric contrastive learning.\" International Conference on Learning Representations. 2021.\n\n[4]  Ermolov, Aleksandr, et al. \"Whitening for self-supervised representation learning.\" International Conference on Machine Learning. PMLR, 2021.\n\n[5] Zhang, Shaofeng, et al. \"Align representations with base: A new approach to self-supervised learning.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\n\n[6] Caron, Mathilde, et al. \"Emerging properties in self-supervised vision transformers.\" Proceedings of the IEEE/CVF international conference on computer vision. 2021.\n\n[7] Jiang, Qian, et al. \"Understanding and constructing latent modality structures in multi-modal representation learning.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n[8] Johnson, Daniel D., Ayoub El Hanchi, and Chris J. Maddison. \"Contrastive learning can find an optimal basis for approximately view-invariant functions.\" arXiv preprint arXiv:2210.01883 (2022).\n\n[9] Hu, Tianyang, et al. \"Your contrastive learning is secretly doing stochastic neighbor embedding.\" arXiv preprint arXiv:2205.14814 (2022)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4089/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700257148882,
                "cdate": 1700257148882,
                "tmdate": 1700257480483,
                "mdate": 1700257480483,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MH9kfTpqIU",
                "forum": "YgMdDQB09U",
                "replyto": "OhKQMnIh80",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4089/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4089/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Gentle reminder"
                    },
                    "comment": {
                        "value": "Dear reviewer vuAt, \n\nWe express our gratitude for your dedicated efforts in reviewing and providing valuable suggestions. Despite the prolonged discussion period, we have yet to receive feedback on your response. Kindly note that we have concluded the pretraining of Resnet-50, as outlined in the main response, and have incorporated the necessary updates into the PDF (Appendix, Table 6), including minor formatting adjustments. We would really appreciate it if you can let us know if our response resolves your concerns. Additionally, please inform us of any lingering concerns you may have. \n\nBest regards, Authors"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4089/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700678263090,
                "cdate": 1700678263090,
                "tmdate": 1700679778770,
                "mdate": 1700679778770,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "b2PQLLgYPA",
                "forum": "YgMdDQB09U",
                "replyto": "77VVapyywJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4089/Reviewer_vuAt"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4089/Reviewer_vuAt"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors"
                    },
                    "comment": {
                        "value": "Thanks for the clarification and additional results. I agree this work is an effective method to make contrastive learning more robust to batch size. I have raised my score to 6.\n\nHowever, I am also aware that the authors claim to accelerate the model convergence. There have already been some works [1][2][3] targeting model convergence acceleration in self-supervised learning, which need to be discussed. Specifically, [1] claims to accelerate contrastive learning 8 times, which is an impressive number.\n\n[1] Ci, Yuanzheng, et al. \"Fast-MoCo: Boost momentum-based contrastive learning with combinatorial patches.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022.\n\n[2] Addepalli, Sravanti, et al. \"Towards Efficient and Effective Self-Supervised Learning of Visual Representations.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022.\n\n[3] Ko\u00e7yi\u011fit, Mustafa Taha, Timothy M. Hospedales, and Hakan Bilen. \"Accelerating Self-Supervised Learning via Efficient Training Strategies.\" Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2023."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4089/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700710359144,
                "cdate": 1700710359144,
                "tmdate": 1700710359144,
                "mdate": 1700710359144,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AZNMjQ8amy",
                "forum": "YgMdDQB09U",
                "replyto": "OhKQMnIh80",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4089/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4089/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you so much for your feedback"
                    },
                    "comment": {
                        "value": "Dear reviewer vuAt,\n\nThank you so much for your response! We shall duly incorporate a discussion drawing parallels between our method and the suggested approaches.\n\nThanking you,\nAuthors"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4089/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700713703403,
                "cdate": 1700713703403,
                "tmdate": 1700713730497,
                "mdate": 1700713730497,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]