[
    {
        "title": "SCHEMA: State CHangEs MAtter for Procedure Planning in Instructional Videos"
    },
    {
        "review": {
            "id": "4ud7IJN3Ny",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3881/Reviewer_7dUf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3881/Reviewer_7dUf"
            ],
            "forum": "abL5LJNZ49",
            "replyto": "abL5LJNZ49",
            "content": {
                "summary": {
                    "value": "This paper proposes a new model called SCHEMA for procedure planning in instructional videos. The model leverages language models and cross-modal contrastive learning to track state changes and establish a more structured state space. The authors conduct experiments on three benchmark datasets and show that SCHEMA outperforms existing methods in terms of state recognition accuracy and task success rate. The paper's contributions include a new approach to procedure planning that accounts for state changes, a novel cross-modal contrastive learning framework, and a new benchmark dataset for evaluating procedure planning models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper has several strengths that make it a valuable contribution to the field of procedure planning in instructional videos:  \n\n1. Originality:  \n\n1.1  The paper proposes a new approach to procedure planning that accounts for state changes, which is a novel idea that has not been explored in previous works.  \n\n1.2 The authors leverage language models and cross-modal contrastive learning to track state changes and establish a more structured state space, which is a creative combination of existing ideas.  \n\n2. Quality:  \n\n2.1 The authors conduct experiments on three benchmark datasets and show that SCHEMA outperforms existing methods in terms of state recognition accuracy and task success rate, which demonstrates the quality of their proposed approach.  \n\n2.2 The paper is well-written and well-organized, making it easy to follow and understand.  \n\n3. Clarity:  \n\n3.1 The authors provide clear explanations of their proposed approach and the experiments they conducted, making it easy for readers to understand their contributions.  \n\n3.2 The paper includes helpful visualizations and tables to illustrate their results and comparisons with existing methods."
                },
                "weaknesses": {
                    "value": "While this paper has several strengths, there are also some weaknesses that could be addressed to improve the work:  \n\n- The paper could benefit from a more detailed discussion of the limitations of the proposed approach. For example, the authors could discuss cases where the model may struggle to recognize state changes or situations where the model may not be applicable.  \n\n- The paper could provide more information on the computational requirements of the proposed approach."
                },
                "questions": {
                    "value": "1. Could you provide more information on the computational requirements of the proposed approach? Specifically, what hardware and software were used to train and run the model, and how long did it take to train the model? \n\n2. How does the proposed approach handle cases where the state changes are not explicitly shown in the video? For example, if a video shows a person making a sandwich, but does not show the person adding mayonnaise, how would the model recognize this state change? \n\n3. Can you provide more information on the limitations of the proposed approach? Specifically, are there any cases where the model may struggle to recognize state changes or situations where the model may not be applicable?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3881/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697167599956,
            "cdate": 1697167599956,
            "tmdate": 1699636346781,
            "mdate": 1699636346781,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yDSx3zbBwI",
                "forum": "abL5LJNZ49",
                "replyto": "4ud7IJN3Ny",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3881/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3881/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 7dUf"
                    },
                    "comment": {
                        "value": "Dear Reviewer 7dUf,\n\nThank you for your detailed comments and suggestions. We tried our best to address all the concerns and questions, and update the main paper and appendix in the new version. Please let us know if you have any further concerns or questions to discuss.\n\nBest,\n\nPaper 3881 Authors\n\n---\n\n**W1&Q3. (Discussion of Limitations) The paper could benefit from a more detailed discussion of the limitations of the proposed approach. For example, the authors could discuss cases where the model may struggle to recognize state changes or situations where the model may not be applicable.**\n\nA: A detailed discussion of the limitations is as follows.\n\n* The limitations of our method are as follows. First, the model may fail to identify state changes if they are not explicitly shown in the video. This is a general challenge for procedure planning models, as the details of objects may be hard to recognize if they are far from the camera. Although we tried to tackle this challenge by associating non-visible state changes with language descriptions, which is learned from the paired vision-text training data, there is no guarantee that the non-visible state changes issue can be totally removed. A potential direction is to generalize the task by taking long-term videos as the input visual observations, so that the model can infer the state changes from video history or ASR narrations. Second, the quality of state descriptions relies on LLMs, which would be a bottleneck of state representation. We will explore more effective and reliable methods to leverage LLMs' commonsense knowledge, or utilize generative vision-language models (VLMs) for state description generation. Third, our approach is under a close-vocabulary setting. We will explore the open-vocabulary setting in the future.\n\nWe included this discussion in Sec. C of the revised appendix.\n\n---\n\n**W2&Q1. (Computational requirements) The paper could provide more information on the computational requirements of the proposed approach. Specifically, what hardware and software were used to train and run the model, and how long did it take to train the model?** \n\nA: The computation requirements are as follows.\n\n* The training process takes 1 hour (500 epochs) on CrossTask and 5.5 hours (400 epochs) on COIN using a single V100 GPU.\n\nWe included these details in the paragraph \"Training Details\" in Appendix Sec. A.\n\n**Q2. (Non-visible state changes) How does the proposed approach handle cases where the state changes are not explicitly shown in the video? For example, if a video shows a person making a sandwich, but does not show the person adding mayonnaise, how would the model recognize this state change?** \n\nA: Please see the answer to W1&Q3.\n\n* The model may fail to identify state changes if they are not explicitly shown in the video. This is a general challenge for procedure planning models, as the details of objects may be hard to recognize if they are far from the camera. Although we tried to tackle this challenge by associating non-visible state changes with language descriptions, which is learned from the paired vision-text training data, there is no guarantee that the non-visible state changes issue can be totally removed. A potential future direction is to generalize the task by taking long videos as the input visual observations, so that the model can infer the state changes from video history or ASR narrations. \n\nWe included this discussion in Sec. C of the revised appendix."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3881/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700032417608,
                "cdate": 1700032417608,
                "tmdate": 1700032417608,
                "mdate": 1700032417608,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "G2tyLOB53s",
                "forum": "abL5LJNZ49",
                "replyto": "yDSx3zbBwI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3881/Reviewer_7dUf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3881/Reviewer_7dUf"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your reply. The explanation in the reply and the overall comment make good sense. I shall retain my rating."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3881/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700466366879,
                "cdate": 1700466366879,
                "tmdate": 1700466366879,
                "mdate": 1700466366879,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mXceMaGSR1",
            "forum": "abL5LJNZ49",
            "replyto": "abL5LJNZ49",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3881/Reviewer_FxtF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3881/Reviewer_FxtF"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel procedural planning method that models the task elegantly as a joint probability of time-series states and actions conditioned by start and end states. The method fills the gap of ungiven states before and after each action by LLM with a Chain-of-though prompt. Ground truth of actions and the estimated states are used to train the model with reasonable loss functions. Experiments show the clear superiority of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The motivation is clear.\n- The presentation is clear.\n- The query design for the state decoder is elegant (a sequence of state vectors, where a known step is the sum of encoded state feature and positional embedding and an unknown step is only positional embedding).\n- Augmenting state description with LLM from action labels is novel.\n- The reported results are thorough and look promising."
                },
                "weaknesses": {
                    "value": "1. Overlooked related work\n\nThe authors overlook two studies focusing on state transition in instructional videos.\n\nFirst, in the paragraph \"Instructional video analysis,\" a dense video captioning method [a] is missing. The method tracks material state change with a MemNet-like architecture [a]. It trains state-modifying actions with distant supervision. It also analyzes the state change obtained as a shift in the latent space. Thus, it definitely relates to this work but is missing.\n\n[a] T. Nishimura et al., \"State-aware Procedural Video Captioning,\" ACMMM, 2021.\n\nSimilarly, in the same paragraph, the authors claimed, \"there are few discussions on state changes in complex videos with several actions, especially instructional videos.\" However, [b] models such complexity of the instructional video as an action graph to retrieve the goal state image with an instructional text (that directs actions) and an image before the action. The authors adequately refer to this study since the work also tries to model state-action relations in complex instructional videos.\n\n[b] K. Shirai et al., \"Visual Recipe Flow: A Dataset for Learning Visual State Changes of Objects with Recipe Flows,\" COLING2022.\n\n2. Minor flaws in presentation.\n\nThe first sentence in 3.3.1 explains about 3.4 but mentions nothing about the content in 3.3.1. This part was confusing for this reviewer.\n\nThe paragraph \"Masked Step Modeling\" in 3.4 claims that \"ground-truth answers $a_t$\"; however, for the readers, it is not known whether ground-truth actions are given at training or not. Please fix this problem.\n \nFYI\nIn Figure 2, there is a type \"oancake.\" However, it is not clear whether the typo is by GPT-3.5 or the authors."
                },
                "questions": {
                    "value": "Please point out any factual errors in this review if the authors find them."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed.",
                        "Yes, Discrimination / bias / fairness concerns",
                        "Yes, Privacy, security and safety",
                        "Yes, Legal compliance (e.g., GDPR, copyright, terms of use)",
                        "Yes, Potentially harmful insights, methodologies and applications",
                        "Yes, Responsible research practice (e.g., human subjects, data release)",
                        "Yes, Research integrity issues (e.g., plagiarism, dual submission)",
                        "Yes, Unprofessional behaviors (e.g., unprofessional exchange between authors and reviewers)",
                        "Yes, Other reasons (please specify below)"
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3881/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698406572327,
            "cdate": 1698406572327,
            "tmdate": 1699636346700,
            "mdate": 1699636346700,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wy7BX0gENj",
                "forum": "abL5LJNZ49",
                "replyto": "mXceMaGSR1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3881/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3881/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer FxtF"
                    },
                    "comment": {
                        "value": "Dear Reviewer FxtF,\n\nThank you for your detailed comments and suggestions. We tried our best to address all the concerns and questions, and update the main paper and appendix in the new version. Please let us know if you have any further concerns or questions to discuss.\n\nBest,\n\nPaper 3881 Authors\n\n---\n\n**W1. Overlooked related work.**\n\nA: Thank you for pointing out these two important references. We followed your suggestions and are pleased to include the following discussion in the related work section of the revision.\n\nSome work in the CV area also investigated the relations between actions and states in videos. Especially, Nishimura et al. (2021) focused on the video procedural captioning task and proposed to model material state transition from visual observation, which is realized by establishing a visual simulator modified from a natural language understanding simulator. Shirai et al. (2022) established a multimodal dataset for object state change prediction, which consists of image pairs as state changes and workflow of receipt text as an action graph.\n\n---\n\n**W2.1. (Minor flaws in presentation) The first sentence in 3.3.1 explains about 3.4 but mentions nothing about the content in 3.3.1. This part was confusing for this reviewer.**\n\nA: We are sorry for the confusion. At the beginning of Sec. 3.4, we mentioned that \"state space learning that aligns visual observations with language descriptions\". The paragraph \"State Space Learning\" explained how we do vision-language alignment, which is first mentioned in Sec. 3.3.1.\n\n---\n\n**W2.2 (Minor flaws in presentation)** **The paragraph \"Masked Step Modeling\" in 3.4 claims that \"ground-truth answers $a_t$\"; however, for the readers, it is not known whether ground-truth actions are given at training or not. Please fix this problem.**\n\nA: As shown in Figure 1 (c) and (d), in our setting, we have ground-truth action sequence as supervision at the training stage. Therefore, the ground-truth actions are given at the training stage.\n\n---\n\n**W2.3 (Minor flaws in presentation) In Figure 2, there is a type \"oancake.\" However, it is not clear whether the typo is by GPT-3.5 or the authors.**\n\nA: The typo is by the authors. We have corrected it in the revision."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3881/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700032007592,
                "cdate": 1700032007592,
                "tmdate": 1700032007592,
                "mdate": 1700032007592,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dcpFsPFxJp",
                "forum": "abL5LJNZ49",
                "replyto": "mXceMaGSR1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3881/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3881/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to discussion (Due Nov 22nd)"
                    },
                    "comment": {
                        "value": "Dear Reviewer FxtF,\n\nWe sincerely thank you for your efforts and time in our work. We tried our best to address all the concerns and questions you raised. We have also updated the main paper and appendix following your comments. Please feel free to let us know if you have any further concerns or questions, and we are happy to discuss them with you.\n\nBest,\n\n#3881 Authors"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3881/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700601377908,
                "cdate": 1700601377908,
                "tmdate": 1700601377908,
                "mdate": 1700601377908,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BSWQrKXQVe",
            "forum": "abL5LJNZ49",
            "replyto": "abL5LJNZ49",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3881/Reviewer_hqkF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3881/Reviewer_hqkF"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new framework for procedure planning in instructional videos called SCHEMA, which leverages LLM and cross-modal contrastive learning to track state changes and establish a more structured state space. The authors introduce a chain-of-thought prompting approach to describe state changes and use a mid-state prediction module to improve performance. The SCHEMA model is evaluated on three benchmark instruction video datasets, CrossTask, COIN, and NIV, and achieves state-of-the-art performance in terms of SR, mAcc, and mIoU."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "**Originality**: The authors propose a new framework for procedure planning in instructional videos that emphasizes the importance of state changes, which is a novel approach to the problem formation. The use of chain-of-thought prompting to describe state changes is a creative and effective way to leverage language models for this task. The idea of mid-state prediction module is interesting and seems to improve the performance of the model. \n\n**Quality**: The paper is well-written and well-organized, making it easy to follow and understand. The experiments are thorough and well-designed, with results presented in a clear and concise manner.\n\n**Clarity**: The paper is written in clear language and is easy to understand. The figures and tables are easy to read, providing a clear summary of the results.\n\n**Significance**: The results demonstrate the effectiveness of the proposed approach and suggest that it could be a valuable tool for procedure planning in instructional videos."
                },
                "weaknesses": {
                    "value": "* Novelty: While the paper proposes a new framework for procedure planning in instructional videos, some of the individual components of the framework (such as LLM and cross-modal contrastive learning) are not novel in themselves. I personally loathe the trend that LLM+everything -> novelty. Thus I feel that the contribution of this proposed framework is incremental. That being said, I recognize that the authors have done non-trivial work in incorporating these components and perform thorough experiments. \n\n* Failure cases: The paper does not provide a detailed analysis of the limitations of the proposed approach or potential failure cases. I would be interested in seeing more examples of failures cases and with detailed explanations on why those cases have failed. \n\n* Scaling up: While the proposed approach shows promising results, the paper does not provide a clear explanation of how it could be applied in real-world scenarios or how it could be scaled up to handle larger datasets. Please note that I do not suggest that the model has to be able to handle larger datasets as long term prediction is hard by nature, but an analysis on the model's potential would be useful. \n\n* The paper could benefit from more detailed explanations of the experimental setup and methodology, particularly for readers who wish to replicate the experiments. I find it difficult to replicate the model and experiment based on information provided in appendix A/B."
                },
                "questions": {
                    "value": "* Can you provide more details on the mid-state prediction module? How does it work, and how does it differ from existing mid-state prediction methods?\n\n* Can you provide more examples of failures cases and with detailed explanations on why those cases have failed?\n\n* Can you provide how the model could be scaled up to handle larger datasets?\n\n* Can you provide more detailed explanations of the experimental setup and methodology?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3881/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698686169555,
            "cdate": 1698686169555,
            "tmdate": 1699636346610,
            "mdate": 1699636346610,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oOHSP9blKC",
                "forum": "abL5LJNZ49",
                "replyto": "BSWQrKXQVe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3881/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3881/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer hqkF (Part 1/3)"
                    },
                    "comment": {
                        "value": "Dear Reviewer hqkF,\n\nThank you for your detailed comments and suggestions. We tried our best to address all the concerns and questions, and update the main paper and appendix in the new version. Please let us know if you have any further concerns or questions to discuss.\n\nBest,\n\nPaper 3881 Authors\n\n---\n\n**W1. (Novelty on individual components) While the paper proposes a new framework for procedure planning in instructional videos, some of the individual components of the framework (such as LLM and cross-modal contrastive learning) are not novel in themselves. I personally loathe the trend that LLM+everything -> novelty. Thus I feel that the contribution of this proposed framework is incremental. That being said, I recognize that the authors have done non-trivial work in incorporating these components and perform thorough experiments.** \n\nA: Our research motivation is to rethink the role of state understanding in procedure planning in instructional videos. The main idea is to represent step as state changes in procedures, and the individual components are established to realize different functions. For example, LLM is to transform step names into state change descriptions, cross-modal contrastive learning is to align visual state observations with language state descriptions, and the transformer model is to predict mid-states and steps. To highlight the main idea, we establish a simple and clean pipeline to avoid complex component designs.\n\nWe respectively disagree that this paper is about \"LLM+everything -> novelty\". First, our work is not a simple LLM extension work. We only use LLM to automatically generate state descriptions as supervision. This can be understood as *distilling the symbolic commonsense knowledge* [A] from LLM to smaller models. We use LLM rather than expert annotation because the usage of LLM is cheaper to generalize our work to large-scale open-vocabulary datasets, which makes our method flexible and practical. Second, we didn't claim our novelty as \"LLM+something\". Our novelty is to highlight the role of state understanding in procedural planning in instructional videos, i.e., State Changes Matter, and a new representation of procedural step as state changes. LLM is just an intuitive tool to achieve this goal, but one can use other tools like vision-language models in the future.\n\n> [A] Peter West, Chandra Bhagavatula, Jack Hessel, Jena D. Hwang, Liwei Jiang, Ronan Le Bras, Ximing Lu, Sean Welleck, Yejin Choi. Symbolic Knowledge Distillation: from General Language Models to Commonsense Models. NAACL 2022.\n\n---\n**W2&Q2. (Failure cases) The paper does not provide a detailed analysis of the limitations of the proposed approach or potential failure cases. I would be interested in seeing more examples of failures cases and with detailed explanations on why those cases have failed.** \n\nA: In the revision, we provided a detailed analysis of failure cases. The examples are shown in Figure 8 of the revision. We grouped them into three cases:\n\n(1) Failed understanding of start/end state. As shown in Figure 8(a), the model predicted \"season steak\" as the first step because it didn't recognize that there is pepper on top of the steak, i.e., it failed to understand the start state. As shown in Figure (b), the model predicted \"flip steak\" rather than \"put steak on grill\" as the second step. The way to distinguish these two steps is whether the steak has its top side grilled. Although the end state shows that the top side of the steak is raw, the steak is very small to be captured. One future solution is to use high-resolution video frames or object detector to ground the object.\n\n(2) Hallucination. As shown in Figure 8(c), the model predicts \"add strawberries to cake\" as the third step. However, the goal is not to make strawberry cake. This failure may be due to the training priors as there are many videos for the task of \"make french strawberry cake\".\n\n(3) Reasonable but not matched with ground-truth plans. As shown in Figure 8(d), the generated plan is reasonable, although it doesn't exactly match the ground-truth annotation. This \"failure\" indicates that this task needs a better evaluation protocol for all the reasonable results, which is a general challenge for sequence generation tasks.\n\nWe included this analysis in the \"Failure Case Analysis\" in Sec. C of the revised appendix."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3881/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700031592316,
                "cdate": 1700031592316,
                "tmdate": 1700031592316,
                "mdate": 1700031592316,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0ODG1uXKG4",
                "forum": "abL5LJNZ49",
                "replyto": "BSWQrKXQVe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3881/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3881/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to discussion (Due Nov 22nd)"
                    },
                    "comment": {
                        "value": "Dear Reviewer hqkF,\n\nWe sincerely thank you for your efforts and time in our work. We tried our best to address all the concerns and questions you raised. We have also updated the main paper and appendix following your comments. Please feel free to let us know if you have any further concerns or questions, and we are happy to discuss them with you.\n\nBest,\n\n#3881 Authors"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3881/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700601340415,
                "cdate": 1700601340415,
                "tmdate": 1700601340415,
                "mdate": 1700601340415,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NjkWUWNSg5",
            "forum": "abL5LJNZ49",
            "replyto": "abL5LJNZ49",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3881/Reviewer_253f"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3881/Reviewer_253f"
            ],
            "content": {
                "summary": {
                    "value": "The paper focuses on the task of procedure planning in instructional videos. Given an initial visual state and a goal visual state as input, the model is tasked with generating a sequence of action steps to form a procedure plan, guiding the progression from the initial visual state to the goal state. The authors highlight the significance of states in these procedures and introduce State CHangEs MAtter (SCHEMA) to model state changes. Specifically, they prompt pre-trained large language models to describe the state changes at each step, enhancing learning the intermediate state and step representations. Then, they use cross-modal contrastive learning to align the visual state observations with language state descriptions. Experiments validate that the proposed method achieves  state-of-the-art performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper introduces a novel approach to address the task of procedure planning in instructional videos, placing a strong emphasis on state changes and utilizing pre-trained large language models. The motivations and ideas presented in this paper are reasonable.\n\nThe proposed method has achieved noticeable performance gains."
                },
                "weaknesses": {
                    "value": "1. The clarity and composition of the paper could be enhanced. Please refer to the questions below.\n\n2. There has been a notable surge in research exploring the use of pre-trained large language models (LLMs) for video-related tasks, e.g., [1]. This submission aligns with this emerging trend, and its overarching idea is conceptually sound. However, the fairness of the comparisons drawn in the paper could become questionable due to the employment of LLMs. Further in-depth discussion and analysis may be necessary to fully understand the extent of the LLM\u2019s impact on the final results.\n\n\n[1] Zhao, Qi, Ce Zhang, Shijie Wang, Changcheng Fu, Nakul Agarwal, Kwonjoon Lee, and Chen Sun. \"AntGPT: Can Large Language Models Help Long-term Action Anticipation from Videos?.\" arXiv preprint arXiv:2307.16368 (2023)."
                },
                "questions": {
                    "value": "1. How does aligning visual state observations with language state descriptions *track* state changes? This process involves cross-modal contrastive learning; it is unclear how it could facilitate *tracking* over state changes.\n\n2. Why are step descriptions not utilized as external memory for the step decoder, while *state* descriptions are used instead? The same $D_s$ is employed in Sections 3.3.2 and 3.3.3.\n\n3. In Sec. 3.4, there are $a_i$ and $A_i$. Could you clarify how these two differ and specifically define $A_i$?\n\n4. In State Space Learning via vision-language alignment, is it necessary for the training data to include temporally localized states or actions corresponding to the intermediate states of procedure plans? While I presume the answer is no, Fig. 4(a) and Sec 3.4 leave some room for ambiguity.\n\n5. Why is Eq. (5) called \u201cMasked State Modeling\u201d? The method described does not involve any mask-based modeling or random masking; instead, it is just predicting intermediate locations in a given sequence. The use of the phrase \u201cMasked State/Step Modeling\u201d seems to be an overstatement.\n\n6. What does \u201cDCLIP\u201d refer to in Table 5?\n\n7. Could you also present the results on procedure planning metrics in Table 7?\n\nMissing related literature:\n\n- Li, Zhiheng, Wenjia Geng, Muheng Li, Lei Chen, Yansong Tang, Jiwen Lu, and Jie Zhou. \"Skip-Plan: Procedure Planning in Instructional Videos via Condensed Action Space Learning.\" In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 10297-10306. 2023.\n\n\n- Fang, Fen, Yun Liu, Ali Koksal, Qianli Xu, and Joo-Hwee Lim. \"Masked Diffusion with Task-awareness for Procedure Planning in Instructional Videos.\" arXiv preprint arXiv:2309.07409 (2023)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3881/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3881/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3881/Reviewer_253f"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3881/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698815441509,
            "cdate": 1698815441509,
            "tmdate": 1700611262039,
            "mdate": 1700611262039,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vWmvDSnSnq",
                "forum": "abL5LJNZ49",
                "replyto": "NjkWUWNSg5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3881/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3881/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 253f (Part 1/3)"
                    },
                    "comment": {
                        "value": "Dear Reviewer 253f,\n\nThank you for your detailed comments and suggestions. We tried our best to address all the concerns and questions, and update the main paper and appendix in the new version. Please let us know if you have any further concerns or questions to discuss.\n\nBest, \n\nPaper 3881 Authors\n\n---\n\n**W1. (Clarity) The clarity and composition of the paper could be enhanced. Please refer to the questions below.**\n\nA: Please see detailed responses to Q1-Q7 below.\n\n---\n\n**W2. (LLM\u2019s impact) The fairness of the comparisons drawn in the paper could become questionable due to the employment of LLMs. Further in-depth discussion and analysis may be necessary to fully understand the extent of the LLM\u2019s impact on the final results.** \n\n---\n\nA: First, the comparisons are fair as our model has a consistent parameter scale as state-of-the-art procedure planning models like P3IV, as we didn't use LLMs as a module of the planning model. Instead, We only use LLM to automatically generate state descriptions as supervision for state representation learning. This can be understood as *distilling the symbolic commonsense knowledge* [A] from LLM to smaller models. Considering the usage of LLMs and the parameter scale of our model, we believe the comparisons are fair.\n\n> [A] Peter West, Chandra Bhagavatula, Jack Hessel, Jena D. Hwang, Liwei Jiang, Ronan Le Bras, Ximing Lu, Sean Welleck, Yejin Choi. Symbolic Knowledge Distillation: from General Language Models to Commonsense Models. NAACL 2022.\n\nSecond, for the impact of LLMs, we provided the results of state-based step classification in Table 5 of the appendix. In short, this preliminary task is to validate the quality of LLM's generated descriptions, which takes the before-state and after-state as input and output the step category (Figure 6). The results in Table 5 demonstrate that LLM itself cannot generate informative descriptions (16.2 for a baseline method DCLIP vs. 16.0 for manual descriptions \"A video of [step]\"), while our chain-of-thought prompting is effective in state descriptions generation (21.1 vs. 16.2 for DCLIP baseline). These results show the impact of LLM with our design prompting on state description generation.\n\nThird, we provided ablation studies to analyze the impact of LLMs' generated state descriptions as supervisions. As shown in Table 4, both vision-language state alignment (i.e., \"State align.\") and mid-state prediction (i.e., \"Mid-state pred.\") significantly contribute to the performance gain, which indicates the impact of state description supervision on the final results.\n\n**Q1. (Vision-language alignment for state changes tracking) How does aligning visual state observations with language state descriptions *track* state changes? This process involves cross-modal contrastive learning; it is unclear how it could facilitate *tracking* over state changes.**\n\nA: State change tracking aims to identify the effects of action steps in a long procedure, i.e., how the states of entities or environment change at different stages. For our task of procedure planning in instructional videos, as the inputs are video frames and the state changes are represented in natural language, state change tracking requires (1) tracking states in text, where we represent each step as LLM's generated descriptions of state changes, resulting in a corpus of state descriptions, (2) identifying states changes in vision, where we match the visual observations with the state descriptions in the text corpus, i.e., aligning visual state observations with language state descriptions. We realize the vision-language alignment by cross-modal contrastive learning."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3881/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700031046398,
                "cdate": 1700031046398,
                "tmdate": 1700031114029,
                "mdate": 1700031114029,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XCYhVpcyZ0",
                "forum": "abL5LJNZ49",
                "replyto": "NjkWUWNSg5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3881/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3881/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to discussion (Due Nov 22nd)"
                    },
                    "comment": {
                        "value": "Dear Reviewer 253f,\n\nWe sincerely thank you for your efforts and time in our work. We tried our best to address all the concerns and questions you raised. We have also updated the main paper and appendix following your comments. Please feel free to let us know if you have any further concerns or questions, and we are happy to discuss them with you.\n\nBest,\n\n#3881 Authors"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3881/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700601304708,
                "cdate": 1700601304708,
                "tmdate": 1700601304708,
                "mdate": 1700601304708,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7pcaJPn8il",
                "forum": "abL5LJNZ49",
                "replyto": "XCYhVpcyZ0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3881/Reviewer_253f"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3881/Reviewer_253f"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the authors' response which addressed most of my questions and concerns, and therefore I'll raise my rating accordingly. However, I would like to suggest that the authors consider making the following revisions:\n\n(1) I wouldn't call \"align the visual state observations with language state descriptions via cross-modal contrastive learning\" is \"for state changes tracking\"; it is more accurate and easier for readers to understand if you just say \"**for state representation**, ...  we align the visual state observations with language state descriptions via cross-modal contrastive learning\". \n\n(2) The sentence before Equation (4) is incomplete, i.e., \"we regard the language descriptions with the same state as positive samples, and take descriptions as negative samples\". It should be \"take descriptions of the other states as negative samples\"."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3881/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700611228806,
                "cdate": 1700611228806,
                "tmdate": 1700611228806,
                "mdate": 1700611228806,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]