[
    {
        "title": "Towards Realistic Unsupervised Fine-tuning with Vision-Language Models"
    },
    {
        "review": {
            "id": "w595MejieA",
            "forum": "k2a2aPOA4b",
            "replyto": "k2a2aPOA4b",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2480/Reviewer_spW5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2480/Reviewer_spW5"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces Universal Entropy Optimization (UEO), a method for unsupervised universal fine-tuning of vision-language models (VLMs) like CLIP. UEO aims to enhance the model's performance in two key aspects: accurate classification of samples from known classes and effective identification of samples from classes not present in the predefined classes. It does this by leveraging sample-level confidence and entropy optimization to handle out-of-distribution (OOD) samples. The paper presents results from experiments conducted across 15 domains, demonstrating that UEO outperforms baseline methods in terms of both generalization and OOD detection."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "**Originality**\n\nThe paper proposes a simple yet efficient solution to address a unique and realistic setting of unsupervised universal fine-tuning. According to the authors, this is the first paper to tackle this practical setting. While the key principle of the method is similar to DANCE, unlike DANCE, the proposed approach does not require hyper-parameter selections, which can be challenging in the unsupervised setting.\n\n**Quality**\n\nThis paper exhibits notable strengths in its hyper-parameter-free approach, Universal Entropy Optimization (UEO), which addresses the challenging task of unsupervised fine-tuning of vision-language models (VLMs) under real-world conditions, including potential out-of-distribution (OOD) samples in unlabeled data. Through comprehensive experiments conducted across diverse domains and the introduction of novel evaluation metrics like the AUC score, the paper showcases the effectiveness of UEO in both in-distribution classification and OOD detection. UEO's parameter-efficient methodology and emphasis on real-world scenarios make it a valuable contribution, marking its quality in the field of VLMs and unsupervised fine-tuning.\n\n\n**Clarity**\n\nThe paper is well-written and easy to follow. The motivation of the paper is clear. The authors build upon previous works and cite them appropriately. \n\n**Significance**\n\nThe proposed setting is practical. The paper tackles the problem where unknown classes can be present in the unlabeled data, replicating real-world scenarios. The analysis is thorough and the experiments across multiple settings show consistent improvements."
                },
                "weaknesses": {
                    "value": "One potential weakness of the paper is that it relies on sample-level confidence weights to approximate entropy minimization and maximization. While this approach is innovative, it may be sensitive to the distribution of confidences within the unlabeled data. If the confidences are not well-calibrated or vary significantly across samples, it could affect the effectiveness of UEO. The performance of UEO might be influenced by the quality and reliability of the confidence estimates, and if the confidence estimates are noisy or inaccurate, it could lead to suboptimal results.\n\nOne experiment that is missing would involve applying the method to VLMs that are not well-calibrated and observing its impact on the performance of the fine-tuned model.\n\nAnother potential drawback is that, when compared to InfoMax, there does not appear to be a significant improvement in accuracy on certain datasets. For instance, on the DomainNet dataset, there is no noticeable difference in performance in accuracy."
                },
                "questions": {
                    "value": "1. Can the universal entropy optimization loss be potentially incorporated during the contrastive pre-training phase as well? \n\n2. Apart from being hyper-parameter free, could the author elaborate more on the benefit of using the proposed universal entropy optimization instead of the entropy separation loss in DANCE? \n\n3. Since the method relies on the confidence of the model, could the authors discuss the effect of calibration of predicted probabilities on the performance of the method? \n\nI would be willing to raise my score if the authors could address my concerns and answer the questions above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2480/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2480/Reviewer_spW5",
                        "ICLR.cc/2024/Conference/Submission2480/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2480/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697788588895,
            "cdate": 1697788588895,
            "tmdate": 1700386485530,
            "mdate": 1700386485530,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iEZuupTsRh",
                "forum": "k2a2aPOA4b",
                "replyto": "w595MejieA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2480/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2480/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for providing valuable comments and address your concerns in the following responses.\n\n> **[Q1]**. While this approach is innovative, it may be sensitive to the distribution of confidences within the unlabeled data. If the confidences are not well-calibrated or vary significantly across samples, it could affect the effectiveness of UEO.\n\n**[A1]**. We agree on the significance of the instance weight $w(x)$. However, UEO has been empirically demonstrated to be not sensitive to confidences. To assess the calibration of instance weights, we utilize the AUC score as an indicator. For instance, when comparing the results on the DN (Avg.) dataset under open-partial shift with two different backbones (ResNet-50 in Table 4 and ViT-B/16 in Table 5), UEO consistently outperforms the CLIP baseline in terms of the accuracy (ACC) score. It is worth noting that improved calibration may not necessarily result in larger gains. Similar observations can be made when comparing different datasets under the same backbone.\n\n|  DN (Avg.) [open-partial]    | ACC  | AUC  |\n|----------------|------|------|\n| CLIP (ResNet-50) | 47.3 | 66.5 |\n| UEO (ResNet-50)  | 51.9 | 67.4 |\n| CLIP (ViT-B/16) | 58.2 | 72.6 |\n| UEO (ViT-B/16)  | 62.0 | 72.9 |\n\n> **[Q2]**. One experiment that is missing would involve applying the method to VLMs that are not well-calibrated and observing its impact on the performance of the fine-tuned model.\n\n**[A2]**. As obtaining a poorly calibrated VLM is challenging, we kindly refer the reviewer to **[A5] @ Reviewer eDVD** for a visual pre-trained source model (VM). Carefully comparing the gains of UEO over the initized model in two following tables, we find that the AUC scores vary a lot across different models, and UEO always increases in terms of the ACC score.\n\n|  OF(A) [open-partial]  | ACC  | AUC  |\n|----------------|------|------|\n| VM | 65.2 | 71.8 |\n| UEO(VM)| 70.2 |\t74.1 |\n| CLIP| 76.6 | 84.8 |\n| UEO(CLIP)  | 80.2 | 84.5 |\n\n|  OH(A) [open-partial]  | ACC  | AUC  |\n|----------------|------|------|\n| VM | 54.5\t|64.5 |\n| UEO(VM)| 58.2 |\t64.7 |\n| CLIP| 70.2 |74.9 |\n| UEO(CLIP)  | 72.3 | 75.0 |\n\n> **[Q3]**. When compared to InfoMax, there does not appear to be a significant improvement in accuracy on certain datasets. For instance, on the DomainNet dataset, there is no noticeable difference in performance in accuracy.\n\n**[A3]**. We respectfully disagree. As stated in **[A2] @ Reviewer pTa8**, UEO consistently outperforms InfoMax when considering both accuracy (ACC) and area under the curve (AUC) together. While the difference between UEO and InfoMax on the DomainNet dataset may appear small, UEO consistently exhibits superior AUC scores under different category shifts and backbones. \n\n> **[Q4]**. Can the universal entropy optimization loss be potentially incorporated during the contrastive pre-training phase as well?\n\n**[A4]**. That sounds like a reasonable and interesting direction for future work. When we have paired text-image pairs along with some unpaired images during the pre-training phase, we can take the texts from the pairs as pre-defined class names and incorporate a simialr loss on these unpaired images with the contrastive pre-training loss over image-text pairs.\n\n> **[Q5]**. Apart from being hyper-parameter free, could the author elaborate more on the benefit of using the proposed universal entropy optimization instead of the entropy separation loss in DANCE?\n\n**[A5]**. In **[A4] @ Reviewer eDVD**, we show the disadvatnages of thresholding-based methods. Thresholding-based methods, including the entropy separation loss in DANCE, indeed rely heavily on the chosen threshold, and their performance can be sensitive to the absence of OOD samples, especially in scenarios like closed-set and partial-set category shifts where OOD samples are not present. Mistakenly classifying hard ID samples as OOD samples can introduce risks and impact the overall effectiveness of these methods."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2480/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700162547595,
                "cdate": 1700162547595,
                "tmdate": 1700162547595,
                "mdate": 1700162547595,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fUCSAT4GYc",
                "forum": "k2a2aPOA4b",
                "replyto": "iEZuupTsRh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2480/Reviewer_spW5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2480/Reviewer_spW5"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal response"
                    },
                    "comment": {
                        "value": "Based on the author's response and other reviewers' comments, I decided to increase my score."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2480/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700386693191,
                "cdate": 1700386693191,
                "tmdate": 1700386693191,
                "mdate": 1700386693191,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "D2mV4YQOSc",
            "forum": "k2a2aPOA4b",
            "replyto": "k2a2aPOA4b",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2480/Reviewer_pTa8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2480/Reviewer_pTa8"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel task setting for unsupervised CLIP fine-tuning, where the label spaces of unlabeled data and predefined text classes are partially overlapped. As a result, the trained model is required to concurrently detect out-of-distribution categories while recognizing samples within the predefined classes. To address this challenge, the paper proposes a straightforward approach that aims to minimize the conditional entropy of confident samples and maximize the marginal entropy of less confident ones. Experiments are performed on benchmark datasets."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. To the best of my knowledge, the proposed task setting is novel. I believe it offers a valuable direction for unsupervised CLIP adaptation.\n2. The proposed approach is straightforward and results in a general improvement.\n3. Experiments are carried out on widely accepted DA benchmarks."
                },
                "weaknesses": {
                    "value": "1. As highlighted in the introduction and method sections, the paper's primary focus is the class discrepancy between unlabeled data and the predefined label space. However, the principal experiments are based on domain adaptation datasets, characterized predominantly by distributional differences between domains. I believe more general classification datasets (e.g., ImageNet and SUN397 as in CoOp) should be employed to define the task setting and verify the efficacy of the proposed method.\n2. The introduced method bears a significant resemblance to existing mutual information maximization losses. The sole distinction appears to be the instance weight, which is based on maximum prediction probability. Additionally, the performance gains seem rather marginal when compared with its peers.\n3. Missing ablation studies. The methodology encompasses two critical components: firstly, the entropy-related training objective, and secondly, the parameter-efficient tuning. I'm uncertain if the competing methods in Tab. 1 also implement the same parameter-efficient tuning. Furthermore, the prompt and affine parameters of the parameter-efficient tuning should be dissected and examined individually.\n4. Missing references. Some related studies delve into various task settings, such as black-box DA in [1], partial DA in [2], and universal DA in [3].\n\n[1] Unsupervised Domain Adaptation of Black-Box Source Models, BMVC21\n[2] Universal domain adaptation, CVPR19\n[3] Partial adversarial domain adaptation, ECCV18"
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2480/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2480/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2480/Reviewer_pTa8"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2480/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698389746641,
            "cdate": 1698389746641,
            "tmdate": 1700449967765,
            "mdate": 1700449967765,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "slIM2dGG8g",
                "forum": "k2a2aPOA4b",
                "replyto": "D2mV4YQOSc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2480/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2480/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for providing valuable comments and address your concerns in the following responses.\n\n> **[Q1]**. I believe more general classification datasets (e.g., ImageNet and SUN397 as in CoOp) should be employed to define the task setting and verify the efficacy of the proposed method.\n\n**[A1]**. A great advice. To validate the effectiveness of the proposed method (UEO), we additionally utilize three widely recognized classification datasets (i.e., ImageNet, SUN397, and Food101) and present the results under four category shifts in the tables below.\n\n| ImageNet | ACC (C) | AUC (C) | ACC (P) | AUC (P) | ACC (O) | AUC (O) | ACC (OP) | AUC (OP) | ACC (Avg.) | AUC (Avg.) |\n|---------|---------|---------|---------|---------|---------|---------|---------|---------|---------|---------|\n| CLIP    | 63.8 | 73.8 | 63.8 | 73.8 | 63.8 | 73.8 | 63.8 | 73.8    | 63.8 | 73.8 |\n| UPL     | 65.6 | 73.6 | 65.2 | 73.9 | 65.8 | 73.5 | 65.6 | 72.9    | 65.5 | 73.5 |\n| POUF    | 65.7 | 72.0 | 65.2 | 72.8 | 65.7 | 71.0 | 65.8 | 71.7    | 65.6 | 71.9 |\n| DANCE   | 64.7 | 72.8 | 64.3 | 73.3 | 64.9 | 72.1 | 64.4 | 72.6    | 64.6 | 72.7 |\n| EntMin  | 64.0 | 73.5 | 64.1 | 73.8 | 63.9 | 73.1 | 63.8 | 73.3    | 63.9 | 73.4 |\n| InfoMax | 66.3 | 71.8 | 65.4 | 73.4 | 65.1 | 64.5 | 65.3 | 68.3    | 65.5 | 69.5 |\n| UEO     | 65.9 | 73.0 | 65.3 | 74.2 | 65.9 | 72.4 | 65.9 | 73.2    | 65.7 | 73.2 |\n\n| SUN397 | ACC (C) | AUC (C) | ACC (P) | AUC (P) | ACC (O) | AUC (O) | ACC (OP) | AUC (OP) | ACC (Avg.) | AUC (Avg.) |\n|---------|---------|---------|---------|---------|---------|---------|---------|---------|---------|---------|\n| CLIP    | 63.7 | 68.6 | 63.7 | 68.6 | 63.7 | 68.6 | 63.7 | 68.6    | 63.7 | 68.6 |\n| UPL     | 66.0 | 69.0 | 65.6 | 69.0 | 66.2 | 69.1 | 66.0 | 69.0    | 65.9 | 69.0 |\n| POUF    | 66.2 | 69.0 | 65.9 | 69.0 | 66.2 | 68.9 | 66.1 | 69.0    | 66.1 | 69.0 |\n| DANCE   | 64.8 | 68.9 | 64.6 | 68.8 | 64.9 | 68.9 | 64.8 | 68.8    | 64.8 | 68.8 |\n| EntMin  | 64.1 | 68.7 | 64.0 | 68.6 | 64.1 | 68.7 | 64.1 | 68.6    | 64.1 | 68.6 |\n| InfoMax | 67.0 | 68.7 | 66.8 | 68.8 | 67.0 | 68.5 | 67.0 | 68.6    | 67.0 | 68.7 |\n| UEO     | 66.5 | 68.9 | 66.4 | 69.0 | 66.7 | 68.8 | 66.6 | 68.8    | 66.5 | 68.9 |\n\n| Food101 | ACC (C) | AUC (C) | ACC (P) | AUC (P) | ACC (O) | AUC (O) | ACC (OP) | AUC (OP) | ACC (Avg.) | AUC (Avg.) |\n|---------|---------|---------|---------|---------|---------|---------|---------|---------|---------|---------|\n| CLIP    | 79.7 | 77.6 | 79.7 | 77.6 | 79.7 | 77.6 | 79.7 | 77.6    | 79.7 | 77.6 |\n| UPL     | 80.2 | 77.7 | 80.6 | 77.3 | 80.3 | 78.0 | 81.2 | 78.4    | 80.6 | 77.8 |\n| POUF    | 81.5 | 77.6 | 81.1 | 77.6 | 81.5 | 77.2 | 81.3 | 77.2    | 81.3 | 77.4 |\n| DANCE   | 81.0 | 77.4 | 80.4 | 77.4 | 80.4 | 76.3 | 80.0 | 76.2    | 80.4 | 76.8 |\n| EntMin  | 80.1 | 77.6 | 79.9 | 77.6 | 80.0 | 77.2 | 79.9 | 77.1    | 80.0 | 77.4 |\n| InfoMax | 82.5 | 77.1 | 82.1 | 77.7 | 82.6 | 76.5 | 82.3 | 76.0    | 82.4 | 76.8 |\n| UEO     | 82.4 | 77.9 | 81.9 | 77.9 | 82.3 | 77.4 | 82.1 | 77.7    | 82.2 | 77.7 |\n\n[(C): closed-set, (P): partial-set, (O): open-set, (OP): open-partial]\n\nOn the ImageNet dataset, UEO and UPL are top two methods when considering both ACC and AUC simultaneously. Similarly, UEO and InfoMax are top two methods on the SUN397 dataset. Notably, UEO achieves the best performance on the FOOD101 dataset. In summary, UEO consistently attains the best or competitive performance across different datasets, while other methods exhibit more variability across the datasets."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2480/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700159397461,
                "cdate": 1700159397461,
                "tmdate": 1700159397461,
                "mdate": 1700159397461,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "P5BqrzVSvk",
                "forum": "k2a2aPOA4b",
                "replyto": "D2mV4YQOSc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2480/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2480/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> **[Q2]**. The introduced method bears a significant resemblance to existing mutual information maximization losses. Additionally, the performance gains seem rather marginal when compared with its peers.\n\n**[A2]**. We acknowledge that a direct comparison of the objective in Equation (4) with InfoMax [Liang et al., ICML 2020] reveals the sole difference lies in the instance weight $w(x)$. However, it is crucial to note that the motivation behind this choice is entirely different. We explain the relaxation from Eq. (3) to Eq. (4) in **[A3] @ Reviewer eDVD**. Concerning the performance gains, we collect the average results of InfoMax and UEO from Tables 1-4 in the table below. It appears that UEO outperforms InfoMax in a majority of cases, with only a few cases where it is inferior to InfoMax.\n\n|         | DN   | DN   | VD   | VD   | OH   | OH   | OF   | OF   |\n|---------|------|------|------|------|------|------|------|------|\n| (C)     | ACC  | AUC  | ACC  | AUC  | ACC  | AUC  | ACC  | AUC  |\n| InfoMax | 52.0 | 66.3 | 92.6 | 81.4 | 77.0 | 75.8 | **79.2** | 82.5 |\n| UEO     | 51.9 | **67.2** | 92.2 | **84.6** | 76.8 | 75.6 | 78.1 | **84.1** |\n| (P)     | ACC  | AUC  | ACC  | AUC  | ACC  | AUC  | ACC  | AUC  |\n| InfoMax | 51.7 | 66.4 | 88.7 | 79.4 | 76.3 | 75.4 | 76.7 | 83.6 |\n| UEO     | 51.8 | **67.2** | **90.0** | **82.9** | **76.6** | 76.3 | 76.4 | 83.4 |\n| (O)     | ACC  | AUC  | ACC  | AUC  | ACC  | AUC  | ACC  | AUC  |\n| InfoMax | 51.9 | 65.8 | 92.5 | 77.5 | 77.0 | 74.4 | **78.8** | 80.9 |\n| UEO     | 51.9 | **67.4** | 92.6 | **81.2** | 76.8 | 75.0 | 77.9 | **82.9** |\n| (OP)    | ACC  | AUC  | ACC  | AUC  | ACC  | AUC  | ACC  | AUC  |\n| InfoMax | 51.9 | 66.0 | 90.6 | 74.6 | 76.5 | 73.9 | 77.2 | 81.5 |\n| UEO     | 51.9 | **67.4** | **92.0** | **81.2** | 76.5 | **75.6** | 77.2 | **83.2** |\n\n[(C): closed-set, (P): partial-set, (O): open-set, (OP): open-partial]\n\n> **[Q3]**. I'm uncertain if the competing methods in Tab. 1 also implement the same parameter-efficient tuning strategy. Furthermore, the prompt and affine parameters of the parameter-efficient tuning should be dissected and examined individually.\n\n**[A3]**. We feel sorry for any confusion. As written in the last paragrpah in Sec 4.1, all the methods are implmented with the same parameter-efficient tuning techique. In particular, we reduce the learning rate for EntMin due to its rapidly decreasing results.\n\nTo verify the effectiveness of the proposed parameter-efficient tuning strategy, we present results with only text prompt tuning in the Appendix (Tables 14-17). Additionally, we investigate the influence of **different combinations of parameters during the fine-tuning process in Fig. 3**. In these figures, the first two columns denote optimizing the text prompt and visual affine parameters, respectively.\n\n> **[Q4]**. Missing references [1,2,3] about domain adaptation?\n\n**[A4]**. Thanks for your kind reminder. [r1] explores the model adaptation problem in a black-box manner, [r2] considers domain adaptation under open-partial category shift, and [r3] addresses domain adaptation under partial-set category shift. These papers represent various domain adaptation problem setups, and we will include them in the related works section. Given that these methods are tailored to specific category shifts, which significantly differ from the problem setup presented here, direct comparisons with UEO in the experiments prove challenging.\n\n- [r1] Zhang, Haojian, et al. \"Unsupervised Domain Adaptation of Black-Box Source Models.\" In Proc. BMVC 2021.\n- [r2] You, Kaichao, et al. \"Universal domain adaptation.\" Proc. CVPR. 2019.\n- [r3] Cao, Zhangjie, et al. \"Partial adversarial domain adaptation.\" Proc. ECCV. 2018."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2480/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700159470647,
                "cdate": 1700159470647,
                "tmdate": 1700159470647,
                "mdate": 1700159470647,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8NGz80CM9y",
                "forum": "k2a2aPOA4b",
                "replyto": "P5BqrzVSvk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2480/Reviewer_pTa8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2480/Reviewer_pTa8"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal response"
                    },
                    "comment": {
                        "value": "Thank you for your response. Your additional experiments on general datasets have been well received.  I have decided to increase my score slightly. While I appreciate the new task setting, my concerns regarding your proposed method have not been addressed. Firstly, as you acknowledged, the only difference between this method and InfoMax is the instance weight, which limits its technical contributions. Secondly, the improvement of this method compared to the baseline (e.g., CLIP, UPL) is relatively marginal, especially when evaluated on widely recognized large-scale datasets such as ImageNet."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2480/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700449940945,
                "cdate": 1700449940945,
                "tmdate": 1700449940945,
                "mdate": 1700449940945,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NXdAj12Gcw",
            "forum": "k2a2aPOA4b",
            "replyto": "k2a2aPOA4b",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2480/Reviewer_tymK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2480/Reviewer_tymK"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel setting called \"unsupervised universal fine-tuning,\" which involves both in-distribution prediction and out-of-distribution detection. To tackle this problem, the authors presented an approach called \"universal entropy optimization.\" It utilizes the confidence of each sample to minimize the entropy of confident samples but maximize the entropy of confident samples. These combined lead to improvement for both generalization and out-of-distribution detection on benchmarks like DomainNet, VISDA-V, Office-OF, etc."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The problem setup of \"unsupervised universal finetuning\" seems reasonable and is grounded in the disadvantages of previous settings.\n* I think the approach of \"universal entropy optimization\" (UEO), especially Eqn. 3, is interesting in achieving maximization and minimization at the same time. I don't directly work in this field and am not sure whether Eqn. 3 has been used by other people. Nonetheless, I think the UEO approach in the paper is intriguing.\n* The performance demonstrated in the experiment section supports the effectiveness of the approach."
                },
                "weaknesses": {
                    "value": "(Details in the questions section) I think the authors might need to clarify several questions to fully illustrate their novel explorations, including the role of vision-language models and the significance of the new setting. Additionally, the numbers in Table 1-4 are quite close in some datasets (though I admire and appreciate the exhaustive evaluation from the authors), so it is also better to make more clarifications."
                },
                "questions": {
                    "value": "1. What is the special role of the \"vision-language model\" in the paper or the investigated problem? It seems to me the approach and problem-setting are applicable to models beyond CLIP?\n\n2. Following the above question, I think the authors need to better clarify how their experiment setting differs from previous works. Specifically, the authors mentioned \"unsupervised universal fine-tuning\" as a novel fine-tuning setup, but it seems the evaluation directly adopted the previous datasets without special curation. Therefore, I am wondering if this is a new setting, or some previous setting adapted to CLIP, or some other cases?\n\n3. The numbers in the tables are quite close for some datasets, and the performance for UEO is not the best on some datasets, such as  the avg numbers. Therefore, I think clarifications on the following questions would be helpful:\n* Is there a clear baseline of the UEO approach, e.g., some simple modification or fine-tuning strategy to CLIP for this setting?\n* What is the variance of these numbers?\n* Which is the largest and hardest dataset?\n* State-of-the-art is not necessary for me, but the authors might need to investigate more into the difference in performance and offer some insights. Let's take Table 1 for example, the gap between UEO and UPL on OH (Avg.) in quite significant, what might be the cause?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2480/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2480/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2480/Reviewer_tymK"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2480/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698702590511,
            "cdate": 1698702590511,
            "tmdate": 1699636184381,
            "mdate": 1699636184381,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xHEgY5pZ7p",
                "forum": "k2a2aPOA4b",
                "replyto": "NXdAj12Gcw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2480/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2480/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate the reviewer for providing valuable comments, and we address your concerns in the following responses.\n\n> **[Q1]**. What is the special role of the \"vision-language model\" in the paper or the investigated problem? It seems to me the approach and problem-setting are applicable to models beyond CLIP?\n\n**[A1]**. Firstly, our approach aligns with established practices in prior studies, including references [r1, r2], where we explore the application of vision-language models with a specific focus on the CLIP model. Secondly, it is true that our method and the associated problem setup remain reasonable even when utilizing other pre-trained networks, such as traditional model adaptation. We present the corresponding results in **[A5] @ Reviewer eDVD**. However, the utilization of vision-language models like CLIP proves more apt for the proposed Unsupervised Universal Fine-Tuning problem. This is attributed to the advantage of not requiring labeled data collection efforts for training, thanks to the pre-defined list of class names.\n\n- [r1]. Zhou, Kaiyang, et al. \"Learning to prompt for vision-language models.\" International Journal of Computer Vision 130.9 (2022): 2337-2348.\n- [r2]. Du, Yu, et al. \"Learning to prompt for open-vocabulary object detection with vision-language model.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\n\n> **[Q2]**. I think the authors need to better clarify how their experiment setting differs from previous works. I am wondering if this is a new setting, or some previous setting adapted to CLIP, or some other cases?\n\n**[A2]**. We feel sorry for any confusion. We would like to clarify that the proposed unsupervised universal fine-tuning problem **is not derived from other related topics but constitutes a new and intriguing task**. Since the proposed task is closely related to several tasks in the domain adaptation area (i.e., model adaptation and universal domain adaptation), we employ several popular benchmarks (e.g., Office, OfficeHome, and DomainNet) in the domain adaptation area. We provide additional results on more general benchmarks in **[A1] @ Reviewer pTa8**.\n\nTo the best of our knowledge, our task represents the first exploration into both ID generalization and OOD detection for unsupervised fine-tuning under unknown category shifts. Unlike most prior domain adaptation methods that assume knowledge of the category shift in advance and predominantly emphasize the ability of ID generalization, our task is more realistic. In the context of universal domain adaptation tasks, where the target data for adaptation and the test data share the same label space, evaluating the ability of OOD detection becomes challenging (as depcited in Evaluation of Sec. 3.2). Instead, our task introduces a distinct fine-tuning and evaluation protocol, along with different evaluation metrics.\n\n> **[Q3]**. Additionally, the numbers in Table 1-4 are quite close in some datasets (though I admire and appreciate the exhaustive evaluation from the authors), so it is also better to make more clarifications. (four questions ...)\n\n**[A3]**. For each question, we offer a point-to-point response below.\n\n- For the proposed parameter-tuning stragey, we show the results of only prompt tuning in Fig. 3 and Tables 14 - 17 in the Appendix. Regarding the objectives, EntMin and InfoMax could be considered as two baselines for UEO. Further comparisons with two-step thresholding variants are elaborated in **[A4] @ Reviewer eDVD**.\n- Regarding the variance of the numbers, we conducted all the experiments using different seeds and observed a relatively small variance, approximately 0.1 for both ACC and AUC. Therefore, we omitted the variance in these tables\n- Generally, DomainNet could be considered as the largest and hardest dataset in this paper. For instance, the Real domain in the DomainNet dataset comprises approximately 120k images across 345 classes. Notably, the ACC and AUC scores of the original CLIP model are consistently the lowest among all the datasets.\n- In some datasets, UEO might not exhibit a clear superiority over the baselines, primarily for two reasons. Firstly, unsupervised fine-tuning is inherently challenging, and the majority of fine-tuning methods do not significantly outperform the original CLIP model. Secondly, enhancing both ACC and AUC scores concurrently is a difficult task. In most situations, a trade-off exists between these two scores. Notably, UEO consistently improves the ACC score without compromising the AUC score, outperforming other methods across the majority of datasets and category shifts."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2480/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700155523985,
                "cdate": 1700155523985,
                "tmdate": 1700155523985,
                "mdate": 1700155523985,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UcdHOqvOJR",
                "forum": "k2a2aPOA4b",
                "replyto": "NXdAj12Gcw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2480/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2480/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Invitation to further discussion"
                    },
                    "comment": {
                        "value": "Dear reviewer, \n\nWe genuinely appreciate the time and effort you've invested in reviewing our paper. We have carefully provided relevant responses and results to your concerns. We are eager to further discuss with you and gain your insights **before the end of the Author/Reviewer phase**. Please let us know if any aspect of our work remains unclear or if you have additional feedback. \n\nThank you."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2480/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700529600200,
                "cdate": 1700529600200,
                "tmdate": 1700529600200,
                "mdate": 1700529600200,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lXWM1V5K1H",
                "forum": "k2a2aPOA4b",
                "replyto": "UcdHOqvOJR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2480/Reviewer_tymK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2480/Reviewer_tymK"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your clarification and detailed answers! I think this paper has done a lot of work and would be meaningful to the researchers in the related fields. I suggest adding the clarified points above into the paper to better illustrate the main message. I don't have further follow-up questions for now."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2480/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700534484053,
                "cdate": 1700534484053,
                "tmdate": 1700534484053,
                "mdate": 1700534484053,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "hOSy93bANH",
            "forum": "k2a2aPOA4b",
            "replyto": "k2a2aPOA4b",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2480/Reviewer_eDVD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2480/Reviewer_eDVD"
            ],
            "content": {
                "summary": {
                    "value": "This study tackles the problem of finetuning a vision-language model like CLIP on new unlabeled data with samples of unknown classes. To this end, a new approach called universal entropy optimization (UEO) is proposed. UEO utilizes the CLIP output score with known classes to determine whether a sample is an out-of-distribution (OOD) one. Then, the in-distribution samples are optimized following the standard entropy minimization strategy whereas the OOD samples are forced to maximize their prediction entropy. This finetuning process is parameter efficient as only the text prompts are involved during finetuning. Results using various methods across different open-set finetuning scenarios are evaluated, and the proposed strategy is validated to be effective."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The attacked problem of the side effect caused by out-of-distribution samples during unsupervised finetuning is interesting, as it is encountered for many downstream applications of a large pretrained model.\n    \n- The effort of trying to adopt a unified adaptive loss function for both ID and OOD samples are appreciated, even though this goal is not quite accomplished in this study as would be later discussed.\n    \n- This method is validated on both ResNet and Vit-B backbones across various domain adaptation datasets, and comparisons with previous studies indicate a superior performance of the current method."
                },
                "weaknesses": {
                    "value": "- The strategy of entropy minimization for ID samples and entropy maximization for OOD samples have been a popular method[1, 2, 3]. This study applies the principle to the field of vision-language models. Despite its effectiveness on different benchmarks, the core idea resembles traditional ones, which would compromise the novelty of this study.\n    \n- I understand that the authors contribute in a generalized form as in Eq. (3) & (4) for the loss function of both ID and OOD samples. However, a similar principle of maximizing Mutual Information ID samples and penalizing the mutual information of OOD samples has been also proposed in [4].\n    \n- The theoretical derivation of Eq. (3) & (4) could be more explicit and detailed. The current version appears to be intuitive and lack thorough theoretical analysis. Eq. (3) is proposed just to satisfy the rule that minimize the entropy ID instances and maximization the entropy of OOD samples\u201d. However, no theoretical guarantee is provided so that Eq. (3) & (4) would always satisfy the above principle. The explanation is also missing of how Eq. (3) & (4) would be more suitable than a simple stepwise function, e.g. $L_{ID}=H(p(x)$ and $L_{OOD}=-H(p(x))$, and the determination of OOD samples follows the common practice as introduced in Sec. 3.1.\n    \n- As for the scope of application of the proposed method, it appears to be a general OOD method that can be also applied to traditional classification networks. I wonder why this method is applied to only CLIP method instead of extending it to other pretrained backbones.\n    \n\nReferences\n\n1.Chan, R., Rottmann, M. & Gottschalk, H. Entropy Maximization and Meta Classification for Out-of-Distribution Detection in Semantic Segmentation. in *2021 IEEE/CVF International Conference on Computer Vision (ICCV)* 5108\u20135117 (IEEE, 2021).\n\n2.Mac\u00eado, D., Ren, T. I., Zanchettin, C., Oliveira, A. L. I. & Ludermir, T. Entropic Out-of-Distribution Detection. in *2021 International Joint Conference on Neural Networks (IJCNN)* 1\u20138 (2021).\n\n3.Lee, K., Lee, H., Lee, K. & Shin, J. Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples. Preprint at [http://arxiv.org/abs/1711.09325](http://arxiv.org/abs/1711.09325) (2018).\n\n4.Nimah, I., Fang, M., Menkovski, V. & Pechenizkiy, M. ProtoInfoMax: Prototypical Networks with Mutual Information Maximization for Out-of-Domain Detection. in *Findings of the Association for Computational Linguistics: EMNLP 2021* (eds. Moens, M.-F., Huang, X., Specia, L. & Yih, S. W.) 1606\u20131617 (Association for Computational Linguistics, 2021)."
                },
                "questions": {
                    "value": "From my point of view, Eq. (3) can also be viewed as an implicit threshold strategy to determine OOD samples. Specifically, assume of the max softmax probability $w$ follows a uniform assumption $w\\sim \\mathcal U(\\frac{1}{C},1)$ , where $C$ denotes the total number of ID classes. The expectation $\\mathbb E(w)=\\frac{1}{2}(1-\\frac{1}{C^2})$ and $\\mathbb E(\\frac 1 w)=log(C)$. Therefore, $\\tilde w(x) - \\tilde \\Phi(w(x))\\approx \\frac{w}{\\mathcal B_t \\mathbb E(w)} - \\frac{1/w}{\\mathcal B_t \\mathbb E(1/w)}$, and the thereshold for determining whether a sample is OOD now becomes $\\lambda=\\frac{1}{2} (1-\\frac{1}{c^2})\\log(C)$. In other words, Eq. (3) could be also one implicit form of thresholding strategy. I think the author should state the explicit benefit brought by the unified form as in Eq. (3) and (4) compared to a hard thresholding one. For example, we can observe in the form of $\\lambda$ that $\\lambda$ increases with the number of classes $C$ , yet I could not understand the rationale of this property."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2480/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2480/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2480/Reviewer_eDVD"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2480/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698761793021,
            "cdate": 1698761793021,
            "tmdate": 1699707480991,
            "mdate": 1699707480991,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "libcdmG2lz",
                "forum": "k2a2aPOA4b",
                "replyto": "hOSy93bANH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2480/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2480/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for providing valuable comments and address your concerns in the following responses.\n\n> **[Q1]**. The resemblance to previous methods [1,2,3] would compromise the novelty of this study.\n\n**[A1]**. After carefully reviewing these papers [1,2,3], we observe that each of them utilizes a similar strategy, aiming to minimize the entropy of ID samples and maximize the entropy of OOD samples to enhance OOD detection capability. While our method does share a similar high-level idea, it diverges from these approaches in two key aspects.\n\n**The first difference lies in the problem setup**, as our fine-tuning process is executed solely with unlabeled data, without explicit ID and OOD samples. In certain category shifts, such as closed-set and partial-set scenarios, OOD samples may not even exist in the unlabeled data, a condition that was not feasible for previous methods.\n\n**The second difference lies in the ultimate objective**. If we know $w(x)\\to 1$ for ID samples and $w(x)\\to 0$ for OOD samples for **unlabeled** data in advance, Eq. (3) exhibits similarities to prior OOD detection methods [1,2,3]. However, our proposed method optimizes the objective in Eq. (4), representing a more relaxed yet robust alternative to Eq. (3).\n\n> **[Q2]**. However, a similar principle of maximizing Mutual Information ID samples and penalizing the mutual information of OOD samples has been also proposed in [4].\n\n**[A2]**. Apologies for any confusion. Our method is different from [4]. Firstly, the method in [4] adopts the meta-learning framework that splits ID and OOD samples in the meta-training step, retaining the ground truth knowledge of ID and OOD during training. Secondly, the Mutual Information Maximization training objective in [4] is defined as the binary cross-entropy loss between ID and OOD prediction, which is distinct from our entropic loss. To be honest, minimizing the scores for ID samples and maximizing the scores for OOD samples a straightforward solution given the presence of ID and OOD samples in the training process. However, in the context of unsupervised fine-tuning, a key challenge arises in distinguishing ID from OOD samples. To circumvent the need for sensitive thresholding operations, we propose the relaxed optimization objective in Eq. (4).\n\nBeyond the unified objective in Eq. (4), our paper introduces **a novel unsupervised fine-tuning problem setup** that considers both ID generalization and OOD detection. This is especially crucial in handling various category shifts between the training data and the pre-defined label set, making our fine-tuning approach a notable contribution to the field.\nparticularly in the face of various category shifts between the training data and the pre-defined label set. We consider such a fine-tuning problem setup to be a significant contribution to the field. Additionally, we provide **a new parameter-efficient fine-tuning strategy** for CLIP that optimizes not only the text prompts but also the normalization layers in the visual branch.\n\n> **[Q3]**. How do Eq. (3) and Eq. (4) theoretically satisfy the principle \"minimize the entropy of ID instances and maximize the entropy of OOD samples\"?\n\n**[A3]**. As explained above, when $w(x)\\to 1$ for unlabeled ID samples and $w(x)\\to 0$ for unlabeled OOD samples, Eq. (3) becomes the optimal objective that minimizes the entropy of ID instances and maximization the entropy of OOD samples. In the context of the unsupervised fine-tuning problem, where binary labels (\"ID\" or \"OOD\") are not available, we suggest employing the maximum softmax probability as an approximation for the ideal weight, as outlined in Eq. (3).\n\nAs the estimated weight $w(x)$ may not always be accurate \u2014 for example, no OOD samples exist in the unlabeled data under the closed-set and partial-set category shifts - maximizing the entropy can be harmful. Intuitively, we could maximize the entropy of mean prediction over OOD samples instead of the average entropy of individual OOD samples, leveraging the inequality $H(\\mathbb{E}(p(x))) \\geq \\mathbb{E}(H(p(x)))$. Honestly, optimizing Eq. (3) and Eq. (4) does not theoretically guarantee the principle in the challenging unsupervised fine-tuning problem, but we believe the proposed objective in Eq. (4) establishes a simple yet strong baseline, as evidenced in extensive experiments."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2480/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700153823078,
                "cdate": 1700153823078,
                "tmdate": 1700153823078,
                "mdate": 1700153823078,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rGvTZW1W1O",
                "forum": "k2a2aPOA4b",
                "replyto": "hOSy93bANH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2480/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2480/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> **[Q4]**. The explanation is also missing of how Eq. (3) & (4) would be more suitable than a simple stepwise function, e.g. $L_{ID}=H(p(x))$ and $L_{OOD}=-H(p(x))$, and the determination of OOD samples follows the common practice as introduced in Sec. 3.1.\n\n**[A4]**. Thanks for the advice. As written in the common practice in Sec. 3.1, $\\lambda$ is chosen so that a high fraction of ID data (e.g., 90%) is above the threshold. Given the absence of explicit ID data in the unsupervised fine-tuning problem, we exclude it as a baseline method, denoted as **EO-ID**, in our experiments.\n\nFollowing the interesting view from the reviewer, we assume $w\\sim \\mathcal{U}(\\frac{1}{C},1)$, then obtain $\\mathbb{E}(w)=\\frac{1-\\frac{1}{C^2}}{2}\\times\\frac{1}{1-\\frac{1}{C}}$ and $\\mathbb{E}(\\frac{1}{w})=log(C)\\times\\frac{1}{1-\\frac{1}{C}}$. To discover the sample whose entropy is minimized in Eq. (3), we have $\\hat{w}(x) - \\hat{\\Phi}(w(x))>0 \\Rightarrow \\frac{w}{\\mathbb{E}(w)} > \\frac{1/w}{\\mathbb{E}(\\frac{1}{w})} \\Rightarrow w^2\\times \\mathbb{E}(\\frac{1}{w})> \\mathbb{E}(w) \\Rightarrow w > \\sqrt{\\frac{1-\\frac{1}{C^2}}{2log(C)}}$. By the way, the threshold $\\lambda=\\sqrt{\\frac{1-\\frac{1}{C^2}}{2ln(C)}}$ sounds reasonable since it decreases when the number of classes ($C$) increases. We refer to the hard thresholding variant utilizing this implicit threshold as **EO-H**.\n\nHere we conduct a simple comparison on the first domain (Ar) of the OfficeHome dataset and show the results across different category shifts below (**EO** denotes the objective in Eq. (3)).\n\n| Metrics (%) | ACC (C) | AUC (C) | ACC (P) | AUC (P) | ACC (O) | AUC (O) | ACC (OP) | AUC (OP) | ACC (Avg.) | AUC (Avg.) |\n|---------|---------|---------|---------|---------|---------|---------|---------|---------|---------|---------|\n| CLIP    | 70.1 | 74.9 | 70.1 | 74.9 | 70.2 | 74.9 | 70.1 | 74.9 | 70.1 | 74.9 |\n| EO-ID   | 69.9 | 75.7 | 69.8 | 75.6 | 70.4 | 76.3 | 70.4 | 76.3 | 70.1 | 76.0 |\n| EO-H    | 68.8 | 74.7 | 68.8 | 74.4 | 69.6 | 76.2 | 70.1 | 76.0 | 69.3 | 75.3 |\n| EO      | 68.9 | 74.9 | 68.6 | 74.7 | 69.6 | 75.8 | 69.5 | 75.7 | 69.1 | 75.3 |\n| UEO     | 72.3 | 75.1 | 72.1 | 75.7 | 73.2 | 74.0 | 72.3 | 74.9 | 72.5 | 74.9 |\n\n[(C): closed-set, (P): partial-set, (O): open-set, (OP): open-partial]\n\nAs can be seen from the table above, UEO consistently outperforms all three thresholding methods in terms of ACC while being inferior to them in terms of AUC under open-set and open-partial shifts. Notably, **the thresholding technique proves harmful for closed-set and partial-set shifts** where no OOD samples exist in the unlabeled data. In summary, UEO demonstrates superior suitability compared to these thresholding variants."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2480/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700153879723,
                "cdate": 1700153879723,
                "tmdate": 1700153879723,
                "mdate": 1700153879723,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pzt6v7iZb8",
                "forum": "k2a2aPOA4b",
                "replyto": "hOSy93bANH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2480/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2480/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> **[Q5]**. The proposed method appears to be a general OOD method that can be also applied to traditional classification networks. I wonder why this method is applied to only CLIP method instead of extending it to other pre-trained backbones.\n\n**[A5]**. Indeed, our objective in Eq. (4) is adaptable to traditional classification networks as well. The reason why we focus on CLIP lies in its flexibility, as a pre-trained model can be easily acquired based on class names without the need for labeled data, rendering it a versatile framework.\n\nRegarding the traditional classification networks, we employ the ResNet-50 backbone with the [backbone - bottleneck layer - classifier layer] architecture to train the source model. During unsupervised fine-tuning in the target domain, we only update the parameters of the bottleneck layer (parameter-efficient), the results on two different tasks are shown below. On both tasks, UEO significantly outperforms other methods in terms of both ACC and AUC metrics.\n\n| OF (D->A) | ACC (C) | AUC (C) | ACC (P) | AUC (P) | ACC (O) | AUC (O) | ACC (OP) | AUC (OP) | ACC (Avg.) | AUC (Avg.) |\n|---------|---------|---------|---------|---------|---------|---------|---------|---------|---------|---------|\n| source only   | 65.2 | 71.8 | 65.2 | 71.8 | 65.2 | 71.8 | 65.2 | 71.8 | 65.2 | 71.8 |\n| UPL        | 69.7 | 73.8 | 67.7 | 72.2 | 69.9 | 74.0 | 68.1 | 70.0 | 68.8 | 72.5 |\n| POUF       | 69.9 | 75.7 | 68.9 | 73.7 | 70.1 | 74.2 | 68.5 | 71.7 | 69.4 | 73.8 |\n| DANCE      | 65.0 | 72.5 | 66.9 | 73.5 | 65.8 | 73.2 | 64.8 | 71.8 | 65.6 | 72.7 |\n| EntMin     | 64.3 | 72.0 | 64.9 | 72.8 | 63.2 | 69.8 | 63.9 | 70.5 | 64.1 | 71.3 |\n| InfoMax    | 71.8 | 77.1 | 70.1 | 74.5 | 72.2 | 74.2 | 69.4 | 70.8 | 70.9 | 74.2 |\n| UEO        | 71.6 | 75.5 | 71.1 | 74.9 | 71.6 | 75.0 | 70.2 | 74.1 | 71.1 | 74.9 |\n\n| OH (Pr->Ar) | ACC (C) | AUC (C) | ACC (P) | AUC (P) | ACC (O) | AUC (O) | ACC (OP) | AUC (OP) | ACC (Avg.) | AUC (Avg.) |\n|---------|---------|---------|---------|---------|---------|---------|---------|---------|---------|---------|\n| source only   | 54.6 | 64.5 | 54.5 | 64.5 | 54.6 | 64.5 | 54.5 | 64.5 | 54.6 | 64.5 |\n| UPL        | 57.0 | 65.3 | 55.6 | 65.8 | 56.2 | 65.7 | 55.4 | 64.9 | 56.1 | 65.4 |\n| POUF       | 57.1 | 64.6 | 56.1 | 64.6 | 57.2 | 63.6 | 56.4 | 63.4 | 56.7 | 64.0 |\n| DANCE      | 53.9 | 65.2 | 54.2 | 65.0 | 53.2 | 62.7 | 54.0 | 64.7 | 53.8 | 64.4 |\n| EntMin     | 52.5 | 64.3 | 53.2 | 64.4 | 51.5 | 62.6 | 52.1 | 62.4 | 52.3 | 63.4 |\n| InfoMax    | 57.9 | 65.4 | 56.9 | 65.5 | 57.2 | 63.8 | 57.1 | 63.6 | 57.3 | 64.5 |\n| UEO        | 58.3 | 66.3 | 58.2 | 66.2 | 57.9 | 64.9 | 58.2 | 64.7 | 58.1 | 65.5 |"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2480/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700153894506,
                "cdate": 1700153894506,
                "tmdate": 1700153894506,
                "mdate": 1700153894506,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pasaz4fEoy",
                "forum": "k2a2aPOA4b",
                "replyto": "hOSy93bANH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2480/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2480/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Invitation to further discussion"
                    },
                    "comment": {
                        "value": "Dear reviewer, \n\nWe genuinely appreciate the time and effort you've invested in reviewing our paper. We have carefully provided relevant responses and results to your concerns. We are eager to further discuss with you and gain your insights **before the end of the Author/Reviewer phase**. Please let us know if any aspect of our work remains unclear or if you have additional feedback. \n\nThank you."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2480/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700529551795,
                "cdate": 1700529551795,
                "tmdate": 1700529551795,
                "mdate": 1700529551795,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "voPp1vTXED",
                "forum": "k2a2aPOA4b",
                "replyto": "rGvTZW1W1O",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2480/Reviewer_eDVD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2480/Reviewer_eDVD"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for adding the comparison with different thresholding strategies"
                    },
                    "comment": {
                        "value": "I would thank the authors for adding the comparison with other thresholding strategies and also for correcting my on the derivation of the implicit threshold when $w$ follows a uniform distribution. \n\nRegarding the above table provided by the authors, it is interesting to find that hard-thresholding strategies like EO-H and EO-ID perform on par with Eq. (3). Does this imply that Eq. (3) is less effective in adaptively reweighting ID and OOD samples? Furthermore, as Eq. (4) is a modified version of Eq. (3) with catering for the case that no OOD samples are presented in the minibatch, is it possible to achieve the same goal without starting from Eq. (3)? That is, what I concern is that the derivation from Eq. (3) to Eq. (4) is less convincing given the above results in the table."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2480/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700622142966,
                "cdate": 1700622142966,
                "tmdate": 1700622142966,
                "mdate": 1700622142966,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oTyTuISsah",
                "forum": "k2a2aPOA4b",
                "replyto": "hOSy93bANH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2480/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2480/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Further clarification on the derivation from Eq. (3) to Eq. (4)"
                    },
                    "comment": {
                        "value": "We express our sincere gratitude to the reviewer for providing us with the opportunity to address the remaining concerns during the author/reviewer phase.\n\nTo validate the effectiveness of both the weighting strategy in Eq. (3) and the relaxation (derivation) strategy in Eq. (4), we extend our comparisons to three additional domains within the OfficeHome dataset. We present the averaged results across various category shifts **[(C): closed-set, (P): partial-set, (O): open-set, (OP): open-partial]** and domains **[OH (Ar)/ OH (Cl)/ OH (Pr)/ OH (Re)]** for different variants in the table below. Note that, UEO-H/ID replaces $\\mathbb{E}(H(p(x)))$ in EO-H/ID with $H(\\mathbb{E}(p(x)))$.\n\n| OH (Avg.)|  ACC (Avg.) | AUC (Avg.) |\n|---------|---------|---------|\n| CLIP   | 73.2 | 75.0 |\n| EO-ID  | 72.9 | 75.7 |\n| EO-H   | 72.3 | 75.5 |\n| EO     | 72.4 | 75.4 |\n| UEO-ID | 74.5 | 75.7 |\n| UEO-H  | 74.8 | 75.9 |\n| UEO    | 76.6 | 75.6 |\n\nWe can derive three main conclusions:\n- Although the weighting strategy does not enable EO to surpass EO-H in Eq.(3), comparing UEO with UEO-H demonstrates the effectiveness of the weighting strategy in the final objective.\n- The relaxation (derivation) strategy significantly improves all three variants (EO-ID, EO-H, and EO), with a notable impact on the ACC metric.\n- UEO achieves the best performance while considering both ACC and AUC metrics.\n\n&nbsp;\n\nWe attach the detailed results under each category shift and each domain as follows.\n\n [(C): closed-set, (P): partial-set, (O): open-set, (OP): open-partial]\n\n&nbsp;\n\n| OH (Ar)| ACC (C) | AUC (C) | ACC (P) | AUC (P) | ACC (O) | AUC (O) | ACC (OP) | AUC (OP) | ACC (Avg.) | AUC (Avg.) |\n|---------|---------|---------|---------|---------|---------|---------|---------|---------|---------|---------|\n| CLIP   | 70.1 | 74.9 | 70.1 | 74.9 | 70.2 | 74.9 | 70.1 | 74.9 | 70.1 | 74.9 |\n| EO-ID  | 69.9 | 75.7 | 69.8 | 75.6 | 70.4 | 76.3 | 70.4 | 76.3 | 70.1 | 76.0 |\n| EO-H   | 68.8 | 74.7 | 68.8 | 74.4 | 69.6 | 76.2 | 70.1 | 76.0 | 69.3 | 75.3 |\n| EO     | 68.9 | 74.9 | 68.6 | 74.7 | 69.6 | 75.8 | 69.5 | 75.7 | 69.1 | 75.3 |\n| UEO-ID | 70.7 | 75.7 | 70.1 | 75.6 | 71.3 | 75.7 | 71.6 | 75.4 | 70.9 | 75.6 |\n| UEO-H  | 71.2 | 75.4 | 70.9 | 75.2 | 72.0 | 75.6 | 71.8 | 75.3 | 71.5 | 75.4 |\n| UEO    | 72.3 | 75.1 | 72.1 | 75.7 | 73.2 | 74.0 | 72.3 | 74.9 | 72.5 | 74.9 |\n\n&nbsp;\n\n| OH (Cl)| ACC (C) | AUC (C) | ACC (P) | AUC (P) | ACC (O) | AUC (O) | ACC (OP) | AUC (OP) | ACC (Avg.) | AUC (Avg.) |\n|---------|---------|---------|---------|---------|---------|---------|---------|---------|---------|---------|\n| CLIP   | 55.3 | 64.7 | 55.2 | 64.7 | 55.2 | 64.7 | 55.3 | 64.7 | 55.3 | 64.7 |\n| EO-ID  | 56.9 | 65.9 | 55.8 | 65.5 | 57.1 | 63.5 | 56.3 | 64.8 | 56.5 | 64.9 |\n| EO-H   | 56.0 | 65.4 | 54.8 | 65.7 | 55.9 | 65.5 | 55.7 | 65.9 | 55.6 | 65.6 |\n| EO     | 56.4 | 65.5 | 55.7 | 65.7 | 56.5 | 65.5 | 55.7 | 66.0 | 56.1 | 65.7 |\n| UEO-ID | 59.9 | 66.3 | 58.2 | 66.4 | 60.0 | 65.0 | 58.8 | 64.1 | 59.2 | 65.5 |\n| UEO-H  | 59.8 | 66.7 | 59.6 | 66.4 | 60.4 | 66.3 | 59.7 | 66.4 | 59.9 | 66.4 |\n| UEO    | 61.2 | 64.8 | 61.4 | 65.2 | 62.0 | 64.7 | 61.0 | 64.9 | 61.4 | 64.9 |\n\n&nbsp;\n\n| OH (Pr)| ACC (C) | AUC (C) | ACC (P) | AUC (P) | ACC (O) | AUC (O) | ACC (OP) | AUC (OP) | ACC (Avg.) | AUC (Avg.) |\n|---------|---------|---------|---------|---------|---------|---------|---------|---------|---------|---------|\n| CLIP   | 84.2 | 78.0 | 84.2 | 78.0 | 84.2 | 78.0 | 84.2 | 78.0 | 84.2 | 78.0 |\n| EO-ID  | 81.6 | 77.6 | 81.0 | 76.0 | 83.0 | 79.7 | 83.1 | 79.0 | 82.2 | 78.1 |\n| EO-H   | 82.5 | 77.2 | 79.1 | 74.4 | 82.6 | 79.4 | 83.1 | 78.8 | 81.8 | 77.4 |\n| EO     | 82.3 | 77.1 | 81.2 | 75.9 | 82.7 | 79.3 | 83.0 | 78.8 | 82.3 | 77.8 |\n| UEO-ID | 83.8 | 78.0 | 82.7 | 77.7 | 84.6 | 78.8 | 84.8 | 78.5 | 84.0 | 78.2 |\n| UEO-H  | 83.5 | 78.0 | 83.1 | 77.3 | 84.8 | 78.8 | 85.0 | 78.7 | 84.1 | 78.2 |\n| UEO    | 87.7 | 79.6 | 87.0 | 80.3 | 87.0 | 78.5 | 86.5 | 78.9 | 87.1 | 79.3 |\n\n&nbsp;\n\n| OH (Re)| ACC (C) | AUC (C) | ACC (P) | AUC (P) | ACC (O) | AUC (O) | ACC (OP) | AUC (OP) | ACC (Avg.) | AUC (Avg.) |\n|---------|---------|---------|---------|---------|---------|---------|---------|---------|---------|---------|\n| CLIP   | 83.0 | 82.5 | 83.1 | 82.5 | 83.1 | 82.5 | 83.0 | 82.5 | 83.0 | 82.5 |\n| EO-ID  | 82.2 | 83.2 | 82.7 | 82.8 | 82.9 | 84.5 | 83.5 | 84.5 | 82.8 | 83.8 |\n| EO-H   | 81.6 | 82.1 | 82.1 | 82.5 | 82.9 | 84.8 | 83.4 | 84.8 | 82.5 | 83.5 |\n| EO     | 81.0 | 82.1 | 82.0 | 81.7 | 82.7 | 83.9 | 82.8 | 84.0 | 82.1 | 83.0 |\n| UEO-ID | 83.1 | 83.1 | 83.5 | 83.0 | 84.2 | 84.2 | 84.6 | 84.1 | 83.8 | 83.6 |\n| UEO-H  | 83.0 | 82.8 | 83.2 | 83.3 | 84.1 | 84.3 | 84.5 | 84.1 | 83.7 | 83.6 |\n| UEO    | 85.6 | 82.9 | 85.4 | 83.9 | 85.3 | 82.6 | 85.5 | 83.5 | 85.4 | 83.2 |"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2480/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700638448026,
                "cdate": 1700638448026,
                "tmdate": 1700641422521,
                "mdate": 1700641422521,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Qf1MS8O8sW",
                "forum": "k2a2aPOA4b",
                "replyto": "hOSy93bANH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2480/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2480/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Your Feedback is Appreciated in the Last Minute."
                    },
                    "comment": {
                        "value": "Dear reviewer, \n\nConcerning the only issue raised about **the convincing derivation from Eq. (3) to Eq. (4)**, we have presented comprehensive comparisons between UEO and various variants in the rebuttal phase. We are eager to understand if these results have successfully addressed your reservations regarding **the effectiveness of both the weighting strategy and the relaxation strategy in Eq. (4)**.\n\nAs for the other concerns that were not reiterated in your recent response, we would appreciate confirmation on whether they have been satisfactorily addressed in our previous response. **We would be very grateful if you could provide your feedback before the end of the rebuttal phase.**\n\nThe authors"
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2480/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700737102024,
                "cdate": 1700737102024,
                "tmdate": 1700737419256,
                "mdate": 1700737419256,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]