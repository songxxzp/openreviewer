[
    {
        "title": "Adaptive Invariant Representation Learning for Non-stationary Domain Generalization"
    },
    {
        "review": {
            "id": "HGMcUqe3i1",
            "forum": "jnZtTUdWyi",
            "replyto": "jnZtTUdWyi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6413/Reviewer_VfLk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6413/Reviewer_VfLk"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the continuous domain generalization problem (also named non-stationary DG setting in the paper), and proposes the  adaptive invariant representation learning algorithm to solve it. The model uses a transformer layer and a LSTM model to capture the evolving patterns between time steps. The experimental results validate the proposed method. However, the non-stationary DG setting is not a new one and there lack some literatures in this paper, and the theoretical analysis seems trivial, which together greatly lower the technical contribution of this work."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "After carefully reading this paper, I think the strengths mainly lie in the design of the model.\n\n1. The authors propose a novel model architecture to handle the continuous DG problem, which contains a transformer layer and an LSTM model to capture the time-dependent patterns. \n2. The experimental results are nice to validate the proposed method, which involves many datasets and baselines."
                },
                "weaknesses": {
                    "value": "I have several concerns regarding to the theoretical analysis, the model design, and the experimental results.\n1. The theoretical analysis is trivial and there is little novelty in it. \n* Firstly, the Theorem 4.5 is a conventional generalization bound via Rademacher Complexity, and the constant term $C$ is the **upper bound** of the loss function, which means the upper bound is quite loose and not quite meaningful to guide the design of methods. That is, if the upper bound is too loose, one could add any term to it that is consistent to the method. Also, the meaning of $K$ is not demonstrated in the paper. \n* Secondly, Proposition 4.6 is also trivial, and the reweighting method that it inspired has little relationship with invariance. \n2. The model design: \n* Firstly, why the alignment of distributions could lead to invariance? The authors did not formally define the invariance property and it seems vague.\n* Secondly, it seems that the designed model has a lot of computation burden, could the authors analyze it or provide some empirical analysis/results on this? For example, extra running time.\n3. The experiments: the authors did not report the variance of different runs. Further, the validation protocol is not reported."
                },
                "questions": {
                    "value": "Please refer to Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A."
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6413/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698861193856,
            "cdate": 1698861193856,
            "tmdate": 1699636714393,
            "mdate": 1699636714393,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yU2IpBClOO",
                "forum": "jnZtTUdWyi",
                "replyto": "HGMcUqe3i1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6413/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6413/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors (1/2)"
                    },
                    "comment": {
                        "value": "Thanks for your review. We would like to address your concerns as follows.\n\n> **Q1. Regarding Theorem 4.5.**\n\nWe clarify that the usage of Rademacher complexity is not our main contribution. Rademacher complexity is one conventional way to relate the learning bound with finite data setting (in practice we only have access to finite data). It has been used to construct learning bounds for many domain adaptation/generalization settings [1,2,3]. The key contribution in **Theorem 4.5** is to construct a learning bound based on the non-stationary mechanism $M^{\\ast}$ which provides the guarantee for the proposed algorithm.\n\nWe also clarify that the assumption about bounded loss function is common in domain adaptation/generalization literature [1,4,5]. This assumption is mild and can be easily satisfied by many loss functions. For example, cross-entropy loss can be bounded by $C$ modifying the softmax output from $\\left(p\\_1, p\\_2, \\cdots. p\\_{|\\mathcal{Y}|} \\right)$ to $\\left(\\hat{p}\\_1, \\hat{p}\\_2, \\cdots. \\hat{p}\\_{|\\mathcal{Y}|} \\right)$ where $\\hat{p}\\_i = p\\_i (1 - \\exp(-C)|\\mathcal{Y}|) + \\exp(-C)$.\n\n$K$ is a constant that depends on statistical distance $\\mathcal{D}$ used measures the distance between two distributions. In particular, $K = \\frac{1}{\\sqrt{2}}$ when $\\mathcal{D}$ is KL-divergence and $K = \\sqrt{2}$ when $\\mathcal{D}$ is JS-divergence. The details of $K$ are already given in the proof of **Lemma A.1** in **Appendix A**. Based on your suggestion, we revised **Theorem 4.5** to include this information.\n\n> **Q2. Regarding Proposition 4.6 and reweighting method.**\n\nCould you provide more details for your argument \u201cSecondly, Proposition 4.6 is also trivial\u201d? We note that this proposition is not trivial and the goal of this one is to help us to connect **Theorem 4.5** (which suggests us finding the mapping $m\\_{t-1}^{\\ast}: \\mathcal{X} \\times \\mathcal{Y} \\rightarrow \\mathcal{X} \\times \\mathcal{Y}$ such that distance of the joint distributions $P^{X,Y}$ between two domains $D\\_t$ and $D\\_t^{\\ast} = m\\_{t-1} \\sharp D\\_{t-1}$ is minimized $\\forall t \\in [2,T]$) to our 2-stage algorithm shown in **Remark 4.7** (which minimizes (1) distance of label distribution $P(Y)$ by importance weighting and (2) distance of label-conditional feature distribution $P(X|Y)$ by invariant representation learning).\n\nWe also clarify that importance weighting method is used to minimize the distance of label distribution $P(Y)$ between two domains $D\\_t$ and $D\\_t^{\\ast}$ rather than achieving label-conditional invariance representation learning which is equivalent to minimizing the distance of label-conditional distribution $P(X|Y)$.\n\n> **Q3. Regarding invariant property.**\n\nAs suggested by **Proposition 4.6**, after conducting importance weighting, we need to find the sequence of mappings in the input space $\\mathcal{X}$ that can help to minimize the distance of label-conditional distribution $P(X|Y)$ between 2 consecutive source domains. However, finding such mappings in input space may not be trivial due to the complexity of input space. We alleviate this issue by finding the sequence of mappings in the representation space $\\mathcal{Z}$ such that each $m\\_t^{\\ast}$ can be replaced by $f\\_t^{\\ast}: \\mathcal{X} \\rightarrow \\mathcal{Z}$ and $g\\_t^{\\ast}: \\mathcal{X} \\rightarrow \\mathcal{Z}$. In essence, we find $f\\_t^{\\ast}$ and $g\\_t^{\\ast}$ such that the distance of the label-conditional distribution $P^{Z|Y}$ between two consecutive source domains is minimized. This objective is equivalent to finding label-conditional invariant representation between two consecutive source domains. (i.e., $P^{Z|Y}$ is invariant across 2 domains $D\\_t$,  $D\\_{t+1}$ is equivalent to $\\mathcal{D}\\left( P^{Z|Y}\\_{D\\_t},  P^{Z|Y}\\_{D\\_{t+1}} \\right) = 0$)."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6413/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700363828309,
                "cdate": 1700363828309,
                "tmdate": 1700363828309,
                "mdate": 1700363828309,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LVwO25J6OD",
                "forum": "jnZtTUdWyi",
                "replyto": "HGMcUqe3i1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6413/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6413/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reminder to reviewer"
                    },
                    "comment": {
                        "value": "Dear reviewer VfLk,\n\nThank you for your time helping review and improve the paper. We hope our response has addressed all your concerns. Since we are approaching the end of the discussion period, please let us know if you have any other questions, and we are happy to discuss more. If you\u2019re satisfied with our response, we sincerely hope you could reconsider the rating.\n\nBest regards,\n\nAuthors"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6413/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700669977378,
                "cdate": 1700669977378,
                "tmdate": 1700669977378,
                "mdate": 1700669977378,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DTrnsp9wGA",
            "forum": "jnZtTUdWyi",
            "replyto": "jnZtTUdWyi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6413/Reviewer_vTu8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6413/Reviewer_vTu8"
            ],
            "content": {
                "summary": {
                    "value": "The paper delves into the challenges associated with Domain Generalization (DG) under non-stationary environments. The authors investigate the effects of such non-stationary environments on model performance and provide theoretical upper bounds for errors when models are applied to target domains. To address the identified challenges, they introduce a new algorithm that is rooted in invariant representation learning. This algorithm uses the observed non-stationary patterns to develop a model that is expected to evolve and achieve better performance on target domains. The paper validates the effectiveness of the proposed algorithm through experiments conducted on both synthetic and real-world data."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The article presents an innovative approach to tackle non-stationary domain generalization, a notable hurdle in practical scenarios. It delivers an extensive analysis of the associated difficulties and meticulously evaluates the impact of environmental changes over time.\n- A major academic contribution of this work is the formulation of theoretical upper limits for model error, lending a robust theoretical underpinning to their methodology.\n- The effectiveness of their method is validated with both synthetic and real data, showcasing its potential real-world applications."
                },
                "weaknesses": {
                    "value": "- The evaluation section lacks significance testing and does not report standard deviations in the results table. Given the close performance across many results, the inclusion of standard deviations is crucial to affirm the method's efficacy.\n- The datasets and networks employed in the study are somewhat limited in size. The paper does not address scalability, leaving questions about the method's performance with larger datasets or more complex network architectures.\n- While the theoretical analysis provided is compelling, the practical implementation details in Section 5 are complex and challenging to comprehend, even with the pseudo-code. A more detailed and clearer illsutration would greatly enhance understanding."
                },
                "questions": {
                    "value": "Please refer to the weaknesses mentioned above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6413/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698994079061,
            "cdate": 1698994079061,
            "tmdate": 1699636714275,
            "mdate": 1699636714275,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lFJ45ymEAw",
                "forum": "jnZtTUdWyi",
                "replyto": "DTrnsp9wGA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6413/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6413/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your review. We would like to address your concerns as follows.\n\n> **Q1. Regarding standard deviation.**\n\nThanks for your suggestion. We\u2019ve revised **Table 1** to include standard deviation calculated from results of 5 different random seeds.\n\n> **Q2. Regarding scalability.**\n\nWe clarify that the design of our method allows it to scale well with larger datasets or more complex network architectures. In particular, compared to the base model (ERM), our method (AIRL) has only one additional Transformer and LSTM layers, incurring only minimal overhead in terms of computation. Note that we only need to use Transformer and LSTM layers during training. In the inference stage, the predictions are made by the fixed representation $g$ mapping and the classifier $h$ pre-generated by LSTM which then results in a similar inference time with ERM. Compared to existing works for non-stationary domain generalization, our method required fewer computational resources. It\u2019s because of our effective design to capture non-stationary patterns. Specifically, LSSAE and DRAIN have more complex architectures and objective functions resulting in much more training time than our method. While DPNET has slightly better training time than ours, this model requires storing previous data to make predictions and is not generalized to multiple target domains. To further support our claim, the average training times (i.e., seconds) of these methods for different datasets are shown in the table below.\n\n|       | Circle | Circle-Hard | RMNIST | Yearbook | CLEAR |\n|-------|--------|-------------|--------|----------|-------|\n| **AIRL**  | 32     | 25          | 382    | 749      | 1504  |\n| LSSAE | 184    | 175         | 1727   | 1850     | 13287 |\n| DRAIN | 460    | 230         | 2227   | 5538     | 1920  |\n| DPNET | 18     | 13          | 208    | 448      | 1542  |\n\n> **Q3. Regarding section 5 writing.**\n\nThanks for your suggestion. Could you provide more details about what thing we should do to improve clarification for section 5? We will revise this section based on your comments in our final version."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6413/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700363697689,
                "cdate": 1700363697689,
                "tmdate": 1700363697689,
                "mdate": 1700363697689,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7l5UZxs0LU",
                "forum": "jnZtTUdWyi",
                "replyto": "DTrnsp9wGA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6413/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6413/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reminder to reviewer"
                    },
                    "comment": {
                        "value": "Dear reviewer vTu8,\n\nThank you for your time helping review and improve the paper. We hope our response has addressed all your concerns. Since we are approaching the end of the discussion period, please let us know if you have any other questions, and we are happy to discuss more. If you\u2019re satisfied with our response, we sincerely hope you could reconsider the rating.\n\nBest regards,\n\nAuthors"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6413/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700669913028,
                "cdate": 1700669913028,
                "tmdate": 1700669913028,
                "mdate": 1700669913028,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gh5HZg4QRQ",
            "forum": "jnZtTUdWyi",
            "replyto": "jnZtTUdWyi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6413/Reviewer_6RJQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6413/Reviewer_6RJQ"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers a non-stationary domain generalization problem.  Authors first establish theoretical upper bounds for the model error at target domain, and then leverage the non-stationary pattern to train a model based using invariant representation learning. Experiments show some improved results over existing methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- A more general setting regarding non-stationary DG problem.\n- A novel invariant algorithm is proposed based on theoretical bounds.\n- Improved empirical results across a wide range of datasets."
                },
                "weaknesses": {
                    "value": "- Description of the setting is not clear and the key assumption seems strong.\n- Proposed algorithm seems to be ad-hoc and complicated.\n- Presentation can be made more concise."
                },
                "questions": {
                    "value": "I have some questions regarding the problem setting and assumptions:\n\n- What's the difference between this problem's setting and the IRM's where it is assumed that some invariance exists across domains? I can see that some datasets you use (like RMNIST) falls into the IRM's setting. A related question is, why do you consider learning invariant representations using only two consecutive domains?\n\n-  And do you implicitly assume that the domain indexes and their order are given? If so, I think this setting is limited in this sense.  Note that most benchmark methods do not utilize the order information. Please clarify. \n\n- **Major Concern**: It is stated that \"We note that Assumption 4.3 is mild because it is required only for the optimal mechanism M\u2217 . This assumption implies that there exists at least one hypothesis in M under which the non-stationary patterns learned from source can generalize sufficiently well to the target (with bounded $\\Phi$ ).\"\n\n  - regarding the criterion $\\Phi$: I don't think Definition 4.1 necessarily implies a good estimate of the mechanism $M$. That is, $\\hat M$ can be bad for all domain pairs, so that each $D$ inside the $|\\cdot|$ is large but the difference is small. Perhaps some discussions shall be added here.\n  - More importantly, I would like to think that Assumption 4.3 is rather strong. Note that it is NOT equivalent to assuming that there exists  a pattern in the hypothesis space; here it is a \"specific\" one, which minimizes the divergence of observed datasets, and could provide an almost optimal estimate of an unseen domain. If so, this would be a strong assumption on the relationship of observed domains and unseen domains. Please clarify.\n\n- Minor: Section 5 is hard to follow. Please be more concise. \n- Experiments: how many domains are used for training? And what if the domain index is re-ordered? \n\n\n\nOverall, I think this paper has some interesting and useful contributions. I am happy to increase my evaluation if authors can address my concerns."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6413/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699024216079,
            "cdate": 1699024216079,
            "tmdate": 1699636714160,
            "mdate": 1699636714160,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rGMDwkPT11",
                "forum": "jnZtTUdWyi",
                "replyto": "gh5HZg4QRQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6413/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6413/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors (1/2)"
                    },
                    "comment": {
                        "value": "Thanks for your review. We would like to address your concerns as follows.\n\n> **Q1. Regarding problem setting and invariant learning.**\n\nWe consider DG in non-stationary environment where domains evolve along specific direction (e.g., time, space). This setting is different from conventional DG setting considered in existing methods (including IRM). In essence, conventional DG assumes domains are sampled from a stationary environment and target domains lie on or are near the mixture of source domains. In addition, evaluation protocol is also different between non-stationary and conventional DG. Models designed for conventional DG follow \u201cleave-one-out\u201d evaluation (i.e., one domain is target while the remaining domains are source) because the goal is to generalize to arbitrary domains near the mixture of source domains. Meanwhile, in non-stationary DG, models are trained on previous domains and evaluated on future domains in domain sequences. We also note that dataset (colored MNIST) used in IRM paper is different from the one (rotated MNIST) used in our work. \n\nUnder the assumption that target domains lie on or are near the mixture of source domains, existing works that enforce invariant representations across all source domains can help to generalize the model to target domains [1,2,3]. However, this assumption may not hold in non-stationary DG where the target domains may be far from the mixture of source domains resulting in the failure of the existing methods. In non-stationary DG, the key is to capture non-stationary patterns from the sequence of source domains which can help us to estimate the target domains. Our proposed method captures these patterns in the representation space via learning the sequence of invariant representation space in which each space is for a pair of consecutive source domains. Note that, this method is different from the existing works that learn a general invariant representation space for all source domains.\n\n> **Q2. Regarding the usage domain index and order.**\n\nWe clarify that both our method and all currently existing baselines for non-stationary DG rely on the utilization of domain order information. This information is necessary to capture non-stationary patterns over the source domains, thereby playing a pivotal role in facilitating models to generalize to target domains in the context of non-stationary DG. It is noteworthy that in numerous applications in computer vision [4], natural language processing [5], and healthcare [6], this information naturally emerges and does not necessitate additional efforts for collection.\n\n> **Q3. Regarding Definition 4.1.**\n\nWe clarify that the \u201cgeneralizability\u201d term $\\Theta\\left( \\widehat{M} \\right)$ defined in **Definition 4.1** is small does not imply $\\widehat{M}$ is a good mechanism because the errors of $\\widehat{M}$ on both source and target domains may be large while $\\Theta\\left( \\widehat{M} \\right)$ is small. Instead, $\\Theta\\left(\\widehat{M}\\right)$ measures how consistent in terms of performance of mechanism $\\widehat{M}$ on capturing non-stationary patterns on source and target domains. To avoid ambiguation, we\u2019ve revised **Definition 4.1** by replacing the term **\u201cgeneralizability\u201d** with **\u201cnon-stationary consistency\u201d**.\n\nWe also note that rather than working with an arbitrary $\\widehat{M}$, we focus on $M^{\\ast} = \\arg\\min\\_{\\widehat{M} \\in \\mathcal{M}} \\frac{1}{T-1} \\sum\\_{t=2} \\mathcal{D} \\left( P^{X,Y}\\_{D_t}, P^{X,Y}\\_{\\widehat{D}\\_t} \\right)$ in our analysis (**Theorem 4.5**). In practice, we can achieve small error of $M^{\\ast}$ on source domains during training. Then, under **Assumption 4.3** for $\\Theta\\left( M^{\\ast} \\right)$, $M^{\\ast}$ is guaranteed to achieve small error on target domain.\n\n> **Q4. Regarding Assumption 4.3.**\n\nWe clarify that **Assumption 4.3** about $\\Theta\\left( M^{\\ast} \\right)$ is reasonable. If **Assumption 4.3** does not hold, it means that non-stationary patterns underlying target domain are very different from the patterns estimated from the source domains, making the task to generalize to target domain infeasible. It is noteworthy that we do not mention **Assumption 4.3** is equivalent to \u201cassuming that there exists a pattern in the hypothesis space\u201d because it only applied for optimal mechanism $M^{\\ast}$. We only mentioned that **Assumption 4.3** implies that assumption. We\u2019ve revised our writing to avoid this ambiguity.\n\n> **Q5. Regarding section 5 writing.**\n\nThanks for your suggestion. Could you provide more details about what thing we should do to improve clarification for section 5? We will revise this section based on your comments in our final version."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6413/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700363563569,
                "cdate": 1700363563569,
                "tmdate": 1700363563569,
                "mdate": 1700363563569,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xYmhVN7MZe",
                "forum": "jnZtTUdWyi",
                "replyto": "rGMDwkPT11",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6413/Reviewer_6RJQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6413/Reviewer_6RJQ"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for clarifications. \n\n- Regarding Q1: To clarify, Rotated MNIST is not the one used in the IRM paper but falls into the setting of IRM. And in combination with Q2, this seems to indicate that for some settings the proposed method does not work as well as stationary-DG methods. Of course, a method does not need to beat all other methods; nevertheless, an explicit discussion shall be appreciated.\n\n- Regarding Q4 on assumption 4.3: sorry for the previous misunderstanding. Yet I still have a different opinion on this assumption, because something is assumed to be \"stationary\" too---here it is the transition--- and it can be obtained by directly minimizing the ERM loss of observed datasets. As such, it would be a strong assumption in terms of \"generalization\". Or can you show it is indeed a necessary condition?\n\n- Regarding Section 5's writing: one suggestion from my side is to move some implementation details into the appendix and also simplify the notations.  \n\nBased on the current response, I would like to keep my evaluation as my major concern remains."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6413/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700366623957,
                "cdate": 1700366623957,
                "tmdate": 1700366623957,
                "mdate": 1700366623957,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Nbq0oNZ5JZ",
                "forum": "jnZtTUdWyi",
                "replyto": "gh5HZg4QRQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6413/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6413/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reminder to reviewer"
                    },
                    "comment": {
                        "value": "Dear reviewer 6RJQ,\n\nThank you for your time helping review and improve the paper. We hope our response has addressed all your concerns. Since we are approaching the end of the discussion period, please let us know if you have any other questions, and we are happy to discuss more. If you\u2019re satisfied with our response, we sincerely hope you could reconsider the rating.\n\nBest regards,\n\nAuthors"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6413/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700669867984,
                "cdate": 1700669867984,
                "tmdate": 1700669867984,
                "mdate": 1700669867984,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "y5cqrvG2Vw",
            "forum": "jnZtTUdWyi",
            "replyto": "jnZtTUdWyi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6413/Reviewer_Uk8r"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6413/Reviewer_Uk8r"
            ],
            "content": {
                "summary": {
                    "value": "Many machine learning algorithms work based on the assumption that training and test data are sampled from IIDs. However, this is commonly violated in real-world cases as the data distribution may shift between train and test times. It has encouraged many researchers to develop techniques such as domain generalization and domain adaptation. However, such methods cannot accommodate the case that the data distribution changes over time based on a mechanism. To tackle such a problem, this paper studies domain generalization in a non-stationary environment, which aims to learn a model from a sequence of source domains that can capture the non-stationary patterns and generalize to unseen target domains. The authors examined the impacts of non-stationary distribution shifts and investigated how the model learned on the source domains performs on the target domains. Experiments were done on simulated, semi-synthetic, and real-world datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The problem this paper addresses is interesting. Multiple domains evolving over time is a realistic scenario but has hardly been studied before. Also, the authors demonstrated that the existing typical multiple-source DG/DA methods \n- The problem setup is more flexible than those of the related works as it allows the modeling of non-stationary dynamics and can be applied to multiple unseen target domains.\n- Well-written and easy to follow."
                },
                "weaknesses": {
                    "value": "- Definition of $\\Phi$ is a bit ambiguous. In Definition 4.1 and Remark 4.2, $\\Phi(\\widehat{M})$ consists of an average error of $\\widehat{M}$ and on the source domains and error of $\\widehat{M}$ on the target domain. It doesn't consider whether each error is low enough but only considers their difference. Thus, based on the definition, $\\widehat{M}$ is generalizable even if $\\widehat{M}$ has high errors on both source and target domains. I am unsure if such $\\widehat{M}$ is generalizable.\n- Moreover, Assumption 4.3 tells us that we can find a decent $\\widehat{M}$ with a small error on the source domains but seems to tell nothing about the error on the target domain. I am unsure if I understood this statement correctly, but it doesn't seem like Assumption 4.3 implies that the error of $M^{*}$ on the target domain is small. I think this is one of the key statements of this study so it needs to be clarified.\n- The authors need to add an uncertainty metric to Table 1."
                },
                "questions": {
                    "value": "- I appreciate the theoretical analysis and experimental evidence that the authors presented. Could the authors further provide under what condition the proposed approach will have guaranteed improvement compared to either the ERM or some conventional DG method?\n- I think the FMoW dataset from the WILDS benchmark aligns with the problem setup in the paper. (The authors already included sufficient experimental results, so I do not request the authors to add an additional dataset.)\n- The authors may want to add some dotted lines to Table 1 to differentiate different approaches - ERMs / conventional DA/DGs, ..."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "n/a"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6413/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6413/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6413/Reviewer_Uk8r"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6413/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699114849325,
            "cdate": 1699114849325,
            "tmdate": 1699636714030,
            "mdate": 1699636714030,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1rteh9yz6L",
                "forum": "jnZtTUdWyi",
                "replyto": "y5cqrvG2Vw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6413/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6413/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your review. We would like to address your concerns as follows.\n\n> **Q1. Regarding definition of generalizability term $\\Theta\\left(\\widehat{M}\\right)$.**\n\nTo clarify, $\\Theta\\left(\\widehat{M}\\right)$ is defined as the difference between two terms: error of  $\\widehat{M}$ on sequence of source domains $\\frac{1}{T-1}\\sum\\_{t=2}\\mathcal{D}\\left( P^{X,Y}\\_{D\\_t},P^{X,Y}\\_{\\widehat{D}\\_t}\\right)$ and error of  $\\widehat{M}$ on target domain $\\mathcal{D}\\left(P^{X,Y}\\_{D\\_{T+1}},P^{X,Y}\\_{\\widehat{D}\\_{T+1}}\\right)$. In other words, $\\Theta\\left(\\widehat{M}\\right)$ measures how consistent in terms of performance of mechanism $\\widehat{M}$ on capturing non-stationary patterns on source and target domains, and the term \u201cgeneralizability\u201d used in **Definition 4.1** implies it. Clearly, good $\\widehat{M}$ makes $\\Theta\\left(\\widehat{M}\\right)$ small but $\\Theta\\left( \\widehat{M} \\right)$ does not imply $\\widehat{M}$ can capture the evolving on target domain because $\\widehat{M}$ may have large error on both source and target domains. Thanks for your comment, we\u2019ve revised **Definition 4.1** by replacing the term **\u201cgeneralizability\u201d** with **\u201cnon-stationary consistency\u201d** to avoid ambiguity.\n\n> **Q2. Regarding Assumption 4.3.**\n\nWe clarify that **Assumption 4.3** implies error of $M^{\\ast}$ on target domain is small. Note that **Assumption 4.3** holds only for the optimal mechanism $M^{\\ast} = argmin\\_{\\widehat{M} \\in \\mathcal{M}} \\frac{1}{T-1} \\sum\\_{t=2} \\mathcal{D} \\left( P^{X,Y}\\_{D_t}, P^{X,Y}\\_{\\widehat{D}\\_t} \\right)$ instead of arbitrary $\\widehat{M}$. During training, we can feasibly achieve small source error for $M^{\\ast}$ (i.e., $\\frac{1}{T-1} \\sum\\_{t=2} \\mathcal{D} \\left( P^{X,Y}\\_{D\\_t}, P^{X,Y}\\_{D^{\\ast}\\_t} \\right)$ by using high-capacity hypothesis class (e.g., neural network). This fact combining with **Assumption 4.3** ($\\left \\| \\mathcal{D} \\left( P^{X,Y}\\_{D\\_{T+1}}, P^{X,Y}\\_{ D^{\\ast}\\_{T+1}} \\right) - \\frac{1}{T-1} \\sum\\_{t=2} \\mathcal{D} \\left( P^{X,Y}\\_{D\\_t}, P^{X,Y}\\_{D^{\\ast}\\_t} \\right) \\right| < \\epsilon$) implies that error of $M^{\\ast}$ on target domain $\\mathcal{D} \\left( P^{X,Y}\\_{D\\_{T+1}}, P^{X,Y}\\_{ D^{\\ast}\\_{T+1}} \\right)$ is small.\n\n> **Q3. Regarding uncertainty metric to Table 1.**\n\nThanks for your suggestion. We\u2019ve revised **Table 1** to include standard deviation calculated from results of 5 different random seeds.\n\n> **Q4. Regarding the setting the proposed approach outperforms conventional DG methods.**\n\nTo clarify, our method is designed for the setting where data evolves along a specific direction (e.g., space, time), and by taking non-stationary environment into consideration, our method achieves better performance compared to existing DG methods in this setting. In essence, existing DG methods often enforce invariant representations across all domains, and it has been shown that when target domains lie on or are near the mixture of source domains, enforcing invariant representations across all source domains can help to generalize the model to target domains [1,2,3]. However, this assumption may not hold in non-stationary DG where target domains may be far from the mixture of source domains resulting in the failure of existing methods.\n\nTo further support this argument, we conduct an experiment on rotated RMNIST dataset with DANN [4] \u2013 a model that learns invariant representations across all domains. Specifically, we create 5 domains by rotating images by 0, 15, 30, 45, and 60 degrees, respectively, and follow leave-one-out evaluation (i.e., one domain is target while remaining domains are source). Clearly, the setting where target domain is images rotated by 0 or 60 degrees can be considered as non-stationary DG while other settings can be considered as conventional DG. The performances of DANN with different target domains are shown in the following table. As we can see, the accuracy drops significantly when target domain is images rotated by 0 or 60 degrees. This result demonstrates that conventional DG methods are not suitable for non-stationary DG.\n\n| Target domain     | 0    | 15   | 30   | 45   | 60   |\n|-------------------|------|------|------|------|------|\n| DANN performance | 51.2 | 59.1 | 70.0 | 69.2 | 53.9 |\n\n> **Q5. Regarding FMoV dataset.**\n\nThanks for pointing out the great resource related to our research. We will explore this dataset in future works.\n\n> **Q6. Regarding Table 1 presentation.**\n\nThanks for your suggestion. We\u2019ve revised **Table 1** to distinguish different approaches.\n\nReferences\n\n[1] Albuquerque, Isabela, et al. \"Generalizing to unseen domains via distribution matching.\"\n\n[2] Sicilia, Anthony, Xingchen Zhao, and Seong Jae Hwang. \"Domain adversarial neural networks for domain generalization: When it works and how to improve.\"\n\n[3] Phung, Trung, et al. \"On learning domain-invariant representations for transfer learning with multiple sources.\"\n\n[4] Ganin, Yaroslav, et al. \"Domain-adversarial training of neural networks.\""
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6413/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700363498312,
                "cdate": 1700363498312,
                "tmdate": 1700363498312,
                "mdate": 1700363498312,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "C6ex6yAtz0",
                "forum": "jnZtTUdWyi",
                "replyto": "y5cqrvG2Vw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6413/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6413/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reminder to reviewer"
                    },
                    "comment": {
                        "value": "Dear reviewer Uk8r,\n\nThank you for your time helping review and improve the paper. We hope our response has addressed all your concerns. Since we are approaching the end of the discussion period, please let us know if you have any other questions, and we are happy to discuss more. If you\u2019re satisfied with our response, we sincerely hope you could reconsider the rating.\n\nBest regards,\n\nAuthors"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6413/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700669821425,
                "cdate": 1700669821425,
                "tmdate": 1700669821425,
                "mdate": 1700669821425,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]