[
    {
        "title": "Deep Models modelled after human brain boost performance in action classification"
    },
    {
        "review": {
            "id": "mlBlzIF8j5",
            "forum": "epFk8e470p",
            "replyto": "epFk8e470p",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7339/Reviewer_W6Z2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7339/Reviewer_W6Z2"
            ],
            "content": {
                "summary": {
                    "value": "The authors investigate how neural networks label action-recognition video frames and compare the neural network against human behavioral performance. They manipulate the stimuli to separate bodies and background and show that both neural networks and humans perform very well with full stimuli, body only or background only. Humans perform better with the body only compared to background only conditions. The authors propose an architecture that is loosely based on notions of modularity in the brain and this new architecture improves performance and matching to human behavioral data."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "Building networks to recognize actions is of high importance to practical applications \n\nComparing how well networks perform to human performance is also of interest in terms of aligning machine and human visual capabilities."
                },
                "weaknesses": {
                    "value": "The bottom line is that highly uncontrolled and bad datasets lead to spurious and uninterpretable results. This is the main challenge throughout. \n\nIn Fig. 1 bottom left, the authors claim that baseline models perform similarly well when tested with ORIG, body or BG. This is NOT what the results show. The results do not have error bars, let alone any minimally rigorous statistical analysis. From eyeballing the figure, it seems that ORIG>>BG>>Body. \n\nThe fact that humans can identify actions purely from the background frames with over 0.7 accuracy shows that\n(1) The dataset is way too easy\n(2) Background is a major confounding factor \n(3) Time is not needed in such an easy task \n\nAs far as I understand, the proposed architecture is trained very differently from the baseline architectures. The proposed architecture is trained with Body-only stimuli and does better on Body-only stimuli, and it is trained on BG-only stimuli and does not perform better on BG-only stimuli (again, no error bars, no statistics, this is all from eyeballing). Training yields better performance in general. \n\nIt would be great to present actual results on how well Yolo v8 separates body and background.\n\nThe manuscript only has one main figure and minimal additional information that does not satisfy basic standards in the field. There are no error bars, there are no comparisons across multiple different models, no comparisons with different datasets, no ablations, no description of the effects of key variables like size, etc."
                },
                "questions": {
                    "value": "A key aspect of action recognition is likely to be dynamics and time, which is not studied here.\n\nWithin the study of action recognition from frames, it would be useful to use rigorous datasets. If the authors are interested in the effect body and background, it would be important to rigorously control for basic variables like contrast, size, and multiple other confounds."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7339/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698595951774,
            "cdate": 1698595951774,
            "tmdate": 1699636878132,
            "mdate": 1699636878132,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BEc1t1Fqwb",
                "forum": "epFk8e470p",
                "replyto": "mlBlzIF8j5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7339/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7339/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your review. Naturalistic datasets such as the HAA500 used in this manuscript are uncontrolled, however this does not invalidate our conclusions. The use of naturalistic datasets is very common in the field (see for example the Kinetics dataset). The use of such datasets makes it possible to train artificial neural networks to generalize across sources of variation that are encountered in the wild. Importantly, we used the same dataset to train both Baseline and DomainNet models. Therefore, differences between the two models cannot be due to differences in the datasets.The only difference between the training schemes was the architecture of the networks. As such, our results can only be explained by the difference between the architectures, with the two-stream architecture that separates body and background being advantageous over the single-stream architecture for the task of action classification.\n\nDynamics and time do indeed contribute to action information and we did study the effects of both static and static plus dynamic information using optic flow. Those results are reported in Table 1 and are discussed in the Results section. Even when dynamic information is available, the model additionally benefits from separating background and body information (52.50% accuracy vs 75% accuracy), showing that dynamics alone does not account for our findings.\n\nIn response to you other comments:\n\n\"The fact that humans can identify actions purely from the background frames with over 0.7 accuracy shows that (1) The dataset is way too easy (2) Background is a major confounding factor (3) Time is not needed in such an easy task.\"\n\n1) The task was easy for humans, but the baseline model nevertheless exhibited lower performance than both human participants and the proposed DomainNet model. There was no indication of a ceiling-effect that would suggest that the dataset was too easy.\n\n2) That is correct, and was the motivation for the paper. This problem is not unique to HAA500, actions tend to be associated with particular backgrounds in naturalistic experience (e.g., \u201cplaying basketball\u201d will most often happen in a basketball court). Therefore, it is important to build models that are robust to this. Results demonstrate that DomainNet was more robust to the confounding effect of background compared to the Baseline model.\n\n3) \u201cTime is not needed in such an easy task\u201d. In contrast with this statement, our results show that including dynamic, time-dependent information improved the accuracy on our task compared to using only static information (see Table 1). Specifically, when processing the original videos (ORIG), DomainNet(frames+flows) \u2013 that uses both static frames and optic flow (which are time-dependent) \u2013 obtained an accuracy of 75%, while DomainNet(frames) \u2013 that does not use time-dependent information \u2013 only obtained an accuracy of 66.25%. This was also the case for processing videos showing only the body: DomainNet(frames+flows) obtained an accuracy of 73.75%, while DomainNet(frames) obtained an accuracy of only 62.5%. These results show that time-dependent information (optic flow) was indeed helpful to perform the task, and that it led to improvements in action recognition accuracy.\n\n\u201cIt would be great to present actual results on how well Yolo v8 separates body and background.\u201d\n\nThese are available in figure S1. \n\n\u201cAs far as I understand, the proposed architecture is trained very differently from the baseline architectures. The proposed architecture is trained with Body-only stimuli and does better on Body-only stimuli, and it is trained on BG-only stimuli and does not perform better on BG-only stimuli (again, no error bars, no statistics, this is all from eyeballing). Training yields better performance in general.\u201d\n\nThe only difference between the training of the Baseline network and the DomainNet was that baseline models were trained with body and background information mixed within the same stream\u201d, while DomainNet models were trained with body and background separated into different streams, and separate loss functions were used for each stream to drive each of the streams to perform as accurately as possible. The loss function used for both the DomainNet and Baseline models was the cross-entropy loss. Both the DomainNet and the Baseline models were trained for 100 epochs, with 500 batches per epoch and 32 stimuli per batch. Both the DomainNet and the Baseline models were trained using stochastic gradient descent (SGD), with learning rate of 0.001 and momentum of 0.9. Therefore, all aspects of training were identical for the DomainNet and the Baseline models, except from the use of separate losses for the two streams which was the key manipulation of interest."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7339/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700571822095,
                "cdate": 1700571822095,
                "tmdate": 1700571822095,
                "mdate": 1700571822095,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zrRBzRgCdX",
                "forum": "epFk8e470p",
                "replyto": "BEc1t1Fqwb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7339/Reviewer_W6Z2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7339/Reviewer_W6Z2"
                ],
                "content": {
                    "title": {
                        "value": "I remain highly skeptic"
                    },
                    "comment": {
                        "value": "The authors failed to address the questions raised in the review, insisting on statements without error bars, statistical analyses or any minimal scientific rigor. \n\nI won't reiterate all the questions. Just to highlight a few examples: \n\nAs noted in the review, time is not needed in such an easy task. From eyeballing the results (the authors still have not provided any rigorous way of evaluating the results), adding time hurts performance for DomainNet for the background only, and helps with the body-only and ORIG conditions. Focusing on body-only, which is the one that most helps for the time argument, chance is 20%, just using frames, performance goes from 20% to 62.50% (42.5% improvement with respect to chance) showing that time is indeed not needed for the main boost in performance. Adding temporal information in the form of optic flow brings a boost of 11% (about 1/4 of the effect due to the frame itself). Again, the gain in performance with respect to chance is mostly driven by each frame. To study the role of time, one would like to design a dataset where frame information is truly insufficient for action recognition. One example of this is the case of recognizing action from simple light sources from actors in a dark background. Using such a controlled dataset could bring insights into how to incorporate temporal information for action recognition. To be clear, I think that temporal information is likely to be extremely interesting to rigorously study for action recognition and I would love to see rigorous work in this direction. \n\nIndeed, the baseline models were trained differently from the proposed architecture. Why is it surprising that if you train with body only, the model will get better performance in the body-only condition? Presumably, the authors could divide the image into 4 quarters, train with the upper left quarter, and show improved performance in the upper left corner. None of this has anything to do with action recognition."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7339/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700709609583,
                "cdate": 1700709609583,
                "tmdate": 1700709609583,
                "mdate": 1700709609583,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "e0VHuFge4y",
            "forum": "epFk8e470p",
            "replyto": "epFk8e470p",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7339/Reviewer_JzJq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7339/Reviewer_JzJq"
            ],
            "content": {
                "summary": {
                    "value": "This work begins by examining the similarities and differences between humans and deep neural networks in terms of action recognition. It demonstrates that a deep neural network trained with cross entropy on the entire video cannot perform action recognition when background information is omitted from the training data. In contrast to this, human subjects are capable of identifying activities solely from the body information. This highlights that DNN trained for action recognition incorrectly balances the body and background information present in the video data. In order for the deep neural network to exclusively distinguish actions coming from the body, the authors suggested using two different backbones, one for the body and one for the background. In addition to this, they implemented a loss function that was more complex and yet nevertheless compatible with their category-selective design. As a consequence of this, they demonstrated that a body-background separated backbone may produce an action recognition pattern that is comparable to the pattern seen in human participants, albeit with a significantly lower level of accuracy."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The authors try to optimize deep neural works towards reproducing action recognition patterns observed in human subjects."
                },
                "weaknesses": {
                    "value": "As the authors already included in the related works, having separate streams for different information in video is not new. For example, an early work on dynamic texture processing used two separate backbones for \u201cappearance\u201d (the scene) and \u201cdynamic\u201d (the optic flow). Their loss function also combines the matching of both appearance and dynamic features. It is possible that the L_{combined} here is new. However, the authors do not include any details on how they define L_{body}, L_{background} or L_{combined}. If the only difference this work has with other work is its usage of L_{combined} (I guess this is the cross entropy between predicted frames vs. the true frames), the novelty is very limited.\u00a0\n\n\n\nTesfaldet et al 2017 Two-Stream Convolutional Networks for Dynamic Texture Synthesis\n\n\n\nThis work needs a much more developed result section to fit as an ICLR manuscript. Its current format has one result. This result is not surprising given previous literature. I would encourage the authors to include a more detailed investigation of the proper loss functions, what predictive features are being used in humans to perform action recognition, etc. These extensions may strengthen this paper."
                },
                "questions": {
                    "value": "Does the background contain any information about the actions in the video? If not, I hope the authors illustrate better why the background information should be used at all for action recognition. Would it be desirable that a neural architecture should focus on the \u201caction\u201d component of the video to perform action recognition? \n\nWhich component of the loss function contributes the most when body-only information is being used? Or when background-only information is being used?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7339/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698784209231,
            "cdate": 1698784209231,
            "tmdate": 1699636877993,
            "mdate": 1699636877993,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Z44KRuIkYP",
                "forum": "epFk8e470p",
                "replyto": "e0VHuFge4y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7339/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7339/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your feedback. Crucially, while previous work used separate streams for static and dynamic information (e.g. Tesfaldet et al 2017) we know of no previous work that segmented a single information modality (e.g. static information) to be processed by separate streams. This idea takes root in domain-selectivity literature (Caramazza & Shelton, 1984) which we demonstrate to be beneficial for the field of machine learning. \n\nThe loss function used to train the DomainNet model was kept purposefully simple and was a combination of three losses: background stream loss, body stream loss and a combined loss. Body stream loss is a cross-entropy loss (prediction of action category from input frame) using body-segmented frames, while background loss is a cross-entropy loss using context only (body in-painted) frames. The combined loss is a sum of the previous two losses. While we agree that additional research on better loss functions could be beneficial, we used this relatively straightforward arrangement of losses in order to clearly demonstrate that performance gains are due to having separate information streams for body and background.\n\nBackground information does indeed contain information about the action class (e.g. playing baseball tends to happen on a baseball field etc.). This is further supported by our findings that both neural networks and human observers can accurately predict action category when body information is removed via inpainting (Figure 1). The challenge is that models can learn to overly rely on this contextual information and ignore body-pose related information during training leading to lower model performance overall (a conclusion supported by our results). Only using body-pose information would result in higher performance than only using background information, however an even better option is to combine both (as we did in DomainNet). As you can see in figure 1 and table 1, combining both body and background information results in higher accuracies than using either source alone. Additionally, this mirrors human behavioral results, suggesting that humans too combine both background and body information when understanding actions."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7339/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700571618644,
                "cdate": 1700571618644,
                "tmdate": 1700571618644,
                "mdate": 1700571618644,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8vEhaCghaV",
            "forum": "epFk8e470p",
            "replyto": "epFk8e470p",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7339/Reviewer_df1j"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7339/Reviewer_df1j"
            ],
            "content": {
                "summary": {
                    "value": "This paper takes insights from neuroscience and builds a model that processes body and background in images separately, aiming to improve the performance of action recognition."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper propose novel ideas of incorporating inductive bias from neuroscience in building artificial neural networks and shows that it does improve the performance of action recognition when compared with a baseline network. The paper is well-written and very clear. The human dataset collected in this paper is also valuable and should be perhaps incorporated into action recognition benchmarks."
                },
                "weaknesses": {
                    "value": "This paper is a rudimentary effort in showing incorporating certain inductive bias from neuroscience could potentially help with artificial networks in certain tasks. However for the scope of the conference, I think the lack of comparison to state-of-art models as well as insights on how to even combine this inductive bias with state-of-art models makes this paper not suitable for application and making real impact on the task of action recognition. It is also not entirely true to assume that state-of-art model, which is much more complicated than a ResNet50 network does not implicitly extract information from the background and body when recognizing action."
                },
                "questions": {
                    "value": "Discussion of how to incorporate this into state-of-art models is recommended."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7339/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698802480510,
            "cdate": 1698802480510,
            "tmdate": 1699636877853,
            "mdate": 1699636877853,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "khZE696rC5",
                "forum": "epFk8e470p",
                "replyto": "8vEhaCghaV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7339/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7339/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your comments. The goal of our work is primarily to advance models of action representations in the human brain, therefore we were interested in testing whether incorporating inductive biases from neuroscience can lead to models with more human-like behavior. It is possible that our goals do not align sufficiently with the typical content of ICLR. We agree with your assessment that our work does not test whether other state-of-the-art action recognition models can achieve similar performance: building the most accurate model was not  our aim. We nonetheless believe that this work can offer insights for artificial networks, in at least two ways. First, this work motivates a more systematic study of the contributions of body and background information in state-of-the-art models. More generally, it motivates the study of models that process different object categories in separate streams, not only in the context of action recognition, but also in the context of other applications such as video prediction. Second, this work could benefit applications for which large models are not viable due to computational constraints (e.g. Internet of Things devices)."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7339/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700571571658,
                "cdate": 1700571571658,
                "tmdate": 1700571571658,
                "mdate": 1700571571658,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]