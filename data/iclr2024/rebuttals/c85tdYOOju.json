[
    {
        "title": "Decoupling Weighing and Selecting for Integrating Multiple Graph Pre-training Tasks"
    },
    {
        "review": {
            "id": "g9827mwDlo",
            "forum": "c85tdYOOju",
            "replyto": "c85tdYOOju",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4998/Reviewer_66Ho"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4998/Reviewer_66Ho"
            ],
            "content": {
                "summary": {
                    "value": "This paper focus on multi-tasking graph pre-training and proposes a weighting and selecting network to model the compatibility and importance of tasks. The proposed WAS consists of knowledge extraction and transfer step, and it is powered by the decoupled siamese networks to assign weights to each teacher and do selection. Experiments are conducted on different benchmark datasets to showcase its effectiveness."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well-written and easy to follow.\n\n2. The evaluation is conducted on both graph-level and instance-level tasks.\n\n3. The idea on solving the compatibility issue of multiple tasks is new and insightful."
                },
                "weaknesses": {
                    "value": "1. The motivation of focusing on graph-structured data is unclear.\n\n2. Compared to the baseline methods for multi-task learning on instance-level, the improvement from WAS is marginal. On the largest graphs in the experiment (ogbn-arxiv), it achieves worse performance compared to baseline method.\n\n3. Besides the empirical results, the theoretical analysis is still needed to answer why both selecting and weighing is needed,  and why there is compatibility issue (i.e., why some tasks shouldn't be selected)."
                },
                "questions": {
                    "value": "1. The proposed method does not treat graph-structured data specially, nor is it tailored for graph modeling. Why is it positioned for pre-training on graphs, instead of for general pre-training?\n\n2. Any in-depth understanding of the relationship for the selected tasks? Are they independent?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4998/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698521202383,
            "cdate": 1698521202383,
            "tmdate": 1699636487651,
            "mdate": 1699636487651,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8zVT6oa8QA",
                "forum": "c85tdYOOju",
                "replyto": "g9827mwDlo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4998/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4998/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 66Ho (1/2)"
                    },
                    "comment": {
                        "value": "Dear Reviewer 66Ho:\n\n\nThank you for your valuable feedback and constructive suggestions. We sincerely appreciate your acknowledgment of effectiveness and writing quality. In the revised manuscript, we have highlighted existing content (which may have been omitted, but which we would like to bring to your attention) in red and revised or new content in blue. Our detailed response to your concerns is listed as follows:\n\n------\n\n**W1.&Q1.**  The motivation of focusing on graph-structured data is unclear. The proposed method does not treat graph-structured data specially, nor is it tailored for graph modeling. Why is it positioned for pre-training on graphs, instead of for general pre-training?\n\n**A1.**\n\nThe reason for your confusion may be because we have not added too many graph-specific priors to our method. This is due to the diversity of downstream tasks in the graph domain, where different downstream tasks contain different instance levels, e.g., graph classification and node classification. This leads to the fact that imposing too many restrictions on a certain method can make it difficult to handle diverse downstream tasks, since the tasks employed in the method already provide domain-specific a priori (e.g., node masking, edge prediction, etc.). For example, the previous work AutoSSL\\[1], used graph homophily as a priori knowledge, which caused it to only be able to handle node-level tasks and not graph-level tasks. We note that this series of subsequent work (ParetoGNN\\[2], AGSSL\\[3]) follows the idea of not imposing a priori on methods. Our starting point is common, i.e., imposing too many a priori on combinatorial methods can limit the scenarios in which the methods can be used.\n\nAs with previous tasks, the reason we are a framework positioned on graph-structured data is that the phenomena we observe (sharp performance degradation due to conflicts among graph pre-training tasks, etc.) are graph-based. In addition, our method does show potential for extension to other domains, but we do not believe that this should be one of the drawbacks, and we will consider extensions of our method as meaningful follow-on work.\n\n\n\n\\[1]Wei Jin, Xiaorui Liu, Xiangyu Zhao, Yao Ma, Neil Shah and Jiliang Tang, Automated Self-Supervised Learning for Graphs, ICLR2022\n\n\\[2] Mingxuan Ju, Tong Zhao, Qianlong Wen, Wenhao Yu, Neil Shah, Yanfang Ye and Chuxu Zhang, Multi-task Self-supervised Graph Neural Networks Enable Stronger Task Generalization, ICLR 2023\n\n\\[3] Lirong Wu, Yufei Huang, Haitao Lin, Zicheng Liu, Tianyu Fan and Stan Z. Li. Automated graph self-supervised learning via multi-teacher knowledge distillation. arxiv2210.02099\n\n\n\n\n------\n\n\n\n**W2.** Compared to the baseline methods for multi-task learning on instance-level, the improvement from WAS is marginal. On the largest graphs in the experiment (ogbn-arxiv), it achieves worse performance compared to baseline method.\n\n**A2.** \n\nWe kindly point out that \"the improvement from WAS is marginal\" might be somewhat inappropriate.\n\nFirstly, the major contribution of our method lies in its ability to **continually achieve performance gains from an expanding task pool**, a capability not possessed by previous methods. Besides this primary contribution, our method outperforms previous multi-task learning methods in terms of performance on seven node-level datasets, even when the task pool is kept the same. Additionally, our method is capable of handling **graph-level tasks**."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4998/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700231756092,
                "cdate": 1700231756092,
                "tmdate": 1700273251207,
                "mdate": 1700273251207,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1IzEjWfBqa",
                "forum": "c85tdYOOju",
                "replyto": "g9827mwDlo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4998/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4998/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 66Ho (2/2)"
                    },
                    "comment": {
                        "value": "**W3.&Q2.** Besides the empirical results, the theoretical analysis is still needed to answer why both selecting and weighing is needed, and why there is compatibility issue (i.e., why some tasks shouldn't be selected). Any in-depth understanding of the relationship for the selected tasks? Are they independent?\n\n**A3.** \n\nWe sincerely appreciate your suggestions for enhancing the clarity of our manuscript, which will make it stronger!  We have provided relevant analysis in Fig.1 of the Introduction, where Fig.1b illustrates the importance of a reasonable weighting scheme, and Fig.1c demonstrates the compatibility issues between different tasks. You may want us to include more detailed explanations regarding these issues. \n\nWe will address your concerns in three steps: 1. Are different tasks relatively independent? 2. Why do compatibility issues arise? 3. Theoretically speaking, what are the advantages of incorporating a selection mechanism as opposed to simply using a weighting approach? For the figures and proofs mentioned in the following content, please refer to the Appendix G.\n\n- **[Are tasks independent? Different teachers acquired different knowledge during pre-training, and this knowledge remains different after migration to downstream tasks]** In Fig. A2, we analyze the knowledge acquired by different teachers. Different pre-training methods, e.g., GraphCL performs contrastive learning between instances, while AM masks node attributes on a single instance, are able to acquire different knowledge. Fig. A2 visualizes the representations obtained by different teachers after training on the same pre-training dataset by T-sne. It can be seen that they are distributed in different spaces, which means that **they do not acquire the same knowledge.** We can conclude that **the knowledge they acquire is essentially independent**. \n- **[Why do compatibility issues arise? Different instances may require different knowledge.]** Then, for downstream tasks, what differences do different teachers' acquired knowledge make? **Fig. A3** visualizes the graphs from the test set of BACE, where blue dots represent molecular graphs that can be correctly predicted for all seven teachers, while red dots represent graphs that can only be correctly predicted by some of the seven teachers. The red dots make up the majority, which means that **different teachers will be able to predict different graphs.** Therefore, for a given instance, if  a teacher whose knowledge is not suitable for them is chosen, this may cause conflicts and could be worse than not having that teacher at all. This is the compatibility issue.\n- **[What are the advantages of incorporating a selecting mechanism? Having only the weighting module can lead to an inability to improve performance.]** In the Appendix G, we provide a theoretical analysis of the performance without the selecting module (only the weighting module). Please refer to the Appendix G. Through this analysis, we can conclude that the selection module is a crucial factor for the model to achieve continuous performance improvement from an ever-expanding task pool.\n\nMore detailed content (figures and theoretical analysis) has been added to the **Appendix G**. Please refer to **Appendix G** for further information.\n\n---\n\nIn light of these responses, we hope we have addressed your concerns, and hope you will consider increasing your score. If we have left any notable points of concern unaddressed, please share and we will attend to these points."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4998/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700231773286,
                "cdate": 1700231773286,
                "tmdate": 1700231773286,
                "mdate": 1700231773286,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nWTZMzUH52",
                "forum": "c85tdYOOju",
                "replyto": "g9827mwDlo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4998/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4998/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to hearing from you"
                    },
                    "comment": {
                        "value": "Dear Reviewer 66Ho,\n\nWe sincerely thank you for taking the time to review our manuscript and providing valuable suggestions.\n\nUnlike previous years, there will be no second stage of author-reviewer discussions this year, and a recommendation needs to be provided by November 22, 2023. Considering that the author-reviewer discussion phase is nearing the end, we would like to be able to confirm whether our responses have addressed your concerns.\n\nWe have provided detailed replies to your concerns a few days ago, and we hope we have satisfactorily addressed your concerns. If so, could you please consider increasing your rating? If you still need any clarification or have any other concerns, please feel free to contact us and we are happy to continue communicating with you.\n\nBest,\n\nAuthors"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4998/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700532338498,
                "cdate": 1700532338498,
                "tmdate": 1700532338498,
                "mdate": 1700532338498,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Yob5RMIl7R",
                "forum": "c85tdYOOju",
                "replyto": "g9827mwDlo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4998/Reviewer_66Ho"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4998/Reviewer_66Ho"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your responses"
                    },
                    "comment": {
                        "value": "Having thoroughly reviewed the responses and comments from other reviewers, I've decided to keep my original score."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4998/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700692969092,
                "cdate": 1700692969092,
                "tmdate": 1700692969092,
                "mdate": 1700692969092,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cxxGWOmNmj",
            "forum": "c85tdYOOju",
            "replyto": "c85tdYOOju",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4998/Reviewer_pHBL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4998/Reviewer_pHBL"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors have studied how to effectively integrate multiple graph pre-training tasks. They have identified two important collaborative processes, i.e., selecting and weighing.  They propose a new instance-level framework for integrating multiple graph pre-training tasks, named WAS (Weigh And Select), where the weighing and selecting processes are combined by decoupled siamese networks. Extensive experiments on 16 graph datasets have been performed to demonstrate the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. In this paper, the authors introduce a new framework, namely WAS, for task selecting and importance weighing to integrate multiple graph pre-training tasks.\n\n2. The authors show the limitations of existing weighing-only schemes and demonstrate the importance of task selecting process.\n\n3. The authors have performed extensive experiments to demonstrate the effectiveness of the proposed method. \n\n4. This paper is clearly written and easy to follow."
                },
                "weaknesses": {
                    "value": "1. In Table 1, 2, and 3, the authors do not compare the proposed WAS framework with existing frameworks that only focus on weighing (i.e., AutoSSL, ParetoGNN, AUX-TS, and AGSSL). However, in Table 4, WAS is compared with these methods. This makes the experimental settings not consistent.\n\n2. The complexity of the proposed has not been studied. Although the proposed method can achieve some performance improvement, it may take more training/inference time, due to combining multiple pre-training tasks. The authors need to have some analysis about the complexity of the proposed method.\n\n3. The authors also need to perform experiments to analyse how many tasks are selected for each instance on average."
                },
                "questions": {
                    "value": "1. On average, how many tasks are usually selected for each instance?\n\n2. From Figure 6(d), we can observe that the following 4 tasks have the largest weights, i.e, EP, AM, IG, and CP. How about we only use these 4 tasks and learning the their weights for prediction?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4998/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698849149781,
            "cdate": 1698849149781,
            "tmdate": 1699636487551,
            "mdate": 1699636487551,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YCyWwTJnO7",
                "forum": "c85tdYOOju",
                "replyto": "cxxGWOmNmj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4998/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4998/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer pHBL"
                    },
                    "comment": {
                        "value": "Dear Reviewer pHBL:\n\n\nThank you for your valuable feedback and constructive suggestions. We sincerely appreciate your acknowledgment of effectiveness and writing quality. In the revised manuscript, we have highlighted existing content (which may have been omitted, but which we would like to bring to your attention) in red and revised or new content in blue.\n\nOur detailed response to your concerns is listed as follows:\n\n------\n\n**W1.** In Table 1, 2, and 3, the authors do not compare the proposed WAS framework with existing frameworks that only focus on weighing (i.e., AutoSSL, ParetoGNN, AUX-TS, and AGSSL). However, in Table 4, WAS is compared with these methods. This makes the experimental settings not consistent.\n\n**A1.** \n\nWe kindly point out that, in Table 1, we have compared these works.\n\nIn Table 2 and 3, we did not compare these works because these methods are primarily focused on **node-level tasks**, rather than **graph-level tasks**. Methods like AutoSSL struggle with the **graph-level tasks** that we addressed in Table 2 and 3, making a comparison unfeasible. Since Table 4 concerns **node-level tasks**, we were able to compare our method with these approaches.\n\n\n\n------\n\n\n**W2.** The complexity of the proposed has not been studied. Although the proposed method can achieve some performance improvement, it may take more training/inference time, due to combining multiple pre-training tasks. The authors need to have some analysis about the complexity of the proposed method.\n\n**A2.** \n\nWe have provided an analysis of additional parameters, extra storage space, additional time consumption, and extra time complexity in **Appendix C (complexity analysis)**, and mentioned this at the end of Sec. 4.2.3 (Line 267).\n\n\n\n------\n\n\n**W3.&Q1.** The authors also need to perform experiments to analyze how many tasks are selected for each instance on average. On average, how many tasks are usually selected for each instance?\n\n**A3.**\n\nWe sincerely appreciate your suggestions for enhancing the clarity of our manuscript, which will make it stronger! In the following Table, we count the average number of selected teachers *number*, and then count the *ratio of selecting Top-1/Top-[number] important teacher*.\n\nFrom the table, we can draw the following conclusions:\n\n1. teachers with the highest weights have a higher probability of being selected\n2. the selection module does not necessarily always select the higher weight teachers\n\n|                                                     | Cora | Citeseer | BACE | BBBP |\n| :-------------------------------------------------- | :--- | :------- | :--- | :--- |\n| *number*                                            | 2.7  | 3.9      | 2.9  | 3.1  |\n| *ratio of selecting Top-1 important teacher*        | 0.63 | 0.71     | 0.68 | 0.55 |\n| *ratio of selecting Top-[number] important teacher* | 0.26 | 0.34     | 0.41 | 0.10 |\n\nThis table is added to the updated **Appendix F.**\n\n\n\n------\n\n\n**Q2.** From Figure 6(d), we can observe that the following 4 tasks have the largest weights, i.e, EP, AM, IG, and CP. How about we only use these 4 tasks and learning the their weights for prediction?\n\n**A4.** \n\nFigure 6(d) represents only the result of one instance. The tasks with higher weights in this instance may not necessarily have high weights in other instances. \n\nUsing only these four tasks could potentially lead to a decline in performance. In the table below, \"7-tasks\" indicates that the task pool contains all 7 tasks, while \"4-tasks\" means the task pool includes only four tasks (EP, AM, IG, and CP).\n\n|         | 7-tasks      | 4-tasks      | EP           | AM           | IG           | CP           |\n| :------ | :----------- | :----------- | ------------ | ------------ | ------------ | ------------ |\n| Toxcast | 63.7$\\pm$1.5 | 62.8$\\pm$1.1 | 59.0$\\pm$0.9 | 62.6$\\pm$0.1 | 60.7$\\pm$0.9 | 61.7$\\pm$0.7 |\n\nWith the reduction of tasks, there was a decrease in performance, but it still remained better than using a single task.\n\n---\n\nIn light of these responses, we hope we have addressed your concerns, and hope you will consider increasing your score. If we have left any notable points of concern unaddressed, please share and we will attend to these points."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4998/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700231716834,
                "cdate": 1700231716834,
                "tmdate": 1700273089524,
                "mdate": 1700273089524,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5pkYNHcuA2",
                "forum": "c85tdYOOju",
                "replyto": "cxxGWOmNmj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4998/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4998/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to hearing from you"
                    },
                    "comment": {
                        "value": "Dear Reviewer pHBL,\n\nWe sincerely thank you for taking the time to review our manuscript and providing valuable suggestions.\n\nUnlike previous years, there will be no second stage of author-reviewer discussions this year, and a recommendation needs to be provided by November 22, 2023. Considering that the author-reviewer discussion phase is nearing the end, we would like to be able to confirm whether our responses have addressed your concerns.\n\nWe have provided detailed replies to your concerns a few days ago, and we hope we have satisfactorily addressed your concerns. If so, could you please consider increasing your rating? If you still need any clarification or have any other concerns, please feel free to contact us and we are happy to continue communicating with you.\n\nBest,\n\nAuthors"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4998/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700532314441,
                "cdate": 1700532314441,
                "tmdate": 1700532314441,
                "mdate": 1700532314441,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PNO67APYD9",
            "forum": "c85tdYOOju",
            "replyto": "c85tdYOOju",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4998/Reviewer_C6hu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4998/Reviewer_C6hu"
            ],
            "content": {
                "summary": {
                    "value": "In recent times, graph pre-training for graph representation learning has gained prominence, with numerous pre-training tasks emerging. Integrating knowledge from various pre-training tasks is now a focal research area. Two critical collaborative processes for integration are: (1) selecting the best task combination considering their compatibility and (2) determining the importance of the chosen tasks. While much research has been on weighing tasks, selection has received lesser attention. This paper introduces a new instance-level framework named Weigh And Select (WAS) that merges both processes using decoupled Siamese networks. WAS adaptively determines the best task combination for individual instances, leading to a tailored instance-level task weighing strategy. Experiments across 16 graph datasets reveal WAS's efficacy, producing results comparable to top-performing methods by merging several basic tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "S1. The authors of the paper have done a good job in providing a compelling motivation for their research. Their thorough analysis of different pre-training tasks combined with a detailed examination of several datasets showcases their comprehensive approach. Furthermore, their assessment of task importance and compatibility provides valuable insights, shedding light on the central issue at hand.\n\nS2. In terms of addressing the research concerns, the authors have carefully identified and highlighted them. These concerns have been formulated into four well-defined research questions that give readers a clear roadmap of the study's objectives. On a broader note, the manuscript is well-organized, with a structured flow that facilitates easy comprehension, making the writing lucid and clear to the audience.\n\nS3. With respect to the empirical aspect of the study, the authors have presented an exhaustive set of experimental results. These results showcase outcomes that are indeed promising."
                },
                "weaknesses": {
                    "value": "W1. The concept of instance is not well defined. Different pre-training tasks may have different definitions of instances fundamentally (eg. at a node-level, subgraph/graph-level, edge-level etc). Having a common and converged definition may not work well. Furthermore, downstream task could also have different definitions of instances.\n\nW2. While 4 questions are clear in the identification of the 4 steps/issues to address, the solutions are quite standard. E.g. using Gumbel-Softmax sampling for Bernoulli distribution,  the weighting scheme and the Siamese network architecture are all well known tools.\n\nW3. The compatibility issue, or interferences among tasks have been observed in previous work in other areas or problem settings [a,b,c]. Some discussion on this aspect, and its particular challenges in graph context, would further strengthen the motivation of the paper. \n\n[a] Zeyuan Wang, Qiang Zhang, HU Shuang-Wei, Haoran Yu, Xurui Jin, Zhichen Gong, and Huajun Chen. 2022. Multi-level Protein Structure Pre-training via Prompt Learning. In The Eleventh International Conference on Learning Representations.\n[b] Sen Wu, Hongyang R Zhang, and Christopher R\u00e9. 2020. Understanding and improving information transfer in multi-task learning. arXiv preprint arXiv:2005.00944 (2020).\n[c] Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. 2020. Gradient surgery for multi-task learning. Advances in Neural Information Processing Systems 33 (2020), 5824\u20135836."
                },
                "questions": {
                    "value": "Please see weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4998/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698982820902,
            "cdate": 1698982820902,
            "tmdate": 1699636487466,
            "mdate": 1699636487466,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2jz8EETOa2",
                "forum": "c85tdYOOju",
                "replyto": "PNO67APYD9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4998/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4998/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer C6hu (1/2)"
                    },
                    "comment": {
                        "value": "Dear Reviewer C6hu:\n\n\nThank you for your valuable feedback and constructive suggestions. We sincerely appreciate your acknowledgment of compelling motivation, thorough analysis and well-organized manuscript. In the revised manuscript, we have highlighted existing content (which may have been omitted, but which we would like to bring to your attention) in red and revised or new content in blue. \n\nOur detailed response to your concerns is listed as follows:\n\n------\n\n**W1.** The concept of instance is not well defined. Different pre-training tasks may have different definitions of instances fundamentally (eg. at a node-level, subgraph/graph-level, edge-level etc). Having a common and converged definition may not work well. Furthermore, downstream task could also have different definitions of instances.\n\n**A1.**\n\nWe kindly point out that your concerns are the same as those mentioned in our manuscript.\n\n> \" The concept of instance is not well defined. \"\n\nWe provided an example of instances on page 1,Line 36-37 (\"e.g., nodes in a social network or graphs in a molecular dataset\").\n\n> \" Different pre-training tasks may have different definitions of instances fundamentally \"\n\n We completely agree with your statement and we discussed this in Line 191-193 (Original text: \"For example, GraphCL performs contrastive learning between instances, while AM masks node attributes on a single instance\"). For a graph-level task, GraphCL defines an instance as a graph, whereas for AM, the instance is still a node.\n\nIt might be that our expression was not clear enough. We will modify it to \"For example, GraphCL performs contrastive learning between graphs, while AM masks node attributes on a single graph\".\n\n> \"downstream task could also have different definitions of instances.\"\n\nWe also fully agree with this. Therefore, we conducted experiments at both the node-level and graph-level to demonstrate the effectiveness of our method.\n\n**W2.** While 4 questions are clear in the identification of the 4 steps/issues to address, the solutions are quite standard. E.g. using Gumbel-Softmax sampling for Bernoulli distribution, the weighting scheme and the Siamese network architecture are all well known tools.\n\n**A2.**\n\nWe sincerely thank you for appreciating our work \"4 questions are clear in the identification of the 4 steps/issues to address\". \n\nThe Gumbel-Softmax, weighing scheme, and the Siamese architecture are seen as a tool rather than a contribution.\n\nOur contributions are:\n\n1. examining the shortcomings of various existing multi-task graph learning methods (they mainly based on weighing-only method), i.e., not considering the compatibility issue.\n2. distinguishing between the compatibility issue and the importance issue, and using decoupling frameworks to deal with these two issues.\n\nConsidering that we are already achieving what the framework was designed for (1. good performance 2. the ability to sustain performance growth) by using those classic components, we kindly point out that we think that there is no need to force modifications to the classic components just to appear that we are different."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4998/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700231552117,
                "cdate": 1700231552117,
                "tmdate": 1700272581928,
                "mdate": 1700272581928,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "i8XgvCp1uv",
                "forum": "c85tdYOOju",
                "replyto": "PNO67APYD9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4998/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4998/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer C6hu (2/2)"
                    },
                    "comment": {
                        "value": "**W3.** The compatibility issue, or interferences among tasks have been observed in previous work in other areas or problem settings. Some discussion on this aspect, and its particular challenges in graph context, would further strengthen the motivation of the paper.\n\n**A3.** \n\nWe sincerely appreciate your suggestions for enhancing the motivation of our manuscript, which will make it stronger! Among the works you mentioned\\[a]\\[b]\\[c], \\[a] is specifically designed for transformers. However, the dominant architecture in the graph domain is still GNN, which limits its applicability. \n\n\\[b] differs in focus from our method. We focus on extracting and combining knowledge from various pre-trained tasks to enhance the performance of downstream tasks, while \\[b] is concerned with handling different downstream tasks when using a single model. \\[c] combines tasks through gradient surgery, a method similar to AutoSSL and ParetoGNN mentioned in our paper. These approaches combine task losses through various weighting methods. Firstly, they are unable to provide a customized combination for each instance. Secondly, they overlook compatibility issues, making it difficult to achieve continual performance improvement as the task pool expands.  Moreover, these works do not address the concept of \"compatibility\". Their solutions differ from ours as their main focus remains on various weighting methods, overlooking the necessity of selection.\n\nWe have updated Sec.2 to include these more discussions.\n\n---\n\n[a] Zeyuan Wang, Qiang Zhang, HU Shuang-Wei, Haoran Yu, Xurui Jin, Zhichen Gong, and Huajun Chen. 2022. Multi-level Protein Structure Pre-training via Prompt Learning. In The Eleventh International Conference on Learning Representations. \n\n [b] Sen Wu, Hongyang R Zhang, and Christopher R\u00e9. 2020. Understanding and improving information transfer in multi-task learning. arXiv preprint arXiv:2005.00944 (2020).\n\n [c] Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. 2020. Gradient surgery for multi-task learning. Advances in Neural Information Processing Systems 33 (2020), 5824\u20135836.\n\n---\n\nIn light of these responses, we hope we have addressed your concerns, and hope you will consider increasing your score. If we have left any notable points of concern unaddressed, please share and we will attend to these points."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4998/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700231601874,
                "cdate": 1700231601874,
                "tmdate": 1700231601874,
                "mdate": 1700231601874,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LK0TPlwgE7",
                "forum": "c85tdYOOju",
                "replyto": "i8XgvCp1uv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4998/Reviewer_C6hu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4998/Reviewer_C6hu"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed responses. I will weigh them carefully in the final review."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4998/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700315095083,
                "cdate": 1700315095083,
                "tmdate": 1700315095083,
                "mdate": 1700315095083,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "elJh4GopUy",
                "forum": "c85tdYOOju",
                "replyto": "PNO67APYD9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4998/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4998/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Replying to Reviewer C6hu"
                    },
                    "comment": {
                        "value": "Dear Reviewer C6hu,\n\nWe sincerely thank you for taking the time to review our manuscript and providing valuable suggestions. \n\nUnlike previous years, there will be no second stage of author-reviewer discussions this year, and a recommendation needs to be provided by November 22, 2023. We hope we have satisfactorily addressed your concerns. If so, could you please consider increasing your rating? If you still need any clarification or have any other concerns, please feel free to contact us and we are happy to continue communicating with you.\n\nBest,\n\nAuthors"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4998/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700359351384,
                "cdate": 1700359351384,
                "tmdate": 1700359392028,
                "mdate": 1700359392028,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "EgO6oTgxBY",
            "forum": "c85tdYOOju",
            "replyto": "c85tdYOOju",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4998/Reviewer_fcHp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4998/Reviewer_fcHp"
            ],
            "content": {
                "summary": {
                    "value": "Since the roles of pre-training tasks vary with downstream tasks and different pre-training tasks may not be compatible, the authors point out that selecting and weighting tasks are key parts of the pre-training. Moreover, the existing methods that select tasks based on learned weights would confuse the roles of selection and weighting. To address these limitations, they propose WAS to decouple these two processes."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed WAS decouples the selecting and weighting to avoid performance reduction caused by task conflicts. They calculate weights based on output distributions instead of losses to address the non-comparability between different loss functions.\n2. The proposed method can automatically select the number and type of suitable pre-training tasks for different downstream instances.\n3. The authors conduct extensive comparison experiments, and the proposed WAS performs better on both node-level and graph-level tasks. In addition, they visualized the evolution processes of selecting tasks and updating weights, which proves the decoupling of selection and importance weighting."
                },
                "weaknesses": {
                    "value": "1. The authors should add more ablation experiments to demonstrate the effectiveness of the proposed model, such as the role of different updating methods in decoupling the selecting and weighting outputs, and the role of reweighting after selection. \n2. The comparison model in A5 needs more description to distinguish it from A1, such as whether Random-Select and ALL use learned weights."
                },
                "questions": {
                    "value": "Please refer to Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethical issues."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4998/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699169763022,
            "cdate": 1699169763022,
            "tmdate": 1699636487387,
            "mdate": 1699636487387,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yZZ0lUqTX8",
                "forum": "c85tdYOOju",
                "replyto": "EgO6oTgxBY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4998/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4998/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer fcHp"
                    },
                    "comment": {
                        "value": "Dear Reviewer fcHp:\n\n\nThank you for your valuable feedback and constructive suggestions. We sincerely appreciate your acknowledgment of the effectiveness of our work. In the revised manuscript, we have highlighted existing content (which may have been omitted, but which we would like to bring to your attention) in red and revised or new content in blue. Our detailed response to your concerns is listed as follows:\n\n------\n\n**W1.** The authors should add more ablation experiments to demonstrate the effectiveness of the proposed model, such as the role of different updating methods in decoupling the selecting and weighting outputs, and the role of reweighting after selection.\n\n**A1.** \n\nYou mentioned \"the role of different updating methods\", and we speculate that you hope to further understand the relationship between the momentum-update method and the decoupling of the selecting and weighting modules. It's about our design intuition, as we want the result of selecting to utilize the information from the importance, not the other way around. Considering that a traditional ablation (removing the momentum update method) would render our method completely ineffective, we use a comparative approach. In the table below, Method A represents gradient-based updating for weighing and momentum-based updating for selecting (i.e., same as WAS); Method B represents gradient-based updating for selecting and momentum-based updating for weighing. It can be seen that Method B performs much worse than Method A (WAS). This indicates that our update method is more effective and aligns better with the design concept.\n\n|            | BACE         | BBBP         | **Sider**    | **HIV**      |\n| :--------- | :----------- | :----------- | :----------- | :----------- |\n| *Method A* | 80.7$\\pm$0.5 | 70.5$\\pm$1.0 | 60.4$\\pm$0.4 | 78.4$\\pm$1.9 |\n| *Method B* | 73.1$\\pm$0.9 | 62.8$\\pm$1.5 | 51.8$\\pm$3.2 | 73.4$\\pm$1.8 |\n\nRegarding \"the role of reweighting after selection,\" this is done to standardize the magnitude of knowledge distilled to the student model. For instance, if we select teachers with weights of 0.1 and 0.2 from three teachers weighted as 0.1, 0.2, and 0.7, but do not perform reweighting, the final sum of the weights will not equal 1. This would result in the number of selected teachers significantly impacting performance. As can be seen from the experiment, eliminating this would make the results very poor, as the teacher would be completely unable to guide the students.\n\n|                       | BACE         | HIV          | **Tox21**    | **BBBP**     |\n| :-------------------- | :----------- | :----------- | :----------- | :----------- |\n| *WAS*                 | 80.7$\\pm$0.5 | 78.4$\\pm$1.9 | 75.4$\\pm$0.7 | 70.5$\\pm$1.0 |\n| *WAS w/o re-weighing* | 65.1$\\pm$3.7 | 63.3$\\pm$2.4 | 64.1$\\pm$3.0 | 60.7$\\pm$5.1 |\n\nThis additional result has already been added to the Table.5. Thank you again for your suggestion!\n\n\n\n**W2.** The comparison model in A5 needs more description to distinguish it from A1, such as whether Random-Select and ALL use learned weights.\n\n**A2.** \n\nWe sincerely appreciate your suggestions for enhancing the readability and clarity of our manuscript, which will make it stronger! \n\nBoth Random-Select and ALL use learned weights, but Random-Select randomly select teachers, and ALL select all the teachers. We add corresponding explanations in the updated manuscript to better clarify this point.\n\n---\n\nIn light of these responses, we hope we have addressed your concerns. If we have left any notable points of concern unaddressed, please share and we will attend to these points."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4998/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700231400386,
                "cdate": 1700231400386,
                "tmdate": 1700272384010,
                "mdate": 1700272384010,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "slZ8BcaisN",
                "forum": "c85tdYOOju",
                "replyto": "EgO6oTgxBY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4998/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4998/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to hearing from you"
                    },
                    "comment": {
                        "value": "Dear Reviewer fcHp,\n\nWe sincerely thank you for taking the time to review our manuscript and providing valuable suggestions.\n\nUnlike previous years, there will be no second stage of author-reviewer discussions this year, and a recommendation needs to be provided by November 22, 2023. Considering that the author-reviewer discussion phase is nearing the end, we would like to be able to confirm whether our responses have addressed your concerns.\n\nWe have provided detailed replies to your concerns a few days ago, and we hope we have satisfactorily addressed your concerns. If you still need any clarification or have any other concerns, please feel free to contact us and we are happy to continue communicating with you.\n\nBest,\n\nAuthors"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4998/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700532244405,
                "cdate": 1700532244405,
                "tmdate": 1700532244405,
                "mdate": 1700532244405,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7Kuo4lT6OB",
                "forum": "c85tdYOOju",
                "replyto": "slZ8BcaisN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4998/Reviewer_fcHp"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4998/Reviewer_fcHp"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the responses"
                    },
                    "comment": {
                        "value": "I have read the author's response and other reviews. I would like to keep my review score for the paper and encourage the authors to incorporate these discussions into the main paper."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4998/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700635457257,
                "cdate": 1700635457257,
                "tmdate": 1700635457257,
                "mdate": 1700635457257,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]