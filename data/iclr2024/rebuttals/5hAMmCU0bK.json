[
    {
        "title": "Towards Robust Offline Reinforcement Learning under Diverse Data Corruption"
    },
    {
        "review": {
            "id": "7KJwbSg4M5",
            "forum": "5hAMmCU0bK",
            "replyto": "5hAMmCU0bK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5914/Reviewer_wKTe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5914/Reviewer_wKTe"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors investigate the robustness of offline RL algorithms when dealing with data corruption, including states, actions, rewards, and dynamics. \nThey then introduce Robust IQL (RIQL), an offline RL algorithm that enhances robustness through observation normalization, Huber loss, and quantile Q estimators. \nEmpirical evaluations demonstrate RIQL's exceptional resistance to various data corruption types, both random and adversarial. \nThis research sheds light on the vulnerability of current offline RL algorithms and provides a promising solution to enhance their robustness in real-world scenarios."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ This study illuminates the susceptibility of existing offline RL algorithms while offering a promising approach to bolster their resilience in real-world environments. The proposed analysis is unquestionably relevant to the offline RL community. While there have been previous works (e.g., Zhang et al. (2022)) that offer theoretical analyses regarding the impact of using contaminated data in offline RL algorithms, this work appears to take it a step further by examining the impact of various forms of contamination (actions, states, etc.) and providing a highly detailed analysis for the offline IQL model. As a conclusion, it suggests a robust alternative for IQL, addressing how to mitigate its identified weaknesses.\n\n\n+ The theoretical and experimental analysis presented in Section 3 is elegantly and clearly articulated. The authors' discovery about the robustness of using a weighted imitation learning strategy is intriguing, but for the specific case of dynamics corruption, this doesn't seem to hold true. This observation leads them to propose up to three modifications to the IQL model to enhance its robustness. Each of these modifications (observation normalization, Huber loss, and quantile Q estimators) is well-founded and justified.\n\n+ The experimental evaluation is comprehensive and very detailed. First, all state-of-the-art models in offline-RL are assessed to see how they perform in perturbation scenarios. Once the analysis is completed, the article focuses on improvements for the IQL model. The environments used for the experiments are well-known within the community (Halfcheetah, Halfcheetah, Halfcheetah). The results presented are conclusive. I also appreciate the ablation study in section 5.3, which allows the reader to understand the impact of the modifications applied to IQL until achieving the robust version (RIQL)."
                },
                "weaknesses": {
                    "value": "- Some particularly interesting, and I would say more challenging, environments have been left out of the experimental analysis, especially in terms of the potential impact of perturbations, and in which the original IQL model was tested. It would be interesting to hear the authors' opinion on this. I'm referring to the following environments: locomotion-v2, antmaze-xx, kitchen, adroit.\n\n- Somewhat, the theoretical analysis and the experimental evidence detailed in the section appear contradictory. The paper explicitly states this as follows: \"Our theoretical analysis suggests that the adoption of weighted imitation learning inherently offers robustness under data corruption. However, empirical results indicate that IQL still remains susceptible to dynamics attacks\".  This contradiction is not further examined in the manuscript. On the contrary, what is proposed is to address IQL's issue with dynamic attacks.\n\n- Previous works on certification protocols for offline RL against attacks has not been considered for the novel RIQL model proposed. This is an important weaknesses that should be addressed in the rebuttal. \n\n- Minor comments:\n(Wu, 2022) Update the reference as it is not anymore an ArXiv manuscript but an ICLR22 paper."
                },
                "questions": {
                    "value": "- As I pointed above, Have the authors evaluated how RIQL performs in any of the environments I mentioned above?\n- Why hasn't the new model RIQL been tested on the COPA protocol in (Wu, 2022)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5914/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698410550801,
            "cdate": 1698410550801,
            "tmdate": 1699636629051,
            "mdate": 1699636629051,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SyyIMORxxr",
                "forum": "5hAMmCU0bK",
                "replyto": "7KJwbSg4M5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5914/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5914/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the constructive comments, and we provide clarification to your concerns as follows. We would appreciate it if you have any further questions or comments.\n\n**Q1: more challenging, environments have been left out of the experimental analysis, especially in terms of the potential impact of perturbations ... I'm referring to the following environments: locomotion-v2, antmaze-xx, kitchen, adroit.**\n\n**A1:** Thank you for your suggestion. Our initial focus was to thoroughly study data corruption in commonly used MuJoCo environments. However, as the reviewer pointed out, we lacked experiments on more challenging tasks. **To address this concern, we conducted experiments on the AntMaze benchmark during the rebuttal period.** The results presented in Table 11 in Appendix I indicate that AntMaze poses a greater challenge compared to the MuJoCo benchmark. Most offline RL algorithms struggle to learn from corrupted datasets in these environments, primarily due to the high dimensionality of states and the issue of sparse rewards. In contrast, our proposed algorithm RIQL demonstrates superior effectiveness in learning policies from corrupted data, surpassing other algorithms significantly. These results further strengthen the findings in our paper and highlight the importance of RIQL. For a quick overview, we also provide the average results across four AntMaze environments below.\n\n\n\n| Attack Element |  BC  |  DT  | EDAC | CQL  |  IQL  | RIQL |\n|:--------------:|:----:|:----:|:----:|:----:|:----:|:-------:|\n|  Observation   |  0.0 |  0.0 |  0.0 |  0.2 |  4.8 | **20.7** |\n|     Action     |  0.0 |  0.0 |  0.0 |  4.0 | **49.0** |   47.8  |\n|     Reward     |  0.0 |  0.0 |  0.0 |  0.0 |  5.6 | **33.0** |\n|    Dynamics    |  0.0 |  0.0 |  0.0 | 16.7 | 18.1 | **36.6** |\n|     Average    |  0.0 |  0.0 |  0.0 |  5.2 | 19.4 | **34.5** |\n\n\n**Q2: the theoretical analysis and the experimental evidence detailed in the section appear contradictory ... empirical results indicate that IQL still remains susceptible to dynamics attacks. This contradiction is not further examined in the manuscript. On the contrary, what is proposed is to address IQL's issue with dynamic attacks.**\n\n**A2:**  Thank you for the valuable question. In fact, **our theoretical analysis not only aligns with our empirical findings but also provides an explanation for why IQL remains susceptible to dynamics corruption.** Specifically, the corruption error term in our theory scales as $\\sqrt{\\zeta/N}$ (ignoring other dependencies on $R_{\\max}, \\gamma$, and $M$), where $\\zeta = \\sum_{i=1}^N (\\zeta_i + \\log \\zeta_i')$ is the cumulative corruption level. This suggests that IQL works well for data corruption scenarios when $\\zeta \\ll N$. **In the dynamics corruption scenario, the heavy-tailed issue can cause $\\\\{\\zeta_i\\\\}\\_{i=1}^N$ to become very large ($\\\\|\\cdot \\\\|_{\\infty}$ over a heavy-tailed distribution can be very large, see definition of $\\zeta_i$ in Assumption 1), thereby making the upper bound in Theorem 3 very loose. This can explain IQL's susceptibility under dynamics corruption.**\nFurthermore, we would like to highlight that addressing the heavy-tailed issue via Huber regression considerably enhances IQL's performance. This, to a certain extent, confirms that the heavy-tailed issue is a crucial factor leading to IQL's poor performance, as the Huber loss serves as an effective method for tackling heavy-tailed regression problems (Lemma 5, Section 5.2).\nWe have added this discussion in Section 5.2, as highlighted in blue. \n\n\n**Q3: Previous works on certification protocols for offline RL against attacks has not been considered for the novel RIQL model proposed. This is an important weaknesses that should be addressed in the rebuttal.**\n\n**A3:** Thank you for the suggestion. Upon thoroughly examining related works, we discovered that COPA [1] is the only study focusing on the certified robustness of offline RL with data corruption. However, **COPA is not directly applicable to our setting due to several differences.** Specifically, COPA corrupts trajectories, while we corrupt independent transitions. Additionally, COPA's certification protocol necessitates a discrete action space, whereas our investigated environments are continuous. As proposing a new certification protocol is beyond the scope of our paper, we have not conducted certification. Meanwhile, Theorem 3 in our paper appears to offer some form of \"certification\" for the policy learned by IQL since it guarantees that IQL achieves reasonable performance when the offline dataset is corrupted.\n\n\n**Q4: Minor comments: (Wu, 2022) Update the reference as it is not anymore an ArXiv manuscript but an ICLR22 paper.**\n\n**A4:** Thank you for bringing this mistake to our attention. We have revised this in the revision.\n\n**References**\n\n[1] Wu F, et al. COPA: Certifying Robust Policies for Offline Reinforcement Learning against Poisoning Attacks. ICLR 2022."
                    },
                    "title": {
                        "value": "Response to Reviewer wKTe"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5914/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700229755972,
                "cdate": 1700229755972,
                "tmdate": 1700230149547,
                "mdate": 1700230149547,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DOofHR1d7a",
            "forum": "5hAMmCU0bK",
            "replyto": "5hAMmCU0bK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5914/Reviewer_dDco"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5914/Reviewer_dDco"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes Robust IQL (RIQL), an offline RL algorithm that works decently even when the dataset is corrupted. It first experimentally and theoretically shows that IQL is robust to dataset noise. Adding upon the discovery, it presents three heuristics that can further improve IQL's performance on the corrupted dataset: (1) observation normalization, (2) Huber loss for value function learning, (3) using the $\\alpha$-quantile of an ensemble of Q functions as the target value. The authors conducted experiments on varying the degree and type of corruption and showed that RIQL outperforms other baselines in most of the settings."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper tackles a novel problem where the offline dataset is corrupted. It provides an exact error bound of IQL under the data contamination setting supported by a mathematically sound argument. The paper also presents an interesting discovery that the Q target distribution has heavy tails and suggests a simple but clever solution, which is to use the Huber loss instead of an MSE loss. To prove the effectiveness of RIQL, multiple experiments were conducted, together with a thorough ablation study. Finally, the paper is overall well-written and easy to understand."
                },
                "weaknesses": {
                    "value": "1. The paper provides no plausible scenarios where a malicious attack on the offline dataset would occur.\n\n2. The definition of $\\pi_{\\mathcal{D}}(a\\mid s)$ is unclear.\n\n3. The paper assumes that IQL can learn the optimal value function $V^*$ from the corrupted dataset without justification.\n\n4. The random corruption setting used in the experiments is a bit unrealistic.\n\n    * If environmental noise exists, the entire dataset would be corrupted, not just a tiny portion as assumed in the experiments.\n\n    * Most offline RL datasets are collections of trajectories not $(s, a, r, s')$ 4-tuples. Adding random noise just to $s$ or $s'$ does not seem to make much sense.\n\n5. The adversarial corruption setting is not really \"adversarial\" towards algorithms other than EDAC since the adversarial noise was computed via projected gradient descent with respect to the Q functions learned by EDAC. To see whether RIQL is robust to malicious attacks, the adversarial noise should be computed with respect to $Q_\\alpha$ learned by RIQL.\n\n6. The paper does not contain experiments for sequence-modeling-based algorithms such as Decision Transformer (Chen et al., 2021) or Diffuser (Janner et al., 2022). As the sequence modeling approach is one of the main branches of offline RL, I believe they should be included.\n\n### Minor comments\n\n1. \u00a74.3 first paragraph \"This is evidenced in Figure 5(b), where the **penalty** for attacked data is larger...\" \u21d2 The term \"Penalty\" is used before being defined.\n\n2. \u00a7C.1 Eq. (11)  third equality \u21d2 swap $\\tilde{\\pi}_E$ and $\\pi_E$\n\n3. \u00a7C.1 \"and the last inequality is obtained by **Cauchy\u2013Schwarz inequality**\" \u21d2 Cauchy\u2013Schwarz inequality \u2192 H\u00f6lder's inequality\n\n4.  \u00a7C.1 Eq. (15)  \u21d2 $\\pi_\\mu(a\\mid s_i)$ \u2192 $\\pi_{\\mathcal{D}}(a\\mid s_i)$\n\n5. \u00a7C.1 Eq. (16)   \u21d2 $\\zeta$ \u2192 $\\zeta_i$\n\n### References\n\n1. Chen, Lili, et al. \"Decision transformer: Reinforcement learning via sequence modeling.\" Advances in neural information processing systems 34 (2021): 15084-15097.\n\n2. Janner, Michael, et al. \"Planning with diffusion for flexible behavior synthesis.\" arXiv preprint arXiv:2205.09991 (2022)."
                },
                "questions": {
                    "value": "1. The main theorem holds for any algorithm that uses AWR to learn the optimal policy. Is it possible to add experimental results of other algorithms based on AWR?\n\n2. Does observation normalization also improve the performance of other algorithms?\n\n3. The sentence \"This is likely because normalization can help to ensure that the algorithm's performance is not unduly influenced by larger-scale features.\" from \u00a74.1 is difficult to understand."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5914/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5914/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5914/Reviewer_dDco"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5914/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698607215383,
            "cdate": 1698607215383,
            "tmdate": 1699636628945,
            "mdate": 1699636628945,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aippkkLkew",
                "forum": "5hAMmCU0bK",
                "replyto": "DOofHR1d7a",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5914/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5914/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the constructive comments, and we provide clarification to your concerns as follows. We would appreciate it if you have any further questions or comments.\n\n**Q1: The paper provides no plausible scenarios where a malicious attack on the offline dataset would occur.**\n\n**A1:** To investigate a more challenging setting than the random corruption, we introduce the adversarial corruption, which modifies the values of state, action, reward, and next-state in an adversarial manner. This setting is particularly relevant in scenarios where malicious attackers exist. For example, some annotators might intentionally provide wrong responses for RLHF data collection or assign higher rewards for harmful responses, potentially rendering the LLM harmful for human use when learning from these annotated data. We include the example in Section 1.\n\n**Q2: The definition of $\\pi_{D}(a|s)$ is unclear.**\n\n**A2:** Thank you for your question. The policy distribution of the corrupted offline dataset is denoted by $\\pi_{\\mathcal{D}}$, and its formal definition in a discrete form is given by:\n\n$$\n\\pi_{\\mathcal{D}}(a|s) = \\frac{\\sum_{i = 1}^N\\mathbf{1}\\\\{s_i = s, a_i = a\\\\}}{\\max\\\\{1, \\sum_{i = 1}^N\\mathbf{1}\\\\{s_i = s\\\\}\\\\}}.\n$$\n\nHere, $\\mathbf{1}$ is an indicator function, and $N$ is the number of samples in dataset $\\mathcal{D}$. Correspondingly, $\\pi_{\\mu}(a|s)$ is the behavior policy that induces the clean data. We have clarified this in the revision.\n\n**Q3: The paper assumes that IQL can learn the optimal value function $V^{*}$ from the corrupted dataset without justification.**\n\n**A3:** As IQL's policy and value training can be separated, the analysis of obtaining the optimal value function $V^*$ and weighted imitation policy learning can be considered as two disjoint parts, with the latter being our primary focus. In fact, we have also demonstrated that Huber regression can effectively find the optimal value function even in the presence of the heavy-tailed issue (the original IQL paper proves this in the uncorrupted setting without the heavy-tailed issue). Please refer to the discussion in Appendix C.2 for details. We have revised the main paper to provide justification for this.\n\n\n**Q4: (1) If environmental noise exists, the entire dataset would be corrupted, not just a tiny portion. (2) Adding random noise just to $s$ or $s'$ does not seem to make much sense.**\n\n **A4:** Thank you for the valuable questions. Regarding the first question, our rationale is based on the fact that in many real-world scenarios, engineers often encounter a combination of high-quality data and poor-quality data. **This mixed data setting is common in robust imitation learning [1][2] and aligns with the $\\epsilon$-Contamination model in theoretical analysis [3]. In Section 6.2, we have investigated varying corruption rates from 0 to 50$\\\\%$, which is not a tiny portion.**  In cases where the entire dataset can be corrupted, it can be considered a special case in our setting by adjusting the corruption rate to 1.0.\n \nRegarding the second question, we remark that the use of full trajectories as the offline dataset is the typical choice in the context of finite-horizon MDPs (e.g., [3]). However, under the framework of discounted MDPs, storing offline RL datasets as independent 4-tuples is a commonly used approach in both theoretical literature [4][5][6][7] and empirical works [8][9]. **Particularly in TD learning, states $s$ and next-states $s'$ play distinct roles: the former defines the input distribution, while the latter determines the future outcome. Consequently, they can be independently corrupted.** In our experiments, we also observed different effects when corrupting $s$ and $s'$: the Huber loss can alleviate dynamics corruption, but it cannot mitigate observation corruption."
                    },
                    "title": {
                        "value": "Response to Reviewer dDco (part 1/3)"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5914/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700229176095,
                "cdate": 1700229176095,
                "tmdate": 1700230264413,
                "mdate": 1700230264413,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "POCc7BNjIJ",
                "forum": "5hAMmCU0bK",
                "replyto": "DOofHR1d7a",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5914/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5914/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q5: The adversarial corruption setting is not really \"adversarial\" towards algorithms other than EDAC. ... the adversarial noise should be computed with respect to $Q_{\\alpha}$ learned by RIQL.**\n\n**A5:** The adversarial corruption is not specific for EDAC, as we utilize pretrained Q functions on the clean dataset rather than the Q functions obtained during EDAC training on the corrupted dataset. Our motivation for the adversarial corruption is to perform projected gradient descent on an oracle Q function, creating a more challenging setting than random corruption. **In our preliminary experiments shown below, we assessed IQL under adversarial attacks using Q functions trained by CQL, MSG, EDAC, and IQL itself. We observed that EDAC's Q functions yield the most effective attack, while IQL's exhibit the weakest attack performance.** We speculate that this is due to value-based methods learning better value functions for attacking purposes. Consequently, we opted to use pretrained EDAC's Q functions to perform the attack.\n\n| Algorithm for corruption |      IQL      |      CQL      |      MSG      |      EDAC      |\n|:------------------------:|:------------:|:------------:|:------------:|:-------------:|\n| IQL Performance \u2193      | 2964.3\u00b1477.9 | 1153.8\u00b1333.7 | 1528.8\u00b1278.4 | **763.9\u00b1203.7** |\n\n\n**Q6: The paper does not contain experiments for sequence-modeling-based algorithms such as Decision Transformer (Chen et al., 2021) or Diffuser (Janner et al., 2022).**\n\n**A6:** Thank your for the suggestion. To solve the reviewer's concern, we (1) add an additional discussion in Appendix A regarding algorithms that utilize transformers and diffusion models, and (2) include the results of Decision Transformer under both random and adversarial corruption (Tables 9 and Table 10 in Appendix G). Despite DT leveraging trajectory history information and taking 3.1 times the epoch time compared to RIQL, it still underperforms RIQL by a large margin. For a quick overview, we also provide the average results across three environments (Halfcheetah, Walker2d, and Hopper) below.\n\n| Attack Method | Attack Element |       DT       |     RIQL     |\n|:-------------:|:--------------:|:-------------:|:------------:|\n|     Random    |  Observation   | **51.1\u00b16.3** |  39.4\u00b14.0  |\n|     Random    |     Action     |  20.7\u00b11.8  | **72.7\u00b13.2** |\n|     Random    |     Reward     |  60.9\u00b13.7  | **70.5\u00b15.4** |\n|     Random    |    Dynamics    |  51.1\u00b16.3  | **57.6\u00b13.4** |\n|  Adversarial  |  Observation   |  50.2\u00b17.5  | **52.2\u00b15.7** |\n|  Adversarial  |     Action     |  13.4\u00b11.3  | **53.8\u00b14.5** |\n|  Adversarial  |     Reward     |  56.8\u00b16.2  | **65.0\u00b14.0** |\n|  Adversarial  |    Dynamics    |  50.2\u00b17.5  | **54.0\u00b115.0** |\n|    Average    |                |     44.3     |   **58.1**   |\n\n\n**Q7: Minor comments**\n\n**A7:** Thank you for pointing out these typos. We have made revisions in our updated manuscript.\n\n**Q8: The main theorem holds for any algorithm that uses AWR to learn the optimal policy. Is it possible to add experimental results of other algorithms based on AWR?**\n\n**A8:** Yes, we add experimental results of AWAC [10], which utilizes a weighted imitation learning framework, in Figure 1 to provide additional validation for our theory. As shown in Figure 1, AWAC exhibits notably robust performance under observation and action corruption, despite not quite reaching the level of IQL. These results not only validate the effectiveness of weighted imitation learning in enhancing robustness but also indicate that IQL's expectile regression and detached value training contribute to further improvements, which is also evidenced by the ablation study in Appendix E.1."
                    },
                    "title": {
                        "value": "Response to Reviewer dDco (part 2/3)"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5914/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700229391092,
                "cdate": 1700229391092,
                "tmdate": 1700230329415,
                "mdate": 1700230329415,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Sjqrey72q6",
                "forum": "5hAMmCU0bK",
                "replyto": "DOofHR1d7a",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5914/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5914/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q9: Does observation normalization also improve the performance of other algorithms?**\n\n**A9:** \nThank you for your question. We implemented observation normalization for both EDAC and CQL on the Walker2d and Hopper tasks, incorporating random data corruption across four elements. The average results, as displayed below, indicate a modest overall improvement for these algorithms. \n\n\n| Environment | Attack Element | EDAC         | EDAC(norm)   | CQL          | CQL(norm)    |\n|-------------|----------------|--------------|--------------|--------------|--------------|\n| Walker2d    | observation    | -0.2\u00b10.3     | -0.1\u00b10.1     | 19.4\u00b11.6     | 33.9\u00b18.9     |\n|       Walker2d      | action         | 83.2\u00b11.9     | 90.8\u00b11.8     | 62.7\u00b17.2     | 67.2\u00b18.0     |\n|      Walker2d       | reward         | 4.3\u00b13.6      | 0.7\u00b11.2      | 69.4\u00b17.4     | 65.3\u00b15.7     |\n|     Walker2d        | dynamics       | -0.1\u00b10.0     | -0.2\u00b10.0     | -0.2\u00b10.1     | -0.3\u00b10.1     |\n| Hopper      | observation    | 1.0\u00b10.5      | 3.2\u00b11.7      | 42.8\u00b17.0     | 45.4\u00b19.1     |\n|  Hopper            | action         | 100.8\u00b10.5    | 100.6\u00b10.2    | 69.8\u00b14.5     | 64.0\u00b19.1     |\n|   Hopper           | reward         | 2.6\u00b10.7      | 4.7\u00b14.3      | 70.8\u00b18.9     | 61.8\u00b114.5    |\n|    Hopper          | dynamics       | 0.8\u00b10.0      | 0.8\u00b10.0      | 0.8\u00b10.0      | 0.9\u00b10.1      |\n| Average score |              | 24.1         | 25.1         | 41.9         | 42.3         |\n\n\n\n**Q10: The sentence \"This is likely because normalization can help to ensure that the algorithm's performance is not unduly influenced by larger-scale features.\" is difficult to understand.**\n\n**A10:** Thank you for pointing out this. To make it clear, we polish the sentence to \"Normalization is likely to help ensure that the algorithm's performance is not unduly affected by features with large-scale values.\".\n\n\n**References**\n\n[1] Sasaki F, Yamashina R. Behavioral cloning from noisy demonstrations. ICLR 2020.\n\n[2] Liu L, et al. Robust imitation learning from corrupted demonstrations[J]. ArXiv 2022.\n\n[3] Jin Y, et al. Is pessimism provably efficient for offline rl? ICML 2021.\n\n[4] Zhang X, et al. Corruption-robust offline reinforcement learning. AISTATS 2022.\n\n[5] Rashidinejad P. et al. Bridging Offline Reinforcement Learning and Imitation Learning:\nA Tale of Pessimism.\n\n[6] Xie T. et al. Bellman-consistent Pessimism for\nOffline Reinforcement Learning\n\n[7] Uehara M and Sun W. Pessimistic Model-based Offline Reinforcement Learning under Partial\nCoverage.\n\n[8] Kumar A, et al. Conservative q-learning for offline reinforcement learning. NeurIPS 2020.\n\n[9] An G, et al. Uncertainty-based offline reinforcement learning with diversified q-ensemble. NeurIPS 2021.\n\n[10] Nair A, et al. Awac: Accelerating online reinforcement learning with offline datasets. arXiv 2020."
                    },
                    "title": {
                        "value": "Response to Reviewer dDco (part 3/3)"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5914/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700229505833,
                "cdate": 1700229505833,
                "tmdate": 1700230518325,
                "mdate": 1700230518325,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VeD07XCiGY",
                "forum": "5hAMmCU0bK",
                "replyto": "Sjqrey72q6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5914/Reviewer_dDco"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5914/Reviewer_dDco"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed response. Here are some questions I have regarding your response.\n\nA2: Under this definition, $\\pi_{\\mathcal{D}}$ becomes a discrete distribution. How would you define the ratio between a discrete distribution $\\pi_{\\mathcal{D}}$ and a continuous distribution $\\pi_{\\mu}$?\n\nA4-2: I agree that regarding the dataset as a collection of 4-tuples can be helpful for the learning process, but in terms of actually collecting and storing the data, I feel the trajectory approach is just superior. First, the environmental interactions are collected in terms of trajectories, not $(s, a, r, s')$ 4-tuples. Second, trajectory-based storage is more memory efficient. Lastly, it is easy to convert a collection of trajectories into a collection of 4-tuples, but not the other way around. I can't think of any reason why one would adopt the 4-tuple approach to create an offline RL DB in practice.\n\nA10: What do you exactly mean by \"larger-scale features\"? Why does normalization help reduce their influence?"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5914/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700486521205,
                "cdate": 1700486521205,
                "tmdate": 1700486521205,
                "mdate": 1700486521205,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6lrwW7kd4R",
                "forum": "5hAMmCU0bK",
                "replyto": "DOofHR1d7a",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5914/Reviewer_dDco"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5914/Reviewer_dDco"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your reply. I want to make some clarifications regarding **Q4-2**, though.\n\n1. We can preserve the sequential information without allocating memory based on the longest trajectory by saving the data as a sequence of (observation, action, reward, truncated, terminated) 5-tuple. This is exactly how D4RL stores data.\n\n2. For MDPs, the 4-tuple view is sufficient, but for POMDPs, we need to preserve the sequential information. Most real-world problems are not Markovian, so the sequential information has to be preserved.\n\nTo sum up, even if an algorithm adopts the 4-tuple approach, it is better to store the data as a sequence of (observation, action, reward, truncated, terminated) 5-tuples and then convert it to a collection of $(s, a, r, s')$ 4-tuples in memory during the learning process. As attacks on the memory values are extremely difficult, I think assuming corruption would happen independently on s and s' is unrealistic."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5914/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700662804156,
                "cdate": 1700662804156,
                "tmdate": 1700662886058,
                "mdate": 1700662886058,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "m0mcgeOV3i",
            "forum": "5hAMmCU0bK",
            "replyto": "5hAMmCU0bK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5914/Reviewer_41NY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5914/Reviewer_41NY"
            ],
            "content": {
                "summary": {
                    "value": "This work concentrates on dealing with diverse types of data corruption associated with state, action, reward, and transition kernel in offline RL history datasets. In particular, it first did data corruption test on some existing offline RL algorithms to show their vulnerable behaviors against data corruption. A theoretical result for the robustness of IQL has been shown w.r.t. the data corruption ratio. Then a robust variant of IQL has been proposed and outperform offline RL baselines, which consist of three key parts: state normalization, Huber loss, and $\\alpha$-quantile Q ensemble."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. It shows interesting testing of the effect of data corruption for current offline RL algorithms.\n2. Some theoretical relationship between the data corruption ratio and the performance of IQL is presented.\n3. A new robust IQL algorithm has been proposed and outperforms conducted baselines when data corruption appears."
                },
                "weaknesses": {
                    "value": "1. Except for the traditional offline RL algorithms based on TD learning or the Bellman operator, there exists some other new baselines using a transformer or a diffusion model. It is helpful to involve the discussion about such algorithms at least.\n2. Since the new algorithm (RIQL) involves in ensemble, which is a very powerful trick in RL algorithms, it is better to add some baselines that also use ensemble Q, while not appearing in the experiments if I didn't miss something.\n3. There does not exist comparisons between the proposed algorithm RIQL to other existing robust RL algorithms, but only to non-robust counterparts."
                },
                "questions": {
                    "value": "1. Is there any study on ablation study for the proposed RIQL that is similar to Figure 1. It will be helpful to see how can RIQL handle different types of data corruption."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5914/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698623680603,
            "cdate": 1698623680603,
            "tmdate": 1699636628801,
            "mdate": 1699636628801,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6SeHt7xMJU",
                "forum": "5hAMmCU0bK",
                "replyto": "m0mcgeOV3i",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5914/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5914/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 41NY"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive comments, and we provide clarification to your concerns below. We would appreciate it if you have any further questions or comments.\n\n**Q1: other new baselines using a transformer or a diffusion model. It is helpful to involve the discussion about such algorithms at least.**\n\n**A1:** Thank you for the suggestion. We include a discussion in Appendix A regarding algorithms that utilize transformers and diffusion models. Additionally, we compare our method to Decision Transformer (DT) in both the random and adversarial corruption benchmarks, as shown in Tables 9 and Table 10 (Appendix G). Despite DT leveraging trajectory history information and taking 3.1 times the epoch time compared to RIQL, it still underperforms RIQL by a large margin. For a quick overview, we also provide the average scores over three environments (Halfcheetah, Walker2d, and Hopper) below.\n\n| Attack Method | Attack Element |   DT   |  RIQL  |\n|:-------------:|:--------------:|:-----:|:-----:|\n|     Random    |  Observation   | **51.1** |  39.4  |\n|     Random    |     Action     |  20.7  | **72.7** |\n|     Random    |     Reward     |  60.9  | **70.5** |\n|     Random    |    Dynamics    |  51.1  | **57.6** |\n|  Adversarial  |  Observation   |  50.2  | **52.2** |\n|  Adversarial  |     Action     |  13.4  | **53.8** |\n|  Adversarial  |     Reward     |  56.8  | **65.0** |\n|  Adversarial  |    Dynamics    |  50.2  | **54.0** |\n|    Average    |                |  44.3  | **58.1** |\n\n**Q2: add some baselines that also use ensemble Q, while not appearing in the experiments if I didn't miss something.**\n\n**A2:** In our initial submission, we have included SOTA ensemble-based baselines, such as EDAC [1] and MSG [2]. While these baselines demonstrate exceptional performance on clean data, we note that they are considerably more vulnerable to data corruption, with a performance drop of approximately $60\\% \\sim 70\\%$, as presented in Table 1 and Table 2 (Section 6.1).\n\n**Q3: no comparisons between the proposed algorithm RIQL to other existing robust RL algorithms**\n\n**A3:** To the best of our knowledge, prior to our submission, there were no empirical robust offline RL baselines designed for the data corruption setting. Previous research primarily focused on theoretical guarantees [3], certification protocols [4], and evaluations [5] for offline RL with data corruption. However, we find a recent study [6] that introduced a solution called UWMSG for addressing reward and dynamics corruption, and we compare it with our proposed method. \n\nThe comparative results can be found in Table 9 and Table 10 (Appendix G). Notably, RIQL demonstrates significantly improved performance compared to UWMSG. We believe that our work will serve as a strong baseline for future research in this setting. For a quick overview, we have also provided the average scores across three environments below.\n\n| Attack Method | Attack Element | UWMSG |  RIQL  |\n|:-------------:|:--------------:|:-----:|:-----:|\n|     Random    |  Observation   |  5.0  | **39.4** |\n|     Random    |     Action     | 65.7  | **72.7** |\n|     Random    |     Reward     | 55.6  | **70.5** |\n|     Random    |    Dynamics    |  9.6  | **57.6** |\n|  Adversarial  |  Observation   |  8.3  | **52.2** |\n|  Adversarial  |     Action     | 26.9  | **53.8** |\n|  Adversarial  |     Reward     | 40.1  | **65.0** |\n|  Adversarial  |    Dynamics    |  4.6  | **54.0** |\n|    Average    |                | 27.0  | **58.1** |\n\n**Q4: Is there any study on ablation study for the proposed RIQL that is similar to Figure 1. It will be helpful to see how can RIQL handle different types of data corruption.**\n\n**A4:** Thank you for the suggestion. In Appendix H, we include an additional ablation study under separate observation, action, reward, and dynamics corruption. These results are helpful for understanding how each component of RIQL contributes to performance under various data corruption scenarios. For example, from the results, we conclude that the Huber loss is more beneficial for reward and dynamics corruption, but less effective for observation and action corruption. Overall, all three components play an important role in enhancing the performance of RIQL.\n\n\n**References**\n\n[1] An G, et al. Uncertainty-based offline reinforcement learning with diversified q-ensemble. NeurIPS, 2021.\n\n[2] Ghasemipour K, et al. Why so pessimistic? estimating uncertainties for offline rl through ensembles, and why their independence matters. NeurIPS, 2022.\n\n[3] Zhang X, et al. Corruption-robust offline reinforcement learning. AISTATS, 2022.\n\n[4] Wu F, et al. COPA: Certifying Robust Policies for Offline Reinforcement Learning against Poisoning Attacks. ICLR 2021.\n\n[5] Li A, et al. Survival Instinct in Offline Reinforcement Learning. NeurIPS 2023.\n\n[6] Ye C, et al. Corruption-Robust Offline Reinforcement Learning with General Function Approximation. NeurIPS 2023."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5914/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700229364907,
                "cdate": 1700229364907,
                "tmdate": 1700229364907,
                "mdate": 1700229364907,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nszfKLy19m",
                "forum": "5hAMmCU0bK",
                "replyto": "6SeHt7xMJU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5914/Reviewer_41NY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5914/Reviewer_41NY"
                ],
                "content": {
                    "title": {
                        "value": "Response to the author"
                    },
                    "comment": {
                        "value": "Thank you for the author's rebuttal. The answers address my concerns and I would love to keep my positive support for this work."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5914/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631059152,
                "cdate": 1700631059152,
                "tmdate": 1700631059152,
                "mdate": 1700631059152,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LKQgnkSU1C",
            "forum": "5hAMmCU0bK",
            "replyto": "5hAMmCU0bK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5914/Reviewer_t4o4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5914/Reviewer_t4o4"
            ],
            "content": {
                "summary": {
                    "value": "This paper concerns the robustness of different offline reinforcement learning methods to comprehensive data corruption, including states, actions, rewards, and dynamics. Their empirical results show that IQL method has better robustness to all types of data corruptions except noisy dynamic. The authors first give a theoretical explanation about IQL's robust performance, and according to empirical evidence, attribute this exceptional phenomenon to the heavy-tailed Q-targets. To verify this assumption and address this issue, this paper adopts observation normalization and Huber loss function for robust value function learning. Besides, this paper finds the potential negative value exploding issue of the clipped double Q-learning technique used in the original IQL and adopts quantile Q estimators instead of the LCB estimation. All the above modifications are verified to improve the performence of IQL under the data corruption of enviorment dynamic."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper provides comprehensive analysis and interesting findings on different types of data corruption in the offline RL;\n2. Sufficient empirical evidence on the possible reason of the failure setting, and efficient solutions to address these issues."
                },
                "weaknesses": {
                    "value": "1. Lack theoretical evidence on heavy-tailed Q-target issue;\n2. Heavy work on parameter finetuning;"
                },
                "questions": {
                    "value": "In Fig.1 only two scales are illustrated, would you please make it more clear about the underlying mechanism that generates the corrupted data in each cases,  and how to ensure that the generated noisy data is realistic and representative in the sense that they do represent typical noisy data we encountered in practice? Intuitively, different level of noise would lead to different robustness outcomes of various algorithms - in this sense, how do you ensure that the empirical evidence is conclusive?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "n/a"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5914/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698801340305,
            "cdate": 1698801340305,
            "tmdate": 1699636628689,
            "mdate": 1699636628689,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "195z7Avp3L",
                "forum": "5hAMmCU0bK",
                "replyto": "LKQgnkSU1C",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5914/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5914/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer t4o4"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive comments, and we provide clarification to your concerns below. We would appreciate it if you have any further questions or comments.\n\n**Q1: Lack the theoretical evidence on heavy-tailed Q-target issue**\n\n**A1:** The identification of the heavy-tailed issue primarily stems from empirical observations, including visualization and the calculation of Kurtosis values. It is challenging to provide a rigorous mathematical explanation for the cause of the heavy-tailed issue. However, we offer some intuitive speculation for this issue \u2014 dynamics corruption can corrupt $s'$ to less frequently visited or even artificial states $\\hat s'$, whose values $V_{\\psi}(\\hat s')$ exhibit higher uncertainty, thereby increasing the variance of the Q-target during offline training. Furthermore, we want to underscore that many existing works [1][2] in this field also identify heavy-tailed problems empirically and adopt assumptions similar to Assumption 4 in our paper. This assumption is mild in that it only requires the existence of $(1+\\nu)$-th moment, and our theoretical analysis (Lemma 5, Section 5.2)  demonstrates why Huber regression can effectively address the heavy-tailed issue. \n\n**Q2: Heavy work on parameter finetuning**\n\n**A2:** Due to varying inherent properties of different attacks and environments, optimal hyperparameters differ. However, after conducting a parameter search, we find that in most settings, $N=5, \\alpha\\in\\{0.1, 0.25\\}, \\delta\\in\\{0.1,1.0\\}$ yields the best or comparable performance. This hyperparameter space is relatively small. Additionally, we provide a hyperparameter study in Appendix E.8 to guide hyperparameter tuning. For instance, it is recommended to use larger $\\delta$ (e.g., $1.0$) to address dynamics and reward corruption, as opposed to smaller values (e.g., $0.1$) for observation and action corruption.\n\n**Q3: making it more clear about the mechanism that generates the corrupted data in each case, and how to ensure that the generated noisy data represent typical noisy data in practice?**\n\n**A3:** In Appendix D.1, we present a comprehensive explanation of the generation of corrupted data, where we consider two major forms of corruption: random corruption and adversarial corruption. Random corruption is similar to random noise encountered in measurement due to the inherent variance of the device or external factors in the environment. To simulate this scenario, we introduce Gaussian noise for states, actions, and next-states. To investigate a more challenging setting, we introduce adversarial corruption, which modifies the values of state, action, reward, and next-state in an adversarial manner. This setting is particularly relevant in scenarios where malicious attackers exist. For example, some annotators might intentionally provide wrong responses for RLHF or assign higher rewards for harmful responses, potentially making the LLM harmful for human use when learning from the annotated data. The two types of corruption also align with prior related works in robust imitation learning [3] and robust RL [4].\n\n**Q4: different levels of noise would lead to different robustness outcomes - how do you ensure that the empirical evidence is conclusive?**\n\n**A4:** We agree that different levels of noise can yield different robustness outcomes. In our setting, the noise level is determined by two factors: the corruption scale (the magnitude of each added noise) and the corruption rate (the proportion of data that is corrupted). **In Section 6.2, we assess the performance of different algorithms across varying corruption rates (0$\\%$ $\\sim$ 50$\\%$)** and demonstrate that RIQL exhibits greater robustness compared to other algorithms at different corruption rates. Additionally, we present results for random and adversarial corruption at scale 1 in Section 6.1 and scale 2 in Appendix E.6. Our findings indicate that RIQL consistently outperforms other algorithms. Considering that many baselines already attain scores lower than 10 under corruption scale 2, which is a notably low score, and taking into account the fact that noise with too large scale can be more easily detected in real-world scenarios, we have not continued to increase the corruption scale. \n\n**References**\n\n[1] Garg S, et al. On proximal policy optimization\u2019s heavy-tailed gradients. ICML, 2021.\n\n[2] Zhuang V, Sui Y. No-regret reinforcement learning with heavy-tailed rewards. AISTATS, 2021.\n\n[3] Sasaki F, Yamashina R. Behavioral cloning from noisy demonstrations. ICLR, 2020.\n\n[4] Zhang H, et al. Robust deep reinforcement learning against adversarial perturbations on state observations. NeurIPS, 2020."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5914/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700228890748,
                "cdate": 1700228890748,
                "tmdate": 1700228890748,
                "mdate": 1700228890748,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jUQuk07lKI",
                "forum": "5hAMmCU0bK",
                "replyto": "195z7Avp3L",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5914/Reviewer_t4o4"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5914/Reviewer_t4o4"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response."
                    },
                    "comment": {
                        "value": "My concerns are well addressed and I will maintain my previous score."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5914/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700707957793,
                "cdate": 1700707957793,
                "tmdate": 1700707957793,
                "mdate": 1700707957793,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]