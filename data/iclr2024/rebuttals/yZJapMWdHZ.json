[
    {
        "title": "Shifting Attention to Relevance: Towards the Uncertainty Estimation of Large Language Models"
    },
    {
        "review": {
            "id": "PD3WPxSOi5",
            "forum": "yZJapMWdHZ",
            "replyto": "yZJapMWdHZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3019/Reviewer_VX9S"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3019/Reviewer_VX9S"
            ],
            "content": {
                "summary": {
                    "value": "This paper identifies the problem of equal treatment of tokens in LLM-generated text, despite variations in their relevance and representativeness due to linguistic redundancy. Existing methodologies overlook these generative inequalities, leading to biased uncertainty estimation where less semantically significant tokens receive excessive weighting. To rectify this, the proposed method, called Shifting Attention to more Relevant (SAR) components, suggests joint attention shifting at both the token and sentence levels for accurate uncertainty estimation. Extensive experiments are conducted on various LLMs, including instruction-tuned models and pretrained models, using different question-answering tasks across domains like reading comprehension, science, and medical Q&A. The experimental results, along with a demographic analysis, demonstrate SAR's superior performance in addressing the challenges of uncertainty estimation in LLMs. Overall, SAR offers a promising approach to tackle hallucinations and improve uncertainty quantification in the context of LLMs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tThe authors propose to renormalize the uncertainty scores calculated by tokens with different importance (relevance). The method is intuitive, simple yet pretty effective.\n\n2.\tThe authors first conduct experiments to verify the existence of generation inequalities, which is a relatively trivial phenomenon. However, the fact that uncertainty estimation is highly affected by the inequalities is also verified and may pose a good contribution to this area.\n\n3.\tThe experiments are comprehensive and able to prove the effectiveness of the proposed method."
                },
                "weaknesses": {
                    "value": "1.\tAlthough there is stable improvement over the baseline models, it seems marginal."
                },
                "questions": {
                    "value": "1. I have a minor question since I am not an expert in this area: why would the unimportant tokens pose a higher uncertainty? In Figure 1, it seems that the unimportant tokens have relatively less possibility to convey information since the model should have been pretty certain about these tokens in my understanding. However, as shown in Figure 3, this is actually the opposite since the token-level mean of UP is actually higher when the relevance is less. Could you please give an explanation for this?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3019/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3019/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3019/Reviewer_VX9S"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3019/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698790615668,
            "cdate": 1698790615668,
            "tmdate": 1699636246728,
            "mdate": 1699636246728,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "M1wEKxjFrI",
                "forum": "yZJapMWdHZ",
                "replyto": "PD3WPxSOi5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3019/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3019/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to Reviewer VX9S"
                    },
                    "comment": {
                        "value": "**Q1: Although there is stable improvement over the baseline models, it seems marginal.**\n\n\nThank you for acknowledging our consistent improvements! We would like to present statistics demonstrating that the advancements in SAR are indeed substantial. Our paper includes approximately 21 experiments: 8 on pre-trained Large Language Models (LLMs), 8 on instruction-tuned LLMs, and 5 on medical datasets. We report the enhancements achieved by SAR in comparison to state-of-the-art methods (such as PE, LN-PE, SE), including average improvements, average improvements per setting, and the number of experiments in which SAR outperforms others by at least 2%.\n\n| Method | Total Exp. Number | Avg. Improvement | # Exps with Improvement > 2% | Avg. Pre-trained | Avg. Instruction-tuned | Avg. Medical |\n| ---- | :----: | :----: | :----: | :----: | :----: | :----: | \n| SAR v.s. PE | 16 | 6.1% | 15/16 | 7.2% | 4.9% | - | \n| SAR v.s. LN-PE| 21 | 5.4% | 19/21 | 3.3% | 7.9% | 4.6% | \n| SAR v.s. SE | 21 | 3.7% | 14/21 | 1.5% | 7.1% | 2.0% |\n\n\n\nIt is shown that our SAR outperforms state-of-the-art experiments with >2% margins for most experiments. Therefore we would like to mention that the improvement is rather substantial and notable.\n\n**Q2: I have a minor question since I am not an expert in this area: why would the unimportant tokens pose a higher uncertainty? In Figure 1, it seems that the unimportant tokens have relatively less possibility to convey information since the model should have been pretty certain about these tokens in my understanding. However, as shown in Figure 3, this is actually the opposite since the token-level mean of UP is actually higher when the relevance is less. Could you please give an explanation for this?**\n\nR2: Thank you for your insightful comment! ***Why would the unimportant tokens pose a higher uncertainty*** is exactly the question that inspired our method. \n\nWe believe this is due to the **intrinsic properties of LLMs**, such as data imbalance in training corpora. For example, if the LLM frequently encounters the phrase \"according to\" during training, the likelihood of the word \"to\" following it becomes very high (meaning the uncertainty of \"to\" is very low) when the preceding token is \"according\". If the LLM has never encountered \"density of\" (as shown in Fig. 1 of our paper), the probability of the word \"of\" following \"density\" is low (with high uncertainty). Thus, the certainty of LLMs regarding these tokens should be determined by their training. In Figs. 2 and 3, we demonstrate that many irrelevant tokens are assigned lower probabilities by LLMs, which we believe impedes accurate uncertainty estimation.\n\nOur motivation is that important tokens should dominate uncertainty estimation, rather than less significant ones. To achieve this, we quantify the importance of tokens and refocus on these significant tokens during uncertainty quantification, leading to improved performance.\n\n\nWe hope these kindly address the reviewer's concerns. We would like to provide responses if the reviewer has further questions."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3019/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700359941422,
                "cdate": 1700359941422,
                "tmdate": 1700360014352,
                "mdate": 1700360014352,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oszvWA1ree",
                "forum": "yZJapMWdHZ",
                "replyto": "PD3WPxSOi5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3019/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3019/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Have we addressed your concern?"
                    },
                    "comment": {
                        "value": "Dear Reviewer VX9S,\n\nThank you again for your insightful comments. This is just a gentle reminder as the discussion period ends soon. We would like to know whether we have addressed your concern. Thank you!\n\nAuthors of paper 3019"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3019/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700615642276,
                "cdate": 1700615642276,
                "tmdate": 1700616604626,
                "mdate": 1700616604626,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qLozgIVFa0",
            "forum": "yZJapMWdHZ",
            "replyto": "yZJapMWdHZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3019/Reviewer_UFP5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3019/Reviewer_UFP5"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies uncertainty quantification in long-form generation in language models. While there are many methods for this now, one common method is the predictive entropy, or the log probability of the generated sentence given the prompt. However, predictive entropy assumes all tokens are equally valued when estimating the uncertainty, even though not all of them contribute to the semantics (which determines correctness). To combat this, the paper introduces SAR, which computes a weighted average of the log probability of each added token, weighted by how much each token contributes to the semantics (measured by how much the semantics change when you remove a token). The authors also experiment with a similar procedure for sentences, where the sentence-level uncertainty is measured in terms of similarity to other generated sentences. The authors find that both the token and sentence uncertainties tend to outperform existing baselines, measured by AUROC."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The paper presents intuitive, effective methods for uncertainty estimation; in particular, their method effectively captures the difference between what\u2019s generated (tokens) and what we want to measure uncertainty over (semantics)\n* The paper is well-written and easy to follow throughout\n* The empirical analysis is good; in particular, their method outperforms baselines across multiple datasets and six different models."
                },
                "weaknesses": {
                    "value": "* The sentSar and tokenSar methods are conceptually quite different, and don\u2019t really make sense to package together (especially since while they are compared, sentSar requires more computation than tokenSar).\n* The sentSar method is more computationally expensive than baselines.\n* The tables in the paper are very difficult to parse; the font size is small, and presentation could be improved."
                },
                "questions": {
                    "value": "* Is removal the right way to test for change in semantics (rather than just considering likely replacements)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3019/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3019/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3019/Reviewer_UFP5"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3019/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698811476054,
            "cdate": 1698811476054,
            "tmdate": 1699636246619,
            "mdate": 1699636246619,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "d1JpkeWf4t",
                "forum": "yZJapMWdHZ",
                "replyto": "qLozgIVFa0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3019/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3019/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to Reviewer UFP5 (1/2)"
                    },
                    "comment": {
                        "value": "**Q1: The sentSar and tokenSar methods are conceptually quite different, and don\u2019t really make sense to package together (especially since while they are compared, sentSar requires more computation than tokenSar).**\n\nThank you for your insightful comment. We agree that sentSAR and tokenSAR differ conceptually, as they focus on distinct aspects of generation. However, as outlined in Sec. 5.2, they are complementary and can be integrated to address uncertainty quantification effectively at both the token and sentence levels. Consequently, we consider sentSAR and tokenSAR as two separate contributions of our paper. These distinctions are emphasized in our new revision (Sec. 4.3).\n\n**Q2: The sentSar method is more computationally expensive than baselines.**\n\nThank you for your insightful comment. We agree that sentSAR is more computationally expensive than the baseline methods. However, it's important to highlight that sentSAR is more **generation-efficient**. It surpasses baseline methods under significantly smaller computational constraints. As demonstrated in Fig. 2, SAR, with just 2~3 generations, outperforms all other methods that use 10 generations.\n\nWe have quantified the time consumed for each step in the overall uncertainty quantification pipeline. This includes sequence generation, computing logits, semantic clustering for SE, and sentence similarity for sentSAR. We exclude the time taken for aggregating logits/scores as it is negligible (less than 0.001 second for all methods). The average time consumed per question, based on an evaluation of 1000 questions from the Vicuna-13b + SciQ dataset, is provided. These measurements were taken using an AMD EPYC 7302 16-Core CPU and a 1xA40 GPU server:\n\n| Method | Number of Generations | Generation | Logits Computing | Semantic Clustering | Sentence Similarity |  sum |\n| ---- | :----: | :----: | :----: | :----: |  :----: | :----: |\n| PE | 5|4.09s|\t1.19s|\t0s|\t0s|\t5.28s|\n| LN-PE | 5| 4.09s|\t1.19s|\t0s|\t0s|\t5.28s|\n| SE | 5|4.09s|\t1.19s|\t1.5s|\t0s|\t6.78s|\n| sentSAR | 5|4.09s|\t1.19s|\t0s|\t2.58s|\t7.86s|\n| sentSAR | 2 |\t1.64s|\t0.48s|\t0s|\t0.52s|\t**2.64s**|\n\nWe present two scenarios for our sentSAR: 1) It takes **7.86 seconds** when utilizing 5 generations; 2) The time reduces to **2.64 seconds** when using just **2 generations**, which is the minimum number of generations required for sentSAR.\n\nThen we compare the 2-generations sentSAR with 5-generations baseline methods:\n\n| Method | Number of Generations | Llama-2-13b-chat + SciQ/Trivia QA | Vicuna-13b + SciQ/Trivia QA| Vicuna-33b + SciQ/Trivia QA|WizardLM-13b + SciQ/Trivia QA|average|  \n| ---- | :----: | :----: | :----: | :----: | :----: |:----:|\n|PE|5|**0.718**/0.647|0.708/0.690|0.665/0.644|0.677/0.647|0.692/0.657|\n|LN-PE|5|0.688/0.615|0.668/0.624|0.668/0.639|0.638/0.615|0.666/0.623|\n|SE|5|0.692/0.622|0.675/0.630|0.674/0.651|0.649/0.634|0.673/0.634|\n|sentSAR|**2**|0.716/**0.689**|**0.718**/**0.709**|**0.700**/**0.674**|**0.697**/**0.701**|**0.708**/**0.685**|\n\nOur sentSAR still surpasses the baseline methods while consuming less than half the time, demonstrating its greater generation efficiency. We have included these results in the updated version of our paper (Appendix C.5)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3019/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700359542780,
                "cdate": 1700359542780,
                "tmdate": 1700360202441,
                "mdate": 1700360202441,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XmwkjUzeLY",
                "forum": "yZJapMWdHZ",
                "replyto": "qLozgIVFa0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3019/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3019/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to Reviewer UFP5 (2/2)"
                    },
                    "comment": {
                        "value": "**Q3: The tables in the paper are very difficult to parse; the font size is small, and presentation could be improved.**\n\nThank you for pointing this out! We have modified the formats and font sizes of tables in our new revision.\n\n**Q4: Is removal the right way to test for change in semantics (rather than just considering likely replacements)?**\n\nThank you for your insightful comment! We believe that removal is an appropriate approach for our method for the following reasons:\n\n1. Removal allows for the quantification of per-token relevance and is easy to implement.\n2. Removal strategies are widely employed in existing works[1-2].\n3. The grammar mistakes or fluency issues caused by removal are not significant for current sentence similarity measurements. For example, consider the following SciQ example, which demonstrates the changes in similarity after the removal of each token, as measured by the cross-encoder-roberta-large model:\n\n| | Mesophilic | has |important |uses| in |food| preparation| including| cheese| yogurt| beer| and |wine|\n| ---- | :----:| :----: |:----: |:----: |:----:| :----: |:----:| :----:| :----: |:----: |:----:| :----: | :----: |\n| $\\Delta$ | 0.25 | 0.01 | 0.02 | 0.05 | 0.02 | 0.10 | 0.04 | 0.01 | 0.16 | 0.19 | 0.21 | 0.01 | 0.19 |\n\nThe similarity significantly changes only when key tokens (such as Mesophilic, food, cheese, yogurt, beer, wine) are removed. This is conceptually aligned with human understanding and also meets our requirements effectively.\n\nUsing replacements could be less effective because if we substitute a token with synonyms, the semantic impact is minimal, making it challenging to quantify token relevance accurately. Additionally, finding suitable replacements for some words or tokens, like definite and indefinite articles, prepositions, etc., can be difficult. We are open to conducting evaluations if the reviewer can suggest specific strategies for replacements.\n\nReference:\n\n[1] Lee, Joosung. \"Stable style transformer: Delete and generate approach with encoder-decoder for text style transfer.\" arXiv preprint arXiv:2005.12086 (2020).\n\n[2] Liu, Junyi, et al. \"TCRA-LLM: Token Compression Retrieval Augmented Large Language Model for Inference Cost Reduction.\" arXiv preprint arXiv:2310.15556 (2023).\n\n\nWe hope these kindly address the reviewer's concerns. We would like to provide responses if the reviewer has further questions."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3019/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700359795073,
                "cdate": 1700359795073,
                "tmdate": 1700360122750,
                "mdate": 1700360122750,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "su1hDeZJZN",
                "forum": "yZJapMWdHZ",
                "replyto": "qLozgIVFa0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3019/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3019/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Have we addressed your concern?"
                    },
                    "comment": {
                        "value": "Dear Reviewer UFP5,\n\nThank you again for your insightful comments. This is just a gentle reminder as the discussion period ends soon. We would like to know whether we have addressed your concern. Thank you!\n\nAuthors of paper 3019"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3019/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700615626162,
                "cdate": 1700615626162,
                "tmdate": 1700616590319,
                "mdate": 1700616590319,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "v2I7pt0mCO",
            "forum": "yZJapMWdHZ",
            "replyto": "yZJapMWdHZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3019/Reviewer_i37c"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3019/Reviewer_i37c"
            ],
            "content": {
                "summary": {
                    "value": "The authors present a new method for quantifying uncertainty in LLM outputs that involves weighting token probabilities by an estimate of the token's relative importance (\"relevance\"). This is combined with a second sentence-level uncertainty measure which adjusts a sentence's importance measure according to how similar it is to other generated sentences. When combined these uncertainty measures outperform multiple baselines across various models, and datasets, and evaluation methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The presented uncertainty estimation is built up out of simple to understand pieces, and yield impressive empirical results compared to competing techniques."
                },
                "weaknesses": {
                    "value": "I believe there is a typo in Equation 9. According to the definition of R_S given in Equation 4, the sum in the second line should be over $g(s_j,s_k)p(s_k|x)$ not $g(s_j,s_k)p(s_j|x)$.\n\n\"The reason we normalize relevance score in Eq. (6) is two-fold: a) to make tokens comparable within a sentence\" - this doesn't make sense, the relative token relevances are the same before and after the normalization, they're all being scaled by the same factor\n\nTable 1 is very crowded (multiple models, multiple datasets, multiple Rouge-L cutoffs, multiple baselines, multiple SAR methods). Maybe stick with just 1 RL cutoff and if you want to include the other cutoff, put it in the appendix."
                },
                "questions": {
                    "value": "\u201cThe area under the receiver operator characteristic curve (AUROC) metric is equivalent to the probability that a randomly chosen correct answer has a higher uncertainty score than a randomly chosen incorrect answer.\u201d - Shouldn\u2019t this be the opposite? If the model\u2019s uncertainty is high, then it implies that the model is more likely to get the answer wrong, no? I realize that this was copied from Kuhn et al, but this still seems like an important point to understand."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3019/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698963867389,
            "cdate": 1698963867389,
            "tmdate": 1699636246539,
            "mdate": 1699636246539,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "W3keyRUbPk",
                "forum": "yZJapMWdHZ",
                "replyto": "v2I7pt0mCO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3019/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3019/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer i37c"
                    },
                    "comment": {
                        "value": "**Q1: I believe there is a typo in Equation 9. According to the definition of R_S given in Equation 4, the sum in the second line should be over $g(s_j, s_k)p(s_k|x)$ not $g(s_j, s_k)p(s_j|x)$.**\n\nR1: Thank you for pointing this out! It is a typo. We have corrected this in our new revision.\n\n**Q2: \"The reason we normalize relevance score in Eq. (6) is two-fold: a) to make tokens comparable within a sentence\" - this doesn't make sense, the relative token relevances are the same before and after the normalization, they're all being scaled by the same factor**\n\nR2: Thank you for pointing this out! You are right, this is a typo. We mean with normalization, token relevances are comparable **across** sentences. We have fixed this statement in our new revision.\n\n**Q3: Table 1 is very crowded (multiple models, multiple datasets, multiple Rouge-L cutoffs, multiple baselines, multiple SAR methods). Maybe stick with just 1 RL cutoff and if you want to include the other cutoff, put it in the appendix.**\n\nR3: Thank you for your suggestion! We have split Table 1 into two parts and only keep the results of RL-0.5 in the main body of our new revision. The results of RL-0.3 are placed in Appendix C.4.\n\n**Q4: \"The area under the receiver operator characteristic curve (AUROC) metric is equivalent to the probability that a randomly chosen correct answer has a higher uncertainty score than a randomly chosen incorrect answer.\" - Shouldn\u2019t this be the opposite? If the model\u2019s uncertainty is high, then it implies that the model is more likely to get the answer wrong, no?**\n\nR4: Thank you for pointing this out! We indeed follow the setting from the SE paper and we didn\u2019t realize this typo before. We have corrected this description as the following (also in our new revision):\n\nFollowing prior work (Kuhn et al.), we evaluate uncertainty estimation by predicting the correctness of the model's generations regarding a given question, i.e. to what extent the generated answers can be trusted. The area under the receiver operator characteristic curve (AUROC) indicates the probability that a random correct generation has a lower uncertainty than a random incorrect generation, predicted by uncertainty estimation methods. AUROC equals 0.5 means the assigned uncertainty is no better than random guessing, i.e., they can not differentiate between correct and incorrect generations. AUROC equals 1 means all the correct generations are assigned lower uncertainty than all incorrect generations.\n\nWe hope these kindly address the reviewer's concerns. We would like to provide responses if the reviewer has further questions."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3019/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700359435979,
                "cdate": 1700359435979,
                "tmdate": 1700359435979,
                "mdate": 1700359435979,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PCp50zDbIO",
                "forum": "yZJapMWdHZ",
                "replyto": "v2I7pt0mCO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3019/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3019/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Have we addressed your concern?"
                    },
                    "comment": {
                        "value": "Dear Reviewer i37c,\n\nThank you again for your insightful comments. This is just a gentle reminder as the discussion period ends soon. We would like to know whether we have addressed your concern. Thank you!\n\nAuthors of paper 3019"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3019/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700615606402,
                "cdate": 1700615606402,
                "tmdate": 1700616568953,
                "mdate": 1700616568953,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]