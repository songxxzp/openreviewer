[
    {
        "title": "Phrase Grounding-based Style Transfer for Single-Domain Generalized Object Detection"
    },
    {
        "review": {
            "id": "D8OYsSYson",
            "forum": "TkdMRKvZDJ",
            "replyto": "TkdMRKvZDJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4948/Reviewer_G99H"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4948/Reviewer_G99H"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a phrase grounding-based style transfer (PGST) approach for single-domain generalized object detection. The authors leverage the grounded language-image pre-training model (GLIP) to learn object-level, language-aware, and semantic-rich visual representations. They define textual prompts for each target domain and use them to train the PGST module, which performs style transfer from the source domain to the target domain. The authors evaluate their approach on five different weather driving benchmarks and achieve significant improvements over existing methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper addresses an important and challenging problem of single-domain generalized object detection.\n- The proposed PGST approach is novel and leverages the strengths of the GLIP model.\n- The evaluation results show significant improvements over existing methods on diverse weather driving benchmarks."
                },
                "weaknesses": {
                    "value": "- The experimental evaluation could benefit from more detailed analysis and discussion of the results.\n- The paper could provide more insights into the reasons behind the observed improvements"
                },
                "questions": {
                    "value": "1. Can the authors provide more insights into the limitations of the proposed approach and potential directions for future research?\n2. How sensitive is the performance of the proposed approach to the choice of textual prompts? Have the authors experimented with different prompt designs and evaluated their impact on the results?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4948/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698763275553,
            "cdate": 1698763275553,
            "tmdate": 1699636481378,
            "mdate": 1699636481378,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SRyHjwFEkm",
                "forum": "TkdMRKvZDJ",
                "replyto": "D8OYsSYson",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4948/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4948/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer G99H"
                    },
                    "comment": {
                        "value": "Thanks for your diligent review work and valuable comments on our paper. In response to your feedback, our detailed replies are as follows:\n\n**Weakness 1: More detailed experimental analysis and discussions.**\n\nThank you for the valuable suggestion you provided.\n\nIn responding to the **Reviewer JhCL**'s comments on **Weakness 2**, we conducted experimental analysis on the catastrophic forgetting issue in large-scale vision-language models and verified that the proposed PGST module can alleviate this problem to some extent.\n\nAddressing the **Reviewer JhCL**'s comments on **Weakness 3**, we experimentally analyzed the performance degradation when using a general prompt set. This is attributed to the difficulty for the model's style transfer module to adaptively filter out irrelevant prompt phrases based on the target domain.\n\nRegarding the **Reviewer JhCL**'s comments on **Weakness 4**, we experimentally analyzed how weather-unrelated prompts decrease the model's performance.\n\nIn response to the **Reviewer Ci7j**'s comments on **Weakness 1**, we experimentally analyzed the effectiveness of the designed module to verify that the significant improvement is not solely derived from GLIP.\n\nAddressing the **Reviewer Ci7j**'s comments on **Weakness 2**, we similarly conducted experimental analysis on the catastrophic forgetting issue in large-scale vision-language models.\n\nRegarding the **Reviewer Ci7j**'s comments on **Question 2**, we experimentally analyzed the impact of different fine-tuning strategies on the model.\n\n**Weakness 2: More insights behind the observed improvements.**\n\nThank you for the valuable suggestion you provided.\n\nAs per your suggestions, we will include more insights to the reasons behind the observed improvements in our revised submission.\n\nAs explained in the response to **Reviewer Ci7j**'s comments on **Question 2**, we hold the view that the effectiveness of large-scale model-based domain generalization approaches derives from exposure to relevant visual concepts. For example, a large-scale model might not have encountered images of driving at night, but it may have learned the essential visual concept of nighttime. Hence, prompts can be employed to evoke these visual concepts in large-scale models, effectively addressing the distribution gap between the source and target domains. Specifically, by leveraging prompts to induce implicit visual concepts in large models, the proposed style transfer module PGST combines these visual concepts to generate the stylistic characteristics of the unseen target domain. Subsequently, it applies this stylistic transformation to the visual features of the source domain, ultimately enhancing the generalization performance of the model trained on the source domain to the unseen target domain.\n\n**Question 1: More insights into limitations and potential future directions.**\n\nThanks for this kind advice. We will include more discussions of limitations and future directions in our revised submission.\n\nFirstly, our model heavily relies on the availability of well-defined and representative textual prompts for each target domain. If these prompts are not adequately defined or do not accurately represent the characteristics of the target domain, the performance of our model may be impacted. Future research could explore methods for automatically generating or adapting textual prompts to improve the model' s adaptability to unseen target domains.\n\nWhile the adopted AdAIN has achieved state-of-the-art performance, there is still room for improvement in terms of the style transfer strategy. For example, incorporating a local feature loss [1] could enhance the quality of local visual transfer, thereby better serving our object-level transfer tasks.\n\n[1] AdaAttN: Revisit Attention Mechanism in Arbitrary Neural Style Transfer, Liu et al. ICCV'2021.\n\n**Question 2: Choice of textual prompts.**\n\nThanks for this kind comment. In our response to the **Reviewer JhCL**, we have conducted experimental analyses on the choice of textual prompts. Firstly, we investigated the effect of a general set of prompts containing various weather conditions on model performance (**Reviewer JhCL Weakness 4**). Subsequently, we experimentally analyzed the impact of weather-unrelated prompts on model performance (**Reviewer JhCL Weakness 3**). The experimental results indicate that choosing either of these prompts would decrease the model's performance. This suggests the importance of designing prompts that are as relevant as possible to each target domain.\n\nWe hope your concerns will be resolved and the rating of the paper can be increased accordingly. Thank you!"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4948/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700326705487,
                "cdate": 1700326705487,
                "tmdate": 1700326705487,
                "mdate": 1700326705487,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WUeN3ajweN",
                "forum": "TkdMRKvZDJ",
                "replyto": "SRyHjwFEkm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4948/Reviewer_G99H"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4948/Reviewer_G99H"
                ],
                "content": {
                    "comment": {
                        "value": "I am pleased to see that you have added experimental analysis on the catastrophic forgetting issue, and that you mentioned the limitation of your model requiring high quality for representative textual prompts. This has explained my doubts, and I will maintain my rating."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4948/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700656782935,
                "cdate": 1700656782935,
                "tmdate": 1700656782935,
                "mdate": 1700656782935,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AIiC5GNpRW",
                "forum": "TkdMRKvZDJ",
                "replyto": "D8OYsSYson",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4948/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4948/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Further response to Reviewer G99H"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful review and positive feedback.\n\nWe would like to bring your attention to our last response to **Reviewer JhCL**, where we specifically addressed the limitation of our model in relation to the need for high-quality and representative textual prompts.\n\nIn the latest submission, we have also developed a set of templates for general prompts that are independent of domain-specific textual prompts. As show in following table, the results of the two prompt designs are nearly identical. **This indicates the robustness of our proposed approach to prompt design, which does not necessitate high-quality and representative textual prompts.**\n\n| Method                           | Night Sunny | Dusk Rainy    | Night Rainy   | Daytime Foggy |\n|----------------------------------|:-------------:|:---------------:|:---------------:|:---------------:|\n| PGST  (w/ domain-specific prompt)                             | 46.6 | **45.1**| 27.2| **42.7**|\n| PGST (w/ general prompt) | **47.9**        | 44.5          | **28.4**          | 42.5          |\n\nWe have updated the experimental results with the general prompt as the **default setting**, while presenting the results using the previous domain-specific prompt as a reference or comparison. Furthermore, all these modifications have been highlighted in **blue** in the revised submission.\n\nOnce again, we sincerely appreciate your feedback and the opportunity to address your concerns. We recognize the significance of prompt design in our work and have taken your concerns seriously. We have made substantial efforts to tackle this limitation and have provided additional insights in our responses. We hope that our explanations and clarifications have effectively addressed your concerns regarding prompt design."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4948/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700661827768,
                "cdate": 1700661827768,
                "tmdate": 1700661855817,
                "mdate": 1700661855817,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CcxK931ecN",
            "forum": "TkdMRKvZDJ",
            "replyto": "TkdMRKvZDJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4948/Reviewer_Ci7j"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4948/Reviewer_Ci7j"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents Phrase Grounding-based Style Transfer (PGST) for single-domain generalized object detection. PGST aligns image regions with textual prompts, enabling the model to perform well in multiple unseen domains. It outperforms existing methods and achieves large improvement across various benchmarks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper is the first work to apply GLIP model to single-domain generalized object detection. In terms of novelty and performance improvement, it is a success. However, I'm a little concerned about the fair comparison, please see Weaknesses."
                },
                "weaknesses": {
                    "value": "- When comparing with other SOTA methods, the comparison is not fair. The proposed method is based on GLIP, while previous methods are based on Faster R-CNN. Apparently, GLIP has much stronger capacity than Faster R-CNN. It's hard to say how much improvement comes from the proposed design, instead of GLIP network architecture or pre-trained data.\n- Will fine-tuning GLIP with PGST degenerate the GLIP's original performance, like its performance on COCO, Flicker30k entities?"
                },
                "questions": {
                    "value": "I assume the following questions are open problems and not considered as the weaknesses of this paper:\n- Since GLIP has been pre-trained on so many data, there might be data leakage of target domain data. If so, does this really follow the problem setting of \"single-domain generalization\" ? \n- For the source domain augmentation with prompt, this paper uses a full-model tuning strategy. Have the authors tried prompt/linear probing and what the performance is ?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4948/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4948/Reviewer_Ci7j",
                        "ICLR.cc/2024/Conference/Submission4948/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4948/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698816477509,
            "cdate": 1698816477509,
            "tmdate": 1700708086465,
            "mdate": 1700708086465,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NlB0Xw4Ii7",
                "forum": "TkdMRKvZDJ",
                "replyto": "CcxK931ecN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4948/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4948/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Ci7j (I)"
                    },
                    "comment": {
                        "value": "Thanks for your diligent review work and valuable comments on our paper. In response to your feedback, our detailed replies are as follows:\n\n**Weakness 1: Comparison fairness.**\n\nThanks for this valuable comment.\n\nGLIP has its own object detector and we are the first to apply GLIP to single domain generalized object detection problem. Moreover, we adopt the region-phrase alignment loss in the object detector to optimize the style transfer module. Therefore, it is non-trivial to achieve a fair comparison you mentioned. To emphasize the crucial role of the proposed design in performance improvements, we conducted experiments and recorded the results in the following table, including Faster-RCNN, C-Gap (based on the large-scale pre-trained model CLIP and employing Faster-RCNN), GLIP, and our proposed design. It can be observed that, on one hand, our proposed method exhibits a significant improvement over GLIP. On the other hand, the relative improvement of our proposed design over GLIP is more substantial compared to the relative improvement of C-Gap over Faster-RCNN. These experimental results indicate the effectiveness of our proposed design. Additionally, in the response to **Weakness 2 of Reviewer JhCL**, the proposed design could effectively mitigate catastrophic forgetting problem in GLIP.\n\n| Method      | Night Sunny         | Dusk Rainy            | Night Rainy           | Daytime Foggy         |\n|-----------|:-------------------:|:---------------------:|:---------------------:|:---------------------:|\n| Faster RCNN | 33.5                | 26.6                  | 14.5                  | 31.9                  |\n| C-Gap       | 36.9(10.1\\%)        | 32.3(21.4\\%)          | 18.7(29.0\\%)          | 38.5(20.7\\%)          |\n| GLIP        | 32.2                | 32.1                  | 16.1                  | 32.1                  |\n| Ours        | **40.0(24.2\\%)** | **39.4(22.7\\%)**| **22.0(36.6\\%)** | **39.8(24.0\\%)** |\n\n**Weakness 2: Fine-tuning GLIP with PGST degenerates GLIP's original performance.**\n\n\nThanks for this valuable comment.\n\nWhat you mentioned is the common challenge of catastrophic forgetting in large-scale models. When the model undergoes fine-tuning on an interested dataset, it may forget its capabilities on other datasets due to overfitting issues, especially when the fine-tuning dataset follows different distribution from the test dataset. As shown in the following table, the performance of the GLIP model fine-tuned on VOC decreased by 10.5\\% when evaluated on COCO.\n\nWhen we fine-tuned the model using the VOC dataset and the proposed style transfer module PGST for Clipart dataset, we observed an improvement from 36.1\\% to 36.6\\%. Therefore, our proposed PGST does not exacerbate the issue of catastrophic forgetting. The fundamental reason lies in the overfitting of the VOC dataset and the distribution difference between the VOC and COCO datasets.\n\nIt is worth noting that the reason why our GLIP (Fine-tuned on VOC+PGST) performs worse than GLIP on the COCO dataset is mainly because our style transfer module PGST is designed for the Clipart dataset. Since COCO is a comprehensive dataset with diverse domains, the generalization from VOC to COCO involves a compound domain generalized object detection task, posing significant challenges in designing prompts, which is beyond the scope of this submission. However, it is an intriguing research direction that we may explore in the future.\n\n| Model                           | mAP of COCO |\n|-------------------------------|:-----------:|\n| GLIP                            | 46.6        |\n| GLIP (Fine-tuned on VOC)        | 36.1        |\n| GLIP (Fine-tuned on VOC + PGST) | 36.6        |"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4948/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700326020768,
                "cdate": 1700326020768,
                "tmdate": 1700706589427,
                "mdate": 1700706589427,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "S2buKJZwpD",
                "forum": "TkdMRKvZDJ",
                "replyto": "CcxK931ecN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4948/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4948/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Ci7j (II)"
                    },
                    "comment": {
                        "value": "**Question 1: Data leakage problem of unseen target domain.**\n\nThanks for this valuable comment.\n\nAs you pointed out, when using GLIP or other large-scale vision-language models, the potential issue of data leakage may exist. However, both previous research [1] and the findings in this paper indicate that fine-tuning a large-scale model on a specific domain can result in a decrease in its generalization performance on another different domain. The main reasons leading to the poor generalization issue are both catastrophic forgetting and distribution shift issues, as discussed in **Weakness 2**.\n\nMoreover, we believe that the success of the large-scale model based domain generalization approaches does not solely stem from encountering specific data from the target domain but rather from exposure to relevant visual concepts. For instance, a large-scale model may not have seen driving images at night, but it may have encountered the crucial visual concept of nighttime. Therefore, we can utilize prompts to evoke these visual concepts in large-scale models, thereby bridging the distribution gap between the source and target domains.\n\n[1] Investigating the Catastrophic Forgetting in Multimodal Large Language Models, Zhai et al., arxiv 2023.\n\n**Question 2: Prompt/linear probing.**\n\nThanks for this kind advice. Firstly, we apologize for any unclear expression in the submission. In fact, we analyzed the impact of fine-tuning strategies in *Table 11* of the revised submission. Additionally, as shown in the following table, we tested the performance of two fine-tuning strategies: full tuning and prompt tuning, and drew the following conclusions: 1) The full tuning strategy outperforms the prompt tuning strategy; 2) The introduction of the proposed style transfer module PGST under the prompt tuning strategy results in larger improvements compared to the one under the full tuning strategy.\n\n| Method           | Mode          | Night Sunny | Dusk Rainy | Night Rainy | Day Foggy |\n|----------------|-------------|:-----------:|:----------:|:-----------:|:---------:|\n| Src. Aug.        | Full tuning   | 43.8        | 40.1       | 24.0        | 37.4      |\n| Src. Aug. + PGST | Full tuning   | 47.9        | 44.5       | 28.4        | 42.5      |\n| Src. Aug.        | Prompt tuning | 32.2        | 32.1       | 16.1        | 32.1      |\n| Src. Aug. + PGST | Prompt tuning | 40.0        | 39.4       | 22.0        | 39.8      |\n\nWe hope your concerns will be resolved and the rating of the paper can be increased accordingly. Thank you!"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4948/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700326123154,
                "cdate": 1700326123154,
                "tmdate": 1700707766209,
                "mdate": 1700707766209,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "clmCryeSYP",
                "forum": "TkdMRKvZDJ",
                "replyto": "S2buKJZwpD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4948/Reviewer_Ci7j"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4948/Reviewer_Ci7j"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for authors' detailed response and added experimental results. I will increase my rating to 6. Good luck."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4948/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700708068013,
                "cdate": 1700708068013,
                "tmdate": 1700708068013,
                "mdate": 1700708068013,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XFkGYr1ag5",
                "forum": "TkdMRKvZDJ",
                "replyto": "CcxK931ecN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4948/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4948/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author-reviewer discussion"
                    },
                    "comment": {
                        "value": "Dear reviewers,\n\nWe genuinely value your important feedback. As the deadline for the author-reviewer discussion phase is approaching, we would like to ascertain if you have any other lingering concerns regarding our revised submission.\n\nWe sincerely appreciate your commitment and dedication in evaluating our submission. Please feel free to inform us if you require any further clarification or have additional suggestions.\n\nBest Regards,\n\nAuthors."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4948/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700708098663,
                "cdate": 1700708098663,
                "tmdate": 1700708098663,
                "mdate": 1700708098663,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5I2lgb3OX1",
            "forum": "TkdMRKvZDJ",
            "replyto": "TkdMRKvZDJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4948/Reviewer_JhCL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4948/Reviewer_JhCL"
            ],
            "content": {
                "summary": {
                    "value": "This paper tackles single-domain generalization tasks for object detection. In this work, authors leverage the GLIP model to estimate different unseen target domains via their style transfer module, PGST, and text prompts which describe the object categories in the new domains. Once the different styles are learned both image and text encoders are finetuned to achieve the best performance. The state-of-the-art results are shown for the standard benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The manuscript is well-written and easy to follow\n2. Experiments are shown on standard domain generalization and adaptation benchmarks.\n3. Though the method takes inspiration from C-Gap(2023) in terms of using text prompts for domain generalization, using GLIP instead of CLIP seems to be a more reasonable direction for object detection tasks. The strong improvement over C-Gap and other baselines goes to show that."
                },
                "weaknesses": {
                    "value": "1. The novelty of this work is using their PGST and GLIP for domain generalization tasks. However, a previous work PODA[1,2], which is not cited in this paper, implements a module similar to PGST using text prompts. This reduces the novelty of the current work. The authors should discuss this in the paper and propose what makes their PGST different from PODA. \n\n2. This work's performance is still much better than in PODA, so there is some merit. But if we remove PGST from the contribution (because of similarity w.r.t PODA ), is the contribution just integrating GLIP for domain generalization?\n\n3. All prompts used in this work directly correspond to the test domains. Why not have a general set of prompts showing all possible weather descriptions? How does that affect the performance? For example: a quick ChatGPT prompts for different weather scenarios and time of the day.\n```\n1. \"an image taken on a rainy day during the morning.\"\n2. \"an image taken on a cloudy day during the evening.\"\n3. \"an image taken on a snowy day during the night.\"\n4. \"an image taken on a sunny day during the early morning.\"\n5. \"an image taken on a foggy day during the late afternoon.\"\n6. \"an image taken on a stormy day during the twilight.\"\n7. \"an image taken on a clear day during the midnight.\"\n8. \"an image taken on a windy day during the golden hour.\"\n9. \"an image taken on a partly cloudy day during the dusk.\"\n10. \"an image taken on a misty day during the early evening.\"\n```\n\n4. Also, what if the prompts are unrelated to the weather , does it degrade the performance? These studies will be useful in judging sensitivity to the prompt's choice and design.\n\n5. It is not clear how the best model is chosen. Please refer to Gulrajani et Lopez-Paz , In search of lost domain generalization , ICLR'21 to indicate what strategy was used. This is crucial for the reproducibility of the method.\n\n[1] PODA: Prompt-driven Zero-shot Domain Adaptation, Fahes et. al. ICCV'23\n\n[2] P\u00d8DA: Prompt-driven Zero-shot Domain Adaptation, Fahes et. al. arxiv, 2022"
                },
                "questions": {
                    "value": "Please have a look at the weakness for my major concerns. Based on the answers, I am willing to change my rating."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4948/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4948/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4948/Reviewer_JhCL"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4948/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698835320413,
            "cdate": 1698835320413,
            "tmdate": 1700749917334,
            "mdate": 1700749917334,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aIQogrqyBL",
                "forum": "TkdMRKvZDJ",
                "replyto": "5I2lgb3OX1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4948/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4948/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer JhCL (I)"
                    },
                    "comment": {
                        "value": "Thanks for your diligent review work and valuable comments on our paper. In response to your feedback, our detailed replies are as follows:\n\n**Weaknesses 1: Differences between PGST and PODA.**\n\nThanks for this valuable comment.\n\nFahes et al. propose PODA to address the domain generalized semantic segmentation task. Specifically, PODA leverages a pre-trained contrastive vision-language model (CLIP) to optimize affine transformations of source features, steering them towards the target text embedding while preserving their content and semantics. To achieve this, PODA utilizes prompt-driven feature augmentation inspired from adaptive instance normalization (AdaIN). Then, they show that these prompt-driven augmentations can be used to deal with the domain generalized semantic segmentation task. The main differences between PODA and the PGST proposed in this submission can be summarized in the following four aspects:\n\n[1] PODA: Prompt-driven Zero-shot Domain Adaptation, Fahes et al. ICCV'23.\n\n1. **Task**. PODA aims to deal with the semantic segmentation task, while the proposed PGST is specifically designed for the object detection task. Object detection task is more complex and challenging compared to semantic segmentation task, as it involves both classification and regression simultaneously.\n2. **Model**. PGST adopts GLIP instead of CLIP in PODA. As you pointed out, using GLIP instead of CLIP seems to be a more reasonable direction for the object detection task.\n3. **Optimization Strategy for Style Transfer**. PODA aligns the visual feature of a whole source image with the target text embedding. To achieve this, PODA minimizes the cosine distance between the source visual features and target text features in CLIP latent space, to optimize the style transfer module. In contrast, the proposed PGST aligns the object-level visual feature of a source image with the target phrase embedding. To optimize the style transfer module, we employ the region-phrase alignment loss in GLIP which could enhance object-level style transfer while preserving the content and semantics.\n4. **Prompt Design**. As PODA mainly focuses on image-level style transfer, they use prompt like \"an image taken on a rainy day during the morning\" to specify the \"rainy-morning\" domain. In contrast, as the proposed PGST aims to realize object-level style transfer, we adopt prompt like \"a photo of a person taken on a rainy day during the morning, a photo of a car taken on a rainy day during the morning...\" to specify objects within the \"rainy-morning\" domain.\n\nNotably, both the PGST and PODA are inspired from the classic style transfer algorithm AdAIN as it can effectively manipulate the style information with a small set of parameters. In future work, we will explore more mature style transfer algorithms to enhance the performance of our proposed model, such as [2] and [3].\n\n[2] Learning Dynamic Style Kernels for Artistic Style Transfer, Xu et al. CVPR'23.\n\n[3] U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation, Kim et al. ICLR'20.\n\nAs per your suggestion, we will include these discussions on the differences between PODA and PGST in the revised submission.\n\n**Weaknesses 2: Contributions of our study.**\n\nThanks for this valuable comment.\n\nWe have discussed the four distinctions between our proposed model and PODA in **Weakness 1**. We hope you will reconsider our contributions. Additionally, we would like to draw your attention to the following contribution.\n\nThe proposed PGST module can alleviate the catastrophic forgetting issue in the GLIP model. This issue arises due to overfitting during the fine-tuning process of large-scale models. The following table shows the forgetting issue of GLIP on VOC and Clipart datasets. It could be observed that GLIP model exhibits test performance of 0.64 and 0.33 on VOC and Clipart datasets (zero-shot), respectively. However, when separately fine-tuned on VOC and Clipart, although GLIP can achieve good results on the corresponding fine-tuning datasets, their performance on the other dataset tends to degrade compared to zero-shot setting. In contrast, with the proposed PGST module, the forgetting issue on VOC and Clipart is significantly alleviated, improving from 0.54 to 0.66 and 0.23 to 0.36.\n\n| Model | mAP of VOC | mAP of Clipart |\n| --- |:----------:|:--------------:|\n| GLIP (Zero-shot)                           | 0.64       | 0.33           |\n| GLIP (Fine-tuned on VOC)                   | 0.71       | 0.23           |\n| GLIP (Fine-tuned on Clipart)               | 0.54       | 0.50           |\n| **GLIP (Fine-tuned on Clipart + PGST)** | **0.66**       | 0.50           |\n| **GLIP (Fine-tuned on VOC + PGST)**   | 0.71       | **0.36**           |"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4948/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700324137075,
                "cdate": 1700324137075,
                "tmdate": 1700324414226,
                "mdate": 1700324414226,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FUEi6tHaC4",
                "forum": "TkdMRKvZDJ",
                "replyto": "5I2lgb3OX1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4948/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4948/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer JhCL (II)"
                    },
                    "comment": {
                        "value": "**Weakness 3: A general set of prompts.**\n\nThanks for this valuable comment.\n\nAs you highlighted, it is crucial to evaluate the model's performance with a general set of prompts encompassing all possible weather descriptions. In our study, we employ specific-domain prompt tailored to the test domains, aiming to enhance the model's representations for each domain and improve its performance in specific scenarios. However, if we adopt a general set of prompts, the challenge arises in adaptively filtering out irrelevant weather descriptions to achieve precise style transfer in unseen target domains. This direction adds an intriguing dimension to our research. To address your concern, we create a general set of prompts for each category by incorporating various weather and time-related words to assess their impact on generalization performance. For instance, in the \"car\" category, we include prefix phrases such as \"daytime, dusk, and night,\" as well as suffix phrases like \"in sunny, rainy, and foggy scenes.\" The following table showcases the model's performance with the general set of prompts, revealing a potential decline in performance. We will include these analysis experiments in our revised submission.\n\n| Method                           | Night Sunny | Dusk Rainy    | Night Rainy   | Daytime Foggy |\n|----------------------------------|:-------------:|:---------------:|:---------------:|:---------------:|\n| PGST                             | **46.6** | **45.1**| **27.2**| **42.7**|\n| PGST (w/ general set of prompts) | 44.9        | 43.5          | 26.1          | 41.8          |\n\n**Weakness 4: Unrelated prompts**.\n\nThanks for this valuable comment.\n\nAs per your suggestion, we have conducted analysis experiments with prompts that are unrelated to weather shown in the following table. As you pointed out, the performance will be degraded as these prompts make it difficult to learn the essential differences between domains and the effectiveness of style transfer is consequently diminished. To produce prompts that are unrelated to weather, we utilize ChatGPT to generate prefixes and suffixes for each category of the target domain. In the table, **A** represents adding the prefix \"dreamlike setting\" and the suffix \"in the scene exudes a boring atmosphere\"; **B** represents adding the prefix \"ineffective\" and the suffix \"crawl slowly through the desert\". We will include these analysis experiments in our revised submission.\n\n| Method                                | Night Sunny | Dusk Rainy    | Night Rainy   | Daytime Foggy |\n|-------------------------------------|:-----------:|:-------------:|:-------------:|:-------------:|\n| PGST                                  | **46.6** | **45.1** | **27.2** | **42.7** |\n| PGST (w/ unrelated prompt **A**) | 44.0        | 40.5          | 24.2          | 37.8          |\n| PGST (w/ unrelated prompt **B**) | 43.2        | 40.7          | 24.6          | 37.2          |\n\n**Weakness 5: Model selection strategy.**\n\nThanks for this kind advice.\n\nGulrajani and Lopez-Paz [4] argue that domain generalization algorithms without a model selection strategy should be regarded as incomplete, where the model selection includes choosing hyper-parameters, training checkpoints, architecture variants). Moreover, they introduce three model selection strategy, i.e., training-domain validation set, Leave-one-domain-out cross-validation, test-domain validation set (oracle). Our work mainly focuses on single domain generalized object detection task, which is a more challenging task. Therefore, we adopt the third model selection strategy, i.e., test-domain validation set (oracle). In order to make our experimental results more convincing, we will submit the code for reproducibility. We will cite [4] and clarify the model selection strategy adopted in our work, and include them in section of implementation details of our revised submission.\n\n[4] Gulrajani and Lopez-Paz, In Search of Lost Domain Generalization, ICLR'21.\n\nIf you think these responses address your concerns, please consider increasing your score. Thank you!"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4948/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700325400298,
                "cdate": 1700325400298,
                "tmdate": 1700412251530,
                "mdate": 1700412251530,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PSplZD2eRH",
                "forum": "TkdMRKvZDJ",
                "replyto": "5I2lgb3OX1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4948/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4948/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author-reviewer discussion"
                    },
                    "comment": {
                        "value": "Dear reviewers,\n\nWe sincerely appreciate your valuable feedback.\n\nAs the deadline for the author-reviewer discussion phase is approaching, we would like to check if you have any other remaining concerns about our paper. \n\nWe sincerely thank you for your dedication and effort in evaluating our submission. Please do not hesitate to let us know if you need any clarification or have additional suggestions.\n\nBest Regards,\n\nAuthors."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4948/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700563738277,
                "cdate": 1700563738277,
                "tmdate": 1700564142539,
                "mdate": 1700564142539,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Sv3VxRbaxs",
                "forum": "TkdMRKvZDJ",
                "replyto": "5I2lgb3OX1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4948/Reviewer_JhCL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4948/Reviewer_JhCL"
                ],
                "content": {
                    "title": {
                        "value": "The approach is more domain adaptation than generalization."
                    },
                    "comment": {
                        "value": "Thank you authors for your replies. \nMy major concerns now are based on Weaknesses 3 and 5. \n\n\nW3: The performance degrades when the set of prompts is general(even when they represent a shift due to weather change). This means one has to be aware of the exact target domains and this is against the domain generalization spirit that the target domains are unknown. The proposed methodology is closer to domain adaptation (as the target domain is known) than generalization.\n\nW5: The model selection strategy used in this work is test-domain validation setting, which is different from C-Gap, where they use training-domain validation. Hence, the metrics computed are not directly comparable with C-Gap. The test-domain validation is the weakest form compared to others as one has access to the test domain during training.  \n\nBased on the replies, it seems more like domain adaptation i.e. without access to target images but still being aware of the exact target domain and validation on the same."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4948/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700567437781,
                "cdate": 1700567437781,
                "tmdate": 1700567474742,
                "mdate": 1700567474742,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "X0ieyIJeFU",
                "forum": "TkdMRKvZDJ",
                "replyto": "5I2lgb3OX1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4948/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4948/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Further response to Reviewer JhCL"
                    },
                    "comment": {
                        "value": "Thank you to the reviewers for the positive feedback on our previous responses during the discussion and for raising additional concerns about some of our responses.\n\nFurther response for **W5**:\n\nSorry, I apologize sincerely. We made a mistake in describing the model selection strategy here. **We have confirmed that we indeed employ the same model selection strategy as C-Gap, namely, the training domain validation strategy.**\n\nOur experimental details are as follows: During the model training process, without accessing any data or images from the target domain, we divided a single source domain (Daytime-Sunny) into a training subset and a validation subset. Specifically, the training subset consists of 19,395 images, and the validation subset involves the remaining 8,313 images. **Then, we select the model that is trained on the training subset and could achieve the highest mAP on the validation subset before being tested on the unseen target domain.** In order to provide more convincing evidence, we have included our open-source code in the supplementary materials. Additionally, in the revised manuscript, we have corrected our description of the model selection which are highlighted in **blue** color (Section of Implementation Details).\n\nOnce again, we apologize for our mistake, and we hope that the above response addresses your concerns regarding our submission.\n\nFurher response for **W3**:\n\nThank you for the concerns raised by the reviewer regarding our submission. Indeed, C-Gap [5] is a signifcant work and we find many aspects of their experimental setup worth following, including the model selection strategy you mentioned and the design of the general prompt.\n\n[5] Vidit et al, CLIP\u00a0the\u00a0Gap: A Single Domain Generalization Approach for Object Detection, CVPR'23.\n\nThe extensive additional experiments that needed to be conducted in our previous response. In order to quickly address your comments, we used only a portion of the source domain data for data augmentation and ran a relatively small number of iterations. Unfortunately, this led to less satisfactory results in our previous response, and we apologize for this. Over the past few days, **we have gradually completed the full experimental process, strictly following the settings in the experimental details of our submission**.\n\nThe obtained results, as shown in the table below, indicate that after incorporating the general prompt you mentioned, our approach performs better on domains with significant differences (e.g., Night Sunny, Night Rainy) compared to the domain-specific prompt. However, on domains with smaller differences (e.g., Dust Rainy, Daytime Foggy), the performance is slightly worse with the general prompt. We speculate that domains with smaller differences are more susceptible to interference from the prompts for the domain with significant differences in the general prompt. **However, the results of the two are almost identical, indicating that our proposed approach is quite robust to prompt design, which is also the aspect you were most concerned about. Furthermore, even with the use of the general prompt, our method still exhibits superiority compared to C-Gap.**\n\n| Method                           | Night Sunny | Dusk Rainy    | Night Rainy   | Daytime Foggy |\n|----------------------------------|:-------------:|:---------------:|:---------------:|:---------------:|\n| C-Gap | 36.9       | 32.3         | 18.7        | 38.5         |\n| PGST  (w/ domain-specific prompt)                             | 46.6 | **45.1**| 27.2| **42.7**|\n| PGST (w/ general prompt) | **47.9**        | 44.5          | **28.4**          | 42.5          |\n\n**In summary, we will use the general prompt you mentioned to ensure that we are in the domain generalization setting and to maintain fairness in comparison with C-Gap.** We updated the experimental results with the general prompt as the **default setting** in our submission, and presented the results with the domain-specific prompt as a reference or comparison, demonstrating the robustness of our proposed approach to prompt design. Moreover, these modifications have all been highlighted in **blue** in the revised submission.\n\nTo further convince you of our results, we have submitted our code and documentation in the supplementary material, and update our results in the revised version of our submission. We apologize for the hastily obtained experimental results in our previous response, and hope that the updated results and analysis will address your concerns about our submission.\n\nOnce again, thank you for your suggestions on our prompt design. Your review has a positive impact on our work, allowing us to refine our proposed framework. **Such a general prompt design is meaningful in clearly defining that the submission addresses domain generalization problem rather than domain adaptation problem, and it contributes to improving the robustness of the model to the prompt design.**"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4948/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700650756500,
                "cdate": 1700650756500,
                "tmdate": 1700651232812,
                "mdate": 1700651232812,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DA63UtTi9i",
                "forum": "TkdMRKvZDJ",
                "replyto": "5I2lgb3OX1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4948/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4948/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author-reviewer discussion"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nAs the rebuttal period is coming to an end, and I have not received further feedback from you regarding our previous responses. Considering the limited time remaining, it is crucial for us to address any concerns you may have and strive to resolve them to the best of our abilities.\n\nWe highly value your feedback and suggestions, and we genuinely want to meet your expectations. If there is any additional information you need, further discussion you would like to have, or any other issues you would like us to address, please let us know. \n\nWe would appreciate the opportunity to have a final discussion with you to ensure that our responses are satisfactory to you. We sincerely appreciate your time and patience and look forward to receiving your feedback as soon as possible.\n\nIf there are any other requests or if you require further assistance, please feel free to let us know.\n\nBest regards,\n\nAuthors."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4948/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700726706402,
                "cdate": 1700726706402,
                "tmdate": 1700734201857,
                "mdate": 1700734201857,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]