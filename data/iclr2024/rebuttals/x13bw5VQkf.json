[
    {
        "title": "A Coefficient Makes SVRG Effective"
    },
    {
        "review": {
            "id": "AJihQ0DcTz",
            "forum": "x13bw5VQkf",
            "replyto": "x13bw5VQkf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission763/Reviewer_auQR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission763/Reviewer_auQR"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies SVRG for training neural networks. The main idea is introducing an additional coefficient vector $\\bf\\alpha$ to control the variance at each iteration, which leads to a new algorithm called $\\alpha$-SVRG. The experimental results show the proposed algorithm has lower gradient variance and training loss compared to baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The topic of this paper is nice. How to fill the gap between the theory of variance reduction and training neural networks has not been well-studied. This paper attempt to address this important problem.\n\nThis paper is well-organized and easy to follow. The motivation and design of $\\alpha$-SVRG is clear. The authors provide sufficient numerical experiments to support their ideas."
                },
                "weaknesses": {
                    "value": "I think the main weakness of this paper is its theoretical contribution is not strong.\n\n1. Section 3 introduces ``optimal coefficient\u2019\u2019 by minimizing the sum of variances for each component of $\\bf g^t$. However, this formulation does not consider the potential correlation between different components of $\\bf g^t$. In other words, it implicitly assumes the components of $\\bf g^t$ are uncorrelated, which looks too strong.\n\n2. The optimal coefficient only considers the current variable $\\theta^t$, but it is unclear how it affects the convergence rate of the algorithm in theoretical. The existing analysis only provide a greedy view, while I am more interested in the global theoretical guarantees of proposed algorithms.\n\n3. It is well-known that stochastic recursive gradient methods has the optimal stochastic first-order oracle (SFO) complexity for nonconvex optimization. For example, SVRG can find $\\epsilon$-stationary point within $n+n^{2/3}\\epsilon^{-2}$ SFO calls (Allen-Zhu & Hazan, 2016; Reddi et al., 2016), while SPIDER only requires $n+n^{1/2}\\epsilon^{-2}$ (Fang et al., 2018), where $n$ is the number of individual functions. Compared with SVRG, the study on SPIDER for training neural networks is more interesting."
                },
                "questions": {
                    "value": "1. Can you design some strategy to improve SVRG by considering the correlation  between the component of gradient estimator?\n2. Can you provide convergence analysis to show the advantage of proposed method?\n3. Is it possible to apply the idea of this paper to improve stochastic recursive gradient methods?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission763/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698515573129,
            "cdate": 1698515573129,
            "tmdate": 1699636003834,
            "mdate": 1699636003834,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Za0h9LgQwU",
                "forum": "x13bw5VQkf",
                "replyto": "AJihQ0DcTz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission763/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission763/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "We sincerely thank you for your constructive comments. We are encouraged that you find our topic is important and our paper is easy to follow and presents sufficient numerical experiments to support $\\alpha$-SVRG. We would like to address the comments and questions below.\n> Can you design some strategy to improve SVRG by considering the correlation between the components of the gradient estimator?\n\n1. **We have provided a detailed rigorous proof that considers the correlation between the components of the gradient estimator.** If we consider the correlation, we need to add a matrix $A^t$ instead of a coefficient to control the variance reduction term in SVRG. For notation, $A^t$ denotes the coefficient matrix; $(a^t_k)^T$ represents the row of the coefficient matrix; $a^t_{k, n}$ as the entry of the coefficient matrix at the $k$-th row and $n$-th column; and $K$ represents the covariance matrix. Formally, the generalization of SVRG using the coefficient matrix can be written as:\n\\begin{flalign}\n     g_i^t = \\nabla f_i(\\theta^t)-A^t(\\nabla f_i(\\widetilde{\\theta})-\\nabla f(\\widetilde{\\theta}))\\end{flalign}\nWe adopt the same gradient variance definition in Section3 and aim to determine the optimal coefficient matrix \\(A^{t*}\\) that minimizes it at each iteration. \n\\begin{flalign}\n     min_{A^t} \\sum_{k=1}^d Var(g^t_{\\cdot, k}) =\\sum_{k=1}^d min_{a^t_k} Var(g^t_{\\cdot, k})\\end{flalign}\nThe order of minimization and summation can be switched because the variance of the $k$-th component of the gradient only depends on the $k$-th row of the coefficient matrix.\nWe then expand the variance reduced gradient with the definition of the variance:\n\\begin{flalign}\n     &=\\sum_{k=1}^d min_{a^t_k} Var(\\nabla f_{i, k}(\\theta^t)-(a^t_k)^T\\nabla f_i(\\widetilde{\\theta})) \\\\\\\\\n     &= \\sum_{k=1}^d min_{a^t_k} Var(\\nabla f_{i, k}(\\theta^t)-\\sum_{n=1}^da^t_{k, n}\\nabla f_{i, n}(\\widetilde{\\theta})) \\\\\\\\\n     &= \\sum_{k=1}^d min_{a^t_k} \\bigg(Var(\\nabla f_{i, k}(\\theta^t)) + \\sum_{n=1}^d(a^t_{k, n})^2Var(\\nabla f_{i, n}(\\widetilde{\\theta})) - 2\\sum_{n=1}^da^t_{k, n}Cov(\\nabla f_{i, k}(\\theta^t), \\nabla f_{i, n}(\\widetilde{\\theta})) + 2\\sum_{n=1}^d\\sum_{m\\neq n}a^t_{k, n}a^t_{k, m}Cov(\\nabla f_{i, m}(\\widetilde{\\theta}), \\nabla f_{i, n}(\\widetilde{\\theta}))\\bigg)\n\\end{flalign}\nDifferentiating the objective with respect to $a^t_{k, n}$, we can determine the optimal matrix $A^{t*}$ satisfies:\n\\begin{flalign}\n    &\\forall k, n:  2a^t_{k, n}Var(\\nabla f_{i, n}(\\widetilde{\\theta})) - 2Cov(\\nabla f_{i, k}(\\theta^t), \\nabla f_{i, n}(\\widetilde{\\theta})) + 2\\sum_{m\\neq n}a^t_{k, m}Cov(\\nabla f_{i, m}(\\widetilde{\\theta}), \\nabla f_{i, n}(\\widetilde{\\theta})) = 0 \\\\\\\\      &\\implies A^{t*} = K_{\\nabla f_i(\\theta^t), \\nabla f_i(\\widetilde{\\theta})}K_{\\nabla f_i(\\widetilde{\\theta}), \\nabla f_i(\\widetilde{\\theta})}^{-1}\n\\end{flalign}\nThis full derivation is also included in Appendix G of the revision.\n\n2. **Using the optimal coefficient (the diagonal entries of the optimal matrix) is a more feasible choice for empirical analysis.** For instance, a logistic regression model taking a 32x32 RGB image and outputting 10 classes has an optimal matrix with $3082^2$ entries, which cannot be easily accommodated by any modern GPU. This has similar implications in second-order optimization. Computing the Hessian matrix in deep learning is often computationally intractable, but it is possible to estimate the diagonal entries of the Hessian matrix [1], making the number of elements only scale linearly with the number of parameters in the model."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission763/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700463126539,
                "cdate": 1700463126539,
                "tmdate": 1700629207091,
                "mdate": 1700629207091,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "30qUfazZHo",
                "forum": "x13bw5VQkf",
                "replyto": "AJihQ0DcTz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission763/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission763/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "> The optimal coefficient only considers the current variable $\\theta^t$, but it is unclear how it affects the convergence rate of the algorithm in theoretical. The existing analysis only provides a greedy view, while I am more interested in the global theoretical guarantees of proposed algorithms. Can you provide convergence analysis to show the advantage of proposed method?\n\nOur primary aim with the development of $\\alpha$-SVRG is to enhance SVRG for practical applications, particularly in training neural networks. **It's crucial to note that while theoretical analyses in SVRG offer valuable insights [2, 3], we should clarify there is a gap between theoretical analysis and empirical results in SVRG. Our work is to bridge this gap by providing a deeper understanding of why SVRG may not work in practice, and how we can adapt it to better suit real-world applications.**\n\n1. Theoretically, SVRG has shown faster convergence than SGD in non-convex settings [2, 3]. **However, empirical studies, including our own, reveal that SVRG does not effectively reduce gradient variance or achieve lower training losses compared to SGD.** This observation holds even for simple models like MLP-4 or LeNet in non-convex settings [4].\n\n2. Theoretically, SVRG has the same convergence rate even if the number of iterations between two consecutive snapshot scales with the number of data points [3]. **Yet, empirically, as the model moves away from the snapshot, the correlation between the snapshot gradients and the model gradients decreases, rendering the variance reduction mechanism in SVRG failed.**\n\n3. Theoretically, the variance of the gradient estimator in SVRG is upper bounded by the iterated distance between the snapshot model and the current model up to a constant factor [2]. **But, the work [4] shows the model iterate \"moves too quickly through the optimization landscape,\" undermining the utility of the variance upper bound.**"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission763/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700463169978,
                "cdate": 1700463169978,
                "tmdate": 1700629955077,
                "mdate": 1700629955077,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UCNL9xrg5n",
                "forum": "x13bw5VQkf",
                "replyto": "AJihQ0DcTz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission763/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission763/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "> It is well-known that stochastic recursive gradient methods have the optimal stochastic first-order oracle (SFO) complexity for nonconvex optimization. For example, SVRG can find $\\epsilon$-stationary point within $n+n^\\frac{2}{3}\\epsilon^{-2}$ SFO calls [2, 3], while SPIDER only requires $n+n^\\frac{1}{2}\\epsilon^{-2}$ [5], where $n$ is the number of individual functions. Compared with SVRG, the study on SPIDER for training neural networks is more interesting.\n\n1. You are absolutely right about the potential of stochastic recursive gradients in training neural networks, **but we found that our current framework based on control variate cannot be applied directly to the family of stochastic recursive gradients**. The main issue is that the variance reduction term is not zero-mean in expectation. We can analyze one of the stochastic recursive gradients methods SpiderBoost as an example. Formally, SpiderBoost defines its variance reduced gradient as:\n$$g^{t} = \\nabla f_i(\\theta^t) - (\\nabla f_i(\\theta^{t-1}) - g^{t-1}), g^{t_0} = \\nabla f(\\widetilde{\\theta}).$$\nIf conditioning the expectation on all the randomness from $t_0$ to $t-1$, the expectation of the first term in the variance reduction term becomes:\n$$\n    \\mathbb{E}[\\nabla f_i(\\theta^{t-1})] = \\nabla f(\\theta^{t-1}).$$\nHowever, the expectation of $g^{t-1}$ could only be $\\nabla f(\\theta^{t-1})$ if we condition the expectation on all the randomness from $t_0$ to $t-2$. \n\n2. **Despite the incompatibility of the control variates method on SpiderBoost, we still find intriguing empirical results when applying a linearly decreasing coefficient to SpiderBoost. We refer to this approach as $\\alpha$-SpiderBoost.** We employ $\\alpha$-SpiderBoost with three different initial coefficients $\\alpha_0$ from \\{0.5, 0.75, 1\\} to train ConvNeXt-Femto on a variety of small datasets. All the experiments use the same training recipe and the hyper-parameters as before (detailed in Appendix B). We present the result [here](https://drive.google.com/uc?id=1fcf9O_ZhZW25ETFEYHYT6WIIRkVukqKf) and compare it to the baseline, the standard SVRG, $\\alpha$-SVRG, and the standard SpiderBoost. \n3. **For small datasets (Pets, STL-10, and DTD), the standard SpiderBoost achieves a lower train loss than the baseline, but $\\alpha$-SpiderBoost could have a higher train loss.** Intuitively, for small datasets, the number of iterations within an epoch is small. As a result, the correlation between the model stochastic gradients and the snapshot stochastic gradients is still high enough for SpiderBoost to reduce gradient variance. This might also suggest SpiderBoost does optimize models better than SVRG on small datasets where SVRG consistently increases the train loss. \n4. **However, for relatively large datasets (CIFAR100, Food-101, and SVHN), $\\alpha$-SpiderBoost could achieve a substantially lower train loss than the standard SpiderBoost and even $\\alpha$-SVRG.** This is likely because as the number of iterations within an epoch increases, the strength of the variance reduction term should decrease and a linearly decreasing coefficient helps weaken the strength. \n\nThe above paradoxical results of $\\alpha$-SpiderBoost warrants future research to further understand how to make SpiderBoost more effective in training neural networks. **And our work on $\\alpha$-SVRG could be a good starting point for this.** All the above results and analysis have been included in Appendix F.\n\nWe thank you again for your valuable feedback and we hope our response can address your questions. If you have any further questions or concerns, we are very happy to answer.\n\nReferences:\n\n[1] Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training, Anonymous, 2023.\\\n[2] Variance reduction for faster non-convex optimization. Allen-Zhu & Hazan, 2016.\\\n[3] Stochastic Variance Reduction for Nonconvex Optimization. Reddi et al., 2016.\\\n[4] On the ineffectiveness of variance reduced optimization for deep learning. Defazio et al., 2019.\\\n[5] SPIDER: Near-Optimal Non-Convex Optimization via Stochastic Path Integrated Differential Estimator. Fang, 2018."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission763/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700463196697,
                "cdate": 1700463196697,
                "tmdate": 1700684602058,
                "mdate": 1700684602058,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0FEh46fxgv",
            "forum": "x13bw5VQkf",
            "replyto": "x13bw5VQkf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission763/Reviewer_LXQJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission763/Reviewer_LXQJ"
            ],
            "content": {
                "summary": {
                    "value": "This paper recalls the ineffectiveness of SVRG for deep neural networks and shows that the gradients variance even increase during late epochs when training deep networks with standard SVRG.\nIt then introduces a modified version of SVRG that involves an $\\apha$ coefficient in front of the variance reduction term.\nAuthors define and derive an optimal coefficient that minimizes the coordinate-wise variance of mini-batch stochastic gradients. \nThey show empirically that SVRG with optimal coefficient and its practical implementation $\\alpha$-SVRG (linear decaying coefficient) do not suffer from increased variance.\nFinally, the authors show the effectiveness of their methods compared to standard SVRG on a classification benchmark including multiple deep architectures, especially on Imagenet dataset."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper clearly show the failure of SVRG for deep networks, especially at later stages of the training (confirming findings of [Defazio & Bottou, 2019])\n- The motivation and derivation of section 3, the practical choice of the linear decay for alpha-SVRG emerges from clear experimental findings showing that the optimal $\\alpha$ decreases in deep models\n- the experiments of section 5 explore multiple scales (\"smaller\" and \"larger\" models) and families (CNN, ViT, Mixer) of deep models and show $\\apha$-SVRG always lowers the training loss and often improves the test accuracy"
                },
                "weaknesses": {
                    "value": "- Additional parametrization (initialization, linear decay) of the proposed $\\apha$-SVRG\n- Results of $\\apha$-SVRG often very close to the baseline: no clear improvement in most cases (except on Pets, STL-10 and DTD datasets)\n- No clear explanation on how $\\apha$-SVRG scales to larger datasets: no explanations about the feasibility of taking full batch gradient every epoch (cf mega-batch size of [Defazio & Bottou, 2019])\n- No discussion on the supposition than the noise of SGD might be an element for better generalization (cf [Jin et al, Towards Better Generalization: BP-SVRG in Training Deep Neural Networks, 2019]"
                },
                "questions": {
                    "value": "**Comments**\n1) $\\theta^{past}$ is not a standard notation, I would recommend $\\tilde{\\theta}$ instead\n2) An explanation of the three metrics in Table 1 is required. How are they different?\n3) $g_i,k$ in metric 2 of Table 1 is not defined\n4) The notations $\\textbf{g}_{i \\cdot}$ is unclear\n5) Precising the baseline in the legends (SGD or AdamW) would be better\n6) The \"snapshot interval\" is better known as \"inner loop size\"\n\n**Questions**\n1) Why are works related to alternative optimization cited page 2? No clear relevance.\n2) How large are the mini-batches for $\\alpha$-SVRG for the different experiments? \n3) Page 5, \"This is likely because the gradient direction varies more and becomes less certain as models become better trained\" -> are there other works confirming this statement?\n4) Page 5, is the standard deviation ratio of equation (6) constant across iterations? \n5) Table 3, validation accuracy of SVRG, datasets Pets, STL-10 : have you double checked this results ? The accuracy gap is very important. How can this be explained ?\n6) How does $\\alpha$-SVRG behaves on Imagenet (not Imagenet-1K) ? In such a setting it is impossible to perform full gradient computations and not every epoch. cf [Defazio & Bottou, 2019]\n\n\n**Suggestions**\n1) Should be cited: works on optimal implementation and parameters for SVRG [Sebbouh et al. \"Towards closing the gap between the theory and practice of SVRG.\", 2019], [Babanezhad Harikandeh, et al. \"Stopwasting my gradients: Practical svrg.\" 2015]\n2) To enrich the bibliography, SVRG has also been extended to policy learning ([Papini et al. \"Stochastic variance-reduced policy gradient.\" 2018] & [Du et al. \"Stochastic variance reduction methods for policy evaluation.\" 2017]) other examples are given in [Gower, et al. \"Variance-reduced methods for machine learning.\" 2020]"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission763/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission763/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission763/Reviewer_LXQJ"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission763/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698611466694,
            "cdate": 1698611466694,
            "tmdate": 1699636003760,
            "mdate": 1699636003760,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8o1nUjKRCS",
                "forum": "x13bw5VQkf",
                "replyto": "0FEh46fxgv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission763/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission763/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "We sincerely thank you for your constructive comments. We are encouraged that you find our motivation, derivations, and empirical analysis in Sections 2 and 3 are clear and the experiments are comprehensive. We would like to address the comments and questions below:\n\n> Additional parametrization (initialization, linear decay) of the proposed $\\alpha$-SVRG.\n\n**We emphasize that these additional parametrization are designed to be robust, meaning they do not require extensive tuning in practical settings.** In Table 6 in Appendix B, we assess $\\alpha$-SVRG across various datasets with different initial coefficients $\\alpha_0 \\in\\\\{0.5,0.75,1\\\\}$. The results are [here](https://drive.google.com/uc?id=1gZ46eLCA1_8CZZMMArL31lCWDF_ogfQv) and have been included in Appendix C. The consistent reduction in training loss across most datasets, irrespective of the initial coefficient choice, underscores $\\alpha$-SVRG's robustness and its effectiveness.\n\n>Results of $\\alpha$-SVRG often very close to the baseline: no clear improvement in most cases (except on Pets, STL-10 and DTD datasets).\n\n1. In Appendix E, we have utilized three different random seeds to run $\\alpha$-SVRG and compare it to the baseline. The results (below) demonstrate that $\\alpha$-SVRG consistently decreases training loss and increases validation accuracy compared to the baseline. Importantly, the mean difference in these metrics for $\\alpha$-SVRG is greater than its standard deviation in most cases. **This indicates that the improvement offered by our method is consistent.**\n\n**Train Loss**:\n|                    | CIFAR-100              | Pets                  | Flowers               | STL-10                | Food-101              | DTD                   | SVHN                  | EuroSAT               |\n|--------------------|------------------------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|\n| baseline           | 2.645 \u00b1 0.013          | 2.326 \u00b1 0.088         | 2.436 \u00b1 0.038         | 1.660 \u00b1 0.017         | 2.478 \u00b1 0.021         | 2.072 \u00b1 0.066         | 1.583 \u00b1 0.005         | 1.259 \u00b1 0.017         |\n| + $\\alpha$-SVRG $      | **2.606 \u00b1 0.017**      | **2.060 \u00b1 0.071**     | **2.221 \u00b1 0.042**     | **1.577 \u00b1 0.022**     | **2.426 \u00b1 0.007**     | **1.896 \u00b1 0.075**     | **1.572 \u00b1 0.011**     | **1.239 \u00b1 0.016**     |\n\n**Validation Accuracy:**\n|                    | CIFAR-100              | Pets                  | Flowers               | STL-10                | Food-101              | DTD                   | SVHN                  | EuroSAT               |\n|--------------------|------------------------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|\n| baseline           | 81.02 \u00b1 0.07           | 70.61 \u00b1 1.55          | 80.33 \u00b1 1.01          | 80.80 \u00b1 1.46          | 85.29 \u00b1 0.47          | 56.21 \u00b1 1.19          | 94.29 \u00b1 0.67          | 97.91 \u00b1 0.12          |\n| + $\\alpha$-SVRG $       | **81.07 \u00b1 0.22**       | **76.37 \u00b1 1.06**      | **84.15 \u00b1 1.15**      | **83.65 \u00b1 0.92**      | **85.45 \u00b1 0.43**      | **61.44 \u00b1 0.35**      | **94.94 \u00b1 0.60**      | **98.13 \u00b1 0.07**      |\n\n2. When comparing $\\alpha$-SVRG to the baseline, we maintain the same hyperparameters, including regularization strength. While $\\alpha$-SVRG effectively lowers training loss, it may lead to overfitting, as indicated by the less significant improvements in validation accuracy in some datasets. This suggests a need for adjusting regularization parameters to enhance model generalization. **We believe this falls outside the scope of $\\alpha$-SVRG as an optimization method.** However, it presents an important area for future research, particularly in how optimization and regularization can be co-adapted for better performance.\n\n> No clear explanation on how $\\alpha$-SVRG scales to larger datasets: no explanations about the feasibility of taking full batch gradient every epoch [1]\n\nIn response to your concern about the feasibility of taking full batch gradients, especially for large datasets like ImageNet21k, we emphasize that it is indeed feasible. **By partitioning the dataset into smaller mini-batches, we can compute gradients for each mini-batch independently. After computing the gradients for each mini-batch, we accumulate them to obtain the full gradient for the entire dataset.**"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission763/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700462810824,
                "cdate": 1700462810824,
                "tmdate": 1700628602332,
                "mdate": 1700628602332,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LKJMlG8HEI",
                "forum": "x13bw5VQkf",
                "replyto": "0FEh46fxgv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission763/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission763/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> No discussion on the supposition than the noise of SGD might be an element for better generalization [2]\n\nWe acknowledge that the noise introduced by SGD can indeed contribute to better generalization in some cases. **We recognize the importance of this aspect and have included a discussion in the last paragraph of Section 5.2 in the revision:**\n\nIntriguingly, SVRG with negative one coefficient has recently shown to be able to improve generalization [2].\n\n> $\\theta^{past}$ is not a standard notation, I would recommend $\\widetilde{\\theta}$ instead.\n\nThank you for pointing out this notation convention. **We have updated the notation for snapshot to $\\boldsymbol{\\widetilde{\\theta}}$ throughout the paper.**\n\n\n> An explanation of the three metrics in Table 1 is required. How are they different? \n\n**In the revision, we replace the original mathematical description with an intuitive explanation for each metric**:\nMetric 1 captures the directional variance of the gradients; metric 2 sums the variance of gradients across each component; metric 3 focuses on the magnitude of the most significant variation.\n\n> The notation $g_i$ is unclear. The notation $g_{i,k}$ in metric 2 of Table 1 is not defined. \n\n1. **To address this, we have included the mini-batch index $i$ when formulating the variance-reduced gradient in the revision.\n$g_i^t = \\nabla f_i(\\theta^t)-(\\nabla f_i(\\widetilde{\\theta})-\\nabla f(\\widetilde{\\theta}))$.** This change has been applied throughout the paper wherever the variance-reduced gradient is mentioned, including the gradient variance paragraph at the bottom of page 2, where we describe how $g_i^t$ is collected from model checkpoints.\n\n2. **We have added the clarification $g_{i,k}^t$ in the caption of** [Table 1](https://drive.google.com/uc?id=1dMku3LfTmax2wZZBMSzPCjXfR2ZPXn_g)**: $k$ indexes the $k$-th component of gradient $g_{i, k}^t$.**\n\n> Precising the baseline in the legends (SGD or AdamW) would be better.\n\nThank you for pointing this out. **We have made the following improvements in the revision to ensure it is explicitly specified:** \n1. In each plot of the revision, we have changed legend from baseline to SGD or AdamW. \n2. Furthermore, throughout sections 2, 3, and 4, we have specified the baseline optimizer in the main text of each experiment.\n3. In section 5, under the training paragraph of the first subsection, we have explicitly mentioned that the base optimizer employed is AdamW.\n\n> The \"snapshot interval\" is better known as \"inner loop size\"\n \nThank you for pointing out this notation convention. **We have changed this in section 5.3 of the revision.**\n\n> Why are works related to alternative optimization cited page 2? No clear relevance.\n\n**Our original intention was to explain how SVRG can be combined with other base optimizers.** In the revised version, this paragraph has been restructured to improve readability:\n\nInitially, SVRG was introduced in the context of vanilla SGD. Subsequent studies [3,4] have integrated SVRG into alternative base optimizers. Following these works, we directly input the variance reduced gradient $g_i^t$ into the base optimizer...\n\n> How large are the mini-batches for $\\alpha$-SVRG for the different experiments?\n\n**We have included all hyper-parameters, including mini-batch sizes, along with the training recipe, in Appendix B.**\n|            | C-100 | Pets | Flowers | STL-10 | Food | DTD  | SVHN | EuroSAT | ImageNet1K |\n|------------|-------|------|---------|--------|------|------|------|---------|------|\n| batch size | 1024  | 128  | 128     | 128    | 1024 | 128  | 1024 | 512     | 4096 |\n\n> Page 5, \"This is likely because the gradient direction varies more and becomes less certain as models become better trained\" -> are there other works confirming this statement?\n\nWe realize our original message in that paragraph is not conveyed clearly. **We rephrase that paragraph as follows:**\n\n*Observation 2: average optimal coefficients of deeper model's in each epoch generally decrease as training progresses.* This suggests that as training advances, the average correlation (Equation 6) of each epoch decreases. We further analyze this epoch-wise decreasing pattern in Appendix D.\n\n> Page 5, is the standard deviation ratio of equation (6) constant across iterations?\n\n**Overall, the std ratio is relatively stable over iterations, but the correlation has a similar trend as the optimal coefficient**. The comparison is [here](https://drive.google.com/file/d/1jraqxte7uLsr_4_H1TX94mJv1fvn9Ri_/view?usp=sharing). We have also included this comparison in Appendix D."
                    },
                    "title": {
                        "value": "Rebuttal by Authors"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission763/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700462907612,
                "cdate": 1700462907612,
                "tmdate": 1700690614481,
                "mdate": 1700690614481,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TAfvfIq1Aq",
                "forum": "x13bw5VQkf",
                "replyto": "0FEh46fxgv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission763/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission763/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "> Table 3, validation accuracy of SVRG, datasets Pets, STL-10 : have you double checked this results? The accuracy gap is very important. How can this be explained?\n\nWe have double-checked the results. We have also rerun these experiments with three different seeds. The results are displayed as below:\n\n**Train Loss**:\n|                    | CIFAR-100              | Pets                  | Flowers               | STL-10                | Food-101              | DTD                   | SVHN                  | EuroSAT               |\n|--------------------|------------------------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|\n| baseline           | 2.645 \u00b1 0.013          | 2.326 \u00b1 0.088         | 2.436 \u00b1 0.038         | 1.660 \u00b1 0.017         | 2.478 \u00b1 0.021         | 2.072 \u00b1 0.066         | 1.583 \u00b1 0.005         | 1.259 \u00b1 0.017         |\n| + $\\alpha$-SVRG      | **2.606 \u00b1 0.017**      | **2.060 \u00b1 0.071**     | **2.221 \u00b1 0.042**     | **1.577 \u00b1 0.022**     | **2.426 \u00b1 0.007**     | **1.896 \u00b1 0.075**     | **1.572 \u00b1 0.011**     | **1.239 \u00b1 0.016**     |\n\n**Validation Accuracy:**\n|                    | CIFAR-100              | Pets                  | Flowers               | STL-10                | Food-101              | DTD                   | SVHN                  | EuroSAT               |\n|--------------------|------------------------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|\n| baseline           | 81.02 \u00b1 0.07           | 70.61 \u00b1 1.55          | 80.33 \u00b1 1.01          | 80.80 \u00b1 1.46          | 85.29 \u00b1 0.47          | 56.21 \u00b1 1.19          | 94.29 \u00b1 0.67          | 97.91 \u00b1 0.12          |\n| + $\\alpha$-SVRG       | **81.07 \u00b1 0.22**       | **76.37 \u00b1 1.06**      | **84.15 \u00b1 1.15**      | **83.65 \u00b1 0.92**      | **85.45 \u00b1 0.43**      | **61.44 \u00b1 0.35**      | **94.94 \u00b1 0.60**      | **98.13 \u00b1 0.07**      |\n\n1. **The results show $\\boldsymbol{\\alpha}$-SVRG\u2019s consistent decrease in train loss and improvement in validation accuracy.**\n\n2. **We believe the significant validation accuracy improvement can be explained by the fact that the models trained on these datasets are still well within the underfitting regime.** From the values of validation accuracy, we can see they still have space to improve. Given that $\\alpha$-SVRG accelerates convergence by reducing gradient variance, it can significantly impact the validation accuracy for very underfitting models.\n\n> Should be cited: works on optimal implementation and parameters for SVRG [5, 6]\n\n**In the revision, we have added these works [5, 6] into the related work section.**\n\n> To enrich the bibliography, SVRG has also been extended to policy learning [7, 8] and other examples are given in [9]\n\n**For the works [7, 8], we have added them at the end of the second paragraph in the introduction. For the work [9], we believe it is a good survey for readers to review related background information about variance reduction methods. Therefore, we add it at the first paragraph in the related works section.**\n\nWe thank you again for your valuable feedback and we hope our response can address your questions. If you have any further questions or concerns, we are very happy to answer.\n\nReferences:\n\n[1] On the ineffectiveness of variance reduced optimization for deep learning. Defazio et al., 2019.\\\n[2] Towards Better Generalization: BP-SVRG in Training Deep Neural Networks. Jin et al., 2019.\\\n[3] Svrg meets adagrad: Painless variance reduction. Dubois-Taine et al., 2021.\\\n[4] Divergence results and convergence of a variance reduced version of adam. Wang et al., 2022.\\\n[5] Towards closing the gap between the theory and practice of SVRG. Sebbouh et al., 2019.\\\n[6] Stopwasting my gradients: Practical svrg.Harikandeh, et al., 2015.\\\n[7] Stochastic variance-reduced policy gradient, Papini et al., 2018.\\\n[8] Stochastic variance reduction methods for policy evaluation, Du et al., 2017.\\\n[9] Variance-reduced methods for machine learning. Gower, et al., 2020."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission763/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700462949950,
                "cdate": 1700462949950,
                "tmdate": 1700727843279,
                "mdate": 1700727843279,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "myBlfGqAZl",
                "forum": "x13bw5VQkf",
                "replyto": "8o1nUjKRCS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission763/Reviewer_LXQJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission763/Reviewer_LXQJ"
                ],
                "content": {
                    "title": {
                        "value": "Robustness wrt $alpha_0$"
                    },
                    "comment": {
                        "value": "I thank the authors for taking into my remark on the additional parametrization. I checked Table 6 in the appendix and agree results seem robust wrt $alpha_0$.\n\nYet, the knowledge of the number of iterations is required to perform a linear decrease to 0. This point has been omitted. This parametrization prevents the usage of the method for new problems"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission763/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700691250234,
                "cdate": 1700691250234,
                "tmdate": 1700691250234,
                "mdate": 1700691250234,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qsrnKV6Glu",
                "forum": "x13bw5VQkf",
                "replyto": "8o1nUjKRCS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission763/Reviewer_LXQJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission763/Reviewer_LXQJ"
                ],
                "content": {
                    "title": {
                        "value": "Std and full Gradient taken for each snapshot"
                    },
                    "comment": {
                        "value": "Adding Table 7 support more clearly the efficiency of the suggested method. \n\nI still have a concern, if the full gradient is taken at each iteration, then how aren\u2019t there plateau on the SVRG curves when comparing to SGD? Do we agree that when entering the outer loop, the full batch Gradient is computed, equivalent to one epoch of computations? I fear the computation of the snapshot is omitted. Or are the curves smoothed?"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission763/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700692024962,
                "cdate": 1700692024962,
                "tmdate": 1700692024962,
                "mdate": 1700692024962,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "b3fhUeFBuI",
                "forum": "x13bw5VQkf",
                "replyto": "TAfvfIq1Aq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission763/Reviewer_LXQJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission763/Reviewer_LXQJ"
                ],
                "content": {
                    "title": {
                        "value": "Concern about the step size schedule, alpha-SVRG becoming SGD for efficiency and opposite conclusion with Defazio & Bottou"
                    },
                    "comment": {
                        "value": "After checking again the paper, I found no reference to the fact that SVR method actually can converge using constant step size as opposed to SGD, in convex settings of course.\n\nThis is worth mentioning, but more importantly  this changes the method. If SVRG to be efficient requires to\n- have a decreasing alpha to zero\n- decreasing step size\nThen it gets very close to be SGD. \n\nFor instance in Defazio & Bottou, the authors use piecewise constant step size. \n\n\nI also have a concern about the general finding which is that alpha should go to zero at the end of the training process and thus recover SGD. This seems to be contradictory with Defazio & Bottou stating in Section 9:\n\u201c As we have shown that SVRG appears to only introduce a benefit late in training,\u201d"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission763/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700693509210,
                "cdate": 1700693509210,
                "tmdate": 1700693509210,
                "mdate": 1700693509210,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Dk1lOZq9KH",
                "forum": "x13bw5VQkf",
                "replyto": "0FEh46fxgv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission763/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission763/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to efficiency of $\\alpha$-SVRG"
                    },
                    "comment": {
                        "value": "Thank you for recognizing the effectiveness of our method.\n\nWe would like to clarify that the full gradient is taken at each epoch instead of each iteration. In addition, in the training loss plot for each experiment, we did not include the train loss of the snapshot when evaluating the full gradient on the snapshot for each epoch. This convention has been adopted in the original work of SVRG [1], later works analyzing SVRG in non-convex settings [2, 3], and the work investigating the practical effectiveness of SVRG [4], when comparing the train loss of SVRG with that of SGD in experiments. **This comparison method in terms of train loss only focuses on how the train loss of the model changes after the optimizer updates the model.**\n\nReferences:\n\n[1] Accelerating stochastic gradient descent using predictive variance reduction. Johnson & Zhang, 2013.\\\n[2] Variance reduction for faster non-convex optimization. Allen-Zhu & Hazan, 2016.\\\n[3] Stochastic Variance Reduction for Nonconvex Optimization. Reddi et al., 2016."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission763/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700694319922,
                "cdate": 1700694319922,
                "tmdate": 1700704259037,
                "mdate": 1700704259037,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "F03bNla3ms",
            "forum": "x13bw5VQkf",
            "replyto": "x13bw5VQkf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission763/Reviewer_3pZ5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission763/Reviewer_3pZ5"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose $\\alpha$-SVRG, an improved version of SVRG designed to tackle the ineffectiveness of SVRG in training deep neural networks. The method involves decreasing the weight added to the variance estimated using the model snapshot. This method is obtained using"
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is presented in a clear manner, especially the experiments.\n2. The proposed method is effective according to the experiments."
                },
                "weaknesses": {
                    "value": "1. The baseline is a bit unclear in some scenarios. See questions.\n2. It seems that by saying that $\\alpha_t$ decreases linearly, the authors mean that $\\alpha_t=O(1/t)$. However, in some literature, a linearly decreasing sequence is a geometric sequence. The authors may want to be more clear about this.\n3. Although it can be observed both intuitively and empirically that $\\alpha_t$ should be decreased within an epoch, it could be a bit too arbitrary to conclude that $\\alpha_t$ should be decreasing **linearly**. It's hard to see why it is preferred over other schedules, e.g., $\\alpha_t=O(t^{-r})$ or $\\alpha_t=O(q^{-t})$.\n4. The part where the proposed method is applied to AdamW is a bit unclear. Could the authors guide me to previous works where SVRG is combined with adaptive momentum methods, if there is any? If there is, then the authors may want to refer to these works and build the proposed method based on the previous ones; otherwise it is worth discussing in greater detail how SVRG can be combined with AdamW, as the effect of the variance estimator could be more complicated due to the existence of moments.\n5. The performance of the proposed method applied to ImageNet-1K is mixed compared to the baseline."
                },
                "questions": {
                    "value": "1. Can the authors provide more details about the baseline? In some sections of this work, the baseline is AdamW, but I'm not sure whether this applies to all experiments.\n2. Can the authors explain more about why even the  only seems to suppress variance more effectively in the early stages of training, but is less effective in later stages?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission763/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission763/Reviewer_3pZ5",
                        "ICLR.cc/2024/Conference/Submission763/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission763/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698710329580,
            "cdate": 1698710329580,
            "tmdate": 1700681192597,
            "mdate": 1700681192597,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5SXwnPipkT",
                "forum": "x13bw5VQkf",
                "replyto": "F03bNla3ms",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission763/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission763/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "We genuinely appreciate your valuable comments. We are pleased that you find our paper well-organized and acknowledge the effectiveness of $\\alpha$-SVRG in training neural networks. We would like to address the comments and questions you raised:\n> The baseline is a bit unclear in some scenarios. See questions. Can the authors provide more details about the baseline? In some sections of this work, the baseline is AdamW, but I'm not sure whether this applies to all experiments.\n\nRegarding the clarity of the baseline in our experiments, **we have made improvements in the revision to ensure it is explicitly specified:** \n1. In each plot of the revision, we have changed legend from baseline to SGD or AdamW. \n2. Furthermore, throughout sections 2, 3, and 4, we have specified the baseline optimizer in the main text of each experiment.\n3. In section 5, under the training paragraph of the first subsection, we have explicitly mentioned that the base optimizer employed is AdamW.\n\n> It seems that by saying that decreases linearly, the authors mean that $\\alpha_t=O(1/t)$. However, in some literature, a linearly decreasing sequence is a geometric sequence. The authors may want to be more clear about this.\n\nWe want to clarify that our linearly decreasing schedule for $\\alpha$-SVRG is not represented as $\\alpha_t=O(1/t)$. **Instead, we use a coefficient that linearly decays across epochs while remaining constant between consecutive snapshots**. For clarity, we decompose the global iteration index $t$ into the epoch-wise index $s$ and the iteration index $i$ within an epoch. We also denote the total training epochs as $T$ and the number of iterations in one epoch $M$. So the value of the default linearly decreasing coefficient at iteration $t$ can be formally written as:\n$$\\alpha_\\text{linear}^t = \\alpha_0(1-\\frac{s}{T}).$$\nDue to the limited space in the main text, we have included this formula in Appendix F of the revision and added a reference to it in the $\\alpha$-SVRG paragraph in Section 4. **The reason behind choosing this schedule will also be discussed in response to your next question.**\n\n> Although it can be observed both intuitively and empirically that should be decreased within an epoch, it could be a bit too arbitrary to conclude that $\\alpha_t$ should be decreasing linearly. It's hard to see why it is preferred over other schedules, e.g., $\\alpha_t=O(t^{-r})$ or $\\alpha_t=O(q^{-t})$\n\n1. We agree that it is crucial to explore different scheduling methods to determine the most effective approach. **In response, we have incorporated five additional coefficient schedules into our study: quadratic, geometric, double-linear, double-quadratic, and double-geometric.** The quadratic and geometric schedules are designed to provide a global decay pattern, starting from an initial value and decreasing uniformly over epochs while remaining constant within each epoch\uff1a\n\\begin{flalign}&\\alpha_\\text{quadratic}^t = \\frac{\\alpha_0}{T^2}s^2\\\\\\\\\n    &\\alpha_\\text{geometric}^t = \\alpha_0(\\frac{10^{-2}}{\\alpha_0})^{\\frac{s}{T}}\\end{flalign}\nIn contrast, the double-x schedules (double-linear, double-quadratic, and double-geometric) introduce local decays within each epoch, starting from 1 and decreasing to a value specified by the global decay pattern:\n\\begin{flalign}&\\alpha_\\text{d-linear}^t = (1-\\alpha_0(1-\\frac{s}{T}))(1-\\frac{i}{M}) + \\alpha_0(1-\\frac{s}{T})\\\\\\\\\n&\\alpha_\\text{d-quadratic}^t =  (1-\\frac{\\alpha_0}{T^2}s^2)\\frac{1}{M^2}(M-i)^2+\\frac{\\alpha_0}{T^2}s^2\\\\\\\\\n&\\alpha_\\text{d-geometric}^t = (\\alpha_0(\\frac{10^{-2}}{\\alpha_0})^{\\frac{s}{T}}+10^{-2})^{\\frac{i}{M}}\\end{flalign}\n\n2. We evaluated these scheduling methods by training ConvNeXt-Femto on the STL-10 dataset. The training loss results for different initial coefficients (0.5, 0.75, 1) across all six schedules are as follows:\n| train loss     | linear    | quadratic | geometric | d-linear | d-quadratic | d-geometric |\n|----------------|-----------|-----------|-----------|----------|-------------|-------------|\n| $\\alpha$-SVRG $(\\alpha_0=0.5)$   | **1.583** | 1.607     | 1.616     | 2.067    | 1.967       | 1.808       |\n| $\\alpha$-SVRG $(\\alpha_0=0.75)$  | **1.568** | 1.576     | 1.582     | 2.069    | 2.003       | 1.931       |\n| $\\alpha$-SVRG $(\\alpha_0=1)$     | 1.573     | **1.563** | 1.574     | 1.997    | 1.970       | 1.883       |"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission763/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700462514199,
                "cdate": 1700462514199,
                "tmdate": 1700630779164,
                "mdate": 1700630779164,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VHJGxuH9KW",
                "forum": "x13bw5VQkf",
                "replyto": "F03bNla3ms",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission763/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission763/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "(continue from the part 2 ...)\n\n**Our results demonstrate that the global schedules (linear, quadratic, and geometric) consistently achieved lower training losses compared to the baseline (1.641). On the other hand, the double schedules (double-linear, double-quadratic, and double-geometric) resulted in higher training losses.** This suggests that the local decreasing coefficient in the double schedules may occasionally overestimate the optimal coefficient, increasing gradient variance and hindering convergence. Conversely, the global schedules, while potentially underestimating the optimal coefficient, provide a milder and more reliable variance reduction effect, leading to consistently lower training losses. The formula of each schedule and the experiment results along with the analysis are provided in Appendix F of our revision.\n\n> The part where the proposed method is applied to AdamW is a bit unclear. Could the authors guide me to previous works where SVRG is combined with adaptive momentum methods, if there is any? If there is, then the authors may want to refer to these works and build the proposed method based on the previous ones; otherwise it is worth discussing in greater detail how SVRG can be combined with AdamW, as the effect of the variance estimator could be more complicated due to the existence of moments.\n\n**The key principle behind combining SVRG with adaptive methods, such as AdamW, is that SVRG only modifies the gradient to reduce its variance and does not update the model weights. The variance reduced gradient is then directly fed into the base optimizer to perform the weights update.** This approach ensures that the core mechanism of SVRG is independent of the baseline optimizer. These conventions are followed from previous works [1, 2]. We have included an additional paragraph on Page 2, above the gradient variance paragraph, to clarify how SVRG can be effectively combined with adaptive methods. \n\n> The performance of the proposed method applied to ImageNet-1K is mixed compared to the baseline.\n\nFor smaller models, a lower training loss usually directly translates to a higher validation accuracy. However, in larger models (Mixer-S, ViT-B, and ConvNeXt-B), while $\\boldsymbol{\\alpha}$-SVRG lowers the training loss, it pushes the model toward the regime of overfitting, thus lowering the validation accuracy. Additional adjustments to the regularization strength may be required to achieve better generalization. **We believe that addressing the issue of overfitting and fine-tuning regularization parameters may fall outside the immediate scope of $\\alpha$-SVRG as an optimization technique.** However, we recognize it is important to understand how optimization methods and regularization strategies can be co-adapted for improved model performance and generalization for future research.\n\n> Can the authors explain more about why even the only seems to suppress variance more effectively in the early stages of training, but is less effective in later stages?\n\nAssume that you are referring to SVRG. **We attribute the diminishing effectiveness of SVRG in later stages to its constant default coefficient of one.**\n\n1. **The optimal coefficient derived in our paper reduces the variance in SVRG optimally.** The coefficient in SVRG controls the strength of the variance reduction term added to the model's stochastic gradient. This coefficient essentially balances between the unmodified stochastic gradient and a modified gradient with potentially higher variance. A coefficient of 0 means no variance reduction, whereas a very high coefficient value can lead to a significant increase in the variance of the modified gradient. \n\n2. **The weakening correlation requires a lower optimal coefficient to effectively reduce variance without adding unwanted risk.** As illustrated in Figure 4, the optimal coefficient typically remains well below one. Therefore, using a coefficient value of one throughout the entire training process is far from optimal and poses a risk of introducing additional gradient variance. In addition, as training advances, the weakening correlation between snapshot gradients and model gradients leads to a lower optimal coefficient, further increasing the risk of introducing unwanted variance.\n\nWe thank you again for your valuable feedback and we hope our response can address your questions. If you have any further questions or concerns, we are very happy to answer.\n\nReferences:\n\n[1] Svrg meets adagrad: Painless variance reduction. Dubois-Taine et al., 2021\\\n[2] Divergence results and convergence of a variance reduced version of adam. Wang et al., 2022."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission763/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700462629915,
                "cdate": 1700462629915,
                "tmdate": 1700627776330,
                "mdate": 1700627776330,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "KDb7AW9IjA",
            "forum": "x13bw5VQkf",
            "replyto": "x13bw5VQkf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission763/Reviewer_FPSM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission763/Reviewer_FPSM"
            ],
            "content": {
                "summary": {
                    "value": "This paper reveals that the variance reduction strength in SVRG should be lower for deep networks and decrease as training progresses.\nThus, this paper introduce a multiplicative coefficient $\\alpha$ to control its\nstrength and adjust it with a linear decay schedule. This paper proposes a novel method  named $\\alpha$-SVRG.\nExperiments are conducted to  demonstrate  that $\\alpha$-SVRG better optimizes neural networks, consistently\nlowering the training loss compared to both baseline and standard SVRG across\nvarious architectures and datasets. This paper is the first to bring the benefit of\nSVRG into training neural networks at a practical scale."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper is the first to bring the benefit of\nSVRG into training neural networks at a practical scale."
                },
                "weaknesses": {
                    "value": "The experiments show the potential of $\\alpha$-SVRG. However, the experiment results and the comparisons does not consider the computation cost. It seems that $\\alpha$-SVRG takes three times computation cost as much as AdamW since $\\nabla f_i(\\theta^{past})$ takes extra cost and $\\nabla f(\\theta^{past})$ is computed for each $39$-iterations.\nIf  the computation cost is considered, I doubt the effciency and effectiveness of $\\alpha$-SVRG."
                },
                "questions": {
                    "value": "No"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission763/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699068667747,
            "cdate": 1699068667747,
            "tmdate": 1699636003603,
            "mdate": 1699636003603,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ncDcVIl0Kz",
                "forum": "x13bw5VQkf",
                "replyto": "KDb7AW9IjA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission763/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission763/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "We sincerely thank you for your comment. We appreciate your recognition of the value of $\\alpha$-SVRG in training real-world neural networks. We address the question below:\n> The experiments show the potential of $\\alpha$-SVRG. However, the experiment results and the comparisons do not consider the computation cost. It seems that $\\alpha$-SVRG takes three times computation cost as much as AdamW since takes extra cost and is computed for each $\\nabla f_i(\\theta^\\text{past})$ takes extra cost and $\\nabla f(\\theta^\\text{past})$ is computed for each-39 iterations. If the computation cost is considered, I doubt the efficiency and effectiveness of $\\alpha$-SVRG.\n\n1. We acknowledge that $\\alpha$-SVRG, in its basic form, appears to require a higher computational cost. **However, we have implemented an optimization that reduces this overhead from 3x to 2x compared to AdamW.** Our approach involves caching the snapshot mini-batch gradient $\\nabla f_i(\\theta^\\text{past})$ (when evaluating the full gradient $\\nabla f(\\theta^\\text{past})$) and utilizing it when computing the variance reduced gradient. This technique is implemented as Cache_SVRG in our codebase (supplementary material). \n\n2. **It is important to note that the additional computational cost is a common characteristic of various variance reduction techniques, not exclusive to** $\\boldsymbol{\\alpha}$**-SVRG.** Similar methods [2, 3, 4, 5, 6] also exhibit increased computational demands but offer theoretical advantages over SGD. These methods all have proved in theory that the 3x increase in computation can lead to a lower error bounds compared to naively calling SGD 3x times. However, this theoretical advantage has yet to be extensively validated in practical settings.\n\n3. **Our primary objective with $\\alpha$-SVRG is to enhance its practical effectiveness.** In our experiments, $\\alpha$-SVRG consistently decreases the train loss than the standard SVRG and the baseline across a variety of models and datasets. Our work on $\\alpha$-SVRG is the first step towards improving the efficiency of SVRG-based methods in training real-world neural networks.\n\nWe thank you again for your valuable feedback and we hope our response can address your questions. If you have any further questions or concerns, we are very happy to answer.\n\nReferences:\n\n[1] Saga: A fast incremental gradient method with support for non-strongly convex composite objectives. Defazio et al., 2014a.\\\n[2] Katyusha: The first direct acceleration of stochastic gradient methods. Allen-Zhu, 2017.\\\n[3] Spider: Near-optimal non-convex optimization via stochastic path integrated differential estimator. Fang et al., 2018.\\\n[4] Sarah: A novel method for machine learning problems using stochastic recursive gradient. Nguyen et al., 2017.\\\n[5] Non-convex finite-sum optimization via scsg methods. Lei et al., 2017.\\\n[6] Spiderboost and momentum: Faster stochastic variance reduction algorithms. Wang et al., 2019."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission763/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700462233734,
                "cdate": 1700462233734,
                "tmdate": 1700625473845,
                "mdate": 1700625473845,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]