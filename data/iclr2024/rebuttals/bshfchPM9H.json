[
    {
        "title": "RAPPER: Reinforced Rationale-Prompted Paradigm for Natural Language Explanation in Visual Question Answering"
    },
    {
        "review": {
            "id": "JtMcluxKGN",
            "forum": "bshfchPM9H",
            "replyto": "bshfchPM9H",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3481/Reviewer_iXW4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3481/Reviewer_iXW4"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces RAPPER, a two-stage Reinforced Rationale-Prompted Paradigm designed to improve Natural Language Explanation (NLE) in Visual Question Answering (VQA) tasks. The first stage employs knowledge distillation from large language models (LLMs) to generate rationales that are fact-based. The second stage introduces a unique Reinforcement Learning from NLE Feedback (RLNF) to incorporate visual facts into the NLE generation. The paper claims that RAPPER outperforms existing state-of-the-art methods in VQA-NLE on two benchmarks, providing more plausible and faithful explanations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper addresses the problem of implausibility and hallucination in NLE for VQA, which is a novel contribution. The two-stage approach combining knowledge distillation and reinforcement learning is also unique and the RLNF technique for incorporating visual facts into NLE is particularly noteworthy.\n\nThe paper is well-organized and clearly articulates the problem, the proposed solution, and its advantages. The use of figures to illustrate the model architecture and the comparison with existing methods is helpful.\n\nImproving the plausibility and faithfulness of NLE in VQA has important implications for real-world applications, such as medical VQA, where interpretability is crucial."
                },
                "weaknesses": {
                    "value": "The two-stage approach, while novel, adds complexity to the model. It would be beneficial to see a discussion on the trade-offs involved, such as computational cost.\n\nThe paper focuses on VQA tasks, and it's not clear how's performance of the proposed method when it was adapted to other vision-language tasks.\n\nNo human evaluation is conducted regarding the generation quality."
                },
                "questions": {
                    "value": "How does the complexity of the two-stage approach impact the computational efficiency of the model?\n\nCould you elaborate on how the RLNF stage specifically tackles the hallucination problem in NLE?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3481/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698558840446,
            "cdate": 1698558840446,
            "tmdate": 1699636301413,
            "mdate": 1699636301413,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zrhvmrQya6",
                "forum": "bshfchPM9H",
                "replyto": "JtMcluxKGN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3481/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3481/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer iXW4 (1/2)"
                    },
                    "comment": {
                        "value": "**Q1: The two-stage approach, while novel, adds complexity to the model. It would be beneficial to see a discussion on the trade-offs involved, such as computational cost. How does the complexity of the two-stage approach impact the computational efficiency of the model?**\n\n**A1:** We thank the reviewer for suggesting further analysis for the computational cost. To discuss the trade-off between computational cost and our model performance, we now present the total and trainable model parameters of Rapper and its ablated variations (as listed in the upper part of Table 3). \n\nFrom the table below, we see that Rapper significantly improves the metrics of natural language generation (e.g., CIDEr, SPICE, and Rough-L) over both the Baseline (Rapper w/o KD and w/o RLNF, a one-stage approach) and Rapper w/o RLNF (a two-stage approach). It is worth noting that, the Baseline only deploys the proposed reasoning module $M$ that is based on BLIP2 [1] and has a total of 3.82B parameters, including 187M trainable parameters (i.e., Qformer, initialized from BERT [2]) and 3.63B freezed parameters (i.e., ViT [3] and OPT [4]). On the other hand, Rapper w/o RLNF and the full version of Rapper are composed of the proposed rationale generator $G$ and reasoning module $M$. For such 2-stage frameworks, both modules are based on BLIP2 and share 3.63B freezed parameters, with 187M trainable parameters for each (and thus a total of 374M trainable parameters). It can be seen that, the two-stage methods do require a larger number (2X) of trainable parameters than the one-stage method does. However, the increased amount is in the same order (187M), and it is remarkably less than the total amount of parameters (~4B).\n\nAs for the training/inference stages, the runtime estimates are also listed in the table below, which are performed on 8/1 Nvidia V100 GPU(s) on the VQA-X [5] dataset. From the table below, we see that the two-stage method of Rapper w/o RLNF requires 2X computation time for both training and testing, and the full version of Rapper needs additional 3X computation time due to the RL optimization scheme. Nevertheless, even with RLNF, training can be done in 18hrs without utilizing an advanced GPU environment. And, acceleration of LLM training/inference is still an active research topic in the community. We sincerely thank the reviewer for suggesting the above analysis, which will be added to the revised version for the completeness of our discussions.\n\n\n| Method | Total / Trainable param. | Training time (hr) | Inference time (ms/sample) | B@4  | Rough_L | CIDEr | SPICE | Acc. |\n|-|-|-|-|-|-|-|-|-|\n| Baseline (Rapper w/o KD w/o RLNF) | 3.82B / 187M | 3 | 167.7 | 29.3 | 51.6 | 112.1 | 22.3 | 85.0 |\n| Rapper w/o RLNF | 4.01B / 374M | 6 | 291.3 | 31.2 | 52.5 | 120.2 | 24.2  | 86.6 |\n| *Rapper* | 4.01B / 374M | 18 | 291.3 | **31.8** | **52.9** | **124.0** | **24.5**  | **87.3** |\n\n[1] Li, Junnan, et al. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. ICML 2023.\n\n[2] Devlin et al. Bert: Pre-training of deep bidirectional transformers for language understanding. NAACL 2019.\n\n[3] Dosovitskiy et al. An image is worth 16x16 words: Transformers for image recognition at scale. ICLR 2021.\n\n[4] Zhang et al. Opt: Open pre-trained transformer language models. arXiv 2022.\n\n[5] Park et al. Multimodal explanations: Justifying decisions and pointing to the evidence. CVPR 2018.\n\n**Q2: The paper focuses on VQA. How's performance when it was adapted to other vision-language tasks?**\n\n**A2:** We thank the reviewer for the suggestion. We are happy to point out that we did apply our Rapper on another VL-NLE (natural lange explanation) task of Visual Entailment (VE), as listed in the lower part of Table 1 of our main paper. \n\nWe note that, VE requires one to assign a label determining the relationship between a premise image and a textual hypothesis (i.e., \u201centailment\u201d, \u201cneutral\u201d, and \u201ccontradiction\u201d), while giving an explanation for supporting the aforementioned relationship [6]. And, VE is tyically applied in cross-modal retrieval, etc. applications. In Table 1 of our main paper, we report the performance on the **e-SNLI-VE** benchmark, in which we observed that our Rapper performed favorably against NLXGPT [7] and S3C [8] in terms of different NLG metrics (i.e., CIDEr, SPICE, and ROUGE-L). We will add the definition of VE with the above additional clarification to Sect. 4.1 of our main paper.\n\n[6] Do et al. e-snli-ve: Corrected visual-textual entailment with natural language explanations. CVPRW 2020.\n\n[7] Sammani et al. Nlx-gpt: A model for natural language explanations in vision and vision-language tasks. CVPR 2022.\n\n[8] Suo et al. S3C: Semi-Supervised VQA Natural Language Explanation via Self-Critical Learning. CVPR 2023."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3481/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740046702,
                "cdate": 1700740046702,
                "tmdate": 1700740076551,
                "mdate": 1700740076551,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PPLWE91UJz",
            "forum": "bshfchPM9H",
            "replyto": "bshfchPM9H",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3481/Reviewer_iCAp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3481/Reviewer_iCAp"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a reinforced rationale-prompted paradigm (Rapper) for natural language explanation (NLE) in visual question answering (VQA). They aim to generate plausible and faithful NLEs to address issues like implausibility and hallucination in existing VQA-NLE methods. Rapper has two stages - knowledge distillation from large language models (LLMs) and reinforcement learning from NLE feedback (RLNF). In stage 1, it elicits pseudo rationales from LLM to encourage plausibility and filters rationales using QA model for quality.\nIn stage 2, it uses RLNF with answer and explanation scores as rewards to inject visual facts into rationales, improving faithfulness.\nRAPPER, when evaluated on VQA-X and e-SNLI-VE datasets, achieves new SOTA on both for NLE metrics. It shows better plausibility via higher CIDEr and SPICE scores compared to prior VQA-NLE methods and demonstrates improved faithfulness through higher RefCLIPScore than previous methods. The approach reduces hallucination and implausibility qualitatively over other approaches."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper offers a novel two-stage approach to inject both language-based facts and visual content into rationales.\nIt leverages powerful knowledge and reasoning capabilities of LLMs through distillation. RLNF provides a way to align rationales with visual input for faithfulness. Rationale prompting is interpretable and improves reasoning module's NLE. Training is end-to-end, does not need ground truth rationales.\nMoreover, the qualitative results show more precise and reasonable NLEs: if achieves new SOTA on VQA-X and e-SNLI-VE for all NLE metrics in both filtered and unfiltered settings. It also shows higher CIDEr and SPICE scores demonstrating enhanced plausibility of NLEs.\nImproved RefCLIPScore indicates increased faithfulness and reduced hallucination.\nAblations validate importance of both knowledge distillation and RLNF stages and analysis of derived rationales indicates progressively better quality.\nQualitative examples exhibit more visually grounded and plausible NLEs than prior methods. It also reduces cases of implausible and hallucinated explanations over other VQA-NLE approaches.\nThe claims seem reasonably supported by the quantitative and qualitative results on the standard benchmarks. The improved performance across NLE metrics substantiates the effectiveness of the Rapper approach for plausible and faithful explanation generation. The ablation studies validate the contribution of the individual components. The qualitative examples provide additional evidence that Rapper produces more precise and reasonable rationales and explanations."
                },
                "weaknesses": {
                    "value": "Some potential weaknesses include:\nThe approach relies on eliciting high-quality pseudo rationales from the LLM, but the process for doing so is not extensively analyzed. In fact LLMs, especially smaller ones  (relative to e.g. GPT4) are prone to hallucinations. \nThe impact of different choices of LLM for knowledge distillation is not addressed.\nEvaluation is limited to VQA; extending Rapper to other VL tasks may reveal additional challenges.\nMore human evaluations on plausibility and faithfulness could further validate the approach."
                },
                "questions": {
                    "value": "How did you determine the optimal hyperparameters (e.g. threshold \u03c4) for filtering pseudo rationales from the LLM? Was any tuning or analysis done to validate these settings?\nDid you experiment with different LLMs for knowledge distillation? If so, how did the choice of LLM impact the quality of the distilled rationales?\nYou mention the potential to extend Rapper to other vision-language tasks. What challenges do you anticipate in adapting the approach to other datasets and tasks?\nThe elicitation process for pseudo rationales is a key component of your approach but is not analyzed in depth. Can you provide more details on this process and how the prompts were designed?\nCan you discuss any trade-offs between plausibility and faithfulness you observed? Does optimizing one tend to hurt the other?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3481/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698717083951,
            "cdate": 1698717083951,
            "tmdate": 1699636301321,
            "mdate": 1699636301321,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "S5ZfWv1XDR",
                "forum": "bshfchPM9H",
                "replyto": "PPLWE91UJz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3481/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3481/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer iCAp (1/2)"
                    },
                    "comment": {
                        "value": "**Q1: Choices of LLM for knowledge distillation (KD).**\n\n**A1:** We thank the reviewer for raising this issue. We are glad to conduct additional experiments to assess the LLM choices for KD in our framework. \n\nInstead of using LLaMA [1] in our proposed work, we now replace LLaMA with GPT-3.5 [2] for experiments, as detailed below. To obtain distilled rationales, we consider the rationales produced by GPT-3.5 as pseudo grounth for training the deployed generator $G$. As explained in Q5, such pseudo ground truth rationales are collected via performing in-context learning (ICL) on the pre-trained LLM of GPT-3.5. To assess the quality of such distilled rationales, we take image-question pairs and the generated rationales as the inputs to conduct VQA and calculate its accuracy, following the ablation studies reported in Fig. 4. That is, we apply the pre-trained multimodal large language model of mPlug-Owl [3] to take the image-question pair with the produced rationale to output the answer. Thus, the higher improved accuracy means the higher quality of rationales. \n\nWith the above experiments, we observe that further improved accuracy can be achieved (i.e., 69.84% with LLaMA vs. 71.35% with GPT-3.5). This suugests that our proposed framework is applicable to the use of SOTA LLMs if computational environments are allowed. We will be happy to add such suggested experiments and evaluations to Sec 4.3, verifying the robustness of our proposed framework.\n\n[1] Touvron, Hugo, et al. Llama: Open and efficient foundation language models. arXiv 2023.\n\n[2] OpenAI, 2022. Introducing chatgpt.\n\n[3] Ye et al. mplug-owl: Modularization empowers large language models with multimodality.\" arXiv 2023.\n\n\n**Q2: More human evaluations on plausibility and faithfulness.**\n\n**A2:** We thank the reviewer for suggesting additional experiments for subjective evaluation. Following the evaluation setting/process applied in NLXGPT [4] and S3C [5] (i.e., two SOTAs on VQA-NLE), we randomly select 200 test samples from the VQA-X [6] dataset with correctly predicted answers. Then, subjective evaluation is performed by 3 different annotators. Note that each annotator has to select one out of 4 choices: *yes, weak yes, weak no*, and *no*, as a response to whether the explanation justifies the answer. And, these 4 decisions are numerically mapped to 1, 2/3, 1/3, and 0, respectively. With averaged results obtained for each method, we present the performance comparisons in the following table. From this table, we see our proposed Rapper is preferable by the users in terms of subjective plausibility and faithfulness assessment. This conclusion also aligns with the objective quantification evaluation of Table 1 presented in the main paper.\n\n| Method | Human score regarding plausibility (&uarr;) | Human score regarding faithfulness (&uarr;) |\n|-|-|-|\n| NLXGPT [4] | 0.771 | 0.795 |\n| S3C [5] | 0.797 | 0.811 |\n| *Rapper* (Ours) | **0.845** | **0.859**  |\n\n[4] Sammani et al. Nlx-gpt: A model for natural language explanations in vision and vision-language tasks. CVPR 2022.\n\n[5] Suo et al. S3C: Semi-Supervised VQA Natural Language Explanation via Self-Critical Learning. CVPR 2023.\n\n[6] Park et al. Multimodal explanations: Justifying decisions and pointing to the evidence. CVPR 2018.\n\n**Q3: How to determine the optimal hyperparameters (e.g. threshold \u03c4) for filtering pseudo rationales from the LLM?**\n\n**A3:** Following [7], we simply set the threshold \u03c4 for filtering pseudo rationales as 0.54. As for other hyperparameters such as the RL discount factor for reward $\\gamma$ and entropy weight $\\alpha$ for calculating the reward function by Eqn (9), we follow [8] and set $\\gamma$ = 1.0 and $\\alpha$ = 0.2.\n\n[7] Changpinyo et al. All you may need for vqa are image captions. NAACL 2022.\n\n[8] Ziegler et al. Fine-Tuning Language Models from Human Preferences. arXiv 2019."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3481/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700739940219,
                "cdate": 1700739940219,
                "tmdate": 1700739940219,
                "mdate": 1700739940219,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QA1x10oM1W",
            "forum": "bshfchPM9H",
            "replyto": "bshfchPM9H",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3481/Reviewer_EbLA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3481/Reviewer_EbLA"
            ],
            "content": {
                "summary": {
                    "value": "The paper is about mitigating implausibility and hallucination problems (non-informative or contradicting visual context) for generating natural language explanation (NLE) under VQA problems. To mitigate the issue, the authors introduced a notion of \u201crationale\u201d which is similar to chain-of-thought prompting. To combat the issue of generating rationale without training data, the authors distill rationale from LLMs into the rationale generator. To penalize hallucinated rationale, \u201cReinforcement Learning from NLE Feedback\u201d is used. The combination of proposed method brings a marginal improvement on benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is largely well-written and easy to understand.\n-  The function of each component in the method is clear and sound.\n- The method achieves SOTA on NLE benchmarks."
                },
                "weaknesses": {
                    "value": "- The role of using rationale to improve implausibility and hallucination is unclear. It is well-known that chain-of-thought can improve reasoning. However, it is unclear to me if adding one more step, i.e., rationale could really mitigate hallucination.\n- While the method is sound, I\u2019m not very convinced that we cannot just use large vision language models and perform a chain-of-thought style prompting (which was actually the inspiration of this method)? How does large vision language models (e.g. BLIP-family or LLAVA models) perform?\n- In ablation study table, the impact of proposed method is small. Especially, RLNF effect is small.\n- A clear definition of \u201crationale\u201d is not presented in the paper. Only mentioned that it is like an \u201cintermediate\u201d just like in chain-of-thought prompting."
                },
                "questions": {
                    "value": "- Please answer my questions in \u201cweaknesses\u201d section. I may raise score if the rebuttal is satisfactory."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3481/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699091036183,
            "cdate": 1699091036183,
            "tmdate": 1699636301223,
            "mdate": 1699636301223,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0qpP4IBFcw",
                "forum": "bshfchPM9H",
                "replyto": "QA1x10oM1W",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3481/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3481/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer EbLA (1/2)"
                    },
                    "comment": {
                        "value": "**Q1: It is well-known that chain-of-thought (CoT) can improve reasoning. The role of using rationale to improve implausibility and hallucination is unclear. Why generating rationale could mitigate implausibility/hallucination?**\n\n**A1:** We thank the reviewer for giving us the opportunity to clarify the raised issues. \n\nWe first explain **how our work is fundamentally different from CoT promoting [1].** In NLP, the use of CoT has been a common technique, which utilizes a pre-trained LLM with manually-designed prompts with in-context learning (ICL) techniques, so that additional reasoning descriptions can be produced by that LLM. However, since the above technique is designed for language data, extending CoT to vision and language is not a trivial task. More specifically, it is not clear how to determine prompting conseutive examples which allow pre-trained vision-language models like LLAVA [2] to fully extract the information from both visual and text modality data for vision-language explanation tasks [3]. Another concern is that, since CoT (for NLP) requires manually selected prompts and does not provide additional facts related to the output, the output would suffer from hallucination [4]. For our Rapper, the deployed rationale generator is designed to exploit language-based facts and visual content from cross-modality data inputs (i.e., with our learning schemes via knowledge distillation (KD) and Reinforcement Learning from NLE Feedback (RLNF), as explained in the following paragraph). With the learned rationales as the text prompts, plausible and faithful natural language explanations (NLEs) for VQA can be performed. Based on the above explanations, we hope the reviewer could see **why CoT cannot be directly applied for reasoning in vision-language tasks.**\n\nWe now explain **how rationale learning helps mitigate implausibility and hallucination problems**. As noted in our paper, *implausibility* refers to the problem when NLEs are irrelevant to the questions or contradictory to the established supporting facts [5]. As discussed in Sect. 3.2 (Plausible NLE Generation) and depicted in Fig. 2(A), we tackle this problem by learning rationales enriched with language-based facts from a pre-trained LLM. That is, we deploy a rationale generator $G$ in Rapper, which is trained via distillating the knowledge from LLM. With such rationales with language-based facts serving as additional text prompts, together with the image-question pair as the inputs, the subsequent reasoning module $M$ can be trained to provide answers with plausible explanations. \n\nOn the other hand, *hallucination* refers to the problem when the output explanation is not related to the visual image [6]. As noted in Sect. 3.3 (Faithful NLE Generation) and illustrated in Fig. 2(B), this is tackled by the proposed Reinforcement Learning from NLE Feedback (RLNF) technique, which jointly trains $G$ and $M$ for enforcing the learned rationale to fully exploit the visual content. The motivation of our RLNF aligns with the hypothesis of Kadavath, et al. [7], which views a learning model towards a faithful one when it exhibits greater confidence in their output answers. In other words, with the proposed RL scheme and objective rewards, the outputs of our reasoning module $M$ need to be supported by the rationale (i.e., predticed by the generator $G$) containing the visual content from the input image. As confirmed in Table 2, our Rapper achieved the best performance on faithful NLE for VQA in terms of RefCLIPScore. We thank the reviewer again for giving us the opportunity to clarify how our Rapper mitigates the problems of implausibility and hallucination.\n\n\n[1] Wei et al. Chain-of-thought prompting elicits reasoning in large language models. NeurIPS 2022.\n\n[2] Liu et al. Visual instruction tuning. NeurIPS 2023.\n\n[3] Li et al. Mimic-it: Multi-modal in-context instruction tuning. arXiv 2023.\n\n[4] Dhuliawala et al. Chain-of-verification reduces hallucination in large language models., arXiv 2023.\n\n[5] Majumder et al., Knowledge-grounded self-rationalization via extractive and natural language explanations. ICML 2022.\n\n[6] Ji et al. Survey of hallucination in natural language generation. ACM 2023.\n\n[7] Kadavath et al. Language models (mostly) know what they know., arXiv 2022."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3481/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700036014081,
                "cdate": 1700036014081,
                "tmdate": 1700036014081,
                "mdate": 1700036014081,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wasW6zVuN8",
                "forum": "bshfchPM9H",
                "replyto": "QA1x10oM1W",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3481/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3481/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer EbLA (2/2)"
                    },
                    "comment": {
                        "value": "**Q2: While the method is sound, why not just use large vision-language models and perform a CoT style prompting (which inspires this method)? How do large vision language models (e.g., BLIP-family or LLAVA models) perform?**\n\n**A2:** We thank the reviewer for the positive feedback. We are glad to perform additional experiments as suggested. As noted in our response to Q1, we would like to point out that existing VL models like LLAVA [2] or BLIP-family [8, 9] do *not* support in-context learning (ICL). Nevertheless, we are happy to take the suggestion from the reviewer to assess the use of VL models with properly designed ICL. We now consider the VQA-X dataset and compare Rapper with Otter [10] (i.e., a VL model built on LLaMA7B). For Otter, we follow the ICL setting in PICa [11] to random sample 16 samples as the demonstration prompts for ICL. \n\n* Filtered results on the VQA-X dataset\n\n|Method|Bleu4|Meteor|Rough-L|CIDEr|SPICE|RefCLIPScore|Acc.|\n|-|-|-|-|-|-|-|-|\n|Otter w/ ICL|4.9|14.9|26.7|29.0|13.0|63.68|39.6|\n|*Rapper* (ours)|**31.8**|**24.3**|**52.9**|**124.0**|**24.5**|**67.05**|**87.3**|\n\n* Unfiltered results on the VQA-X dataset\n\n| Method|Bleu4|Meteor|Rough-L|CIDEr|SPICE|\n|-|-|-|-|-|-|\n| Otter w/ ICL|4.5|12.5|24.9|22.0|9.7|\n| *Rapper* (ours)|**30.0**|**23.3**|**51.3**|**116.0**|**23.2**|\n\nFrom the above tables, we see that Otter with ICL did not achieve comparable explanation performances as ours for both filtered and unfiltered cases (i.e., explanations associated with correct answers or not). This suggests that simple combination of VL models with ICL would not sufficiently address the VQA with natural language explanation, and a properly design VL model like ours would be desirable. \n\n[8] Li et al. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. ICML 2023.\n\n[9] Dai et al. InstructBLIP: Towards general-purpose vision-language models with instruction tuning. NeurIPS 2023.\n\n[10] Li et al. Otter: A multi-modal model with in-context instruction tuning. arXiv 2023.\n\n[11] Yang et al. An empirical study of gpt-3 for few-shot knowledge-based vqa. AAAI 2022\n\n**Q3: Small impact of the proposed method in the ablation study table (e.g., w/ or w/o RLNF)**\n\n**A3:** We thank the reviewer for pointing this out, and we are glad to clarify the performance improvements. In the upper part of Table 3, we perform ablation studies to verify the effectiveness of KD and RLNF. From this table, we see that the introduction of KD produced improved performance over the baseline model (e.g.,  CIDEr: 112.1 -> 120.2 and SPICE: 22.3 -> 24.2). This confirms the use of KD for predicting plausible outputs. Since our RLNF is deployed to mitigate the hallucination problem **based on plausible prediction**, adding RLNF to KD only increased the performance with smaller margins (e.g., CIDER: 120.2 -> 124.0 and SPICE: 24.2 -> 24.5). However, such improvements are expected. Recall that, as we explained in Q1, our Rapper is based on the introduction of KD and RLNF for providing plausible and faithful language explanation, and we cannot simply utilize RLNF without enforcing the NLE to be plausible. \n\nNevertheless, even with simply KD introduced and without RLNF, our model already performed favorably against SOTAs like S3C [12] and NLXGPT [13] (e.g., list CIDER/SPICE numbers, as shown in Table 1). And the full version of Rapper (i.e., with KD and RLNF) achieved the best results, which confirms our proposed learning schemes.\n\n[12] Suo et al. S3C: Semi-Supervised VQA Natural Language Explanation via Self-Critical Learning. CVPR 2023.\n\n[13] Sammani et al. Nlx-gpt: A model for natural language explanations in vision and vision-language tasks. CVPR 2022.\n\n**Q4: A clear definition of \u201crationale\u201d is not presented in the paper. Only mentioned that it is like an \u201cintermediate\u201d just like in chain-of-thought prompting.**\n\n**A4:** We thank the reviewer for the suggestion. Although rationale-based prompting techniques have been presented in [14, 15], a proper definition should be given in our paper for clarity purposes. More precisely, in our work, rationales are the text prompts that generated by the rationale generator $G$, which are injected with language-based facts and visual contents. As illustrated in Fig. 1, the reasoning module $M$ takes the learned rationales as the text prompts, together with the image and question inputs, realizing plausible and faithful NLE. We will add the above definition to the fourth paragraph of the introduction.\n\n[14] Zhang et al. Multimodal chain-of-thought reasoning in language models. arXiv 2023.\n\n[15] Krishna et al. Post hoc explanations of language models can improve language models. arXiv 2023."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3481/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700036146876,
                "cdate": 1700036146876,
                "tmdate": 1700036306573,
                "mdate": 1700036306573,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]