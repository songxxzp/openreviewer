[
    {
        "title": "Conservative Prediction via Data-Driven Confidence Minimization"
    },
    {
        "review": {
            "id": "OWMWvV7lf3",
            "forum": "HKV45Y0rFz",
            "replyto": "HKV45Y0rFz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6830/Reviewer_QVJM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6830/Reviewer_QVJM"
            ],
            "content": {
                "summary": {
                    "value": "Pretrained models can perform well on known observations but might be overconfident on unknown points, which hence incur high risks in safety-critical domains. Motivated by this, the authors proposed a conservative approach based on the Data-Driven Confidence Minimization for both selective classification and out-of-distribution detection tasks.  Particularly, they introduced a regularizer in the objective function to penalize the over-confidence in those unknown observations."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The authors provided insight into the choice of the auxiliary dataset severed as unknown observations, which can be used in the regularization part. \n* Empirically, the authors conducted extensive experiments to show that the proposed method is promising."
                },
                "weaknesses": {
                    "value": "* To be honest, I got overwhelmed for a while due to some confusing notations and definitions.\n  *  I am confused about the definition of \"unknown\" in selective classification. For examples,\n     * The authors first referred to the unknown examples as those not well-represented in training (paragraph 1, section 2), later they said that unknown examples are those misclassified points (paragraph 2, section 2). In selective classification, one may use \"hard\" observations (easily misclassified) instead of \"unknown\" to avoid the confusion of the \"unknown\" in OOD.  \n     * When it comes to the notation for unlabeled data $D_u$ exclusively used in the OOD detection, Proposition 4.1 and 4.2 then sounds targeting OOD detection with the conclusion \"DCM provably detects unknown examples\". Here I presume that \"detects unknown examples\" solely means \"detects OOD examples\", excluding the \"unknown\" examples in the selective classification task.\n  * The authors need to well-articulate the notations before these are used. For examples, \n    * What is $\\mathcal{P}_{ID}$? It is not friendly for readers not familiar with this topic.\n    * Please add the appendix reference for the definition of $\\delta$-neighborhoods in the comment after Proposition 4.1.\n    * What does $i$ in (4) stand for?\n\n* The uniform label distribution used for \"unknown\" observations is ad-hoc. It sounds like you are assuming all unknown examples overlap together and hence you cannot clearly distinguish them. However, what if there are only several overlapped classes? For example, some unknown observations from class 1 overlap with class 2, but they are disjoint with other classes. In this case, it is not appropriate to give non-zero probability to generate other \u201cpseudo\u201d labels for these unknown examples from class 1. Moreover, if this unknown example is an OOD point that is unlike any of the \"known\" classes, does it still make sense to give a non-zero probability to be labeled as \"known\" classes?\n\n* The authors need to discuss the practicality of the assumption \"$D^\\delta_{k}$ and $D^\\delta_{unk}$ are disjoint\" in Proposition 4.2 since the proposed method follows the corresponding theoretical guidance. In other words, is this assumption strong? \n  * The unknown/misclassified points (I follow your definition of \"unknown\") in selective classification could be those hard points from some classes that intrinsically overlap with each other, then \"known\" and \"unknown\" are not disjoint. \n  * In OOD detection, now that we have the assumption of \"disjoint\", why do we still bother with the unlabeled data? Why do not generate auxiliary data around but separable from ID?\n\n* The theoretical guidance is not that clear: Are the two theorems practically useful and how do the authors capitalize on these theorems in the experiments? In particular, as per Line 2, Page 5, what kind of \"appropriate threshold\" is used? Did the authors use the value of the left-hand side of the inequality in (4)?"
                },
                "questions": {
                    "value": "* In Algorithm 2, since there are no \"easily misclassified\" examples with known labels like the validation data in Algorithm 1, why do not just train the model based on $\\mathcal{L}\\_{xent}+\\lambda\\mathcal{L}\\_{conf}$? In other words, is there any necessity for the prior step to optimize $\\mathcal{L}\\_{xent}$?\n\n* Equation (9) and the comment \"resulting in a mixture between the true label distribution $p$ and the uniform distribution $\\mathcal{U}$, with mixture weight $\\lambda$\" after Proposition 4.2: Is this rearrangement (9) correct? Since $\\mathcal{P}\\_{u}=\\alpha_{test}\\cdot\\mathcal{P}\\_{ID} + (1-\\alpha_{test})\\cdot\\mathcal{P}\\_{OOD}$, why is the mixture weight just $\\lambda$, wouldn't there be an extra factor $\\alpha_{test}$?\n\n* $\\epsilon$ in Proposition 4.2 and the involved proof:  \n  * The exact value of $\\epsilon$ depends on the model performance or $\\mathcal{L}(\\theta)$, how can we allow $\\epsilon\\leq\\frac{1}{2N}(\\frac{M-1}{(1+\\lambda)M})^2$ to conclude the inequality (15)? \n  * What is $M$ in (15)?\n\n* Proposition A.1: I think the dimension of $p$ is $C+1$ when it comes to OOD detection (as the authors mentioned $p$ is the true label distribution). However, the dimension of $s$ is $C$ as the authors explicitly showed. Then is that legitimate for the expression $s-p$ and $s-\\frac{\\mathbf{1}}{C}$ in (6)?\n\n* Other minor issues: \n  * Please consistently add a comma after \"i.e.\"\n  * What is (5) used for?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6830/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698420557257,
            "cdate": 1698420557257,
            "tmdate": 1699636790251,
            "mdate": 1699636790251,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JlWIYy6Sdu",
                "forum": "HKV45Y0rFz",
                "replyto": "OWMWvV7lf3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6830/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6830/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Initial Response to Reviewer QVJM [1/4]"
                    },
                    "comment": {
                        "value": "Thank you for your feedback. We address your comments below. Please let us know if our response addresses all of your concerns.\n\n> The theoretical guidance is not that clear: Are the two theorems practically useful and how do the authors capitalize on these theorems in the experiments? \n\nYes, the two theorems are practically useful. We explain their practical implications in the last part of Section 4. They show that it is important to use the right uncertainty dataset, one that includes examples close to the unknown examples that the model may encounter at test time.\n\nIn our experiments (Section 6), we capitalize on these findings by using the following uncertainty datasets: (1) unlabeled ID and OOD examples for OOD detection, and (2) misclassified validation examples for selective classification. These choices are validated by our results. We hope this clarification helps.\n\n> In particular, as per Line 2, Page 5, what kind of \"appropriate threshold\" is used? Did the authors use the value of the left-hand side of the inequality in (4)?\n\nOur empirical evaluation considers different thresholds of confidence, as is standard in prior selective classification works [7,8,9,10,11]: Acc@90 measures with a threshold such that the model predicts on 90% of the data, AUC is an aggregate metric that considers all thresholds, etc. \n\nYes, we use a version of the LHS of (4), since all of our evaluation is based on the interplay between confidence (=MSP) and accuracy.\n\n> Practicality of the assumption that known and unknown datasets are disjoint. The unknown/misclassified points (I follow your definition of \"unknown\") in selective classification could be those hard points from some classes that intrinsically overlap with each other, then \"known\" and \"unknown\" are not disjoint. In OOD detection, now that we have the assumption of \"disjoint\", why do we still bother with the unlabeled data? Why do not generate auxiliary data around but separable from ID?\n\nFor selective classification: even though the known and unknown examples come from the same classes, they can be disjoint partitions of the overall distribution.\nFor OOD detection: it\u2019s not that any disjoint distribution works equally well; confidence minimization performs best when the uncertainty set is, in addition to being disjoint from the known data, close to the hard examples that the model may see at test time. Being separate from ID by itself does not make a dataset a useful uncertainty dataset.\n\nMoreover, there are prior work that use synthetic outlier data: VOS [4], NPOS [5] and Dream-OOD [6]. We have provided comparisons to these methods on CIFAR-10 and CIFAR-100 as OOD datasets. Our method outperforms them in most cases, showing the efficacy of using the unlabeled auxiliary set.\n\n| ID Dataset / Network | Method | SVHN | | LSUN (Crop) | | iSUN | | Texture | | Places365 | |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n| | | AUROC (\u2191) | FPR@95 (\u2193) | AUROC (\u2191) | FPR@95 (\u2193) | AUROC (\u2191) | FPR@95 (\u2193) | AUROC (\u2191) | FPR@95 (\u2193) | AUROC (\u2191) | FPR@95 (\u2193) |\n| CIFAR-10 /ResNet-18 | | | | | | | | | | |\n|| VOS | 96.37 | 15.69 | 93.82 | 27.64 | 94.87 | 30.42 | 93.68 | 32.68 | 91.78 | 37.95 |\n|| NPOS | 97.64 | 5.61 | 97.52 | 4.08 | 94.92 | 14.13 | 94.67 | 8.39 | 91.35 | 18.57 |\n|| DCM-Softmax |  99.7 (0.1) | 0.4 (0.3) | 98.6 (0.8) | 6.6 (3.0) | 99.7 (0.1) | 0.6 (0.2) | 97.1 (0.2) | 14.8 (0.3) | 92.4 (0.3) | 32.6 (2.1) |\n|| DCM-MaxLogit |  99.8 (0.1) | 0.3 (0.1) | 98.7 (0.7) | 6.0 (3.0) | 99.8 (0.1) | 0.5 (0.2) | 97.1 (0.2) | 14.9 (0.4) | 92.5 (0.3) | 34.4 (2.1) |\n|| DCM-Energy | 99.8 (0.1) | 0.1 (0.1) | 98.8 (0.7) | 5.3 (3.6) | 99.8 (0.1) | 0.1 (0.1) | 97.1 (0.2) | 16.1 (1.1) | 92.5 (0.3) | 35.6 (2.2) |\n| CIFAR-100 /ResNet-34 | | | | | | | | | | |\n|| VOS |  73.11 | 78.50 | 85.72 | 59.05 | 82.66 | 72.45 | 80.08 | 75.35 | 75.85 | 84.55 |\n|| NPOS |  97.84 | 11.14 | 82.43 | 56.27 | 85.48 | 51.72 | 92.44 | 35.20 | 71.30 | 79.08 |\n|| Dream-OOD | 87.01 | 58.75 | 95.23 | 24.25 | 99.73 | 1.10 | 88.82 | 46.60 | 79.94 | 70.85 |\n|| DCM-Softmax |  99.3 (0.2) | 1.8 (0.9) | 98.6 (0.3) | 8.7 (1.9) | 99.3 (0.2) | 2.3 (1.4) | 88.5 (0.5) | 46.6 (2.7) | 78.6 (0.4) | 67.7 (2.3) |\n|| DCM-MaxLogit |  99.3 (0.2) | 2.0 (1.1) | 98.7 (0.2) | 7.3 (1.9) | 99.3 (0.2) | 2.3 (1.4) | 88.5 (0.5) | 46.9 (3.0) | 78.6 (0.4) | 68.8 (1.8) |\n|| DCM-Energy |  99.2 (0.2) | 2.2 (1.2) | 98.9 (0.2) | 5.2 (2.0) | 99.4 (0.2) | 1.9 (0.8) | 89.3 (0.6) | 48.6 (3.3) | 78.3 (0.6) | 68.9 (1.9)|"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6830/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700450393757,
                "cdate": 1700450393757,
                "tmdate": 1700488625744,
                "mdate": 1700488625744,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "E6TGpxcWKj",
                "forum": "HKV45Y0rFz",
                "replyto": "OWMWvV7lf3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6830/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6830/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Initial Response to Reviewer QVJM [2/4]"
                    },
                    "comment": {
                        "value": "Additionally, DCM significantly outperforms VOS [4] and NPOS [5] with larger datasets and model architectures. We ran additional OOD detection experiments using a pretrained CLIP ViT-B/16 with ImageNet-1K as ID.\n\n| Methods | iNaturalist |  |  SUN | | Places | | Textures | |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n|  | FPR@95 (\u2193) | AUROC (\u2191) | FPR@95 (\u2193) | AUROC (\u2191) | FPR@95 (\u2193) | AUROC (\u2191) | FPR@95 (\u2193) | AUROC (\u2191) |\n| Fort et al.,MSP | 54.05 | 87.43 | 73.37 | 78.03 | 72.98 | 78.03 | 68.85 | 79.06 |\n| VOS | 31.65 | 94.53 | 43.03 | 91.92 | 41.62 | 90.23 | 56.67 | 86.74 |\n| VOS+ | 28.99 | 94.62 | 36.88 | 92.57 | 38.39 | 91.23 | 61.02 | 86.33 |\n| NPOS | 16.58 | 96.19 | 43.77 | 90.44 | 45.27 | 89.44 | 46.12 | 88.80 |\n| DCM-Softmax | 2.6 (0.5) | 99.2 (0.1) | 32.9 (1.5) | 94.2 (0.2) | 35.9 (1.8) | 93.8 (0.3) | 11.2 (1.0) | 97.9 (0.1) |\n| DCM-MaxLogit | 1.8 (0.4) | 99.4 (0.1) | 27.5 (1.4) | 94.9 (0.2) | **32.5 (2.8)** | 94.5 (0.3) | 8.2 (0.8) | 98.3 (0.1) |\n| DCM-Energy | **0.5 (0.2)** | **99.6 (0.1)** | **24.5 (1.7)** | **95.8 (0.2)** | **30.8 (3.0)** | **95.4 (0.3)** | **4.3 (0.6)** | **98.8 (0.1)** |\n\n> I am confused about the definition of \"unknown\" in selective classification.\n\n\u201cUnknown\u201d is a term we use to talk about the two problem settings in a unified way: (1) for OOD detection, it is the OOD data, and (2) for selective classification, it is misclassified validation set examples. For selective classification, misclassified examples are \"unknown\" to the model because they indicate a gap in the model's knowledge due to insufficient coverage of the training distribution.\n\n\n> Proposition 4.1 and 4.2 then sounds targeting OOD detection with the conclusion \"DCM provably detects unknown examples\". Here I presume that \"detects unknown examples\" solely means \"detects OOD examples\", excluding the \"unknown\" examples in the selective classification task.\n\nBy \u201cdetection\u201d, we mean that the confidence values of a model trained with the DCM loss is high for known examples and low for unknown examples. This property is relevant to both the OOD detection and selective classification problem settings.\n\n\n> The authors need to well-articulate the notations before these are used. For examples,\nWhat is $P_{ID}$? It is not friendly for readers not familiar with this topic.\n\n$P_{ID}$ represents the distribution of in-distribution, or \u201cknown\u201d, examples. It is the distribution of the training data. We\u2019ve revised Section 2.2 to introduce this notation.\n\n\n> Please add the appendix reference for the definition of $\\delta$-neighborhoods in the comment after Proposition 4.1.\n\nThank you for the suggestion; we have added a pointer to the appendix when $\\delta$-neighborhoods are first introduced in the main text.\n\n>What does $i$ in (4) stand for?\n\n$i$ was not needed in (4) and the proof; this was something we wrote in an earlier version of the proposition but forgot to delete. Thank you for catching this!\n\n> The uniform label distribution used for \"unknown\" observations is ad-hoc. It sounds like you are assuming all unknown examples overlap together and hence you cannot clearly distinguish them. However, what if there are only several overlapped classes? For example, some unknown observations from class 1 overlap with class 2, but they are disjoint with other classes. In this case, it is not appropriate to give non-zero probability to generate other \u201cpseudo\u201d labels for these unknown examples from class 1. Moreover, if this unknown example is an OOD point that is unlike any of the \"known\" classes, does it still make sense to give a non-zero probability to be labeled as \"known\" classes?\n\nWe respectfully disagree with the claim that the uniform label distribution is ad-hoc. While we agree that it is possible for classes to have fine-grained relationships as the reviewer mentions, we are not assuming such additional knowledge. The uniform label distribution is the maximum entropy distribution among categorical distributions, and thus, regularizing towards it is a natural choice for increasing uncertainty. We note that regularizing towards the uniform distribution is a standard choice that has been successful in several prior works [1,2,3].\n\n> What is (5) used for?\n\n(5) in the appendix states the DCM objective before we move on to the propositions and proofs. It is implicitly used in all later equations that involve the loss (6, 9, 10\u2026)"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6830/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700450453454,
                "cdate": 1700450453454,
                "tmdate": 1700488710212,
                "mdate": 1700488710212,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lI3DiczOo5",
                "forum": "HKV45Y0rFz",
                "replyto": "OWMWvV7lf3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6830/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6830/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Initial Response to Reviewer QVJM [3/4]"
                    },
                    "comment": {
                        "value": "> In Algorithm 2, since there are no \"easily misclassified\" examples with known labels like the validation data in Algorithm 1, why do not just train the model based on $L_{xent} + \\lambda L_{conf}$? In other words, is there any necessity for the prior step to optimize $L_{xent}$?\n\nThe primary reason we use only $L_{xent}$ is computational efficiency: the fine-tuning step is much shorter than the pre-training steps, and the ratio of compute required becomes smaller when we use larger and larger datasets. When we train one epoch using $L_{xent} + \\lambda L_{conf}$, the model essentially sees 2x the number of images compared to training using $L_{xent}$ only and hence takes 2x compute, since it sees equal number of images from the training and auxiliary datasets. Also this lets us take one ID pre-trained model and adapt it relatively quickly to different test distributions, avoiding the costly pre-training process for each different test distribution. We note that our method is resonant with the philosophy of using large pre-trained foundation models and fine-tuning them for individual downstream tasks [12].\nIndeed, as the reviewer notes, it is possible to train the model based on $L_{xent} + \\lambda L_{conf}$. We have added experiment results in Appendix K for this: in general, we see that training directly from scratch using $L_{xent} + \\lambda L_{conf}$ leads to slightly lower ID accuracy but comparable performance on OOD detection tasks. This is possibly because the cross-entropy loss on ID training set and confidence minimization loss on the ID examples within the unlabeled uncertainty set work in opposite directions, making learning the ID classification task harder. Whereas if we fine-tune an ID pre-trained model, the model already has learned the ID task, and the fine-tuning step only further modifies the decision boundary, maintaining higher ID performance.\n\n\n| ID Dataset | OOD Dataset    | Classification Accuracy |       | OOD Detection AUROC |      | OOD Detection FPR@95 |      |\n|------------|----------------|----------------|----------------|----------------|----------------|----------------|----------------|\n|            |                | Fine-tune               | Pre-train | Fine-tune           | Pre-train | Fine-tune            | Pre-train |\n| CIFAR-10   | SVHN           | 90.6                    | 93.7  | 99.7                | 99.7 | 0.8                  | 0.4  |\n|            | TinyImageNet   | 90.4                    | 93.5  | 99.3                | 99.3 | 2.2                  | 2.6  |\n|            | LSUN           | 90.6                    | 93.7  | 99.2                | 99.8 | 2.5                  | 0.5  |\n|            | iSUN           | 90.2                    | 93.5  | 99.5                | 99.7 | 1.4                  | 0.6  |\n| CIFAR-100  | SVHN           | 69.3                    | 71.4  | 99.6                | 99.6 | 0.4                  | 0.6  |\n|            | TinyImageNet   | 68.1                    | 71.1  | 99.0                | 98.7 | 3.2                  | 5.9  |\n|            | LSUN           | 69.0                    | 71.0  | 99.7                | 99.5 | 0.7                  | 1.1  |\n|            | iSUN           | 68.1                    | 71.2  | 99.4                | 99.1 | 2.0                  | 2.7  |\n> Equation (9) and the comment \"resulting in a mixture between the true label distribution $p$ and the uniform distribution $U$, with mixture weight $\\lambda$\" after Proposition 4.2: Is this rearrangement (9) correct? Since $P_u = \\alpha_{test} P_{ID} + (1 - \\alpha_{test})P_{OOD}$, why is the mixture weight just $\\lambda$, wouldn't there be an extra factor $\\alpha_{test}$?\n\nIn our theoretical setup, we have assumed $\\alpha_{test} = \\frac{1}{2}$ for simplicity, which is mentioned at the start of Appendix A.1:\n\nLet $D_u$ be an unlabeled test set where half the examples are sampled from $P_{ID}$, the other half are sampled from $P_{OOD}$.\n\nLemma A.3 also deals with the simplified transductive setup (which is generalized in proposition A.4), where the ID examples in $D_u$ also appear in $D_{train}$. However, the more general version can be proved similarly, assuming $\\alpha_{test}$ is absorbed into the constant $\\lambda$.\n\n>  $\\epsilon$ in Proposition 4.2 and the involved proof: The exact value of $\\epsilon$ depends on the model performance or $L(\\theta)$, how can we allow $\\epsilon \\leq \\frac{1}{2N} \\left(\\frac{M - 1}{(1 + \\lambda)M}\\right)^2$ to conclude the inequality (15)?\n\nThe reviewer\u2019s statement, \u201cthe exact value of $\\epsilon$ depends on the model performance\u2026\u201d, is incorrect. $\\epsilon$ is a free variable, and the lemma claims that there exists _some_ $\\epsilon$ such that (8) is true.\n\nThis is a standard trick in proving inequalities. We know that (14) holds for _all_ $\\epsilon > 0$. We can set $\\epsilon$ to any positive value; by setting it to a value lower than $\\frac{1}{2N} \\left(\\frac{M - 1}{(1 + \\lambda)M}\\right)^2$, we get equation 15."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6830/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700450491147,
                "cdate": 1700450491147,
                "tmdate": 1700450936901,
                "mdate": 1700450936901,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Np9barF3SV",
                "forum": "HKV45Y0rFz",
                "replyto": "OWMWvV7lf3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6830/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6830/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Initial Response to Reviewer QVJM [4/4]"
                    },
                    "comment": {
                        "value": "> What is $M$ in (15)?\n\n$M$ refers to the number of classes. We realize that this was a duplicate notation; $M$ was used in an earlier version, and we did not revise it. We have replaced $M$ with $C$ in the text.\n\n> Proposition A.1: I think the dimension of $p$ is $C + 1$ when it comes to OOD detection (as the authors mentioned $p$ is the true label distribution). However, the dimension of $s$ is $C$ as the authors explicitly showed. Then is that legitimate for the expression $s - p$ and $s - \\frac{1}{C}$ in (6)\n\nThe dimensionality of $p$ is always $C$, as Appendix A states throughout. Our analysis operates in a setting where $p$ assigns a $C$-way categorical distribution to all possible inputs, including OOD ones. Note that this is consistent with the evaluation of ours and many prior OOD detection papers, which uses the confidence of a $C$-way prediction to separate ID inputs from OOD ones.\n\n> Please consistently add a comma after \"i.e.\"\n\nThanks for the suggestion: we\u2019ve added commas after all instances of \u201ci.e.\u201d in the revised version (in red text).\n\n[1] \"Deep anomaly detection with outlier exposure.\" ICLR 2019\n\n[2] \"When does label smoothing help?.\" NeurIPS 2019\n\n[3] \"Conservative q-learning for offline reinforcement learning.\" NeurIPS 2020\n\n[4] VOS: Learning What You Don't Know by Virtual Outlier Synthesis, ICLR 2022\n\n[5] Non-parametric Outlier Synthesis, ICLR 2023\n\n[6] Dream the Impossible: Outlier Imagination with Diffusion Models, NeurIPS 2023\n\n[7] \"Selective Classification for Deep Neural Networks.\" NeurIPS 2017\n\n[8] \"A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks.\" ICLR 2018\n\n[9] \"Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks.\" ICLR 2019\n\n[10] \"Deep Gamblers: Learning to Abstain with Portfolio Theory.\" NeurIPS 2019\n\n[11] \"Self-Adaptive Training: beyond Empirical Risk Minimization.\" NeurIPS 2020\n\n[12] On the Opportunities and Risks of Foundation Models,"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6830/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700450530232,
                "cdate": 1700450530232,
                "tmdate": 1700450637595,
                "mdate": 1700450637595,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Zyq9lxw8KS",
                "forum": "HKV45Y0rFz",
                "replyto": "OWMWvV7lf3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6830/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6830/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Checking in"
                    },
                    "comment": {
                        "value": "We wanted to follow up to see if the response and revisions address your concerns. We are open to discussion if you have any additional questions or concerns, and if not, we kindly ask you to reevaluate your score. Thank you again for your reviews which helped to improve our paper!"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6830/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700597112463,
                "cdate": 1700597112463,
                "tmdate": 1700597112463,
                "mdate": 1700597112463,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iomulftmXZ",
                "forum": "HKV45Y0rFz",
                "replyto": "OWMWvV7lf3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6830/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6830/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Checking in to see if we have addressed reviewer concerns"
                    },
                    "comment": {
                        "value": "Since the discussion period is ending soon, we wanted to check if you had any feedback based on our response. Specifically, let us know if we have addressed your concerns sufficiently. If you have any questions/suggestions based on our response, we are happy to discuss them.\n\nThanks for putting thoughts into our paper and your valuable insights."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6830/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700684039257,
                "cdate": 1700684039257,
                "tmdate": 1700684234433,
                "mdate": 1700684234433,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jTpZCfABiW",
                "forum": "HKV45Y0rFz",
                "replyto": "iomulftmXZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6830/Reviewer_QVJM"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6830/Reviewer_QVJM"
                ],
                "content": {
                    "comment": {
                        "value": "Sorry for the delayed response. I really appreciate the authors' effort and provide more empirical results. I still have the below concerns.\n\n* The assumption of \u201cdisjoint sets\u201d does not convince me. In selective classification (some works of literature call it rejection) [1], those hard observations could intrinsically overlap within many classes. If they are disjoint, I do not think there is a necessity to bother selective classification since you have a clear separation to distinguish those classes. Even in OOD detection, the OOD class could be near-OOD [2] which is hard to detect.\n\n* \u201cThis property is relevant to both the OOD detection and selective classification problem settings.\u201d By following your notation of $D_u$, which is exclusively introduced and used in sec. 3.2 (OOD detection), I get confused as to why this property holds for both tasks. \n\n* \u201cuniform label distribution\u201d:  uniform label distribution assumption theoretically helps to yield the conclusion like proposition 4.1. But it may sabotage other nice properties since you equally treat every class without explicitly using potential information conveyed, like by x.\n\n* $\\alpha_{test}=1/2$: This is another biggest concern in my mind. The authors didn\u2019t clearly disclose it in the main article, which may mislead readers to ignore this key parameter. Moreover, your loss function and the follow-up work based on this big assumption make the work convenient. Without this assumption, does the theorem still hold? If not, what is the alternative or general theorem? Moreover, does this assumption work well when the ground truth of $\\alpha_{test}\\neq \\frac{1}{2}$? How about the sensitivity analysis of this parameter?\n\n* I do not agree with your statement on $\\varepsilon$. On one hand, you say there \u201cexists\u201d $\\epsilon>0$. on the other hand, you say $\\epsilon $ is a free variable and \u201c setting it to a value lower than \u2026.\u201d. First of all, we do not know if this kind of $\\epsilon$ exists or not (you didn\u2019t show it). Second, suppose there exists this kind of $\\epsilon$, how could we let the existing value be lower than the value we want?\n\n* \u201cThe dimension of $p$\u201d: Your presentation \u201cLet $\\mathcal{P}_{ID}$ be a distribution over $\\mathcal{X}\\times\\\\{1,\\cdots, C\\\\}\\subset\\mathcal{X}\\times\\mathcal{Y}$ i.e., there are $C$ classes\u201d indicates $C$ is the number of known/inlier classes. Now when it comes to OOD detection, there is an extra label for the OOD class. Then how could the label distribution $p$ have the same dimension as the number of inlier classes? \n\n[1] Gangrade, Aditya, Anil Kag, and Venkatesh Saligrama. \"Selective classification via one-sided prediction.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2021.\n\n[2] Fort, Stanislav, Jie Ren, and Balaji Lakshminarayanan. \"Exploring the limits of out-of-distribution detection.\" Advances in Neural Information Processing Systems 34 (2021): 7068-7081."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6830/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700713864242,
                "cdate": 1700713864242,
                "tmdate": 1700713864242,
                "mdate": 1700713864242,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "e7o8Bp3R46",
                "forum": "HKV45Y0rFz",
                "replyto": "OWMWvV7lf3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6830/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6830/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer QVJM (2/2)"
                    },
                    "comment": {
                        "value": "> \u201cThe dimension of $p$\u201d: Your presentation \u201cLet $P_{ID}$ be a distribution over $\\mathcal{X} \\times \\{1, \\ldots, C\\} \\subset \\mathcal{X} \\times \\mathcal{Y}$ i.e., there are $C$ classes\u201d indicates $C$ is the number of known/inlier classes. Now when it comes to OOD detection, there is an extra label for the OOD class. Then how could the label distribution $p$ have the same dimension as the number of inlier classes?\n\nMost of the OOD detection literature does not consider a separate C+1-th class. Instead, the goal is to have the predictions be close to uniform on OOD inputs. This is a standard choice in the OOD detection literature [1,2,3] and has the advantage of not needing architectural modifications. Our work follows this convention.\n\n[1] \"Enhancing the reliability of out-of-distribution image detection in neural networks.\" arXiv preprint arXiv:1706.02690 (2017).\n\n[2] \"Deep anomaly detection with outlier exposure.\" ICLR 2019\n\n[3] \"A simple unified framework for detecting out-of-distribution samples and adversarial attacks.\" Advances in neural information processing systems 31 (2018)."
                    }
                },
                "number": 26,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6830/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700727915994,
                "cdate": 1700727915994,
                "tmdate": 1700727949477,
                "mdate": 1700727949477,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "tUWnu8qzYd",
            "forum": "HKV45Y0rFz",
            "replyto": "HKV45Y0rFz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6830/Reviewer_UGo2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6830/Reviewer_UGo2"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a data-driven method to penalize the over-confident prediction on unknown samples. Specifically, the authors suggest that auxiliary datasets that contain unknown samples should be mixed with the original training dataset to obtain a conservative prediction. In addition, the authors propose a two-stage training scheme. For the first stage, the model is trained with training data. Then auxiliary dataset is combined with the training data to train the model with a loss composed of cross-entropy loss and regularization. To further understand the training scheme, theoretical analysis is provided by the authors which suggests that the proposed method can get a prediction confidence always lower than the true confidence.\nAdditionally, according to the analysis, a known sample tends to be given larger confidence. \nTo verify the proposed method, extensive experiments are conducted. In detail, the method is validated with selective classification and OOD detection across several image classification datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper proposes to use an auxiliary dataset combined with a penalized loss function to reduce the confidence in unseen samples. To further understand the proposed method, the authors analyze the proposed method theoretically and get two reasonable interpretations of the proposed method. To validate the efficacy of the proposed method, several datasets are selected to conduct experiments with different counterpart methods. The proposed method shows good performance on selective classification as well as ood detection.  An ablation study of the component of the method is also given to further analyze the method. The authors also show us the prediction histogram to validate the proposition."
                },
                "weaknesses": {
                    "value": "It seems that the Figure 2 is not in correct order. \nBy observing Table1, we can find a performance drop on iid setting for relative simple datasets that can achieve classification accuracy more than 99\\%. And we can also find an enhancement in the ood setting. \nHowever on relatively hard setting like FMoW, the iid performance is enhanced by the performance, but the ood and iid+ood performance does not show a significant gap compared with other methods. \nTake the loss into consideration, I am wondering whether the key is to use a strong regularization on training set and the enhancement for ood is in sacrifice of performance drop of the iid setting. \nCould the authors show the result of adding a strong label smoothing or similar regularization during pretraining for more complete comparison?\nIn addition, could the author show the results of larger datasets? I am wondering whether the regularization still works as the classification task becomes harder."
                },
                "questions": {
                    "value": "Please refer to weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6830/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698758914677,
            "cdate": 1698758914677,
            "tmdate": 1699636790135,
            "mdate": 1699636790135,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yNVStWmN8B",
                "forum": "HKV45Y0rFz",
                "replyto": "tUWnu8qzYd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6830/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6830/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Initial Response to Reviewer UGo2"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful feedback. We address your comments below. Please let us know if you have any remaining questions or concerns.\n\n> It seems that the Figure 2 is not in correct order.\n\nThanks for catching this; we have fixed the caption and associated text in the paper related to this.\n\n> By observing Table1, we can find a performance drop on iid setting for relative simple datasets that can achieve classification accuracy more than 99%. And we can also find an enhancement in the ood setting. However on relatively hard setting like FMoW, the iid performance is enhanced by the performance, but the ood and iid+ood performance does not show a significant gap compared with other methods. \n\nFirst, there is an inherent tension between ID and OOD performance. Many interventions for making models more robust (in different senses of the word) improve OOD performance at the cost of a slight drop in ID performance.\nWe note that Camelyon17 is a bigger dataset than FMoW (455k vs 141k images) and involves a more severe drop in ID->OOD performance: the Acc@90 for MSP drops by 21.6% in Camelyon vs 7.4% in FMoW. In this challenging setting, we see substantial benefits over existing works. While other methods are also competitive on FMoW, DCM is consistently among the highest performing in all settings involving OOD data.\n\n> \u2018Could the authors show the result of adding a strong label smoothing or similar regularization during pretraining for more complete comparison?\u2019\n\nThank you for the suggestion. We ran additional experiments comparing pre-training with label smoothing for both selective classification and OOD detection. Adding label smoothing to pre-training does not enhance performance.\n\nBelow, we compare selective classification performance of DCM with an MSP classifier on FMoW pre-trained with varying degrees of label smoothing. \n\n(**Selective classification AUC on FMoW**)\n|             | 0      | 0.25   | 0.5    | 0.75   | 1      | DCM |\n|-------------|--------|--------|--------|--------|--------|-----------------|\n| ID          | *81.3* | 81.0   | 80.7   | 71.0   | 59.6   | **82.9**          |\n| ID+OOD      | *77.1* | 76.9   | 76.4   | 67.8   | 52.5   | **78.9**          |\n| OOD         | *74.5* | 74.3   | 74.2   | 64.0   | 55.1   | **76.4**          |\n\nWe also conducted experiments comparing pre-training with label smoothing and weight decay as forms of regularization for OOD detection, detailed in Appendix K of the revised paper. We present results for label smoothing with CIFAR-100 as the ID dataset below; pre-training with label smoothing does bridge the performance gap to DCM.\n\n(**OOD detection AUROC on CIFAR-100 as ID**)\n|             | 0      | 0.25   | 0.5    | 0.75   | 1      | DCM |\n|---------------|-----------------------|-----|-----|-----|-----|-------------|\n| SVHN          | 77.7                  | 74.9| 67.2| 76.4| 50.0| 99.7        |\n| LSUN          | 68.5                  | 56.8| 61.7| 63.3| 50.0| 99.5        |\n| TinyImageNet  | 68.0                  | 59.4| 62.4| 58.5| 50.0| 98.7        |\n| iSUN          | 67.1                  | 55.1| 61  | 59.2| 50.0| 99.1        |\n\nWe suspect that label smoothing during pre-training reduces the model\u2019s ability to differentiate between classes, degrading selective classification and OOD detection performance.\n\n> In addition, could the author show the results of larger datasets? I am wondering whether the regularization still works as the classification task becomes harder.\n\nThank you for the suggestion. We ran our method with a ViT-B/16 model pre-trained on ImageNet-1K, and tested on 4 large OOD datasets, including iNaturalist and Places365. Our method outperforms all baselines on all 4 OOD datasets by a large margin, including several strong recent approaches such as VOS [4] and NPOS [5].\n\n| Methods | iNaturalist |  |  SUN | | Places | | Textures | |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n|  | FPR@95 (\u2193) | AUROC (\u2191) | FPR@95 (\u2193) | AUROC (\u2191) | FPR@95 (\u2193) | AUROC (\u2191) | FPR@95 (\u2193) | AUROC (\u2191) |\n| Fort et al.,MSP | 54.05 | 87.43 | 73.37 | 78.03 | 72.98 | 78.03 | 68.85 | 79.06 |\n| VOS | 31.65 | 94.53 | 43.03 | 91.92 | 41.62 | 90.23 | 56.67 | 86.74 |\n| VOS+ | 28.99 | 94.62 | 36.88 | 92.57 | 38.39 | 91.23 | 61.02 | 86.33 |\n| NPOS | 16.58 | 96.19 | 43.77 | 90.44 | 45.27 | 89.44 | 46.12 | 88.80 |\n| DCM-Softmax | 2.6 (0.5) | 99.2 (0.1) | 32.9 (1.5) | 94.2 (0.2) | 35.9 (1.8) | 93.8 (0.3) | 11.2 (1.0) | 97.9 (0.1) |\n| DCM-MaxLogit | 1.8 (0.4) | 99.4 (0.1) | 27.5 (1.4) | 94.9 (0.2) | **32.5 (2.8)** | 94.5 (0.3) | 8.2 (0.8) | 98.3 (0.1) |\n| DCM-Energy | **0.5 (0.2)** | **99.6 (0.1)** | **24.5 (1.7)** | **95.8 (0.2)** | **30.8 (3.0)** | **95.4 (0.3)** | **4.3 (0.6)** | **98.8 (0.1)** |\n\nWe also note that our evaluations for selective classification included two sizable datasets: Camelyon-17 (around 450,000 examples), and FMoW (around 150,000 examples, 63 classes)."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6830/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700450334548,
                "cdate": 1700450334548,
                "tmdate": 1700488726041,
                "mdate": 1700488726041,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "stdF2rkVAY",
                "forum": "HKV45Y0rFz",
                "replyto": "tUWnu8qzYd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6830/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6830/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Checking in"
                    },
                    "comment": {
                        "value": "We wanted to follow up to see if the response and revisions address your concerns. We are open to discussion if you have any additional questions or concerns, and if not, we kindly ask you to reevaluate your score. Thank you again for your reviews which helped to improve our paper!"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6830/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700597097540,
                "cdate": 1700597097540,
                "tmdate": 1700597097540,
                "mdate": 1700597097540,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4likxY4Fxp",
                "forum": "HKV45Y0rFz",
                "replyto": "m62k0ZmCZR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6830/Reviewer_UGo2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6830/Reviewer_UGo2"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the response, but I will keep my score."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6830/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700725976697,
                "cdate": 1700725976697,
                "tmdate": 1700725976697,
                "mdate": 1700725976697,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AWPiPpf9gq",
            "forum": "HKV45Y0rFz",
            "replyto": "HKV45Y0rFz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6830/Reviewer_bNMd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6830/Reviewer_bNMd"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes the Data-Driven Confidence Minimization (DCM) framework for detecting unknown inputs in safety-critical machine learning applications. By minimizing model confidence on an uncertainty dataset, DCM achieves provable detection of unknown test examples. Experimental results demonstrate that DCM outperforms existing approaches in selective classification and out-of-distribution detection tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Overall I think the paper is well motivated and well written. The proposed method is well motivated by the theretical analysis and the empirical performance is convincing."
                },
                "weaknesses": {
                    "value": "* For the theretical part, what can we say when the following does not hold \"(1) if the auxiliary set contains\nunknown examples similar to those seen at test time, confidence minimization leads to provable detection of unknown test examples\". Specifically, what if the auxiliary set DOES NOT contain unknown examples similar to those seen at test time. It is important to know the theretical property in this case.\n\n* The experiment results are shown in CIFAR. I would be more interested in experiments on larger scale datasets with foundation models. For example, CLIP on ImageNet. Nowadays, the interest of the community has shifted to foundation models. I believe the paper can benefit from this aspect."
                },
                "questions": {
                    "value": "See weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6830/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6830/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6830/Reviewer_bNMd"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6830/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698818596288,
            "cdate": 1698818596288,
            "tmdate": 1699636789999,
            "mdate": 1699636789999,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Dfr5fWey7f",
                "forum": "HKV45Y0rFz",
                "replyto": "AWPiPpf9gq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6830/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6830/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Initial Response to Reviewer bNMd"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful feedback. We address your comments below. Please let us know if you have any remaining questions or concerns.\n\n> For the thoretical part, what can we say when the following does not hold \"(1) if the auxiliary set contains unknown examples similar to those seen at test time, confidence minimization leads to provable detection of unknown test examples\". Specifically, what if the auxiliary set DOES NOT contain unknown examples similar to those seen at test time. It is important to know the thoretical property in this case.\n\nIn the absence of any assumptions about how the auxiliary dataset relates to the test distribution, we cannot make any theoretical claims about the effect of confidence minimization on test inputs. In this scenario, Proposition A.1 still informs us about the resulting model\u2019s behavior on the auxiliary data distribution: DCM will make the model more conservative in the sense that the predictive probability will be upper-bounded by the true probability distribution. How this affects the test distribution is a matter of generalization from auxiliary->test, which we would need further assumptions to study. We note that empirically, unrelated auxiliary distributions still help in making the model more overall conservative [1,2], though not as much as more relevant auxiliary data as in ours.\n\n> \u2018The experiment results are shown in CIFAR\u2026\u2019\n\nWe thank the reviewer for their suggestions. We have run DCM on a ViT-B/16 model pre-trained on ImageNet-1K, and tested on 4 large OOD datasets, including iNaturalist and Places365. Our method outperforms all baselines on all 4 OOD datasets, including recent baselines such as VOS [3] and NPOS [4], by a large margin.\n\n| Methods | iNaturalist |  |  SUN | | Places | | Textures | |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n|  | FPR@95 (\u2193) | AUROC (\u2191) | FPR@95 (\u2193) | AUROC (\u2191) | FPR@95 (\u2193) | AUROC (\u2191) | FPR@95 (\u2193) | AUROC (\u2191) |\n| Fort et al.,MSP | 54.05 | 87.43 | 73.37 | 78.03 | 72.98 | 78.03 | 68.85 | 79.06 |\n| VOS | 31.65 | 94.53 | 43.03 | 91.92 | 41.62 | 90.23 | 56.67 | 86.74 |\n| VOS+ | 28.99 | 94.62 | 36.88 | 92.57 | 38.39 | 91.23 | 61.02 | 86.33 |\n| NPOS | 16.58 | 96.19 | 43.77 | 90.44 | 45.27 | 89.44 | 46.12 | 88.80 |\n| DCM-Softmax | 2.6 (0.5) | 99.2 (0.1) | 32.9 (1.5) | 94.2 (0.2) | 35.9 (1.8) | 93.8 (0.3) | 11.2 (1.0) | 97.9 (0.1) |\n| DCM-MaxLogit | 1.8 (0.4) | 99.4 (0.1) | 27.5 (1.4) | 94.9 (0.2) | **32.5 (2.8)** | 94.5 (0.3) | 8.2 (0.8) | 98.3 (0.1) |\n| DCM-Energy | **0.5 (0.2)** | **99.6 (0.1)** | **24.5 (1.7)** | **95.8 (0.2)** | **30.8 (3.0)** | **95.4 (0.3)** | **4.3 (0.6)** | **98.8 (0.1)** |\n\n[1] Deep Anomaly Detection with Outlier Exposure, ICLR 2019\n\n[2] Energy-based Out-of-distribution Detection, NeurIPS 2020\n\n[3] VOS: Learning What You Don't Know by Virtual Outlier Synthesis, ICLR 2022\n\n[4] Non-parametric Outlier Synthesis, ICLR 2023"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6830/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700450298664,
                "cdate": 1700450298664,
                "tmdate": 1700488742000,
                "mdate": 1700488742000,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aYKTqFk0MW",
                "forum": "HKV45Y0rFz",
                "replyto": "AWPiPpf9gq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6830/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6830/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Checking in"
                    },
                    "comment": {
                        "value": "We wanted to follow up to see if the response and revisions address your concerns. We are open to discussion if you have any additional questions or concerns, and if not, we kindly ask you to reevaluate your score. Thank you again for your reviews which helped to improve our paper!"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6830/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700597081796,
                "cdate": 1700597081796,
                "tmdate": 1700597081796,
                "mdate": 1700597081796,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4a2IuZrlWh",
                "forum": "HKV45Y0rFz",
                "replyto": "QCLBFFszLz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6830/Reviewer_bNMd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6830/Reviewer_bNMd"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed response."
                    }
                },
                "number": 28,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6830/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700729407185,
                "cdate": 1700729407185,
                "tmdate": 1700729407185,
                "mdate": 1700729407185,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FBuKM5jezt",
            "forum": "HKV45Y0rFz",
            "replyto": "HKV45Y0rFz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6830/Reviewer_sVZ1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6830/Reviewer_sVZ1"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method called Data-Driven Confidence Minimization (DCM) for OOD detection and selective classification (i.e., a reject option). The method builds on Outlier Exposure, using different uncertainty datasets. For selective classification, the uncertainty dataset is misclassified examples in a val set. For OOD detection, the uncertainty dataset is a potential mixture of in-distribution and OOD data. The paper includes a proof that having a noisy uncertainty dataset in this manner still allows for separating ID and OOD examples. In experiments, DCM performs well compared to several OOD detection baselines including OE."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Good empirical results\n- The paper is well-written and easy to follow"
                },
                "weaknesses": {
                    "value": "- There isn't much technical novelty on top of OE. The method is mainly about selecting a new uncertainty dataset, which is a fine direction to explore, but the approach is technically simple and possible not substantial enough for ICLR.\n\n- The proof seems fairly obvious; it seems to be saying that datasets are separable even when the training data are noisy. I may have missed some details, but surely this is already well-known and a standard result in learning theory. I'm worried that this proof doesn't contribute new knowledge to the field and may give rise to a false impression.\n\n- There are numerous more recent baselines, e.g., Virtual Outlier Synthesis. It would be good to include some of these."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6830/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699311908888,
            "cdate": 1699311908888,
            "tmdate": 1699636789892,
            "mdate": 1699636789892,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Hi7KfXmgui",
                "forum": "HKV45Y0rFz",
                "replyto": "FBuKM5jezt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6830/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6830/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Initial Response to Reviewer sVZ1 [1/2]"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful feedback. We address your comments below. Please let us know if you have any remaining questions or concerns.\n\n> \u2018There isn't much technical novelty on top of OE\u2026\u2019\n\nWhile our method is similar to OE in that both methods minimize confidence, we note the following key differences between our work and OE:\nWe analyze the role of the uncertainty set both theoretically and empirically.\nWe extend this confidence minimization framework to selective classification, which prior works such as OE do not explore.\nOur method outperforms prior works that share our data assumptions (WOODS, ICML 2022 [1]; ERD, UAI 2022 [2]) while being simpler and computationally cheaper to run. Our method requires fewer algorithm-specific hyperparameters than WOODS [1] (1 for DCM, 8 for WOODS) and 5x reduction in compute compared to the ensemble-based ERD [2].\n\n> The proof seems fairly obvious; it seems to be saying that datasets are separable even when the training data are noisy. I may have missed some details, but surely this is already well-known and a standard result in learning theory. I'm worried that this proof doesn't contribute new knowledge to the field and may give rise to a false impression.\n\nWe believe that our analysis establishes a novel and useful result in the context of using auxiliary datasets to minimize confidence. \nWe agree that the proof technique itself is straightforward; we are not claiming that the proof itself constitutes a theoretical advance. However, its application in the realm of using auxiliary datasets for confidence minimization is a novel contribution. We welcome any suggestions for relevant prior work to compare and cite. To our knowledge, our analysis in the context of conservative prediction hasn't been explored in existing literature.\n\n> \u2018There are numerous more recent baselines\u2026\u2019\n\nThank you for the suggestions. \n\nWe added comparisons to VOS [3], NPOS [4] and Dream-OOD [5] on CIFAR-10 and CIFAR-100 to Appendix H of our revised paper. DCM outperforms these methods by 2.5% on CIFAR-10 and 2.9% on CIFAR-100, in terms of OOD detection AUROC, averaged over 5 OOD datasets each.\n\n| ID Dataset / Network | Method | SVHN | | LSUN (Crop) | | iSUN | | Texture | | Places365 | |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n| | | AUROC (\u2191) | FPR@95 (\u2193) | AUROC (\u2191) | FPR@95 (\u2193) | AUROC (\u2191) | FPR@95 (\u2193) | AUROC (\u2191) | FPR@95 (\u2193) | AUROC (\u2191) | FPR@95 (\u2193) |\n| CIFAR-10 /ResNet-18 | | | | | | | | | | |\n|| VOS | 96.37 | 15.69 | 93.82 | 27.64 | 94.87 | 30.42 | 93.68 | 32.68 | 91.78 | 37.95 |\n|| NPOS | 97.64 | 5.61 | 97.52 | 4.08 | 94.92 | 14.13 | 94.67 | 8.39 | 91.35 | 18.57 |\n|| DCM-Softmax |  99.7 (0.1) | 0.4 (0.3) | 98.6 (0.8) | 6.6 (3.0) | 99.7 (0.1) | 0.6 (0.2) | 97.1 (0.2) | 14.8 (0.3) | 92.4 (0.3) | 32.6 (2.1) |\n|| DCM-MaxLogit |  99.8 (0.1) | 0.3 (0.1) | 98.7 (0.7) | 6.0 (3.0) | 99.8 (0.1) | 0.5 (0.2) | 97.1 (0.2) | 14.9 (0.4) | 92.5 (0.3) | 34.4 (2.1) |\n|| DCM-Energy | 99.8 (0.1) | 0.1 (0.1) | 98.8 (0.7) | 5.3 (3.6) | 99.8 (0.1) | 0.1 (0.1) | 97.1 (0.2) | 16.1 (1.1) | 92.5 (0.3) | 35.6 (2.2) |\n| CIFAR-100 /ResNet-34 | | | | | | | | | | |\n|| VOS |  73.11 | 78.50 | 85.72 | 59.05 | 82.66 | 72.45 | 80.08 | 75.35 | 75.85 | 84.55 |\n|| NPOS |  97.84 | 11.14 | 82.43 | 56.27 | 85.48 | 51.72 | 92.44 | 35.20 | 71.30 | 79.08 |\n|| Dream-OOD | 87.01 | 58.75 | 95.23 | 24.25 | 99.73 | 1.10 | 88.82 | 46.60 | 79.94 | 70.85 |\n|| DCM-Softmax |  99.3 (0.2) | 1.8 (0.9) | 98.6 (0.3) | 8.7 (1.9) | 99.3 (0.2) | 2.3 (1.4) | 88.5 (0.5) | 46.6 (2.7) | 78.6 (0.4) | 67.7 (2.3) |\n|| DCM-MaxLogit |  99.3 (0.2) | 2.0 (1.1) | 98.7 (0.2) | 7.3 (1.9) | 99.3 (0.2) | 2.3 (1.4) | 88.5 (0.5) | 46.9 (3.0) | 78.6 (0.4) | 68.8 (1.8) |\n|| DCM-Energy |  99.2 (0.2) | 2.2 (1.2) | 98.9 (0.2) | 5.2 (2.0) | 99.4 (0.2) | 1.9 (0.8) | 89.3 (0.6) | 48.6 (3.3) | 78.3 (0.6) | 68.9 (1.9)|"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6830/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700450238600,
                "cdate": 1700450238600,
                "tmdate": 1700498609744,
                "mdate": 1700498609744,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "czn3KlLheh",
                "forum": "HKV45Y0rFz",
                "replyto": "FBuKM5jezt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6830/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6830/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Initial Response to Reviewer sVZ1 [2/2]"
                    },
                    "comment": {
                        "value": "Additionally, DCM significantly outperforms VOS [3] and NPOS [4] with larger datasets and model architectures. We ran additional OOD detection experiments using a pretrained CLIP ViT-B/16 with ImageNet-1K as ID and iNaturalist, Places365, SUN and Textures as OOD datasets. The complete table is in Appendix I.\n\n| Methods | iNaturalist |  |  SUN | | Places | | Textures | |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n|  | FPR@95 (\u2193) | AUROC (\u2191) | FPR@95 (\u2193) | AUROC (\u2191) | FPR@95 (\u2193) | AUROC (\u2191) | FPR@95 (\u2193) | AUROC (\u2191) |\n| Fort et al.,MSP | 54.05 | 87.43 | 73.37 | 78.03 | 72.98 | 78.03 | 68.85 | 79.06 |\n| VOS | 31.65 | 94.53 | 43.03 | 91.92 | 41.62 | 90.23 | 56.67 | 86.74 |\n| VOS+ | 28.99 | 94.62 | 36.88 | 92.57 | 38.39 | 91.23 | 61.02 | 86.33 |\n| NPOS | 16.58 | 96.19 | 43.77 | 90.44 | 45.27 | 89.44 | 46.12 | 88.80 |\n| DCM-Softmax | 2.6 (0.5) | 99.2 (0.1) | 32.9 (1.5) | 94.2 (0.2) | 35.9 (1.8) | 93.8 (0.3) | 11.2 (1.0) | 97.9 (0.1) |\n| DCM-MaxLogit | 1.8 (0.4) | 99.4 (0.1) | 27.5 (1.4) | 94.9 (0.2) | **32.5 (2.8)** | 94.5 (0.3) | 8.2 (0.8) | 98.3 (0.1) |\n| DCM-Energy | **0.5 (0.2)** | **99.6 (0.1)** | **24.5 (1.7)** | **95.8 (0.2)** | **30.8 (3.0)** | **95.4 (0.3)** | **4.3 (0.6)** | **98.8 (0.1)** |\n\n[1] Training OOD Detectors in their Natural Habitats, ICML 2022\n\n[2] Semi-supervised novelty detection using ensembles with regularized disagreement, UAI 2022\n\n[3] VOS: Learning What You Don't Know by Virtual Outlier Synthesis, ICLR 2022\n\n[4] Non-parametric Outlier Synthesis, ICLR 2023\n\n[5] Dream the Impossible: Outlier Imagination with Diffusion Models, NeurIPS 2023"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6830/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700450266650,
                "cdate": 1700450266650,
                "tmdate": 1700488763668,
                "mdate": 1700488763668,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WZPEIWWtVM",
                "forum": "HKV45Y0rFz",
                "replyto": "FBuKM5jezt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6830/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6830/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Checking in"
                    },
                    "comment": {
                        "value": "We wanted to follow up to see if the response and revisions address your concerns. We are open to discussion if you have any additional questions or concerns, and if not, we kindly ask you to reevaluate your score. Thank you again for your reviews which helped to improve our paper!"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6830/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700597057139,
                "cdate": 1700597057139,
                "tmdate": 1700597057139,
                "mdate": 1700597057139,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ck8C2fAw0g",
                "forum": "HKV45Y0rFz",
                "replyto": "MJh3zXmjtN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6830/Reviewer_sVZ1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6830/Reviewer_sVZ1"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Hello,\n\nMy apologies for the late reply. The new results are helpful, but I'm still a bit hung up on the simplicity of the method and the practical value of the proof. From what I can see, the main takeaway of the proof is \"we then demonstrate that DCM can provably detect unknown examples similar to those in the uncertainty set with an appropriate threshold on predicted confidence\", but this just seems obvious to me. If one is training against a specific dataset (uncertainty set), then surely examples similar to that dataset can be separated. I'm just not sure what this is telling us that most readers wouldn't already know.\n\nWith respect to other sources of novelty highlighted in the responses, the selective classification setting is interesting, but I'm not sure if applying something very similar to OE with a specific uncertainty set of misclassified examples is enough technical novelty. The results are good, but that always has to be weighed against other aspects of a paper, and I think overall I'm still not convinced enough by the response to increase my score. If the authors don't have time to reply, I'll wait a few days and reread the rebuttal and paper before finalizing my score."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6830/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700708427245,
                "cdate": 1700708427245,
                "tmdate": 1700708427245,
                "mdate": 1700708427245,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GhFcOFkVr0",
                "forum": "HKV45Y0rFz",
                "replyto": "FBuKM5jezt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6830/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6830/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer sVZ1"
                    },
                    "comment": {
                        "value": "Dear Reviewer sVZ1,\n\nThank you for your response.\n\nWe agree our method is simple, but we view simplicity as a strength, especially in light of the strong empirical performance. Many impactful machine learning methods are valued for being straightforward, as they're easier to use and build upon.\n\nThe proof's practical value is to show that if some OOD examples are included in the uncertainty set, then filtering out ID examples from the uncertainty dataset (as done by OE) is unnecessary. \n\nFinally, we also note that the ICLR 2024 call for papers lists societal considerations including \"fairness, safety, privacy\" as specifically relevant topics. Considering the critical reliability and safety risks of machine learning model errors, we feel that our work showing substantial mitigation of this problem is very relevant and will be of great interest to the ICLR community.\n\nWe appreciate your feedback and hope this clarifies our perspective."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6830/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700711701522,
                "cdate": 1700711701522,
                "tmdate": 1700729774616,
                "mdate": 1700729774616,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]