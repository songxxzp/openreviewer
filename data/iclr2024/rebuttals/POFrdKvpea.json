[
    {
        "title": "ACRF: Compressing Explicit Neural Radiance Fields via Attribute Compression"
    },
    {
        "review": {
            "id": "Fw09PJjxR7",
            "forum": "POFrdKvpea",
            "replyto": "POFrdKvpea",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2447/Reviewer_sVJV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2447/Reviewer_sVJV"
            ],
            "content": {
                "summary": {
                    "value": "Given a pre-trained NeRF model with an explicit 3D representation, this paper aims to reduce the model size while maintaining good performance. It proposes reformulating the task as 3D data compression with three steps.\nFirstly, it proposes view-dependent pruning, which eliminates features with low absolute values.\nThen, it performs a point-based wavelet transform with octree coding to convert features with importance to coefficients.\nLastly, it minimizes the information entropy for the coefficients.\nDepending on whether there is a rendering loss, two versions are introduced.\nExperiments are conducted on the Synthetic-NeRF and Tank&Temples datasets to show their significant compression rates."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "[Results] The quantitive results in Fig. 4 and encoding time comparison Tab. 1 show the proposed method achieves a modestly larger compression ratio in comparison with voxel-based methods and a remarkably higher compression ratio in comparison with point-based methods.\n\n[Novelty] The proposed three-step method is reasonable, achieves remarkable improvements in compression performance, and simultaneously reduces the encoding time."
                },
                "weaknesses": {
                    "value": "[Clarity]\n\n- In Sec.4.1, the paper uses a half page and two formulations to introduce view-dependent pruning. I think this part seems redundant. Because the following view-independent pruning is not based on the view-dependent pruning.\n\n- In Sec.4.2, the paper introduces $e = RAHT(p,f \\cdot I_l)$ as a straightforward implementation. I am not sure how to introduce the importance and then recover $f$. Actually, I think $f \\cdot I_l $ destroy $f$ and it is hard to recover $f$. It is not a noise issue. Instead, I think the proposed mask strategy is more intuitive. Also, if $e = RAHT(p,f \\cdot I_l)$ is important, it would be better to use it as a baseline. I think this part introduces more confusion.\n\n- I could not follow the first formula in Section 4.3, the independent variables are distributions p and q, but in the right of the equation, there is only q. I am not sure how to eliminate the distribution of p. Are any assumptions or references missing?\n\n- Fig.1 is too small, and it does not provide enough explanation. I cannot follow the meaning of the color or the connection between local patterns and latent features.\n\n- Fig.5 is hard to distinguish. All the results of different methods seem the same. It would be better to highlight their difference.\n\n[Fig.4] In Fig.4, the compressed model achieves a 100 $\\times$ compression ratio while obtaining a higher SSIM (top right). Would you please provide more discussion on it?\n\n[Speed & Practicality] In Sec.5.1 Encoding Time Comparison part, the paper claims that the proposed method only requires additional 1s for decoding. This part is vague. I am not sure whether this time has a huge impact or is insignificant during testing. The overall compression seems complex. I suggest the decoding time and the overall inference time should be discussed in detail. More specific settings, including batch size, image size, and overall inference time, are needed.\n\n[$|f_v|$] Eq.5 is not so convicting. For me, I think it is easy to understand that the minimum absolute value means low information. But I am not sure the maximum absolute value can be employed to quantify the amount of information. The paper claims this is based on''statistical measures''. It would be better to provide statistical evidence or some references to support this claim.\n\n[Task specific] The feature encoding and the entropy minimization seem like standard strategies for data compression and are not specifically designed for Nerf. It would be better to highlight the main contribution of those two parts regarding Nerf.\n\n\n[Experiments]\n\n- This paper claims that integrating pruning strategies into training pipeline is time-consuming and impractical for compression(Section 4.1). But it is still important to compare those existing methods(Deng & Tartaglione, 2023; Xie et al., 2023), even the proposed method is a post-optimization method.\n\n- The comparison experiments are insufficient. I suggest to compare previous works like (Li et al.,2023; Rho et al., 2023;)\n\n[Typo]\n- Section 1, propsoe -> propose\n- Appendix Section C, subscript error\n-The description of baselines should be more specific (Fig. 5), like the definitions of \"ACRF(DVGO)\" and \"ACRF-F(DVGO)\"."
                },
                "questions": {
                    "value": "See weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2447/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698589515659,
            "cdate": 1698589515659,
            "tmdate": 1699636180690,
            "mdate": 1699636180690,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iJbAjbaitO",
                "forum": "POFrdKvpea",
                "replyto": "Fw09PJjxR7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2447/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2447/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer sVJV"
                    },
                    "comment": {
                        "value": "Thank you for your time and helpful feedback. We respond below to your questions and concerns. Please note that additional figures are included in the updated version of the appendix (Supplementary Material).\n\n**Q1. Introduction of view-dependent pruning.**\n\nWe shrink the introduction of view-dependent pruning in the main paper and move detailed descriptions to the appendix.\n\n\n**Q2. Straightforward implementation of importance-guided feature encoding.**\n\n\nWe provide more elaborations about straightforward implementation of importance-guided feature encoding in appendix Sec. H as follows.\n\nGiven point positions $\\mathbf{p}$ and features $\\mathbf{f}$, we transform features into coefficients through RAHT transform as $\\mathbf{e}=\\text{RAHT}(\\mathbf{p}, \\mathbf{f})$. Details about RAHT are provided in appendix Sec. D.\n\nIncorporating importance $I_l$ as prior information, our objective is to leverage $I_l$ to effectively preserve point features with heightened significance. In the initial implementation described in Sec. 4.2, we obtain coefficients as:\n$$\\mathbf{e}=\\text{RAHT}(\\mathbf{p}, \\mathbf{f}_I), \\quad \\mathbf{f}_I=\\mathbf{f} \\cdot I_l.$$Throughout the decoding process, the reconstructed features $\\hat{\\mathbf{f}}$ can be recovered from the inverse RAHT transform while considering importance:\n$$\\hat{\\mathbf{f}}=\\frac{\\hat{\\mathbf{f}}_I}{I_l}, \\quad \\hat{\\mathbf{f}}_I=\\text{iRAHT}(\\mathbf{p}, \\mathbf{e}).$$Note that, for the recovery of point features, the transmission of importance values is requisite for decoding.\n\nWe further provide an examination of the high-frequency noise issue. As shown in appendix Fig. 6 (a) and (b), we visualize the GT image, view-independent and view-independent importance, and the top 1\\% point features sorted by their corresponding importance. Importance is visualized as a heatmap, where red signifies higher values and blue lower ones, and features are projected into color space through t-SNE. \n\n\nIn appendix Fig. 6 (a), the point feature representation of the black stick of the lego in the GT image appears as green in the projected feature image. However, in the importance figure, it is composed of red, yellow, and blue elements. A similar observation is also shown in appendix Fig. 6 (b). This illustrates that, unlike point features, point importance lacks a robust correlation with the scene color. Consequently, the direct combination of features with importance may introduce additional noise to the original features.\n\nWe agree that the proposed mask strategy is also an intuitive approach. Given that our strategy is directly built upon the original RAHT, we use RAHT as the baseline algorithm.\n\n\n**Q3. The first formula in Section 4.3.**\n\nThe first formula in Section 4.3 indicates the Shannon cross entropy. We agree that updating our formula following previous works [1] would be more appropriate, as $D = \\mathbb{E}_{\\mathbf{e} \\sim p}\\left[-\\log _2 q\\left(\\mathbf{e} \\mid I_l\\right)\\right]$. This formula is widely employed in image compression, for instance, in equation (7) of [2] and equation (1) of [1].\n\n[1] Variational image compression with a scale hyperprior.\n\n[2] End-to-end optimized image compression.\n\n\n\n**Q4. Explanation of Fig.1.**\n\nTo visualize latent features, we employ t-SNE to project high-dimensional features from feature space to a 3-dimensional color space, as depicted in Fig. 1 in the main paper and Sec. A in the appendix. It is clear that latent features follow a certain distribution closely associated with the original color. Moreover, the latent features exhibit clustering, resembling visualization results often observed in scene decomposition tasks. In order words, the local patterns (or patches) of latent features reveal significant spatial redundancy among nearby features.\n\n\n**Q5. Fig.5 is hard to distinguish.**\n\nWe highlight the specific model size and quality info in Fig.5 to indicate their difference.\n\n\n**Q6. Discussion on SSIM in Fig.4.**\n\nThe main reason of the higher SSIM lies in the joint learning stage of ACRF, which further optimizes the DVGO model. To ensure a fair comparison with VQRF, the optimization iteration is set at 10K, consistent with the finetune steps of VQRF."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2447/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700653274267,
                "cdate": 1700653274267,
                "tmdate": 1700653274267,
                "mdate": 1700653274267,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "kNHA3rGIPc",
            "forum": "POFrdKvpea",
            "replyto": "POFrdKvpea",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2447/Reviewer_GWvi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2447/Reviewer_GWvi"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel approach for explicit NeRF compression, focusing on compressing the latent features of the NeRF model. More specifically, the authors first conduct a comprehensive analysis of NeRF compression and reformulate the task as 3D data compression. Then, they propose their ACRF framework, comprising pruning, feature encoding, and entropy minimization. Experiments were conducted to demonstrate the effectiveness of the proposed method, both for the compression performance and coding speed."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Originality: The analysis and reformulation of NeRF compression seems reasonable, and the utilization of attribute compression is practical.\n2. Quality: The experimental results validate the effectiveness of the proposed algorithm, which integrates the conventional image compression pipeline with several NeRF-oriented modules.\n3. Clarity: The paper is well-written and easy to read.\n4. Significance: Given the rising popularity of NeRF models, the investigation of NeRF compression is of great significance."
                },
                "weaknesses": {
                    "value": "1. From the perspective of conventional image compression, additional computation and time budget is required for encoding and decoding, which might be a problem for real NeRF applications. \n2. In the related works (Sec. 2.2), it would be better to illustrate the main difference between the proposed method and prior research, such as the mentioned 3DAC, Rho et al. and ReRF.\n3. Experiments: In Table 2, it seems that there is a performance drop with additional information (Voxel Grid, A). Please check the result and give some explanations. Similarly, in Figure 6, Baseline A outperforms A+Imp FE. Please justify.\n4. Experiments: In Figure 5, the qualitative results of two relatively simple scans are provided. It would be beneficial to extend the analysis to include more complex scans (e.g., object with high reflectance).\n5. In the appendix (Sec. D), it seems that the proposed modules do not perform well on PointNeRF. It would be beneficial to extend the analysis in the appendix, and add some discussions in the main paper."
                },
                "questions": {
                    "value": "1. Include discussion and future direction for recent NeRF algorithms. The gaussian-splatting algorithm represents the radiance field in a point-cloud-like structure, showcasing notable achievements in training and rendering performance. One limitation of this approach is the significant storage budget. For example, the output model takes over 1GB for unbounded scenes, like bicycle in mipnerf360. It will be interesting to integrate these techniques with the proposed algorithm.\n2. See weakness 3, 4, 5 and 6."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2447/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698756360142,
            "cdate": 1698756360142,
            "tmdate": 1699636180609,
            "mdate": 1699636180609,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TtyTzcA8sR",
                "forum": "POFrdKvpea",
                "replyto": "kNHA3rGIPc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2447/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2447/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer GWvi"
                    },
                    "comment": {
                        "value": "Thank you for your time and helpful feedback. We respond below to your questions and concerns. Please note that additional figures are included in the updated version of the appendix (Supplementary Material).\n\n\n**Q1. Additional computation and time budget.**\n\nWe agree that the necessity of additional encoding and decoding steps in the current compression pipeline. Similar to image and video compression, NeRF-oriented compression algorithms become necessary for the transmission and storage of NeRF models, especially given the rising popularity of explicit NeRF. Consequently, the additional computation and time budget are inevitable. In this regard, the exploration of real-time NeRF compression stands as a practical research direction, and we leave it as a future study.\n\n\n\n**Q2. Difference between ACRF and prior works.**\n\n3DAC constructs a RAHT-based entropy model for point cloud attribute compression, Rho et al. introduce a 2D wavelet transform-based algorithm for plane-based NeRF models (TensoRF) and ReRF presents a residual field for dynamic NeRF models. \n\n\n**Q3. Performance drop in Table 2 and Figure 6.**\n\n1. The observed performance drop with additional information (Voxel Grid, A) can be attributed to the use of two separate grid representations by DVGO. Specifically, DVGO models density and color features with two distinct grids, allowing voxels with low density to still contribute valuable features to sampled ray points through trilinear interpolation.\n\n2. The observed results in Figure 6 can be attributed to the mixed coefficients distributions generated by transforms in regions with varying importance. A naive entropy model, commonly used in deep image compression, fails to adequately fit this composite distribution during optimization. Consequently, we introduce importance as side information for entropy modelling to develop our full model.\n\n\n**Q4. More qualitative results.**\n\nAdditional qualitative results are available in appendix Sec. K, and these results are consistent with those provided in the main paper. Notably, for more complex scans such as object with high reflectance, our algorithm's rendering results resemble the uncompressed model, exhibiting similar artifacts. Addressing this issue may involve enhancing the original NeRF model through the incorporation of advanced reconstruction algorithms, such as BRDF estimation.\n\n\n**Q5. Limited improvement on PointNeRF.**\n\nThe main reason is that PointNeRF adopts a more compact point representation. As shown in appendix Figure 4, neural points in PointNeRF exhibit closely similar importance values, which leads to the ineffectiveness of the importance-based modules. Nonetheless, with our basic framework, we achieve a 10:1 compression ratio with negligible performance degradation on PointNeRF.\n\n**Q6. Discussion about gaussian-splatting and future direction.**\n\nGaussian-splatting has garnered considerable attention recently due to its remarkable rendering performance and fast optimization and inference speed. The memory and storage consumption of this algorithm primarily consist of millions of 3D Gaussians, each containing a 3D position, scales, rotation, opacity, and spherical harmonics. We agree that exploring storage and memory compression for Gaussian-splatting is an interesting research avenue. Addressing this challenge could involve employing model-specific pruning and feature encoding algorithms, and we leave it for future study."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2447/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700653211317,
                "cdate": 1700653211317,
                "tmdate": 1700653211317,
                "mdate": 1700653211317,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8SpCtKFwx7",
            "forum": "POFrdKvpea",
            "replyto": "POFrdKvpea",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2447/Reviewer_HSPa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2447/Reviewer_HSPa"
            ],
            "content": {
                "summary": {
                    "value": "This manuscript proposed a novel framework of Radiance Field attribute compression, which treated the compression task of explicit neural 3D representation as 3D data compression. Specifically, the neural 3D structure is pruned and converted to points with features, which are further encoded using importance-guided feature encoding. An importance-prioritized entropy model is proposed to estimate the probability distribution of transform coefficients, which are then entropy coded with an arithmetic coder using the predicted distribution. Experimental results demonstrate that the proposed method achieves superior performance on both synthetic and real-world datasets such as Synthetic-NeRF and Tanks&Temples."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The whole manuscript is well structured and the technique details are easy to follow. Experimental results demonstrate that the proposed method achieves superior performance on both synthetic and real-world datasets, in terms of RD performance and encoding/decoding time,.\n\nThe method proposed in this manuscript follows the standard point cloud attribute compression process, and has been optimized based on the characteristics of the explicit neural 3D representation in multiple stages such as data pruning, feature encoding and entropy minimization. The idea is reasonable and interesting."
                },
                "weaknesses": {
                    "value": "1\uff09Some technique details are not clear enough. For example, only encoding time data are provided in Table 1, it is suggested to provide decoding time data to highlight the practicality of the proposed algorithm. The model size and quality info are missing in Fig. 5.\n\n2)  Other type of compression methods such as Rho et al. (Rho et al., 2023) are not evaluated in this manuscript.\n\n3) Typos. ``As depicted in 6,``=>``As depicted in Fig. 6,``"
                },
                "questions": {
                    "value": "Please refer to the Weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2447/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2447/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2447/Reviewer_HSPa"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2447/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698807798018,
            "cdate": 1698807798018,
            "tmdate": 1699636180512,
            "mdate": 1699636180512,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ni3krrhfJ3",
                "forum": "POFrdKvpea",
                "replyto": "8SpCtKFwx7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2447/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2447/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer HSPa"
                    },
                    "comment": {
                        "value": "Thank you for your time and helpful feedback. We respond below to your questions and concerns. Please note that additional figures are included in the updated version of the appendix (Supplementary Material).\n\n\n**Q1. Decoding time in Table 1 and model size with quality info in Fig. 5.**\n\n\n1. The decoding stage comprises two parts, first entropy decoding (around 0.5s) and then transform decoding (around 0.4s). Consequently, the overall decoding time takes under 1s. It is worth noting that these components are currently implemented in Python. However, in practical applications, re-implementation in C++ is recommended [1, 2].\n\n\n2. Detailed quantitative results are available in appendix (Table 1 to 4). We have incorporated the corresponding results to Fig. 5 in our revised manuscript.\n\n[1] Compression of 3d point clouds using a region-adaptive hierarchical transform\n\n[2] https://github.com/digitalivp/RAHT\n\n\n**Q2. Comparison with other compression methods.**\n\nWe provide more comparison with other compression methods in appendix Sec. J as follows.\n\nAdditional quantitative compression results for different methods are presented in appendix Fig. 8. We compare our ACRF and ACRF-F with VQRF (Li et al.,2023), Re:NeRF (Deng \\& Tartaglione, 2023) and Rho et al. (Rho et al., 2023). Notably, ACRF, ACRF-F, VQRF and Re:NeRF are based on DVGO, while Rho et al. is built upon TensoRF. \n\n\nAppendix Fig. 8 (a) shows comparisons on the Synthetic-NeRF dataset. It is clear that our ACRF outperforms other DVGO-based algorithms. Note that Rho et al. attains the highest rendering result due to its original model, TensoRF-VM-384, achieving a PSNR of 33.21 dB on Synthetic-NeRF, while DVGO only gets 31.91 dB. Despite a relatively large performance drop in PSNR, Rho et al. still gets the best quantitative results.\n\n\nAppendix Fig. 8 (b) shows comparisons on the Tanks\\&Temples dataset. In this more complex real-world dataset, our ACRF outperforms all other algorithms. Although TensoRF still achieves a better rendering quality than DVGO (28.56 dB versus 28.31 dB in PSNR), the relatively large distortion introduced by Rho et al. places this TensoRF-based algorithm behind both ACRF and VQRF. Notably, our lightweight model, ACRF-F, achieves a similar compression result to Rho et al, while our ACRF-F requires only a few seconds for encoding, while Rho et al. demands 24 mins for training (see Table 1 in appendix of Rho et al.).\n\n\n**Q3. typo**\n\nWe thank reviewer for the detailed comment. We have fixed the typo in our revised manuscript."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2447/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700653155134,
                "cdate": 1700653155134,
                "tmdate": 1700653155134,
                "mdate": 1700653155134,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lnjfswkWcS",
            "forum": "POFrdKvpea",
            "replyto": "POFrdKvpea",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2447/Reviewer_MUJE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2447/Reviewer_MUJE"
            ],
            "content": {
                "summary": {
                    "value": "This work addresses the problem of compressing explicit Neural Radiance Fields (NeRFs) for 3D data representation. The authors introduce a framework called Attributed Compression of Radiance Field (ACRF) to achieve this. ACRF prunes the neural 3D structure and encodes it as points with features using importance-guided encoding. It also employs an importance-based entropy model to optimize the encoding process. The authors present two models, ACRF and ACRF-F, balancing compression performance and encoding time. Experiments on synthetic and real-world datasets, including Synthetic-NeRF and Tanks&Temples, showcase the superior performance of their approach."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The idea is simple yet effective.\n- The experimental results are convincing and the ablation study shows the necessity of each component."
                },
                "weaknesses": {
                    "value": "Major\n- The proposed method is based on voxel-grid compression of NeRF with limited change so the novelty of it is limited.\n- The paper writing can be further improved. For instance:\n  - In section 3, the concrete definitions of r and N are not shown.\n  - The motivation for adopting entropy minimization mentioned in section 4.3 is unclear.\nMinor:\n- I recommend the authors mention which section in the supplementary describes the details in the main paper."
                },
                "questions": {
                    "value": "Major:\n- Could you provide more elaboration about why RAHT with point importance introduces additional high-frequency noise to the original features and necessitates the transmission of importance values for decoding?\n- Could the author provide more explanations for the motivation for adopting entropy minimization?\n- How can the \\labmda be tuned to control the model size?\n\nMinor:\n- Why do the authors only conduct the ablation study on the chair of the Synthetic-NeRF dataset? Could the authors provide experimental results of the ablation study on different datasets?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2447/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699068942085,
            "cdate": 1699068942085,
            "tmdate": 1699636180453,
            "mdate": 1699636180453,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "i8RuUUrRXp",
                "forum": "POFrdKvpea",
                "replyto": "lnjfswkWcS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2447/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2447/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer MUJE"
                    },
                    "comment": {
                        "value": "Thank you for your time and helpful feedback. We respond below to your questions and concerns. Please note that additional figures are included in the updated version of the appendix (Supplementary Material).\n\n**Q1. Novelty.**\n\nFirst and foremost, we would like to underscore the value of our work:\n\n1. Our study presents a comprehensive analysis of explicit Neural Radiance Fields (NeRF) models, transforming the NeRF compression task into a paradigm of 3D data compression.\n\n2. To the best of our knowledge, our ACRF is the first NeRF compression framework built upon 3D data compression techniques, including integration of pruning, position/feature coding and entropy coding.\n\n3. The enhancement of our algorithm is realized through the incorporation of several modules, including view-dependent pruning, importance-guided feature encoding and entropy modeling.\n\nThus, we believe that our ACRF constitutes a task-oriented algorithm with effective improvements.\n\n\n\n**Q2. The concrete definitions of r and N.**\n\nWe thank reviewer for the detailed comments. We have updated a revision of the background section (Section 3), aligning it more closely with the original NeRF paper. Specifically, we update 'r' as the ray and 'N' as the number of sampled intervals along the ray.\n\n\n**Q3. The motivation of entropy minimization.**\n\nWe rewrite the motivation of entropy minimization in section 4.3 as follows.\n\n\nAccording to information theory, the information entropy $H(\\mathbf{e})$ of coefficients $\\mathbf{e}$ indicates the average level of 'information' as $H(\\mathbf{e}) = \\mathbb{E}\\left[-\\log _2 p\\left(\\mathbf{e}\\right)\\right]$, where $p(\\mathbf{e})$ is the probability distribution of coefficients $\\mathbf{e}$. The objective of this section is to minimize the information entropy $H(\\mathbf{e})$ inherent to the NeRF model through imposing constraints on the distribution $p(\\mathbf{e})$, thereby reducing the model size of NeRF. Inspired by deep image compression [1], we learn an estimated distribution $q(\\mathbf{e})$ to constrain $p(\\mathbf{e})$. It is also worth noting that there exists a strong interrelation between the distribution of point features and their corresponding importance $I_l$. Thus, we further introduce $I_l$ as a prior information component.\n\n\n\n[1] Variational image compression with a scale hyperprior.\n\n\n**Q4. Appendix section number for the main paper.**\n\nWe thank reviewer for the detailed comment. We have added appendix section number for the main paper.\n\n\n**Q5. Elaboration about straightforward implementation of Importance-Guided Feature Encoding.**\n\nWe provide more elaborations about straightforward implementation of importance-guided feature encoding in appendix Sec. H as follows.\n\nGiven point positions $\\mathbf{p}$ and features $\\mathbf{f}$, we transform features into coefficients through RAHT transform as $\\mathbf{e}=\\text{RAHT}(\\mathbf{p}, \\mathbf{f})$. Details about RAHT are provided in appendix Sec. D.\n\nIncorporating importance $I_l$ as prior information, our objective is to leverage $I_l$ to effectively preserve point features with heightened significance. In the initial implementation described in Sec. 4.2, we obtain coefficients as:\n$$\\mathbf{e}=\\text{RAHT}(\\mathbf{p}, \\mathbf{f}_I), \\quad \\mathbf{f}_I=\\mathbf{f} \\cdot I_l.$$Throughout the decoding process, the reconstructed features $\\hat{\\mathbf{f}}$ can be recovered from the inverse RAHT transform while considering importance:\n$$\\hat{\\mathbf{f}}=\\frac{\\hat{\\mathbf{f}}_I}{I_l}, \\quad \\hat{\\mathbf{f}}_I=\\text{iRAHT}(\\mathbf{p}, \\mathbf{e}).$$Note that, for the recovery of point features, the transmission of importance values is requisite for decoding.\n\nWe further provide an examination of the high-frequency noise issue. As shown in appendix Fig. 6 (a) and (b), we visualize the GT image, view-independent and view-independent importance, and the top 1\\% point features sorted by their corresponding importance. Importance is visualized as a heatmap, where red signifies higher values and blue lower ones, and features are projected into color space through t-SNE. \n\n\nIn appendix Fig. 6 (a), the point feature representation of the black stick of the lego in the GT image appears as green in the projected feature image. However, in the importance figure, it is composed of red, yellow, and blue elements. A similar observation is also shown in appendix Fig. 6 (b). This illustrates that, unlike point features, point importance lacks a robust correlation with the scene color. Consequently, the direct combination of features with importance may introduce additional noise to the original features."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2447/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700653013074,
                "cdate": 1700653013074,
                "tmdate": 1700653013074,
                "mdate": 1700653013074,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]