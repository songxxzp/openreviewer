[
    {
        "title": "Feature Accentuation: Explaining 'what' features respond to in natural images"
    },
    {
        "review": {
            "id": "i8rxGqHjGT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2666/Reviewer_bchW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2666/Reviewer_bchW"
            ],
            "forum": "Ufp0DVjRs0",
            "replyto": "Ufp0DVjRs0",
            "content": {
                "summary": {
                    "value": "Previous methods focus on either where a model attends or what concept the model is looking for. This paper presents a method that can show both where and what a model is focused on -- this new method is called \"feature accentuation\". Feature accentuation is tested for natural-ness of images."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "-  The paper is well-presented, along with a large number of visualizations littered both through the main text as well as the supplementary. The method is clearly explained in detail, as well as motivated explicitly each step of the way.\n- The paper clearly proves what it sets out to -- namely, that the visualizations it produces highlight \"where\" and \"what\" in a natural-looking way.\n- The evidence of natural-looking visualization is strong, and I'm convinced that feature accentuation can produce relatively realistic transformations, at least relative to previous deepdream-esque variants."
                },
                "weaknesses": {
                    "value": "- My main concern is I'm not sure what the utility of feature accentuation is. A good chunk of the experiments focuses on natural-looking images, but I'm slightly less concerned about that. It's certainly a desirable property -- to look more natural and less like hallucinations. However, I'm not sure how this helps us, concretely: Can this help us improve accuracy? Diagnose mistakes? (This is suggested in several of the figures) Suggest a path for fixing the model? (Maybe by pruning \"wrong\" nodes?) There's a preview of this in figures 9 and 10, but the utility isn't studied in the paper. [1] for example has several approaches for defining and quantifying interpretability/utility of a method.\n- Along the lines of the above, I'm not sure how to use the information these visualizations provide. For example, in figure 9, for \"bow\" vs. \"chainsaw\", I can certainly see that the \"chainsaw\" visualization is more chainsaw-like, but couldn't I just pull a random other word and visualize that? For example, I could feature accentuate \"matchsticks\" or \"juggler fire torches\", and I'm not sure how any of those visualizations would help me diagnose mistakes in the original model. Another way to put this would be: If I feature accentuate something clearly unrelated, eg.., \"snail\" for the \"bow\" image, it seems like this method would find *some way to insert a snail, but what does that tell me exactly, if I can insert any concept into the image? (One possibility is that natural looking insertions are \"valid\" explanations, and grotesque abstract art is unrelated? This would need some more fleshing out though, but that would be one way). On a side note, the main utility in figures 9 and 10 is ironically that it highlights some part of the original image, like saliency maps. (I'm not sure how important it is to be able to modify the image, per the above). A study would probably need to show that the image modification is helpful too.\n- Have you tried the sanity checks in [2]? I know [2] is actually cited in your related works. Although feature accentuation is not a saliency map per se, the randomized tests in that paper should still apply. It would help (partially) my concerns above if the method does pass sanity checks -- in that it shows there *is meaning.\n\n[1] Poursabzi-Sangdeh, et al. Manipulating and Measuring Model Interpretability. https://arxiv.org/abs/1802.07810\n[2] Adebayo, et al. Sanity Checks for Saliency Maps. https://arxiv.org/abs/1810.03292"
                },
                "questions": {
                    "value": "I've left my questions above. In summary, I'm not convinced of the technique's utility, but I'm open to being convinced. The approach is certainly thoroughly explored and visually appealing. I'm just afraid that visual appeal could be a misleading objective."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "none"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2666/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697278775235,
            "cdate": 1697278775235,
            "tmdate": 1699636207389,
            "mdate": 1699636207389,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iwjgIknPOn",
                "forum": "Ufp0DVjRs0",
                "replyto": "i8rxGqHjGT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2666/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2666/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for taking the time to read our article and for the valuable feedback.\n\n**\u201cMy main concern is I'm not sure what the utility of feature accentuation is.\u201d**\n\nFeature Accentuation represents a novel tool that, as you rightly pointed out, yields feature visualizations ranging from more to less naturalistic. In recent years, general feature visualizations have significantly impacted the literature, enabling the understanding of internal mechanisms within neural networks (such as specific filters and invariances)[1,2,3], detection of biases [4], enhanced comprehension of complex concepts [6], and visualization of internal states of intricate models[7].\n\nWe have specifically chosen to illustrate the utility of our method with a use-case presented in Figure 9. This example demonstrates how Feature Accentuation can provide valuable insights in explaining failure cases. For instance, in the first image, it might seem perplexing that an image of a barber shop is classified as a moving truck. Our approach sheds light on such cases revealing that the model mistakenly associates the white sheet on the wall with the trailer of a truck.\n\nWe posit that Feature Accentuation serves as a superset of traditional feature visualizations, offering the potential to accelerate all those use cases simultaneously. By providing a way to explore a diverse set of explanations ranging from a natural image to purely abstract feature visualization, our method contributes to the broader landscape of interpretability tools.\n\n\n**\u201ccouldn't I just pull a random other word and visualize that? For example, I could feature accentuate \"matchsticks\" or \"juggler fire torches\", and I'm not sure how any of those visualizations would help me diagnose mistakes in the original model.\u201d**\n\nThis is an excellent point, and something that concerned us as we were developing the method. Indeed, if we can accentuate arbitrary features, it\u2019s possible the technique could mislead the user, \u2018revealing\u2019 how features are expressed in an image when in fact there is nothing in the image that excites the feature. We demonstrate this phenomenon empirically in Figure 6. Given this concern, we demonstrate in section 2.4 how feature accentuations can be combined with attribution masks as a means of filtering, revealing exaggerated versions of the original image only in those image regions that excited the feature in the original. In this way, feature accentuation can help intuit the many different ways natural images excite a feature, without suggesting to the user that everything excites it. \n\n**\u201cHave you tried the sanity checks?\u201d**\n\nThank you for this suggestion. We have implemented the randomization sanity check from the paper, and our method successfully passes it. We\u2019ve added the results to the appendix (section E).\n\n[1] Cammarata, et al., \"Curve Detectors\", Distill, 2020.\n[2] Olah, et al., \"Naturally Occurring Equivariance in Neural Networks\", Distill, 2020.\n[3] Schubert, et al., \"High-Low Frequency Detectors\", Distill, 2021.\n[4] Singla, et al., \u201cCore Risk Minimization using Salient ImageNet\u201d, ICLR 2022\n[5] Fel, et al., \u201cCRAFT: Concept Recursive Activation FacTorization for Explainability\u201d, CVPR 2023\n[6] Ghiazi, et al., \u201cWhat do Vision Transformers Learn? A Visual Exploration\u201d"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2666/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700245923372,
                "cdate": 1700245923372,
                "tmdate": 1700245923372,
                "mdate": 1700245923372,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Zd1vYsyjzC",
            "forum": "Ufp0DVjRs0",
            "replyto": "Ufp0DVjRs0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2666/Reviewer_L6NW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2666/Reviewer_L6NW"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new method for generating visualisations of the features leading to specific activity of neural network models, termed Feature Accentuation.\nThe primary novelty here is to optimise a new image such that a weighted tradeoff is generated between maximising the activation of the unit under study while staying close to the original image.\nVarious techniques are used (image parameterization, augmentation) to make this work.\nThe results of this process look generally appealing, and for certain examples quite compelling, in showing how the technique can swap class labels with subtle or not-so-subtle but still sensible image changes. \nIt is also appealing that the technique can be applied without needing to use an auxiliary generative model."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "My initial recommendation is to accept the paper, since it appears to solve (or at least, offer paths to solution) for a core problem in explainability / network visualisation.\nI have some minor comments on the paper.\nHowever, I must caveat this by saying that I have not worked in this area for several years, and so my knowledge of the literature is outdated. \nIt is possible that other reviewers are aware of work that undermines the novelty or contribution of this approach."
                },
                "weaknesses": {
                    "value": "- The paper is sloppily formatted (parentheses missing from citations, missing references, etc).\n- Unpack the equation for $z^{*}$ (top of page 4) into words, since it's the key equation of the paper.\n- raster plots in figure 8 are poor quality; hard to see detail.\n- Figure 8 B, C: lambda should probably be 1.0 not 10.0\n- Figure 10 caption should make explicit what the rows and columns are."
                },
                "questions": {
                    "value": "- How novel is the frequency domain parameterisation? citations to other literature should be provided.\n- What does it mean to achieve higher correlations in circuit similarity than natural images themselves? In a positive view, this could mean that the feature accentuation technique is settling onto good *prototypes* that provide a coherent illustration of the core concept of the label, and thereby reduces variance from natural depictions of the concept. A less positive view could be that this means the technique is finding local minima that will fail to generalize.\n- Figure 8: what are the smooth curves, and how were their hyperparameters chosen? The underlying data are very noisy, so the choice of smoothing and its associated uncertainty should be reported."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2666/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698686732393,
            "cdate": 1698686732393,
            "tmdate": 1699636207305,
            "mdate": 1699636207305,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vwXVU3KB06",
                "forum": "Ufp0DVjRs0",
                "replyto": "Zd1vYsyjzC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2666/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2666/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We are delighted to hear that you found the paper enjoyable, and we extend our sincere gratitude for investing time and effort in providing a thorough review. We address some of your concerns below.\n\n**Formatting, references, image/figure quality**\n\nThank you for your feedback; we have incorporated it into the manuscript. As our paper contains a lot of images we were constrained by the file size limit on our submission, and subsequently compressed the PDF too aggressively. Our undated manuscript is higher resolution, and the camera-ready version will feature the highest possible quality. \nHow novel is the frequency domain parameterisation? citations to other literature should be provided.\nWhile the frequency domain parameterization is not a novel concept and has been discussed in [1,2], we have, nonetheless, integrated and tested a more recent parametrization (NeurIPS 2023) that proves effective on newer models [3]. Additionally, we have included several missing citations.\n\n**\"What does it mean to achieve higher correlations in circuit similarity than natural images themselves? In a positive view, this could mean that the feature accentuation technique is settling onto good prototypes that provide a coherent illustration of the core concept of the label, and thereby reduces variance from natural depictions of the concept. A less positive view could be that this means the technique is finding local minima that will fail to generalize.\"**\n\nOur interpretation aligns with your \u2018positive view\u2019, which you articulate quite well (perhaps better than we articulate ourselves in the paper)! With regards to generalization, we are confident this result will generalize to new examples given how it was calculated. Each natural image in our experiment can be paired one-to-one with an accentuation of that image towards its class logit. These accentuations are conditioned in no way on any other images in the experiment. Within a given class, we get the average pair-wise correlation for each natural image, and similarly the average correlation of each accentuation to the natural images (excluding the correlations between accentuations and their natural image pair, which would be trivially high). We find the accentuations correlate higher on average, which means accentuating an image towards its class tends to make it follow a circuit closer to other, randomly selected images of the class.\n\n\n**''Figure 8: what are the smooth curves, and how were their hyperparameters chosen? The underlying data are very noisy, so the choice of smoothing and its associated uncertainty should be reported.''**\n\nThe curves depicted in these figures are a spline interpolation (degree 2), across the underlying data-points (class-wise average correlations) averaged into 10 bins. We took this approach so as to convey the raw data and the general trend simultaneously. A difficulty with plotting this data stems from the fact that there is a large amount of variance in the correlation measure across nearby layers of different architectural type. We can filter out this variance in Figure 8.a and isolate the effect we are interested in by plotting the difference between the correlation measure for accentuations and natural images in each layer. We have added this version of the plot \u2013 which requires no smoothing \u2013 to appendix A, as well as additional clarification on the figures and design of the path coherence experiment.\n\n**\u201cFigure 8 B, C: lambda should probably be 1.0 not 10.0\u201d**\n\nWe actually meant for this to be 10.0, as we wanted to measure the effects setting lambda to extreme values.\n\n[1] Differentiable Image Parameterizations, Mordvintsev, et al. Distill, 2018.\n[2] Feature Visualization, Olah, et al. Distill, 2017.\n[3] Unlocking Feature Visualization for Deeper Networks with Magnitude Constrained Optimization, Fel, et al. Neurips 2023."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2666/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700245848977,
                "cdate": 1700245848977,
                "tmdate": 1700245848977,
                "mdate": 1700245848977,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "clhgTp4u1T",
                "forum": "Ufp0DVjRs0",
                "replyto": "vwXVU3KB06",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2666/Reviewer_L6NW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2666/Reviewer_L6NW"
                ],
                "content": {
                    "title": {
                        "value": "Review score unchanged"
                    },
                    "comment": {
                        "value": "I thank the authors for their replies to my questions. I have read the replies and the other comments. My review score will remain unaltered."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2666/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700661492562,
                "cdate": 1700661492562,
                "tmdate": 1700661492562,
                "mdate": 1700661492562,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0VF8JDX04K",
            "forum": "Ufp0DVjRs0",
            "replyto": "Ufp0DVjRs0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2666/Reviewer_kHZK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2666/Reviewer_kHZK"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a model explanation technique that aims to amplify image features for a discriminative model. Specifically, the method finds visualizations that resemble an input image and a target feature, without relying on external models. The paper demonstrates the effectiveness of the framework via qualitative examples and showcases multiple applications."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The approach is well-motivated and well-explained, starting from an overall objective and then the necessary additional components to address challenges encountered.\n* The fact that the proposed approach does not rely on external models makes the framework a standalone explainability framework that probes the internal knowledge within one model only."
                },
                "weaknesses": {
                    "value": "* All results in the main paper are quantitative and are shown only on a few selected examples. \n* The framework is sensitive to hyperparameters such as learning rates and regularization weights, as noted in Appendix C-D, which makes it challenging to adapt to different models."
                },
                "questions": {
                    "value": "* Multiple qualitative examples are shown but the analysis is lacking. For example, how to interpret the results from Figure 12 in the Appendix?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2666/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698821652279,
            "cdate": 1698821652279,
            "tmdate": 1699636207231,
            "mdate": 1699636207231,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5gVufBmrev",
                "forum": "Ufp0DVjRs0",
                "replyto": "0VF8JDX04K",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2666/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2666/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your review, you raise some important concerns that we address below.  \n\n**\u201cAll results in the main paper are quantitative (assume means qualitative)\u201d**\n\nIt\u2019s true that a significant portion of our results focuses on qualitative outcomes, as in some sense feature accentuation is an intrinsically qualitative explanation method; it is designed to produce visualizations a user can qualitatively evaluate to gain intuition for what excites a given feature about an image. Attribution methods and feature visualizations, the standard approaches in this area, are also qualitative in this respect. That said, we respectfully disagree with the notion that our paper lacks quantitative depth. We would like to draw attention to an entire section dedicated to quantitative results (Section 3, circuit analysis, Figure 8 a, b, and c) that we feel compelled to highlight.\nIn this specific section, we argue, akin to the findings presented in [1], that feature accentuations for maximizing an activation should do so in a \u2018natural\u2019 way, i.e. by a similar path through the network as the natural images that excite the neuron. Quantitatively, we demonstrate that our method \u2013 which does not require additional models that enforce a natural-image prior \u2013 achieves excellent results when compared to state-of-the-art feature visualizations on this metric (Figure 8.a). Furthermore, we argue this analysis can be extended to address whether feature accentuations are providing veridical, local explanations for their seed image. That is, a feature accentuation is not a good explanation for why a neuron is excited by an image if the accentuation takes a distinct path through the network. Figure 8.c shows that increasing regularization strength keeps the path through the entire network closer to the seed image. The quantitative evidence presented in Section 3, coupled with our method's independence from additional models, make this work an advancement on the original feature visualizations. We believe this unique aspect of our methodology not only adds depth to the interpretability toolkit but also addresses concerns raised by previous studies.\n\n**\u201cThe framework is sensitive to hyperparameters such as learning rates and regularization weights, as noted in Appendix C-D, which makes it challenging to adapt to different models.\u201d**\n\nIndeed, our framework includes a regularization term designed to keep accentuations close to the seed image while performing activation maximization for a target feature. We posit that this hyperparameter doesn't necessarily require tuning, but rather allows the user to control the extent to which the target feature is exaggerated, a desirable property for VCEs [2]. Additionally, we have added a small experiment to the appendix (section F) that further explores the setting of regularization strength in different contexts. We find that while the appropriate level of regularization must be identified across models, the same regularization strength works well across images/features within a model layer. Further, we present a method for finding the appropriate amount of regularization that does not require visual inspection by the user, thereby automating the selection of this hyperparameter. \nRegarding other hyperparameters, while we do spend a considerable portion of the paper exploring the hyperparameter space, we ultimately prescribe settings that work well in the general case. With regards to the learning rate in particular, our phrasing in appendix C was too strong (it has been updated), a more accurate statement is that the upper limit for the learning rate in feature accentuation is lower than for feature visualization. As long as the learning rate is sufficiently low, it will work well in a variety of contexts. To demonstrate this, we have added Appendix F, which shows feature accentuations for a range of models. The only hyperparameter that is adjusted in this experiment is the regularization strength, which is set once per model, but not adjusted across different images. All other hyperparameters are fixed across all examples. The regularization layer, which of course can not be identical across models, is simply set to the ReLU layer closest to \u00bc depth through each model.\n\u201chow to interpret the results from Figure 12 in the Appendix?\u201d\nAn interpretation of \u2018super-natural images\u2019 like those depicted in Figure 12, can be found in Section 3; they are images generated by feature accentuation that take a prototypical path through the network for their corresponding class. We put Figure 12 in the appendix to limit the number of images in the main paper (as there are a lot already). Still, we presumed the reader would be curious to see what these \u2018super-natural\u2019 images actually looked like. We have clarified the figure caption.\n\n[1] Don't trust your eyes: on the (un)reliability of feature visualizations, Geirhos et al., 2023\n[2] Sparse visual counterfactual explanations in image space, Boreiko et. al.  2022"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2666/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700245599169,
                "cdate": 1700245599169,
                "tmdate": 1700245747621,
                "mdate": 1700245747621,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "toSx5AuGrQ",
            "forum": "Ufp0DVjRs0",
            "replyto": "Ufp0DVjRs0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2666/Reviewer_zCew"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2666/Reviewer_zCew"
            ],
            "content": {
                "summary": {
                    "value": "this paper proposes to add the class at the input of the enconder of an ecoder-decoder transformer model and claim that this leads to an interpretable classifier based on transformer architecture that is suited for interpretable fine-grained cassification."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The simplicity of the the method is the main strength of the paper.\nIt reports results in a plethora of datasets to showcase the soundness of the method."
                },
                "weaknesses": {
                    "value": "Comparison the ResNet model is not relevant in this set up. The comparison should be with the same architecture without the class query at the decoder and other similar architectures changing the relevant parameters.\nThe relevance of interpretability is arguable because it is not quantified. It is also not clear how different this is with the typical existent transformer architectures. A more meaningful comparison would strengthen the paper contribution."
                },
                "questions": {
                    "value": "See weaknesses points."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2666/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699031273331,
            "cdate": 1699031273331,
            "tmdate": 1699636207148,
            "mdate": 1699636207148,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mP7euQAwPi",
                "forum": "Ufp0DVjRs0",
                "replyto": "toSx5AuGrQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2666/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2666/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Unfortunately, the written review pertains to another article; the abstract is different from our paper."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2666/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700245403459,
                "cdate": 1700245403459,
                "tmdate": 1700245403459,
                "mdate": 1700245403459,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "asY3sHIPMZ",
                "forum": "Ufp0DVjRs0",
                "replyto": "mP7euQAwPi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2666/Reviewer_zCew"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2666/Reviewer_zCew"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to sincerely apologize for the mismatch review. \nI write here the one that was originally written for this paper:\n\nSummary:\nThis paper introduces a method called feature accentuation. It is a new explainability method that it indicates which pixels in the image are relevant for the final decision of the model. As well as which kind of features activate relevant neurons.\n\nSoundness: 3 good\nPresentation: 3 good\nContribution: 2 fair\nStrengths:\nA strength of this method is not needed auxiliary generative models and being seeded to images. Also, the release as an open-source library as part of Lucent will make it accessible to those who would like to use it for their applications or built on top of it.\n\nTo overcome some gaps from previous related research, this methods incorporates several techniques to avoid that the modified image that accentuates some feature changes how it activates the neuron\u2019s with respect to the original seed image as a regularisation term in the loss. There is an analysis on which layers play an important role to avoid undesired distortions, and it is reported that enforcing it in earlier layers yields better visualizations in early layers. An analysis in the impact of regularisation, parametrisation and augmentation techniques from the literature applied in this method is conducted, highlighting the right combination of those factors. Additionally, to improve relevance of the feature representations, a global normalisation is proposed\n\nThe experiments reported use circuit coherence assessment from another paper, which is a reasonable measure.\n\nWeaknesses:\nThe other applications showcased are also of high importance, but the results become more difficult to assessed. How confirmation biased is overcomed with this method? It is still based on visualisations which need human assessment. The what is based on visual information that is difficult to parse for a human. How useful then it really is still an open question. This is already mentioned in limitations, but it is a strong self-critic that should be given more thought on how to overcome those. How to in corporate over tools for the interpretation is also not clear.\n\nMINOR: There is a reference missing with a question mark. There are a couple of blank space missing to segment words.\n\nQuestions:\nPlease refer to weakness points.\n\nFlag For Ethics Review: Yes, Other reasons (please specify below)\nDetails Of Ethics Concerns:\nIt's a minor concern. It is not clear how useful is the explainability with this tool.\n\nRating: 6: marginally above the acceptance threshold\nConfidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct: Yes"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2666/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700479913028,
                "cdate": 1700479913028,
                "tmdate": 1700479913028,
                "mdate": 1700479913028,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]