[
    {
        "title": "Demystifying Local & Global Fairness Trade-offs in Federated Learning Using Partial Information Decomposition"
    },
    {
        "review": {
            "id": "eCLngYKtWy",
            "forum": "SBj2Qdhgew",
            "replyto": "SBj2Qdhgew",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2342/Reviewer_GNty"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2342/Reviewer_GNty"
            ],
            "content": {
                "summary": {
                    "value": "The paper considers the problem of trying to achieve fairness in a federated learning setting. There are multiple data sets that are all privately held and a model is trained for each one. The questions are: When are the models fair on each data set? And, when are the models fair on all the data?\n\nThe paper relates fairness in both settings and mutual information. In particular, they show that mutual information between the  model's prediction and the sensitive attribute is an upper bound on the square of statistical parity (Lemma 1). They also define \"local disparity\" as mutual information conditioned on the particular machine. They then analyze mutual information and show it comes from three sources: information only in the predictions or sensitive attributes, information in both individually, or information in both together.\n\nThey prove necessary/sufficient conditions about when mutual information is low depending on the sources of mutual information. I did not check the appendix for the proofs of these results.\n\nThey introduce a convex optimization problem for minimizing classification error subject to constraints that the mutual information and local information are low. They they solve the problem experimentally for different datasets and visualize the results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "\u2022 The paper investigates the problem of relating global and local fairness in the federated learning setting. According to them (I have not checked), the problem has not been studied before.\n\n\u2022 The use existing literature on partial information decomposition to identify sources of mutual information. \n\n\u2022 There are a ton of lemmas and theorems about when mutual information is small. It appears very comprehensive but I'd like a more direct narrative about what I should be surprised and impressed with.\n\n\u2022 I like the idea of the convex optimization problem and optimizing for error under mutual information constraints.\n\n\u2022 The experiments seem very comprehensive in terms of data sets and different settings of data distributions across clients."
                },
                "weaknesses": {
                    "value": "\u2022 They don't persuade me that mutual information is the \"right\" notion of fairness. Lemma 1 establishes that mutual information is an *upper bound* on statistical parity but it could be a loose upper bound.\n\n\u2022 I think the presentation is difficult to follow and the paper should be rewritten in the following ways:\n- Give an example of the way the theorems and lemmas are proved.\n- I was confused by the general approach until I read the examples in Section 3.1. These examples aren't results so I think they should be moved up to the preliminaries section to facilitate understanding.\n- Lemma 1 and Lemma 2 are results proved by the authors but they appear in the preliminaries section. This was confusing to me especially because there was no discussion of how they were proved.\n- Unique information is used in the preliminaries before it is defined in Definition 3. I didn't find this definition helpful and I don't see similar definitions for redundant information or synergistic information. It would be great if you could define these three quantities in a direct and similar way. I don't know if this is true but maybe something like I(Z,Y|A \\cap B) is redundant information and I(Z,Y |A \\cup B) is synergistic information. I found the notation you used excessive.\n- I'm not sure from reading the main result section if the proofs of the theorems/lemmas following trivially from the definitions or not. Please make this clear with a proof of one of them.\n\n\u2022 The convex optimization section is very short. I think you should restructure to spend more space here given that it's one of your contributions.\n\n\u2022 I got the sense that the experiments were comprehensive but I was missing a discussion about what was interesting here. It'd be great to highlight interesting observations and findings from the experiments. Examples of how the mutual information perspective gives insight into local and global fairness would be great."
                },
                "questions": {
                    "value": "Lemma 1 upper bounds statistical parity with mutual information, how loose is the upper bound?\n\nWhat are interesting observations from your expeirments?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2342/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2342/Reviewer_GNty",
                        "ICLR.cc/2024/Conference/Submission2342/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2342/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698351797681,
            "cdate": 1698351797681,
            "tmdate": 1700685712729,
            "mdate": 1700685712729,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3ZBTtT9Kvx",
                "forum": "SBj2Qdhgew",
                "replyto": "eCLngYKtWy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2342/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2342/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer GNty"
                    },
                    "comment": {
                        "value": "Thank you for your valuable feedback. We have incorporated the suggested changes into our paper to improve the clarity and would love to hear your feedback!  \n\n---\n\n**Presentation of the paper**\n\nWe have rearranged the paper as follows:\n\nPreliminary\n\n* Background on Federated learning \n\n* Background on Partial Information Decomposition \n\n* Intuitive example to understand PID terms\n* Formal PID definitions \n\nMain Results \n\n* Formalizing Global and Local Disparity in FL, Lemma 1, and 2 \n\n* PID of Global and Local Disparity Proposition 1\n\n* Canonical examples to understand decomposition \n\n* Fundamental limits on tradeoffs between Local and Global Disparity\n\n* An optimization framework for exploring the accuracy-fairness trade-off\n\nWe have provided some intuitive examples in the preliminary section to facilitate understanding of PID. We have briefly described a few proofs in the main paper and referred to the location of full proofs in the Appendix. Section 3.2 on fundamental limits has been updated to highlight key results (Theorem 3 and previous Lemma 3 merged, Interaction information definition has been moved to Appendix B). We have further elaborated the section on convex optimization formulation (AFGLOP). \n\n---\n\n**Mutual information as a fundamental metric for assessing fairness**\n\nMutual information, denoted by $I(A;B)$, quantifies the dependency between two random variables, A and B (it captures correlation as well as all non-linear dependencies). Within the scope of fairness, consider the sensitive attribute $ Z $ (such as gender or race) and the model's predictions $ \\hat{Y} $. The mutual information between $ Z $ and $ \\hat{Y} $ provides a measure of their dependence. $I(Z; \\hat{Y}) = 0 $ if and only if $ Z $ and $ \\hat{Y} $  independent.  Conversely, a high mutual information value indicates a strong correlation between the model's predictions and the sensitive attribute, signaling model unfairness. Mutual information has been explored in fairness in the context of centralized machine learning in [1,2,3]. \n\nA recent work [4] provides another interesting interpretation of  $I(Z,\\hat{Y})$ in fairness as the accuracy of predicting $ Z $ from $ \\hat{Y} $ (or the expected probability of error in correctly guessing $Z$ from $\\hat{Y}$). Even in information bottleneck literature [5], mutual information has been interpreted as a measure of how well one random variable predicts (or, aligns with) the other. \n\nWhile other works [3] have used mutual information measures to enforce statistical parity, we are the first to bring Prinsker\u2019s Inequality [4] in the context of fairness to show that the statistical parity gap is actually upper bounded by the square root of mutual information (see Lemma 1). \n\n**Regarding the tightness of this Lemma 1**: The only inequality used in the proof of Lemma 1 is the Pinsker\u2019s inequality:  $TV(P,Q) \\leq \\sqrt{0.5KL(P,Q)}$ [6]. Pinsker\u2019s inequality is known to be tight for $KL\\leq 1$  (we refer to a short paper on the Tightness of Pinsker\u2019s Bound [6]). We have added Remark 5 in Appendix B Page 15 to highlight this.\n\n*Remark on Tightness of Lemma 1:*\nSince our proof exclusively utilizes Pinsker's inequality, their tightness is equivalent. The mutual information (which can also be defined as a KL divergence) is upper bounded by the entropy $H(\\hat{Y})$, that is $ I(Z;\\hat{Y}) \\leq \\text{min}$ { $H(Z), H(\\hat{Y})$}$ \\leq H(\\hat{Y})$. In binary classification  $ H(\\hat{Y}) \\leq 1 $. Hence, we have $ I(Z;\\hat{Y}) \\leq 1 $  which is aligned with the known tight regime of Pinsker's inequality (i.e., $KL \\leq 1$) [6]. The inequality gets tighter with smaller mutual information $I(Z;\\hat{Y})$ values [6].\n\n---\n\n**Definitions for redundant information or synergistic information** \n\nWe have made some edits in the preliminary section on Page 3 to clarify this further.\n\nWe only provide the formal definition of $\\text{Uni(Z:A|B)}$ because defining any one of the PID terms suffices to get the others. $\\text{Red}(Z:A, B)$ is the sub-volume between $I(Z;A)$ and $I(Z;B)$ (see Fig.1). Hence,  $\\text{Red}(Z:\\hat{Y}, S) = I(Z;\\hat{Y}) - \\text{Uni}(Z: \\hat{Y}| S)$ and $\\text{Syn}(Z:\\hat{Y},S) =  I(Z; \\hat{Y}, S) - \\text{Uni}(Z: \\hat{Y}|S) -\\text{Uni}(Z: \\hat{Y}|S)-\\text{Red}(Z:\\hat{Y}, S)$ (from Equation 1). \n\n---\n\n[1]Fairness-Aware Classifier with Prejudice Remover Regularizer https://link.springer.com/chapter/10.1007/978-3-642-33486-3_3\n\n[2] A Fair Classifier Using Mutual Information, https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9174293\n\n[3] INFOFAIR: Information-Theoretic Intersectional Fairness  https://arxiv.org/pdf/2105.11069.pdf\n\n[4] Can Information Flows Suggest Targets for Interventions in Neural Circuits? https://arxiv.org/abs/2111.05299\n\n[5]The Information Bottleneck Problem and Its Applications in Machine Learning  https://arxiv.org/pdf/2004.14941.pdf\n\n[6] https://blog.wouterkoolen.info/Pinsker/post.pdf\n\n**Continues 1/3**"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2342/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700192702512,
                "cdate": 1700192702512,
                "tmdate": 1700204952200,
                "mdate": 1700204952200,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9knLY24Kg2",
                "forum": "SBj2Qdhgew",
                "replyto": "eCLngYKtWy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2342/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2342/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**How Theorems and Lemmas are proved**\n\nWe have now briefly described a few proofs in the main paper and referred to the location of full proofs in the appendix. \n\nThe proof of Lemma 1 is non-trivial as it requires Pinsker\u2019s inequality.  Since Mutual information can be written in KL divergence, i.e., $I(Z; \\hat{Y}) =  KL(P(\\hat{Y}, Z)|| P(\\hat{Y})P(Z))$. We show that $TV(P(\\hat{Y}, Z), P(\\hat{Y})P(Z)) = 2 \\alpha (1-\\alpha)|SP|$, where $\\alpha = P(Z=0)$ (Complete proof in Appendix B).\n\nThe proof of Lemma 2 leverages writing the definition of conditional mutual information in terms of the joint and marginal probability distributions (Complete proof in Appendix B). \n\nFor the proof of Proposition 1, Global disparity decomposition (2) follows from the relationship between different PID terms, Local disparity decomposition (3) requires chain rule of mutual information. Showing non-negativity is challenging with some additional steps needed. Theorem 1 and 2  follow from Proposition 1 but particularly require non-negativity with additional steps.\n\nAs an example, in Theorem 1:\n    As local disparity $I(Z;\\hat{Y}|S) \\rightarrow 0$, then $\\text{Uni}(Z:\\hat{Y}|S) \\rightarrow 0$ and $\\text{Syn}(Z:\\hat{Y},S) \\rightarrow 0$, therefore the global disparity $I(Z;\\hat{Y}) \\rightarrow \\text{Red}(Z:\\hat{Y},S) \\geq 0$.\n\nTheorem 3 also follows from Proposition 1. However, the sufficiency step requires us to leverage PID of $I(Z;S)$ and the mutual information independence property.\n\nIn Theorem 4, using Proposition 1, the argument goes as follows.\n\nBy leveraging the PID of $I(Z;S|\\hat{Y}) = \\text{Uni}(Z:S|\\hat{Y})+\\text{Syn}(Z:\\hat{Y},S)$\n\nMarkov chain $Z - \\hat{Y} - S$ implies, $I(Z;S|\\hat{Y}) = 0 $. Hence, $ \\text{Syn}(Z:\\hat{Y},S)=0$. \n\nRest of proof follows from nonnegative property of PID terms: $I(Z;\\hat{Y}|S)=\\text{Uni}(Z:\\hat{Y}|S)  \\leq \\text{Uni}(Z:\\hat{Y}|S)  + \\text{Red}(Z:\\hat{Y},S)  = I(Z;\\hat{Y}).$\n\nIn the proof of Theorem 6, we show the AFGLOP is a convex optimization problem by showing the objective and mutual information constraints are convex functions on a convex set $\\Delta_p$. This required the log sum inequality and an observation that some conditional distributions are convex in the joint distribution $(Z,S,Y,\\hat{Y})$ (Complete proof in Appendix E). \n\n**Continues 2/3**"
                    },
                    "title": {
                        "value": "Response to Reviewer GNty"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2342/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700193651486,
                "cdate": 1700193651486,
                "tmdate": 1700204295949,
                "mdate": 1700204295949,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TYUHFhspWZ",
                "forum": "SBj2Qdhgew",
                "replyto": "eCLngYKtWy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2342/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2342/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**On the interesting observations from our experiments**\n\nWe are grateful for the reviewer's recognition of the thoroughness of our experiments.  Our experimental results reinforces our findings and highlight the various scenarios in which unique, redundant, and masked disparities can occur in a practical setting. Our information-theoretic framework provides a nuanced understanding of the three sources of disparity in FL. Our experiments offer insights into the agreement and disagreement between local and global fairness under various data distributions. Our experiments and theoretical results show that depending on the data distribution achieving one (global or local fairness) can often come at the cost of the other (disagreement), emphasizing the need for global and local fairness to be considered together when addressing group fairness problems in FL. The way data is distributed across clients significantly impacts the type of disparity (unique, redundant, masked) that dominates. Our optimization framework establishes the accuracy-fairness tradeoffs for a given dataset and client distribution. We have updated the discussions on Page 9. \n\n---\n We summarize more observations here (also added to Appendix F on Page 25):\n\nWhen data is uniformly distributed across clients, unique disparity is dominant and contributes to both global and local unfairness (see Figure 4 Scenario 1: model trained using FedAvg on the adult dataset and distributed uniformly across clients). In the trade-off Pareto Front  (Figure 3, Row 1), we see that both local and global fairness constraints have balanced tradeoffs with accuracy. The PID decomposition (Figure 4, Row 1, column 2,3,4) explains this as we see the disparity is mainly unique disparity, with little redundant or masked disparity. The unique disparity highlights where local and global disparity are in agreement. \n\nIn the case with sensitive attribute heterogeneity (sensitive attribute imbalance across clients). The disparity observed is mainly redundant disparity (see Figure 4 scenario 2 and middle), this is a globally unfair but locally fair model (recall proposition 1). Observe in the tradeoff plot (see Figure 3, Row 2) that the accuracy trade-off is with mainly global fairness (an accurate model could have zero local disparity but be globally unfair). \n\nIn the cases with sensitive-attribute synergy across clients. For example, in a two-client case (one client is more likely to have qualified women and unqualified men and vice versa at the other client). We observe that the mask disparity is dominant (see Figure 4 Scenario 3). The trade AGLFOP tradeoff plot (see Figure 3, Row 3) is characterized by Masked Disparity with trade-offs mainly between local fairness and accuracy (an accurate model could have zero global disparity but be locally unfair). \n\nThe AGLFOP provides the theoretical boundaries trade-offs, capturing the optimal performance any model or FL technique can achieve for a specified dataset and client distribution. For example, say one wants a perfectly globally fair and locally fair model, i.e., ($\\epsilon_g=0$, $\\epsilon_l=0$). Under high sensitive attribute heterogeneity (see Figure 3 Row 2), they cannot have a model that does better than 64% accuracy on the dataset.\n\n**3/3**"
                    },
                    "title": {
                        "value": "Response to Reviewer GNty"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2342/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700193743308,
                "cdate": 1700193743308,
                "tmdate": 1700204766265,
                "mdate": 1700204766265,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jO93xQ3MLn",
                "forum": "SBj2Qdhgew",
                "replyto": "TYUHFhspWZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2342/Reviewer_GNty"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2342/Reviewer_GNty"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your *very* thorough responses.\n\nI now see how your upper bound is quite tight (depending on mutual information).\n\nI appreciate your engagement with my restructuring suggestions.\n\nI understand that your experiments broadly give insight into the trade off between global and local fairness. I would love to see *specific* examples. Such as: \"In X data set, we see this trade off between synergistic and redundant information indicating Y concept which intuitively makes sense for Z reason.\"\n\nBased on your thorough responses and with the understanding that you'll add more specific examples such as the one above, I'll increase my rating to accept."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2342/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700685699122,
                "cdate": 1700685699122,
                "tmdate": 1700685699122,
                "mdate": 1700685699122,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BjZyW3z2GT",
            "forum": "SBj2Qdhgew",
            "replyto": "SBj2Qdhgew",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2342/Reviewer_rTkq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2342/Reviewer_rTkq"
            ],
            "content": {
                "summary": {
                    "value": "This work presents an information-theoretic perspective on group fairness trade-offs in federated learning (FL) with respect to sensitive attributes. This paper leverages partial information decomposition to identify three sources of unfairness in FL. They introduce AGLFOP, a convex optimization that defines the theoretical limits of accuracy and fairness trade-offs, identifying the best possible performance any FL strategy can attain given a dataset and client distribution."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper studies group fairness from an information theory perspective, which is valuable for the community to understand the group fairness of FL.\n2. The decomposition result is interesting."
                },
                "weaknesses": {
                    "value": "1. Although this paper proposes an optimization framework with Definition 5, it does not give any solution or algorithm for solving the problem. \n2. The experiments are somehow weak, both the baseline and dataset are rare. There are other works focusing on FL group fairness like [1,2], and also about fairness and accuracy tradeoffs, like [3], that should be compared.\n3. The visualization (table or figure) of accuracy and global-local fairness trade-off results is relatively insufficient relying solely on the Pareto Frontiers shown in Figure 3. \n4. It seems of vital importance to properly set the hyper-parameters $\\epsilon_g$ and $\\epsilon_L$ for the optimal trade-off. The acc-fairness Trade-off figure displayed in Figure 3, lacks discussion based on more experimental settings and datasets.  \n5. The experiment setting details are not clear, for example, what is the used model and parameter settings?\n\n[1]Ezzeldin Y H, Yan S, He C, et al. Fairfed: Enabling group fairness in federated learning[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2023, 37(6): 7494-7502.\n[2]Papadaki A, Martinez N, Bertran M, et al. Minimax demographic group fairness in federated learning[C]//Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency. 2022: 142-159.\n[3]Wang L, Wang Z, Tang X. FedEBA+: Towards Fair and Effective Federated Learning via Entropy-Based Model[J]. ICLR 2023 Workshop ML4IoT.\n\nMinors:\nWhat is the formal definition of global fairness and local fairness?"
                },
                "questions": {
                    "value": "1. Could you explain the main difference from [4]?  It seems it is a trivial improvement (Apply the PID analysis on FL) compared with this paper.\n2. Could you provide experiments of different baselines and different datasets, in the FL setting (partial client participation of cross-device FL.)\n3. Could you provide more results for trade-offs on accuracy and global-local fairness?\n4. Could you provide more details about the selection of hyperparameters to ensure optimal trade-off strategy under different data distributions and datasets?\n\n[4] Dutta S, Hamman F. A Review of Partial Information Decomposition in Algorithmic Fairness and Explainability[J]. Entropy, 2023, 25(5): 795."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2342/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2342/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2342/Reviewer_rTkq"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2342/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698767378142,
            "cdate": 1698767378142,
            "tmdate": 1700751716455,
            "mdate": 1700751716455,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZZCMWQd7Rr",
                "forum": "SBj2Qdhgew",
                "replyto": "BjZyW3z2GT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2342/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2342/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer rTkq"
                    },
                    "comment": {
                        "value": "We thank the reviewer for reviewing our paper! \n\n---\n\n**Clarification on our contributions**\n\nWe want to highlight the main contribution of this paper is to provide a **theoretical**  perspective to global and local fairness trade-offs in FL. We use PID to decompose global & local disparity into three sources of unfairness: Unique Disparity, Redundant Disparity, and Masked Disparity. This decomposition separates out the regions of agreement and disagreement of local and global disparity, demystifying their trade-offs. We also formulate a convex optimization framework for quantifying accuracy-global and local fairness trade-offs identifying the best possible performance any FL strategy can attain given a dataset and client distribution.  Our aim is not to provide a state-of-the-art algorithm but to provide a nuanced understanding of the sources of disparity in FL, which can inform the use of disparity mitigation techniques and their effectiveness when deployed in practice. We provide a comprehensive experimental section with various settings of data distributions across clients to support our claims (also acknowledged by reviewer GNty).\n\n---\n\n**Regarding the comparison with the works [1,2, 3]**\n\nOur paper's aim diverges from these works. While [1,2] predominantly focuses on mitigating unfairness in FL, our work delves into theoretically exploring the fundamental limitations on the trade-offs between accuracy-global and local fairness. Our work establishes the theoretical limits of what any FL technique can achieve in terms of accuracy and fairness given a dataset and client distribution.\n\nSimilarly in [3], the definition and scope of **fairness** differ substantially from our work (we highlight the different notions of fairness in FL in the related works section and have cited [3]). [3] concentrates on **client fairness**, emphasizing model performance is consistent across clients. In contrast, our work is centered around **group fairness**, particularly addressing model discrimination against sensitive attributes (e.g, race, gender). \n\n---\n\n**Clarification on Accuracy-Global and Local Fairness Tradeoff  (W2, W4, Q3, Q4)**\n\nWe introduce the Accuracy and Global-Local Fairness Optimality Problem (AGLFOP) as an optimization to delineate the theoretical boundaries of accuracy and fairness trade-offs, capturing the optimal performance any model or FL technique can achieve for a specified dataset and client distribution.\n\nWe aim to find the maximum achievable accuracy, quantified as $1 - c(Q)$, within the context of specific constraints on global and local disparities in fairness ($I(Z;\\hat{Y}) \\leq \\epsilon_g$) for global disparity and \\($I(Z;\\hat{Y}|S) \\leq \\epsilon_l$) for local disparity. Here, $\\epsilon_g$ and $\\epsilon_l$ are fairness relaxation constants and not hyperparameters.\nFor example, consider the scenario where both $\\epsilon_l = 0$ and $\\epsilon_g = 0$. This represents a condition of perfect fairness both locally and globally. In such a case, our objective is to determine the highest level of accuracy that a model can achieve while adhering to these ideal global and local fairness constraints.  \n\n---\n\n**W1: Solution and algorithm for solving optimization**  \n\nWe show that the AGLFOP is a convex optimization problem and hence can be solved efficiently using standard convex solvers (which are well-established and robust, to find globally optimal solutions efficiently). This optimization problem can be computed in any FL environment. Specifically, their computation necessitates the characterization of the joint distribution $\\Pr(Z{=}z,S{=}s,Y{=}y)= \\Pr(S{=}s)\\Pr(Z{=}z|S{=}s)\\Pr(Y{=}y|Z{=}z,S{=}s)$, which can be acquired by aggregating pertinent statistics across all participating clients. For instance, $\\Pr(S=s)$ denotes the proportion of data at client $s$, $\\Pr(Z=z|S=s)$ signifies the fraction of individuals with sensitive attribute $z$ at client $s$, and $\\Pr(Y=y|Z=z,S=s)$ represents the proportion of individuals labeled with true label $y$ among those with sensitive attribute $z$ at client $s$. \n\n---\n\n[1] Fairfed: Enabling group fairness in federated learning, arxiv.org/abs/2201.08304\n\n[2] Minimax demographic group fairness in federated learning, arxiv.org/abs/2201.08304\n\n[3]  Towards Fair and Effective Federated Learning via Entropy-Based Model, arxiv.org/abs/2301.12407\n\n**Continues 1/2**"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2342/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700191331305,
                "cdate": 1700191331305,
                "tmdate": 1700205049481,
                "mdate": 1700205049481,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Au978CyyCy",
                "forum": "SBj2Qdhgew",
                "replyto": "BjZyW3z2GT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2342/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2342/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**W3, Q3: Visualization of accuracy and global-local fairness trade-off using Pareto Frontiers**\n\nThe use of Pareto Frontiers is a well-established method for trade-off analysis in optimization, offering a clear and concise representation of the optimal trade-offs between competing objectives. In Figure 3, we provide the AGLFOP Pareto Frontiers showing maximum accuracy (shown as contour) that can be achieved on a dataset and client distribution for a given global and local fairness various relaxation $\\epsilon_g$ ($x$-axis) and $\\epsilon_l$ ($y$-axis) respectively. This helps visualize the effects of global and local fairness constraints on the accuracy of the model (i.e., Accuracy-global local fairness tradeoff). This method allows for a comprehensive overview of the optimal solutions, facilitating informed decision-making about the trade-offs that are inherent to the model's performance under fairness constraint.\n\n\n---\n\n**W5: On experimental setting details; model and parameter settings**\n\n Due to lack of space, we provided detailed descriptions of our experimental setup, including model specifications and parameter settings, in Appendix F2 of our paper. \n\n---\n\n**Formal Definition of Global Fairness and Local Fairness**\n\nGlobal fairness is the overall disparity of the model across all clients while local fairness is the disparity of the model at each client. We introduce formal mathematical measures of global fairness in Definition 1 (Global Disparity) and local fairness in Definition 2 (Local Disparity).\n\n---\n\n**Q1: Difference between Survey Paper and our Paper**\n\nIt is important to note that [1] is a survey paper that surveys other papers that apply partial information decomposition (PID) in algorithmic fairness and explainability and provides a taxonomy. PID is a mathematical tool whose origin goes back to [2] and has been used previously in neuroscience. Our contribution is in leveraging PID specifically for the setup of group fairness in FL and using it to decompose disparities as well as formulate a novel convex optimization problem to characterize tradeoffs.\n\n---\n\n**Q2: On partial client participation of cross-device FL**\n\nOur approach does not make specific assumptions about the training process of the federated learning model, which allows our results to be applicable to scenarios involving partial client participation in cross-device federated learning methods as well.  This flexibility is a significant aspect of our contribution, as it allows our methods to be adapted and applied in diverse federated learning contexts.\n\n---\n\n[1] Dutta S, Hamman F. A Review of Partial Information Decomposition in Algorithmic Fairness and Explainability.\n\n[2] Nonnegative Decomposition of Multivariate Information, https://arxiv.org/abs/1004.2515 \n\n**2/2**"
                    },
                    "title": {
                        "value": "Response to Reviewer rTkq"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2342/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700191516240,
                "cdate": 1700191516240,
                "tmdate": 1700205225711,
                "mdate": 1700205225711,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "op7UB2bKdB",
                "forum": "SBj2Qdhgew",
                "replyto": "ZZCMWQd7Rr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2342/Reviewer_rTkq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2342/Reviewer_rTkq"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your rebuttal"
                    },
                    "comment": {
                        "value": "I would like to thank the authors for their detailed explanations. Now I understand that the solution is based on the fact that the proposed AGLFOP is a convex problem and the roles of $\\epsilon_g$ and $\\epsilon_L$ are the relaxation constants instead of hyperparameters. It is also nice to see the discussion on $\\Delta_p$ involved in AGLFOP has been added in the revised manuscript.\n\nAs this paper is centered around group fairness, is it possible to show the superiority of AGLFOP compared with some baselines of group fairness in the experiments?"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2342/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700546443746,
                "cdate": 1700546443746,
                "tmdate": 1700546443746,
                "mdate": 1700546443746,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "T3izjlHf5g",
                "forum": "SBj2Qdhgew",
                "replyto": "BjZyW3z2GT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2342/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2342/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for appreciating our rebuttal!\n\n*The goal of the AGLOP is to theoretically study the tradeoff between accuracy and global local fairness for a specified dataset and client distribution rather than provide a state-or-the-art algorithm*. To the best of our knowledge, there are no suitable baselines to compare with that quantify such fundamental tradeoffs between local and global fairness. Nonetheless, we ran some methods that attempt to minimize only the global fairness on the Adult dataset. For these methods, we measure the global and local disparities for various data splits and see that the accuracy achieved by the models is lower than the maximum accuracy limit for the given local and global disparity relaxation using our proposed optimization. We can include a remark regarding this observation in the paper as recommended by the reviewers.\n\n\n\n\n\n\n\n\n| Method                | **Local Disparity** | **Global Disparity** | **Accuracy** |**AGLFOP**             |\n|-----------------------------|---------------------|----------------------|------------------|-----------------------|\n| FedFB (scenario 1)             | 0.0249     \t   |   0.0241            | 81%                | 86%       \t|\n| FedFB (scenario 1)             | 0   \t\t  |   0      \t       | 75%         | 82%     \t\t |\n| FairBatch FL (scenario 1)             | 0.0019              | 0.0019               | 81%            | 83%                  |\n| FedAvg (scenario 2)     | 0.0180              | 0.0335               | 84%               | 86%                |\n| FedAvg  (scenario 3)                 | 0.0148              | 0.0067     | 76%           | 78%      |\n\nScenario 1:  sensitive-attribute independently distributed across clients\n\nScenario 2: high sensitive-attribute heterogeneity across clients ($\\alpha = 0.67$)\n\nScenario 3: high sensitive-attribute synergy level across clients ($\\lambda = 0.90$)\n\n**Methods**\n\nFedAvg (+Local Fair Training): Communication-Efficient Learning of Deep Networks from Decentralized Data, https://proceedings.mlr.press/v54/mcmahan17a.html\n\nFedFB: Improving Fairness via Federated Learning, https://arxiv.org/abs/2110.15545\n\nFairBatch FL (FairBatch Training + Ensemble): FairBatch: Batch Selection for Model Fairness, https://arxiv.org/abs/2012.01696"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2342/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700713720090,
                "cdate": 1700713720090,
                "tmdate": 1700714637494,
                "mdate": 1700714637494,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "sbAhdaeStC",
            "forum": "SBj2Qdhgew",
            "replyto": "SBj2Qdhgew",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2342/Reviewer_P5hg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2342/Reviewer_P5hg"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces information theoretic tools to interpret the relationship among multiple group fairness trade-offs in federated learning. It is commonly known that global and local fairnesses both contribute to unfairness in federated learning, but their relationship (e.g. whether one implies the other) is unknown. The authors identify three fundamental sources of unfairness, and utilize them to derive fundamental limits on the trade-offs between global and local unfairnesses."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "This paper provides a novel \"solution\" for unfairness in federated learning. Even if this issue has been extensively studied, there has been few work attempting to investigate the fundamental root that causes unfairness, let alone giving a theoretical explanation. This work uses a mathematically rigorous tool to give a promising attempt to explain unfairness. The theoretical justifications are rigorous and insightful.\n\nIn particular, Theorems 1, 2 and 3 are both conclusive and powerful, so that one may predict the fairness performances based upon those three sources of unfairness."
                },
                "weaknesses": {
                    "value": "Overall this paper is well-written, but in Section 2, it would be great if the authors could provide some more justifications for Definitions 1 and 2, both mathematically and conceptually, even if the definitions are indeed fairly intuitive. This may be immensely helpful for readers especially those who do not have a strong background in information theory."
                },
                "questions": {
                    "value": "1. This was mentioned in the Weaknesses section, and I would be very interested in seeing more explanations for choosing those definitions.\n\n2. Under a concrete data set, how are Uni(), Red(), and Syn() efficiently computed? I might be wrong, but my first impression is that, since they are relevant with mutual information, such computation may be expensive?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2342/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698793093829,
            "cdate": 1698793093829,
            "tmdate": 1699636166307,
            "mdate": 1699636166307,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kFp352wSOT",
                "forum": "SBj2Qdhgew",
                "replyto": "sbAhdaeStC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2342/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2342/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer P5hg"
                    },
                    "comment": {
                        "value": "We are grateful to the reviewer for appreciating our contributions. We thank the reviewer for their valuable feedback regarding the definitions and computations of our measures. We discuss these below and have updated the manuscript (see Appendix A on Page 14).\n\n---\n\n**Justifications for Definitions 1 and 2 (mathematically and conceptually)**\n\nLet's consider a case involving three banks training a federated model for credit scoring: Bank 1, predominantly serving men; Bank 2, catering to a diverse demographic; and Bank 3, primarily serving women. Global fairness means that the model doesn\u2019t discriminate against any protected group when evaluated across all client (Bank) data. Meanwhile, local fairness means fairness within each client\u2019s data (fairness at each Bank). \n\nIn this work, we use mutual information as a measure of the unfairness or disparity of a model. \nMutual Information has been interpreted as the dependence between sensitive attribute $Z$  and model prediction $\\hat{Y}$ (captures correlation as well as all non-linear dependencies). Mutual information is zero if and only if $Z$ and $\\hat{Y}$ are independent. This means that if the model\u2019s predictions are highly correlated with sensitive attributes (like gender or race), that\u2019s a sign of unfairness. Mutual information has been explored in fairness in the context of centralized machine learning in [1,2,3]. \n\nA recent work [5] provides another interpretation of mutual information $I(Z; \\hat{Y})$ in fairness as the accuracy of predicting $Z$ from $\\hat{Y}$  (or the expected probability of error in correctly guessing $Z$ from $\\hat{Y}$). Even in information bottleneck literature [6], mutual information has been interpreted as a measure of how well one random variable predicts (or, aligns with) the other. \n\nFor local fairness, we are interested in the dependence between model prediction $ \\hat{Y} $ and sensitive attributes $Z$ at each and every client, i.e., the dependence between $\\hat{Y}$ and $Z$ conditioned on the client $S$. For example, the disparity at client $S=1$ (Bank 1) is $I(Z; \\hat{Y}|S=1)$ (the mutual information (dependence) between model prediction and sensitive attribute conditioned on client $S=1$ (considering data at client $S=1$). Our measure for local disparity (Definition 2) is the conditional mutual information (dependence) between $Z$ and $\\hat{Y}$ conditioned on $S$, denoted as $I(Z;\\hat{Y}|S)$. Local disparity $I(Z;\\hat{Y}|S) =\\sum_s p(s)I(Z; \\hat{Y}|S=s)$, is an average of the disparity at each client weighted by the $p(s)$, the fraction of data at client $S=s$.  The local disparity is only zero if and only if all client has zero disparity in their local dataset.\n\nWhile other works [3] have used mutual information measures to enforce statistical parity, we are the first to bring Prinsker\u2019s Inequality [4] in the context of fairness to show that the statistical parity gap is actually upper bounded by the square root of mutual information (see Lemma 1). Though this is not the main result of the paper, it further justifies the use of mutual information measures to study global and local fairness in federated learning. \n\nWe thank the reviewer for their valuable feedback on making our work accessible to readers without strong backgrounds in information theory. We have included a brief background on concepts and definitions in Appendix A on Page 14.\n\n\n\n\n[1]Fairness-Aware Classifier with Prejudice Remover Regularizer https://link.springer.com/chapter/10.1007/978-3-642-33486-3_3\n\n[2] A Fair Classifier Using Mutual Information, https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9174293\n\n[3] INFOFAIR: Information-Theoretic Intersectional Fairness  https://arxiv.org/pdf/2105.11069.pdf\n\n[4] https://blog.wouterkoolen.info/Pinsker/post.pdf\n\n[5] Can Information Flows Suggest Targets for Interventions in Neural Circuits? https://arxiv.org/abs/2111.05299\n\n[6]The Information Bottleneck Problem and Its Applications in Machine Learning  https://arxiv.org/pdf/2004.14941.pdf\n\n\n**Continues 1/2**"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2342/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700190067841,
                "cdate": 1700190067841,
                "tmdate": 1700205073966,
                "mdate": 1700205073966,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qCN1nQcmc6",
                "forum": "SBj2Qdhgew",
                "replyto": "sbAhdaeStC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2342/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2342/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**On the Computation of Uni, Red, Syn**\n\nThe computation of Uni, Red, Syn disparity necessitates the characterization of the joint distribution $\\Pr(Z{=}z,S{=}s,\\hat{Y}{=}\\hat{y})=\\Pr(S{=}s)\\Pr(Z{=}z|S{=}s)\\Pr(\\hat{Y}{=}\\hat{y}|Z{=}z,S{=}s)$, which can be acquired by aggregating pertinent statistics across all participating clients.  For instance, $\\Pr(S=s)$ denotes the proportion of data at client $s$, $\\Pr(Z=z|S=s)$ signifies the fraction of individuals with sensitive attribute $z$ at client $s$, and $\\Pr(\\hat{Y}=\\hat{y}|Z=z,S=s)$ represents the proportion of individuals with predictived label $\\hat{y}$ among those with sensitive attribute $z$ at client $s$. \n\n\nThe global and local disparity is first measured empirically using the classical definition of mutual information, i.e., $ I(Z; \\hat{Y}) = \\sum_{z , \\hat{y} } p(z, \\hat{y}) \\log \\frac{p(z, \\hat{y})}{p(z)p(\\hat{y})}$ and conditional mutual information, i.e.,  ${ I(Z; \\hat{Y} | S) = \\sum_{s, z , \\hat{y}} p(s, z, \\hat{y}) \\log \\frac{p(z, \\hat{y} | s)}{p(z | s)p(\\hat{y} | s)}}$. For the Unique disparity, we use Definition 3 which solves a convex optimization problem over the joint distribution $(Z,\\hat{Y}, S)$ keeping the marginals over $(Z,\\hat{Y}) $ and $(Z, S)$ fixed (we use an implementation from the Discrete Information Theory [1] python package) as shown in [2].\n\nDefining any one of the PID terms suffices to get the others. $\\text{Red}(Z:A, B)$ is the sub-volume between $I(Z;A)$ and $I(Z;B)$ (see Fig.1). Hence,  $\\text{Red}(Z:\\hat{Y}, S) = I(Z;\\hat{Y}) - \\text{Uni}(Z: \\hat{Y}| S)$ and $\\text{Syn}(Z:\\hat{Y},S) =  I(Z; \\hat{Y}, S) - \\text{Uni}(Z: \\hat{Y}|S) -\\text{Uni}(Z: S|\\hat{Y})-\\text{Red}(Z:\\hat{Y}, S)$ (from Equation 1). \n\nThis estimation largely depends on: \n\n(i) the empirical estimators of the probability distributions; and\n\n (ii) the efficiency of the convex optimization algorithm used for calculating the unique information. \n\nConvex optimization problems are generally well-studied and have efficient solvers. In our experiments on the Adult dataset with $2$,$5$, and $10$ federated clients, estimating the disparity terms was not computationally expensive.  However, the method's computational cost can increase with the number of clients or sensitive attributes. We have highlighted this limitation in the paper (see Discussion on Page 9). There's also emergent interest in the estimation of these measures using Gaussian relaxations [3] or neural networks [4,5]. \n\n\n[1] dit: a Python package for discrete information theory, doi.org/10.21105/joss.00738\n\n[2] Quantifying unique information, arxiv.org/abs/1311.2852\n\n[3] Partial Information Decomposition via Deficiency for Multivariate Gaussians, arxiv.org/abs/2105.00769\n\n[4] MINE: Mutual Information Neural Estimation, arxiv.org/abs/1801.04062\n\n[5] Redundant Information Neural Estimation, www.mdpi.com/1099-4300/23/7/922\n\n\n **2/2**"
                    },
                    "title": {
                        "value": "Response to Reviewer P5hg"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2342/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700190429382,
                "cdate": 1700190429382,
                "tmdate": 1700205111405,
                "mdate": 1700205111405,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]