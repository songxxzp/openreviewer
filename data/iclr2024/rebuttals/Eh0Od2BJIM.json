[
    {
        "title": "HyperAttention: Long-context Attention in Near-Linear Time"
    },
    {
        "review": {
            "id": "InJIAFIMGD",
            "forum": "Eh0Od2BJIM",
            "replyto": "Eh0Od2BJIM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2706/Reviewer_xXnD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2706/Reviewer_xXnD"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a method to reduce the computation complexity of transformers down from quadratic in squence length to near liner. The proposed idea proposes to perform sampling to approximate Attention. The authors claim that compared to prior works there proposed idea is practical and is able to outperform existing approaches.\n\nThe core idea is to perform sampling of the value matrix based on the norms and the \\sigma(QK^T) matrix to come up with important entries."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. I think this a resonably effective idea. The bounds provided are kind of standard and the paper is well structured.\n2. The idea of separating into multiple matrix when performing causal masking is very interesting. I think this is a very nice contribution."
                },
                "weaknesses": {
                    "value": "Before highlighting the weaknesses I would like to point out where my criticism's are coming from. More often then not in Machine Learning approximation results, I have personally observed that the approximation often takes more time than the actual operation, often due to hardware constraints, which makes the approach moot and impractical. Stating this most my questions are coming from a point of view to discern how realistic will these speedups be in real world. I hope the authors are not too bothered by my naive questions.\n\n1. My first question is regarding evaluation. Are the speedups proposed end to end. As in the runtime reported in Figure 3(a) and 3(b) Figure 4 include the run time for generating the mask M_h . The reason I am a bit skeptical is that approximation using Algoirthm 1 does not look very GPU friendly to my naive understanding. And you have mentioned in your contributions (On Page 3) \"We assume these procedures are fast, and that after removing the heavy entries, two parameters in the resulting attention matrix are small: (1) the max column l1-norm, and (2) the ratio of row norms in the un-normalized attention matrix.\" This makes me wonder if you include the runtime. I would ideally love to see a breakdown of the time spent in different procedures as that will help new and unfamiliar readers to understand how with your novel approach bottlenecks have shifted and where the new bottlenecks are.\n\n2. For your proofs you assume the following - \"To be more precise, we assume that for any i \u2208 [n] there exists some \u03b1 = no(1) such that D\u22121A \u00b7 e(i) 2 \u2264 \u03b1n .\" Can you verify this assumption experimentally. I have generally found a proof to stand the test of time a bit more if the assumptions have some sort of experimental verification.\n\n3. Do the authors have some understanding if there is better way of choosing the number of layers to use for their proposed \"HyperAttention\" mechansim. It currently feels arbitary to choose these layers. It would be interesting to understanding why it works in certain cases and where it doesn't.\n\n4. The other thing I found missing is the \"m\" value (Algorithm 2). Can authors enumerate the \"m\" values used in their setup. Is it based on the bound or is it another hyper-parameter. Especially for long sequence results, since the accuracy can not be verified there it would be interesting to see the parameters which authors have used.\n\n\nIn general I found the paper easy to follow and appreciate the various figure the authors have included. I like the experiments which authors have performed and showed how on long sequence length their method is more useful. I am happy to bump up the score to a weak accept or accept based on authors reponses. Nice work !!."
                },
                "questions": {
                    "value": "See weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2706/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2706/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2706/Reviewer_xXnD"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2706/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698780052039,
            "cdate": 1698780052039,
            "tmdate": 1699636212307,
            "mdate": 1699636212307,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wea3M2mFO5",
                "forum": "Eh0Od2BJIM",
                "replyto": "InJIAFIMGD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2706/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2706/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors"
                    },
                    "comment": {
                        "value": "> 1. My first question is regarding evaluation. ... and where the new bottlenecks are.\n\n- All runtimes in our results are end-to-end. Although in the right-most figure in Figure 1 the sparsified attention by ${\\mathbf {M}}^{\\mathcal{H}}$ seems to be computationally inefficient and not parallelizable, we follow the operations in the middle figure in practice, which does not compute the mask ${\\mathbf  M}^{\\mathcal{H}}$ explicitly. More specifically, rows in $\\mathbf{Q}$ and $\\mathbf{K}$ are sorted using Hamming LSH, then they are partitioned into \u201c$b$\u201d (i.e., the block size in Figure 1) groups. \nFinally,  the i-th set of rows in $\\mathbf{Q}$ is multiplied by the corresponding set in $\\mathbf{K}$, resulting in a block-diagonal approximation of $\\mathbf{A}_{P_Q, P_K}$ . The required operations involve permuting $n$ rows, reshaping tensors, and small matrix multiplications. Since every batch, head and block has the same configurations, the implementation can be parallelized using GPUs. We have added more details in the updated submission.\n\n- The two parameters $\\alpha$ and $\\kappa$ are used to analyze the runtime of our algorithm but they are not required to compute in our practical algorithm."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2706/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700516134290,
                "cdate": 1700516134290,
                "tmdate": 1700516134290,
                "mdate": 1700516134290,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PFeSK7Frsb",
                "forum": "Eh0Od2BJIM",
                "replyto": "InJIAFIMGD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2706/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2706/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors"
                    },
                    "comment": {
                        "value": "> 2. For your proofs you assume the following  ... the assumptions have some sort of experimental verification.\n\n- We additionally conducted experiments to verify our assumption. For the attention with non-causal masking, we use the T2T-ViT model and take query, key and values from its first attention layer on the ImageNet test data set. Then, we compute $\\alpha$ to be the maximum of the squared $\\ell_2$-norms of the columns in $\\mathbf{D}^{-1} \\mathbf{A}$. The sequence length of T2T-ViT is $n=3136$ and the average value of $\\alpha$ is observed to be 8.1801, which can be sublinear in $n$. \n\n- To further investigate the dependence on $n$, we choose the ChatGLM and longbench narrative-qa dataset used in the submission and change the sequence length n from 1k to 9k. As a result, we observe that the value of $\\alpha$ increases sub-linearly in $n$, as below. This supports the claim that our assumption holds in practice. We have added these results in our updated submission.\n\n  | $n$  |$\\alpha / n$|\n  | :------------- |:-------------:|\n  | 1024      |  0.09170  |\n  | 2048      |  0.06379  |\n  | 3072      |  0.05395  |\n  | 4096      |  0.04817  |\n  | 5120      |  0.04559  |\n  | 6144      |  0.04128  |\n  | 7168      |  0.03918  |\n  | 8192      |  0.03694  |\n  | 9216      |  0.03601  |"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2706/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700516954233,
                "cdate": 1700516954233,
                "tmdate": 1700516954233,
                "mdate": 1700516954233,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FETiylFKE8",
                "forum": "Eh0Od2BJIM",
                "replyto": "InJIAFIMGD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2706/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2706/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors"
                    },
                    "comment": {
                        "value": "> 3. Do the authors have some understanding ... It would be interesting to understanding why it works in certain cases and where it doesn't.\n\n- In our experiments, we choose to replace the final L-layers where L changes from 1 to the total number of layers in the given pretrained Transformers, e.g.,  [1]. We motivate this setting from fine-tuning methods where the last few layers are updated. We believe that finding better configurations of layers to approximate is a very interesting future problem.\n\n  [1] Chen, Boqi, Fandi Yi, and D\u00e1niel Varr\u00f3. \"Prompting or Fine-tuning? A Comparative Study of Large Language Models for Taxonomy Construction.\" https://arxiv.org/pdf/2309.01715.pdf."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2706/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700517080161,
                "cdate": 1700517080161,
                "tmdate": 1700517080161,
                "mdate": 1700517080161,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "H2JjqhVQT0",
                "forum": "Eh0Od2BJIM",
                "replyto": "InJIAFIMGD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2706/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2706/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> 4. The other thing I found missing is the \"m\" value (Algorithm 2). Can authors enumerate the \"m\" values used in their setup. Is it based on the bound or is it another hyper-parameter. Especially for long sequence results, since the accuracy can not be verified there it would be interesting to see the parameters which authors have used.\n\n- In our experiments, we consider the value \u201c$m$\u201d to be a hyper-parameter, which corresponds to the number of columns in $\\mathbf{D}^{-1} \\mathbf{A}$ selected uniformly at random. Motivating our analysis that m can be sublinear in \u201c$n$\u201d, we fix this value to be 256 for all the experiments. We observe that a larger value of m shows a better performance on LLMs (e.g., perplexity) but it makes our algorithm slower."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2706/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700517141465,
                "cdate": 1700517141465,
                "tmdate": 1700517141465,
                "mdate": 1700517141465,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IadpCXewid",
                "forum": "Eh0Od2BJIM",
                "replyto": "InJIAFIMGD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2706/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2706/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Discussion ends in 2 days"
                    },
                    "comment": {
                        "value": "Dear reviewer,\n\nSince you indicated that you were\n\n> happy to bump up the score to a weak accept or accept based on authors reponses. \n\nwe wanted to inquire whether you were satisfied with our responses. If not, please let us know soon so that we could provide more details or address your other questions/concerns before the discussion period ends soon.\n\nThank you again for your time!"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2706/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700622075834,
                "cdate": 1700622075834,
                "tmdate": 1700622075834,
                "mdate": 1700622075834,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rW2xAnaqRc",
            "forum": "Eh0Od2BJIM",
            "replyto": "Eh0Od2BJIM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2706/Reviewer_NFiG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2706/Reviewer_NFiG"
            ],
            "content": {
                "summary": {
                    "value": "Authors propose a new LSH-based approximation algorithm for attention layer, called HyperAttention. Similar to KDEformer, authors aim to bound the operator norm of the error, and this is done by two-steps: approximating the normalization matrix, and the attention matrix. Both approximations are primarily based on sortLSH, as with KDEformer, although authors generalize the analysis to be applicable to other sketching algorithms. Subquadratic runtime is proved. For empirical experiments, authors evaluate with LongBench tasks, perplexity on LongBench. Wall-clock time of HyperAttention on 131k sequence is also measured."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Significance: This paper provides a new theoretically-grounded approximation algorithm for attention layers. The algorithm doesn't rely on kernel density estimation, and the analysis is more straightforward to associate the assumed properties of attention matrix with theoretical guarantees. Hence, future work will find it useful to build upon this work to adapt assumptions and derive new results. \n\nOriginality: Although the algorithm is based on the same primitive sortLSH, as prior work KDEformer did, the actual algorithm is new. Also, authors generalize to other sketching algorithms as well, which makes a stronger connection to sketching literature.\n\nClarity: The overall proof strategy is clearly described. High-level intuition and interpretation of proof parameters are explained such that readers shall follow the main logic without having to read the proof. However, intuitive explanation of algorithms is mostly reserved to appendix, which is unfortunate but understandable given page constraints.\n\nQuality: I wasn't able to closely verify proofs, but overall proof strategy of the work seems sound."
                },
                "weaknesses": {
                    "value": "Empirical evaluations of this paper is not aligned well with prior work, which makes it difficult for readers to understand the practical usefulness of the proposed approach. For example, prior work KDEformer evaluates on BigGAN, ImageNet Classification, and Long Range Arena Benchmark, comparing against not only exact baseline but stronger baselines such as Reformer/Performer. While HyperAttention shared many similarity with KDEformer and claims to be more practical due to not using KDE, HyperAttention is not compared against KDEformer, making it difficult to see whether the difference between to actually make a practical difference. Also, end-to-end training experiment is lacking, and only forward/backward computation time was evaluated.\n\nAlthough HyperAttention shares the overall proof strategy with KDEformer, they differ significantly in assumptions they make. I believe it is potentially a strength, as HyperAttention shall make more realistic assumptions and strengthen guarantees. However, it wasn't clear how HyperAttention's assumptions relate to KDEformer. A more detailed discussion of comparison between KDEformer and HyperAttention will clarify the theoretical contribution of this paper despite sharing the overall proof strategy."
                },
                "questions": {
                    "value": "Eq (10) in Section 1.2 probably mean Eq(1).\n\nIn equation (5) in Appendix A, which result are authors using to prove this?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2706/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698909449883,
            "cdate": 1698909449883,
            "tmdate": 1699636212201,
            "mdate": 1699636212201,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CXs1LjIHnP",
                "forum": "Eh0Od2BJIM",
                "replyto": "rW2xAnaqRc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2706/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2706/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors"
                    },
                    "comment": {
                        "value": "> Empirical evaluations of this paper is not aligned well with prior work, .... A more detailed discussion of comparison between KDEformer and HyperAttention will clarify the theoretical contribution of this paper despite sharing the overall proof strategy.\n\n- We thank the reviewer for their insightful comments. As the reviewer points out, HyperAttention is much simpler and more streamlined than the KDEformer. Particularly, the KDEformer requires a kernel density estimation (KDE) algorithm which is impractical and fairly hard to implement in general, in particular, with GPUs. We streamline such a complicated KDE to a simple approximation via sortLSH and uniform sampling. The assumptions we use are for guaranteeing that our uniform sampling approach can well-approximate the kernel density. We empirically observe that these assumptions hold in practice (please refer to our responses to reviewer **xXnD**).\n\n\n- Additionally, KDEformer does not support **causal masking** and this might be the reason why it mostly works on vision transformers, which does not require causal masking. In this work, we propose a novel method (HyperAttention) to support causal masking, and it allows us to apply it to causal language models. Furthermore, HyperAttention can be implemented in a modular way,  thereby allowing us to utilize other accelerated self-attention algorithms such as FlashAttention. This makes the gap in runtimes between HyperAttention and KDEformer even larger. \n\n- To see the practical differences between HyperAttention and KDEformer, we conduct ImageNet classification experiments with the T2T-ViT model. We apply HyperAttention and KDEformer to the first attention layer because it is the main computational bottleneck. The sequence length is 3136 and we set all hyperparameters in both methods so that they have the same memory and time complexity. The per-image runtime of KDEformer in the attention layer was 13.175 ms while HyperAttention shows 8.634 ms runtime (x1.53 faster). The test accuracies of KDEformer and HyperAttention were 81.11% and 81.14%, respectively, which are the same within a standard deviation. \n\n> Eq (10) in Section 1.2 probably mean Eq(1).\n\n- We appreciate your bringing attention to this typo. Equation (1) is indeed correct and we have corrected it in our updated submission.\n\n> In equation (5) in Appendix A, which result are authors using to prove this?\n\n- Equation (5) shows an upper bound on the number of rows for which the row sum is greater than $\\tau$. To derive this bound, we make use of the fact that tau is the maximum of the row sums of a random subset of rows, denoted by T. Recall that $|T|=m$. Now note that the probability that the size of the set $S_0$ is greater than $t$ for some $t$, equals $\\Pr[|S_0| \\ge t] \\le (1-t/n)^{m}$. Plugging $m=\\Omega(\\kappa^7 \\alpha^2 \\varepsilon^{-6} \\log n)$ and $t=O(\\kappa^{-7} \\alpha^{-2} \\varepsilon^6 n)$ gives Equation (5) with probability $1-1/\\text{poly}(n)$. We have updated this in our submission file."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2706/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700514680691,
                "cdate": 1700514680691,
                "tmdate": 1700514680691,
                "mdate": 1700514680691,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2dEEyPmpgY",
            "forum": "Eh0Od2BJIM",
            "replyto": "Eh0Od2BJIM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2706/Reviewer_cCwA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2706/Reviewer_cCwA"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new approach of accelerating the computation of attention layer. The proposed method has near linear time (under plausible assumption on column norm) and demonstrates significant advantage in experiments.\n\nThe attention layer in Transformer requires $\\Omega(n^2)$ computation given input sequence of length $n$, this costs is prohibitive for long sequence and there are many recent work on reducing this computation cost. The paper proposes a new approach that uses near linear time, when the column norm of the attention matrix (i.e., $\\exp(KQ)$, where $K$ is the key matrix and $Q$ is the query matrix at attention layer) is small (or balance). This is a challenging task as one needs to take account into the softmax function. The algorithm first uses LSH to identify large entries, then apply fast approximate matrix multiplication by random sampling rows.\n\nThe paper also performs empirical study on real word dataset. When the sequence length is roungly 30k, the inference time decreases roughly 50% with a slight increase of perplexity (roughly 20% -- 50%, depend on the dataset and the number of layer replaced). The speedup is significant for longer sequence with one single transformer layer (up to 50x without causual masking and 5x with causual masking).\n\n--------------------------------------------------------------------\nI have read the author's response and I would keep my positive evaluation towards the paper. Personally I feel the paper could benefit from conducting more extensive experiments, but I feel the idea itself sounds interesting and it is worth publication."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The proposed approach has nearly linear runtime in theory (ableit with some assumptions) and has good performance in practice."
                },
                "weaknesses": {
                    "value": "There is no major weakness.\n\nThe presentation is good overall, but the theory part could be improved.\nFor example, add in-line comments for algorithm description and give explanation on which steps the assumption are required."
                },
                "questions": {
                    "value": "."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2706/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2706/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2706/Reviewer_cCwA"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2706/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699057612698,
            "cdate": 1699057612698,
            "tmdate": 1700845870312,
            "mdate": 1700845870312,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2fPCOM2ZYR",
                "forum": "Eh0Od2BJIM",
                "replyto": "2dEEyPmpgY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2706/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2706/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their detailed and insightful comments. We have modified the submission to include in-line comments describing the purpose of each quantity in algorithm 2, as well as an explanation before the algorithm of where the assumptions of Lemma 1 are used; in particular, the assumptions are used in the proofs to show that the variance of the estimator is small, therefore allowing for small sample complexity."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2706/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700513283077,
                "cdate": 1700513283077,
                "tmdate": 1700513283077,
                "mdate": 1700513283077,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]