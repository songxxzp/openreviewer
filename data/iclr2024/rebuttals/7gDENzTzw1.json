[
    {
        "title": "Belief-Enriched Pessimistic Q-Learning against Adversarial State Perturbations"
    },
    {
        "review": {
            "id": "KOZahWTXUL",
            "forum": "7gDENzTzw1",
            "replyto": "7gDENzTzw1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3965/Reviewer_cDHy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3965/Reviewer_cDHy"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the vulnerability of reinforcement learning agents to adversarial attacks by manipulating state observations. The authors propose a algorithm focusing on developing a pessimistic policy that accounts for uncertainties in state information. This is supplemented by belief state inference and diffusion-based state purification techniques."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Addressed an important problem in Reinforcement Learning, providing fresh perspective and insights.\nThe experimental design and methodology are well-constructed."
                },
                "weaknesses": {
                    "value": "The innovative aspects presented over WocaR-DQN appear to be incremental in nature."
                },
                "questions": {
                    "value": "How extensive or generalizable are the empirical results presented? Given that the study focuses on the Continuous Gridworld, which is relatively simple, and only includes two Atari games. \n\nConsidering that Atari game screens are depicted using 8-bit (256 levels) RGB, yet typically select colors from a narrower palette, how effective are the different attack budgets (15/255, 3/255, 1/255) tested in this study? Specifically, can these perturbations alter the colors enough to cause confusion with other colors in the game's limited palette? If not, could the observed robustness of the learning method simply be a consequence of learning to filter out specific colors?\n\nIs the proposed method also applicable to environments like Mujoco or Mountain Car, where the input variables are more continuous and less discrete than those in Atari games?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3965/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698650740220,
            "cdate": 1698650740220,
            "tmdate": 1699636357638,
            "mdate": 1699636357638,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lwaYmAlild",
                "forum": "7gDENzTzw1",
                "replyto": "KOZahWTXUL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3965/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3965/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate your constructive comments and feedback. Now we will explain your concerns and questions point by point in the following.\n\nQ1: The innovative aspects presented over WocaR-DQN appear to be incremental in nature.\n\nA1: Our method differs from WocaR-DQN in three important aspects. First, we adopt the maximin framework as a principled approach for achieving robustness, which is applied at both training and test stages using the perturbed states as input. In contrast, WocaR-DQN uses a regularization term and an estimation of the worst-case loss in the loss function to gain robustness. While the former is insufficient in the face of strong attacks, the latter is estimated using a separate neural network, suffering from the gap between training and testing. Second, we incorporate belief modeling into robust decision making, which is crucial for combating strong attacks, but has not been considered in WocaR-DQN. Third, we introduce diffusion-based state purification for environments with raw-pixel input, which allows us to train a single policy against different attack budgets. In contrast, a separate policy is required in WocaR-DQN.\n\nQ2: We only test on two atari games.\n\nA2: We have conducted a new experiment and tested our methods on the Bankheist environment that is widely used in other baselines. However, due to the recent upgrade of the Gym package, all pre-trained models provided by other baselines failed to work anymore. Thus, we retrained all other baselines ourselves, which resulted in performances different from those reported in the original papers. The result of Bankheist is shown below. \n|     Env    |      Model     |  Natural  Reward |         PGD        |                    |                     |       MinBest      |                    |                     |        PA-AD       |                    |                     |\n|:----------:|:--------------:|:----------------:|:------------------:|:------------------:|:-------------------:|:------------------:|:------------------:|:-------------------:|:------------------:|:------------------:|:-------------------:|\n|            |                |                  | $\\epsilon = 1/255$ | $\\epsilon = 3/255$ | $\\epsilon = 15/255$ | $\\epsilon = 1/255$ | $\\epsilon = 3/255$ | $\\epsilon = 15/255$ | $\\epsilon = 1/255$ | $\\epsilon = 3/255$ | $\\epsilon = 15/255$ |\n| Bank Heist |       DQN      |    $680 \\pm 0$   |      $0 \\pm 0$     |      $0 \\pm 0$     |      $0 \\pm 0$      |      $0 \\pm 0$     |      $0 \\pm 0$     |      $0 \\pm 0$      |     $10 \\pm 0$     |     $10 \\pm 0$     |      $0 \\pm 0$      |\n| Bank Heist |     SA_DQN     |    $690 \\pm 0$   |     $690 \\pm 0$    |      $0 \\pm 0$     |      $10 \\pm 0$     |     $690 \\pm 0$    |      $0 \\pm 0$     |      $0 \\pm 0$      |     $670 \\pm 0$    |     $630 \\pm 0$    |      $60 \\pm 0$     |\n| Bank Heist |    WocaR_DQN   |    $680 \\pm 0$   |    $354 \\pm 309$   |      $0 \\pm 0$     |      $0 \\pm 0$      |     $430 \\pm 0$    |      $0 \\pm 0$     |      $0 \\pm 0$      |     $580 \\pm 0$    |     $120 \\pm 0$    |      $0 \\pm 0$      |\n| Bank Heist | DP_DQN_O(Ours) |   $680 \\pm 10$   |    $684 \\pm 13$    |    $638 \\pm 50$    |     $108 \\pm 28$    |    $700 \\pm 10$    |    $678 \\pm 15$    |     $85 \\pm 5.4$    |    $708 \\pm 23$    |    $686 \\pm 18$    |    $192 \\pm 107$    |\n| Bank Heist | DP_DQN_F(Ours) |  $694 \\pm 11.4$  |   $688 \\pm 13.0$   |   $682 \\pm 13.0$   |    $472 \\pm 132$    |    $686 \\pm 5.4$   |     $700 \\pm 7$    |    $244 \\pm 106$    |    $700 \\pm 12$    |    $702 \\pm 4.4$   |    $490 \\pm 50.5$   |\n\nOur DP-DQN-O and DP-DQN-F methods outperform other baselines in the BankHeist environment, although they suffer from a notable performance loss when the attack budget is 15/255 due to the complexity of the game. Also, DP-DQN-F shows better robustness than DP-DQN-O when $\\epsilon=15/255$.\n\nAnswers for Q3 and Q4 are in the next comment due to the characters limitation."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3965/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700168424890,
                "cdate": 1700168424890,
                "tmdate": 1700168424890,
                "mdate": 1700168424890,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "r5hv6QJ538",
                "forum": "7gDENzTzw1",
                "replyto": "KOZahWTXUL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3965/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3965/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Does our response address your concerns?"
                    },
                    "comment": {
                        "value": "Dear reviewer cDHy,\n\nAs the stage of the review discussion is ending soon, we would like to kindly ask you to review our response and consider making adjustments to the scores.\n\nSincerely,\n\nPaper3965 Authors"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3965/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700673624124,
                "cdate": 1700673624124,
                "tmdate": 1700673624124,
                "mdate": 1700673624124,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "UwfCirMikx",
            "forum": "7gDENzTzw1",
            "replyto": "7gDENzTzw1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3965/Reviewer_GsDy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3965/Reviewer_GsDy"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a novel RL algorithm that aims to protect RL agents from adversarial state perturbations. The authors propose a pessimistic DQN algorithm that takes into consideration both the worst-case scenarios and belief about the true states. The algorithm also features a diffusion-based state purification method for applications like Atari games. The paper shows empirical results demonstrating that their approach significantly outperforms existing solutions in robustness against strong adversarial attacks, while maintaining comparable computational complexity."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The empirical results show performance improvement under strong attacks compared to baseline methods. The algorithm works well for both simplistic environments and more complex scenarios like Atari games with raw pixel input.\n2. The algorithm's computational overhead is comparable to existing methods that use regularization terms."
                },
                "weaknesses": {
                    "value": "1. The algorithm assumes access to a clean environment during training, which may not always be the case in real-world applications.\n2. While the diffusion model adds robustness, it also adds computational overhead, potentially making it slower at test time."
                },
                "questions": {
                    "value": "1. How sensitive is your algorithm to the choice of hyperparameters?\n2. Given the requirement for a clean training environment, how would your method perform in a scenario where such an environment is not readily available?\n3. The diffusion model increases computational complexity during the test stage. Are there ways to optimize this without compromising the robustness?\n4. Why PA-AD is not evaluated on continuous gridworld?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3965/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698730493260,
            "cdate": 1698730493260,
            "tmdate": 1699636357548,
            "mdate": 1699636357548,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MLQQcVna2D",
                "forum": "7gDENzTzw1",
                "replyto": "UwfCirMikx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3965/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3965/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate your constructive comments and feedback. Now we will explain your concerns and questions point by point in the following.\n\nQ1: How sensitive our algorithm to the choice of hyperparameters?\n\nA1: We did not fine-tune most of the hyperparameters. For training the RL policies, we used the same default hyperparameters (e.g. learning rate, batch size, buffer size) as SA-MDP. For training the belief model, we took the default hyperparameters from the PF-RNN paper. For training the diffusion models, we used the default parameters in the DDPM and Progressive Diffusion papers, except for the number of reverse steps, which was significantly reduced to adapt the models to adversarial perturbations. As discussed in the second question proposed by Reviewer QHWx and Appendix F.3.2, our DP-DPN-O method is less sensitive to the number of reverse steps during testing, while our DP-DQN-F method is more sensitive to it.\n\nQ2: How would our method perform in a scenario where such an environment is not readily available?\n\nA2: We have added a general response to this question.\n\nQ3: PA-AD is not evaluated on continuous gridworld.\n\nA3:  Following the reviewer\u2019s suggestion, we have adapted PA-AD to our continuous Gridworld environment and tested our methods and other baselines. The results are as follows: \n|  Env |  Model | PA-AD | PA-AD |\n|:---:|:---:|:---:|:---:|\n|  |  | $\\epsilon = 0.1$ | $\\epsilon = 0.5$ |\n| Continuous Gridworld | DQN | $-10.7 \\pm 136$ | $-35.9 \\pm 118 $ |\n| Continuous Gridworld | SA_DQN | $-97.5 \\pm 13.6$ | $-67.8 \\pm 78.3$ |\n| Continuous Gridworld | WocaR_DQN | $-100 \\pm 0$ | $-63.2 \\pm 88.6$ |\n| Continuous Gridworld | BP_DQN | $171.9 \\pm 17$ | $177.2 \\pm 10.6$ |\n\nSimilar to other attack baselines reported in our paper, our BP-DQN method achieves the best robustness under the PA-AD attack.\n\nQ4: Computational overhead brought by diffusion model.\n\nA4:  We admit that using a diffusion model introduces additional overhead to our methods, especially for the DP-DQN-O method where the DDPM model is used. In this work, we considered a fast diffusion method called Progressive Diffusion in our DP-DQN-F method. As shown in Table 2(b), DP-DQN-F has significantly lower training and testing overhead than DP-DQN-O. To further speed up state purification, one possibility is to change the U-net structure used by the diffusion models to accelerate the reverse process. Another direction is to utilize other generative models such as GANs and autoencoders for state purification, which may not be as effective as diffusion-based methods but incur less computational overhead. \n\nWe hope that our explanations address your major concerns. If there are still concerns or questions, we would be happy to hear and discuss them."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3965/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700168178734,
                "cdate": 1700168178734,
                "tmdate": 1700168178734,
                "mdate": 1700168178734,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lrjf9cAtqE",
                "forum": "7gDENzTzw1",
                "replyto": "UwfCirMikx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3965/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3965/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Does our response address your concerns?"
                    },
                    "comment": {
                        "value": "Dear reviewer GsDy,\n\nAs the stage of the review discussion is ending soon, we would like to kindly ask you to review our response and consider making adjustments to the scores.\n\nSincerely,\n\nPaper3965 Authors"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3965/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700673590977,
                "cdate": 1700673590977,
                "tmdate": 1700673590977,
                "mdate": 1700673590977,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CUvgFs90bN",
                "forum": "7gDENzTzw1",
                "replyto": "lrjf9cAtqE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3965/Reviewer_GsDy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3965/Reviewer_GsDy"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you to the authors for the clarification and the additional experiments. My opinion remains in favor of accepting the paper."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3965/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700691029268,
                "cdate": 1700691029268,
                "tmdate": 1700691029268,
                "mdate": 1700691029268,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7T2OU6u2pH",
            "forum": "7gDENzTzw1",
            "replyto": "7gDENzTzw1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3965/Reviewer_achV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3965/Reviewer_achV"
            ],
            "content": {
                "summary": {
                    "value": "The authors study the problem of defense in presence of perceived state attack in Reinforcement learning and propose a method to approximately solve the Stackelberg equilibrium between the agents and the adversary. Their method involves solving pessimistic Q-learning and estimating belief state of the agent and using them for state purification. They propose two algorithms called BP-DQN and DP-DQN to defend against adversarial attacks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Proposes a novel method to defend against adversarial attacks in RL by combining pessimistic Q-learning with belief state estimation and state purification objective.\n    \n2. Authors provided theoretical results to compare the policy found by their algorithm to the optimal policy.\n    \n3. Presented interesting empirical experiments to demonstrate effectiveness of their algorithm on several examples."
                },
                "weaknesses": {
                    "value": "1. It has been shown that Stackelberg Equilibrium as defined in Definition 2 need not always exist, refer to theorem 4.3 [https://arxiv.org/pdf/2212.02705.pdf](https://arxiv.org/pdf/2212.02705.pdf). So, finding an approximate solution for them is meaningless. However, non-existence of Stackelberg equilibrium is a worst case phenomenon. Authors should incorporate this in their paper.\n    \n2. It will be great if authors can include a short paragraph before section 3.2 discussing a big picture of their strategies before diving into each one of them. It would also help to include some mathematical details in each section.\n    \n3. Abstract wrongly mentions that past methods either regularize or retrain the policy. However, methods like Bharti et.al. just purify the states directly."
                },
                "questions": {
                    "value": "1. It is well known that defense against perceived state attack requires solving a partially observable MDP which is a hard problem to solve in general. Could you clarify how your method is able to avoid these hardness issues?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3965/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698770242823,
            "cdate": 1698770242823,
            "tmdate": 1699636357474,
            "mdate": 1699636357474,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "38RBCQD6Sb",
                "forum": "7gDENzTzw1",
                "replyto": "7T2OU6u2pH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3965/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3965/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate your constructive comments and feedback. Now we will explain your concerns and questions point by point in the following.\n\nQ1: It has been shown that Stackelberg Equilibrium as defined in Definition 2 need not always exist, refer to theorem 4.3 https://arxiv.org/pdf/2212.02705.pdf. So, finding an approximate solution for them is meaningless. However, non-existence of Stackelberg equilibrium is a worst case phenomenon. Authors should incorporate this in their paper.\n\nA1: We are aware of the negative result the reviewer mentioned. Please see the discussion below Definition 2 in Section 3, where we have cited the SA-MAP paper where this result was originally proved (Theorem 4 in the SA-MDP paper). We are also aware of the paper the reviewer mentioned and have cited its journal version in the Related Work section (Appendix B.1). The negative result implies that a policy that obtains the best possible value (against the strongest attack) across all states does not exist in general, but it does not exclude the possibility of finding an approximate solution that is close to the optimal state-wise. Although we were not able to identify a policy that obtains an approximate Stackelberg Equilibrium in the standard sense, we showed that our pessimistic Q learning algorithm (Algorithm 1) obtains a value function that is close to the optimal Q function without attacks (Theorem 1) under common assumptions.\n\nQ2: It will be great if authors can include a short paragraph before section 3.2 discussing a big picture of their strategies before diving into each one of them. \n\nA2: We present our basic solution framework, pessimistic Q-learning, in Section 3.3. Our approach derives maximin actions from the Q-function using perturbed states as input to safeguard against the agent\u2019s uncertainty about true states. Notably, this approach is applied at both training and test stages, thus bridging the gap between the two as in previous work. We then improve the algorithm by incorporating the agent\u2019s belief about true states into action selection in Section 3.4. To obtain a practical solution in high-dimensional state spaces, we present our BP-DQN algorithm in this section, which incorporates the above ideas into the classic Deep Q-Network (DQN) algorithm to obtain pessimistic DQN with belief states approximated using the PF-RNN method. To further scale our method to environments with raw-pixel input, we present another algorithm, DP-DQN, in Section 3.5, where we incorporate a diffusion-based purification scheme into pessimistic DQN to purify the pixel-wise perturbations. \n\nQ3: Abstract wrongly mentions that past methods either regularize or retrain the policy. However, methods like Bharti et.al. just purify the states directly.\n\nA3: We would like to thank the reviewer for pointing out the work beyond regularization-based methods and alternating training methods. We are aware of the purification-based methods, including the work by Bharti et al., and have provided a detailed comparison between our method and Bharti et al. in Appendix B.3. In particular, our method has two important components not considered in Bharti et al., namely, the maximin formulation and belief update. There also exist studies that detect if a state is perturbed or not at the test stage, e.g., Xiong et al., which uses an autoencoder-based method to detect perturbations, as we mentioned in Section 3.3 and Appendix B.1. We did not mention them in the abstract because Bharti et al. focuses on the backdoor attack rather than test-stage state perturbation we considered, while Xiong et al. focuses on test stage detection given a pre-trained policy. \n\nQ4: How do our methods avoid the hardness in POMDP?\n\nA4:  We first note that while training a robust agent against adversarial state perturbations is closely related to POMDPs, they are also fundamentally different. In POMDPs, the agent\u2019s observation is derived from a predetermined observation function as part of the environment. In contrast, the agent\u2019s observation is determined by the attacker\u2019s policy in adversarial state perturbations, which might be non-stationary or even adaptive, making the problem even harder. \n\nIn this work, we have exploited two key observations to bypass this obstacle. First, the attacker has a bounded budget to avoid detection, where the perturbed state must land in the \\epsilon ball centered at the true state so that the agent could utilize historical information to generate a relatively accurate belief about true states, which might not be the case in general POMDPs. Further, for environments with raw-pixel input, existing attacks generate perturbations in the form of pixel-wise noise, which could be purified with the help of a diffusion model to generate a belief about the true state. These ideas together with our pessimistic training framework ensure high robustness against strong state perturbation attacks. \n\nWe hope that our explanations address your major concerns."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3965/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700167752238,
                "cdate": 1700167752238,
                "tmdate": 1700167752238,
                "mdate": 1700167752238,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nJPV24P9Cn",
                "forum": "7gDENzTzw1",
                "replyto": "7T2OU6u2pH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3965/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3965/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Does our response address your concerns?"
                    },
                    "comment": {
                        "value": "Dear reviewer achV,\n\nAs the stage of the review discussion is ending soon, we would like to kindly ask you to review our response and consider making adjustments to the scores.\n\nSincerely,\n\nPaper3965 Authors"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3965/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700673549384,
                "cdate": 1700673549384,
                "tmdate": 1700673549384,
                "mdate": 1700673549384,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cxPhOe6MHV",
                "forum": "7gDENzTzw1",
                "replyto": "7T2OU6u2pH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3965/Reviewer_achV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3965/Reviewer_achV"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors,\nThank you for your response! I am not very satisfied with the response to Q1(it is still not clear to me how can your algorithm provide an approximate solution to the problem which admits no solution? The notion of \"approximate solution\" being used does not seem to be a standard one) and also Q4(In my opinion just by constraining the attacker in $\\epsilon$-ball radius would not permit the defender to recover back true states even if uses the historical information. If it can indeed do so, is there some threshold on $\\epsilon$ above which defense is not possible and does your theory say something about it - because for large epsilon one can always change any state to any other state?). Hence, I would maintain my score for now."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3965/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700714258354,
                "cdate": 1700714258354,
                "tmdate": 1700714398994,
                "mdate": 1700714398994,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Wt79Ifshhs",
                "forum": "7gDENzTzw1",
                "replyto": "7T2OU6u2pH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3965/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3965/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Further explainations for Q1 and Q4"
                    },
                    "comment": {
                        "value": "Thank you for carefully reading our response and we are glad to further clarify your concerns.\n\nFor Q1, an accurate definition of approximate Stackelberg Equilibrium is as follows. First, as shown in Theorem 4.11 of https://arxiv.org/pdf/2212.02705.pdf, when the initial state distribution $Pr(s_0)$ is known, there is an agent policy $\\pi^*$ that maximizes the worst-case **expected** state value against the optimal state perturbation attack, that is \n\n$\\forall \\pi$, $\\Sigma_{s_0\\in S} Pr(s_0) V_{\\pi^*\\circ\\omega_{\\pi^*}}(s_0) \\ge \\Sigma_{s_0\\in S} Pr(s_0) V_{\\pi\\circ\\omega_{\\pi}}(s_0)$ \n\nIn particular, when the initial state $s_0$ is fixed, an optimal policy that maximizes the worst-case state value (against the optimal state perturbation attack) can be found. Let $V^*(s_0)$ denote this optimal state value. Note that $V^*(s_0)$ is obtained using a different policy for different $s_0$. Then a reasonable definition of approximate Stackelberg Equilibrium is a policy $\\pi$ where its state value under each state $s_0$ is close to $V^*(s_0)$, that is, $|V_{\\pi\\circ\\omega_{\\pi}}(s_0) - V^*(s_0)| \\leq \\Delta$ for some constant $\\Delta$. Note that this condition should hold for each state $s_0$. Also note that we compare $\\pi$ with a different optimal policy with respect to each $s_0$, thus bypassing the impossibility result. \n\nIn the paper, we further approximated $V^*(s_0)$ by the optimal value without attacks and derived $\\Delta$ in the simplified setting (Theorem 1). However, we believe that this definition presents a novel extension of approximate Stackelberg Equilibrium in the standard sense and will incorporate it into the next version of the paper.\n\nFor Q4, we did not claim that our method can recover the exact true state from the perturbed state. Both our historical information-based and diffusion-based belief models generate a set of belief states $M_t$ to approximate the true state. Ideally, the belief states should provide a better approximation of the true state beyond what can be estimated from the epsilon ball centered at the perturbed state. Although we do not have a theoretical characterization of how the belief model reduces the defender\u2019s uncertainty about the true state, we have provided some empirical evidence in the paper. Figures 5 (a)-(c) (Appendix F.3.2) show that the $l_2$ distance between the denoised state (generated from the belief) and the true state is significantly smaller than the $l_2$ distance between the perturbed state and the true state. \n\nFurther, Theorem 1 shows that the performance loss of our pessimistic Q-learning approach (compared with the optimal policy without attacks) increases linearly over the defender\u2019s uncertainty about the true state, showing the importance of reducing this uncertainty. \n\nWe hope that our explanations address your concerns."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3965/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700728565595,
                "cdate": 1700728565595,
                "tmdate": 1700728801027,
                "mdate": 1700728801027,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mcRQltYSgH",
            "forum": "7gDENzTzw1",
            "replyto": "7gDENzTzw1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3965/Reviewer_QHWx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3965/Reviewer_QHWx"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes two new algorithms to robustify agent policies against adversarial attacks. It formulates the problem of finding a robust policy as a Stacleberg game (where agents choose policies), and then further incorporates beliefs into the derived algorithm. For pixel-based observation spaces, the game uses a diffusion-based method to derive valid possible states. The paper is very well structured and easy to follow."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- the paper addresses many shortcomings of current works\n- the theoretical algorithms and derivations are insightful\n- the practical implementation of the derived algorithms are well motivated"
                },
                "weaknesses": {
                    "value": "- the paper assumes that both the clean MDP and the perturbation budget are known to both the victim and the attacker\n- it would be interesting to run an ablation on these assumptions. How well does the method work if the budget is not known exactly, or if the MDP transition function is not known exactly?"
                },
                "questions": {
                    "value": "see weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3965/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698827071366,
            "cdate": 1698827071366,
            "tmdate": 1699636357402,
            "mdate": 1699636357402,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IdNoNhX0Zm",
                "forum": "7gDENzTzw1",
                "replyto": "mcRQltYSgH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3965/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3965/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate your constructive comments and feedback. Now we will explain your concerns and questions point by point in the following.\n\nA1: How our methods perform when a clean environment is unavailable?\n\nQ1: We have added a general response to this question. \n\nA2: How our methods perform when the attack budget is unknown?\n\nQ2: Our BP-DQN method does not require accurate knowledge about the attack budget. The results in the paper were obtained using a single policy trained against the PGD attack with $\\epsilon = 0.1$, without further adaptation at the test stage against the actual attack budget (0.1 or 0.5). \n\nOur DP-DQN methods also do not require accurate knowledge about the attack budget at the training stage. All the results shown in the paper for DP-DQN-O (resp. DP-DQN-F) were obtained using a single policy trained against the PGD attack with $\\epsilon = 1/255$, and with the reverse steps $k= 5$ in DDPM (resp. $k=1$ in Progressive Distillation). However, we did adapt the value of $k$ at the test stage according to the actual attack budget in the paper. To understand the performance of our algorithms under an unknown attack budget at the test stage, we have conducted a new ablation study on these methods using the Pong environment with a fixed number of reverse steps in the diffusion model ($k = 30$ for DP-DQN-O and $k = 1$ for DP-DQN-F) and a fixed level of manually added noise (1/255) at the test stage, and tested them on PGD attacks with three different attack budgets (1/255, 3/255 and 15/255). The results for DP-DQN-O are $\\epsilon = 1/255: 20 \\pm 0$, $\\epsilon = 3/255: 19.8 \\pm 0.4$, $\\epsilon = 15/255: 19.7 \\pm 0.5$, which are on par with the results using an adapted $k$ shown in the paper. On the other hand, the results for DP-DQN-F are $\\epsilon = 1/255: 20.4 \\pm 0.9$, $\\epsilon = 3/255: 13 \\pm 7.4$, $\\epsilon = 15/255: -20.6 \\pm 0.4$, showing that it is more sensitive to attack budget. As a comparison, we have also tested other baselines using the policy trained under $\\epsilon = 1/255$ against a PGD attack with budget $\\epsilon = 3/255$, and the results are SA-DQN: $12 \\pm 1.2$, WocaR-DQN: $-21 \\pm 0$, showing that these methods are also sensitive to attack budget. \n\nWe hope that our explanations address your major concerns. If there are still concerns or questions, we would be happy to hear and discuss them."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3965/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700167346677,
                "cdate": 1700167346677,
                "tmdate": 1700167346677,
                "mdate": 1700167346677,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LUVqNnhxNJ",
                "forum": "7gDENzTzw1",
                "replyto": "IdNoNhX0Zm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3965/Reviewer_QHWx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3965/Reviewer_QHWx"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors, thank you for the additional clarifications and the additional experiments (which are very positive). I will maintain my score."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3965/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700597767262,
                "cdate": 1700597767262,
                "tmdate": 1700597767262,
                "mdate": 1700597767262,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]