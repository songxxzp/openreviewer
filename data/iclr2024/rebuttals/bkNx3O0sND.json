[
    {
        "title": "MBR and QE Finetuning: Training-time Distillation of the Best and Most Expensive Decoding Methods"
    },
    {
        "review": {
            "id": "VwRSUchbs6",
            "forum": "bkNx3O0sND",
            "replyto": "bkNx3O0sND",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4008/Reviewer_t5J4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4008/Reviewer_t5J4"
            ],
            "content": {
                "summary": {
                    "value": "Previous research has shown that ranking techniques (with QE metrics or MBR) generally improve machine translation quality, being very expensive at inference time. This paper proposes MBR/QE finetuning\u2014using QE/MBR outputs at training time, while employing efficient decoding algorithms (e.g., beam search) during inference. Their experiments suggest that (1) MBR/QE finetuning alone are worse than vanilla finetuning with human references using the same high-quality data; (2) self-MBR/QE finetuning can complement finetuning on a small dataset of high-quality human references; and (3) using MBR/QE data generated by a stronger teacher model further improves quality. A subset of the results is validated by professional translators."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The motivation is clear and the paper is well written.\n- MBR/QE finetuning (their method) seems to work well with beam/greedy search, making inference faster when compared to standard reranking techniques that employ rerankers at inference time.\n- Their models are evaluated with multiple state of the art evaluation metrics for MT. Their findings are further validated by 9 professional translators using the MQM framework."
                },
                "weaknesses": {
                    "value": "- Some parts of the paper (e.g., the first three paragraphs of the introduction) talk about NLG in general but the paper focuses on machine translation. Reranking methods, in particular, rely on good quality estimation models that exist for MT but may not exist for other tasks. Of course, MBR finetuning can be applied to other NLG tasks (e.g., summarization) but this paper does not touch this problem. I think the authors should either revise the writing and make the scope more clear from the beginning, or perform experiments in other NLG tasks. The contribution is mainly empirical and only validated for MT. \n- The study is limited in terms of language coverage. They perform experiments on 2 language pairs only (mid & high resource), both for translation out of English. It\u2019s not clear if the findings hold for lower resource languages and when translating into English. In particular, I\u2019m expecting quality estimation models to be worse for low resource languages, which may end up affecting the quality of the translations produced with their method. Have you tried other languages? If so, what happens in that case?\n- The related work discusses other methods for training NMT models beyond MLE (e.g., RL methods) but none of them is used as a baseline."
                },
                "questions": {
                    "value": "I left some questions in the weaknesses part. Other comments/questions: \n\n- MBR decoding uses BLEURT and is prohibitively expensive. Note, however, that more efficient alternatives exist. See implementation details discussed in [1], Section 4.3. Is there a reason to choose BLEURT? While efficiency at inference time is an advantage, MBR/QE finetuning is expensive at training time. It would be good to see some numbers to understand the training/inference time difference when compared to other existing approaches.\n- In the introduction you say that MBR decoding requires the generation of a large number of candidates. While this is true if you get unbiased samples from the model, it does not seem to be the case when you bias the distribution (e.g., using nucleus sampling). I suggest you see the discussions in [2] and [3] and comment. Also, see my comment above about [1]. \n- Results use COMET-20 instead of more recent versions already available online such as COMET-22. Is there a reason for using the 2020 version?\n- Human evaluation results are important and should not be in the appendix (Table 7). In fact, I think they should be more highlighted in the paper! It would also be interesting to see if they generalize for En-Ja. Can you explain the reasoning for using 9 professional translators for En-De instead of using fewer and evaluate En-Ja as well?\n- What happens if you decode with reranking techniques (QE/MBR) using a model trained with your method, instead of using beam/greedy search? Even though this would make the method very inefficient at inference time, it would be interesting to see if it further boosts the performance.\n- According to Table 3, both beam and greedy search work well. Does this mean that your method helps solve the beam search curse [4]? What happens when you increase the beam width?\n\n[1] Identifying Weaknesses in Machine Translation Metrics Through Minimum Bayes Risk Decoding: A Case Study for COMET (Amrhein & Sennrich, AACL-IJCNLP 2022)\n\n[2] Quality-Aware Decoding for Neural Machine Translation (Fernandes et al., NAACL 2022).\n\n[3] An Empirical Study of Translation Hypothesis Ensembling with Large Language Models (Farinhas et al., EMNLP 2023).\n\n[4] Six Challenges for Neural Machine Translation (Koehn & Knowles, NGT 2017)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4008/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4008/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4008/Reviewer_t5J4"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4008/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698946878035,
            "cdate": 1698946878035,
            "tmdate": 1700738760324,
            "mdate": 1700738760324,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "b87Lqo3jjn",
                "forum": "bkNx3O0sND",
                "replyto": "VwRSUchbs6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4008/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4008/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer t5J4"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nThank you for your very careful and thoughtful review of our paper. We appreciate your detailed feedback and will seek to individually address each your outstanding concerns here.\n\n1. *Some parts of the paper (e.g., the first three paragraphs of the introduction) talk about NLG in general but the paper focuses on machine translation. Reranking methods, in particular, rely on good quality estimation models that exist for MT but may not exist for other tasks. Of course, MBR finetuning can be applied to other NLG tasks (e.g., summarization) but this paper does not touch this problem. I think the authors should either revise the writing and make the scope more clear from the beginning, or perform experiments in other NLG tasks. The contribution is mainly empirical and only validated for MT.*\n\nMT is a common testbed for NLG tasks. In the abstract, we state the paper's scope: \" Using the canonical NLG task of Neural Machine Translation (NMT), ...\". Moreover, while the first three paragraphs of the introduction discuss NLG in general (establishing the motivation and setup in the most general terms), the fourth paragraph already narrows the focus to MT: \"In this work, we focus on the NLG task of Neural Machine Translation (NMT)...\". In the conclusion, we propose \"extending MBR and QE finetuning to other NLG tasks besides NMT\" as an avenue for future work. \n\nNote that MBR decoding has been successfully applied to other NLG tasks including text summarization, image captioning, and data-to-text using BERTScore as the utility function, as described in https://arxiv.org/pdf/2311.05263.pdf (Model-based Minimum Bayes Risk Decoding; Jinnai et al., 2023). So applying MBR finetuning to these other tasks would be a natural extension of this work, and would likely perform well.\n\nAll of this said, given your concerns about the clarity of the scope, we have revised the first three paragraphs of the introduction, to make explicit from the very beginning that the scope is limited to MT.\n\n2. *The study is limited in terms of language coverage. They perform experiments on 2 language pairs only (mid & high resource), both for translation out of English. It\u2019s not clear if the findings hold for lower resource languages and when translating into English. In particular, I\u2019m expecting quality estimation models to be worse for low resource languages, which may end up affecting the quality of the translations produced with their method. Have you tried other languages? If so, what happens in that case?*\n\nThe quality of MT metrics on low-resource language pairs and into-English language pairs has been evaluated in the WMT Metrics Shared Tasks. See results from WMT'22 (https://aclanthology.org/2022.wmt-1.2.pdf)  and WMT'23 (https://www2.statmt.org/wmt23/pdf/2023.wmt-1.51.pdf), which show that 1) the MetricX metric (including the QE variant) generalizes well to language pairs unseen during training, including low-resource language pairs (e.g. see Hebrew-English in Table 8 of the WMT'23 Metrics Shared Task Report) and 2) BLEURT and MetricX-XXL perform well on into-English language pairs. Note that result 2) is not surprising given that most neural MT metrics, including BLEURT and MetricX-XXL, were finetuned on large language models which were pretrained mostly on English data. \n\nWhile this paper focuses on mid and high-resource language pairs, existing evidence about the quality of utility functions for low-resource language pairs and into-English language pairs suggests that MBR and QE finetuning would likely work in these settings as well. This would be a fruitful avenue for future work. \n\n3. *The related work discusses other methods for training NMT models beyond MLE (e.g., RL methods) but none of them is used as a baseline.*\n\nFor a correct comparison to RL baselines, we would need to implement them in our framework, to be able to run them on the same data and using the same utility function. Additionally, we would need to tune the RL settings to ensure a fair comparison. Thus, evaluating RL baselines is outside of the scope of this paper. Moreover, the MBR and QE finetuning methods proposed here use MLE, so comparison with RL methods would be tangential to this paper's focus. \n\n(Please see the follow-up comment for individual responses to questions.)"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4008/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700260156530,
                "cdate": 1700260156530,
                "tmdate": 1700260156530,
                "mdate": 1700260156530,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "16i2b0sFP9",
                "forum": "bkNx3O0sND",
                "replyto": "FqDaOnVKks",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4008/Reviewer_t5J4"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4008/Reviewer_t5J4"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your response and for updating the paper with some of my suggestions. My comment about the study being limited in terms of language coverage remains unchanged. In any case, I think you did a good job at addressing some of my concerns and updated my score accordingly."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4008/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700561559583,
                "cdate": 1700561559583,
                "tmdate": 1700561559583,
                "mdate": 1700561559583,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kujmmH58px",
                "forum": "bkNx3O0sND",
                "replyto": "VasHzpUYPB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4008/Reviewer_t5J4"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4008/Reviewer_t5J4"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for providing evidence that your method also works when translating into English. I think these results, when finished, should be added to the paper for completeness. I updated my initial review accordingly."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4008/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738811671,
                "cdate": 1700738811671,
                "tmdate": 1700738811671,
                "mdate": 1700738811671,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PO9iVUsla5",
            "forum": "bkNx3O0sND",
            "replyto": "bkNx3O0sND",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4008/Reviewer_VTjn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4008/Reviewer_VTjn"
            ],
            "content": {
                "summary": {
                    "value": "This paper uses expensive decoding methods like MBR decoding and QE using neural metrics to generate data for finetuning. After fine-tuning, the model can be used with cheaper decoding strategies like beam search/greedy search and still retain the gains of the expensive decoding methods.\nThe paper explores different setups: finetuning on references, finetuning on MBR/QE decoded reference set, finetuning on MBR/QE decoded sampled monolingual data, finetuning on MBR/QE decoded samples from a very strong teacher (finetuned LLM) and show that finetuning on MBR/QE decoded monolingual data can get additional benefits over finetuning on references. They also show that finetuning using decoded data from a very strong teacher shows the best results.\nThe paper also does ablations around the effect of candidate size/source sentences selected/forward translation vs. backward translation while generating the finetuning data.\nFinally the QE finetuned systems are evaluated against baselines in an MQM human evaluation to confirm the rankings produced by COMET-20"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Strengths of the paper:\n\n1. Sound experimental setup, clear description of the proposed method and showing its effectiveness\n2. Important contribution for practitioners because currently MBR and QE decoding with neural metrics is computationally infeasible to be deployed in real time, the proposed approach can help bring some of those gains to production systems"
                },
                "weaknesses": {
                    "value": "Weaknesses of the paper:\n\n1. The paper presents a section of the results from Palm2 finetuning and show large improvements. I think that is expected because it is a much stronger model. This set of experiments doesn't add too much value to the paper. The paper should also show the beam and MBR decoded results of the Palm2 model used for finetuning, so that the readers can understand show much of the teacher model performance can be transferred to the student via this finetuning.\n2. After using MBR/QE finetuning, do we still see a difference between beam decoding and MBR/QE decoding? Table 3 summarizes the results using beam/greedy and sampling, but MBR/QE decoding results should also be included for completeness.\n3. In Table 1, we see 2a perform better than 1c) and 2c) perform better than 1b). This is a bit suprising because I would have assumed that finetuning performance will be upper bounded by MBR/QE used during run-time. I couldn't find any comment on this comparison in the paper.\n4. The paper does not have any numbers on lexical metrics like chrF. Would have been nice to include those numbers atleast in the Appendix, for interested readers"
                },
                "questions": {
                    "value": "Questions covered in the weakness section. No other specific questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4008/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699042674097,
            "cdate": 1699042674097,
            "tmdate": 1699636363025,
            "mdate": 1699636363025,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DaREwr8pRI",
                "forum": "bkNx3O0sND",
                "replyto": "PO9iVUsla5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4008/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4008/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer VTjn"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nThank you for your thoughtful review of our paper. We appreciate that you recognize both the soundness and effectiveness of the proposed methods, as well as their importance for practitioners. We will seek to individually address each your outstanding concerns here.\n\n1. *The paper presents a section of the results from Palm2 finetuning and show large improvements. I think that is expected because it is a much stronger model. This set of experiments doesn't add too much value to the paper. The paper should also show the beam and MBR decoded results of the Palm2 model used for finetuning, so that the readers can understand show much of the teacher model performance can be transferred to the student via this finetuning.*\n\nAs requested, we have added the performance of the en-de and en-ja PaLM-2 QE-reranked, MBR-decoded, and greedy-decoded teacher models in **Table 10** and **Table 12** in the new revision, respectively. Note that for en-de, the student model is strong enough that MBR finetuning brings its performance almost on par with that of the PaLM-2 teacher model (student Comet20 = 63.62 vs teacher Comet20 = 63.76). Also note that across both language pairs, the MBR-decoded teacher is strongest. Moreover, even though the PaLM-2 QE-reranked teacher is weaker than the greedy-decoded teacher, PaLM-2-QE finetuning achieves much larger gains that PaLM-2-greedy finetuning (rows 3a versus 3c in Table 2). This suggests that QE finetuning is able to smooth out bad QE translations, e.g. due to utility function failure modes (while QE reranking at inference time is not robust to bad translations), allowing the student to exceed the performance of the teacher. In particular, the en-de PaLM-2-QE-finetuned student achieves a Comet20 score of 62.35, exceeding its teacher's score of 60.86.\n\n2. *After using MBR/QE finetuning, do we still see a difference between beam decoding and MBR/QE decoding?*\n\nAs requested, we have added **Table 19** with MBR/QE decoding results for the MBR/QE-finetuned models, in comparison against the more efficient decoding methods of beam search, greedy decoding, and epsilon sampling. (In addition, we have also added the performance of not only QE-finetuned, but also MBR-finetuned, models to **Table 4**.)\n\nWe see that after MBR finetuning, QE reranking works better than MBR decoding. Similarly, after QE finetuning, MBR decoding works better than QE reranking. This suggests that using the same decoding method both to generate the finetuning data and at inference time overfits to the metric used as the utility function. Also note that the gap between MBR/QE decoding and beam search/greedy decoding is small for the MBR/QE-finetuned models, and for Phases 2 and 3 of the QE-finetuned models, as well as Phase 3 of the MBR-finetuned model, beam search or greedy decoding is actually the top-performing decoding strategy.\n\n3. *In Table 1, we see 2a perform better than 1c) and 2c) perform better than 1b). This is a bit suprising because I would have assumed that finetuning performance will be upper bounded by MBR/QE used during run-time. I couldn't find any comment on this comparison in the paper.* \n\nMBR decoding and QE reranking can produce translations with high utility score but low quality, due to honeypots/failure modes of their utility functions. While this directly degrades performance of the teacher models at inference time, the MBR and QE-finetuned students are able to smooth out these bad translations. This explains why student performance is not upper-bounded by performance of the teacher and can, in fact, exceed it.\n\nThis hypothesis is supported by work from Amrhein and Sennrich, 2022 (https://arxiv.org/pdf/2202.05148.pdf) which shows that, despite the strong performance of MBR decoding and QE reranking on average, these decoding methods suffer from failure modes which can be attributed to underlying problems with the utility function. They performed the study using Comet20 as the utility function, and showed that this metric is not sensitive enough to discrepancies in numbers and named entities. MBR and QE finetuning, on the other hand, would not necessarily be as susceptible to these same failure modes since, even though the model would be exposed to these extreme examples, its representation would be smoothed out over the course of training.\n\n4. *The paper does not have any numbers on lexical metrics like chrF. Would have been nice to include those numbers atleast in the Appendix, for interested readers.*\n\nAs requested, we have added performance on chrF to **Table 8** (for English-German) and **Table 11** (for English-Japanese) in the Appendix.\n\nWe hope the additions we have made to our paper allay your concerns and will be taken into account when assigning your final score. Please let us know if you have any further questions or if we can provide any additional clarifications to help finalize your assessment of our paper."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4008/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700260096448,
                "cdate": 1700260096448,
                "tmdate": 1700260096448,
                "mdate": 1700260096448,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MortOlkL57",
            "forum": "bkNx3O0sND",
            "replyto": "bkNx3O0sND",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4008/Reviewer_2DMZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4008/Reviewer_2DMZ"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes two new training methods called MBR finetuning and QE finetuning to distill the benefits of high-quality decoding methods like Minimum Bayes Risk (MBR) decoding and Quality Estimation (QE) reranking into the model weights, while avoiding expensive inference costs. The results demonstrates their effectiveness for machine translation including exceeding human references"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. MBR and QE produce superior quality but are too expensive for inference. The finetuning approach provides a way to achieve most of their benefits without sacrificing efficiency. This could enable deploying higher performance models in real applications.\n2. The proposed methods are clearly explained and technically sound. The experiments are comprehensive and rigorous, spanning various metrics, domains, language pairs, and resource settings. The results consistently validate the effectiveness of MBR and QE finetuning over strong baselines."
                },
                "weaknesses": {
                    "value": "1.\tThe novelty of the method is unclear. Using high-quality pseudo data generated by better decoding methods is a common technique in neural machine translation. This paper does not seem to provide new insights beyond what is already known.\n2.\tWhile this paper reports significant gains with MBR and QE finetuning, the reasons for the improvements are not well analyzed. In some cases, QE finetuning performs better, while in others, MBR finetuning is superior. More details and an in-depth analysis of the advantages and limitations of each finetuning method would be expected."
                },
                "questions": {
                    "value": "1.\tPlease report the performance of the PaLM-2 Bison model.\n2.\tFor English-German, MBR finetuning seems to outperform QE finetuning. Why was only QE finetuning evaluated for English-Japanese instead of also evaluating MBR?\n3.\tCan you report the training time and compute requirements for the QE and MBR finetuning experiments? Providing the concrete training costs for comparison would be helpful."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4008/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4008/Reviewer_2DMZ",
                        "ICLR.cc/2024/Conference/Submission4008/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4008/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699280157821,
            "cdate": 1699280157821,
            "tmdate": 1700742539427,
            "mdate": 1700742539427,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "l49YDQhiP6",
                "forum": "bkNx3O0sND",
                "replyto": "MortOlkL57",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4008/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4008/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 2DMZ"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nThank you for your constructive and thoughtful review of our paper. We appreciate your recognition of the soundness and practical importance of the proposed methods. We also appreciate your taking the time to carefully review our experiments, and your acknowledgement that they are comprehensive and rigorous. To allay your outstanding concerns, we will respond individually here to each of the weaknesses and questions that you raised.\n\n1. *The novelty of the method is unclear. Using high-quality pseudo data generated by better decoding methods is a common technique in neural machine translation. This paper does not seem to provide new insights beyond what is already known.*\n\nThe review states that \"using high-quality pseudo data generated by better decoding methods is a common technique in neural machine translation\", but does not cite any papers to support this claim. We are not aware of any such work, so if calling into question the novelty of MBR and QE finetuning, please kindly provide evidence that \"this paper does not seem to provide new insights beyond what is already known\". Note: We show that self-finetuning using beam search output does **not** improve quality, so the decoding strategy used to generate the finetuning data is crucial to the success of this method.\n\n2. *While this paper reports significant gains with MBR and QE finetuning, the reasons for the improvements are not well analyzed. In some cases, QE finetuning performs better, while in others, MBR finetuning is superior. More details and an in-depth analysis of the advantages and limitations of each finetuning method would be expected.*\n\nThis paper does provide an explanation for the improvements observed with MBR and QE finetuning in paragraph 2 of the Discussion section (Section 7), and also provides a comparison of MBR versus QE finetuning in paragraph 3. Both paragraphs are pasted below for reference:\n\n1. **Explanation for improvements (Section 7, paragraph 2)**: \"Given the effectiveness of an external teacher model (which differs in architecture, training data, etc. from the student), the success of these finetuning methods can likely not be primarily explained by mitigation of the label exposure bias problem (Schmidt, 2019), since the data generated from an external teacher is less similar to the data the model has already seen (during previous training stages) than self-generated MBR or QE data. Instead, the effectiveness of MBR and QE finetuning seemingly can be attributed primarily to the high quality of the translations used as finetuning data.\"\n* Also note that the experiments were designed so that each of the 3 phases answered a different question regarding the effectiveness of MBR and QE finetuning (see **Section 5.4**). For example, by comparing Phase 1 and Phase 2, we show that using a larger monolingual corpus to generate the MBR and QE finetuning data improves performance, and by comparing Phase 2 and Phase 3, we show that using a stronger teacher model also improves performance. \n* Also see **Appendix D** for extensive ablation studies which isolate the effect of various source-side and target-side variables on the performance of MBR and QE-finetuned models. For instance, we show that QE finetuning on forward-translated data is more effective than QE finetuning on backtranslated data (where the QE translations are on the source, rather than target, side). All of these ablations serve to understand the reasons for the observed improvements.\n2. **Comparison of MBR vs QE finetuning (Section 7, paragraph 3)**: \"When comparing MBR against QE finetuning, we see that QE tends to perform at least as well as MBR finetuning using a self-teacher, while MBR outperforms QE finetuning using the LLM teacher. We hypothesize that the sampling translations from the self-teacher do not closely approximate the true reference distribution, so using them as pseudo-references does not help (or even hurts), relative to using a reference-free (QE) metric. For higher-quality models (e.g. PaLM-2), the candidate translations are good enough approximations to references that they provide a useful signal.\"\n* We also mention in the introduction that generating data via QE reranking is much more efficient than generating data via MBR decoding: \"Despite its quality advantages, the slow inference speed of MBR decoding remains a limitation even when generating distillation data\". In terms of computational cost, this is the key limitation of MBR finetuning. So the limitations of MBR relative to QE finetuning both in terms of quality (MBR finetuning requires a stronger teacher) and computational cost are included in the paper. Also see **Table 7** (added to the paper based on your comment #3). \n\nIf 1. and 2. above do not satisfy the requirement for 1) an analysis of the reasons for the improvements due to MBR and QE finetuning and 2) a comparison of the advantages and limitations of MBR vs QE finetuning, please do clarify what is missing."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4008/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700259866021,
                "cdate": 1700259866021,
                "tmdate": 1700259866021,
                "mdate": 1700259866021,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "O3jTJoXKNC",
                "forum": "bkNx3O0sND",
                "replyto": "w0n0tMJDUo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4008/Reviewer_2DMZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4008/Reviewer_2DMZ"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your thoughtful response addressing my concerns and questions. While I still think that using complex decoding methods to generate higher quality pseudo data is unsurprising, I appreciate the revisions made to complete this paper with more extensive results presented. Considering these additional experiments and analysis, I update my score accordingly."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4008/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700742467907,
                "cdate": 1700742467907,
                "tmdate": 1700742467907,
                "mdate": 1700742467907,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BQLoIw3WtP",
            "forum": "bkNx3O0sND",
            "replyto": "bkNx3O0sND",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4008/Reviewer_5sbu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4008/Reviewer_5sbu"
            ],
            "content": {
                "summary": {
                    "value": "The paper describes approaches to training NMT models that use different decoding methods than beam search, namely, QE reranking and MBR decoding. Using such decoding methods in inference can be quite expensive computationally but using them in training time allows for maintaining inference time efficiency while leveraging their mitigation of model-perplexity-vs-quality. This approach can be seen as an alternative to aligning MT models with human translation preferences, given by the QE or utility function used in MBR. Furthermore, using either QE reranking or MBR decoding allows for exploring monolingual data augmentation for training as neither require references for scoring translation hypotheses. Automatic and manual evaluation results indicate that QE-finetuned systems outperform the reference-finetuned baseline, and QE finetuning using a PaLM-2 teacher outperforms using the self-teacher model."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is a good contribution towards aligning MT models with human preferences as given by reference-less or reference-based metrics (QE reranking and the utility function used for MBR, respectively). It allows leveraging monolingual data relevant to the MT model application setting for improving its translation quality. Automatic evaluation is consistent with manual evaluation and the experimental setting is sound. This is an interesting alternative to reinforcement learning models that have shown to be quite unstable in MT settings. The approaches shown here are also generalizable to other natural generation tasks though no experiments have been devised to show that."
                },
                "weaknesses": {
                    "value": "The paper is quite dense at some parts, mainly at the experimental setting. It takes a few passes on those sections to completely understand the results and the settings but it is clear. The QE approach is not really reproducible as the QE model is not publicly available. Furthermore, the best teacher model seems to be a PaLM2-Bison that is also not publicly available."
                },
                "questions": {
                    "value": "* Is there any description of MetricX-XXL-QE in another paper? I could not find a paper describing except a paragraph in the Findings paper. How many languages does it support? This seems to be a very big QE model, 30B parameters. \n\n* How much time is added to the training process when using the MBR and QE reranking? Maybe a table illustrating the added wallclock time would be interesting to contrast with the gains in performance. \n\n* How much does the data used to train the QE model and the utility function used in MBR affect the final results for specific domains? Do yo have any insights on this? For example, for English-German, it seems most of the MT training data is similar to the data used to train the QE and BLEURT models. What if this is not the case? Do you observe the same improvements or does something change in the translation quality of the best models?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4008/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4008/Reviewer_5sbu",
                        "ICLR.cc/2024/Conference/Submission4008/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4008/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699368763062,
            "cdate": 1699368763062,
            "tmdate": 1700736053613,
            "mdate": 1700736053613,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kQBzD4F9iC",
                "forum": "bkNx3O0sND",
                "replyto": "BQLoIw3WtP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4008/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4008/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 5sbu"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nThank you for your careful and thoughtful review of our paper. We observed that you lowered our score from 8 to 6 on November 14, and our hope is that we can address all of your outstanding concerns here.\n\nFirst, to address your concerns brought up in the initial revision of the review: \n\nAcknowledged that the experimental settings section of the paper is \"dense\". If there is anything unclear about the setup, please do specify and we will be happy to make changes to clarify. We have added **Table 6** in the Appendix (with a reference to this table in Section 5.4) with a summary of the experimental settings (in particular, the datasets used for each experimental phase). We hope this will make the experimental settings section more easily digestible with a single pass.\n\nWhen the score was revised down from 8 to 6, we observed that the diff of the revised review with respect to the original review was the following sentence: *\"Some missing points of comparison are approaches that leverage feedback for MT.\"*\n\nCould you kindly clarify which approaches that leverage feedback for MT are missing? Since no citations were provided, we are unable to comment on this further without additional information. Also, please note that leveraging feedback for MT is not the topic of this paper, so such a comparison may be out of scope.\n\nWe will now respond individually to each of your questions:\n1. *Is there any description of MetricX-XXL-QE in another paper? I could not find a paper describing except a paragraph in the Findings paper. How many languages does it support? This seems to be a very big QE model, 30B parameters.*\n\nThe description of the reference-based counterpart to MetricX-XXL-QE (trained on the same human judgements data and with the same model architecture) can be found in https://www2.statmt.org/wmt23/pdf/2023.wmt-1.63.pdf (MetricX-23: The Google Submission to the WMT 2023 Metrics Shared Task; Juraska et al., 2023). The only difference between the original MetricX model and the QE version we use is that the QE model input is constructed as the concatenation of the source segment, rather than the reference, with the candidate translation (as described in paragraph 3 of Section 2.3 in our paper). This model has a mT5-XXL backbone with 13B parameters.\n\nAs shown in the WMT'23 Metrics Shared Task Report (https://www2.statmt.org/wmt23/pdf/2023.wmt-1.51.pdf), MetricX generalizes well to language pairs that it didn't see during training, including low-resource language pairs. See, for example, Hebrew-English performance in Table 8.\n\n2. *How much time is added to the training process when using the MBR and QE reranking? Maybe a table illustrating the added wallclock time would be interesting to contrast with the gains in performance.*\n\nTo address your question, we have added **Table 7** (**Appendix B**) in our paper revision, which shows the time and compute requirements for performing MBR-BLEURT and QE-MetricX-XXL scoring, across all three experiment phases and across both language pairs. We use TPUs for BLEURT and MetricX-XXL scoring, as described in https://dl.acm.org/doi/fullHtml/10.1145/3360307. (The TPUv4 hardware is a faster and newer generation than TPUv3.) MBR scoring takes about 7.65 TPUv4 minutes/source sentence, while QE scoring takes about 4.2 TPUv3 seconds/source sentence. So even when using faster hardware for MBR, QE scoring is 109x faster. Please do let us know if you have additional questions about this new table.\n\n3. *How much does the data used to train the QE model and the utility function used in MBR affect the final results for specific domains? Do yo have any insights on this? For example, for English-German, it seems most of the MT training data is similar to the data used to train the QE and BLEURT models. What if this is not the case? Do you observe the same improvements or does something change in the translation quality of the best models?*\n\nIn the WMT'22 Metrics Shared Task (https://aclanthology.org/2022.wmt-1.2.pdf), the metrics (including BLEURT and MetricX-XXL) were evaluated on various domains, and they performed well even on the domains that they were not finetuned on (see **Figure 2**).\n\nMoreover, just as the utility functions generalize across domains, the same is true for the MBR-finetuned and QE-finetuned models which use these utility functions, as shown in **Table 9** in our paper. Also note that while the Phase 1 models were finetuned on past WMT test sets (with the source side partially overlapping the data used to train the utility functions), the Phase 2 and Phase 3 models were finetuned on Common Crawl data, and we do see that finetuning on Common Crawl improves generalization across domains.\n\nWe hope the additions we have made to our paper allay your concerns. Please let us know if you have any further questions or if we can provide any additional clarifications to help finalize your assessment and rating of our paper."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4008/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700259814248,
                "cdate": 1700259814248,
                "tmdate": 1700259814248,
                "mdate": 1700259814248,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "akhetAJEV7",
                "forum": "bkNx3O0sND",
                "replyto": "kQBzD4F9iC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4008/Reviewer_5sbu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4008/Reviewer_5sbu"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the answers. I've seen the discussion about comparison with other methods that leverage feedback and agree that this is somewhat tangential. Regarding the other questions, thank you for the the work, I think the paper improved after this discussion period. I updated the scores accordingly."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4008/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700736147932,
                "cdate": 1700736147932,
                "tmdate": 1700736147932,
                "mdate": 1700736147932,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]