[
    {
        "title": "On Formal Feature Attribution and Its Approximation"
    },
    {
        "review": {
            "id": "HWcEIV4hys",
            "forum": "CgU3bllpEQ",
            "replyto": "CgU3bllpEQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4509/Reviewer_2iyx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4509/Reviewer_2iyx"
            ],
            "content": {
                "summary": {
                    "value": "The authors of this study have outlined the shortcomings of current XAI algorithms and have put forward a new concept called Formal Feature Attribution (FFA) that offers more reliable results and can be approximated more efficiently. They have compared the results obtained from FFA with other state-of-the-art algorithms and have shown that their FFA approximation algorithm has unique properties that set it apart from the rest."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper did a good job of introducing the two important properties in XAI, abductive explanations (AXp's) and contrastive explanations (CXp's), and the attractive properties of having these two properties satisfied. The authors then proposed to use the scheme of formal feature attribution (FFA) to address these two properties. \n\nTo address the computational challenge of FFA, the authors further proposed an approximation algorithm of FFA that can be done efficiently. The empirical results demonstrated the unique properties of FFA compared to other SOTA algorithms."
                },
                "weaknesses": {
                    "value": "The work can potentially be improved from either the theoretical side or the experimental side. \n\nFrom theory direction:\nIt is not clear about the time complexity of the approximating formal feature attribution and the approximation error compared to the actual FFA. \n\nFrom the experimental direction:\nThe authors highlighted the advantages of FFA compared to existing algorithms. However, the evidence showing this is the case in this work is limited. \n1. The authors mentioned the \"unsoundness of explanations\" exists in prior work. It would be great to specify what type of unsoundness the authors specifically refer to, and explain why the FFA can increase such \"soundness\". The same justification is needed for the \"out-of-distribution sampling\" performance compared to existing works\n2. All the experiments on the comparison between FFA and other SOTA XAI algorithms are based on real-life examples. It is hard to evaluate, if two methods disagree with each other, which one is better.  Why don't the authors start with clean synthetic data, where the ground truth is known, and show where FFA generates better results than other XAI algorithms? And further show that the proposed approximation FFA algorithm did a better job than other XAI algorithms."
                },
                "questions": {
                    "value": "1. The authors mentioned that \"Formal explanation also tend to be larger than their model-agnostic counterparts...\", can you formally define what \"larger\" means in this context?\n2. The authors mentioned, \"We argue that formal feature attribution is hard for the second level of the polynomial hierarchy\". Can you clarify what \" the second level of the polynomial hierarchy\" specifically means here? When you say \"hard\", what do you mean specifically? Can you quantify the hardness or show any examples?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4509/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698613676703,
            "cdate": 1698613676703,
            "tmdate": 1699636427635,
            "mdate": 1699636427635,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "98DVwJxcHY",
            "forum": "CgU3bllpEQ",
            "replyto": "CgU3bllpEQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4509/Reviewer_vzsC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4509/Reviewer_vzsC"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes formal feature attributions (FFA). The proposed method assigns attribution scores to a feature corresponding to the share of valid logical explanations in which a feature appears. An algorithm to compute approximate and exact FFA is presented."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "*  Definitions and somewhat intuitive and are clear to follow.\n*  The algorithm to compute FFA seems to work and scales up to tree models and datasets of MNIST size. The experiments highlight that is is feasible to compute good approximations of FFA within a realistic runtime budget"
                },
                "weaknesses": {
                    "value": "*  Lack of related work. As there is no dedicated related work section, the relation to prior work can be strengthened. As this paper is dealing with feature attributions, there is a multitude of related approaches that could potentially be mentioned [1, 2, 3, 4]. There should be much more space invested to motivate why FFA are relevant and what practival problems they could potentially solve.\n*   The propositions don\u2019t have proofs. I cannot follow the argumentation leading up to Proposition 1 and 2. While they may be intuitive to experts, I think the exposition is insufficient for the average reader to follow. I agree that conference papers have limited space, but the proofs certainly need to be amenable to verification also by non-experts. I don\u2019t think is insufficient to make imprecise statements like \u201cthis is a direct consequence of a result in Huang et al.\u201d. The exact result should be named, and restated in the appendix together with an explanation for how the proposition can be derived from it. This also helps the reader to assess the novelty of this claims. \n*  Scope: The scope of the method is limited to Boosted-Trees. While I agree that this is a relevant model for tabular data, it is not the only one. This may limit the paper\u2019s potential impact.\n*   The evaluation metrics use FFA\u2019s as a ground truth (Table 2) and seem to hypothesize that LIME and SHAP should be approximating FFAs. The disagreement is obvious as SHAP and lime are meant to serve different purposes and are derived from different desiderata\n*  Some claims are exaggerated. For instance, \u201cIn this paper we define the first formal approach to feature attribution\u201d (Conclusion) is too strong and incorrect in its current form. Other attribution methods such as Shapley Values or IG [2] also have formal grounding, and can therefore also be considered formal approaches to feature attribution. I think such overly strong statements should be avoided.\n\n**Minor:**\nClarity of the write-up: Unfortunately, there are many writing/grammar issues in the paper, particularly in the introduction. I encourage the authors to check the writeup again. For instance, already the first sentence of the introduction seems a bit puzzling, as it does not mention what is growing and appears to be overcomplicated. What is meant by \u201cWe argue that formal feature attribution is hard for the second level of the polynomial hierarchy.\u201d (Section 1)? Usually, a comma is employed after \u201ce.g.\u201d and \u201ci.e.\u201d.\n\n## Summary\n\nA work on formal explainability that proposes a novel feature attribution method grounded in formal explanations (FFA). The algorithm to compute FFAs seems interesting and shows that these attributions can be computed for problems up to MNIST size. Unfortunately, there are many competing notions of attribution, the technical novelty seems limited, and the motivation for this approach is not extensive. Furthermore, following the draft is complicated by some missing explanations for the propositions and minor writing issues. While I don\u2019t think the work has substantial flaws, I still lean towards reject as of now.\n\n\n--------------------------------\n**References**\n\n[1] Lapuschkin, S., Binder, A., Montavon, G., Klauschen, F., M\u00fcller, K.-R., and Samek, W. On pixel-wise explanations for non-linear classi\ufb01er decisions by layer-wise relevance propagation. PloS one, 10(7):e0130140, 2015.\n\n[2] Sundararajan, M., Taly, A., and Yan, Q. Axiomatic attribution for deep networks. In International Conference on Machine Learning, pp. 3319\u20133328. PMLR, 2017.\n\n[3] Kasneci, G. and Gottron, T. Licon: A linear weighting scheme for the contribution of input variables in deep arti\ufb01cial neural networks. In Proceedings of the 25th ACM International on Conference on Information and Knowledge Management, pp. 45\u201354, 2016.\n\n[4] Petsiuk, V., Das, A., and Saenko, K. Rise: Randomized input sampling for explanation of black-box models. In Proceedings of the British Machine Vision Conference (BMVC), 2018."
                },
                "questions": {
                    "value": "*   What is the purpose of the weighted attributions, if they are \u2013 as claimed in the beginning of the experimental section \u2013 almost identical?\n*   Figure 5 and 6 (particularly) show that FFA are not necessarily meaningful for humans to understand the prediction. They suffer from the same drawbacks of all feature attribution explanations that reason on a pixel level, which is not a level human users usually reason in. Can the authors give a concrete use-case where FFA can however unfold advantages over other feature attribution techniques?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4509/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698701308515,
            "cdate": 1698701308515,
            "tmdate": 1699636427551,
            "mdate": 1699636427551,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "oANR9Q5pTQ",
            "forum": "CgU3bllpEQ",
            "replyto": "CgU3bllpEQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4509/Reviewer_LjFe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4509/Reviewer_LjFe"
            ],
            "content": {
                "summary": {
                    "value": "The authors argue that despite the success of AI and ML, there are still critical issues that need to be addressed, such as model brittleness, fairness, and interpretability. They propose a new approach to feature attribution based on formal explanation enumeration, which aims to provide a more sound and reliable method for understanding the weight of a feature in making a classification decision.\n\nThe authors begin by discussing the two major lines of work in XAI: feature selection methods and feature attribution techniques. They note that while these approaches show promise, they are susceptible to a range of critical issues, including explanation unsoundness and out-of-distribution sampling. They argue that a formal approach to feature attribution is needed to address these limitations.\n\nThe authors then introduce their proposed approach, which is based on the proportion of abductive explanations in which a feature occurs to weigh its importance. They show that this approach can be used to compute feature attribution for many classification problems, and when exact computation is not possible, effective approximations can be used. They compare their approach to existing heuristic approaches to feature attribution and show that they do not always agree with their proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The authors provide a theoretical framework for understanding the soundness and reliability of their approach. They show that their approach can be used to compute feature attribution exactly for many classification problems, which is a significant contribution. However, they also acknowledge that exact computation may not always be possible, and in such cases, effective approximations can be used. It would be useful for the authors to provide more details on the approximations used and their accuracy.\n\n2. The authors compare their approach to existing heuristic approaches to feature attribution and show that they do not always agree with their proposed method. This is an important contribution, as it highlights the limitations of existing methods and the need for a more formal approach to feature attribution. However, it would be useful for the authors to provide more details on the specific heuristic approaches they compared their method to and how they conducted the comparison."
                },
                "weaknesses": {
                    "value": "1. While the authors propose a new approach to feature attribution based on formal explanation enumeration, it is not entirely clear how this approach differs from existing methods. The authors briefly mention that their approach is based on the proportion of abductive explanations in which a feature occurs to weigh its importance, but it is not clear how this differs from existing methods such as LIME and SHAP. \n\n2. The authors focus primarily on the theoretical aspects of their approach and do not provide sufficient empirical evaluation. While the authors show that their approach can be used to compute feature attribution exactly for many classification problems, they do not provide sufficient empirical evaluation to demonstrate the practical usefulness of their approach. \n\n3. The authors do not provide sufficient details on the approximations used when exact computation is not possible. While the authors mention that effective approximations can be used, they do not provide sufficient details on the specific approximations used and their accuracy."
                },
                "questions": {
                    "value": "1. The authors propose a new approach to feature attribution based on formal explanation enumeration. Can you provide more details on how this approach differs from existing methods such as LIME and SHAP? What are the specific advantages of your approach?\n\n2. The authors show that their approach can be used to compute feature attribution exactly for many classification problems. Can you provide more details on the types of classification problems for which exact computation is possible? What are the limitations of your approach in terms of the types of problems it can handle?\n\n3. The authors mention that effective approximations can be used when exact computation is not possible. Can you provide more details on the specific approximations used and their accuracy? How were these approximations evaluated?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4509/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698718966719,
            "cdate": 1698718966719,
            "tmdate": 1699636427397,
            "mdate": 1699636427397,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "dWrtMSSDmE",
            "forum": "CgU3bllpEQ",
            "replyto": "CgU3bllpEQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4509/Reviewer_oi1W"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4509/Reviewer_oi1W"
            ],
            "content": {
                "summary": {
                    "value": "The paper builds on formal explainability, introducing Formal feature attribution. In formal explainability, the explanations are equated with abductive explanations (AXp's), which are subset-minimal sets of features formally proved to suffice to explain an ML prediction given a formal representation of the classifier of interest; AXp's requires a formal reasoner to check all combinations of feature values, which makes AXp\u2019s long and there can be various AXp\u2019s for a single data instance. The paper introduces formal feature attribution of a given feature as the proportion of abductive explanations where it occurs i.e it is the percentage of (formal abductive) explanations that make use of a particular feature $i$. To compute FFA AXp enumeration is required, the paper posed an approximation to that: instead of using MARCO targeting AXp\u2019s the paper proposes the target contrastive explanations (CXp) enumeration with AXp\u2019s as dual explanations the exact algorithm is shown in Algorithm 1.\n\nExperiments:\n\n- The paper tests FFA for gradient boosted trees for image and tabular datasets and compares there results with LIME, TreeSHAP, and KernalSHAP.\n\n- For dataset the paper considered MNIST but only as a binary classification problem, PneumoniaMNIST and 11 tabular datasets the maximum size of the input features for tabular dataset is 15.\n\n- The paper compared the exact formal attribution with the baselines on tabular dataset and showed that none of the baselines agree with FFA.\n\n- The paper then compared approximate FFA constrained by time with exact FFA and showed that they can gets good FFA approximations even if they only collect AXp\u2019s for a short time."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "### Originality -- Strong:\nThe idea of calculating feature attribution on top of formal explanations is original and novel. \n\n\n### Clarity -- Strong:\n- The paper is well-written and easy to follow.\n- The proposed method is well-motived and related work is clearly explained."
                },
                "weaknesses": {
                    "value": "Significance -- Weak:\n\n- The main drawback of the method is that it depends on the ability to compute Axp's. Although the paper does propose an approximation (algorithm 1) it is still can be quite expensive in practice.\n\n- Most current AI methods use deep neural networks, all experiments done in the paper used boosted tree this is because FFA requires the ML to have some first-order logic, which is not practical so the use of the proposed method in practice will be very limited.\n\n- The results and time comparison between exact FFA, approximate FFA and baselines is done on datasets with very small input space and binary classification problem. These results will probably not generalize for larger input space for example,  if LIME takes < 1 second for 10x10 images scaling for larger images like 256\u00d7256 lime will still be pretty quick if but FFA_10 will probably not give any useful results."
                },
                "questions": {
                    "value": "- Why were the datasets reduced to a binary classification problem rather than a multiclass problem?\n\n- For tabular data, can FFA be used on larger datasets trained like Rossmann Store Sales [1] i.e much larger input space if so how does it compare to LIME and others in terms of computation?\n\n- For the wide use of FFA, practitioners need to be able to use FFA on black-box neural networks, how can FFA (or a variation of the currently proposed FF) be used to explain neural networks like TabNet [2] with no first-order logic ?\n\n\n[1] https://www.kaggle.com/c/rossmann-store-sales/data\n\n[2] TabNet: Attentive Interpretable Tabular Learning"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4509/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699120063552,
            "cdate": 1699120063552,
            "tmdate": 1699636427325,
            "mdate": 1699636427325,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]