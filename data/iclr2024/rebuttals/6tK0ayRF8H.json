[
    {
        "title": "Angle-optimized Text Embeddings"
    },
    {
        "review": {
            "id": "LHilFD7wMs",
            "forum": "6tK0ayRF8H",
            "replyto": "6tK0ayRF8H",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission217/Reviewer_7GFK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission217/Reviewer_7GFK"
            ],
            "content": {
                "summary": {
                    "value": "This paper notices that existing text embedding models mainly use cosine function as a part of the objective function, but cosine function has a saturation zone, which may cause gradient vanishing problem and influence the quality of text embeddings. To mitigate this problem, this paper proposes to evaluate the angle difference between two text embeddings for optimization. Experiments on variable lengths of text datasets, including a newly introduced long-text dataset, are conducted to evaluate the performance of the proposed model."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper identifies an interesting research question, the gradiant vanishing problem appearing at the saturation zone of cosine function influences the quality of text embeddings.\n\n2. The proposed solution of using angle difference for optimization is orginal and novel.\n\n3. Experiments on semantic textual similarity task are sufficiently conducted."
                },
                "weaknesses": {
                    "value": "Despite an appealing motivation and an interesting solution, I still have the following concerns:\n\n1. From my point of view, the only technical contribution of this paper is to design how to evaluate angle difference. This contribution is indeed interesting, but is a bit superficial and insufficient for a long research paper of ICLR standard. I expect authors to propose more __insightful__ designs to better solve the gradient vanishing problem.\n\n2. The explanation of why saturation zone in cosine function influences text embedding learning is not clearly written at the Introduction section. Authors are suggested to explain more about the meaning of saturation zone and why it causes gradient vanishing problems.\n\n3. Usually we encourage authors to conduct the same experiment multiple times and report both mean and standard deviation, in order to verify that the proposed model indeed significantly outperforms baselines. However, I see mean but not standard deviation in the paper."
                },
                "questions": {
                    "value": "1. Authors use absolute value at Eq. 6. But absolute function in pytorch or tensorflow is not differentiable, how do authors deal with error backpropagation for absolute function?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission217/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission217/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission217/Reviewer_7GFK"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission217/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698698575148,
            "cdate": 1698698575148,
            "tmdate": 1699635947422,
            "mdate": 1699635947422,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Bh2OAiZtJf",
                "forum": "6tK0ayRF8H",
                "replyto": "LHilFD7wMs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission217/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission217/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We want to thank the reviewer for the professional and constructive comments. \n\n> **Weakness 1:** From my point of view, the only technical contribution of this paper is to design how to evaluate angle difference. This contribution is indeed interesting, but is a bit superficial and insufficient for a long research paper of ICLR standard. I expect authors to propose more insightful designs to better solve the gradient vanishing problem.\n\n> **Weakness 2:** The explanation of why saturation zone in cosine function influences text embedding learning is not clearly written at the Introduction section. Authors are suggested to explain more about the meaning of saturation zone and why it causes gradient vanishing problems.\n\n**Answer:** Thanks for your suggestions. We have updated the paper to include more insight and discussions in the Introduction Section. You can read the latest revision. We also provide an explanation in the following comments.\n\nIt is important to note that functions with saturation zones, such as sigmoid and tanh, can lead to the problem of gradient vanishing [1]. Of course, cosine also has this issue. This means that if two points $x_1$ and $x_2$ fall within the saturation zone, the outputs $f(x_1)$ and $f(x_2)$ will be very close to each other,  where $f(*)$ denotes the neural networks. As a result, the gradient  $\\Delta = f(x_1) - f(x_2) \\approx 0$ will cause the gradient to vanish during backpropagation. We have illustrated this issue in Figure 1. \n\nMany STS datasets, such as MNLI and SNLI, provide three supervised labels: entailment, neutral, and contradiction. In most cases, the boundary between neutral and entailment is vague, as some neutral pairs may resemble entailment pairs.\nFor instance, consider the following cases from SNLI: (1) the text pair of \\textit{Two blond women hug each other.} and \\textit{Some women hug each other on vacation.} are labeled as neutral, and (2) the text pair of \\textit{Two blonde women hugging.} and \\textit{There are women showing affection.} are labeled as entailment. The texts in the neutral pair appear very similar (a subtle difference) to each other and may fall within the saturation zone of cosine, resulting in gradient vanishing. This can confuse the model and lead to misidentification of the neutral pair as entailment.\nAdditionally, many other STS datasets, such as MRPC and QQP, provide binary labels representing dissimilar ($0$) and similar ($1$), which naturally fall within the saturation zone of the cosine function. \n\nCapturing such subtle differences poses a challenge for existing text embedding models. To the best of our knowledge, no existing work has investigated this issue. To address this issue, we propose a fresh angle optimization in complex space. Optimizing the angle difference in complex space is intuitive and practical for text embedding because if the angle difference between two text embeddings is smaller, it means that the two text embeddings are closer to each other in the complex space, i.e., their similarity is larger. Other manifold-based approaches also might help, but they are complicated and have relatively higher complexity, making the model inefficient. Efficiency is essential for text embedding learning, especially in the era of LLMs. Because LLMs have large-scale parameters and are hard to finetune. Compared to them, optimizing angle differences in complex space is more practical and efficient.\n\n---\n\n**Reference:**\n\n[1] Roodschild M, Gotay Sardi\u00f1as J, Will A. A new approach for the vanishing gradient problem on sigmoid activation[J]. Progress in Artificial Intelligence, 2020, 9(4): 351-360."
                    },
                    "title": {
                        "value": "Response to Reviewer 7GFK [1/2]"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission217/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699694612415,
                "cdate": 1699694612415,
                "tmdate": 1699949781217,
                "mdate": 1699949781217,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RtXmfvXhPA",
                "forum": "6tK0ayRF8H",
                "replyto": "LHilFD7wMs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission217/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission217/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 7GFK [2/2]"
                    },
                    "comment": {
                        "value": "---\n\n> **Weakness 3:** Usually we encourage authors to conduct the same experiment multiple times and report both mean and standard deviation, in order to verify that the proposed model indeed significantly outperforms baselines. However, I see mean but not standard deviation in the paper.\n\n**Answer:** Thanks for your suggestion. We think this suggestion can make our experiment more convincing. The reason we did not report the mean and standard deviation is that most STS baselines, including ours, follow SimCSE, a famous baseline in this task, to set a random seed = 42 for reproducibility. It is worth noting that after setting the random seed, the deviation difference is negligible, almost zero, in multiple runs. Notably, we have reported the p-value for the marginal results; the p-value indicates that our improvement is significant. We will add the mean and standard deviation in the camera-ready version.\n\n---\n\n> **Question 1:** Authors use absolute value at Eq. 6. But absolute function in pytorch or tensorflow is not differentiable, how do authors deal with error backpropagation for absolute function?\n\n**Answer:** The absolute value operation $abs(*)$ is indeed a commonly used operation in deep learning. It is utilized in various loss functions, such as the Triplet Margin Loss [2], as well as in normalizations like L1 normalization. In our proposed angle loss, we also employ the absolute value operation.\n\nWe think the non-differentiating is not a significant issue in deep learning frameworks like PyTorch and TensorFlow. Because even the widely-used activation function **ReLU is also non-differentiable**. The situation of $abs(*)$ is similar to ReLU. They are not differentiable at a singular point, $x=0$. Nevertheless, we can still use what is known as sub-derivatives [3] in the backpropagation algorithm. \n\n---\n\nFinally, we would like to claim our contribution again. We have investigated the gradient vanishing problem caused by the cosine saturation zone. To the best of our knowledge, no existing works have investigated this issue before. We have proposed a fresh approach to address this issue by introducing angle optimization in complex spaces. Extensive experimental results show that we achieve SOTA performance on STS tasks, which verifies the effectiveness of our proposed model.\n\nThank you once again for your valuable feedback!\n\n---\n\n**Reference:**\n\n[2] Balntas V, Riba E, Ponsa D, et al. Learning local feature descriptors with triplets and shallow convolutional neural networks[C]//Bmvc. 2016, 1(2): 3.\n\n[3] https://en.m.wikipedia.org/wiki/Subderivative?wprov=sfla1"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission217/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699950401722,
                "cdate": 1699950401722,
                "tmdate": 1699970236875,
                "mdate": 1699970236875,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FwhdxIvFmi",
            "forum": "6tK0ayRF8H",
            "replyto": "6tK0ayRF8H",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission217/Reviewer_NPkC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission217/Reviewer_NPkC"
            ],
            "content": {
                "summary": {
                    "value": "To overcome the negative impact of vanishing gradients caused by the cosine optimization function, this paper proposed a novel angle-optimized target to improve the quality of text embeddings. Moreover, this paper conducted extensive experiments to prove the effectiveness of the proposed method. Meanwhile, this paper also developed a novel long-text STS dataset to support the community."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThis paper proposed a novel angle-optimized target to enhance the learning ability of contrastive learning-based representation learning models, which tried to alleviate the problem of vanishing gradients. \n2.\tThis paper developed a novel long-text STS dataset to better evaluate the performance of representation learning models. \n3.\tThis paper also explored LLM-based supervised data generation and contrastive learning, which is very interesting."
                },
                "weaknesses": {
                    "value": "1.\tFirst of all, the authors argued that gradient vanishing problem is caused by the saturation zones in cosine functions in the optimization target. However, as far as I know, the gradient vanishing problem is mainly due to the deep structure. The saturation zones can be used to prove the high similarity between sentences. Therefore, the motivation of this paper is not so convincing. More explanations are needed. \n2.\tSecond, the authors focused on contrastive learning target, which limits the application range of the proposed method. The authors should provide more evidence to demonstrate the effectiveness of their method since their main contribution is adding an additional target in contrastive loss. \n3.\tThird, the related work in this paper is not sufficient enough. More content should be cited, such as different contrastive loss designs, sentence similarity measurement designs, etc."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission217/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698716370381,
            "cdate": 1698716370381,
            "tmdate": 1699635947353,
            "mdate": 1699635947353,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iHecloWXZk",
                "forum": "6tK0ayRF8H",
                "replyto": "FwhdxIvFmi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission217/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission217/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We want to thank the reviewer for the professional and constructive comments. \n\n> **Weakness 1:**  First of all, the authors argued that gradient vanishing problem is caused by the saturation zones in cosine functions in the optimization target. However, as far as I know, the gradient vanishing problem is mainly due to the deep structure. The saturation zones can be used to prove the high similarity between sentences. Therefore, the motivation of this paper is not so convincing. More explanations are needed.\n\n**Answer:** Thanks for your suggestions. We have updated our paper to include more discussion. You can read it in our paper's latest revision or the following comments. \n\nIt is important to note that functions with saturation zones, such as sigmoid and tanh, can lead to the problem of gradient vanishing [1], regardless of the depth of the network. Of course, cosine also has this issue. This means that if two points $x_1$ and $x_2$ fall within the saturation zone, the outputs $f(x_1)$ and $f(x_2)$ will be very close to each other, where $f(*)$ denotes the neural networks, it can be shallow or deep. As a result, the gradient  $\\Delta = f(x_1) - f(x_2) \\approx 0$ will cause the gradient to vanish during backpropagation. We have illustrated this issue in Figure 1. \n\nMany STS datasets, such as MNLI and SNLI, provide three supervised labels: entailment, neutral, and contradiction. In most cases, the boundary between neutral and entailment is vague, as some neutral pairs may resemble entailment pairs.\nFor instance, consider the following cases from SNLI: (1) the text pair of \\textit{Two blond women hug each other.} and \\textit{Some women hug each other on vacation.} are labeled as neutral, and (2) the text pair of \\textit{Two blonde women hugging.} and \\textit{There are women showing affection.} are labeled as entailment. The texts in the neutral pair appear very similar (subtle differences) to each other and may fall within the saturation zone of cosine, resulting in gradient vanishing. This can confuse the model and **lead to the misidentification of the neutral pair as entailment**.\n\nCapturing such subtle differences poses a challenge for existing sentence embedding models. To the best of our knowledge, no existing work has investigated this issue. To address this issue, we propose a fresh angle optimization in complex space. Optimizing the angle difference in complex space is intuitive and practical for text embedding because if the angle difference between two text embeddings is smaller, it means that the two text embeddings are closer to each other in the complex space, i.e., their similarity is larger. Our extensive experimental results have demonstrated the effectiveness of our proposed model.\n\n---\n\n> **Weakness 2:** Second, the authors focused on contrastive learning target, which limits the application range of the proposed method. The authors should provide more evidence to demonstrate the effectiveness of their method since their main contribution is adding an additional target in contrastive loss.\n\n**Answer:** In our ablation study, we have demonstrated the effectiveness of our proposed model. In particular, we observed that AnglE experienced a greater performance degradation without the angle objective than without the in-batch negative (ibn) objective, i.e., the contrastive objective. This suggests that angle optimization is important for improving text embedding. In addition, we find that **using the angle objective alone yields better performance than that using the ibn (contrastive) objective alone**. From the ablation study, we can also see that the angle optimization can mitigate the negative effects of the cosine function in the cosine objective (as shown in Equation (1), which is different from the contrastive learning). This evidence demonstrates the effectiveness of angle optimization for improving different models even used alone.\n\n---\n\n**Reference:**\n\n[1] Roodschild M, Gotay Sardi\u00f1as J, Will A. A new approach for the vanishing gradient problem on sigmoid activation[J]. Progress in Artificial Intelligence, 2020, 9(4): 351-360."
                    },
                    "title": {
                        "value": "Response to Reviewer NPkC [1/2]"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission217/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699694617649,
                "cdate": 1699694617649,
                "tmdate": 1699950485838,
                "mdate": 1699950485838,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mNklc0ZwJE",
                "forum": "6tK0ayRF8H",
                "replyto": "FwhdxIvFmi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission217/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission217/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer NPkC [2/2]"
                    },
                    "comment": {
                        "value": "> **Weakness 3:** Third, the related work in this paper is not sufficient enough. More content should be cited, such as different contrastive loss designs, sentence similarity measurement designs, etc.\n \n**Answer:** Thank you for your suggestions. Actually, we have cited many widely used contrastive works proposed in recent years, such as CT-BERT, SimCSE, ConSERT, DiffCSE, PromptCSE, and others, in the related work section and experiment section. Due to space limitations, we only briefly introduced them in these sections.  \n\nFollowing your advice, **we have presented a new table (Table 6 in Appendix section A.1) to list the similarity measurements and learning algorithms employed in various widely used text embedding models in the latest revision.** Notably, we observe that the cosine similarity is predominantly used across the majority of these models. This observation underscores the significance of our work, as our proposed model, AnglE, aims to address the negative effects of the saturation zones in cosine.\n\n\n\n---\nFinally, we would like to claim our contribution again. We have investigated the gradient vanishing problem caused by the cosine saturation zone. To the best of our knowledge, no existing works have investigated this issue before. We have proposed a fresh approach to address this issue by introducing angle optimization in complex spaces. Extensive experimental results show that we achieve SOTA performance on STS tasks, which verifies the effectiveness of our proposed model.\n\nThank you once again for your valuable feedback!"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission217/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699949196632,
                "cdate": 1699949196632,
                "tmdate": 1700450518326,
                "mdate": 1700450518326,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FDc0qfe2nz",
                "forum": "6tK0ayRF8H",
                "replyto": "FwhdxIvFmi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission217/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission217/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Kind reminder to look at the authors' reply"
                    },
                    "comment": {
                        "value": "Dear Reviewer NPkC:\n\nWe thank you for the precious review time and valuable comments. We have provided corresponding responses with elaborate discussions on the issues you have raised. We hope to talk more with you about whether or not your concerns have been taken care of appropriately. Please let us know if you have additional questions or ideas for improvement.\n\nLooking forward to your reply.\n\nAuthors."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission217/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700535468322,
                "cdate": 1700535468322,
                "tmdate": 1700535468322,
                "mdate": 1700535468322,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "UoPbGhJmhs",
            "forum": "6tK0ayRF8H",
            "replyto": "6tK0ayRF8H",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission217/Reviewer_3nS6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission217/Reviewer_3nS6"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel angle-optimized text embedding model to improve the semantic textual similarity (STS) tasks, by mitigating the vanishing gradients of cos similarity. Specifically, the authors employ a contrastive learning objective and introduce optimization in a complex space to address the saturation zone in the cosine function. Extensive experiments are conducted to show the effectiveness of the proposed method on various tasks including short-text STS, long-text STS, and domain-specific STS."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed method of calculating similarity looks novel to me.\n\n2. The impact of the method has the potential to be significant in many fields."
                },
                "weaknesses": {
                    "value": "1. According to the paper, the motivation for introducing a complex space is to deal with the vanishing gradient of cos. In this sense, it would be great if techniques like gradient clipping and gradient normalization could be compared. \n\n2. The writing can be improved. E.g., section 3.4 is a bit confusing to me. See my questions below.\n\n3. I am also worried about the empirical significance. In table 2, the proposed method only improves the performance marginally (<1%) compared to SimCSE-BERT. I appreciate the effort that the p-value is reported and yet the p-value is smaller than 0.05 according to the caption of table 2."
                },
                "questions": {
                    "value": "1. In section 3.4, X is decomposed into real part Xre and imaginary part Xim, both of which have dimension 1. However, in the context of contrastive learning / the use of cos similarity, X is often high dimensional. How do you decompose X? If I am not mistaken, this part is missing in the paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission217/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698819190810,
            "cdate": 1698819190810,
            "tmdate": 1699635947263,
            "mdate": 1699635947263,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "j9NQliPvYF",
                "forum": "6tK0ayRF8H",
                "replyto": "UoPbGhJmhs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission217/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission217/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We want to thank the reviewer for the professional and constructive comments. \n\n> **Weakness 1:** According to the paper, the motivation for introducing a complex space is to deal with the vanishing gradient of cos. In this sense, it would be great if techniques like gradient clipping and gradient normalization could be compared.\n\n**Answer:** To the best of our knowledge, techniques such as gradient clipping or gradient normalization are mainly used to prevent **exploding** gradients [1]. However, we mainly address the gradient **varnishing** problem caused by cosine. \nActually, gradient clipping is widely used in many STS baselines, such as SimCSE and SBERT. Following these baselines, we also adopted this technique in our training phase to avoid potential gradient explosion. \n\n---\n\n> **Weakness 2:** The writing can be improved. E.g., section 3.4 is a bit confusing to me. See my questions below.\n\n> **Question 1:** In section 3.4, X is decomposed into real part Xre and imaginary part Xim, both of which have dimension 1. However, in the context of contrastive learning / the use of cos similarity, X is often high dimensional. How do you decompose X? If I am not mistaken, this part is missing in the paper.\n\n**Answer:** We apologize for not providing a detailed implementation of the angle objective earlier due to space limitations. We have updated our paper. You can read the latest revision in Appendix Section A.5 or the following summarized comments.\n\nFirstly, our input batch data for training is as follows:\n\\begin{equation}\n    \\mathbf{X} = [\\mathcal{A}_1;\\mathcal{A}_2;\\mathcal{B}_1;\\mathcal{B}_2;...; \\mathcal{N}_1; \\mathcal{N}_2] \\in \\mathbb{R}^{2B\\times L\\times 2D},\n\\end{equation}\nwhere $\\mathcal{A}_1$ and $\\mathcal{A}_2$ are input embeddings for the texts A1 and A2. \nThey are supervised pairs with a positive or negative label. \n$2B$ is the batch size, $L$ is the padded sequence length, and $2D$ is the embedding size. \nWe then pass it to the encoder to obtain the contextual representation as follows:\n\\begin{equation}\n    \\begin{split}\n        \\mathbf{O} &= \\mathrm{encoder}(\\mathbf{X})\\\\\\\\\n        & = [\\mathbf{O}_A{_1}; \\mathbf{O}_A{_2}; \\mathbf{O}_B{_1}; \\mathbf{O}_B{_2}; ...; \\mathbf{O}_N{_1}; \\mathbf{O}_N{_2}]  \\in \\mathbb{R}^{2B\\times L\\times 2D} .\n    \\end{split}\n\\end{equation}\n\nWe extend the complex number to the complex vector (see appendix section A.5). For a complex vector $\\mathbf{c} = \\mathbf{a} + \\mathbf{b}i$, $\\mathbf{a}$ is a real vector representing the real part, $\\mathbf{b}$ is also a real vector denoting the imaginary part, and $i$ represents the imaginary unit. \n\nNotably, our embedding size is $2D$ instead of $D$. This is because our embedding includes two halves: the first half with size $D$ represents the real part $\\mathbf{X}^{re}$, and the second half with size $D$ represents the imaginary part $\\mathbf{X}^{im}$. Therefore, the embedding is presented as $[\\mathbf{X}^{re}; \\mathbf{X}^{im}]$. This representation strategy follows RotatE[2], which decomposes the entity embedding into the real part and imaginary part for knowledge graph embedding learning.\n\nTo compute the angle difference, we need two complex vectors. Specifically, we decompose the output through tensor slicing to obtain pairs: $\\mathbf{X}_i = \\mathbf{O}[::2] = [\\mathbf{O}_A{_1}; \\mathbf{O}_B{_1};...; \\mathbf{O}_N{_1}] \\in \\mathbb{R}^{B\\times L \\times 2D}$ and $\\mathbf{X}_j = \\mathbf{O}[1::2] = [\\mathbf{O}_A{_2}; \\mathbf{O}_B{_2};...; \\mathbf{O}_N{_2}]  \\in \\mathbb{R}^{B\\times L \\times 2D}$, where $[::]$ is the tensor slicing operation. \n\nNext, as discussed before, we can easily obtain the real and imaginary parts of pairs $\\mathbf{X}_i$ and $\\mathbf{X}_j$.\nThe real part and imaginary part are $\\mathbf{a} \\in \\mathbb{R}^{B\\times L \\times D}$ and $\\mathbf{b}  \\in \\mathbb{R}^{B\\times L \\times D}$ for $\\mathbf{X}_i$, and $\\mathbf{c}  \\in \\mathbb{R}^{B\\times L \\times D} $ and $\\mathbf{d}  \\in \\mathbb{R}^{B\\times L \\times D}$ for $\\mathbf{X}_j$. We denote $\\mathbf{z} = (\\mathbf{a}, \\mathbf{b}) \\in \\mathbb{C}$ and $\\mathbf{w} = (\\mathbf{c}, \\mathbf{d}) \\in \\mathbb{C}$. Next, we can compute the angle difference of pairs ($A_1$, $A_2$),  ($B_1$, $B_2$), and others, following Equations (4), (5), and (6) in the paper.\n\nWe also provide a figure (Figure 6) to illustrate intuitively the process of computing the angle difference. For more implementation details, you can read the appendix section A.5 in our latest revision paper.\n\n\n---\n\n**Reference**\n\n[1] Zhang J, He T, Sra S, et al. Why gradient clipping accelerates training: A theoretical justification for adaptivity[J]. arXiv preprint arXiv:1905.11881, 2019.\n\n[2] rotate: knowledge graph embedding by relational rotation in complex space (ICLR19)"
                    },
                    "title": {
                        "value": "Reponse to Reviewer 3nS6 [1/2]"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission217/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699694620694,
                "cdate": 1699694620694,
                "tmdate": 1699947380967,
                "mdate": 1699947380967,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Pronr9gyhU",
                "forum": "6tK0ayRF8H",
                "replyto": "UoPbGhJmhs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission217/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission217/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reponse to Reviewer 3nS6 [2/2]"
                    },
                    "comment": {
                        "value": "We want to thank the reviewer for the professional and constructive comments.\n\n> **Weakness 3:** I am also worried about the empirical significance. In table 2, the proposed method only improves the performance marginally (<1%) compared to SimCSE-BERT. I appreciate the effort that the p-value is reported and yet the p-value is smaller than 0.05 according to the caption of table 2.\n\n**Answer:** Although our results are marginally better than SimCSE-BERT in the transfer STS setting in terms of average scores, it is worth noting that AnglE-BERT outperforms SimCSE-BERT in 6 out of 7 STS datasets. Specifically, AnglE-BERT shows obvious improvement in STS13, STS15, STS16, STSB, and SickR datasets. The reported p-value for the average score confirms the significance of the marginal improvement in the average score. \nIn addition, our updated LLaMA results show that AnglE-LLaMA outperforms SimCSE-LLaMA, achieving 6 out of 7 best scores.\n\nOn the other hand, AnglE-BERT shows an **absolute improvement** over SimCSE-BERT in the non-transfer STS settings. \n\nThis evidence suggests that AnglE is effective and can be applied to both transfer and non-transfer settings.\n\n---\n\nFinally, we would like to claim our contribution again. We have investigated the gradient vanishing problem caused by the cosine saturation zone. To the best of our knowledge, no existing works have investigated this issue before. We have proposed a fresh approach to address this issue by introducing angle optimization in complex spaces. Extensive experimental results show that we achieve SOTA performance on STS tasks, which verifies the effectiveness of our proposed model.\n\n\nThank you once again for your valuable feedback!"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission217/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699947863198,
                "cdate": 1699947863198,
                "tmdate": 1699950541370,
                "mdate": 1699950541370,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RFudlBiN9r",
                "forum": "6tK0ayRF8H",
                "replyto": "UoPbGhJmhs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission217/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission217/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Kind reminder to look at the authors' reply"
                    },
                    "comment": {
                        "value": "Dear Reviewer 3nS6:\n\nWe thank you for the precious review time and valuable comments. We have provided corresponding responses with elaborate discussions on the issues you have raised. We hope to talk more with you about whether or not your concerns have been taken care of appropriately. Please let us know if you have additional questions or ideas for improvement.\n\nLooking forward to your reply.\n\nAuthors."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission217/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700535401108,
                "cdate": 1700535401108,
                "tmdate": 1700535401108,
                "mdate": 1700535401108,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xr77tYDQzN",
                "forum": "6tK0ayRF8H",
                "replyto": "UoPbGhJmhs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission217/Reviewer_3nS6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission217/Reviewer_3nS6"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. After reading all the review comments I tend to keep my rating unchanged."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission217/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700636284361,
                "cdate": 1700636284361,
                "tmdate": 1700636284361,
                "mdate": 1700636284361,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Xvqsg5vBOi",
            "forum": "6tK0ayRF8H",
            "replyto": "6tK0ayRF8H",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission217/Reviewer_DPjM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission217/Reviewer_DPjM"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new method called AnglE to address the vanishing gradient problem of optimizing cosine similarity in text embedding learning models. AnglE uses an angle-based optimization method to learn text embeddings in a complex space. The method is demonstrated to outperform state-of-the-art models on various semantic textual similarity (STS) tasks, including short-text STS, long-text STS, and domain-specific STS. Additionally, AnglE can be used with limited labeled data and LLM-annotated data, and it achieves competitive performance in these settings."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The paper addresses an important issue in optimizing the cosine similarity of learning text embeddings, and the proposed method is interesting and novel.\n* It introduces the GitHub Issues Similarity Dataset as a testbed for evaluating model performance on long-text STS tasks.\n* The proposed method achieves promising results on a wide range of STS tasks."
                },
                "weaknesses": {
                    "value": "* Some technical details are not clearly explained. For example, while the angle objective optimizes the text representations in a complex space, it's unclear how these complex vectors are obtained as the representations from language models are real vectors.\n* The paper seems to have missed discussions with a few important related studies. For example, [1] addresses the gradient vanishing issue by incorporating cosine distance in learning text embeddings, [2] designs angular softmax objectives to learn visual representations. The LLM-supervised learning procedure largely follows the prompt-based training data generation paradigm in [3,4,5]. While this part is not the major contribution of the paper, it's better to reference these related works as well.\n\nReferences:  \n- [1] \u201cSpherical Text Embedding.\u201d NeurIPS (2019).\n- [2] \u201cSphereFace: Deep Hypersphere Embedding for Face Recognition.\u201d CVPR (2017).\n- [3] \u201cGenerating Datasets with Pretrained Language Models.\u201d EMNLP (2021).\n- [4] \u201cGenerating Training Data with Language Models: Towards Zero-Shot Language Understanding.\u201d NeurIPS (2022).\n- [5] \u201cZeroGen: Efficient Zero-shot Learning via Dataset Generation.\u201d EMNLP (2022)."
                },
                "questions": {
                    "value": "* Could you explain how the complex vectors are obtained exactly from the language models?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission217/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698957897398,
            "cdate": 1698957897398,
            "tmdate": 1699635947089,
            "mdate": 1699635947089,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8USDbI3Uep",
                "forum": "6tK0ayRF8H",
                "replyto": "Xvqsg5vBOi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission217/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission217/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We want to thank the reviewer for the professional and constructive comments. \n\n\n> **Weakness1**: Some technical details are not clearly explained. For example, while the angle objective optimizes the text representations in a complex space, it's unclear how these complex vectors are obtained as the representations from language models are real vectors.\n\n> **Question1**: Could you explain how the complex vectors are obtained exactly from the language models?\n\n**Answer:** We apologize for not providing a detailed implementation of the angle objective earlier due to space limitations. We have updated our paper. You can read the latest revision in Appendix Section A.5 or the following summarized comments.\n\nFirstly, our input batch data for training is as follows:\n\\begin{equation}\n    \\mathbf{X} = [\\mathcal{A}_1;\\mathcal{A}_2;\\mathcal{B}_1;\\mathcal{B}_2;...; \\mathcal{N}_1; \\mathcal{N}_2] \\in \\mathbb{R}^{2B\\times L\\times 2D},\n\\end{equation}\nwhere $\\mathcal{A}_1$ and $\\mathcal{A}_2$ are input embeddings for the texts A1 and A2. \nThey are supervised pairs with a positive or negative label. \n$2B$ is the batch size, $L$ is the padded sequence length, and $2D$ is the embedding size. \nWe then pass it to the encoder to obtain the contextual representation as follows:\n\\begin{equation}\n    \\begin{split}\n        \\mathbf{O} &= \\mathrm{encoder}(\\mathbf{X})\\\\\\\\\n        & = [\\mathbf{O}_A{_1}; \\mathbf{O}_A{_2}; \\mathbf{O}_B{_1}; \\mathbf{O}_B{_2}; ...; \\mathbf{O}_N{_1}; \\mathbf{O}_N{_2}]  \\in \\mathbb{R}^{2B\\times L\\times 2D} .\n    \\end{split}\n\\end{equation}\n\nWe extend the complex number to the complex vector (see appendix section A.5). For a complex vector $\\mathbf{c} = \\mathbf{a} + \\mathbf{b}i$, $\\mathbf{a}$ is a real vector representing the real part, $\\mathbf{b}$ is also a real vector denoting the imaginary part, and $i$ represents the imaginary unit. \n\nNotably, our embedding size is $2D$ instead of $D$. This is because our embedding includes two halves: the first half with size $D$ represents the real part $\\mathbf{X}^{re}$, and the second half with size $D$ represents the imaginary part $\\mathbf{X}^{im}$. Therefore, the embedding is presented as $[\\mathbf{X}^{re}; \\mathbf{X}^{im}]$. This representation strategy follows RotatE[1], which decomposes the entity embedding into the real part and imaginary part for knowledge graph embedding learning.\n\nTo compute the angle difference, we need two complex vectors. Specifically, we decompose the output through tensor slicing to obtain pairs: $\\mathbf{X}_i = \\mathbf{O}[::2] = [\\mathbf{O}_A{_1}; \\mathbf{O}_B{_1};...; \\mathbf{O}_N{_1}] \\in \\mathbb{R}^{B\\times L \\times 2D}$ and $\\mathbf{X}_j = \\mathbf{O}[1::2] = [\\mathbf{O}_A{_2}; \\mathbf{O}_B{_2};...; \\mathbf{O}_N{_2}]  \\in \\mathbb{R}^{B\\times L \\times 2D}$, where $[::]$ is the tensor slicing operation. \n\nNext, as discussed before, we can easily obtain the real and imaginary parts of pairs $\\mathbf{X}_i$ and $\\mathbf{X}_j$.\nThe real part and imaginary part are $\\mathbf{a} \\in \\mathbb{R}^{B\\times L \\times D}$ and $\\mathbf{b}  \\in \\mathbb{R}^{B\\times L \\times D}$ for $\\mathbf{X}_i$, and $\\mathbf{c}  \\in \\mathbb{R}^{B\\times L \\times D} $ and $\\mathbf{d}  \\in \\mathbb{R}^{B\\times L \\times D}$ for $\\mathbf{X}_j$. We denote $\\mathbf{z} = (\\mathbf{a}, \\mathbf{b}) \\in \\mathbb{C}$ and $\\mathbf{w} = (\\mathbf{c}, \\mathbf{d}) \\in \\mathbb{C}$. Next, we can compute the angle difference of pairs ($A_1$, $A_2$),  ($B_1$, $B_2$), and others, following Equations (4), (5), and (6) in the paper.\n\nWe also provide a figure (Figure 6) to illustrate intuitively the process of computing the angle difference. For more implementation details, you can read the appendix section A.5 in our latest revision paper.\n\n\n---\n**Reference**\n\n[1] rotate: knowledge graph embedding by relational rotation in complex space (ICLR19)"
                    },
                    "title": {
                        "value": "Response to Reviewer DPjM [1/2]"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission217/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699694624034,
                "cdate": 1699694624034,
                "tmdate": 1699947404873,
                "mdate": 1699947404873,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PW4VLL2KjJ",
                "forum": "6tK0ayRF8H",
                "replyto": "Xvqsg5vBOi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission217/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission217/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer DPjM [2/2]"
                    },
                    "comment": {
                        "value": "We want to thank the reviewer for the professional and constructive comments.\n\n\n> **Weakness 2**: The paper seems to have missed discussions with a few important related studies. For example, [1] addresses the gradient vanishing issue by incorporating cosine distance in learning text embeddings, [2] designs angular softmax objectives to learn visual representations. The LLM-supervised learning procedure largely follows the prompt-based training data generation paradigm in [3,4,5]. While this part is not the major contribution of the paper, it's better to reference these related works as well.\n\n**Answer:** Thanks for pointing out these related works. We have addressed this issue in our latest revision of the paper by including the citations and discussing the differences between our approach and the referenced works in the introduction and experiment sections.\nRegarding work [1], it utilizes the marginal loss as the objective function and optimizes it through Riemannian optimization in spherical manifolds. As for work [2], they manipulate decision boundaries to produce an angular margin. \nOur work is different from theirs. We compute the angle difference in complex space and aim to address the negative effect caused by the saturation zone of cosine.\n\nFor other related works on LLM-based training data generation, we have cited them in Section 4.5 in the latest revision of our paper.\n\n---\n\nFinally, we would like to claim our contribution again. We have investigated the gradient vanishing problem caused by the cosine saturation zone. To the best of our knowledge, no existing works have investigated this issue before. We have proposed a fresh approach to address this issue by introducing angle optimization in complex spaces. Extensive experimental results show that we achieve SOTA performance on STS tasks, which verifies the effectiveness of our proposed model.\n\nThank you once again for your valuable feedback!"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission217/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699946644219,
                "cdate": 1699946644219,
                "tmdate": 1699947050685,
                "mdate": 1699947050685,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BJjFIGOLY1",
                "forum": "6tK0ayRF8H",
                "replyto": "Xvqsg5vBOi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission217/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission217/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Kind reminder to look at the authors' reply"
                    },
                    "comment": {
                        "value": "Dear Reviewer DPjM:\n\nWe thank you for the precious review time and valuable comments. We have provided corresponding responses with elaborate discussions on angle optimization, which we hope to address your concerns. We hope to talk more with you about whether or not your concerns have been taken care of appropriately. Please let us know if you have additional questions or ideas for improvement.\n\nLooking forward to your reply.\n\nAuthors."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission217/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700535278336,
                "cdate": 1700535278336,
                "tmdate": 1700535278336,
                "mdate": 1700535278336,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fcW0pI5StD",
                "forum": "6tK0ayRF8H",
                "replyto": "BJjFIGOLY1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission217/Reviewer_DPjM"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission217/Reviewer_DPjM"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors"
                    },
                    "comment": {
                        "value": "I thank the authors for their response. I'm keeping my score since it was given under the expectation that the authors could provide clarifications for my questions."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission217/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700712806720,
                "cdate": 1700712806720,
                "tmdate": 1700712806720,
                "mdate": 1700712806720,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]