[
    {
        "title": "ECoFLaP: Efficient Coarse-to-Fine Layer-Wise Pruning for Vision-Language Models"
    },
    {
        "review": {
            "id": "ZInbnkNNVf",
            "forum": "iIT02bAKzv",
            "replyto": "iIT02bAKzv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6256/Reviewer_q2nJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6256/Reviewer_q2nJ"
            ],
            "content": {
                "summary": {
                    "value": "This article primarily addresses the task of model compression for Large Vision-\nLanguage Models (LVLMs). Traditional iterative global pruning methods involve\ncomputationally expensive operations, while layer-wise pruning approaches lack a\nglobal perspective, potentially resulting in suboptimal performance after pruning. This\npaper proposes a two-stage coarse-to-fine weight pruning approach for LVLMs, which\nutilizes the global importance score for unstructured pruning."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper introduces a layer-wise pruning method for LVLMs that utilizes\nglobal importance scores. The global importance score is obtained by\napproximating the first-order gradients of the model parameters. By\nconsidering the global perspective, this approach aims to effectively identify\nand prune less important weights in each layer of the LVLM model.\n2. This paper clearly highlights the challenges dealing multimodal compression\ncompared to single-modal model compression. The modularization of\nmultimodal models makes compression more challenging, and the paper\nprovides visualizations of gradients that effectively demonstrate the imbalance\nin magnitude and gradient distributions between vision and language models.\n3. Experimental results have shown that this method is effective across various\nbackbones and modal models."
                },
                "weaknesses": {
                    "value": "1. The method in this paper is relatively simple, and novelty is insufficient.\n2. The paper's writing could be improved as it lacks a theoretical analysis in the\nmethod introduction section to explain the effectiveness of the proposed method.\nAdditionally, there is a scarcity of formulas in the paper, and many details are\nnot adequately clarified.\n3. The experiments conducted were not comprehensive enough.\na) The validation of the models in the experiments was also insufficient. Only\nthe compression effects on the BLIPs model were tested, and the widelyused\nCLIP model, which is a mainstream multimodal model, was not\nevaluated. Furthermore, the comparison with the UPop model was only\nconducted on the NLVR2 and COCO Caption datasets, without\ncomparisons on other datasets such as Flickr30k and VQA2.0.\nb) The ablation study experiments were somewhat simplistic and did not\nsubstantiate the fundamental reasons for the effectiveness of the proposed\nmethod. It would be beneficial to enhance the content of the ablation study\nexperiments to provide a more detailed analysis and strengthen the overall\ncredibility of the article.\n4. Some minor mistakes:\nFor example, \u201cand then convert the scores to sparsity by three steps: (1)\nCompute the total parameters that need to be selected based on p, (2) Normalize\nthe scores, (3) Compute the parameters that should be picked for each layer, (4)\nObtain the sparsity for each layer based on the number of parameters to be\npicked and the parameters of this layer. \u201d\nThe phrase \"three steps\" in this sentence can also be changed to \"four steps.\""
                },
                "questions": {
                    "value": "See the weakness parts."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6256/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698627806781,
            "cdate": 1698627806781,
            "tmdate": 1699636684756,
            "mdate": 1699636684756,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "44bFfEfHSx",
                "forum": "iIT02bAKzv",
                "replyto": "ZInbnkNNVf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6256/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6256/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for your review and comments. We provide our response in the following.\nWe strongly believe we have clearly addressed all your concerns. Please don\u2019t hesitate to let us know if your concerns/questions are still not clearly resolved. We are open to active discussion and will do our best to address your concerns during the rebuttal period.\n\n---\n\n> **C1. It lacks a theoretical analysis in the method introduction section to explain the effectiveness of the proposed method. And the method in this paper is relatively simple, and novelty is insufficient.**\n\n\nWe respectfully disagree that theoretical analysis is necessary for our introduction since we already provided clear motivations/challenges for tackling efficient pruning for large vision-language models. And, we propose a new method the coarse-to-fine layer-wise pruning approach upon these motivations suggested :\n\n1) layer-wise one-shot pruning used for large models cannot figure out the optimal pruning rate per layer due to the lack of global (i.e., entire model) information (please see Introduction and Section 3.2).\n\n2) In large multi-modal models, training modules for different modalities show a significant disparity in their weight and gradient scales. This distributional gap in different modules makes layer-wise one-shot pruning further challenging, and often fails to preserve the model performance with a higher sparsity ratio (please see Figure 1 and Figure 4).\n\n\nFrom the perspective of novelty, our ECoFLaP takes unique advantages from both layer-wise and global pruning methods. EcoFLaP preserves the performance of full-precision models by computing intra-/inter weight importance (global pruning), yet this process is rapid and efficient (layer-wise pruning) through **our proposed two-step pruning procedures with zeroth-order approximation**. We utilize zeroth-order gradients for global information and bypass the need to construct a backpropagation graph, and thus **our approach can use a similar amount of GPU memory as layer-wise pruning**. To the best of our knowledge,**there has been no previous work similar to our idea for network pruning** and we believe this novel approach sufficiently contributes to the large and/or multimodal model pruning fields with clear insights.\n\nTo demonstrate the effectiveness of the proposed method, we provide extensive empirical evidence/analyses with multiple architectures (BLIP2, FlanT5, ViT, LLaMA) and tasks (VQA, Captions, MMLU, ImageNet, WikiText-2, etc), and we also report the results on CLIP in the next response. In those experiments, our approach outperforms the baseline by a margin in the high-sparsity regime, and we believe that **compared to theoretical analysis, the actual experimental results are the most straightforward and useful way to demonstrate the effectiveness of our approach**. \n\n---\n\n> **C2. there is a scarcity of formulas in the paper, and many details are not adequately clarified.**\n\nWe politely disagree with the notion that using many formulas necessarily makes the paper clearer. And we believe we already have included all the essential equations required to explain the algorithm effectively. \nCould you please help specify which parts you think are unclear, so we can address them more clearly/thoroughly in our rebuttal?"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6256/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700248784112,
                "cdate": 1700248784112,
                "tmdate": 1700585879798,
                "mdate": 1700585879798,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jNr08CAPtR",
                "forum": "iIT02bAKzv",
                "replyto": "ZInbnkNNVf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6256/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6256/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> **C3. The experiments conducted were not comprehensive enough. a) The validation of the models in the experiments was also insufficient. Only the compression effects on the BLIPs model were tested, and the widelyused CLIP model, which is a mainstream multimodal model, was not evaluated.**\n\nThank you for your suggestions on more experiments on CLIP. However, as we mentioned in the Introduction and Section 3.1, in the paper, we focused on the modularized vision-language model, in which the vision and language components are trained independently. We also found this kind of modularized VL model (Flamingo, MiniGPT series, LLaVA, Qwen-VL, etc) becomes more common as the strong LLM emerges, and therefore our study will be useful for these VL models. On the other hand, we didn\u2019t conduct experiments on CLIP because its vision and language transformers are trained together, which makes it not in our target models. However, we still perform the experiments on CLIP during the rebuttal, and show the results in the following table. We use 11 classification tasks to evaluate the zero-shot performance of CLIP before and after pruning. We ran Wanda and ECoFLaP over the sparsity 0.3 to 0.5, and we observed that ECoFLaP outperforms Wanda in the three sparsity ratios (we only report the results on sparsity 0.4 to make the reply succinct). In the 0.4 sparsity, **our approach shows a 5% to 18% improvement over Wanda in the 11 tasks**. We have included this experiment in Table 8 and Appendix F of our revision.\n\n| Methods                |  caltech101 | dtd | eurosat | fgvc_aircraft | food101 | imagenet | oxford_flowers | oxford_pets | stanford_cars | sun397 | ucf101 |\n|------------------------|---|---|---|---|---|---|---|---|---|---|---|\n| Full Model             |  92.9 | 44.5 | 47.8 | 24.8 | 86.1 | 66.7 |    71.3  | 89.1 | 65.3 | 62.6 |66.8 |\n| Wanda @ 0.4 sparsity   | 78.9 | 23.9 | 21.1  | 5.9  | 50.0 | 36.8 |    30.4 | 56.0  | 24.6  | 42.9  | 48.2|\n| ECoFLaP @ 0.4 sparsity | **86.6** | **31.7** | **27.1** | **12.9** | **65.0** | **48.7** |   **46.8**  | **74.6**  | **40.1**  | **53.9** | **57.7** |\n---\n\n> **C4. Furthermore, the comparison with the UPop model was only conducted on the NLVR2 and COCO Caption datasets, without comparisons on other datasets such as Flickr30k and VQA2.0.** \n\nWe also ran the Flickr30k experiments to compare with UPop and show the results in the following Table. We show that ECoFLaP outperforms Wanda in the no-fine-tuning setting, and outperforms UPop after the fine-tuning with almost no performance drop. This trend is the same as what we found in the NLVR2 and COCO experiments. We are running the VQA experiments and will release the results soon.\n\n\n| Methods                   | Flickr30k (TR@1/IR@1) |\n|---------------------------|-----------------------|\n| Full model                | 96.8/86.9             |\n| Wanda (w/o fine-tuning)   |  85.3/72.3    (-11.5/-14.6)           |\n| ECoFLaP (w/o fine-tuning) | 90.2/79.5  (-6.6/-7.4)              |\n| UPop (w/ fine-tuning)     | 94.0/82.0  (-2.8/-4.9)             |\n| ECoFLaP (w/ fine-tuning)  |   **96.8**/**85.6** (-0.0/-1.3)            |"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6256/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700249606864,
                "cdate": 1700249606864,
                "tmdate": 1700338897927,
                "mdate": 1700338897927,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "L3EXZQaLjd",
                "forum": "iIT02bAKzv",
                "replyto": "ZInbnkNNVf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6256/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6256/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "---\n> **C5. The ablation study experiments were somewhat simplistic and did not substantiate the fundamental reasons for the effectiveness of the proposed method. It would be beneficial to enhance the content of the ablation study experiments to provide a more detailed analysis and strengthen the overall credibility of the article.**\n\nTo show the benefits of our main novelty in **\u201clayer-wise pruning with global dynamic sparsity ratios\u201d**, we demonstrated that our proposed approach is stronger than the plain layer-wise pruning methods on various models and tasks in Table 1, Figure 4, and Figure 7. **We believe these experiments have proved the effectiveness of our hypothesis on \u201cdynamic layer ratios are critical for layer-wise pruning\u201d**, and this is the fundamental reason that our approach outperforms the baselines. Moreover, in Table 1, we also show that **our approach uses a lot less GPU memory than other methods using first-order gradients**, and this justifies our idea of replacing first-order gradients with forward-only zeroth-order gradients.\n\nFor the ablation studies, we **comprehensively presented the hyperparameter choice of the zeroth-order optimization**, which is the main component of our approach. We showed that the results are quite **robust to the hyperparameters of the zeroth-order optimization**. In the last paragraph of Section 6.2 (ablation study) and Figure 6, we also visualized the loss landscape of BLIP-2 and found the landscape is bell-shaped with a smooth basin and a single local minimum. **This loss landscape also justifies the success of utilizing zeroth-order optimization** as we avoid sampling weights located in other basins that may cause an inaccurate gradient estimation.\n\nIn sum, we believe that our experiments have adequately justified the main reason for the effectiveness of our approach and hypothesis. Could you please be more specific about which part of the experiments is unclear or what hypothesis needs to be clarified more, so we try to address them during the rebuttal?\n\n---\n\n> **C6. Some minor mistakes: For example, The phrase \"three steps\" in this sentence can also be changed to \"four steps.\"**\n\nThank you for catching the typo, and we have incorporated it in our revision.\n\n---\n\n*If our response addresses your concerns, please consider increasing the scores. Also, feel free to ask follow-up questions during the rebuttal period.*"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6256/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700249748306,
                "cdate": 1700249748306,
                "tmdate": 1700249748306,
                "mdate": 1700249748306,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4wj45bxNIi",
            "forum": "iIT02bAKzv",
            "replyto": "iIT02bAKzv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6256/Reviewer_5Mvp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6256/Reviewer_5Mvp"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to develop an efficient pruning methods for BLIP like multimodal models. Unlike previous layer-wise pruning methods, this paper proposed a global importance score which can be efficiently approximated using the global model gradients. Layer-wise weight pruning then was applied on the multimodal model. The proposed methods were compared with recently developed pruning methods and consistently improve upon existing methods on accuracy with the same level of sparsity."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. This paper is easy to consume and logically smooth.\n2. The scope of this paper is well defined, which is to prune Blip-like multimodal architecture. Challenges of pruning Blip-like models were presented clearly.\n3. The key idea of using zeroth-order approximated gradient makes computing the global important score efficient, which is useful."
                },
                "weaknesses": {
                    "value": "1. In addition to the numerical comparison, can you show some example results that compare the before and after results? \n2. While it is not easy to measure the real performance improvement, can you discuss about it with the proposed pruning method?\n3. Based on the importance score, what are important layers? What's the distribution of scores across all layers?"
                },
                "questions": {
                    "value": "Please refer to the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6256/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698795618832,
            "cdate": 1698795618832,
            "tmdate": 1699636684627,
            "mdate": 1699636684627,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pjnb61e8I7",
                "forum": "iIT02bAKzv",
                "replyto": "4wj45bxNIi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6256/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6256/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for your review and comments. We provide our response in the following.\nWe strongly believe we have clearly addressed all your concerns. Please don\u2019t hesitate to let us know if your concerns/questions are still not clearly resolved. We are open to active discussion and will do our best to address your concerns during the rebuttal period.\n\n---\n\n> **C1. In addition to the numerical comparison, can you show some example results that compare the before and after results?**\n\nWe conducted an analysis comparing the predictions of the Full model (before pruning) and the model pruned by ECoFlaP (with 0.5 sparsity) on the GQA datasets, and presented three illustrative examples in Table 9 (in Appendix G) of our revision. Please refer to them for the details.\n\nWe found one interesting observation: out of 12578 questions, the Full model correctly answered 1219 questions that our model did not, yet our model also accurately responded to 1072 questions where the Full model failed to respond. **This indicates that pruning does not always lead to performance degradation; in some instances, it may even enhance model robustness for certain questions** [1]. During our review of incorrect predictions by either our model or the Full model, we noticed that most were conceptually similar to the ground truth. This suggests that the pruned model still retains most of the capability. Furthermore, in some cases, such as with the 'horses running' example, both models provided answers that might be considered correct by many humans, even though they differ from the ground truth.\n\n[1] Zhangheng Li, Tianlong Chen, Linyi Li, Bo Li, Zhangyang Wang, Can Pruning Improve Certified Robustness of Neural Networks?\n\n---\n\n> **C2. While it is not easy to measure the real performance improvement, can you discuss about it with the proposed pruning method?**\n\nIn this paper, we evaluate the pruning approaches on large pre-trained models (BLIP2, FlanT5, ViT, LLaMA) with downstream tasks in a zero-shot setting, and this approach is one of the most common ways to benchmark pre-trained models. With this evaluation approach, we demonstrated that ECoFLaP has an improvement over the compared approaches on various architectures and downstream tasks in Table 1, Figure 4, and Figure 7. While we have tried our best to evaluate the models in this way, could you elaborate more on the method in your mind that can measure the real performance improvement, so we can conduct the method during the rebuttal phase?\n\n---\n\n> **C3. Based on the importance score, what are important layers? What's the distribution of scores across all layers?**\n\nOur sparsity ratios for layers are computed by the importance scores and thus the ratios can reflect the importance, for example, the higher sparsity for one layer denotes that this layer is less important for the tasks. In Figure 5, we can see that the sparsity ratio generally is lower for the vision transformer and higher for the language model, and this suggests that the language model is more prunable and has more redundancy for the vision-language tasks.\n\n---\n\n*If our response addresses your concerns, please consider increasing the scores. Also, feel free to ask follow-up questions during the rebuttal period.*"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6256/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700248669471,
                "cdate": 1700248669471,
                "tmdate": 1700338829991,
                "mdate": 1700338829991,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zShClMcNmh",
            "forum": "iIT02bAKzv",
            "replyto": "iIT02bAKzv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6256/Reviewer_GD72"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6256/Reviewer_GD72"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses the challenges of deploying Large Vision-Language Models (LVLMs) due to their high computational and energy costs. Traditional global pruning methods are costly, and recent layer-wise pruning approaches lack a global perspective. To overcome this, the paper introduces Efficient Coarse-to-Fine Layer-Wise Pruning (ECoFLaP), a two-stage approach that leverages global importance scores for determining sparsity ratios and then performs efficient layer-wise weight pruning. The computation of global importance scores is achieved using the zeroth-order approximate gradient through a forward-forward algorithm.   ECoFLaP demonstrates significant performance improvements in both multimodal and single-modal models, particularly in high-sparsity scenarios. Notably, it achieves these improvements while using only 40% of GPU memory compared to backpropagation and maintaining competitive accuracy."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ Global pruning indeed faces challenges in sparsity allocation, particularly for multi-modal models. The paper provides a thorough and persuasive analysis of the motivation behind this issue.\n+ The method presented in the paper is straightforward yet proven to be effective, and the experiments conducted are comprehensive.\n+ The manuscript is readily understandable and offers sufficient experimental details for reproducibility."
                },
                "weaknesses": {
                    "value": "- There is a need for a more detailed description of Algorithm 1.\n- Several metrics are used for importance scores, including weight magnitude, the multiplication of gradient and weight magnitude, gradient only, and sensitivity analysis. It would be beneficial to provide a comprehensive comparison of these metrics.\n- There are some typos in this paper: (1) In Section 4.1, \"and then convert the scores to sparsity by three steps\" should be \"and then convert the scores to sparsity by four steps\"? (2) In Section 4.2, \"like less than 5B parameters\" should be \"Like less than 5B parameters\"?"
                },
                "questions": {
                    "value": "- The sparsity ratios in the experiments cover a range from 0.1 to 0.6. Is it possible to extend this range to include a wider spectrum of sparsity ratios?\n- Can this method be combined with SparseGPT?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6256/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6256/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6256/Reviewer_GD72"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6256/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698804565289,
            "cdate": 1698804565289,
            "tmdate": 1699636684493,
            "mdate": 1699636684493,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "e9AHEoW7nm",
                "forum": "iIT02bAKzv",
                "replyto": "zShClMcNmh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6256/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6256/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for your review and comments. We provide our response in the following.\nWe strongly believe we have clearly addressed all your concerns. Please don\u2019t hesitate to let us know if your concerns/questions are still not clearly resolved. We are open to active discussion and will do our best to address your concerns during the rebuttal period.\n\n---\n\n> **C1. There is a need for a more detailed description of Algorithm 1.**\n\nSorry to hear that you feel further details are needed in Algorithm 1. However, we made effort to providing  extensive explanations of our algorithm using comments in Algorithm 1 and also used the equations in Section 4.1 as support. Could you please elaborate more detail which part is unclear so we can improve it during the rebuttal phase?\n\n\n---\n\n> **C2. Several metrics are used for importance scores, including weight magnitude, the multiplication of gradient and weight magnitude, gradient only, and sensitivity analysis. It would be beneficial to provide a comprehensive comparison of these metrics.**\n\nThank you for your suggestion. We presented a table to compare the approaches in Table 1 in terms of whether they are global or not and the used importance measure. We have included this table in our revision (Please see Table 6 in the Appendix). \n\n| Methods                  | Global or Layer-wise | Importance Measure                                                                |\n|--------------------------|----------------------|-----------------------------------------------------------------------------------|\n| Global Magnitude Pruning | Global               | $\\| W \\|_{ij}$                                                                   |\n| Iterative OBD Pruning    | Global               | $ \\| W \\| _{ij} \\cdot \\| dL / dW \\| _{ij}$                                               |\n| SparseGPT                | Layer-wise           | $[\\|W\\|^ 2 / diag (XX^T + \u03bbI)^{\u22121}]_{ij}$                                         |\n| Wanda                    | Layer-wise           | $\\|W_{ij}\\| \\cdot \\|X_{j}\\|^2 $                                                   |\n| ECoFLaP (zeroth-order)   | Mixed     | First Stage: $ \\| dL / dW \\| _{ij} $ Second Stage: $\\|W _{ij}\\| \\cdot \\|X _{j}\\|^2 $ |\n\n---\n\n> **C3. There are some typos in this paper: (1) In Section 4.1, \"and then convert the scores to sparsity by three steps\" should be \"and then convert the scores to sparsity by four steps\"? (2) In Section 4.2, \"like less than 5B parameters\" should be \"Like less than 5B parameters\"?**\n\nThank you for catching the typos. We have incorporated them in the revision.\n\n---\n\n> **C4. The sparsity ratios in the experiments cover a range from 0.1 to 0.6. Is it possible to extend this range to include a wider spectrum of sparsity ratios?**\n\nThank you for your suggestion on evaluation with a wider sparsity range. During the rebuttal period, we have compared Wanda and ECoFLaP at 0.7 sparsity in the following table. \n\nAs shown, ECoFLaP consistently outperforms Wanda by a significant margin, especially on NoCaps and Flickr30k (30% up improvement). This is because recent layer-wise pruning approaches can only compute local (i.e., within-layer) importance, which leads to suboptimal compression, while our proposed method can efficiently and accurately prune model weights based on estimated global importance. In this case, our approach **assigns lower pruning ratios on the vision transformer, and this prevents a serious performance drop at such a high sparsity**. \n\n| **Methods @ 0.7 sparsity** | **VQA (Acc)** | **OK-VQA (Acc)** | **GQA (Acc)** | **NoCaps (CIDEr/SPICE)** | **Flickr30k (TR@1/IR@1)** |\n|--------------------|---------------|------------------|---------------|--------------------------|---------------------------|\n| Wanda                      | **4.76**          | 3.76             | 2.96          |  8.13/5.97         | 7.6/22.76                 |\n| ECoFLaP (zeroth-order)     | **4.76**          | **5.05**             | **4.14**          | **38.28**/**8.74**         | **78.1**/**68.34**      |"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6256/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700248163911,
                "cdate": 1700248163911,
                "tmdate": 1700337991991,
                "mdate": 1700337991991,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "B30OaiYw6M",
                "forum": "iIT02bAKzv",
                "replyto": "zShClMcNmh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6256/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6256/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> **C5. Can this method be combined with SparseGPT?**\n\nThank you for your constructive suggestion. As the key contribution of our method for efficient estimation of layer-adaptive pruning rate for layer-wise pruning is orthogonal to SparseGPT, ECoFLaP can also be combined with SparseGPT. As shown in the table below, EcoFLaP+SparseGPT consistently achieves superior performance over multiple tasks. \n \nAdditionally, the performance improvement seems relatively smaller compared to ECoFLaP with Wanda (i.e., original EcoFLaP). We hypothesize that this is because the reweighting mechanism in SparseGPT is somewhat correlated to our dynamic sparsity ratios, where both of them are important to preserve the performance in high sparsity ratios. We have added this discussion and results in Appendix E and Table 7 of our revision.\n\n| **Methods @ 0.6 sparsity** | **VQA (Acc)** | **OK-VQA (Acc)** | **GQA (Acc)** | **NoCaps (CIDEr/SPICE)** | **Flickr30k (TR@1/IR@1)** |\n|--------------------|---------------|------------------|---------------|--------------------------|---------------------------|\n| SparseGPT    | 48.68  | 27.88      | 35.10       |  101.04/13.41       | 95.2/84.98 |\n| ECoFLaP on SparseGPT (zeroth-order)     | **50.36**          | **27.93**             | **36.26**          | **105.25**/**13.96**   | **96.3**/**86.12**      |\n\n---\n\n*If our response addresses your concerns, please consider increasing the scores. Also, feel free to ask follow-up questions during the rebuttal period.*"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6256/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700248197567,
                "cdate": 1700248197567,
                "tmdate": 1700338088544,
                "mdate": 1700338088544,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Yuf20GHrKb",
                "forum": "iIT02bAKzv",
                "replyto": "zShClMcNmh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6256/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6256/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Gentle reminder of the rebuttal deadline"
                    },
                    "comment": {
                        "value": "Dear reviewer GD72: \n\nThis is a reminder that tomorrow (Nov 22) is the last day of the rebuttal, and we would like to follow up to see if the response addresses your concerns or if you have any further questions. We would really appreciate the opportunity to discuss this further if our response has not already addressed your concerns. If our response addresses your concerns, please kindly consider increasing the scores. Thank you again!"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6256/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700588950983,
                "cdate": 1700588950983,
                "tmdate": 1700588950983,
                "mdate": 1700588950983,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "aKsGXI4Iu4",
            "forum": "iIT02bAKzv",
            "replyto": "iIT02bAKzv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6256/Reviewer_4r21"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6256/Reviewer_4r21"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents the Efficient Coarse-to-Fine Layer-Wise Pruning (ECoFLaP) method for Large Vision-Language Models. The method utilizes zeroth-gradient optimization to determine layer-specific sparsity ratios and applies layer-wise unstructured weight pruning. Experimental results demonstrate improvements on some benchmarks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "-- Pruning is a crucial topic for efficient large models.\n\n-- The proposed method is interesting to me.\n\n-- Extensive experimental results demonstrate some improvement compared to previous methods."
                },
                "weaknesses": {
                    "value": "Overall, the paper leans more towards an engineering-focused approach. The proposed method appears to be a straightforward combination of existing techniques without offering mathematical insights or clear motivations. The paper requires significant revisions to improve its writing quality.\n\n-- The paper claims that pruning multi-modal large models differs from other large models, but lacks mathematical or experimental support for this assertion.\n\n-- The use of layer-wise pruning due to the computational cost of calculating the inverse of the Hessian is common in other large model pruning approaches.\n\n-- The assumption of having sufficient GPU resources in the pruning scenario is not adequately discussed.\n\n-- While zeroth-gradient optimization is effective for efficient fine-tuning of large models [1], its motivation and suitability in a pruning scenario, considering challenges like slow convergence and sensitive hyper-parameters, remain unclear.\n\n-- The use of calibration datasets in experiments raises concerns about fairness and the significance of employing zeroth-order optimization to save memory if calibration datasets are used for fine-tuning.\n\nThe paper suffers from poor writing quality and contains noticeable typos:\n\n1. that finds adaptive sparsity per layer by leveraging the global importance score approximated via first-order gradients. (should be zeroth-order gradients)\n\n2. Note that our proposed method is computationally efficient by leveraging the first-order gradient to obtain a global importance score without Hessian operations (should be zeroth-order gradients)\n\n[1] Malladi, Sadhika, et al. \"Fine-Tuning Language Models with Just Forward Passes.\" arXiv preprint arXiv:2305.17333 (2023)."
                },
                "questions": {
                    "value": "Please refer to my detailed comments in the weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6256/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6256/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6256/Reviewer_4r21"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6256/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699057251094,
            "cdate": 1699057251094,
            "tmdate": 1700707863249,
            "mdate": 1700707863249,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iT0Skq1461",
                "forum": "iIT02bAKzv",
                "replyto": "aKsGXI4Iu4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6256/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6256/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for your review and comments. We provide our response in the following.\nWe strongly believe we have clearly addressed all your concerns. Please don\u2019t hesitate to let us know if your concerns/questions are still not clearly resolved. We are open to active discussion and will do our best to address your concerns during the rebuttal period.\n\n---\n\n>  **C1. The proposed method appears to be a straightforward combination of existing techniques without offering mathematical insights or clear motivations.**\n\n\nWe want to emphasize that we already provide two clear motivations for our approach: \n\n1) Layer-wise one-shot pruning used for large models cannot figure out the optimal pruning rate per layer due to the lack of global (i.e., entire model) information (please see Introduction and Section 3.2).\n\n2) In large multi-modal models, training modules for different modalities show a significant disparity in their weight and gradient scales. This distributional gap in different modules makes layer-wise one-shot pruning further challenging, and often fails to preserve the model performance with a higher sparsity ratio (please see Figure 1 and Figure 4).\n\nTo address these two key problems in pruning large multi-modal models, we propose our method, ECoFLaP. \n\nOur design takes strong advantages from both layer-wise and global pruning methods. Our EcoFLaP preserves the performance of full-precision models by computing intra-/inter weight importance (global pruning), yet this process is rapid and efficient (layer-wise pruning) through our proposed two-step pruning procedures with zeroth-order approximation. We utilize **zeroth-order gradients for global information and bypass the need to construct a backpropagation graph**, and thus **our approach can use a similar amount of GPU memory as layer-wise pruning**. To the best of our knowledge, we didn\u2019t see previous work utilizing our idea for network pruning.\n\n\n\nWe argue that our approach **provides insights into both layer-wise and global pruning approaches** and **the method is practical to use a minimal amount of GPU memory**. We also presented our motivations in the Introduction and Section 3.2.\n\n---\n\n> **C2.The paper claims that pruning multi-modal large models differs from other large models, but lacks mathematical or experimental support for this assertion.**\n\nWe politely argue that this is a misunderstanding. we clearly provided analyses and results that pruning multi-modal models differs from pruning single-modal models in Section 3.2, and **conventional single-modal layer-wise pruning methods suffer from significant performance degeneration in multi-modal model in high sparsity regime (Please see Figure 4(a))**. In Section 3.2 and Figure 1(c), we demonstrated that **local score estimation is imbalanced for vision and language models**, which causes inaccurate estimation of the pruning ratios for each layer in the model. Also, the local score of each parameter cannot infer the importance of the final loss, and that is one reason that layer-wise pruning only uses a fixed pruning ratio for layers.\n\n---\n\n> **C3. The use of layer-wise pruning due to the computational cost of calculating the inverse of the Hessian is common in other large model pruning approaches.**\n\nWe believe there is a misunderstanding in our contribution. \nOur contribution is not on layer-wise pruning itself. As described in the second paragraph of Section 3.2, we clarify the critical challenges in pruning large vision-language multi-modal models, and suggest a simple yet powerful pruning approach using the **memory-efficient zeroth-order optimization to estimate dynamic pruning ratio per layer with minimal computations**. Our EcoFLaP successfully mitigates severe performance drops even with a high degree of model compression (Figure 4), by overcoming the limitations of general layer-wise pruning methods.\n\n---\n\n> **C4. The assumption of having sufficient GPU resources in the pruning scenario is not adequately discussed.**\n\nIn Table 1, we showed that ECoFLaP only uses slightly more GPU memory (0.06GB) than Wanda and even less memory than SparseGPT. Furthermore, ECoFLaP also only uses 0.5GB more memory than Magnitude Pruning (which directly uses weight magnitude as the importance score and does not require any computation). We believe that the efficiency of ECoFLaP makes it a strong method as layer-wise pruning in the low computational cost regime."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6256/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700247427979,
                "cdate": 1700247427979,
                "tmdate": 1700247427979,
                "mdate": 1700247427979,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IrKxdm3NYZ",
                "forum": "iIT02bAKzv",
                "replyto": "aKsGXI4Iu4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6256/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6256/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> **C5. While zeroth-gradient optimization is effective for efficient fine-tuning of large models [1], its motivation and suitability in a pruning scenario, considering challenges like slow convergence and sensitive hyper-parameters, remain unclear.**\n\nWe believe there is a clear misunderstanding in our pruning strategy. We note that our proposed method is a **one-shot pruning approach, and there is no convergence issue as we didn\u2019t use zeroth-order optimization to update the network like [1]**.  We only use zeroth-order optimization to **estimate the gradient** of the pre-trained weight **once** and use the gradient as the importance of parameters. Moreover, our ablation studies in Tables 3 - 5 demonstrated that our approach is robust to hyperparameters of zeroth-order optimization. \n\n---\n\n> **C6. The use of calibration datasets in experiments raises concerns about fairness and the significance of employing zeroth-order optimization to save memory if calibration datasets are used for fine-tuning.**\n\nThere is a clear misunderstanding of the basics of one-shot pruning. We first note that using small calibration data is a standard approach for one-shot (layer-wise) pruning [2,3,4]. And we also followed the same setting for our approach and didn\u2019t re-train the model using the calibration set after pruning. \n\n[2]  Hubara, I., Chmiel, B., Island, M., Banner, R., Naor, S., and Soudry, D. Accelerated sparse neural training: A provable and efficient method to find N:M transposable masks.\n\n[3] Elias Frantar, Dan Alistarh, SparseGPT: Massive Language Models Can be Accurately Pruned in One-Shot\n\n[4] Mingjie Sun, Zhuang Liu, Anna Bair, J. Zico Kolter, A Simple and Effective Pruning Approach for Large Language Models\n\n---\n\n> **C7. that finds adaptive sparsity per layer by leveraging the global importance score approximated via first-order gradients. (should be zeroth-order gradients)**\n\nThanks for catching the typo. We have reflected this in the revision. \n\n---\n\n> **C8. Note that our proposed method is computationally efficient by leveraging the first-order gradient to obtain a global importance score without Hessian operations (should be zeroth-order gradients)**\n\nThis part is not a typo. We first show another version of our approach with the first-order gradient (we also show the result of this version in Table 1), which is cheaper than the second-order Hessian method, and then propose a more efficient version with the zeroth-order gradient.\n\n---\n\n*If our response addresses your concerns, please consider increasing the scores. Also, feel free to ask follow-up questions during the rebuttal period.*"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6256/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700247553924,
                "cdate": 1700247553924,
                "tmdate": 1700247850523,
                "mdate": 1700247850523,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4IMADZNb5D",
                "forum": "iIT02bAKzv",
                "replyto": "IrKxdm3NYZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6256/Reviewer_4r21"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6256/Reviewer_4r21"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your detailed response and the effort put into addressing the concerns raised. After careful consideration of your responses and a re-evaluation of the manuscript, I have decided to maintain my original score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6256/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700558951338,
                "cdate": 1700558951338,
                "tmdate": 1700558951338,
                "mdate": 1700558951338,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YkoymRXGOK",
                "forum": "iIT02bAKzv",
                "replyto": "4IMADZNb5D",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6256/Reviewer_4r21"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6256/Reviewer_4r21"
                ],
                "content": {
                    "title": {
                        "value": "Additional Comment"
                    },
                    "comment": {
                        "value": "I genuinely value the authors' efforts and the additional results they have incorporated.\n\nUpon thorough examination, I acknowledge the insights presented in this paper concerning the challenges associated with pruning the vision-language model (referenced in Section 3.2). The statistics regarding weight magnitudes, gradients, and SparseGPT's local score highlight a disparity between the visual and linguistic components of the Vision Language Model (VLM). Moreover, as noted in Section 3.2, the prevalent layer-wise pruning methods overlook the differential nature of the vision and language models, an observation that I find particularly innovative.\n\nHowever, I maintain significant reservations regarding two critical aspects:\n\n1. The paper asserts distinct differences between the vision and language segments within the VLM, yet the applied scoring function (Equation 5) remains uniform for both. Given that this function is intended to assess the relevance of each layer in the VLM, **the paper seemingly contradicts its own rationale by not differentiating between the two segments**.\n\n2. Concerning the primary technical contribution of the paper detailed in Section 4.2, it employs zeroth-order gradient approximation for a layer's weight to circumvent the substantial memory demands associated with storing first-order gradients. Nonetheless, Equation 5 implies that computing the score (L2 norm of zeroth-order gradients) for layer i necessitates a memory allocation identical to that needed for storing first-order layer-wise gradients. **This suggests that zeroth-order gradient use does not yield memory savings when compared with first-order gradients.** This claim (utilizing zeroth-order gradients save memory) is further challenged by the experimental data in Table 1, where using zeroth-order gradients for the BLIP-2 model at 0.5 sparsity consumes 8.93GB of GPU memory, comparable to other layer-wise methods approximating 9GB. Conversely, the ECoFLaP + First-order Gradient approach incurs a 22.4GB memory cost, matching that of Iterative OBD Pruning, which accounts the entire model's first-order gradient and weight magnitude multiplication. These findings imply that ECoFLaP + First-order Gradient's reported memory usage encompasses the entire model's 1st-order gradients. Hence, a more equitable memory usage comparison between layer-wise zeroth and first-order gradients is warranted, potentially leading to analogous outcomes.\n\nIn summation, while the paper sheds new light on the difference between the visual and linguistic components in VLMs, it fails to bride the gap. The main technical proposition of the paper\u2014the use of layer-wise zeroth-order gradient norms as a scoring mechanism\u2014counters the authors' stated objectives of solving the difference between VLM components and achieving memory efficiency. Therefore, my assessment remains unchanged, and I lean towards rejecting this paper."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6256/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700671771163,
                "cdate": 1700671771163,
                "tmdate": 1700671771163,
                "mdate": 1700671771163,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fYu3lGgLaO",
                "forum": "iIT02bAKzv",
                "replyto": "udz7Iz6KX1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6256/Reviewer_4r21"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6256/Reviewer_4r21"
                ],
                "content": {
                    "comment": {
                        "value": "I sincerely appreciate for author\u2019s effort in their timely response.\n\nI accept their clarification on the memory cost of zeroth-order gradients. However, I still kindly remind the memory cost of computing first-order gradients should be carefully considered, since a lower memory cost ( much less than 2N shown in the paper) can be achieved via some back propagation trick.  \n\nBased on the above reasons, I raised my score to 5. I still hold concerns about the consistency of methodology and motivation, but I don\u2019t mind the paper getting accepted."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6256/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700707818730,
                "cdate": 1700707818730,
                "tmdate": 1700707818730,
                "mdate": 1700707818730,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fJ1R6F5B5K",
                "forum": "iIT02bAKzv",
                "replyto": "aKsGXI4Iu4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6256/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6256/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for your time to engage in the discussion and for your acknowledgment of our explanation. We will carefully consider your additional feedback and incorporate it into the paper. \n\nHappy Thanksgiving!"
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6256/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700716435015,
                "cdate": 1700716435015,
                "tmdate": 1700716465796,
                "mdate": 1700716465796,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]