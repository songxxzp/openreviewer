[
    {
        "title": "Beyond Demographic Parity: Redefining Equal Treatment"
    },
    {
        "review": {
            "id": "vIpl4DHYiY",
            "forum": "cVea4KQ4xm",
            "replyto": "cVea4KQ4xm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission645/Reviewer_pKVE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission645/Reviewer_pKVE"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new notion of equal treatment (ET) which requires the model\u2019s explanations to be independent from the sensitive attribute, as opposed to the demographic parity (DP) that require the independence of model prediction and sensitive attribute. Given the proposed notion, the paper first explores the relation between ET and DP, and then proposes a method to inspect ET via statistical independence test.  Such an inspector may further help interpret the sources of unequal treatment."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Inspecting whether a model violates fairness and explaining the sources that cause unfairness is an important and interesting problem. \n2. The paper proposed a new notion of fairness based on explanation distribution, which is novel to the best of my knowledge. \n3. The paper validates the proposed inspector on both synthetic and real data."
                },
                "weaknesses": {
                    "value": "1. The main concern I had was the novelty of the paper, which I think is not sufficient. Specifically, using model attribution methods such as Shapley values to interpret model unfairness has been explored in prior works; the idea of using the two-sample test to examine the independence of two sets of variables has also been studied. While the settings are not the same, the techniques are somehow similar. \n2. Because the notion of equal treatment is strictly stronger than the demographic fairness notion, it can be much more challenging to attain ET in practice than DP. Moreover, the trade-off between fairness and accuracy may make ET less suitable for real applications. While the paper has compared the two notions, it is still not convincing why equal treatment is a superior notion. It is helpful if authors can provide more justification with a real example.\n3. While the settings with non-linear models and non-i.i.d. data are considered in experiments, most theoretical results and illustrating examples are limited to linear models and i.i.d. data. Moreover, the synthetic data used in the evaluation is also very simple: logistic model with Gaussian distributed data. \n4. The paper is not using the ICLR template."
                },
                "questions": {
                    "value": "1. It seems that the sources of unequal treatment can only be explained for linear models (as illustrated in Example 4.4 and Figure 3). How can the method be generalized to non-linear cases? \n2. Since ET can be much more difficult to achieve, can you provide a real example to illustrate why equal treatment is a more appropriate notion than demographic parity?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission645/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698633886694,
            "cdate": 1698633886694,
            "tmdate": 1699635992296,
            "mdate": 1699635992296,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "00uxqeo1dw",
                "forum": "cVea4KQ4xm",
                "replyto": "vIpl4DHYiY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission645/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission645/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer, many thanks for the comments.\nPlease consider the main points of our rebuttal.\nWe address specific points below:\n\n\n> I.1. The main concern I had was the novelty of the paper, which I think is not sufficient. Specifically, using model attribution methods such as Shapley values to interpret model unfairness has been explored in prior works; the idea of using the two-sample test to examine the independence of two sets of variables has also been studied. While the settings are not the same, the techniques are somehow similar.\n\nA.1. The intersection of fairness and explainable AI has been an active topic in recent years. We compare our approach with related work in Appendix B3. We would thank the reviewer for any  additional paper to be considered. \n\n\n> The paper is not using the ICLR template.\n\nWe use the following in the latex preamble: \\usepackage{iclr2024_conference}\n\n\n>I.2. It seems that the sources of unequal treatment can only be explained for linear models (as illustrated in Example 4.4 and Figure 3). How can the method be generalized to non-linear cases?\n\nA.2. The approach is not restricted to linear models only. For instance, in Appendix E.3 non-linear models are experimented with. If the ET Inspector is linear, then the explanations are readily available, but, in general, the explanation of a non-linear ET inspector is proposed in the process from Figure 1. \n\n\n> I.3. Since ET can be much more difficult to achieve, can you provide a real example to illustrate why equal treatment is a more appropriate notion than demographic parity?\n\n\n A.3 The main philosophical critique to the liberalism notion of equal of treatment as blindness, is that it is rarely achieved. Our paper confirms this classical intuition both, theoretically and experimentally. \n\nIn Figure 3, we provide cases where Demographic Parity is achieved but Equal Treatment is not, these are cases where demographic parity may flag wrongly a fair ML model. This figure is further extended in table 5 where we compare against distinct methods to measure demographic parity."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission645/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700216978951,
                "cdate": 1700216978951,
                "tmdate": 1700216978951,
                "mdate": 1700216978951,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "du5ToDHLXi",
            "forum": "cVea4KQ4xm",
            "replyto": "cVea4KQ4xm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission645/Reviewer_9JSK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission645/Reviewer_9JSK"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a new fairness definition motivated by the pursuit of equal treatment. The authors first showed that it is insufficient to use statistical measures of equal outcome, e.g. demographic parity, to evaluate equal treatment. They then defined Equal Treatment (ET) as requiring indistinguishable explanation distribution for the non-protected features between populations with different protected features. The explanation distribution relies on an explanation function, for which Shapley value is used as the example in the paper, to quantity how non-protected features affect the trained model. Based on the new ET definition, they also designed a Classifier Two Sample Test (C2ST) to test whether a ML model provides equal treatment based on the AUC of the model. In numerical experiments, the authors demonstrated that the new ET definition is more effective at inspecting treatment equality in a model, and their method could provide explanation for the underlying causes of treatment inequality."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The proposed Equal Treatment is a novel method that combines fairness and explainability. These two goals are both important components in the broad domain of ethical machine learning, and they are typically studied separately. The Equal Treatment Inspector workflow from this paper examines both issues and can answer the useful question of what causes unfairness. \n\nThe paper is well-written and follows a well-thought-out flow. The examples provided throughout the paper are helpful for understanding the concept. The related works (majority in appendix) are thoroughly reviewed to help position the paper in literature."
                },
                "weaknesses": {
                    "value": "I disagree with some statements that the paper used to motivate the research question. For example, the abstract states \u201cRelated work in machine learning has translated the concept of equal treatment into terms of equal outcome and measured it as demographic parity (also called statistical parity)\u201d. To my understanding, it is well-recognized in the fair ML literature that equal treatment and equal outcome are different concepts. While I agree that equal outcome is often measured with statistical measures, I think it is inaccurate to frame equal outcome as a convenient proxy of equal treatment. Instead, one simplified interpretation of equal treatment is \u201cfairness through unawareness\u201d or \u201ccolorblindness\u201d. Rather than relying on the distinction between \u2018equal outcome vs. equal treatment\u2019, which can refer to much more high-level philosophical differences than what is captured in this paper, I would find it clearer to simply focus on equal treatment (new definition) vs. demographic parity."
                },
                "questions": {
                    "value": "1.\tIn Section 4, the theoretical analysis relies on assuming exact calculations of Shapley value are available. How realistic is this assumption in practice? When we do not have access to exact Shapley values, how will the theoretical results be affected?\n\n2.\tWhat are other explanation functions that can be used in the framework? In the appendix, another example is given, but I wonder is there a large set of options or is designing an effective explanation function an open question itself? If there are multiple candidate explanation functions, what makes one function better than another?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission645/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698781775303,
            "cdate": 1698781775303,
            "tmdate": 1699635992223,
            "mdate": 1699635992223,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1y6WshbK1C",
                "forum": "cVea4KQ4xm",
                "replyto": "du5ToDHLXi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission645/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission645/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer, many thanks for the comment, please consider the main points of our rebuttal above and our specific points that follow now:\n\n> I.1. I disagree with some statements that the paper used to motivate the research question. For example, the abstract states \u201cRelated work in machine learning has translated the concept of equal treatment into terms of equal outcome and measured it as demographic parity (also called statistical parity)\u201d. To my understanding, it is well-recognized in the fair ML literature that equal treatment and equal outcome are different concepts. While I agree that equal outcome is often measured with statistical measures, I think it is inaccurate to frame equal outcome as a convenient proxy of equal treatment. Instead, one simplified interpretation of equal treatment is \u201cfairness through unawareness\u201d or \u201ccolorblindness\u201d. Rather than relying on the distinction between \u2018equal outcome vs. equal treatment\u2019, which can refer to much more high-level philosophical differences than what is captured in this paper, I would find it clearer to simply focus on equal treatment (new definition) vs. demographic parity.\n\nA.5.1 We agree with the reviewer. We will compare equal treatment vs demographic parity instead of equal outcomes. \n\n\n> I.2 In Section 4, the theoretical analysis relies on assuming exact calculations of Shapley value are available. How realistic is this assumption in practice? When we do not have access to exact Shapley values, how will the theoretical results be affected?\n\nI.2. The mathematical derivation of Shapley values in real cases, (non-iid data and non linear model), is not feasible. There are related works that focus on to improve the computation of Shapley values in those scenarios:\n\n[1] Sebastian Bordt, Ulrike von Luxburg: From Shapley Values to Generalized Additive Models and back. pdf. AISTATS, 2023\n\n[2] Interventional shap values and interaction values for piecewise linear regression trees, AAAI 2023\n\n\n\n> I.3. What are other explanation functions that can be used in the framework? In the appendix, another example is given, but I wonder is there a large set of options or is designing an effective explanation function an open question itself? If there are multiple candidate explanation functions, what makes one function better than another?\n\nA.3. In this work, we have focused on feature attribution explanations that are well-developed for tabular data. We have selected LIME and Shapley because they comply with the efficiency property (equation 6), and this is required so the theoretical analysis hold. It is possible that alternative XAI explanation techniques, such as logical reasoning, argumentation, or counterfactual explanations, could be useful and offer their unique advantages. This remains a further work to explore. (This issue is discussed in the \u201cLimitations\u201d paragraph of the Conclusions.)"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission645/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700216461299,
                "cdate": 1700216461299,
                "tmdate": 1700216694451,
                "mdate": 1700216694451,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BHxT3UKSgN",
                "forum": "cVea4KQ4xm",
                "replyto": "1y6WshbK1C",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission645/Reviewer_9JSK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission645/Reviewer_9JSK"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the responses"
                    },
                    "comment": {
                        "value": "I thank the authors for responding to my comments."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission645/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700447321560,
                "cdate": 1700447321560,
                "tmdate": 1700447321560,
                "mdate": 1700447321560,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "e5qMabNbRV",
            "forum": "cVea4KQ4xm",
            "replyto": "cVea4KQ4xm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission645/Reviewer_ZXLv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission645/Reviewer_ZXLv"
            ],
            "content": {
                "summary": {
                    "value": "The paper questions the traditional approach of equal outcome and demographic parity as a measure of fairness and proposes a new formalization for equal treatment. The authors measure equal treatment by accounting for the influence of feature values on model predictions. They formalize equal treatment by considering the distributions of explanations and comparing them between populations with different protected features. The paper proposes a classifier two-sample test based on the AUC (Area Under the Curve) of an equal treatment inspector, which compare the degree of equal treatment between different groups. The application on synthetic and real datasets show that this new equal treatment definition might actually yield higher AUCs for downstream classifiers than when using demographic parity."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is well-written and well-structured.\n- It appears to be the first clear attempt to connect explanations with algorithmic fairness through the introduction of the new \"equal treatment\" definition. While other approaches have used explainability as a proxy for fairness, none have established such strong foundations as presented by the authors.\n- The examples with simple linear models effectively illustrate potential impacts and counterexamples.\n- The experiments provide compelling evidence of the potential implications of this novel \"equal treatment\" definition."
                },
                "weaknesses": {
                    "value": "The main weaknesses I can observe are (a) practical implications of the new equal treatment definition and (b) the novelty and implication of using a classifier-two-sample test.\n\n(a) I agree with the authors that in the case of exact demographic parity (independence), then this definition of equal treatment works (Lemma 4.2). However, my concerns arise in cases where the demographic parity is violated only by a small amount, which is the case in practice; no (useful) algorithm has a demographic parity of exactly zero, and most of the decision making algorithms usually have a small violation tolerance. Can the authors comment how equal treatment can be used on bounding demographic parity, or whether there exists any relationship there? This scenario is important for e.g., credit lending scenarios; in the U.S., the Equal Credit Opportunity Act [2] enforces no discrimination *on the outcomes* of the decision-making algorithm. From a law standpoint, one might not necessarily mind different explanations as long as the outcomes are not too dissimilar (i.e., low demographic parity).\n\n(b) First of all, unfortunately using AUC as a test statistic for classifier-two-sample test is not novel, see [1] for example (the good thing is that AUC is a relatively well behaved statistic, so that does not change the framework). By using a C2ST in the framework, we introduce (i) a data-driven algorithm to judge the level of equal treatment in the data but also (ii) an additional notion of uncertainty in our fairness definition. For (i), in practice this means that this approach is not necessarily low-sample-size friendly (as it does not use permutations), the complexity of the classifier directly affects type I and type II error and results may vary considerably according to which classifier is chosen (which the authors have actually explored in the Appendix). For (ii), we are rejecting the null-hypothesis with a certain probability threshold, as opposed to provide a single (deterministic) number as in demographic parity. That is, we are now guaranteeing that \"up to a level 1-\\alpha\" the algorithm is providing equal treatment. Citing again the equal credit opportunity act of 1961, such a definition of fairness would not be admissible in a credit lending scenario, which puts into question once again the practical feasibility of this new definition of equal treatment.\n\n[1] Model-independent detection of new physics signals using interpretable SemiSupervised classifier tests, Chakravarti, Purvasha and Kuusela, Mikael and Lei, Jing and Wasserman, Larry, The Annals of Applied Statistics, 2023\n[2] https://www.justice.gov/crt/equal-credit-opportunity-act-3#:~:text=prohibits%20creditors%20from%20discriminating%20against,under%20the%20Consumer%20Credit%20Protection\n\n\nFor completeness, I am personally unsure whether ICLR is the best venue to reach the type of audience who would be interested in this work. However, I believe this is not my judgment call to make; I assure the authors I did not take this into account when writing this review."
                },
                "questions": {
                    "value": "I have included two points in the \"Weaknesses\" section above, so I'd be grateful if the authors would post their comments to those.\n\nMinor points:\n- The word \"natural data\" sounds a bit weird, as usually the machine learning community uses \"real data\".\n- Figure 2 is a bit too small overall, increase the font size and marker size would go a long way."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission645/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699484801958,
            "cdate": 1699484801958,
            "tmdate": 1699635992151,
            "mdate": 1699635992151,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dfvGP74sm2",
                "forum": "cVea4KQ4xm",
                "replyto": "e5qMabNbRV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission645/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission645/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer, many thanks for the comments, above the main points of our rebuttal and our specific points that follow now:\n\n> I.1(a) I agree with the authors that in the case of exact demographic parity (independence), then this definition of equal treatment works (Lemma 4.2). However, my concerns arise in cases where the demographic parity is violated only by a small amount, which is the case in practice; no (useful) algorithm has a demographic parity of exactly zero, and most of the decision making algorithms usually have a small violation tolerance.  Can the authors comment how equal treatment can be used on bounding demographic parity, or whether there exists any relationship there? This scenario is important for e.g., credit lending scenarios; in the U.S., the Equal Credit Opportunity Act [2] enforces no discrimination on the outcomes of the decision-making algorithm. From a law standpoint, one might not necessarily mind different explanations as long as the outcomes are not too dissimilar (i.e., low demographic parity).\n\nA.1  Thank you for this comment. Lemma 4.2 shows that Equal Treatment implies Demographic Parity in the case of perfect independence. Apart from the trivial case where the turns out to coincide (Lemma 4.1), we have not explored in the paper bounds on DP given bounds on ET. This is something that we intend to do in future work, as it is a non-trivial property relating multivariate independence (ET) to univariate independence (DP). A promising approach will be to exploit the linearity property of Shapley values.\n\n\n\n> I.2. (b) First of all, unfortunately using AUC as a test statistic for classifier-two-sample test is not novel, see [1] for example (the good thing is that AUC is a relatively well behaved statistic, so that does not change the framework). By using a C2ST in the framework, we introduce (i) a data-driven algorithm to judge the level of equal treatment in the data but also (ii) an additional notion of uncertainty in our fairness definition. For (i), in practice this means that this approach is not necessarily low-sample-size friendly (as it does not use permutations), the complexity of the classifier directly affects type I and type II error and results may vary considerably according to which classifier is chosen (which the authors have actually explored in the Appendix). For (ii), we are rejecting the null-hypothesis with a certain probability threshold, as opposed to provide a single (deterministic) number as in demographic parity. That is, we are now guaranteeing that \"up to a level 1-\\alpha\" the algorithm is providing equal treatment. \n\nA.2. Many thanks for pointing to relevant related work; we will integrate it on our work as it helps to clarify the strength of our submission. \n\n\n> I.3.Citing again the equal credit opportunity act of 1961, such a definition of fairness would not be admissible in a credit lending scenario, which puts into question once again the practical feasibility of this new definition of equal treatment.\n\n\nA.3. May we ask in which section of the ECOA of 1961 does the regulation state this?\n\nNote that equal opportunity, if understood by the Rawlsian \"luck\" egalitarianism, may fall closer to our notion than to TPR differences.\n\nIn the more recent regulation discussion [3,4] Wachter advocates for demographic parity. It is important to note the EU and US law are different. And there may be other worldwide incoming regulations from other countries that might be also different.\n\nThe applicability of AI regulation is an ongoing discussion with recent workshops specifically targetting this area (https://regulatableml.github.io/)\n\nIn [4]\"\"In particular, some of the tests used by the European Court of Justice and Member State courts to measure indirect discrimination match the metric of demographic parity from\nalgorithmic fairness.\"\"\n\n\n\n[1] Model-independent detection of new physics signals using interpretable SemiSupervised classifier tests, Chakravarti, Purvasha and Kuusela, Mikael and Lei, Jing and Wasserman, Larry, The Annals of Applied Statistics, 2023 \n[2] https://www.justice.gov/crt/equal-credit-opportunity-act-3#:~:text=prohibits%20creditors%20from%20discriminating%20against,under%20the%20Consumer%20Credit%20Protection\n\n[3] Why fairness cannot be automated: Bridging the gap between EU non-discrimination law and AI https://www.sciencedirect.com/science/article/abs/pii/S0267364921000406\n\n[4] Bias Preservation in Machine Learning: The Legality of Fairness Metrics Under EU Non-Discrimination Law\nhttps://papers.ssrn.com/sol3/papers.cfm?abstract_id=3792772\n\n\n\n> I4. The word \"natural data\" sounds a bit weird, as usually the machine learning community uses \"real data\".\n\nA.4 The opposite to real data, is unreal data. While the contrary of natural data, is synthetic. We made this choice as we believe it's an improvement wrt terminology precision."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission645/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700216627993,
                "cdate": 1700216627993,
                "tmdate": 1700216727044,
                "mdate": 1700216727044,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kg4uMtBRf9",
                "forum": "cVea4KQ4xm",
                "replyto": "dfvGP74sm2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission645/Reviewer_ZXLv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission645/Reviewer_ZXLv"
                ],
                "content": {
                    "title": {
                        "value": "Acknowledged Response"
                    },
                    "comment": {
                        "value": "With this message I acknowledge the response of the author.\nThe response is centered around the philosophy behind the paper, to which, unfortunately, I am in no position to effectively comment on.\nGiven that (i) using AUC as test statistic is not novel and (ii) all my comments on using a CS2T and related statistical issues were not addressed, I unfortunately don't think this work is quite ready to be published, especially in a technical conference like ICLR."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission645/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700669374453,
                "cdate": 1700669374453,
                "tmdate": 1700669374453,
                "mdate": 1700669374453,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bR3JOzNtoH",
            "forum": "cVea4KQ4xm",
            "replyto": "cVea4KQ4xm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission645/Reviewer_fCKF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission645/Reviewer_fCKF"
            ],
            "content": {
                "summary": {
                    "value": "This paper highlights the issue with current fairness notions, which emphasize equal outcomes rather than equal treatment. The philosophical definition of fairness aligns more closely with the principle of equal treatment. The paper delves into the theoretical relationship between equal treatment and equal outcomes and introduces a methodology for assessing equal treatment."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "High-level Idea is simple and intuitive."
                },
                "weaknesses": {
                    "value": "- [Major] I remain unconvinced that 'equal treatment' is a superior notion of fairness. The paper advocates for the use of Shapley values to distribute explanations when defining equal treatment. However, the rationale for this preference is unclear to me. A notable limitation of this fairness notion is its potential indirect correlation with protected attributes like Z. For example, height is often closely associated with gender. Therefore, a model's Shapley values may not depend on the protected attribute Z and might predominantly base predictions on height equally across different gender groups, which superficially appears gender-neutral and meets the paper's fairness criteria, yet it may still result in substantial unfairness. I welcome corrections if my understanding of Shapley values is inaccurate.\n- [Major] The paper omits a critical discussion on related work. There appears to be a study, specifically on individual fairness [1], that resonates with the motivations of this paper. Individual fairness emphasizes that individuals with similar backgrounds (e.g., salary, job status) should receive similar treatment. However, this paper does not draw any comparisons with its own concept of fairness to that of individual fairness.\n- [Medium] The motivation presented within the paper is somewhat unclear, and it is concerning that significant discussions related to the work are relegated to the appendix. This decision diminishes the visibility and importance of such discussions.\n- [Medium] I cannot agree with the authors that equal opportunity could lead to reverse discrimination and overcorrection. As far as I know, equal opportunity is proposed to address these limitations you mentioned which suffered by demographic parity. Can you cite the corresponding works that draw this conclusion?\n- [Medium] Although Shapley values are central to defining 'equal treatment', they are introduced late in the appendix. It is my suggestion that the authors reconsider the organization of the paper, as many pivotal elements seem to be understated by their placement in the appendix.\n\nReference:\n\n- Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard S. Zemel. Fairness through awareness. In ITCS, pp. 214\u2013226. ACM, 2012."
                },
                "questions": {
                    "value": "See the weakness above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission645/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699602604147,
            "cdate": 1699602604147,
            "tmdate": 1699635991977,
            "mdate": 1699635991977,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "D3tg9ntcwK",
                "forum": "cVea4KQ4xm",
                "replyto": "bR3JOzNtoH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission645/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission645/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer, many thanks for the comments, see the main and specific rebuttal:\n\n\n>I1.1 [Major] I remain unconvinced that 'equal treatment' is a superior notion of fairness. \n\nA.1.1.In the main rebuttal, we have argued why Equal Treatment is a superior notion to Demographic Parity. In Appendix B1, we have provided an illustrative example regarding blind reviews of papers\n\n\n>I1.2  A notable limitation of this fairness notion is its potential indirect correlation with protected attributes like Z....\n\nA 1.2.In example 4.4 we have studied the indirect correlation with protected attributes, which is derived from the Shapley value theoretical propertie (Appendix A). Here, we adapt the example to the case that the reviewer proposes. We will predict a fictitious variable \u201cweight\u201d based on \u201cage\u201d and \u201cplace of birth\u201d (both independent of \u201cweight\u201d) and \u201cheight\u201d which is related to \u201cweight\u201d.\n\nLet $X = X_1,X_2,X_3$ be independent features, and $X_1, X_2 \\perp Z$, and $X_3 \\not \\perp Z$. Where $X_1$ and $X_2$ are \u201cplace of birth\u201d and \u201cage\u201d respectively, and $X_3$ is \u201cheight\u201d which correlates with gender $Z$ and \u201cweight\u201d $Y$.\n\nWe now have a linear model $f_\\beta(x_1, x_2, x_3) = \\beta_0 + \\beta_1 \\cdot x_1 + \\beta_2 \\cdot x_2 + \\beta_3 \\cdot x_3$ with $\\beta_1, \\beta_2, \\beta_3 \\neq 0$. \n\nWe can calculate the Shapley values by $S(f_\\beta, x)_i = \\beta_i \\cdot x_i$. (Aas,2021)\n\nNow we measure Equal Treatment by $g_\\psi(s) = \\psi_0 + \\psi_1 \\cdot s_1 + \\psi_2 \\cdot s_2+\\psi_3 \\cdot s_3$, which can be written in terms of the $x$'s as: $g_\\psi(x) = \\psi_0 + \\psi_1 \\cdot \\beta_1 \\cdot x_1 + \\psi_2 \\cdot \\beta_2 \\cdot x_2+\\psi_3 \\cdot \\beta_3 \\cdot x_3$. \nBy OLS estimation properties, we have $\\psi_1 \\approx cov(\\beta_1 \\cdot X_1, Z)/var(\\beta_1 \\cdot X_1) = cov(X_1, Z)/(\\beta_1 \\cdot var(X_1))  = 0$ and analogously $\\psi_2 \\approx 0$. Finally, $\\psi_3 \\approx cov(X_3, Z)/(\\beta_3 \\cdot var(X_3)) \\neq 0$. \n\nThe coefficients of $g_\\psi$ provide information about which feature contributes to the dependence between the explanation $S(f_\\beta, X)$ and the protected feature $Z$. \n\nUsing the example the reviewer proposed $\\psi_3$ shows how related \u201cheight\u201d is to the protected attribute in contributing to the prediction, thus violating Equal Treatment. \n\nMany thanks to the author for the question, which helps clarify the paper's contributions. Does the reviewer find this clarification helpful?\n\n> I.2[Major] The paper omits a critical discussion on related work. There appears to be a study, specifically on individual fairness [1], that resonates with the motivations of this paper. \n\nA.2 Individual fairness can not often be applied and is therefore not a popular metric [2]\u2013 unlike group fairness metrics. For example, when a company hires for one position, it obviously has to discriminate even if two individuals are identical wrt non-protected characteristics. Talking about unfainress is not  plausible in this situation. Only if the company shows a systematic, statistically significant pattern of discriminating a group of people, one can talk about unfairness. Some Individual fairness metrics may have the challenge of achieving a precise mathematical definition of \u201csimilarity\u201d. See [1] for a summary of critics on fairness metrics.\n\n[1] Salvatore Ruggieri, Jos\u00e9 M. \u00c1lvarez, Andrea Pugnana, Laura State, Franco Turini:\nCan We Trust Fair-AI? AAAI 2023\n\n[2] Fleisher, W. What's fair about individual fairness?. AIES 2021\n\n> I.3[Medium] The motivation presented within the paper is somewhat unclear, and it is concerning that significant discussions related to the work are relegated to the appendix. This decision diminishes the visibility and importance of such discussions.\n\nA.3. We will adapt the introduction based on the feedback of the reviewers, but, due to space constraints, we decided to privilege the main contributions in the main text, and the detailed discussions in the appendix.\n\n\n> I.4 [Medium] I cannot agree with the authors that equal opportunity could lead to reverse discrimination and overcorrection.\n\nA.3 See first answer for clarification (A.1). \n\nFrom a technical perspective, Equal Opportunity requires labeled data which in practice, which after deployment is hard to obtain. Equal Opportuntiy forces to meet some quotas (True Positive Rate), this introduces a second order discrimination. Equal Treatment has no quotas, it is fine to \u201chire\u201d the best, while the decision is taken independently from the protected attribute.\n\nFrom a philosophical perspective much is discussed on the literature, the commentary at https://link.springer.com/content/pdf/10.1057/palgrave.cpt.9300060.pdf about the book [1] provides some hints.\n\n\n\n[1] Matt Cavanagh, Against Equality of Opportunity. Clarendon Press, 2002."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission645/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700218162511,
                "cdate": 1700218162511,
                "tmdate": 1700218162511,
                "mdate": 1700218162511,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OPOg9XlosB",
                "forum": "cVea4KQ4xm",
                "replyto": "D3tg9ntcwK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission645/Reviewer_fCKF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission645/Reviewer_fCKF"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response"
                    },
                    "comment": {
                        "value": "Thanks for your detailed responses. \n\nRe A1.2, thanks for the clarification. While this example seems to assume the features are independent but still helps. \n\nHowever, I am still not convinced that the proposed notion is better. \n\nMeanwhile, while I agree that the proposed fairness notion is better than the demographic parity from the aspect of philosophy, as I mentioned, there are other metrics that are more reasonable from the aspect of philosophy. Establishing some connection between the proposed fairness notion with them (not just DP) would be very helpful. \n\nMoreover, I cannot see how will the paper integrate the related work discussion here into the final version.\n\nTo summarize, I re-evaluate the idea and agree that the proposed equal treatment is interesting. However, there is still some work to be done to make it ready for being published. Therefore, I will keep my original score."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission645/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700637487072,
                "cdate": 1700637487072,
                "tmdate": 1700637487072,
                "mdate": 1700637487072,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "f4aKTDoBXN",
                "forum": "cVea4KQ4xm",
                "replyto": "ckAAQQVRBJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission645/Reviewer_fCKF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission645/Reviewer_fCKF"
                ],
                "content": {
                    "title": {
                        "value": "To Authors"
                    },
                    "comment": {
                        "value": "The causal fairness, and individual fairness are also very reasonable fairness notions. However, as the authors mentioned, there are some limitations of causal fairness and individual fairness. Therefore, it would be very helpful if the authors could establish some connection between equal treatment and these existing \"reasonable\" fairness notions just like what the authors did for demographic parity, and claim that the proposed fairness notion also enjoys some empirical advantages (e.g., convenient, etc). This more thorough comparison would be very helpful."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission645/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700642650773,
                "cdate": 1700642650773,
                "tmdate": 1700642650773,
                "mdate": 1700642650773,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ALd7uBgMdC",
                "forum": "cVea4KQ4xm",
                "replyto": "bR3JOzNtoH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission645/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission645/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer, in the following, we relate further metrics to philosophical requirements, as you have asked for. Please let us know if further questions or doubts remain.\n\n__Individual fairness__ is specified according to the original definition by requiring that\n$d (f(x),f(x')) \\leq L \\cdot d(x,x')$ where L > 0 is a Lipschitz constant and\nd(\u00b7, \u00b7) is a distance function.\n\nFrom a philosophical perspective, individual fairness is closest to the liberal point of view. However, it fails the requirements of liberalist arguments. Liberalism argues for meritocracy, i.e. disparate treatment is okay if it is based on varying efforts or preferences of individuals but not fair if it is based on characteristics that individuals did not choose.\nE.g. it's fair to hire someone because of better grades, but it is unfair if these grades depend on ethnicity. These varying treatments of attributes is not captured by the definition of Individual Fairness above - but it is captured by our notion of Equal Treatment.\n\nAlso, from the technical perspective, the philosophical requirements are not matched: (i) a definition of distance including continuous and discrete attributes leaves a lot of leeways and leaves decisions to the choice of hyperparameters. (ii) Individual fairness is not a statistical measure and will fail to account for situations, such as hiring decisions, where one needs to decide even if the differences between individuals are minimal (also our rebuttal answer A.2)\n\n \n \n__Fairness through un-awareness__\nAn algorithm is fair according to this definition of fairness if all the protected attributes $Z$ are not explicitly used in decision-making.\nAny learned function $f: X \\rightarrow Y$ that excludes $Z$ satisfies this. Fairness by unawareness has the shortcoming that elements of $X$ can depend on $Z$. Such features are called proxy features. They can be used to train $f$ to make discriminatory decisions.\n\n__Counterfactual Fairness__\nAnother metric iscounterfactual fairness[1]. In the literature, it has been observed that this definition is \u201cbasically demographic parity\u201d[2]. Thus, counterfactual fairness exhibits the same drawbacks that we have elaborated about Demographic Parity\n \n \n__Causal Fairness__\nMore commonly, causal fairness refers to metrics that require causal structure to be known for computing fairness or its violation. For example, Definition 1 ( taken from [3]) includes interventions to define fairness. Even if this definition may mirror what the liberal school of thought asks for. It remains a huge challenge to model or learn causal structures - and most often they remain unavailable.\n\n\n\n__To conclude__, our notion of equal treatment is a practical and useful measure for judging the amount of unfairness, following the liberal school of thought in philosophy. Dealing with philosophical and practical requirements is indeed a superior fairness metric. We acknowledge that we should have elaborated on these metrics more broadly. However, as the reviewer may see from our rebuttal, we have concise arguments as to the superiority of our metric and are happy to include them in the paper - most likely in the appendix due to length restrictions.\n\n[1] Counterfactual Fairness.Matt J. Kusner, Joshua R. Loftus, Chris Russell, Ricardo Silva: NIPS 2017\n\n[2] Counterfactual Fairness Is Basically Demographic Parity. Lucas Rosenblatt, R. Teal Witter. AAAI 2023\n\n[3] Causal feature selection for algorithmic fairness. Galhotra, S., Shanmugam, K., Sattigeri, P., & Varshney, K. R. In Proceedings of the 2022 International Conference on Management of Data."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission645/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700656850421,
                "cdate": 1700656850421,
                "tmdate": 1700656876342,
                "mdate": 1700656876342,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cVo2wWgXtq",
            "forum": "cVea4KQ4xm",
            "replyto": "cVea4KQ4xm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission645/Reviewer_1Jx6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission645/Reviewer_1Jx6"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose an Equal Treatment Inspector that identifies features responsible for the equal treatment fairness violation. \nThe authors perform experiments using LIME and Shapley explanation methods and use xgboost for the models and logistic regression for the inspectors."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The authors identify an interesting problem in fair predictive decision-making. \nThey propose a feasible solution and perform various experiments. \nIn addition, authors operationalize their method, which is rare."
                },
                "weaknesses": {
                    "value": "Operationalized tool: ``explanationspace `` https://explanationspace.readthedocs.io/en/latest/auditTutorial.html\n- I tried out the code, and while I found it impressive, several issues made the test hard.  \n  - When I investigated an example: https://explanationspace.readthedocs.io/en/latest/audits.html,  I realized that installing ``explanationspace`` from  https://pypi.org/project/explanationspace/#description was effective whereas the provided step in the installation doc didn't work (https://explanationspace.readthedocs.io/en/latest/installation.html)\n  - The Fairness Audits: Equal Treatment example uses ``fairtools. detector import ExplanationAudit``. I couldn't find the documentation for the functions from https://pypi.org/project/FAIRtools, and the described functions directly below the example correspond to ``explanationspace.audits.ExplanationAudit``. I changed other aspects of the code(``from fairtools.detector import ExplanationAudit`` to ``from explanationspace import ExplanationAudit`` and ``detector.fit(X, y, Z=\"var4\")`` to ``detector.fit(X, yu, Z=X[\"var4\"])`` and .get_auc_val() to predict_proba).\n- Authors should please improve the documentation in terms of ``all`` the required packages to install (requirements.txt) and the description of results in the tutorial to facilitate easy usage and adoption.  \n\nPaper structuring and related works: \n- While the authors propose an interesting perspective, the paper's structuring makes it hard to appreciate their contributions. The crucial and informative information that could have made the paper stronger is relegated to the appendix. \nFor example, the better experiments, presentation, explanation of results, and the description of explanation functions used, among others, are in the appendix. \n\n- In the introduction and section 2, several introduced ideas are not well connected or explained.  There are so many ideas, it's easy to miss the gist. Additionally, the paper is more oriented towards using explanation methods (SHAP and LIME) to investigate disparities in feature importance across protected groups. However, the authors provide insufficient related work in the area and problem background.  For example, there are lots of similarities between this work and other works; ``Model Explanation Disparities as a Fairness Diagnostic``: https://arxiv.org/pdf/2303.01704.pdf, ``Explanability for fair machine learning``:https://arxiv.org/pdf/2010.07389.pdf.\n\nMethodology\n- To me, some proofs and examples seem limited and don't explore corner cases. For example, I think that the statistical independence of Z from the explanation of features is a necessary but not sufficient condition for the statistical independence of the model from Z. Additionally, in example 4.3, feature X_{3} not being statistically independent of Z and the function being a linear model makes it easy to do the proof through zeroing out that features.  In most cases, the function/model might not be linear, and the relationship between features might be complex and causal graphs hard to uncover. It seems like maybe the tool being diagnostic instead of a fixture might be a better point of view. \n- Given that one might not have access to test data, would it be better to apply the ET inspector as a diagnostic on the train/val data instead? \n\nDiscussion and Experiments in the main body\n- It's hard to appreciate authors' experiments and results because of the following reasons; \n  - The experiment setup of 3 features and one with varied dependence on Z makes it hard to appreciate the author's contributions.\n  - The authors don't provide sufficient explanations or discussion of the results.\n  - Authors could have compared their experimental results to other related works and shown the impact on ET inspector and explanations on fairness on the different groups (something similar to table 5 in the appendix). \n\nMinor or okay to address later\n- Having an algorithm or bulleted procedure could have improved readability.\n- For novelty, authors use AUC rather than accuracy in their C2ST instead of accuracy as previously done.  This is a bit of a tradeoff, and while the scale invariance might be good, it is damaging when inspecting other cases of fairness where one might, for example care more about false positives than false negatives. \n- Given the importance of understanding the features of fairness, I think it might be important to distinguish between protected and sensitive attributes.  Not all protected features are sensitive attributes. For example, gender plays a key role in admission to single-sex schools, or age plays a crucial role in admission to age-range sports or activities. \n- Reliance on Z as a binary variable is restrictive, especially since there are lots of intersectionalities. \n- The explanation highly relies on f_{\\theta}. It might be informative to also look at features independent of the model."
                },
                "questions": {
                    "value": "While the proposed method has several similarities with ``Explanability for fair machine learning`` and ``Model Explanation Disparities as a Fairness Diagnostic`` papers, operationalizing their model has positively influenced my score.  \nHowever, issues in the writeup and code documentation negatively influenced my score. Authors should please address these issues in the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission645/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission645/Reviewer_1Jx6",
                        "ICLR.cc/2024/Conference/Submission645/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission645/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699793180863,
            "cdate": 1699793180863,
            "tmdate": 1699794182705,
            "mdate": 1699794182705,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sTluGhV2Yw",
                "forum": "cVea4KQ4xm",
                "replyto": "cVo2wWgXtq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission645/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission645/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer, many thanks for the comments. Please consider the main points of our rebuttal. We address specific points below:\n\n> I.1Operationalized tool:\n\nA.1 First of all, we would like to thank the reviewer for trying out the code. We really appreciate it. \nWe have now made a software release `explanationspace==0.0.2, addressing the reviewer\u2019s concerns. \n\nWe would like to distinguish between the open-source software, which we will disseminate and maintain, and the repository  of the experimental results of this paper. \n\nWe have now fixed the mentioned issues so the installation and tutorial of https://explanationspace.readthedocs.io/en/latest/index.html\nworks accordingly.\n\n\n>I.2. While the authors propose an interesting perspective, the paper's structuring makes it hard to appreciate their contributions. \n\nA.2 Due to space constraints, we decided to privilege the main contributions in the main text, and the detalled discussions in the appendix - bade based on what we believe it is more adequate to ICLR. We are open to any restructuring suggestions.\n\n\n> I.3. The authors provide insufficient related work in the area and problem background....\n\nA.3. Both papers are distinct from ours. They do not focus on the intersection of equality notions and ML, nor on the techniques used (Classifier Two Sample Test), proposing methods that don\u2019t have the theoretical background solidity of our approach.  \n\nWe will be happy to acknowledge the recent work reported in the two arxiv papers. To the best of our knowledge, the first paper is still under review, while the second, we are unaware of its publication status. However, we hope that their absence in the submission is not held against us.\n\n\n> I.4. Some proofs and examples seem limited and don't explore corner cases.....\n\nA.4 This basic example serves us to compare with the benefits of measuring bias on the input data.\nWe agree that in most cases the model might not be linear. It's in this situation where the Shapley values become of use, we have studied non-lineal models in Appendix E.2 and E.3\nOften in tabular data problems, ML models are fed with many more features than the model actually use. This method helps to deal with this problem. \n\n\n> I.5 Given that one might not have access to test data, would it be better to apply the ET inspector as a diagnostic on the train/val data instead?\n\nA.5 We agree with the reviewer on this point. ET inspector can be used to detect bias (in train/val data), and to monitor bias in the absence of labeled data. We will point out this in the paper. \n\n\n> I.7Authors could have compared their experimental results to other related works and shown the impact on ET inspector and explanations on fairness on the different groups (something similar to table 5 in the appendix)\n\nA.7.  We kindly ask the reviewer which other related works our approach should be compared to.\n\n\n> I.8. For novelty, authors use AUC rather than accuracy in their C2ST instead of accuracy as previously done. This is a bit of a tradeoff, and while the scale invariance might be good, it is damaging when inspecting other cases of fairness where one might, for example, care more about false positives than false negatives.\n\nA.8. Even if we use AUC, other metrics are also possible as far as a test statistics is available (e.g., accuracy in Lopez-Paz and Oquab 2017)\n\n\n\n> I.9Reliance on Z as a binary variable is restrictive, especially since there are lots of intersectionalities.\n\nA.9 The inspector $g$ can be a multi-class classifier. In the experiment of Figure 3, there are several ethnicities. In this case we have used a 1vsAll approach, further methods can be extended even to continuous protected attributes using regression instead of classification\n\n\n> I.10 The explanation highly relies on f_{\\theta}. It might be informative to also look at features independent of the model.\n\nA.10 This is done in Example 4.3 in the Theoretical section and in the \u201cFairness Input\u201d analyses in the Experimental section (e.g., Figure 2).\n\n\n> I.11 While the proposed method has several similarities with Explanability for fair machine learning and Model Explanation Disparities as a Fairness Diagnostic paper, operationalising their model has positively influenced my score. However, issues in the writeup and code documentation negatively influenced my score. Authors should please address these issues in the weakness section.\n\nA.11  We have aimed to address the reviewer\u2019s concerns in the above discussion.\n\nWe do not think that not comparing to recent work that is still currently under review should be held against us. We will add a comparison against this work in the updated version of the manuscript."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission645/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700218497261,
                "cdate": 1700218497261,
                "tmdate": 1700218497261,
                "mdate": 1700218497261,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Qs2xPSBkHP",
                "forum": "cVea4KQ4xm",
                "replyto": "sTluGhV2Yw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission645/Reviewer_1Jx6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission645/Reviewer_1Jx6"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to thank the authors for addressing the questions I had"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission645/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700391831168,
                "cdate": 1700391831168,
                "tmdate": 1700391831168,
                "mdate": 1700391831168,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]