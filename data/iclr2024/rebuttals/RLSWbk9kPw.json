[
    {
        "title": "Generalized Neural Sorting Networks with Error-Free Differentiable Swap Functions"
    },
    {
        "review": {
            "id": "ZAfJO9KsFf",
            "forum": "RLSWbk9kPw",
            "replyto": "RLSWbk9kPw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1020/Reviewer_M9Vx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1020/Reviewer_M9Vx"
            ],
            "content": {
                "summary": {
                    "value": "The authors investigate the properties of differentiable swap functions for neural sorting. They conclude that the existing methods shrink the difference between different elements at each application of the swap function. This has a negative effect on the performance of such networks. To alleviate this problem, the authors propose an Error-Free Differentiable Swap Function, based on the straight-through estimator. Also, they propose to use transformers without positional encodings for processing the raw inputs before they are fed to"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Simple method\n- Better performance on long sequences\n- Motivated by analyzing existing methods"
                },
                "weaknesses": {
                    "value": "- No clear distinction between the Transformer-based network vs the differentiable swap function. These are two orthogonal changes, and the Transformer seems to have much more performance impact than the swap function itself. Yet, the paper focuses mostly on the swap function.\n- The method uses both the soft and the hard sorting network for training. Also, Tab 15 shows that training with $\\lambda = 0$ has basically no effect on the quality of the trained network. As the standard deviations are not shown, it is unclear whether there is any effect here, but $\\lambda=1$ seems to hinder accuracy. This begs the question if the Error-Free Differentiable Swap Function has any contribution to training, or just during inference, in which case there is no need for the straight-through estimator. It would be worth evaluating the baseline Optimal Diffsort with hard min/max during inference, to see this.\n\nBased on the 2 points above, I'd like to see two sets of experiments:\n- the best optimal diff sort models with identical transformer architecture to those of the Error-Free DSF\n- The same architecture as the best Error-Free DSF with \\lambda=0 (thus, being equivalent to using hard min/max during inference, but being trained as \"Diffsort Optimal\")"
                },
                "questions": {
                    "value": "Why are the results on 4-digit MNIST with a seq length of 32 significantly better in Tab 1 than in Tab 15?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1020/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1020/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1020/Reviewer_M9Vx"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1020/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698575919956,
            "cdate": 1698575919956,
            "tmdate": 1700154591969,
            "mdate": 1700154591969,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "i7mNJdrT0r",
                "forum": "RLSWbk9kPw",
                "replyto": "ZAfJO9KsFf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1020/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1020/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer M9Vx (1/n)"
                    },
                    "comment": {
                        "value": "We appreciate your constructive comment to improve our work.\n\n> No clear distinction between the Transformer-based network vs the differentiable swap function. These are two orthogonal changes, and the Transformer seems to have much more performance impact than the swap function itself. Yet, the paper focuses mostly on the swap function.\n\nWe agree with your point. To accommodate your comment, we have updated **Section 4** and added **Section G** and **Figure 7**. These updates explain our Transformer-based network in detail. Please see our revision as well as the discussion described in the **Effects of Multi-Head Attention in the Problem (3)** paragraph of **Section 7**.\n\n>  As the standard deviations are not shown,\n\nWe provide the standard deviations here. These results show the standard deviations of the last three rows of **Table 1**.\n\n|   | 3 | 5 | 7 | 9 | 15 | 32 |\n|---|---|---|---|---|----|----|\n| CNN | 0.41 (0.29) | 1.08 (0.44) | 2.12 (0.76) | 2.74 (0.85) | 2.75 (0.81) | 0.40 (0.49) |\n| Transformer-S | 0.20 (0.16) | 0.21 (0.11) | 0.57 (0.22) | 0.56 (0.24) | 0.99 (0.30) | 1.54 (0.46) |\n| Transformer-L | 0.27 (0.18) | 0.35 (0.17) | 0.52 (0.24) | 0.36 (0.18) | 0.76 (0.20) | 1.14 (0.34) |\n\nBased on the values shown above, the training of our networks is dependent on random seeds but the dependency is not strong. To consistently present the experimental results, we omitted them in the submission.\n\n> $\\lambda = 1$ seems to hinder accuracy.\n\nYou are right, but the use of our hard loss is effective for $0 < \\lambda < 1$. As described in **Section 7**, our hard loss is likely to make loss values discrete. Therefore, the selection of appropriate balancing parameters is significant. We would like to note that this selection process is a typical problem in the deep learning community,\nand we also solved a similar problem by following the previous literature.\n\n> the best optimal diffsort models with identical transformer architecture to those of the Error-Free DSF\n\nIf we understand your concern correctly, we have already included this ablation study. We can see these results in **Table 4**. For your convenience, we highlight these results here.\n\n| Method | Model | 3 | 5 | 7 | 9 | 15 | 32 |\n|--------|-------|---|---|---|---|----|----|\n| Diffsort | CNN | 94.6 (96.3) | 85.0 (93.3) | 73.6 (90.7) | 62.2 (88.5) | 31.8 (82.3) | 1.4 (67.9) |\n| Ours | CNN | 94.8 (96.4) | 86.9 (94.1) | 74.2 (90.9) | 62.6 (88.6) | 34.7 (83.3) | 2.1 (69.2) |\n| Diffsort | Transformer-S | 95.9 (97.1) | 90.2 (95.4) | 83.9 (94.2) | 77.2 (92.9) | 57.3 (89.7) | 16.3 (81.7) |\n| Ours | Transformer-S | 95.9 (97.1) | 94.8 (97.5) | 90.8 (96.5) | 86.9 (95.7) | 74.3 (93.6) | 37.8 (87.7) |\n| Diffsort | Transformer-L | 96.5 (97.5) | 92.6 (96.4) | 87.6 (95.3) | 82.6 (94.3) | 67.8 (92.0) | 32.1 (85.7) |\n| Ours | Transformer-L | 96.5 (97.5) | 95.4 (97.7) | 92.9 (97.2) | 90.1 (96.5) | 82.5 (95.0) | 46.2 (88.9) |\n\n> The same architecture as the best Error-Free DSF with $\\lambda = 0$ (thus, being equivalent to using hard min/max during inference, but being trained as \"Diffsort Optimal\")\n\nSince we directly calculated accuracy from the scores $\\mathbf{s}$ produced by the neural network (see **Equations (19) and (20)**), the results shown in the paper (including Diffsort (Optimal)) are reported by following your suggestion, which uses hard min/max during inference.\n\n> Why are the results on 4-digit MNIST with a seq length of 32 significantly better in Tab 1 than in Tab 15?\n\n**Table 15** uses fixed hyperparameters (i.e., steepness and learning rate) for fair comparison, so that it can show a thorough analysis on our models. Therefore, the results in **Table 1** is better than ones in **Table 15**. Please see **Section K** for more details."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1020/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700068665029,
                "cdate": 1700068665029,
                "tmdate": 1700068665029,
                "mdate": 1700068665029,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IVvEQm2at0",
                "forum": "RLSWbk9kPw",
                "replyto": "i7mNJdrT0r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1020/Reviewer_M9Vx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1020/Reviewer_M9Vx"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the clarifications. This clears most of my doubts. My remaining concern is about Tab 15. Is there any reason why it does not use better hyperparameters from Table 1 as the fixed hyperparameters? Reporting ablations on a weaker hyperparameter set casts doubts on the validity of the trends that we can conclude from the results."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1020/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700146739051,
                "cdate": 1700146739051,
                "tmdate": 1700146739051,
                "mdate": 1700146739051,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kzIdayyRLY",
                "forum": "RLSWbk9kPw",
                "replyto": "Zk8igZDo8t",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1020/Reviewer_M9Vx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1020/Reviewer_M9Vx"
                ],
                "content": {
                    "comment": {
                        "value": "Please include the experiments with the new hyperparameters in the final version of the paper. \n\nGiven that most of my doubts are resolved, I'm increasing my score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1020/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700154566800,
                "cdate": 1700154566800,
                "tmdate": 1700154566800,
                "mdate": 1700154566800,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rpawCYBJ0N",
                "forum": "RLSWbk9kPw",
                "replyto": "jJ4apDOQMb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1020/Reviewer_M9Vx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1020/Reviewer_M9Vx"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to thank the authors for the update."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1020/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700666474365,
                "cdate": 1700666474365,
                "tmdate": 1700666474365,
                "mdate": 1700666474365,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1KSEqY8jM8",
            "forum": "RLSWbk9kPw",
            "replyto": "RLSWbk9kPw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1020/Reviewer_rtfd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1020/Reviewer_rtfd"
            ],
            "content": {
                "summary": {
                    "value": "This paper examines an error-free differentiable swap function, which enables the creation of trainable sorting networks which do not accumulate errors as multiple applications of this function are performed. The authors evaluate the sorting network created using their swap function, and demonstrate improvements over previous differentiable sorting techniques."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The proposed method is simple and easy to understand. The function used is equivalent to using the soft version of the maximum / minimum functions during the backward step, and the exact functions during the forward step. This allows for differentiability, without accumulating errors.  I especially appreciated the presentation of the samples in Figure 3, as it helped me understand how the method precisely works.\n\n- The authors perform experiments on a variety of architectural choices and sequence lengths, which affect the accuracy of sorting algorithms. They show how incorporating an attention based part in the architecture is the better choice for sorting large sequences of data, since it provides a way to incorporate the entire sequence of data to sort and contextualize the output for each sample."
                },
                "weaknesses": {
                    "value": "- Overall, while interesting, the applicability of the method seems limited at first glance, since one can in principle learn the function that maps samples to ordered objects first and then perform the sort using standard methods. I believe that the authors should either better motivate the need for this sort of method, or demonstrate in their experiments why learning the ordinal value directly does not perform as well.\n\n- I think there is a gap in the proof of Proposition 2. Namely, the fact that the soft minimum and the soft maximum are the same as $k \\to \\infty$ seems like it is missing a step in how it is derived. I believe that it should hold, given the differentiability of the function, but filling in the missing step here is needed.\n\n- I believe that certain parts can be improved with respect to the clarity of the paper. More specifically:\n\n  - It seems to me that the paper assumes that the function used to soften the maximum is the sigmoid, and some of its properties are listed in Section 2 - however, it is not clear if the sigmoid is the only choice, or if these properties are sufficient for a function to satisfy. I would appreciate it if the authors could clarify this point.\n\n  - Again in Section 2, I had some trouble understanding the wire-based construction of the permutation matrix, especially since the associated Figure is in the Appendix. I believe that the point of the authors would be clearer if they moved the Figure in the main paper, as well as simplify the explanation for the construction (saying, e.g., that each wire is a step to fix a single wrong ordering, and iterative application of these steps results in a sorted set).\n\n  - I believe that Figures 1 and 2 can be simplified / combined, since they are essentially showing similar things - namely, that the function is error-free, which is not surprising given that it is not softened during the forward pass.\n\n  - In Section 7, the paragraph \u201cUtilization of Hard Permutation Matrices\u201d is not very clear to me. I would appreciate it if the authors could rephrase the point they are trying to make here.\n\n- As a minor point, in Section 6, the authors should rephrase how they refer to previous works, for example: \u201cThe work (Petersen et al., 2021) has suggested \u2026\u201d -> \u201cPetersen et al. (2021) have suggested\u2026\u201d.\n\nOverall, I think this is an interesting work, but I believe that the authors should improve upon the clarity of the paper and the motivation of their technique."
                },
                "questions": {
                    "value": "As mentioned above, I would be grateful if the authors could clarify the paragraph \u201cUtilization of Hard Permutation Matrices\u201d of Section 7."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1020/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698707371930,
            "cdate": 1698707371930,
            "tmdate": 1699636027941,
            "mdate": 1699636027941,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "orXXh9Au4f",
                "forum": "RLSWbk9kPw",
                "replyto": "1KSEqY8jM8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1020/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1020/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer rtfd (1/n)"
                    },
                    "comment": {
                        "value": "We appreciate your constructive comment to improve our work.\n\n> I believe that the authors should either better motivate the need for this sort of method, or demonstrate in their experiments why learning the ordinal value directly does not perform as well.\n\nBy considering your comment, we have revised **Section N** of our work. For your convenience, we provide the updated paragraph that can enhance the motivation of our work here:\n\nIt is challenging to directly sort a sequence of generic data instances without using auxiliary networks and explicit supervision. Unlike earlier sorting methods, this sorting network-based research (Petersen et al., 2021; 2022) including our work ensures that we can train a neural network that predicts numerical scores and eventually sorts them, even though we do not necessitate accessing explicit supervision such as exact numerical values of the contents in high-dimensional data. In this sense, the practical significance of our proposed methods can be highlighted by offering this possibility of solving a sorting problem with high-dimensional inputs. For example, as shown in Section 5, we can compare images of street view house numbers using the sorting network where our neural network is trained without exact house numbers.\n\nMoreover, instead of using costly supervision, our networks allow us to sort high-dimensional instances in a sequence where information on comparisons between instances is only given. This scenario often occurs when we cannot obtain complete supervision. For example, if we would sort four-digit MNIST images, ordinary neural networks are designed to solve a classification task by predicting class probabilities each of which indicates one of all labels from \"0000\" to \"9999\". If some labels are missing and further we do not know the exact number of labels, they might fail in predicting unseen data corresponding to those labels. Unlike these methods, it is possible to solve sorting problems with our networks in such a scenario.\n\nAs discussed in Section 7, the hard permutation matrices produced by our methods encourage us to swap instances exactly, instead of the linear combination of instances. This characteristic is required when we are given the final outcomes of sorting as supervision. This scenario is tested by the experiments presented in Section 5.2. In these experiments, we can consider that original images are provided as supervision. Building on the advantages of neural network-based sorting networks, we extend their practical significance into the cases that need hard permutation matrices.\n\nFurthermore, this study can be applied in diverse deep learning tasks for learning to sort generic high-dimensional data, such as information retrieval (Cao et al., 2007; Liu, 2009) and top-$k$ classification (Berrada et al., 2018)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1020/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700068514071,
                "cdate": 1700068514071,
                "tmdate": 1700068514071,
                "mdate": 1700068514071,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "h6QmPw0ayP",
                "forum": "RLSWbk9kPw",
                "replyto": "1KSEqY8jM8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1020/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1020/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer rtfd (2/n)"
                    },
                    "comment": {
                        "value": "> I think there is a gap in the proof of Proposition 2. Namely, the fact that the soft minimum and the soft maximum are the same as $k \\to \\infty$ seems like it is missing a step in how it is derived. I believe that it should hold, given the differentiability of the function, but filling in the missing step here is needed.\n\nThank you for pointing this out. We have updated the proof of Proposition 2. Please see **Section E**.\n\n> It seems to me that the paper assumes that the function used to soften the maximum is the sigmoid, and some of its properties are listed in Section 2 - however, it is not clear if the sigmoid is the only choice, or if these properties are sufficient for a function to satisfy. I would appreciate it if the authors could clarify this point.\n\nAs described in **Section 2**, the required properties of $\\sigma$ are that $\\sigma(x)$ is differentiable, $\\sigma(x)$ is non-decreasing, $\\sigma(x) = 1$ if $x \\to \\infty$, $\\sigma(x) = 0$ if $x \\to -\\infty$, $\\sigma(0) = 0.5$, and $\\sigma(x) = 1 - \\sigma(-x)$.\nIf a function is (point) symmetric, bounded, and differentiable by satisfying the aforementioned properties, that function should be $s$-shaped or sigmoid. Note that the sigmoid function in machine learning, which is widely used as an activation function, is called the logistic function in this paper.\n\n> Again in Section 2, I had some trouble understanding the wire-based construction of the permutation matrix, especially since the associated Figure is in the Appendix. I believe that the point of the authors would be clearer if they moved the Figure in the main paper, as well as simplify the explanation for the construction (saying, e.g., that each wire is a step to fix a single wrong ordering, and iterative application of these steps results in a sorted set).\n\nWe have moved the figure and updated **Section 2**. Please refer to **Section 2** and **Figure 2**.\n\n> I believe that Figures 1 and 2 can be simplified / combined, since they are essentially showing similar things - namely, that the function is error-free, which is not surprising given that it is not softened during the forward pass.\n\nBy considering your comment, **Figure 1** only remains in the main article. **Figure 2** is replaced with a figure with wire sets.\n\n> In Section 7, the paragraph \"Utilization of Hard Permutation Matrices\" is not very clear to me. I would appreciate it if the authors could rephrase the point they are trying to make here.\n\n> As mentioned above, I would be grateful if the authors could clarify the paragraph \"Utilization of Hard Permutation Matrices\" of Section 7.\n\nThe experiments shown in **Section 5.2** solve a task to place image fragments on right positions. It requires swapping image fragments exactly, not combining image fragments via linear combination. Our sorting network ensures that final outcomes preserve initial instances from the application of sorting operations. We have also updated the corresponding paragraph of **Section 7**.\n\n> As a minor point, in Section 6, the authors should rephrase how they refer to previous works, for example: \"The work (Petersen et al., 2021) has suggested ...\" -> \"Petersen et al. (2021) have suggested...\".\n\nThank you for pointing this out. We have fixed it and also similar cases; please see **Section 6**."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1020/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700068571242,
                "cdate": 1700068571242,
                "tmdate": 1700068571242,
                "mdate": 1700068571242,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ozBtueYgFN",
                "forum": "RLSWbk9kPw",
                "replyto": "1KSEqY8jM8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1020/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1020/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank you for your constructive feedback again.\n\nWe have answered your concerns in the rebuttal.\n\nPlease take a look into our response and let us know if you still have any concerns."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1020/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700489767781,
                "cdate": 1700489767781,
                "tmdate": 1700489767781,
                "mdate": 1700489767781,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "apymNjQH6g",
                "forum": "RLSWbk9kPw",
                "replyto": "1KSEqY8jM8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1020/Reviewer_rtfd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1020/Reviewer_rtfd"
                ],
                "content": {
                    "title": {
                        "value": "Response to Author Rebuttal"
                    },
                    "comment": {
                        "value": "I'd like to thank the authors for responding to my points. I now better understand the points in Section 2 and 7. For Section 7 in particular (as a minor point), I believe it would be good if the authors explicitly stated that $P_soft$ is not a good idea, precisely because it mixes representations, instead of simply sorting them (potentially replacing the current first sentence in the paragraph). I also now understand why simple supervision is not a good idea, since it is also affected by the amount of samples needed for each ordinal that we are trying to sort.\n\nRegarding the proof in Section E, I think it still contains a small issue - namely, the fact that $\\bar{min}^k$ is strictly increasing and $\\bar{max}^k$ is strictly decreasing does not (by itself) prove that these converge to the same limit (since it could be the case that $\\lim_{k \\to \\infty} \\bar{max}^k > \\lim_{k \\to \\infty} \\bar{min}^k$ - they could converge to different points). To fully complete the proof, I think something like $\\lim_{k \\to \\infty} (\\bar{max}^k - \\bar{min}^k) = 0$ is necessary, which I believe can be derived by differentiability/continuity of $\\sigma$. Explicitly showing that they both converge to $(max(x,y) + min(x,y)) / 2 = (x+y)/2$ would also suffice. I understand that this is a minor point overall, but I believe it should be corrected."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1020/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700515041026,
                "cdate": 1700515041026,
                "tmdate": 1700515041026,
                "mdate": 1700515041026,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OmNGYOwbdV",
                "forum": "RLSWbk9kPw",
                "replyto": "1KSEqY8jM8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1020/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1020/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "The reviewer-author discussion period is ending soon.\n\nIf your concerns are resolved, please update your review."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1020/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700680943769,
                "cdate": 1700680943769,
                "tmdate": 1700680943769,
                "mdate": 1700680943769,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WLCaKRn9Vl",
            "forum": "RLSWbk9kPw",
            "replyto": "RLSWbk9kPw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1020/Reviewer_Stfx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1020/Reviewer_Stfx"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a new approach to sorting through neural networks. The authors extend traditional sorting mechanisms to handle more abstract and high-dimensional data types like multi-digit images and image fragments. The core contribution is an \"error-free differentiable swap function,\" which allows the network to learn a mapping from complex inputs to sorted sequences effectively. This work aims to broaden the applicability of sorting algorithms by making them more versatile for dealing with intricate data types."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Novelty: The paper introduces a novel \"error-free differentiable swap function\" to generalize sorting algorithms for more complex data types.\n- Mathematical Rigor: The paper employs rigorous mathematical formulations to define the sorting problem and its extension for high-dimensional data.\n- Methodological Approach: The use of a permutation-equivariant Transformer network with multi-head attention is a strong methodological choice for capturing dependencies between elements in the sequence.\n- Clarity and Organization: The paper is well-organized and clearly written, making it accessible to readers who are familiar with the domain."
                },
                "weaknesses": {
                    "value": "- Insufficient Discussion on Limitations: While the paper mentions that its methods are limited to sorting algorithms, it doesn't provide a thorough discussion on other potential limitations, such as scalability or conditions under which the method may not work well.\n- Hyperparameter Sensitivity: The paper discusses various hyperparameters like steepness and learning rate but does not provide a comprehensive analysis of their impact, which could be a concern for practical implementations.\n- Complexity of the Model: The use of a permutation-equivariant Transformer network with multi-head attention could make the model computationally expensive, which is not addressed in the paper.\n- Missed Opportunity for Theoretical Analysis: While the paper is empirically driven, a more in-depth theoretical analysis could strengthen the paper by providing bounds or guarantees on the performance of the proposed methods."
                },
                "questions": {
                    "value": "How sensitive is the model's performance to the choice of hyperparameters like steepness and learning rate? Could the authors provide more insights into this?\nCould the authors provide a more comprehensive discussion on the limitations of their approach, particularly in terms of scalability and conditions where the method may not be applicable?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1020/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1020/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1020/Reviewer_Stfx"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1020/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698779910487,
            "cdate": 1698779910487,
            "tmdate": 1699636027832,
            "mdate": 1699636027832,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mbVslPzJDW",
                "forum": "RLSWbk9kPw",
                "replyto": "WLCaKRn9Vl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1020/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1020/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Stfx (1/n)"
                    },
                    "comment": {
                        "value": "We appreciate your constructive comment to improve our work.\n\n> Insufficient Discussion on Limitations: While the paper mentions that its methods are limited to sorting algorithms, it doesn't provide a thorough discussion on other potential limitations, such as scalability or conditions under which the method may not work well.\n\n> Could the authors provide a more comprehensive discussion on the limitations of their approach, particularly in terms of scalability and conditions where the method may not be applicable?\n\nBy considering your comment, we have updated **Sections M and N**.  We present those sections here:\n\n### M. Limitations\n\nWhile a sorting task is one of the most significant problems in computer science and mathematics (Cormen et al., 2022), our ideas, which are built on sorting networks (Knuth, 1998; Ajtai et al., 1983), can be limited to sorting algorithms. It implies that it is not easy to devise neural network-based approaches to solving general problems in computer science, e.g., combinatorial optimization, which are inspired by our ideas. Nevertheless, the use of our algorithms enables us to learn a neural network-based sorting network for high-dimensional data, and employ the neural network in the tasks on sorting high-dimensional data instances.\n\nIn addition, while our proposed methods show the superior performance compared to the baseline methods, this line of research suffers from the performance degradation for longer sequences as shown in Tables 1, 2, and 3. More precisely, for longer sequences, the element-wise accuracy of our methods does not decline dramatically, but the sequence-wise accuracy of our methods drops due to the nature of sequences. Incorporating our contributions such as the error-free DSFs and the Transformer-based networks, we expect that the further progress of neural network-based sorting networks can be achieved. In particular, the consideration of more sophisticated neural networks with a huge number of parameters, which are capable of handling longer sequences, might help improve performance. This will be left for future work.\n\nFurthermore, our frameworks successfully learn relationships between high-dimensional data with ordinal contents as shown in Section 5. However, our methods are supposed to fail in sorting data without ordinal information; the elaborate discussion on this topic can also be found in Section 7. In order to sort more ambiguous high-dimensional data, we can combine our work with part-based or segmentation-based approaches.\n\n### N. Broader Impacts\n\nIt is challenging to directly sort a sequence of generic data instances without using auxiliary networks and explicit supervision. Unlike earlier sorting methods, this sorting network-based research (Petersen et al., 2021; 2022) including our work ensures that we can train a neural network that predicts numerical scores and eventually sorts them, even though we do not necessitate accessing explicit supervision such as exact numerical values of the contents in high-dimensional data. In this sense, the practical significance of our proposed methods can be highlighted by offering this possibility of solving a sorting problem with high-dimensional inputs. For example, as shown in Section 5, we can compare images of street view house numbers using the sorting network where our neural network is trained without exact house numbers.\n\nMoreover, instead of using costly supervision, our networks allow us to sort high-dimensional instances in a sequence where information on comparisons between instances is only given. This scenario often occurs when we cannot obtain complete supervision. For example, if we would sort four-digit MNIST images, ordinary neural networks are designed to solve a classification task by predicting class probabilities each of which indicates one of all labels from \"0000\" to \"9999\". If some labels are missing and further we do not know the exact number of labels, they might fail in predicting unseen data corresponding to those labels. Unlike these methods, it is possible to solve sorting problems with our networks in such a scenario.\n\nAs discussed in Section 7, the hard permutation matrices produced by our methods encourage us to swap instances exactly, instead of the linear combination of instances. This characteristic is required when we are given the final outcomes of sorting as supervision. This scenario is tested by the experiments presented in Section 5.2. In these experiments, we can consider that original images are provided as supervision. Building on the advantages of neural network-based sorting networks, we extend their practical significance into the cases that need hard permutation matrices.\n\nFurthermore, this study can be applied in diverse deep learning tasks for learning to sort generic high-dimensional data, such as information retrieval (Cao et al., 2007; Liu, 2009) and top-$k$ classification (Berrada et al., 2018)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1020/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700068372435,
                "cdate": 1700068372435,
                "tmdate": 1700068372435,
                "mdate": 1700068372435,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tWC9oBR92v",
                "forum": "RLSWbk9kPw",
                "replyto": "WLCaKRn9Vl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1020/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1020/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Stfx (2/n)"
                    },
                    "comment": {
                        "value": "On the other hand, this nature of neural network-based sorting networks can yield a potential negative societal impact. If this line of research including our proposed approaches is employed to sort controversial high-dimensional data such as beauty and intelligence, it can be thought of as the unethical use cases of artificial intelligence.\n\n> Hyperparameter Sensitivity: The paper discusses various hyperparameters like steepness and learning rate but does not provide a comprehensive analysis of their impact, which could be a concern for practical implementations.\n\n> How sensitive is the model's performance to the choice of hyperparameters like steepness and learning rate? Could the authors provide more insights into this?\n\nWe have revised **Section L**. Please refer to **Section L**.\n\n> Complexity of the Model: The use of a permutation-equivariant Transformer network with multi-head attention could make the model computationally expensive, which is not addressed in the paper.\n\nWe have shown the complexity of our model empirically. As shown in **Tables 1, 2, and 3**, We reported FLOPs and the number of parameters. When we determined the size of our Transformer-based network, we have considered these two factors. Moreover, discussion on this issue is described in the **Effects of Multi-Head Attention in the Problem (3)** paragraph of **Section 7**; please refer to this paragraph.\n\n> Missed Opportunity for Theoretical Analysis: While the paper is empirically driven, a more in-depth theoretical analysis could strengthen the paper by providing bounds or guarantees on the performance of the proposed methods.\n\nWe have proved **Propositions 1, 2, and 3** where their proofs are provided in **Sections D, E, and F**. By these propositions, our error-free DSF can provide more accurate signals to the neural networks we are training. However, as described in your comment, our work is to propose a new framework of neural network-based sorting networks, which can be considered as the empirically driven work. More rigorous guarantees on the performance of the proposed methods will be left for future work."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1020/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700068435846,
                "cdate": 1700068435846,
                "tmdate": 1700068435846,
                "mdate": 1700068435846,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ELMrZTXBBl",
                "forum": "RLSWbk9kPw",
                "replyto": "WLCaKRn9Vl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1020/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1020/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank you for your constructive feedback again.\n\nWe have answered your concerns in the rebuttal.\n\nPlease take a look into our response and let us know if you still have any concerns."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1020/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700489732081,
                "cdate": 1700489732081,
                "tmdate": 1700489732081,
                "mdate": 1700489732081,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9q4CdiSTjS",
                "forum": "RLSWbk9kPw",
                "replyto": "WLCaKRn9Vl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1020/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1020/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "The reviewer-author discussion period is ending soon.\n\nIf your concerns are resolved, please update your review."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1020/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700680895697,
                "cdate": 1700680895697,
                "tmdate": 1700680895697,
                "mdate": 1700680895697,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "EloHK7HyzU",
            "forum": "RLSWbk9kPw",
            "replyto": "RLSWbk9kPw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1020/Reviewer_LP9f"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1020/Reviewer_LP9f"
            ],
            "content": {
                "summary": {
                    "value": "The main content of the paper is the proposal of a generalized neural sorting network with error-free differentiable swap functions. The authors introduce scaled dot-product and multi-head attention and provide proofs for several propositions related to their network. They also discuss the effects of multi-head attention in the problem they are addressing and present empirical studies on the performance gains achieved by their proposed methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Generalization: The proposed neural sorting networks with error-free differentiable swap functions provide a general framework for solving sorting tasks on various types of data, including multi-digit images and image fragments.\n\n2. Performance Improvement: The experimental results demonstrate that the proposed methods outperform the baseline method in terms of sorting performance, as measured by accuracy metrics.\n\n3. Numerical Analysis: The authors provide a thorough numerical analysis of the effects of their methods, compared to the baseline method. This analysis validates the effectiveness of the proposed methods in sorting tasks.\n\n4. In terms of writing, it is explained quite clearly, and one can generally follow the author's train of thought."
                },
                "weaknesses": {
                    "value": "1. Since the methods proposed in the paper are based on sorting networks, they are not applicable to solving general problems in computer science, such as combinatorial optimization. This implies that it may not be easy to devise a neural network-based approach for solving these types of problems using the ideas presented in this work.\n\n2. In terms of the results, it can be observed that as the sequence length increases or the number of segments grows, the performance decreases significantly.\n\n3. I find it somewhat challenging to grasp the practical significance of this work. I believe that this work lacks practical significance to some extent."
                },
                "questions": {
                    "value": "1. I hope the author can provide a detailed explanation of the practical significance of this work.\n\n2. Why does the performance drop significantly when the quantity increases?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1020/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1020/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1020/Reviewer_LP9f"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1020/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698817497633,
            "cdate": 1698817497633,
            "tmdate": 1700492429138,
            "mdate": 1700492429138,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VRaQ2BiNGE",
                "forum": "RLSWbk9kPw",
                "replyto": "EloHK7HyzU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1020/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1020/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer LP9f (1/n)"
                    },
                    "comment": {
                        "value": "We appreciate your constructive comment to improve our work.\n\n> Since the methods proposed in the paper are based on sorting networks, they are not applicable to solving general problems in computer science, such as combinatorial optimization. This implies that it may not be easy to devise a neural network-based approach for solving these types of problems using the ideas presented in this work.\n\n> I hope the author can provide a detailed explanation of the practical significance of this work.\n\n> I find it somewhat challenging to grasp the practical significance of this work. I believe that this work lacks practical significance to some extent.\n\nBy considering your comment, we have updated **Section N**. For your convenience, we present part of that section here:\n\nIt is challenging to directly sort a sequence of generic data instances without using auxiliary networks and explicit supervision. Unlike earlier sorting methods, this sorting network-based research (Petersen et al., 2021; 2022) including our work ensures that we can train a neural network that predicts numerical scores and eventually sorts them, even though we do not necessitate accessing explicit supervision such as exact numerical values of the contents in high-dimensional data. In this sense, the practical significance of our proposed methods can be highlighted by offering this possibility of solving a sorting problem with high-dimensional inputs. For example, as shown in Section 5, we can compare images of street view house numbers using the sorting network where our neural network is trained without exact house numbers.\n\nMoreover, instead of using costly supervision, our networks allow us to sort high-dimensional instances in a sequence where information on comparisons between instances is only given. This scenario often occurs when we cannot obtain complete supervision. For example, if we would sort four-digit MNIST images, ordinary neural networks are designed to solve a classification task by predicting class probabilities each of which indicates one of all labels from \"0000\" to \"9999\". If some labels are missing and further we do not know the exact number of labels, they might fail in predicting unseen data corresponding to those labels. Unlike these methods, it is possible to solve sorting problems with our networks in such a scenario.\n\nAs discussed in Section 7, the hard permutation matrices produced by our methods encourage us to swap instances exactly, instead of the linear combination of instances. This characteristic is required when we are given the final outcomes of sorting as supervision. This scenario is tested by the experiments presented in Section 5.2. In these experiments, we can consider that original images are provided as supervision. Building on the advantages of neural network-based sorting networks, we extend their practical significance into the cases that need hard permutation matrices.\n\nFurthermore, this study can be applied in diverse deep learning tasks for learning to sort generic high-dimensional data, such as information retrieval (Cao et al., 2007; Liu, 2009) and top-$k$ classification (Berrada et al., 2018)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1020/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700068140343,
                "cdate": 1700068140343,
                "tmdate": 1700068140343,
                "mdate": 1700068140343,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9sbvtuuaeW",
                "forum": "RLSWbk9kPw",
                "replyto": "EloHK7HyzU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1020/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1020/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer LP9f (2/n)"
                    },
                    "comment": {
                        "value": "> In terms of the results, it can be observed that as the sequence length increases or the number of segments grows, the performance decreases significantly.\n\n> Why does the performance drop significantly when the quantity increases?\n\nSince our neural networks are trained from scratch using only supervision on permutation matrices, the cases with longer sequences are hard to train. In addition, this performance drop becomes more significant because the impact of the performance drop is magnified as a sequence length increases. In particular, as presented in the results of $\\textrm{acc}\\_{\\textrm{ew}}$, an individual score $s_i$ is relatively accurate. If a sequence length increases, $\\textrm{acc}\\_{\\textrm{em}}$ exponentially drops. For example, if $\\textrm{acc}\\_{\\textrm{ew}} = x$, $\\textrm{acc}\\_{\\textrm{em}}$ is approximately $x^n$ where $n$ is a sequence length.\n\nIt is worth noting that the previous studies struggled to show better performance in the same benchmarks and our methods outperform those baseline methods; please see **Tables 1, 2, and 3**."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1020/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700068257891,
                "cdate": 1700068257891,
                "tmdate": 1700068257891,
                "mdate": 1700068257891,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZiDnOZXWgi",
                "forum": "RLSWbk9kPw",
                "replyto": "EloHK7HyzU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1020/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1020/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank you for your constructive feedback again.\n\nWe have answered your concerns in the rebuttal.\n\nPlease take a look into our response and let us know if you still have any concerns."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1020/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700489709932,
                "cdate": 1700489709932,
                "tmdate": 1700489709932,
                "mdate": 1700489709932,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dfZAGLHxBA",
                "forum": "RLSWbk9kPw",
                "replyto": "EloHK7HyzU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1020/Reviewer_LP9f"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1020/Reviewer_LP9f"
                ],
                "content": {
                    "title": {
                        "value": "Feedback to author's response"
                    },
                    "comment": {
                        "value": "Thanks for the reply. My problem was solved. I hope to present these in more detail in the final version. I will raise my score."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1020/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700716461152,
                "cdate": 1700716461152,
                "tmdate": 1700716461152,
                "mdate": 1700716461152,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]