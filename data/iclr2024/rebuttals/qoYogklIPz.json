[
    {
        "title": "Demystifying Embedding Spaces using Large Language Models"
    },
    {
        "review": {
            "id": "M5WE4JgmGW",
            "forum": "qoYogklIPz",
            "replyto": "qoYogklIPz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2991/Reviewer_viWv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2991/Reviewer_viWv"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a framework (ELM) for interpreting the vectors in domain-embedding spaces using text-only LLMs. The main idea is to project vector representations and textual tokens to a conjoint vector space. This is achieved by training adapter layers that serve as an interface to the language model. This adapter layers, together with the pre-trained model, create a new, extended model that maps both tokens and embeddings (from a arbitrary latent metric space) into a common space. The authors put this framework to the test by attempting to interpret two different semantic embeddings that they generate from the MovieLens dataset: One that models the users by their ratings of movies and one  that models the movies themselves based on their descriptions. The input data for the ELM framework then consists, for example, of text sequences in which the title of a movie has been replaced by the correspondingly generated semantic embedding. In order to verify the results of the framework, the authors rely on the expertise of test persons on the one  hand, and on the other hand, they also design two metrics to test for consistency."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I think the strengths of the article lie in the idea of extending a model to accept sequential tokens and embeddings as input data. I agree with the authors that this approach has the potential to provide a tool to study corresponding vector spaces spanned by semantic representations. In addition, this offers interesting possibilities for applications such like recommendation systems. Based on the elaborate and convincing experiments, the authors can very well prove the merits of their approach. Another positive feature is the detailed appendix."
                },
                "weaknesses": {
                    "value": "Unfortunately, in my opinion, the actual core of the article is somewhat overshadowed by the detailed experiments. I would also have liked to see a figure (in addition to Figure 2) that shows the structure of the framework at a glance. In my opinion, the surface planes shown in the 3D graphs of Figure 1 do not contribute much to the clarity either, since they only exemplify a distance of points in the vector space. The authors take effords to show the benefits of their framework with two examples (movie ratings - behavioral embeddings and movie descriptions - semantic embeddings), but perhaps one of the vector spaces studied could have come from a completely different domain, perhaps derived from a different dataset. But this probably falls into future work. On the whole, the paper is very well structured and easy to read, but the section on related work seems a bit out of place before the section conclusions. Perhaps it could have been placed further forward in the text."
                },
                "questions": {
                    "value": "\u2022\tHas the framework been tested on other embeddings that may contain less semantic information than the domain embeddings used here (similar to the example in Appendix C)?\n\u2022\tHow do the authors assess the potential usefulness of their approach for the interpretation of other dense vector representations, for example graph or image embeddings?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2991/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698744371657,
            "cdate": 1698744371657,
            "tmdate": 1699636243925,
            "mdate": 1699636243925,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rlcpMyQx6J",
                "forum": "qoYogklIPz",
                "replyto": "M5WE4JgmGW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2991/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2991/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "Thank you for your positive review and feedback. We appreciate the fact that you recognized the potential of ELM as a tool for studying embedding spaces. Below are some brief responses to the points you raise.\n\nWe added a figure showing the structure of the framework (see the updated version of Figure 2), which we hope will clarify our approach\u2014thanks for the suggestion.\n\nAlso, thank you for your comments regarding the embeddings. Notice that our behavioral embeddings are trained solely on user/movie ID and rating data (with no semantic information). Our user profiles task successfully demonstrates the ability of extracting semantic information from behavioral embeddings. Hope that addresses the concern regarding generalizability on embedding information from one domain to another. To emphasize this further, we are also adding experiments on Amazon public data, where the embeddings similarly come from a different source. We hope this will answer your concern.  \n\nThank you again for your positive review and feedback. Please let us know if there are any more concerns we can address."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2991/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699947056675,
                "cdate": 1699947056675,
                "tmdate": 1699947056675,
                "mdate": 1699947056675,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AIuOZzYByI",
            "forum": "qoYogklIPz",
            "replyto": "qoYogklIPz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2991/Reviewer_bEK6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2991/Reviewer_bEK6"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents the Embedding Language Model (ELM), a novel language model framework for interpreting domain-embedding spaces. By inserting trainable adapters into existing large language models (LLMs), ELM can accept domain embedding vectors as parts of its textual input sequence to allow the interpretation of continuous domain embeddings using natural language. Abundant experiments demonstrate ELM's proficiency in understanding, navigating, and manipulating complex embedding representations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Interpreting abstract embeddings into human-understandable natural language descriptions is intuitively appealing.\n- The proposed approach is simple and effective by reasonably leveraging the power of large language models (LLMs).\n- The authors have comprehensively assessed the quality of ELM's outputs using a variety of evaluation techniques, including qualitative human evaluations and specific consistency metrics."
                },
                "weaknesses": {
                    "value": "Honestly, one of my greatest concerns is the practicality of the proposed framework, given that training an ELM requires manually constructing a batch of tasks, which need to be diverse enough to extract rich semantic information from the target domain $\\mathcal{W}$ to support the interpretation of the embeddings. Admittedly, the experimental part of the paper validates ELM's proficiency in interpreting two forms of embeddings on a movie-related dataset. However, I am not sure if ELM performs equally well in other more specialized domains, considering that the original training corpus of LLMs is likely to contain rich semantic information relevant to movies. In addition, constructing diverse task prompts may be tedious for a realistic user, so are the prompts presented in Appendix D basically applicable to other domains? After all, the most straightforward need for a user is to know what an embedding represents in general.\n\n- Given the prevalence of adapter tuning [1], there is nothing new to me in the method.\n\n1. Parameter-efficient transfer learning for NLP, Neil Houlsby et al."
                },
                "questions": {
                    "value": "- What is the detailed form of the loss function $\\mathcal{L}\\_{\\theta}$ used in the experiment? Did the authors use reinforcement learning from AI feedback to optimize $\\mathcal{M}_{ELM}$?\n- Since the output of instances in the training data is generated by LLMs, can ELM reasonably interpret embeddings in specialized domains outside the scope of LLMs?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethical issues found."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2991/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2991/Reviewer_bEK6",
                        "ICLR.cc/2024/Conference/Submission2991/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2991/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698756910795,
            "cdate": 1698756910795,
            "tmdate": 1700627421657,
            "mdate": 1700627421657,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hvvsmHynS3",
                "forum": "qoYogklIPz",
                "replyto": "AIuOZzYByI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2991/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2991/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "Thank you for your helpful comments. We are happy you found the problem of interpreting embeddings using language appealing, and our approach simple and effective. \n\nWe understand your concern w.r.t. ELM, which requires the availability of tasks mapping the domain embeddings to text. While some problems would indeed require human intervention for the creation of such tasks (or possibly using the power of existing LLMs, as we\u2019ve shown in the paper), many tasks readily associate enormous amounts of text with specific entities. Users and items in recommender systems, as an example, are abstract entities which are often described using behavioral embeddings, which are hard to interpret. Nevertheless, users also engage with text and items by commenting, reviewing, and conversing with other users about items (e.g., Youtube comments, Amazon reviews, MovieLens tags, etc.). \n\nTo address your concern, we are adding experiments for the large public Amazon dataset [1], which consists of 9.35M items with textual descriptions, 20.9M users, 233.1M reviews, and 82.83M ratings. We are training ELM on 5-7 tasks over this dataset (starting with item descriptions, positive reviews, negative reviews, neutral reviews, convincing item summaries, and user profiles), and will update our paper with these new results by the end of the week, including human evaluation. We hope this will address most of your concerns.\n\nRegarding your questions:\n1. We used supervised learning (i.e., next token cross-entropy loss) to train all our models in the paper. In Appendix G, we discuss training ELM using reinforcement learning from AI feedback (RLAIF). Specifically, we attempt to train ELM with an additional reward to encourage semantic and behavioral consistency. Our results, however, did not show significant improvement compared to the regular supervised learning baseline. We leave further experimentation using different RLAIF approaches for future work. \n2. To address your second question, we are adding new results for the Amazon product dataset (as discussed above).\n\nReference for the Amazon Dataset: \\\n[1] McAuley, Julian, Christopher Targett, Qinfeng Shi, and Anton Van Den Hengel. \"Image-based recommendations on styles and substitutes.\" In Proceedings of the 38th international ACM SIGIR conference on research and development in information retrieval, pp. 43-52. 2015."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2991/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699946997660,
                "cdate": 1699946997660,
                "tmdate": 1699946997660,
                "mdate": 1699946997660,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0Ud04VPGsB",
                "forum": "qoYogklIPz",
                "replyto": "fgROpkqdIJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2991/Reviewer_bEK6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2991/Reviewer_bEK6"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "I appreciate the authors' comprehensive response, and I think the added experiments on the large public Amazon dataset and further human evaluation will improve the quality of this paper. My concerns have been largely addressed, so I raised my score from 5 to 6."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2991/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700627353460,
                "cdate": 1700627353460,
                "tmdate": 1700627353460,
                "mdate": 1700627353460,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "poYmJZxiqK",
            "forum": "qoYogklIPz",
            "replyto": "qoYogklIPz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2991/Reviewer_pa1w"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2991/Reviewer_pa1w"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents ELM, a framework that uses LLM to interpret embeddings. By training an adapter, ELM maps the domain embedding space to the LLM token embedding space, allowing users to use natural language to query and understand the domain embedding space. An evaluation of 24 original tasks with a movie rating dataset shows that ELM provides high semantic and behavioral consistency. Finally, the paper shows promising results of using ELM to query hypothetical embeddings and generalize concept activation vectors."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- [S1] The proposed method ELM is novel and intuitive. The two-stage training (adapter first, full model next) is interesting and makes sense.\n- [S2] I appreciate including human evaluation in section 3. The analysis and discussion on hypothetical embedding vectors and concept activation vectors (section 4) are very interesting.\n- [S3] The paper overall is well-written and easy to follow."
                },
                "weaknesses": {
                    "value": "## Major weaknesses\n\n- [W1] The evaluation can be improved. (1) The training data are all synthesized from an LLM. (2) The 24 tasks are all original. (3) ELM is only evaluated on the MovieLens. (4) There is no baseline method. It is unclear how well ELM will perform on real data and tasks compared to other methods.\n\n## Minor weaknesses\n\n- [M1] It would be great to explain $E_A$ in Figure 2 (the definition is on page 4).\n- [M2] The 24 tasks are essential to understand this paper. I recommend adding a table to provide a high-level description of these 24 tasks in the main paper.\n- [M3] The ELM method is quite intuitive, but the writing in section 2 is overly-complex. It would be helpful to have a small glossary to explain all notations.\n- [M4] It is hard to make sense of Table 1. Are these numbers good?"
                },
                "questions": {
                    "value": "- [Q1] Have you tried ELM on other datasets and tasks other than movie reviews?\n- [Q2] Figure 1 is a bit hard to understand. For example, how do people generate the embedding for the animated version of Forrest Gump? Interpolating the embedding of Forrest Gump with an animated movie? Is the black text output from an LLM? Who writes the blue text (e.g., users, researchers)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2991/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698767811978,
            "cdate": 1698767811978,
            "tmdate": 1699636243772,
            "mdate": 1699636243772,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Me4iPbbalT",
                "forum": "qoYogklIPz",
                "replyto": "poYmJZxiqK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2991/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2991/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "Thank you for your positive feedback and fruitful comments. We appreciate that you found our method novel and intuitive, and the discussion in the paper interesting. Please find our response below.\n\nTo address your main concern, we are adding new tasks on real data from the public Amazon product dataset [1], which consists of 9.35M items with textual descriptions, 20.9M users, 233.1M reviews, and 82.83M ratings. We are training tasks involving item descriptions, positive reviews, negative reviews, neutral reviews, convincing item summaries, and user profiles. We will update the paper with these results by the end of the week, and also include human rater evaluations to improve the evaluation of our paper. We hope this will address your main concern.\n\nThank you also for your other suggestions, based on which we\u2019ve updated the paper as follows (changes marked in red in the paper): \\\n[M1] We\u2019ve added an explanation for $E_A$ in Figure 2. \\\n[M2] We\u2019ve added a short description of each task to the paper (see Table 3) \\\n[M3] We\u2019ve added a small glossary to explain our notation, which we hope will improve clarity. (see Table 1) \\\n[M4] For human rater evaluations, the numbers correspond to ratings between \u201cagree\u201d and \u201cstrongly agree\u201d. For semantic and behavioral consistency metrics, we show in Figure 3 attempts of using GPT4 and PaLM2 text-only baselines, with significantly lower scores.\n\n\nRegarding your questions: \\\n[Q1] As mentioned above, we will add experiments using the public Amazon dataset. \\\n[Q2] To generate a new movie (item) embedding, we found two potential options: \\\nOption 1: One could create an item embedding of a hypothetical movie for which the specific attribute is most salient (whereas the other attributes are average). This movie does not necessarily need to exist. \\\nOption 2: Moving in the direction of the \u201cconcept activation vector\u201d with respect to a specific movie soft attribute (e.g., violent, funny), as we demonstrate with user profiles.\n\nIn the paper we demonstrate Option 1 for movie tasks and Option 2 for the user profile task.\n\nFinally, regarding the text in Figure 1:\nThe black text is the generated output of ELM.\nThe blue and red text are the prompts used as input to ELM, where the red part is an embedding vector not text.\nFor instance if Forrest Gump\u2019s movie embedding is [1.0, 1.0] and the direction of funny is [0, 1], then the prompt could be of the following sort: \u201cList five positive characteristics of the movie [1.0, 1.1]\u201d, where here, the vector [1.0, 1.1] is inputted through the adapters of ELM, and not as text.\nWe hope this clarifies your question.\n\nReference for the Amazon Dataset: \\\n[1] McAuley, Julian, Christopher Targett, Qinfeng Shi, and Anton Van Den Hengel. \"Image-based recommendations on styles and substitutes.\" In Proceedings of the 38th international ACM SIGIR conference on research and development in information retrieval, pp. 43-52. 2015."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2991/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699946882291,
                "cdate": 1699946882291,
                "tmdate": 1699946882291,
                "mdate": 1699946882291,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BO0oecqONK",
                "forum": "qoYogklIPz",
                "replyto": "Me4iPbbalT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2991/Reviewer_pa1w"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2991/Reviewer_pa1w"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response!"
                    },
                    "comment": {
                        "value": "Thank you for the quick and thorough response! All my concerns have been addressed. Some final minor \"style\" suggestions:\n\n1. In the glossary table, \"pretrained LLM\" $\\rightarrow$ \"Pretrained LLM\"\n2. It would be great if you can incorporate your response to [M4] to the table caption and highlight important number in the table. The table is currently a \"block of numbers\" \u2014it is hard to know what is the take-away message.\n3. Regarding Q2, it would be great if you can explain the color encodings (e.g., through annotations in the figure or explaining it in the caption).\n\nGood work!"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2991/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699994789599,
                "cdate": 1699994789599,
                "tmdate": 1699994789599,
                "mdate": 1699994789599,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1lTciRI4VA",
                "forum": "qoYogklIPz",
                "replyto": "277Vsh8ejg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2991/Reviewer_pa1w"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2991/Reviewer_pa1w"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the update!"
                    },
                    "comment": {
                        "value": "Thank you for the update! The new experiment results look good. Two minor comments:\n\n1. It's a good practice to report the compensation provided to the study participants. One way is to report an approximated hourly wage by multiplying the task duration by the task reward. If the rate is found to be below the federal minimum wage, I recommend considering adding bonus reward to the crowd workers.\n2. Typo: there is a \"Google\" after \"Rohan Anil\" in the bibliography on page 10."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2991/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700281241401,
                "cdate": 1700281241401,
                "tmdate": 1700281241401,
                "mdate": 1700281241401,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3yRueUQdOf",
                "forum": "qoYogklIPz",
                "replyto": "STOQduJoSQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2991/Reviewer_pa1w"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2991/Reviewer_pa1w"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the update!"
                    },
                    "comment": {
                        "value": "Got it. Yes, it's important to mention how the raters were recruited (I thought they were Amazon MTurk workers) and if they had been compensated fairly in the paper. I have no further questions. Happy thanksgiving!"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2991/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700603640683,
                "cdate": 1700603640683,
                "tmdate": 1700603640683,
                "mdate": 1700603640683,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "V7AuandHis",
            "forum": "qoYogklIPz",
            "replyto": "qoYogklIPz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2991/Reviewer_BRjZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2991/Reviewer_BRjZ"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a paradigm for demystifying the compressed embedding representations of deep models (such as MF trained on the MovieLens dataset in the paper) with Large Language Models (ELM). ELM first generates training data by prompting PaML 2-L with the movie title and task-specific information. The training procedure is composed of two stages: (1) training an adapter to project the domain embedding; and (2) full-training the full model including the language model and the adapter. Then the authors construct different tasks on the MovieLens dataset to test the performance of demystifying domain embeddings."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. It is sound to demystify domain embedding with Large Language Models.\n2. The authors introduce their approach step-by-step in Sec 2 & 3, which is clear. \n3. The performance is evaluated by human raters."
                },
                "weaknesses": {
                    "value": "1. The training data is generated by PaML 2-L, and ELM uses PaLM 2-XS to interpret the domain embeddings. Their similar architectures and training procedures may make the contribution limited.\n2. The authors argue that ELM is a general framework for different tasks, but the experiments only involve one model (MF) on one dataset (MovieLens). Therefore, the generalization of ELM to other models and datasets is not guaranteed.\n3. Some related works are overlooked. Existing works [1][2] have shown that tuning the adapter can let LLM demystify and reason over the embeddings of images. The authors should provide more explanations and experiments for their differences. \n\nreference:\n* [1] Zhu, Deyao, et al. \"Minigpt-4: Enhancing vision-language understanding with advanced large language models.\" arXiv:2304.10592 (2023).\n* [2] Koh, Jing Yu, et al. \"Generating images with multimodal language models.\" arXiv: 2305.17216."
                },
                "questions": {
                    "value": "1. Can ELM perform well with training data generated by other LLMs such as ChatGPT?\n2. Can the training data be real other than generated? Some datasets contain descriptions and revives (such as Amazon datasets). Why not use these kinds of datasets?\n3. Can ELM perform well with other models and other datasets."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2991/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699318051494,
            "cdate": 1699318051494,
            "tmdate": 1699636243705,
            "mdate": 1699636243705,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CZiwbtrF4M",
                "forum": "qoYogklIPz",
                "replyto": "V7AuandHis",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2991/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2991/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "Thank you for your review and your helpful comments. To address your concerns, as you\u2019ve proposed, we are adding new experiments for ELM using the public Amazon product dataset [1], which consists of 9.35M items with textual descriptions, 20.9M users, 233.1M reviews, and 82.83M ratings. Indeed, this dataset contains real data of item descriptions, ratings, and reviews, for which we train ELM on 5-7 tasks (starting with item descriptions, positive reviews, negative reviews, neutral reviews, convincing item summaries, and user profiles). We will update the paper with results for these tasks including human rater evaluations by the end of the week, to leave enough time for the discussion period. We hope this will address your major concerns.\n\nWe also note that, while our experiment procedures utilize both PaLM2 models for data generation and embedding interpretation, our method does not require the use of the same model architecture for these procedures. We hope our new experiments on the Amazon dataset will address your concern.\n\nReference for the Amazon Dataset: \\\n[1] McAuley, Julian, Christopher Targett, Qinfeng Shi, and Anton Van Den Hengel. \"Image-based recommendations on styles and substitutes.\" In Proceedings of the 38th international ACM SIGIR conference on research and development in information retrieval, pp. 43-52. 2015."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2991/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699946681593,
                "cdate": 1699946681593,
                "tmdate": 1699946701762,
                "mdate": 1699946701762,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]