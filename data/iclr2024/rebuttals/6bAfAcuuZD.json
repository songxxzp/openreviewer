[
    {
        "title": "Emergence of Surprise and Predictive Signals from Local Contrastive Learning"
    },
    {
        "review": {
            "id": "BbOUGYsQ0Q",
            "forum": "6bAfAcuuZD",
            "replyto": "6bAfAcuuZD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8109/Reviewer_D7tC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8109/Reviewer_D7tC"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a slight modification of the forward-forward algorithm, in which activity is suppressed for positive instances and increased for negative instances. While this is a slight modification, the primary contribution of this research is the demonstration of several emergent dynamics of the trained network, which are then related to the concept of predictive coding and may replicate several experimental neuroscience findings."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is overall well organized and concise. Taking a well-publicized learning rule and showing a slight modification to relate to neuroscience theories should go a long way to keeping an open dialogue between ML and theoretical neuroscience. Additionally, nearly all of the questions I thought of asking, such as order and direction of surprise and expected signals, were addressed over the course of section 3, suggesting that the authors have put forethought into these analyses.Beyond a clarification described below, I have only two minor concerns with this paper. First, I wonder how interest it may be to ICLR attendees outside of the bio-inspired crowd. Some minimal discussion and possibly an additional abstract sentence to explain how predictive coding may be of broader ML interest could help alleviate this. Secondly, the loss function requires an explicit positive/negative cue (eta). Historically this sort of global signal has been contentious in computational models. It may be worth emphasizing that this error signal is singular, and how this may be much more easily achieved than a global vector valued error/target signal."
                },
                "weaknesses": {
                    "value": "Beyond clarifications described below, I have only two minor concerns with this paper. First, I wonder how interest it may be to ICLR attendees outside of the bio-inspired crowd. Some minimal discussion and possibly an additional abstract sentence to explain how predictive coding may be of broader ML interest could help alleviate this. Secondly, the loss function requires an explicit positive/negative cue (eta). Historically this sort of global signal has been contentious in computational models. It may be worth emphasizing that this error signal is singular, and how this may be much more easily achieved than a global vector valued error/target signal."
                },
                "questions": {
                    "value": "1.\tIn section 3.0 the optimizer is defined as RMSProp operating on the loss defined in equation 1. My understanding however is that standard RMSProp implementations calculate chained gradients and applied as a global optimization (though possibly with sub-losses defined on each layer), leading to weight transport and making the model non biologically plausible. If a modification, such as gradient stopping, was applied to prevent this global optimization then it should be described.\n2.\tSection 2, just before figure 2, states that only intralayer weights are trained, suggesting that recurrent (W)eights are, while biases (theta), (F)orward and (B)ackward weights are not. However, the surrounding text obfuscates this by speaking of the weights as \u201clocal properties\u201d. If this is the case, that is a critical aspect of the model, and should highlighted and compared to a version in which the remaining parameters are trained. It would be beneficial to highlight in a single location which parameters do/don\u2019t update, as well as defining abbreviations.\n3.\tIf these questions are resolved/clarified, I would be happily convinced to increase my soundness and overall score."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8109/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8109/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8109/Reviewer_D7tC"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8109/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698708679724,
            "cdate": 1698708679724,
            "tmdate": 1700841157315,
            "mdate": 1700841157315,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gfDQuaiBQ3",
                "forum": "6bAfAcuuZD",
                "replyto": "BbOUGYsQ0Q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8109/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8109/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Reply"
                    },
                    "comment": {
                        "value": "Thank you for your constructive feedback on our paper. We are pleased that you found our paper well-organized and our approach relevant to the dialogue between machine learning and theoretical neuroscience. We have addressed your concerns and questions as follows:\n\n**Re: Supervisory signal limiting biological plausibility**\n\nThe critique concerning the supervised nature of the contrastive learning in the model raises an important point about biological plausibility. While the model employs supervised learning, it is important to note that this approach is in the context of a field where the boundary between supervised and unsupervised learning mechanisms, and their roles in biological systems, is still an active area of research.\n\nThe use of the singular error signal, \u03b7, in the loss function of the model can be seen as a reflection of a simplified global learning signal, which could, in some respects, be analogous to the role of neurotransmitters in biological neural networks. This analogy suggests that just as \u03b7 provides a generalized feedback mechanism to the entire model, neurotransmitters in the brain can act over a wide area (via volume transmission) to modulate the activity of many neurons, rather than just those directly connected by synapses. This is a way to understand how global signals might occur in response to learning or other processes.\n\nWe appreciate this feedback and have included the above details relating to the supervisory signal in our manuscript.\n\n**Re: Bioplausibility of RMSProp optimizer**\n\nAlthough the RMSProp optimizer utilizes moving averages of gradients, we don\u2019t believe that observation in itself renders it biologically implausible. For example, it is biologically plausible for a neuron to use local information to adjust its learning. Classic rules like BCM theory leverage such history dependence to reduce the variance of the updates. Thirdly, we reset the parameter gradients each time a layer trains, ensuring the independence and localization of learning at each layer. Finally, all presynaptic input tensors are detached, acting as a stop grad, which is critical in preventing non-local weight updates. These modifications collectively serve to lend our implementation of Forward Forward as much as possible with local learning. We have updated the text to explain each of these points.\n\n**Re: Question regarding which weights are trained**\n\nWe apologize for the confusion caused by our description. We mean that each layer is its own learning agent. When training is performed on a specific layer, only the weights connected to this layer are updated (i.e. forwards, backwards, lateral). We have updated this description to eliminate confusion.\n\n**Miscellaneous**\n\nWe have also added some information to our abstract about the broader applicability of predictivity coding, and taken additional care in mentioning that this global error signal is simple and singular rather than vectored and complex."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8109/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700705993213,
                "cdate": 1700705993213,
                "tmdate": 1700705993213,
                "mdate": 1700705993213,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "C5bZIvdl94",
            "forum": "6bAfAcuuZD",
            "replyto": "6bAfAcuuZD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8109/Reviewer_fE2s"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8109/Reviewer_fE2s"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a novel and biologically plausible mechanism for understanding cortical processing, focusing on predictive aspects. It introduces a model based on the Forward-Forward approach, a form of contrastive learning, and makes the following three contributions.\n\n\n1) Hierarchical and Temporal Predictive Properties: The model reproduces some hierarchical and temporal aspects of predictive computations. It generates information flows that lead to surprise and cancellation signals, demonstrating its ability to capture, to some degree, the spatiotemporal predictive nature of cortical processing.\n\n2) Mechanistic Understanding of Information Flow: The paper offers insights into the emergence of information flows by tracing their origins to the ability of the neural circuit to implement spatiotemporal cancellation across different layers of activity. This understanding could explain how the brain processes and predicts sensory information.\n\n\n3) Biological Plausibility: The model's contrastive learning rule is shown to be equivalent to a unique form of three-factor Hebbian plasticity, which has strong connections to predictive coding. This highlights the biological plausibility of the proposed model. The training process involves positive and negative datasets, where surprise modulates layer activity based on whether the label and input match. The surprise calculation is defined in terms of individual layer activations. Importantly, training only involves forward passes, making the Forward-Forward algorithm biologically plausible and avoiding the non-locality of weight information inherent in backpropagation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is well written. \n\nIt provides substantial novelty in introducing a training procedure that mimics signal processing in biological networks, distinguishing between the presentation and processing phases. Surprise signals arise when there is a mismatch between sensory inputs and top-down signals (label information), similar to how cortical processing operates. \n\nThe authors pay significant attention to the interpretation of the learning and how this could be implemented in the brain. Obviously, it is to some degree biologically plausible, and the existing literature that looks into cortical microcircuits for predictive coding could be a good thing to add to discuss the possible implementation of the algorithm."
                },
                "weaknesses": {
                    "value": "Although the paper offers a theoretically sound model, it lacks empirical validation through experiments or real-world data. \n\nIncluding practical applications or experiments to support the model's claims would enhance its credibility and applicability in real-world scenarios."
                },
                "questions": {
                    "value": "Could you elaborate more on the algorithm, as in presenting some pseudo-code as well as a description of the neural architecture used that can be more easily connected to the standard literature of artificial neural networks?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8109/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698789750561,
            "cdate": 1698789750561,
            "tmdate": 1699637004888,
            "mdate": 1699637004888,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yHBYaFtuBp",
                "forum": "6bAfAcuuZD",
                "replyto": "C5bZIvdl94",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8109/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8109/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Reply"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful and detailed review of our paper. We appreciate your positive feedback on the novelty and biological plausibility of our model, as well as your suggestions for improvement.\n\nWe acknowledge your concern regarding the lack of empirical validation through experiments or real-world data. To address this, we have plans to extract from existing experimental data such as that done on the change detection task in mice[1]. This will not only provide empirical support for our model's claims but also demonstrate its applicability in real-world scenarios.\n\nIn response to your question about a more detailed explanation of our algorithm, we have enhanced and consolidated the model architecture descriptions which were previously spread over multiple sections. Additionally, we have open sourced the code of our model, which is intended to aid in transparency regarding our methods.\n\n[1]Visual Behavior Neuropixels - brain-map.org. Retrived on 11/20/2023 from https://portal.brain-map.org/explore/circuits/visual-behavior-neuropixels."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8109/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700705649060,
                "cdate": 1700705649060,
                "tmdate": 1700705649060,
                "mdate": 1700705649060,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "En9Lnmka61",
            "forum": "6bAfAcuuZD",
            "replyto": "6bAfAcuuZD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8109/Reviewer_3133"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8109/Reviewer_3133"
            ],
            "content": {
                "summary": {
                    "value": "Backpropagation in deep networks has been widely acknowledged to suffer from a lack of biological plausibility.  Recently, a novel framework has been proposed to leverage contrastive learning on matching and non-matching datasets to learn only through forward passes through the network, avoiding several of the least biological features of backpropagation.  The authors utilize this approach to model neural correlates of surprise within a predictive learning framework, finding network dynamics that are broadly consistent with neuroscientific findings: stimuli that match \u201cpredicted\u201d labels result in little activity, while those that do not result in large deviations from baseline activity.  They further analyze the resultant spatiotemporal dynamics using PCA, characterizing both the dynamics and decodability of the network across space (layers) and time. Finally, the authors observe that the trained networks result in a local 3-factor Hebbian rule, and perform an analysis on the linearized form to provide additional intuitive insight into the observed prediction-cancellation process."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors provide a strong biological background and motivation for the importance of moving beyond backprop-based methods to better understand learning in cortex, and more specifically for predictive learning that their results qualitatively match"
                },
                "weaknesses": {
                    "value": "_Major_\n\n1. While I am unaware of other explicit work that matches the qualitative results the authors provide, the overall framework, including the revised loss function and connection to predictive coding (including the cancellation process demonstrated by the authors) were all explicitly anticipated in Hinton 2022.\n   -  Moreover, the usage by the authors of a supervised contrastive learning process somewhat lessens the local learning process that can strengthen the biological plausibility of the results.  In particular, even though the learning process does not rely on propagating error information from the top layer, the local layer loss functions still contain explicit information about whether the inputs match the labels ($\\eta(t)$). The novelty would be increased by adding a more biologically realistic unsupervised learning process to the network (e.g., SimCLR).\n2. The PCA section does not seem to add much of substance to the manuscript\n   - Different numbers of components are included in different analyses for reasons that are somewhat difficult to discern (see Questions)\n   - Fig. 4a (top) seems to essentially describe the periodic patterns for the positive data observed in Fig. 2b.  Similarly for the periodic aspects in Fig. 4a (bottom) and Fig. 4c.\n   - Fig. 4a (bottom) is used to illustrate the strong visual decodability of PCs 4-6 vs. 1-3 (top).  However, in Fig. 4d we observe that PCs 1-3 in fact allow for decodability seemingly well above chance (if the x-axis is at a decodability value of 0), and that this decodability does not substantially increase for layers 1 and 2 and perhaps even 3.\n   - Fig. 4c does not seem to lead to much more insight than Fig. 4a (bottom), together with the observation that each loop in 4a (bottom) starts and ends in the center, as described for 4c.  This would be even more so if the trajectories in 4a (bottom) were modulated in intensity/luminosity/transparency to correspond to the associated time steps.\n   - In Fig. 4e, the presentation phase ends before the associated decodability traces either stabilize or peak (see Questions)\n   - Similarly, it seems as though the negative dynamics may cycle quasi-periodically as well based on Fig. 2b if enough time steps were included\n   - Overall, it seems as though the full-network analysis approach as shown in Figs 2-3 comprise a better analytical approach, given the generally simple network dynamics\n3. The model and training regimen are not explained well, and spread over several sections\n   - The model description is terse in the Introduction, with further details added in Sec. 3.0 and 3.4.  However, the notation between the Introduction and Sec. 3.4 (and App. A) are inconsistent (x vs. r, $\\theta$ vs T).  Similarly for training details\n   - The readability would be improved by providing all of the model and training data in the Introduction, before any results are described\n\n*Minor*\n1. I could not find any reference to App. B in the main text\n2. The final Results section (3.4) observes that the local aspect of the learning process is Hebbian.  While biologically important, this seems to fall directly out of the FF framework itself and only deserving of a passing observation\n3. The linear dynamics results in App. A.1 allow for the input to approximately equal the label.  While such a representation could be learned, I don\u2019t see how such a direct approximation is justified.\n4. \u201cSurprise\u201d seems to be defined in two ways (\u201cactivity as surprise,\u201d p. 2 top, then defined as components of the layer loss function and in terms of the activity on the bottom of p. 2 in Eq. 1)\n5. Some typos (e.g., \u201cat each time $x_{layer}(t\u2019)$\u201d p.2; \u201cdynamics dynamics\u201d in Supp. p. 2; \u201cthree-paired\u201d p. 5; lack of equal signs in Supp. p. 2, $t$ vs. $t\u2019$ in $z(t-1)$ on p. 7--also with and without arrow-vector decoration)\n   - Similarly, some mathematical notation not explicitly described, though easily inferred ($L(t)$, $I(t)$, $\\el(t)$)\n   - While \"PC\" is implicitly defined by reference to PCA in Fig. 4's caption, both PCA and PC should initially be explicitly referenced with a full name (e.g., \"Principle Component Analysis\") in the main text before using the abbreviation\n6. Fig. 3b would be more compelling if traces included those from several different network instantiations and included error shading\n7. Figs 4d-e would provide some additional intuition if they were to being at a decodability of 0 and include a dashed horizontal line corresponding to chance levels of decodability for comparison purposes"
                },
                "questions": {
                    "value": "1. Fig 4d \u2014 what are the \u201cTriplet of PCs\u201d indicated in the abscissa label?  In the caption, it instead indicates they simply are the PCs, while in the main text it\u2019s stated they are \u201csliding-window three-paired PCs.\u201d   Please clarify what exactly is being decoded and provide consistent labels/descriptions in the manuscript\n2. Why are the different principal components chosen in the analyses?  How many are included seems to confusingly vary from one analysis to the next with little or no explanation.  \n   - E.g., components 1-3 and then 4-6 are used in the primary analysis in Fig. 4a-c. Then the first 20 components are used for Fig 4d\u2014while the main text indicates that \u201cmost of the variance is captured within the first 20 PCs,\u201d it is not indicated either how much of the variance is captured, or if this is the reason for going to 20 but no further.  Graphically, there is a clear drop-off at 20, perhaps to within-chance levels of decodability. Perhaps this is the reason for choosing 20?  \n   - Finally, for Fig. 4e, 50 PCs are chosen.  Why are 50 chosen? Based off of Fig. 4d, it seems as though a more principled analysis would only include the first 19 or 20 or so components\u2014ie, the ones that allow for above-chance levels of decodability. \n3. How would the PCs in Fig. 4e continue to evolve if the presentation phase were prolonged?  In the figure, all PCs have an upward trajectory before the processing phase begins, making it difficult to discern how much of the continued rise during the processing phase is due to continued, prolonged and perhaps delayed dynamics arising during the presentation phase.  A more compelling analysis might be to allow the dynamics to either stabilize or peak before ending the presentation phase and beginning the processing phase.\n4. Do the authors use layer normalization as per Hinton 2022?  Or is there a reason to not be concerned about the corollaries mentioned there in the authors\u2019 model?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8109/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698818588370,
            "cdate": 1698818588370,
            "tmdate": 1699637004757,
            "mdate": 1699637004757,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "A4PEp8DnBr",
                "forum": "6bAfAcuuZD",
                "replyto": "En9Lnmka61",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8109/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8109/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Reply"
                    },
                    "comment": {
                        "value": "We are grateful for the thoroughness of your review and the opportunity to clarify and improve upon our work. We have addressed your comments and questions as follows:\n\n**Re: Forward-Forward framework novelty**\n\nWe appreciate the reference to Hinton 2022 and acknowledge the foundational work it established. While Hinton anticipated an inverted loss function, our work is aimed at implementing this anticipated network in an attempt to model cortical activity, and exploring the emergent dynamics of spatiotemporal cancellation across layers. The enhancements our work provides in interpretability applied to this architecture will enable more effective future comparisons, taking the FF one step closer to neural dynamics.\n\n**Re: Supervisory signal limiting biological plausibility**\n\nThe critique concerning the supervised nature of the contrastive learning in the model raises an important point about biological plausibility. While the model employs supervised learning, it is important to note that this approach is in the context of a field where the boundary between supervised and unsupervised learning mechanisms, and their roles in biological systems, is still an active area of research.\n\nThe use of the singular error signal, \u03b7, in the loss function of the model can be seen as a reflection of a simplified global learning signal, which could, in some respects, be analogous to the role of neurotransmitters in biological neural networks. This analogy suggests that just as \u03b7 provides a generalized feedback mechanism to the entire model, neurotransmitters in the brain can act over a wide area (via volume transmission) to modulate the activity of many neurons, rather than just those directly connected by synapses. This is a way to understand how global signals might occur in response to learning or other processes.\n\nWe appreciate this feedback and have included the above details relating to the supervisory signal in our manuscript.\n\n**Re: PCA Analysis Clarity**\n\nWe recognize the need for greater clarity in our PCA analysis and the rationale behind the number of components chosen for each figure. We have now revised these sections and figures to provide a clear explanation and the reasons for the chosen components. We have removed figures which do not serve to drive home our broader points.\n \n**Re: Description of the model**\n\nWe appreciate the reviewer for noting the lack of clarity around model architecture. We have revised the structure of the paper to formally present the model at the beginning, and give a more clear explanation.\n\n**Re: Linear analysis in appendix A.1**\n\nIndeed, the linear dynamics description is confusing. In the original inverted FF we are not forcing the input approximately equal to the label. Rather, with respect to a layer, the forward (input-driven) representation and the backward (label-driven) representation are approximately equal. However, in the linear dynamics where we apply strong simplifications to drive intuition. Specifically, we enforce a single-neuron-per-layer assumption. Here, this transforms both the label and the input into a one-dimensional quantity which is convenient to consider as approximately equal during positive data and not equal during negative data for this three-layer, single-neuron linear dynamics.  We have updated the text to make this more clear and to emphasize the contribution of this simplified argument.\n\n**Miscellaneous**\n\nWe thank the reviewer for their thoroughness in reviewing our article. The following contains our response to the minor issues and the answers to the questions raised.\n1. Inconsistencies in notation have been corrected to ensure a coherent presentation throughout the manuscript. The following minor changes have been addressed in the text:\n2. Added explicit references to Appendix B in the main text.\n3. Clarified the definition of \"surprise\" by referring only to the level of activity as surprise.\n4. Corrected typos and ensured consistent mathematical notation.\n5. Defined PCA and PC explicitly before abbreviation.\n\n\nRegarding the explicit questions:\n- Question 1: Figure 4d: we have clarified the labels and descriptions regarding the \"Triplet of PCs\" and ensured consistency across the manuscript.\n- Questions 2 & 3: PCA Component Selection: The selection rationale is now articulated, including why certain numbers of components were included.\n- Questions 4: We do use layer normalization in this architecture, just as Hinton did. We have updated the text to explain this, along with the section 3.4."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8109/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700705559900,
                "cdate": 1700705559900,
                "tmdate": 1700705559900,
                "mdate": 1700705559900,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ck94ivYABn",
            "forum": "6bAfAcuuZD",
            "replyto": "6bAfAcuuZD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8109/Reviewer_mvwV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8109/Reviewer_mvwV"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a variation of the Forward-Forward Algorithm, which is an unsupervised learning model with local learning rules, using contrastive samples (positive and negative samples). The original Forward-Forward has as its objective maximizing the squared activity of each layer (greedily) for positive samples while minimizing it for negative samples. The variation proposed here uses the opposite objective, minimizing the activity of positive samples, and maximizing of negative samples, which makes it related to surprise signals and predictive coding models. The paper focuses on the dynamics of the model latent unit activity over time, inspecting for each layer, training phase and sample type. In particular, there is a difference between the dynamics of negative and positive samples, with cancellation happening in the bottom-up dynamics for positive samples. They also show the dynamics of the principal components of the latent space for different layers. Lastly, they derive the update rule from the proposed loss and map it to biologically plausible three-factor Hebbian learning rules."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The two main contributions of the paper:\n- a potentially novel local surprise / predictive coding loss for contrastive samples in multi-layer networks;\n- analysis of the dynamics of the latent variables for the hierarchical model.\n\nRelating the loss function and activity of artificial deep networks to cortical activity is a promising approach for revealing how the cortex learns representations."
                },
                "weaknesses": {
                    "value": "The main weaknesses of the paper are:\n- lack of comparison with previous models, especially with predictive coding models;\n- unclear insights brought by the analysis of the dynamics, especially concerning previous literature, alternative models and experimental findings;\n- weak theoretical development.\n\nWhile the authors motivate their model as a variation of the Forward-Forward model, since it is a predictive coding-like model, the authors should also relate it to previous prediction coding models (Golkar et al., Neurips 2022), biologically plausible models (Illing et al., Neurips 2021) and related literature (Grill et al., Neurpis 2020). How is this model similar or different from previous models? Is this a novel procedure? Does it work better or worse than other models?\n\nWithout further motivation and contextualization for the model, the in-depth dynamical analysis is also difficult to assess. How would the dynamics of similar models behave? In any case, if the model is novel, it should be motivated and analyzed further, before diving into such specific properties. As the authors motivate the model as a potential model for cortical learning and activity, there should be more precise references to data and papers on what these dynamics look like in the brain (e.g. Rabinovich, Huerta, Laurent, 2008), and why they matter.\n\nThe theoretical argument of mapping the update to a three-factor learning rule is not convincing, as almost any update for a local loss in a neural network will have such a general form. The model should also be presented formally at the beginning of the paper. \n\nMinor:\n\nWhat's the difference between r_i and x_i? And threshold \\theta or T?\n\nThe accuracy for different number of layers should be included in the supplementary material, including the result for alternative models."
                },
                "questions": {
                    "value": "How is this model different from previous models? Is this a novel procedure? Does it work better or worse than other models?\n\nHow would the dynamics of similar models behave? \n\nWhat experimental evidence on cortical activity does this model relate to and why do they matter?\n\nWhy is this model particularly biologically plausible compared to related models?\n \nWhat's the difference between r_i and x_i? And threshold \\theta or T?\n\nWhat is the performance of the model for different architectures, and in comparison to other models?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8109/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8109/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8109/Reviewer_mvwV"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8109/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699292859685,
            "cdate": 1699292859685,
            "tmdate": 1699637004627,
            "mdate": 1699637004627,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "s1ZQKYvJge",
                "forum": "6bAfAcuuZD",
                "replyto": "ck94ivYABn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8109/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8109/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Reply"
                    },
                    "comment": {
                        "value": "We are grateful for the thoroughness of your review and the opportunity to clarify and improve upon our work. We have addressed your comments and questions as follows:\n\n**Re: Lack of comparison with previous models**\n\nWe acknowledge the importance of comparing our model to existing predictive coding and biologically plausible models. \n\nTo this end, we have added a new section in the paper that provides a direct comparison with a newly implemented predictive coding model. This comparison outlines the similarities and differences, substantiating the novelty of our approach and its performance relative to these established models.\n\n**Re: Description of the model**\n\nWe appreciate the reviewer for noting the lack of clarity around model architecture. We have revised the structure of the paper to formally present the model at the beginning, and give a more clear explanation.\n\n**Re: Theoretical argument for the three-factor learning rule.**\n\nWe thank the reviewer for their helpful critique on our three-factor learning rule. We have since clarified the significance of this three-factor claim within the manuscript by 1) adding a discussion of how three-factors both unifies and differentiates learning rules and 2) a new discussion targeting the value of this unique third factor to both the ML and neuroscience communities.\n\nThe key difference of the three-factor learning form of the inverted FF compared to previous learning rules (Gerstner 2018, Ku\u015bmierz 2017) is the global supervisory signal (-1)^\\eta term which tells the network whether it should maximize or minimize activations. In ML, this functional difference drives home the power of contrastive techniques and underscores recent (since our submission) innovations in merging contrastive-inspired methods with predictive coding using modern variants of BCM-like learning rules (Srinath-Halvagal, 2023). In neuroscience, this viewpoint suggests novel hypotheses about the varied role of volume transmitted neuromodulators such as dopamine (Gerstner, 2018), serotonin, acetylcholine, and norepinephrine in STDP learning."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8109/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700705220621,
                "cdate": 1700705220621,
                "tmdate": 1700705220621,
                "mdate": 1700705220621,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]