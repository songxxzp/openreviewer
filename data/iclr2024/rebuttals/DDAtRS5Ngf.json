[
    {
        "title": "Ceci n'est pas une pomme: Adversarial Illusions in Multi-Modal Embeddings"
    },
    {
        "review": {
            "id": "OQzEGTgtkT",
            "forum": "DDAtRS5Ngf",
            "replyto": "DDAtRS5Ngf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8508/Reviewer_8PvS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8508/Reviewer_8PvS"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to use adversarial perturbation to align the perturbed image with a given target text/sound."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This paper studies the vulnerability of multi-modal embedding.\n- The experiments also cover the acoustic information."
                },
                "weaknesses": {
                    "value": "- The literature is not well-surveyed, which makes the novelty of the attack not convincing. For example, BadEncoder (S&P'22) already implements such an idea to attack the CLIP model. There are also many following works which cite BadEncoder and study the vulnerability of multi-modal embeddings. How do the authors position the novelty of this work in these literatures?"
                },
                "questions": {
                    "value": "Please see the weakness part above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8508/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698478417022,
            "cdate": 1698478417022,
            "tmdate": 1699637063288,
            "mdate": 1699637063288,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AAPAXRO4YT",
                "forum": "DDAtRS5Ngf",
                "replyto": "OQzEGTgtkT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8508/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8508/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "BadEncoder is a training-time attack, it assumes that the attacker poisons the training of the model.  In the case of ImageBind or CLIP, it could be used only when the models were trained.  Ours is an evaluation-time attack on the original, unmodified and unpoisoned ImageBind.  It does not involve injecting anything into the training data and does not assume that the attacker has access to the model during training.  We will clarify this fundamental difference between the threat models of backdoor attacks (like BadEncoder) and adversarial inputs (like ours) in Background and Related Work."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8508/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700547929864,
                "cdate": 1700547929864,
                "tmdate": 1700547929864,
                "mdate": 1700547929864,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HEhJTfn4H8",
            "forum": "DDAtRS5Ngf",
            "replyto": "DDAtRS5Ngf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8508/Reviewer_xAWL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8508/Reviewer_xAWL"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a new attack called \"adversarial illusions\" against multi-modal embedding models like ImageBind. These models embed inputs like images, text, and audio into a shared embedding space. The attack involves making small perturbations to an input, like an image, so that its embedding becomes very close to a completely different, adversary-chosen input in another modality, like text. This fools downstream tasks relying on the embeddings, as they now interpret the perturbed input based on the adversary's target instead of the original semantics. Experiments demonstrate the effectiveness of the attack. The authors also discuss potential defenses like adversarial training and certifications. Overall, the work demonstrates serious vulnerabilities in cross-modal alignment of current multi-modal embeddings."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This proposes adversarial illusions that are attacks for multimodal embedding models. Given the growing popularity of multi-modal models, this line of research is important and interesting. The idea to adversarially associate a modality with another one unrelated to the semantic of the input is interesting."
                },
                "weaknesses": {
                    "value": "**Main comments:**\n\nWhile the paper is interesting and this first version is decent, there are a lot of missing experiments that could strengthen the paper and better motivate certain choices: \n\n- The paper proposes to use the I-FGSM attack to create their adversarial illusions:\n\t- Why use I-FGSM and not PGD, which is the best-known and better attack? \n\t- The authors could also experiment with DiffPGD, a newer PGD attack based on the diffusion model [1]. The adversarial perturbation is really visible in Figure 7, leveraging diffusion models could improve the attack. \n\t- Why using the $\\ell_\\infty$ norm? Have the authors experimented with other norms (e.g. $\\ell_2$)?\n\n- The authors seem to have experimented only with cross-modality? Can the authors create adversarial illusions on the same modality? \n- Is it possible to investigate the transferability of the attack?  e.g. against other multimodal foundation models? \n- The authors seem to be experimenting only with targeted attacks. Would it be possible to maximize the following loss: \n$$\n\\ell = 1 - \\cos\\left( \\theta^{m_1}(x^{m_1}+\\delta), \\theta^{m_1}(x^{m_1}) \\right)\n$$\n\n\n**Other comments:**\n- What is the difference between figures 1,3,4,5? It seems only the modality and examples are different. These figures take up a lot of space in the paper, I think these figures could be reduced or some of them could be put in the appendix to leave space for further experiments. \n- The authors devote a whole section to countermeasures, but it seems that they do not do any experiments. If the authors focus so much on countermeasures, some experiments should be done. \n\nXue et al. Diffusion-Based Adversarial Sample Generation for Improved Stealthiness and Controllability"
                },
                "questions": {
                    "value": "See weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8508/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8508/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8508/Reviewer_xAWL"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8508/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698594767382,
            "cdate": 1698594767382,
            "tmdate": 1700691905550,
            "mdate": 1700691905550,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TFNusBh1ba",
                "forum": "DDAtRS5Ngf",
                "replyto": "HEhJTfn4H8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8508/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8508/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank for the review!\n\n1. I-FGSM comments.\n    1. In general, PGD refers to I-FGSM with a random initialization in an $\\ell_\\infty$ ball around the source image. This method may perform marginally better for \u201cconventional\u201d adversarial examples, in our setting this benefit is greatly diminished since adversarial alignment must only \u2018beat\u2019 organic alignment. We will add an experiment confirming this observation in the **Appendix**.\n    2. As with the above, the $\\ell_\\infty$ norm (the most common in the literature) is sufficient for an attack that achieves all its objectives, i.e., strong adversarial alignment that fools all downstream tasks. We can add some experiments on other $\\ell_p$ norms into the **Appendix**.\n    3. The image in question uses the perturbation bound of $\\epsilon = \\frac{16}{256}$. As we reported elsewhere in the submission, our attack is effective with much lower bounds. We regenerated the image with a lower bound and it shows no visible noise. While DiffPGD is fascinating, it appears unnecessary in our case since our attack already achieves the desired adversarial alignment with tiny perturbations (DiffPGD can only improve image quality, not performance).\n2. Same-modality adversarial examples are strictly easier than cross-modal illusions, because, as discussed in Cross-Modal Illusions, the modality gap only arises between modalities. In particular, the unbounded case can be trivially solved within the same modality (i.e., $\\delta = \\mathbf{a}^{m_2} - x^{m_1}$) whereas a solution may not exist for the cross-modal case. We will provide some experiments in the Appendix.\n3. To expand the evaluation, we have shown that our attack is effective on AudioCLIP (with a 100% success rate for $\\epsilon$ larger than or equal to $\\frac{4}{256}$. In addition, we are conducting experiments on the transferability between embedding spaces. The experiments will be added to our **Evaluation** section.\n4. The targeted attacks we present in the paper are strictly harder than the suggested untargeted attacks. Rather than perturb images to induce *any* non-source class, we align with a *specific* non-source class. The suggested loss function reduces to finding any orthogonal vector in the unbounded case. Since we are in high-dimensional space, sampling randomly is likely to produce an orthogonal vector, and can be computed far more efficiently. In the bounded case, empirically, optimizing our suggested attack with *any* target until the alignment between the perturbed input and the source is less than the organic alignment is sufficient. This can be achieved with extremely small perturbation bounds.\n5. Thank you for the feedback. \u00a0 We will move some examples to the Appendix.\n6. **Add JPEG experiments** Our **Countermeasures** section has been updated to show how our attack evades the JPEG compression defense, a common method to mitigate adversarial perturbations. The JPEG-resistant attack still achieves a $75\\\\%$ success rate in the presence of the defense. It also evades the defense that checks consistency of embeddings of different input augmentations.\u00a0 As we note in the paper, there are limitations to most traditional defenses. Developing new defenses specifically for embeddings is an interesting research direction."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8508/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700547676060,
                "cdate": 1700547676060,
                "tmdate": 1700547676060,
                "mdate": 1700547676060,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hsLgOZMN1H",
                "forum": "DDAtRS5Ngf",
                "replyto": "TFNusBh1ba",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8508/Reviewer_xAWL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8508/Reviewer_xAWL"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you to the reviewers for their response. Most of my comments have been answered, I'll raise my score from 5 to 6."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8508/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700691883765,
                "cdate": 1700691883765,
                "tmdate": 1700691883765,
                "mdate": 1700691883765,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NPvb7avh9X",
            "forum": "DDAtRS5Ngf",
            "replyto": "DDAtRS5Ngf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8508/Reviewer_cUzT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8508/Reviewer_cUzT"
            ],
            "content": {
                "summary": {
                    "value": "This paper demonstrates that the semantic meaning of multi-modal embeddings can be easily manipulated using a simple white-box attack, which is termed adversarial illusion. An attacker only needs to describe in text what they want the input data to mean, and the multi-modal model ImageBind will interpret the attacked but seemingly normal images or audio as conveying the attacker's intended meaning, resulting in cross-modal illusions. This causes ImageBind to make mistakes on downstream tasks even without knowing what these tasks are."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper's approach to cross-modal adversarial attacks on images, audio, and text is quite novel.\n- The discovery that multi-modal embeddings can be aligned to a target input arbitrarily chosen by an attacker is interesting.\n- The paper is mostly well-written and well-organized."
                },
                "weaknesses": {
                    "value": "The main weakness of this paper is the incompleteness of the experiments. There's a lack of experiments involving other models, and the variety of experimental tasks is insufficient. Therefore, I give a 5-point rating initially.\n- Using just one multi-modal model, ImageBind, in the experiments to demonstrate that \u201cmulti-modal embeddings can be vulnerable to an attack\u201d may be somewhat insufficient. Conducting experiments on AudioCLIP[1], another contrastively pre-trained multi-modal model, would make the claim more convincing.\n- Regarding using ImageBind in the experiments, more experiments, e.g. audio classification, image-to-text retrieval, and audio-to-video retrieval, could have been done to strengthen the claim.\n\n[1] Andrey Guzhov, Federico Raue, J\u00f6rn Hees, Andreas Dengel. AudioCLIP: Extending CLIP to Image, Text and Audio. ICASSP 2022."
                },
                "questions": {
                    "value": "- I'm uncertain about why black-box attacks cannot be applied in this context. For instance, images and mel spectrograms can still be misclassified into the target input by introducing specific noises calculated by SimBA[2]. Some further clarification regarding the limitations or inapplicability of black-box attacks in this context would be helpful.\n- Are the other white-box attacks as effective as I-FGSM in performing adversarial illusions?\n- The experiments are conducted using ImageBind, a model that is contrastively pre-trained and projects all modality data into an image embedding space. If ImageBind were replaced with a multi-modal model like ChatBridge[3], which is not contrastively pre-trained and projects all modality data into a text embedding space, would this still demonstrate the vulnerability of multi-modal embeddings to the adversarial illusion attack?\n- In Figure 5, could you please clarify why the target input, a sheep image, is encoded by a text encoder rather than an image encoder?\n- (Minor) Is the I-FGSM formula complete? I think a clipping operation is missing in it.\n- (Minor) I find it a bit confusing whether unCLIP can be used as a generative model. In the \"Downstream models\" section of Section 2, the paper mentions that \"diffusion models that operate on CLIP embeddings, e.g., unCLIP, can also operate on ImageBind embeddings.\" However, in Section 6, the paper also notes that BindDiffusion, which employs the unCLIP model, struggles to generate images from the multi-modal embeddings. Some clarification on this apparent discrepancy would be appreciated.\n\n[2] Chuan Guo, Jacob R. Gardner, Yurong You, Andrew Gordon Wilson, Kilian Q. Weinberger. Simple Black-box Adversarial Attacks. ICML 2019.  \n[3] Zijia Zhao, Longteng Guo, Tongtian Yue, Sihan Chen, Shuai Shao, Xinxin Zhu, Zehuan Yuan, Jing Liu. ChatBridge: Bridging Modalities with Large Language Model as a Language Catalyst. arXiv:2305.16103."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Potentially harmful insights, methodologies and applications"
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8508/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8508/Reviewer_cUzT",
                        "ICLR.cc/2024/Conference/Submission8508/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8508/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699078189483,
            "cdate": 1699078189483,
            "tmdate": 1700725460112,
            "mdate": 1700725460112,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nAKjNJxhY0",
                "forum": "DDAtRS5Ngf",
                "replyto": "NPvb7avh9X",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8508/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8508/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the review!\n\n**Weaknesses:**\n\n1. To expand the evaluation, we have shown that our attack is effective on AudioCLIP (with a 100% success rate for $\\epsilon$ larger than 4. In addition, we are conducting experiments on the transferability between embedding spaces. The experiments will be added to our **Evaluation** section.\n2. We are also conducting experiments on Audio Classification for both ImageBind and AudioCLIP. We expect the numbers to be good, considering audio retrieval is a strictly harder desiderata than classification. The experiments will be added to our **Evaluation** section.\n\n**Questions:**\n\n1. Black-box attacks are interesting future work but there are some inherent challenges to this formulation that we will add to the **Limitations** section. Consider two alternate definitions of \u2018black-box\u2019: (1) the attacker has no access to the internal representations of the encoder, but does have access to the resulting embeddings, and (2) the attacker has only access to the outputs of a specific downstream task. Recall that our attack is targeted.\n    \n    For (1), satisfying the classification constraint (i.e., changing the label to a specific target) is a significantly easier task than finding an image that is arbitrarily close, in a high-dimensional embedding space, to a specific target embedding. Modern black-box techniques that rely on Bayesian Optimization are particularly ill-suited for this high-dimensional objective. That said, this is an interesting topic for future work.\n    \n    For (2), while it is possible that an adversarial example generated for a specific downstream task may generalize to others, the representation-merging of the example and its target could happen in the downstream model as opposed to the encoder, making generalization to all downstream tasks hard to achieve. This is also a promising direction for future work.\n    \n2. We demonstrate that in our setting, I-FGSM\u2019s performance is essentially perfect, achieving high adversarial alignment (much higher than organic alignment) with tiny perturbations.\u00a0 We are not sure about the motivation for exploring other methods but can evaluate PGD if requested.\n3. Given the strength of our adversarial alignment, we expect that any system that relies on a unified embedding space, regardless of whether it was contrastively trained, will be vulnerable to attack. In particular, our attack relies on the former as opposed to the latter.\n4. Thanks for pointing that out! That should read \u2018Image Encoder\u2019.\n5. Thanks again, we are missing the clipping operation.\n6. We will clarify this distinction in the **Limitations** section. To rephrase, ImageBind embeddings work on unCLIP, but seemingly perform worse. We hope to disentangle our observations on the alignment of our attack and the limitations (due to training) of the models we use."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8508/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700547530654,
                "cdate": 1700547530654,
                "tmdate": 1700547530654,
                "mdate": 1700547530654,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rW8H1NeEP9",
                "forum": "DDAtRS5Ngf",
                "replyto": "nAKjNJxhY0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8508/Reviewer_cUzT"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8508/Reviewer_cUzT"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you to the authors for addressing my questions and resolving most of my concerns. If the authors can present the experimental results of AudioCLIP and audio classification here or in the paper, and if these results also show the effectiveness of adversarial illusion, I would raise my score to 6."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8508/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700705948687,
                "cdate": 1700705948687,
                "tmdate": 1700705948687,
                "mdate": 1700705948687,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "k7bkehRFRn",
                "forum": "DDAtRS5Ngf",
                "replyto": "RTDQL4Q0CA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8508/Reviewer_cUzT"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8508/Reviewer_cUzT"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for providing the AudioCLIP experiment. How about the audio classification experiment?"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8508/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700710412978,
                "cdate": 1700710412978,
                "tmdate": 1700710412978,
                "mdate": 1700710412978,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6r4PHG0oJt",
                "forum": "DDAtRS5Ngf",
                "replyto": "NPvb7avh9X",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8508/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8508/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We are still in the process of getting those numbers, but in small scale experiments on ImageBind for $\\epsilon = 0.05$ we achieve 100% accuracy. We will report the expanded results for these experiments in the final version of the paper with the remaining choices of $\\epsilon$ as well as equivalents on AudioCLIP. Given this result and the efficacy on audio retrieval, we project that the results will be better than or equal to the audio retrieval experiments. Recall that zero-shot retrieval requires the identification of an arbitrary caption related to an audio while classification seeks out labels that the model was already trained on (in the case of ImageBind). Importantly, in the space of multimodal encoders, the mechanics of the two tasks are equivalent: finding the label or caption nearest in embedding space to the input. Since, our attack works on arbitrary (audio, caption) pairs, we believe that the attack will be effective on (audio, label) pairs as well.\n\n(edits: 1. a previous version of this comment mentioned the wrong epsilon bound and 2. added details about retrieval.)"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8508/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700719422531,
                "cdate": 1700719422531,
                "tmdate": 1700722295339,
                "mdate": 1700722295339,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1iq7gVvka1",
                "forum": "DDAtRS5Ngf",
                "replyto": "6r4PHG0oJt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8508/Reviewer_cUzT"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8508/Reviewer_cUzT"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for informing the current progress. Since my concerns have been addressed, I will raise the score to 6 as I promised."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8508/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700725420932,
                "cdate": 1700725420932,
                "tmdate": 1700725420932,
                "mdate": 1700725420932,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dMWYNW142M",
            "forum": "DDAtRS5Ngf",
            "replyto": "DDAtRS5Ngf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8508/Reviewer_Tpfm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8508/Reviewer_Tpfm"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies adversarial attacks for images and audio using multimodal embeddings. The method builds upon a pretrained multimodal embedding model such as ImageBind, and can be used to attack downstream models that also use this model as the embedding model. Given an image/audio and an adversarial text, the adversarial attack is applied to the image/audio space to maximize the cosine similarity between the image/audio embedding and the adversarial text embedding. Experiments show that adversarial examples can fool downstream tasks that use the same embedding model."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Experiments include image attacks and audio attacks, which are more comprehensive than previous works that mostly experiment with one modality.\n2. The paper is well-written."
                },
                "weaknesses": {
                    "value": "1. The downstream task uses exactly the same embedding model as the one being attacked. Therefore, it is not surprising that they can be fooled. It would be more interesting if some unexpected findings/insights were provided. \n2. As the authors acknowledged, several existing papers have studied adversarial attacks for multimodal learning (e.g., for CLIP and for multimodal large language models). More thorough comparisons with the previous works should be done."
                },
                "questions": {
                    "value": "How robust is the attack to adversarial defense?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8508/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699595149039,
            "cdate": 1699595149039,
            "tmdate": 1699637062920,
            "mdate": 1699637062920,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EnTTJDe8sA",
                "forum": "DDAtRS5Ngf",
                "replyto": "dMWYNW142M",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8508/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8508/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your review.\n\n**Weaknesses:**\n\n1. Ours is the first attack that targets multi-modal embeddings, as opposed to a specific task, and is completely agnostic of downstream tasks.\u00a0 Success of an embedding attack cannot be evaluated without measuring its effect on tasks that use the embedding.\n    \n    To expand the evaluation, we have shown that our attack is effective on AudioCLIP (with a $100\\\\%$ success rate for $\\epsilon$ larger than 4. In addition, we are conducting experiments on the transferability between embedding spaces. The experiments will be added to our **Evaluation** section.\n    \n    Previous attacks on multimodal learning target a single modality and don\u2019t need to deal with modality gap. We emphasize our main observation: *adversarial* alignment can be made significantly closer than any *organic* alignment regardless of input and target modality. As a result, our attack affects **any current and future downstream task** based on the attacked embedding (or, as we show in our new experiments, similar embeddings). With an increased industry focus on large, centralized encoders, the same embedding will underpin many downstream use cases.\u00a0 We show they can be attacked wholesale, without the attacker even knowing what they are.\n    \n2. Previous work targeted specific tasks, not multi-modal embeddings themselves. Our updated **Background and Related Work** section explains how our attack relates to [1, 2, 3], which were helpfully provided by our other reviewers.\n\n**Questions:**\n\n1. Our **Countermeasures** section has been updated to show how our attack evades the JPEG compression defense, a common method to mitigate adversarial perturbations. The JPEG-resistant attack still achieves a $75\\\\%$ success rate in the presence of the defense. It also evades the defense that checks consistency of embeddings of different input augmentations."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8508/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700547354850,
                "cdate": 1700547354850,
                "tmdate": 1700547354850,
                "mdate": 1700547354850,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Px9YnnnXvQ",
                "forum": "DDAtRS5Ngf",
                "replyto": "EnTTJDe8sA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8508/Reviewer_Tpfm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8508/Reviewer_Tpfm"
                ],
                "content": {
                    "title": {
                        "value": "Similar idea explored by ICLR 2023 paper \"Understanding Zero-Shot Adversarial Robustness for Large-Scale Models\""
                    },
                    "comment": {
                        "value": "I respectfully disagree with the claims that (1) it is the first attack that targets multi-modal embeddings and (2) previous work targeted specific tasks but not multi-modal embeddings themselves. Many previous works have attacked the CLIP model. For instance, why is the current manuscript fundamentally different from this work from ICLR 2023 [1], except that the audio experiments are included?\n\n[1] Chengzhi Mao, Scott Geng, Junfeng Yang, Xin Wang, Carl Vondrick, **Understanding Zero-Shot Adversarial Robustness for Large-Scale Models** (ICLR 2023)"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8508/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700703628285,
                "cdate": 1700703628285,
                "tmdate": 1700703628285,
                "mdate": 1700703628285,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WVcXvDLsDG",
                "forum": "DDAtRS5Ngf",
                "replyto": "dMWYNW142M",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8508/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8508/Authors"
                ],
                "content": {
                    "title": {
                        "value": "re ICLR'23 paper"
                    },
                    "comment": {
                        "value": "Thank you for providing the reference, we want to point that the attack defined in **Sec 3.1** of the paper is an attack on a *specific* downstream task (zero-shot image classification), not a **downstream-task agnostic** attack on the multi-modal embedding that affects all tasks based on that embedding (which is our contribution in this submission) including **generative** tasks that don\u2019t assign a fixed label to the input.\n\nWe, therefore, argue that our attack is the first downstream-task agnostic attack unlike the method presented in that paper.\n\n| Input type                 | Top-1         |          | Top-5         |          |\n|----------------------------|---------------|----------|---------------|----------|\n|                            | Original label| Target label | Original label| Target label |\n| Original image x           | 85%           | 0%       | 99%           | 0%       |\n| Adversarial illusion xa    | 77%           | 0%       | 95%           | 0%       |\n| Generated(ImageBind(x))    | 42%           | 0%       | 64%           | 2%       |\n| Generated(ImageBind(xa))   | 0%            | 64%      | 1%            | 92%      |"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8508/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700706971030,
                "cdate": 1700706971030,
                "tmdate": 1700719587511,
                "mdate": 1700719587511,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]