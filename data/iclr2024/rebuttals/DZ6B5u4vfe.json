[
    {
        "title": "Instruction-tuned LLMs with World Knowledge are More Aligned to the Human Brain"
    },
    {
        "review": {
            "id": "cHtkyU5bx7",
            "forum": "DZ6B5u4vfe",
            "replyto": "DZ6B5u4vfe",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9249/Reviewer_WxVH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9249/Reviewer_WxVH"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a study about the relation between LLMs and humans. The motivation is that the instruction-tuned LLMs carry out human instructions better (seems closer to humans). Analysis on both brain activities  shows a closer alignment from LLMs after instruction tuning. The authors also found that the world knowledge and model size are strongly correlated with brain alignment."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The authors examined two famous and commonly used families of instruction tuned models and find a consistent phenomenon. They also observed the gradual increase in brain score during instruction tuning.\n\n2. The authors studied the models\u2019 fit to both human neural and behavioral data."
                },
                "weaknesses": {
                    "value": "1. It remains unclear in the whole passage that which \u201cinternal representations\u201d from LLMs are used, which makes it difficult to reproduce the results.\n\n2. The \u201cworld knowledge\u201d part in the BBH dataset is different from the knowledge required in MMLU. The formal consists subsets such as Sports Understanding, Movie Recommendation, and Causal Judgement; While the latter is mainly about disciplinary knowledge such as Anatomy and College Physics. This makes the key term, \u201cworld knowledge\u201d, much ambiguous. What knowledge are considered \u201cworld knowledge\u201d? Are there any difference between factual, general and disciplinary knowledge?\n\n3. The authors tried to study the effect of world knowledge and model size separately in Section 4.2. However, the two factors are deeply intertwined, given that larger LLMs tend to outperform smaller ones in knowledge-related question answering. The results in Figure 2 also show that model size has even stronger and more significant effect on the brain score. As a result, it cannot be concluded that \u201cworld knowledge\u201d is a key contributor to the increase in brain score. Instead, it can be just another indirect effect of the larger model size.\n\n4. The authors use the performance on MMLU and BBH to represent the models\u2019 capability of \u201cworld knowledge\u201d. However, performance on these benchmarks is affected not only by the quantity of knowledge that the models possess, but also by their ability to follow instructions. Thus, a higher performance on MMLU and BBH doesn\u2019t necessarily mean that the model has more world knowledge, and the correlation between benchmark scores and brain scores does not necessarily show a link between world knowledge and the fit to human neural data.\n\n5. The authors use the correlation between model per-token perplexity and human reading time to represent the behavioral fit. However, they pointed out in Section 6.2 that this approach is controversial when applied to large Transformer-based models. Thus, the choice of this approach is confusing. Why not use other ways to test the behavioral fit?\n\n6. It is counter-intuitive that factual, domain knowledge can contribute the higher human fit in general reading. In fact, many questions in MMLU are difficult even for most people (e.g., Anatomy, Astronomy, College Physics, ...), and are not going to be retrieved during story reading. It is confusing why the authors choose MMLU as an aspect of the \u201cworld knowledge\u201d, and how this can guide Neuroscience research in human language understanding.\n\n7. The three fMRI datasets are in different settings, i.e., reading sentence by sentence, listening the whole passage, and reading word by word, which could bring different activation patterns in the human brain. However, the authors did not discuss the difference between them."
                },
                "questions": {
                    "value": "See the weaknesses part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9249/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698830940712,
            "cdate": 1698830940712,
            "tmdate": 1699637164566,
            "mdate": 1699637164566,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "E1ujAQwo1C",
                "forum": "DZ6B5u4vfe",
                "replyto": "cHtkyU5bx7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9249/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9249/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We are happy that the reviewer appreciated that (1) our experiments examine commonly-used model families, and that (2) our results are consistent. Based on their feedback, we ran additional experiments and improved our paper\u2019s clarity. We respond to their points below:\n\n**Q1: The authors tried to study the effect of world knowledge and model size separately in Section 4.2. However, the two factors are deeply intertwined.**\n\nOur work shows that larger LLMs are more aligned to the brain. However, we wish to find out why. What aspects of larger LLMs make them more brain-aligned? Some possibilities: (1) greater world knowledge, (2) better problem-solving abilities, (3) better next-word prediction ability. Our work conducts further experiments to identify the underlying properties of LLMs (beyond model size) that contribute towards greater brain alignment. We show that world knowledge is a key determinant of brain alignment, more than other tested factors.\n\nTo show that the plots of world knowledge are not dependent on model size, we wish to highlight that smaller models instruction-tuned to gain greater capabilities (e.g., Vicuna-13B) can achieve greater brain alignment scores than larger models before instruction-tuning (e.g., LLaMA-33B). For reference, here are the average brain alignment values of the models: LLaMA-13B = 0.220, Vicuna-13B = 0.229, LLaMA-33B = 0.227, and Vicuna-33B = 0.232. The Vicuna-13B model has greater brain alignment than LLaMA-33B although it is less than 40% the size. We observe a similar trend when looking at another set of four models: T5-base, Flan-T5-base, T5-large, Flan-T5-large.\n\nTo further demonstrate that model size alone does not determine the brain alignment of an LLM, we ran new experiments to evaluate the brain alignment of a randomly-initialized LLaMA-7B. It is a large model with 7B parameters. The randomly-initialized LLaMA was not trained on any data and hence does not contain world knowledge. It also achieves roughly zero accuracy on the BBH and MMLU benchmarks. We observe that the randomly-initialized model achieves lower brain alignment than the trained LLaMA and instruction-tuned Alpaca-7B version. Prior works have also shown that large random embedding models do not achieve high brain alignment scores [1]. These results demonstrate that there are LLM properties aside from model size that contribute significantly to brain alignment.\n\n[1] Schrimpf, Martin, et al. \"The neural architecture of language: Integrative modeling \u2026\u201d\n\n**Q2: The authors use the performance on MMLU and BBH to represent the models\u2019 capability of \u201cworld knowledge\u201d. However, performance on these benchmarks is affected not only by the quantity of knowledge that the models possess, but also by their ability to follow instructions.**\n\nWe agree with the reviewer. Hence, we restricted our selection of LLMs to only two model families. In each model family, the models are trained on a similar format of instruction data, so they have similar ability to follow the instruction format of MMLU and BBH. Thus, any differences in their MMLU or BBH performance would point to differences in the quantity of knowledge they possess, rather than their ability to follow the instruction format of MMLU or BBH.\n\n**Q3: Additional details about our experiment methods**\n\nWe thank the reviewer for pointing out areas that require additional clarifications. We have improved our paper\u2019s clarity by adding details on which layers were used to evaluate brain alignment, and the types of world knowledge in BBH and MMLU."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9249/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700736906669,
                "cdate": 1700736906669,
                "tmdate": 1700736906669,
                "mdate": 1700736906669,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "sFD4UNdzNe",
            "forum": "DZ6B5u4vfe",
            "replyto": "DZ6B5u4vfe",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9249/Reviewer_KJEe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9249/Reviewer_KJEe"
            ],
            "content": {
                "summary": {
                    "value": "The paper explores the impact of instruction-tuning on large language models (LLMs) to determine their alignment with the human brain in terms of brain and behavioral alignment. Experimental results from two renowned LLM families indicate that instruction-tuning improves brain alignment by 6.2%, with world knowledge and model size being the primary contributors. However, instruction-tuning does not have a similar effect on behavioral alignment. The authors emphasize the importance of integrating world knowledge in future LLM developments."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper focuses on the instruction tuning of LLMs, exploring the neuroscience behind language models. This unique perspective advances the understanding of LLMs in the context of human cognition.\n\nThe experiment design is intuitive and relatively easy to follow. The experiments are extensive, including 3 datasets for brain and behavioral alignment, respectively."
                },
                "weaknesses": {
                    "value": "Lack of comparative analysis with other tuning techniques such as reinforcement learning from human feedback (RLHF).\n\nThe investigation of behavioral alignment is limited. A more comprehensive exploration could offer insights into the discrepancy between brain and behavioral alignments and its implications for LLM development and application.\n\nOther alignment measure methods may be considered to increase the reliability of the results, such as:\n\nJiaang, Li, et al. \"Structural Similarities Between Language Models and Neural Response Measurements.\" arXiv preprint arXiv:2306.01930 (2023).\n\nLiu, Xu, et al. \"Coupling Artificial Neurons in BERT and Biological Neurons in the Human Brain.\" arXiv preprint arXiv:2303.14871 (2023)."
                },
                "questions": {
                    "value": "What is the computational cost of this study? 33B model is quite large, are there any quantization techniques used like LoRA?\n\nMay need proofreading: Table 3 and Table 4 in the Appendix have the same caption."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9249/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9249/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9249/Reviewer_KJEe"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9249/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698837823549,
            "cdate": 1698837823549,
            "tmdate": 1699637164447,
            "mdate": 1699637164447,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "osbhYjnmcJ",
                "forum": "DZ6B5u4vfe",
                "replyto": "sFD4UNdzNe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9249/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9249/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We are happy that the reviewer thinks (1) our paper presents a unique neuroscientific perspective for understanding LLMs, (2) our experiment design is intuitive and easy to follow, and (3) our experiments are extensive. We respond to their points below and improve our paper based on their feedback:\n\n**Q1: Lack of comparative analysis with other tuning techniques such as reinforcement learning from human feedback (RLHF).**\n\nRLHF is often performed *after* the instruction-tuning phase. Hence, we did not consider it as a comparable alternative to instruction-tuning. In order to perform a comparative analysis between RLHF and instruction-tuning, as suggested by the reviewer, we would need to find LLMs trained with RLHF *instead of* instruction-tuning. Ideally, both LLMs should be trained on the same amount of training data and compute, to allow for a fair comparison. These LLMs also need to be open-source, in order to evaluate their brain alignment. Unfortunately, we could not find such LLMs.\n\nWe believe that investigating how RLHF affects the brain alignment of instruction-tuned LLMs constitutes future work. RLHF explicitly aims to align LLMs using human feedback. It would be an interesting direction to see if it also aligns LLMs to human brain activity (brain alignment) and behavior (behavioral alignment). \n\n**Q2: The investigation of behavioral alignment is limited. A more comprehensive exploration could offer insights into the discrepancy between brain and behavioral alignments and its implications for LLM development and application.**\n\nWe agree with the reviewer\u2019s point. Hence, we mentioned this in our original submission (and keep it in our rebuttal version too). We included a paragraph in the discussion section 6.2, where we argue for the need to examine more dimensions of behavior. We mention that our work and many prior works compare LM and human next-word surprisal on reading tasks as a measure of behavioral alignment. However, this evaluates only a single dimension of LM and human behavior (per-word perplexity and reading times). We highlight the need to create more benchmarks to expand the dimensions of behavior examined for both LLMs and humans, in order to holistically evaluate LLM behavior, as well as LLM-human behavioral alignment.\n\nHence, we focus instead on the brain alignment of LLMs.  Our paper focuses on investigating how instruction-tuning affects LLMs from a neuroscientific perspective and the key factors underlying brain-LLM alignment (e.g., world knowledge).\n\n**Q3: 33B model is quite large, are there any quantization techniques used like LoRA?**\n\nFor the 33B models, we only performed inference, not training. Hence, LoRA was not used. All our evaluations involve only model inference. This includes the brain alignment evaluations (Pereira2018, Blank2014, Wehbe2014), behavioral alignment evaluations (Futrell2018), and the evaluation of world knowledge (MMLU) and problem-solving abilities (BBH). These evaluations do not involve model training."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9249/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700736759729,
                "cdate": 1700736759729,
                "tmdate": 1700736759729,
                "mdate": 1700736759729,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rT0R39WKAw",
            "forum": "DZ6B5u4vfe",
            "replyto": "DZ6B5u4vfe",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9249/Reviewer_fAu2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9249/Reviewer_fAu2"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates the effect of instruction-tuning in the alignment between LLMs\u2019 representations and human language processing. The authors use two types of human data: brain activity patterns (brain alignment) and reading times (behavioral alignment). The brain alignment is defined as the extent to which a linear regression model predicts brain activity patterns using the LLMs\u2019 representations. The behavioral alignment is defined as a correlation between LLM perplexity and human reading time for each word. Through the experiments across 25 vanilla and finetuned models from the T5 and LLaMA families, the authors conclude that instruction tuning improves brain alignment, (2) the performance on the world knowledge-related tasks and model size are correlated with brain alignment, and (3) instruction-tuning and other examined factors are not correlated with behavioral alignment."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Using 25 models and two benchmarking datasets covering various task categories, the authors perform detailed analysis between LLM representations and human brain and behavioral data.\n- The discussion includes implications both for NLP and neurosciences along with the literature review, which can encourage interdisciplinary research across both fields.\n- The paper is well-written and easy to follow."
                },
                "weaknesses": {
                    "value": "- The authors use models from just two families, T5 and LLaMA. Looking at Figure 2, it seems that LLaMA models do not show a significant correlation between brain alignment and MMLU score, BBH world knowledge, and model size. The results would be more convincing if the authors could use a few more families such as GPT.\n- Concerning the tasks related to world knowledge, it appears that these tasks may simply exhibit greater linguistic diversity compared to the other tasks examined. The concept of world knowledge seems somewhat ambiguous, and any clarification could be insightful. For instance, would similar results be observed if more language understanding tasks were added? I was unable to determine how the BBH tasks are categorized into \"language understanding\" and \"world knowledge.\"\n\nI think that expanding the experiments to address these points could lead to more reliable results."
                },
                "questions": {
                    "value": "- How did the authors determine the category classification for the BBH tasks?\n- Is it possible to provide a more detailed analysis regarding world knowledge? Any discussion and additional analysis would be appreciated."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9249/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698838239568,
            "cdate": 1698838239568,
            "tmdate": 1699637164339,
            "mdate": 1699637164339,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RH0ARuxH1A",
                "forum": "DZ6B5u4vfe",
                "replyto": "rT0R39WKAw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9249/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9249/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We are happy the reviewer found that (1) our analyses were detailed, (2) our results had implications for both NLP and neuroscience, and that (3) our paper was well-written and easy to follow. We respond to their points below and improve our paper based on their feedback:\n\n**Q1: The authors use models from just two families, T5 and LLaMA.**\n\nIn our work, we evaluate the correlation of brain alignment to other LLM properties, such as world knowledge (MMLU) and problem-solving abilities (BBH). We agree with the reviewer that adding a greater variety of models would strengthen the generalizability of our claims. \n\nHowever, we realized that the performance on these benchmarks is affected not only by the quantity of knowledge that the models possess, but also by their ability to follow the instruction format of MMLU and BBH. (This point was also mentioned by reviewer WxVH.)\n\nHence, we restricted our selection of LLMs to only two model families. In each model family, the models are trained on a similar format of instruction data, so they have similar ability to follow the instruction format of MMLU and BBH. Thus, any differences in their MMLU or BBH performance would point to differences in the quantity of knowledge they possess, rather than their ability to follow the instruction format of MMLU or BBH.\n\nIf we selected LLMs from many different model families, it would be difficult to control for their ability to follow the instruction format of MMLU and BBH, as they are trained on vastly different instruction formats. This would make it challenging to use their MMLU and BBH results to suggest the quantity of knowledge that the models possess. \n\nHowever, we overall agree with the reviewer that greater model variety would strengthen generalizability, so we added it to our paper\u2019s section on limitations and future work.\n\n**Q2: How did the authors determine the category classification for the BBH tasks?**\n\nWe use the same category classification as the original BBH paper. The BBH paper categorizes tasks into four categories: (1) Algorithmic and Multi-Step Arithmetic Reasoning, (2) Natural Language Understanding, (3) Use of World Knowledge, and (4) Multilingual Knowledge and Reasoning. We add this clarification to our paper.\n\nBBH paper:\nM. Suzgun et al., \u201cChallenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them.\u201d\n\n**Q3: Is it possible to provide a more detailed analysis regarding world knowledge? Any discussion and additional analysis would be appreciated.**\n\nFor both MMLU and BBH, we follow their respective papers for how they categorized the world knowledge tasks.\n\nThe MMLU paper describes that they design the benchmark to measure knowledge from many domains. It contains 57 tasks, categorized by the subject domain of world knowledge tested: STEM, Humanities, Social Sciences, and Others. The STEM category includes questions on computer science, physics, mathematics, etc. The Humanities category includes questions on philosophy, law, history, etc. The Social Sciences category includes questions on politics, sociology, economics, geography, etc. The Others category includes questions on business topics such as finance, accounting, as well as general knowledge of global facts.\n\nThe world knowledge category of BBH contains tasks that test for factual and general knowledge. Tasks requiring factual knowledge include: \u201cSports Understanding\u201d and \u201cMovie Recommendation\u201d. Tasks requiring general knowledge include: \u201cCausal Judgement\u201d, which tests knowledge about causal-reasoning suppositions, and \u201cRuin Names\u201d, which requires knowledge about human perception and usage of humor in the English language."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9249/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700736481088,
                "cdate": 1700736481088,
                "tmdate": 1700736481088,
                "mdate": 1700736481088,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "p0KUMoF6gG",
            "forum": "DZ6B5u4vfe",
            "replyto": "DZ6B5u4vfe",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9249/Reviewer_co7p"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9249/Reviewer_co7p"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates the correlation between instruction-tuned LLMs and human similarity in the field of neuroscience by examining brain alignment and behavioral alignment. The authors evaluate 25 LLMs on a reading task to identify the effects of instruction tuning LLMs in terms of human language processing. Instruction turned LLMs have higher brain scores than vanilla LLMs, and further analyzed the properties of LLMs that contribute high alignment and found out that the model size and world knowledge are correlated to brain alignments. For the behavioral alignment, there was no correlation between per-word LLM perplexity and per-word human reading times."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "This paper explains why instruction-tuned LLMs perform better than vanilla LLMs from a neuroscience perspective by measuring brain scores."
                },
                "weaknesses": {
                    "value": "- This paper appears to be a replication of [1,2], specifically focusing on instruction-tuned models. It lacks novelty and originality.\n    - Increasing the model size and integrating world knowledge (using a larger training dataset) are not surprising new discoveries for improving language modeling.\n    - Additionally, this paper measures the correlation between world knowledge tasks and brain alignment.\n    - This paper should demonstrate the effects of contributing factors separately (world knowledge and model size). The plots seem to be dependent on model size.\n- The current version of the paper requires further improvement.\n    - It lacks details for readers without a background in neuroscience.\n        - How is the brain score computed for each model? Does it compute the hidden state of every layer?\n    - In Section 4.1, the last paragraph seems to be located too early, making it difficult to understand before explaining the dataset.\n    - In Figure 3A, shouldn't the language stimuli be labeled as Futrell2018?\n    - Figure 3B appears to be an empty plot.\n\n\n[1] Schrimpf, Martin, et al. \"The neural architecture of language: Integrative modeling converges on predictive processing.\"\u00a0*Proceedings of the National Academy of Sciences*\u00a0118.45 (2021): e2105646118. \\\n[2] Oh, Byung-Doh, and William Schuler. \"Why does surprisal from larger transformer-based language models provide a poorer fit to human reading times?.\"\u00a0*Transactions of the Association for Computational Linguistics*\u00a011 (2023): 336-350."
                },
                "questions": {
                    "value": "- How is the 'No Instruction' model trained in Figure 1D? The Alpaca instruction dataset is formed with both non-empty input fields (instruction, input, output) and empty input fields (instruction, output). Did you only use non-empty input fields and remove the instruction in those cases?\n- What aspect do you believe instruction tuning contributes to the correlation between world knowledge and brain alignment?\n- In Figure 2, it appears that there are different correlations for each dataset (Pereira2018, Blank2014, and Wehbe2014). Why is Blank2014's correlation so much lower compared to the other two?\n- How is word perplexity measured? Could you provide an example of input and the corresponding NWP loss?\n    - Since Flan-T5 models are encoder-decoder models, I'm not sure how they are measured differently from decoder-only models. Were the same inputs passed into both the encoder and the decoder?\n    - Did the vanilla LLMs also show no correlation?\n- Why is there a performance drop when Flan-T5 is fine-tuned on instruction tuning datasets (Alpaca, GPT4ALL, ShareGPT) as seen in Table 5 (Flan-T5-XL results)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9249/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699243055584,
            "cdate": 1699243055584,
            "tmdate": 1699637164245,
            "mdate": 1699637164245,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zOcb5B5Rm3",
                "forum": "DZ6B5u4vfe",
                "replyto": "p0KUMoF6gG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9249/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9249/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We are happy that the reviewer appreciated our neuroscientific analysis of instruction-tuning. Based on their feedback, we ran additional experiments and improved our paper\u2019s clarity. We respond to their points below:\n\n**Q1: This paper appears to be a replication of [1,2], specifically focusing on instruction-tuned models. It lacks novelty and originality.** \n\nOur work presents many new insights for researchers studying the parallels between LLMs and human brains: (a) Instruction-tuning aligns LLM representations to human brain activity, and (b) World knowledge is a key factor underlying this alignment. These are both new directions of study that produced new insights. Hence, our work is different from both [1] and [2], which focus on studying how the next-word prediction surprisal of language models relates to human brain activity and behavior.\n\n[1] Schrimpf, Martin, et al. \"The neural architecture of language: Integrative modeling \u2026\u201d\n\n[2] Oh, Byung-Doh, and William Schuler. \"Why does surprisal from larger transformer-based language models provide a poorer fit to human reading times?.\" \n\n**Q2: Increasing the model size and integrating world knowledge (using a larger training dataset) are not surprising new discoveries for improving language modeling.** \n\nOur paper is not focused on discovering new methods for improving language modeling. Rather, our paper focuses on evaluating the representational and behavioral similarity of LLMs to the human language system. In particular, we investigate how instruction-tuning affects LLMs from a neuroscientific perspective and the key factors underlying brain-LLM alignment (e.g., world knowledge and model size). \n\n**Q3: The plots of world knowledge (against brain alignment)  seem to be dependent on model size.**\n\nOur work shows that larger LLMs are more aligned to the brain. However, we wish to find out why. What aspects of larger LLMs make them more brain-aligned? Some possibilities: (1) greater world knowledge, (2) better problem-solving abilities, (3) better next-word prediction ability. Our work conducts further experiments to identify the underlying properties of LLMs (beyond model size) that contribute towards greater brain alignment. We show that world knowledge is a key determinant of brain alignment, more than other tested factors.\n\nTo show that the plots of world knowledge are not dependent on model size, we wish to highlight that smaller models instruction-tuned to gain greater capabilities (e.g., Vicuna-13B) can achieve greater brain alignment scores than larger models before instruction-tuning (e.g., LLaMA-33B). For reference, here are the average brain alignment values of the models: LLaMA-13B = 0.220, Vicuna-13B = 0.229, LLaMA-33B = 0.227, and Vicuna-33B = 0.232. The Vicuna-13B model has greater brain alignment than LLaMA-33B although it is less than 40% the size. We observe a similar trend when looking at another set of four models: T5-base, Flan-T5-base, T5-large, Flan-T5-large.\n\nTo further demonstrate that model size alone does not determine the brain alignment of an LLM, we ran new experiments to evaluate the brain alignment of a randomly-initialized LLaMA-7B. It is a large model with 7B parameters. The randomly-initialized LLaMA was not trained on any data and hence does not contain world knowledge. It also achieves roughly zero accuracy on the BBH and MMLU benchmarks. We observe that the randomly-initialized model achieves lower brain alignment than the trained LLaMA and instruction-tuned Alpaca-7B version. Prior works have also shown that large random embedding models do not achieve high brain alignment scores [1]. These results demonstrate that there are LLM properties aside from model size that contribute significantly to brain alignment.\n\n[1] Schrimpf, Martin, et al. \"The neural architecture of language: Integrative modeling \u2026\u201d\n\n**Q4: How is the 'No Instruction' model trained in Figure 1D? Did we use all the training data from the Alpaca dataset?**\n\nWe used all provided training samples from the Alpaca dataset. This is to ensure that the ablation model is trained on the same data as the instruction-tuned Alpaca-7B, with the only difference that the ablation model does not receive the \u201cinstruction\u201d portion during training. We added this clarification (that we use all the provided training samples) to our paper.\n\n**Q5: Additional details about our experiment methods**\n\nWe thank the reviewer for pointing out areas that require additional clarifications. We have improved our paper\u2019s clarity by adding details on the methods used for computing brain alignment, measuring next-word prediction loss, and running our ablation experiments."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9249/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700736336054,
                "cdate": 1700736336054,
                "tmdate": 1700736336054,
                "mdate": 1700736336054,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]