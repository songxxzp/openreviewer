[
    {
        "title": "Quantifying and Enhancing Multi-modal Robustness with Modality Preference"
    },
    {
        "review": {
            "id": "kNbacmHtzc",
            "forum": "XyrB1Ay44j",
            "replyto": "XyrB1Ay44j",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5565/Reviewer_hY8P"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5565/Reviewer_hY8P"
            ],
            "content": {
                "summary": {
                    "value": "This manuscript concerns the robust multi-modal representation learning, which are positioned well away from the discriminative multi-modal decision boundary. To address this issue, they theoretically discover that larger uni-modal representation margins and more reliable integration for modalities are essential components for achieving higher robustness. Inspired by our theoretical finding, we introduce a training procedure called Certifiable Robust Multi-modal Training (CRMT), which can alleviate this influence from modality preference and explicitly regulate essential components to significantly improve robustness in a certifiable manner. Experiments validate the effectiveness."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tThe multi-modal robustness learning is meaningful and challenging. The paper is well-written and the proposed method is easy to understand.\n2.\tThe authors theoretically discover that larger uni-modal representation margins and more reliable integration for modalities are essential components for achieving higher robustness.\n3.\tExperiments on various datasets validate the proposed method."
                },
                "weaknesses": {
                    "value": "The manuscript claims that they focus on the commonly used joint multi-modal framework, more multi-modal fusion method, and different multi-modal backbones should be compared. For example, the early fusion, and hybrid fusion strategy. On the other hand, different modalities can employ various backbones, the reviewer is curious about the influence of different backbones, and more ablation studies are expected.\n\nIn related work and comparison methods, more state-of-the-art multi-modal robustness approaches should be introduced and compared.\n\nHow can this setup be extended to three modalities? More explanations and experiments are needed."
                },
                "questions": {
                    "value": "refer to the weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5565/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698734859255,
            "cdate": 1698734859255,
            "tmdate": 1699636572371,
            "mdate": 1699636572371,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SQY1pC0CtI",
                "forum": "XyrB1Ay44j",
                "replyto": "kNbacmHtzc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5565/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5565/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response by authors"
                    },
                    "comment": {
                        "value": "Thank you for your valuable feedback and insightful comments! We respond to some concerns below:\n\n1. **Different multi-modal fusion backbones**\n\nThank you for your advice. In our study, we focus on intermediate fusion by employing the widely used MultiModal Transfer Module (MMTM) [1] and conducting experiments on the Kinetic-Sounds dataset. The results from these experiments indicate that our method is well-suited for more multi-modal fusion strategies.\n\n\n|   Kinetic-Sounds   |  Clean  | Missing #v | Missing #a |   FGM   |  PGD-$\\ell_2$ |\n|:--------:|:-------:|:---------:|:---------:|:-------:|:-------:|\n| MMTM | 0.6693  |  0.4542   |  0.2028   | 0.3438  | 0.3205  |\n|   CRMT-MMTM (ours)   | 0.6737  |  0.5211   |  0.3169   | 0.3445  | 0.3358  |\n\n\n[1] H. R. Vaezi Joze, A. Shaban, M. L. Iuzzolino, and K. Koishida, \u201cMmtm: Multimodal transfer module for cnn fusion,\u201d in *Conference on Computer Vision and Pattern Recognition*(CVPR), 2020.\n\n2. **Different modalities can employ various backbones**\n\nThank you for pointing it out. We carry out experiments with different backbones on the Kinetic-Sounds dataset, specifically by replacing the audio backbone with a Transformer. The results, detailed in the table below, empirically validate our approach's effectiveness across different backbone architectures.\n\n|  ResNet18 (V) + Transformer (A)  |  Clean  | Missing #v | Missing #a |   FGM   |  PGD-$\\ell_2$ |\n|:--------:|:-------:|:---------:|:---------:|:-------:|:-------:|\n| JT | 0.5538  |  0.3387   |  0.2703   | 0.2456  | 0.2100  |\n|  CRMT-JT(ours) | 0.5807  |  0.3721   |  0.3539   | 0.3067  | 0.2711  |\n\n\n3. **State-of-the-art multi-modal robustness approaches**\n\nThank you for your suggestion. We compare our method with two recently proposed methods; Robust Multi-Task Learning (RMTL) [1] and Uni-Modal Ensemble with Missing Modality Adaptation (UME-MMA) [2]. For RMTL, we apply the idea of multi-task learning to obtain a robust multi-modal model, which includes full-modal tasks, and modality-specific tasks. We apply these attacks on the Kinetic-Sounds dataset and report the result to show the effectiveness of our method.\n\n|  Method |  Clean  | Missing #v | Missing #a |   FGM   |  PGD-$\\ell_2$ |\n|:-------:|:-------:|:----------:|:----------:|:-------:|:-------:|\n|   RMTL  | 0.6672  |   0.5015   |   0.2994   | 0.3641  | 0.3445  |\n| UME-MMA | 0.6999  |   0.5334   |   0.4666   | 0.3394  | 0.3125  |\n| CRMT-JT (ours) | 0.7580  |   0.5596   |   0.5908   | 0.4906  | 0.4680  |\n\n[1] M. Ma, J. Ren, L. Zhao, D. Testuggine, and X. Peng, \u201cAre multi-modal transformers robust to missing modality?\u201d in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2022, pp. 18 177\u201318 186.\n\n[2] S. Li, C. Du, Y. Zhao, Y. Huang, and H. Zhao, \u201cWhat makes for robust multi-modal models in the face of missing modalities?\u201d *arXiv preprint arXiv:2310.06383*, 2023.\n\n4. Extended to **three modalities**? More **explanations and experiments** are needed.\n\nThank you for your suggestions. In this study, our analysis discusses the universal situation of two modalities, and can also be extended to the scenario with more than two modalities. To consider more modalities, the key is to introduce the margin of these modalities\u2019s representation. Suppose we have $l$ different modality, the input of the $m$-th modality is $\\boldsymbol{x}^{(m)}$, define the representation margin $\\zeta_{j}^{(m)}(\\boldsymbol{x}^{(m)})$, and the corresponding Lipschitz constant $\\tau_j^{(m)}$. Thus, our bound can be extended to the following formulation.\n\n\\begin{equation}\n\\begin{aligned}\n \\min_{\\boldsymbol{x}'} \\left\\| \\left\\|\\boldsymbol{x} - \\boldsymbol{x}'\\right\\| \\right\\|_2 \n \\geq \\frac{ \\sum _{m=1}^l c_j^{(m)} \\zeta_j^{(m)}(\\boldsymbol{x}^{(m)}) +  \\beta_j}{\\sqrt{ \\sum _{m=1}^l (c_j^{(m)} \\tau_j^{(m)})^2  }} \\\\\\\\\n  where   \\quad j \\neq y \\quad s.t. \\quad   \\sum _{m=1}^l c_j^{(m)} \\zeta_j^{(m)}(\\boldsymbol{x}'^{(m)}) +  \\beta_j = 0.\n\\end{aligned}\n\\end{equation}\n\nThus, our approach can analyze the robustness of more than two modalities. \nWe conducted experiments on the UCF101 dataset using three modalities: RGB, Optical Flow, and RGB Frame Difference. These experiments were performed both from scratch and with an ImageNet-pretrained ResNet18. The outcomes demonstrate the effectiveness of our method in enhancing multi-modal robustness.\n \n|   Three modality   | JT (Scratch) | CRMT-JT (Scratch, ours) | JT (Pretrained) | CRMT-JT (Pretrained, ours) |\n|:----------:|:--------------------------:|:-------------------------------:|:---------------:|:--------------------:|\n|    Clean   |           0.4490           |             0.5640              |      0.8312     |        0.8506        |\n|     FGM    |           0.4005           |             0.4567              |      0.2471     |        0.4138        |\n|   PGD_$\\ell_2$   |           0.3963           |             0.4312              |      0.0783     |        0.2623        |"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5565/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700373522234,
                "cdate": 1700373522234,
                "tmdate": 1700373522234,
                "mdate": 1700373522234,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yefVXVRptk",
            "forum": "XyrB1Ay44j",
            "replyto": "XyrB1Ay44j",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5565/Reviewer_BCRJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5565/Reviewer_BCRJ"
            ],
            "content": {
                "summary": {
                    "value": "This work employs an orthogonal-based framework that formulates an alternative bound, eliminating the interrelation and explicitly presenting integration.  Building on the theoretical analysis, they introduce a two-step procedure called Certifiable Robust Multi-modal Training (CRMT) to progressively enhance robustness."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "(1) Following a more comprehensive analysis, the researchers furnish compelling evidence that demonstrates the constraining effect of multi-modal preference on the robustness of multi-modal systems, which contributes to the vulnerability of multi-modal models to specific modalities.\n\n(2) Building upon their theoretical insights, they present a two-step training protocol designed to alleviate the limitations stemming from modality preference. The suggested approach significantly enhances both the performance and robustness of multi-modal models across different real-world multi-modal datasets."
                },
                "weaknesses": {
                    "value": "(1) Since FGM and PGD are the two white-box attacks chosen in the adversarial robustness experiments, why not consider the stronger white-box Auto Attack?  It is suggested to add the experiment results about Auto Attack in Section4.\n\n(2) \"Robustness against multi-modal attacks\" mentioned In Section 4.2, since multi-modal attacks are considered, the experimental results in Table 1 only consider single-mode attacks (#a,#v). Is the method proposed in this paper effective when co-attacks (both modality attacks) are existing?  In [1,2], more effective multi-modal attack methods are proposed than uni-modal(such as #a and #v) attack. Can the proposed method effectively resist these multi-modal attack methods? It is suggested that the relevant experiments should be added to Section 4.2, otherwise the conclusion of \"Robustness against multi-modal attacks\" is somewhat not convincing.\n\n\n[1]\tZhang J, Yi Q, Sang J. Towards adversarial attack on vision-language pre-training models[C]//Proceedings of the 30th ACM International Conference on Multimedia. 2022: 5005-5013.\n[2]\tLu D, Wang Z, Wang T, et al. Set-level Guidance Attack: Boosting Adversarial Transferability of Vision-Language Pre-training Models[J]. arXiv preprint arXiv:2307.14061, 2023."
                },
                "questions": {
                    "value": "No"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5565/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698758294195,
            "cdate": 1698758294195,
            "tmdate": 1699636572256,
            "mdate": 1699636572256,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VypKMKWBJq",
                "forum": "XyrB1Ay44j",
                "replyto": "yefVXVRptk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5565/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5565/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response by authors"
                    },
                    "comment": {
                        "value": "Thank you for your valuable feedback and insightful comments! We respond to some concerns below:\n\n1. Consider the stronger **white-box Auto Attack**? It is suggested to add the experiment results about Auto Attack in Section 4.\n\nThank you for your advice. In our research, we employed the Fast Gradient Method (FGM) and Projected Gradient Descent with $\\ell_2$-norm (PGD-$\\ell_2$) to assess model robustness. Due to their scalability and effectiveness, these methods can be readily adapted for multi-modal settings, enabling the generation of both uni-modal and multi-modal attacks. However, the Auto Attack, a complex attack method that combines four distinct attacks, is primarily designed for uni-modal models with a single input and output, making its application to multi-modal settings challenging. Meanwhile, there has been a lack of targeted research on conducting these effective attacks with multiple inputs in the past. We have made some attempts during response, but we still cannot well implement Auto Attack for multiple inputs. Meanwhile, we want to explain that, the conducted experiments in the paper cover various attacks (e.g., multi-modal co-attack, missing condition and uni-modal attack), which are mostly considered by the previous work for multi-modal robustness, and the results can be used to validate the method effectiveness. Even so, we will also conduct research on how to build complex attack like Auto Attack, but in the setting of multiple inputs, in future work. Thanks again for such a valuable suggestion!\n\n2. **Table 1 only consider single-mode attacks (#a,#v)**. Is the method proposed in this paper effective when **co-attacks (both modality attacks) exist**? \n\nThank you for your comment. In fact, **the adversarial results in Table 1 is actually the multi-modal attack, where both of the modalities are perturbed**. This multi-modal attack will be more effective than each single-modality attack of the same size. Moreover, **we also report results about additional multi-modal attacks**.  First, we apply the Multi-modal Embedding Attack (MEA) method [1], where the co-attack is designed based on the alteration of the joint representation rather than the prediction. Second, we also apply two multi-modal attack methods, introducing Multi-modal Gaussian Noise (MGN) and randomly Multi-modal Pixel Missing (MPM).  These experiments indicate that except for existing multi-modal attacks (FGM, PGD-$\\ell_2$), our method can also resist various types of multi-modal attacks. \n\n|                           |   JT   |    GB   |   OGM   |   PMR   |    AT   |   Mixup   |  MSEFM  | CRMT-JT (ours) | CRMT-Mix (ours)| CRMT-AT (ours) |\n|:-------------------------:|:-------:|:-------:|:-------:|:-------:|:-------:|:-------:|:-------:|:-------:|:--------:|:-------:|\n|     MGN     | 0.5254  | 0.4622  | 0.5269  | 0.4797  | 0.5240  | 0.5015  | 0.4775  | 0.5603  |  0.6337  | 0.6039  |\n| MPM | 0.3154  | 0.3445  | 0.4462  | 0.3278  | 0.3699  | 0.4673  | 0.2980  | 0.5073  |  0.5698  | 0.5480  |\n|            MEA            | 0.3401  | 0.4121  | 0.3626  | 0.4549  | 0.5879  | 0.4782  | 0.5000  | 0.5560  |  0.5654  | 0.6890  |\n\n[1] J. Zhang, Q. Yi, and J. Sang, \u201cTowards adversarial attack on vision- language pre-training models,\u201d in *Proceedings of the 30th ACM International Conference on Multimedia*, 2022, pp. 5005\u20135013."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5565/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700371632496,
                "cdate": 1700371632496,
                "tmdate": 1700371632496,
                "mdate": 1700371632496,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uuPktQCWhe",
            "forum": "XyrB1Ay44j",
            "replyto": "XyrB1Ay44j",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5565/Reviewer_zQSE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5565/Reviewer_zQSE"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies adversarial robustness for multi-modal learning by building a new lower bound for the perturbation radius through uni-modal margins and the Lipschitz constraint. Based on the proposed lower bound, a two-step adversarial training framework has been provided to improve the robustness of multi-modal learning. Experimental results on three benchmark datasets were provided regarding multiple attack methods, compared with several strong baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- **New findings**: While discussing a new lower bound with the Lipschitz constraint is nothing new for adversarial robustness, the proposed method provides theoretical and insightful analyses of how the attack on a preferred modality would impact the overall robustness. This is a practical and common problem in multi-modal integration, as one modality often dominates the others. \n- **Good presentation**: The reviewer enjoyed reading the presentation of the proposed method, where each step was well demonstrated with theoretical supports and clearly developed through proper treatments. One minor suggestion is to provide a pseudo `Algorithm` to outline the method, as well as add a `Remark` to better summarize and explain the training steps. \n- **Decent experiment design**: Despite some minor issues, the experiment is overall well-designed and sufficient by 1) comparing with two groups of strong baselines, 2) adopting multiple attack methods (e.g., FGM, PGD, and missing modality), and 3) providing detailed ablation study and model discussions."
                },
                "weaknesses": {
                    "value": "- **Missing implementation details**: I may have missed something; however, I did not find any implementation details about the multi-modal encoders. What are the backbones used in the experiment? Can the proposed method apply to different backbones? \n- **Unclear model-specific weights/classifiers**: The exact role of introducing model-specific weights $a^{(m)}$ is somewhat unclear to me. How will it be used to guide the orthogonal classifier of each modality? Also, it remains unclear to me how the proposed eventually gets the prediction result upon different modalities's classifiers. \n- **Lacking empirical evidence**: One main motivation of the proposed approach is one modality may be more vulnerable than the others.  While the adversarial accuracy (between uni-modal and multi-modal attacks) could support this observation empirically, it would be more convincing to provide more evidence that can be used to back-up the theoretical results, such as plotting the vulnerability indicator ($\\eta$) values, visualizing the perturbation radius over modalities, etc."
                },
                "questions": {
                    "value": "Please refer to the questions raised in the *Weakness* section. Plus, the reviewer is interested in the following questions:\n- Can the proposed method apply to multiple modalities larger than 2?\n- What's the selection criterion in choosing datasets for the experiment? \n- Are the provided theoretical results applicable to vision-text data? Any empirical evidence? \n- Could the proposed method be incorporated into the pre-trained multi-modal model (e.g., CLIP or BLIP)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5565/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699213010075,
            "cdate": 1699213010075,
            "tmdate": 1699636572141,
            "mdate": 1699636572141,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jmbfQuZjMY",
                "forum": "XyrB1Ay44j",
                "replyto": "uuPktQCWhe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5565/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5565/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response by authors"
                    },
                    "comment": {
                        "value": "Thank you for your valuable feedback and insightful comments! We respond to some concerns below:\n\n1. Missing implementation details: What are the **backbones** used in the experiment? **Apply to different backbones**?\n\nThank you for your comments. \n\na. As shown in Section 4.1 in the manuscript, we present that we use the widely used backbone ResNet18 as the uni-modal encoder, for Audio, Vision, and Optical Flow modalities. Then, the uni-modal representation is concatenated to form the multi-modal joint representation. We have updated the experiment section to clarify this setting.\n\nb. We conduct experiments on different backbones, including ResNet34 (Vision, V+Audio, A), and ResNet18 (Vision, V) + Transformer (Audio, A) on the Kinetic-Sounds dataset. These experiments show the improvement and the flexibility of our method across different backbones. \n\n|  ResNet34 (V)+ResNet34(A) |  Clean  | Missing #v | Missing #a |   FGM   |  PGD-$\\ell_2$ |\n|:--------:|:-------:|:---------:|:---------:|:-------:|:-------:|\n| JT  | 0.6424  |  0.4528   |  0.2471   | 0.3132  | 0.2863  |\n|  CRMT-JT (ours) | 0.7435  |  0.5269   |  0.5705   | 0.4978  | 0.4746  |\n\n|  ResNet18(V)+Transformer(A)  |  Clean  | Missing #v | Missing #a |   FGM   |  PGD-$\\ell_2$ |\n|:--------:|:-------:|:---------:|:---------:|:-------:|:-------:|\n| JT | 0.5538  |  0.3387   |  0.2703   | 0.2456  | 0.2100  |\n|  CRMT-JT(ours) | 0.5807  |  0.3721   |  0.3539   | 0.3067  | 0.2711  |\n\n\n2. The exact **role of introducing model-specific weights $a$** is somewhat unclear to me. How will it be used to guide the orthogonal classifier of each modality? Also, it remains unclear to me how the proposed eventually **gets the prediction** result upon different modalities' classifiers.\n\nThank you for your comments. The modality-specific weights can help the model identify the importance of each modality. As shown in Equation 8:\n\n\\begin{equation}\n\\begin{aligned}\n         \\tilde{h}_k(\\boldsymbol{x}) =  a_k^{(1)} \\tilde{W} _{k\\cdot}^{(1)} \\phi^{(1)} (\\boldsymbol{x}^{(1)}) +  a_k^{(2)}\\tilde{W} _{k\\cdot}^{(2)} \\phi^{(2)} (\\boldsymbol{x}^{(2)}) + \\tilde{b}_k, \n\\end{aligned}\n\\end{equation}\nThe $\\tilde{h} _k(\\boldsymbol{x})$ denote the logits score of $k$-th class. Since the valuable information between modalities is different, the weight $\\boldsymbol{a}^{(m)}$ can lead the model to focus on the more reliable modalities. Integrating these weights can enhance the model's ability to effectively utilize the most valuable information from each modality. And the orthogonal $\\tilde{W} _{k\\cdot}^{(m)}$ for each modality, is approximated through the weight normalization method [1]. After we obtain the classifier, we can further get the logits score using the equation above and further obtain the prediction result. We have improved our presentation in Section 3.4 to explain the design more precisely. \n\n[1] L. Huang, X. Liu, B. Lang, A. Yu, Y. Wang, and B. Li, \u201cOrthogonal weight normalization: Solution to optimization over multiple dependent stiefel manifolds in deep neural networks,\u201d in *Proceedings of the AAAI Conference on Artificial Intelligence*, vol. 32, no. 1, 2018.\n\n3. Evidences that **back-up the theoretical results**, such as plotting the vulnerability indicator (eta) values, and visualizing the perturbation radius over modalities.\n\nThank you for your suggestions. As shown in Figure 3 in the manuscript, we demonstrate how the ratio of the vulnerability indicator ($\\eta$) varies in our method. As shown in Figure 3(a), in AT method, there is also a large imbalance on vulnerable indicators, which is alleviated by our proposed CRMT method. Furthermore, to clearly explain this phenomenon, in revised Section B.4.1, we provide the heat map of the indicator $\\eta$ to represent the robustness of each uni-modality, where the smaller indicator means the modality is more robust. Meanwhile, we also provide the uni-modal perturbation radius to further verify this modality preference, where we list the percentage of safe samples that can always resist this size of perturbation. It can be seen that in the Kinetic-Sounds dataset, the audio modality is definitely more vulnerable than the vision modality, thus explaining the phenomenon in Figure 1 in the manuscript.  \n\n| Perturbation radius | 0.25    | 0.50     | 0.75    | 1.00       | 1.25    | 1.50     |\n|---------------------|---------|---------|---------|---------|---------|---------|\n|  Percentage of safe samples #v                 | 0.6366  | 0.6279  | 0.6214  | 0.6163  | 0.6068  | 0.6010  |\n| Percentage of safe samples #a                  | 0.4172  | 0.2573  | 0.1584  | 0.0996  | 0.0596  | 0.0422  |"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5565/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700370847664,
                "cdate": 1700370847664,
                "tmdate": 1700370847664,
                "mdate": 1700370847664,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NZkYV1WPgB",
                "forum": "XyrB1Ay44j",
                "replyto": "uuPktQCWhe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5565/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5565/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response by authors"
                    },
                    "comment": {
                        "value": "4. Apply method to **multiple modalities larger than 2**.\n\nThank you for your suggestion. We apply the experiments on UCF101 with three modalities (RGB, Optical Flow (OF), and RGB Frame Difference (FD)), with two training strategies, training from scratch and with ImageNet-pretrained ResNet18. The following results show that our method can be well extended to multiple modalities.  \n\n|   UCF101 (RGB+OF+FD)    | JT (Scratch) | CRMT-JT (Scratch, ours) | JT (Pretrained) | CRMT-JT (Pretrained, ours) |\n|:----------:|:--------------------------:|:-------------------------------:|:---------------:|:--------------------:|\n|    Clean   |           0.4490           |             0.5640              |      0.8312     |        0.8506        |\n|     FGM    |           0.4005           |             0.4567              |      0.2471     |        0.4138        |\n|   PGD_$\\ell_2$   |           0.3963           |             0.4312              |      0.0783     |        0.2623        |\n\n\n5. The **selection criterion** in **choosing datasets** for the experiment.\n\nIn this study, we analyze the robustness through the perturbation radius, where the perturbation inside this radius can be always defended. Hence, our methods have no special requirements for certain modalities. Following previous studies in multi-modal learning [1,2], we apply the universally used Kinetic-Sounds, UCF101, and VGG-Sounds datasets. Our method can also extend to more modalities like the text to enhance the robustness. \n\n[1] J. Chen and C. M. Ho, \u201cMm-vit: Multi-modal video transformer for compressed video action recognition,\u201d in *Proceedings of the IEEE/CVF winter conference on applications of computer vision*, 2022, pp. 1910\u20131921.\n\n[2] X. Peng, Y. Wei, A. Deng, D. Wang, and D. Hu, \u201cBalanced multimodal learning via on-the-fly gradient modulation,\u201d in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2022, pp. 8238\u20138247.\n\n\n6. Are the provided theoretical results applicable to **vision-text data**? Any empirical evidence?\n\nThank you for your suggestion. We apply experiments on a vision-text dataset Food101, which inputs an image-text pair for classification. We employ a Vision Transformer (ViT) as our image encoder and BERT as our text encoder, subsequently concatenating their outputs to achieve a unified joint representation. To evaluate robustness, we apply multiple attacks including modality missing and descent-based attacks (FGM and PGD-$\\ell_2$). It is important to note that attacks like the Fast Gradient Method (FGM) and Projected Gradient Descent with $\\ell_2$ norm (PGD-$\\ell_2$) are typically applied to continuous data. Given that text represents discontinuous data, we focus on implementing these attack methods on the image modality. Our results reveal that the text modality is more critical than the image modality, as its absence significantly impacts model performance. Concentrating on the $\\ell_2$-norm, we achieve enhanced robustness under both FGM and PGD-$\\ell_2$ attacks. Our method demonstrates a notable performance increase in scenarios where text is absent, though there is a slight decline in performance when the image modality is missing. This could be attributed to the huge performance difference among text and image. This is a very interesting and challenging issue in building a robust model for both modalities, which we will focus on in the upcoming work. Thanks again for such a valuable suggestion!\n\n\n| Food101 |  Clean  | FGM on Image | PGD-$\\ell_2$ on Image | Missing Text | Missing Image |\n|:-------:|:-------:|:------------:|:---------------------:|:------------:|:-------------:|\n|    JT   | 0.8218  |    0.7603    |        0.7281         |    0.0497    |    0.7831     |\n| CRMT-JT | 0.8257  |    0.7656    |        0.7313         |    0.0759    |    0.7779     |\n\n7. Could the proposed method be **incorporated into the pretrained multi-modal model** (e.g., CLIP or BLIP)?\n\nCLIP and BLIP stand out as highly effective multi-modal models, with their robustness gaining considerable attention in recent studies. Notably, the CLIP models also prefer certain modalities, such as text [1]. Hence, it is expected to first analyze the potential influence of such preference within CLIP with our proposed approach. However, there is a slight difference in that CLIP (also BLIP) employs contrastive loss across modalities instead of discriminative learning over joint representation. In future work, we will attempt to theoretically analyze the potential influence of this, thanks again for the precious suggestion, which undoubtedly helps to improve the potential value of this work.\n\n[1] Z. Liu, C. Xiong, Y. Lv, Z. Liu, and G. Yu, \u201cUniversal vision-language dense retrieval: Learning a unified representation space for multi-modal retrieval,\u201d in *The Eleventh International Conference on Learning Representations*, 2022."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5565/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700371173723,
                "cdate": 1700371173723,
                "tmdate": 1700371173723,
                "mdate": 1700371173723,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Ky9FfK2aQH",
            "forum": "XyrB1Ay44j",
            "replyto": "XyrB1Ay44j",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5565/Reviewer_yeWY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5565/Reviewer_yeWY"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors tackle the challenge of improving the robustness of multi-modal models against perturbations, such as uni-modal attacks and missing modalities. They provide valuable theoretical insight, emphasizing the importance of larger uni-modal representation margins and reliable integration for modalities in achieving higher robustness. They introduce a training procedure, Certifiable Robust Multi-modal Training (CRMT), which effectively addresses modality preference imbalances and enhances multi-modal model robustness. Experimental results validate the superiority of CRMT in comparison to existing methods, demonstrating its versatility and effectiveness. Overall, this paper contributes to the field by providing a theoretical foundation and a practical method for enhancing the robustness of multi-modal models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Overall, the paper advances multi-modal robustness understanding and presents a practical solution in CRMT with strong empirical results, and the potential for broader applications in the ML/Multimodal community.\n- The paper offers a fresh perspective on multi-modal robustness, emphasizing the importance of larger uni-modal representation margins and reliable integration within a joint multi-modal framework.\n- The research is methodologically sound, with well-designed experiments and clear presentation.\n- The authors effectively communicate complex concepts, enhancing accessibility."
                },
                "weaknesses": {
                    "value": "The paper included some results on transformer as fusion models, particularlly the Multi-Modal Transformer-based framework with hierarchical attention on the VGGS dataset. However, all experriments, especailly the one with transformerr adopt training from scratch and did not consider any pre-training strategies, such as uni-modal pretraining, or multi-modal pretraining. It will be interesting to consider such methods as baselines and also to see how much CRMT can help to improve. \n\nAlso, except for experimenting, it will be good if authors can discuss how does their method generalize to other fusion mechanisms, besides late fusion."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5565/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699218807559,
            "cdate": 1699218807559,
            "tmdate": 1699636572030,
            "mdate": 1699636572030,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "51IvKcVqbt",
                "forum": "XyrB1Ay44j",
                "replyto": "Ky9FfK2aQH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5565/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5565/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response by authors"
                    },
                    "comment": {
                        "value": "Thanks for your valuable feedback and insightful comments! We respond to some concerns below:\n\n1. Transformer adopted training from scratch and did not consider any **pretraining strategies**.\n\nThank you for your advice on our work. To validate it, we conduct experiments using the ImageNet-pretrained Transformer on the Kinetic-Sounds dataset. Experimental results demonstrate that our method can also perform well with pretraining.\n \n| Transformer |  clean  |   FGM   |  PGD-$\\ell_2$ |\n|:-----------:|:-------:|:-------:|:-------:|\n|   Baseline  | 0.6788  | 0.1366  | 0.0865  |\n|   CRMT (ours)   | 0.7406  | 0.3198  | 0.2078  |\n\n2. How does the method **generalize to other fusion mechanisms**, besides late fusion.\n\nThank you for your comments. In this manuscript, our analysis mainly focuses on the representative fusion strategy, late fusion, which is widely used in multi-modal research [1,2]. \nMeanwhile, our method are adaptable to other fusion mechanisms, where the modality-specific representations could interact earlier in the process. Previously, we defined the margin as $\\zeta^{(m)}_j(\\boldsymbol{x}^{(m)})$, where $j$ is the nearest class to calculate the margin, and the margin is only determined by uni-modal input $\\boldsymbol{x}^{(m)}$. To adapt our method for these scenarios including intermediate fusion, we can redefine the representation margin as $\\zeta^{(m)}_j(\\boldsymbol{x}^{(1)}, \\boldsymbol{x}^{(2)})$, indicating that both modalities' input influence the margin. This modification allows us to extend the framework to measure multi-modal perturbations in a more integrated manner. Additionally, we can adapt the definition of the Lipschitz constant in our theoretical analysis here to measure how the multi-modal perturbation influences the margin:\n\n\\begin{equation}\n\\begin{aligned}\n|\\zeta_j^{(m)}(\\boldsymbol{x}^{(1)}, \\boldsymbol{x}^{(2)}) - \\zeta_j^{(m)}(\\boldsymbol{x}'^{(1)}, \\boldsymbol{x}'^{(2)})| \\le \\tau_j^{(m1)}\\left\\|\\left\\| \\boldsymbol{x}^{(1)} - \\boldsymbol{x}'^{(1)} \\right\\|\\right\\|_2 + \\tau_j^{(m2)}\\left\\|\\left\\| \\boldsymbol{x}^{(2)} - \\boldsymbol{x}'^{(2)} \\right\\| \\right\\|_2\n\\end{aligned}\n\\end{equation}\nwhere $\\tau_j^{(m1)}$ represents the Lipschitz constant of modality $m$ from modality $1$. This constant can reflect how the alteration in modality $1$ influences the margin in modality $m$. Then following the proof in the manuscript, we can observe that:\n \n\\begin{equation}\n\\begin{aligned}\n      c_j^{(1)} |\\zeta_{j}^{(1)}(\\boldsymbol{x}^{(1)}, \\boldsymbol{x}^{(2)}) - \\zeta_{j}^{(1)}(\\boldsymbol{x}'^{(1)}, \\boldsymbol{x}'^{(2)})|+ c_j^{(2)} |\\zeta_{j}^{(2)}(\\boldsymbol{x}^{(1)}, \\boldsymbol{x}^{(2)}) - \\zeta_{j}^{(2)}(\\boldsymbol{x}'^{(1)}, \\boldsymbol{x}'^{(2)})| \\\\\\\\         \\leq  (c_j^{(1)} \\tau_j^{(11)} + c_j^{(2)} \\tau_j^{(21)})\\left\\|\\left\\|\\boldsymbol{x}^{(1)} - \\boldsymbol{x}'^{(1)}\\right\\|\\right\\|_2 + (c_j^{(1)} \\tau_j^{(12)} + c_j^{(2)} \\tau_j^{(22)})\\left\\|\\left\\|\\boldsymbol{x}^{(2)} - \\boldsymbol{x}'^{(2)}\\right\\|\\right\\|_2 .\n\\end{aligned}\n\\end{equation}\nThus, we can obtain the perturbation bound in this setting:\n\n\\begin{equation}\n\\begin{aligned}\n & \\min_{\\boldsymbol{x}'} \\left\\|\\left\\|\\boldsymbol{x} - \\boldsymbol{x}'\\right\\|\\right\\|_2 \n \\geq \\frac{c_j^{(1)} \\zeta_j^{(1)}(\\boldsymbol{x}^{(1)}, \\boldsymbol{x}^{(2)}) + c_j^{(2)} \\zeta_j^{(2)}(\\boldsymbol{x}^{(1)}, \\boldsymbol{x}^{(2)})+ \\beta_j}{\\sqrt{(c_j^{(1)} \\tau_j^{(11)} + c_j^{(2)} \\tau_j^{(21)})^2 +(c_j^{(1)} \\tau_j^{(12)} + c_j^{(2)} \\tau_j^{(22)})^2 }} \\\\\\\\\n  whe&re   \\quad j \\neq y \\quad s.t. \\quad  c_j^{(1)} \\zeta_j^{(1)}(\\boldsymbol{x}'^{(1)}, \\boldsymbol{x}'^{(2)}) + c_j^{(2)} \\zeta_j^{(2)}(\\boldsymbol{x}'^{(1)}, \\boldsymbol{x}'^{(2)}) + \\beta_j = 0.\n\\end{aligned}\n\\end{equation}\n\nThe idea of the proof is similar to the one in Section A.1 in the Appendix. \n\nAdditionally, we present an experiment where our method is applied to intermediate fusion using the widely adopted MultiModal Transfer Module (MMTM) [3] on the Kinetic-Sounds dataset. The results demonstrate that our approach can enhance the robustness of intermediate fusion mechanisms.\n\n|   Kinetic-Sounds   |  Clean  | Missing #v | Missing #a |   FGM   |  PGD-$\\ell_2$ |\n|:--------:|:-------:|:---------:|:---------:|:-------:|:-------:|\n| MMTM | 0.6693  |  0.4542   |  0.2028   | 0.3438  | 0.3205  |\n|   CRMT-MMTM (ours)   | 0.6737  |  0.5211   |  0.3169   | 0.3445  | 0.3358  |\n\n\n[1] Y. Huang, J. Lin, C. Zhou, H. Yang, and L. Huang, \u201cModality competition: What makes joint training of multi-modal network fail in deep learning?(provably),\u201d *arXiv preprint arXiv:2203.12221*, 2022. \n\n[2] Y. Huang, C. Du, Z. Xue, X. Chen, H. Zhao, and L. Huang, \u201cWhat makes multi-modal learning better than single (provably),\u201d *Advances in Neural Information Processing Systems*, vol. 34, 2021.\n\n[3] H. R. Vaezi Joze, A. Shaban, M. L. Iuzzolino, and K. Koishida, \u201cMmtm: Multimodal transfer module for cnn fusion,\u201d in *Conference on Computer Vision and Pattern Recognition* (CVPR), 2020."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5565/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700369567153,
                "cdate": 1700369567153,
                "tmdate": 1700369567153,
                "mdate": 1700369567153,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]