[
    {
        "title": "EPIC: Compressing Deep GNNs via Expressive Power Gap-Induced Knowledge Distillation"
    },
    {
        "review": {
            "id": "gqgWAi5q1L",
            "forum": "04UvXg4CvW",
            "replyto": "04UvXg4CvW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7817/Reviewer_8ZML"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7817/Reviewer_8ZML"
            ],
            "content": {
                "summary": {
                    "value": "This paper theoretically and experimentally analyzes the expressive power gap between deep GNN teachers and lightweight students. It formulates the estimation of the expressive power gap to an embedding regression problem. The results on three large-scale datasets demonstrate the effectiveness of the proposed method. Overall, the paper is well written with an interesting topic."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The analysis of graph knowledge distillation from the perspective of expressive power is novel.\n- The paper is well written and presented, especially about the notation and background.\n- EPIC outperforms 1001-layer GNNs with a 4-layer GNN."
                },
                "weaknesses": {
                    "value": "- My main concern is the motivation for the paper. Although deep GNNs have been a popular research topic, deep GNNs are prone to suffer from overfitting and over-smoothing and so far have not been widely used as shallow GNNs. I am confused about the significance of studying distillation for deep GNN.\n- The authors did not conduct their experiments in a transductive setting as GLNN and NOSMOG did.\n- Why does the performance of NOSMOG drop so much in Table 1? According to their original paper, NOSMOG can achieve performance even better than the teacher GNN.\n- Can the authors provide more results on the different layers of teacher GNN in Table 1? In particular, I would like to know how generalizable EPIC is in a shallow teacher GNN setting?\n- As a GNN-to-GNN distillation method, the authors mainly compare EPIC with two GNN-to-MLP methods (GLNN and NOSMOG) in their experiments. Comparisons with some SOTA GNN-to-GNN distillation methods are missing, such as those introduced by the authors in related work."
                },
                "questions": {
                    "value": "Please explain (3-5) in the weakness part.\nHow about the time complexity (especially the bound estimation) and running time?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics review is needed."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7817/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697511476125,
            "cdate": 1697511476125,
            "tmdate": 1699636956965,
            "mdate": 1699636956965,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UbV6KmoC3K",
                "forum": "04UvXg4CvW",
                "replyto": "gqgWAi5q1L",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7817/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7817/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 8ZML (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the insightful and valuable comments. We respond to your comments as follows and sincerely hope that our rebuttal could properly address your concerns. If so, we would deeply appreciate it if you could raise your score. If not, please let us know your further concerns, and we will continue actively responding to your comments and improving our submission.\n\n**1. The motivation for studying the distillation for deep GNNs.**\n\nThanks for your comments.\n\nWe summarize the motivation for studying the distillation for deep GNNs as follows.\n\n- First, on large-scale datasets (e.g., ogbn-arxiv, ogbn-products, and ogbn-proteins), the deep teacher models that we distill in our paper (i.e., RevGCN-Deep, RevGNN-112, and RevGNN-Deep) outperform what existing KD4GNN works aim to distill, such as GCN, GraphSAGE, GAT, and APPNP. Compared with shallow GNNs, deep GNNs with more message passing layers are better at capturing long-range patterns [1, 2], which are critical for many downstream tasks such as search engine [3], recommendation systems [4], and molecular property prediction [5].\n\n- Second, although some deep GNNs still face challenges such as over-fitting and over-squashing, there have been many efforts to solve these challenges, such as [6, 7, 8, 9, 10, 11]. We believe that the challenges are temporary obstacles to the development of deep GNNs, and we believe that deep GNNs will receive widespread attention in the future.\n- Last but not least, the slow inference speed is also one of the important factors limiting the real applications of deep GNNs. Despite the great success in compressing and accelerating moderate-sized or shallow GNNs of existing KD techniques, it still remains a tough challenge to distill deep GNNs due to the huge expressive power gap between the teachers and students. Therefore, this paper aims to address the challenge of distilling deep GNNs that excel on large-scale graphs and possess numerous layers.\n\n[1] Chen, Tianlong, et al. \"Bag of tricks for training deeper graph neural networks: A comprehensive benchmark study.\" *IEEE Transactions on Pattern Analysis and Machine Intelligence* 45.3 (2022): 2769-2781.\n\n[2] Cong, Weilin, Morteza Ramezani, and Mehrdad Mahdavi. \"On provable benefits of depth in training graph convolutional networks.\" *Advances in Neural Information Processing Systems* 34 (2021): 9936-9949.\n\n[3] Brin, Sergey, and Lawrence Page. \"The anatomy of a large-scale hypertextual web search engine.\" *Computer networks and ISDN systems* 30.1-7 (1998): 107-117.\n\n[4] Fan, Wenqi, et al. \"Graph neural networks for social recommendation.\" *The world wide web conference*. 2019.\n\n[5] Kearnes, Steven, et al. \"Molecular graph convolutions: moving beyond fingerprints.\" *Journal of computer-aided molecular design* 30 (2016): 595-608.\n\n[6] Li, Guohao, et al. \"Training graph neural networks with 1000 layers.\" *International conference on machine learning*. PMLR, 2021.\n\n[7] Chen, Ming, et al. \"Simple and deep graph convolutional networks.\" *International conference on machine learning*. PMLR, 2020.\n\n[8] Zhao, Lingxiao, and Leman Akoglu. \"Pairnorm: Tackling oversmoothing in gnns.\" *arXiv preprint arXiv:1909.12223* (2019).\n\n[9] Guo, Xiaojun, et al. \"Contranorm: A contrastive learning perspective on oversmoothing and beyond.\" *arXiv preprint arXiv:2303.06562* (2023).\n\n[10] Jaiswal, Ajay, et al. \"Old can be gold: Better gradient flow can make vanilla-gcns great again.\" *Advances in Neural Information Processing Systems* 35 (2022): 7561-7574.\n\n**2. The experiments in the transductive setting.**\n\nThanks for your comments. We conduct experiments in the setting of transductive training. The results are as follows.\n\n| Datasets | Teacher | LSP   | GLNN  | NOSMOG | EPIC (Ours) |\n| -------- | ------- | ----- | ----- | ------ | ----------- |\n| Arxiv    | 72.96   | 69.77 | 57.41 | 65.79  | 73.33       |\n| Products | 82.11   | 78.31 | 61.45 | 64.62  | 83.73       |\n| Proteins | 87.42   | -     | 75.19 | 73.84  | 87.13       |\n\n**3. The drop of the performance of NOSMOG compared to that reported in the original paper.**\n\nThanks for your comments.\n\nWe train MLPs using the source code of NOSMOG in the inductive setting. We speculate that the reasons for the drop of performance of NOSMOG compared to that reported in the original paper are as follows.\n\nIn the original paper, the authors train NOSMOG in the transductive setting and a loose inductive setting (with only 20% of test nodes invisible to the models). In these settings, the teacher model's predictions on most test nodes are available to students, hence the students can directly fit the predictions of their teachers on test nodes to achieve comparable or even superior performance to the teachers. However, we conduct experiments in the strictly inductive setting, which is more challenging for student MLPs than the experiment settings in the original paper."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7817/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700594459076,
                "cdate": 1700594459076,
                "tmdate": 1700594459076,
                "mdate": 1700594459076,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SlwpZ4hfjF",
                "forum": "04UvXg4CvW",
                "replyto": "gqgWAi5q1L",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7817/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7817/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We are looking forward to your further comments."
                    },
                    "comment": {
                        "value": "Dear Reviewer 8ZML,\n\nThank you again for your careful reading and insightful comments, which are of great significance for improving our work. The deadline for the discussion stage is approaching, and we are looking forward to your feedback and/or questions. We sincerely hope that our rebuttal has properly addressed your concerns. If so, we would deeply appreciate it if you could raise your score. If not, please let us know your further concerns, and we will continue actively responding to your comments and improving our submission.\n\nBest,\n\nAuthors"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7817/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733233106,
                "cdate": 1700733233106,
                "tmdate": 1700733233106,
                "mdate": 1700733233106,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bkcHNPfMqB",
            "forum": "04UvXg4CvW",
            "replyto": "04UvXg4CvW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7817/Reviewer_ZSeG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7817/Reviewer_ZSeG"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on the knowledge distillation of deep GNNs, such as that with over 100 layers. To address the issue of the representation ability gap between teacher and student networks, this paper proposes a framework, EPIC, leveraging the embedding regression based on the theory of polynomial approximation. Specifically, EPIC first shows the EPIC bound exponentially converges as the number of student layers increases with experiments. Then, with this observation, it selects an appropriate layer number of the student network by analyzing the EPIC bound.  Furthermore, an expressive power gap-induced loss term is proposed to reduce the gap. In the experiments, it reduces 94% of layers, improves 8x speed, and obtains comparable performance for 1,001-layer RevGNN-Deep."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The field of distilling a very deep GNN model is less explored. By distilling, the inference speed can be improved by a large margin, which benefits the potential applications.\n2. A theoretical framework is proposed to analyze the gap between the student and teacher network from a spectral perspective. \n3. The theoretical results with an EPIC bound are used to select the appropriate layer number. It is interesting and meaningful to reduce the cost of the hyper-parameter tuning."
                },
                "weaknesses": {
                    "value": "The main concerns lie in the method details and experimental settings. The details are shown below:\n1. The hyper-parameter $\\gamma$ in Eq. (8) weakens (somehow) the theoretical results. The optimal # layers $M^*$ are dependent on this hyper-parameter, regardless of the EPIC bound. How to choose $\\gamma$, is there any guidance?   Does it involve the extra hyperparameter tuning cost?  When comparing tuning M (layer number) and $\\gamma$, what is the advantage of the latter?\n2. The scalability is also a concern, since it requires eigenvalue decomposition to find out all the eigenvalues, incurring high memory and time complexity, especially for large-scale graphs, which is the setting in this work. It is suggested to add some experimental cost to the process of calculating EPIC bounds. \n3. The singular value decomposition (SVD) and Laplacian requires the assumptions of linearity.  I understand that this may be easier for derivation. However, it still would be helpful if you could explain why this linear-based theory could be used for non-linear graph models, such as GCN and GAT.  It seems that the used teacher model is also non-linear. \n4. Novelty concerns on the EPIC loss.  The paper claims the new EPIC loss (expressive power gap-induced loss). However, If I understand correctly, H_S and H_T are representations of student and teacher models respectively. It seems the same as standard knowledge-distilling loss where the embeddings in the student network mimic the ones in the teacher network. It is suggested to mention this and avoid the new terms if they are similar. Otherwise, it is better to clarify the differences and provide corresponding evidence (insights and experimental results.)\n5. There are some concerns in the experiment design. First, the comparison needs to be more fair. As for GLNN and NOSMOG, the student model is MLP, while for EPIC, the student model is a GNN, the performance gain may be caused by the enhanced expressivity of student models. Second, this experiment can't be used to verify the correctness of the proposed theorem (i.e., is this bound tight when deciding the # of layers in student GNNs). Do the results of the real experiments align with the curves in Figure 1?  I suggest redesigning this experiment to stress the correctness of the proposed theorem. Third, It is suggested to add baselines that train the GNN with fewer layers from scratch.  Again, the cost of hyper-parameter $\\gamma$ is an issue. \n6. (Minor), It is suggested to refine Figures 1 and 2.  For example, it is better to provide more details in Figure 2. Currently, the details of EPIC and Bound and EPIC loss are lost."
                },
                "questions": {
                    "value": "1) Regard the $\\gamma$. How to choose $\\gamma$ in the experiments? Does it involve the extra hyperparameter tuning cost (How much)?  When comparing tuning M (layer number) and $\\gamma$, what is the advantage of the latter?\n\n2) What is the experimental cost of the calculating process of EPIC bounds?\n\n3) Could you please explain why the proposed linear-based theory could be used for non-linear graph models?\n\n4) What is the unique contribution of the proposed EPIC loss?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7817/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698625221365,
            "cdate": 1698625221365,
            "tmdate": 1699636956855,
            "mdate": 1699636956855,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ro1f6VPsyL",
                "forum": "04UvXg4CvW",
                "replyto": "bkcHNPfMqB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7817/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7817/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ZSeG"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the insightful and valuable comments. We respond to your comments as follows and sincerely hope that our rebuttal could properly address your concerns. If so, we would deeply appreciate it if you could raise your score. If not, please let us know your further concerns, and we will continue actively responding to your comments and improving our submission.\n\n**1. The selection of the hyperparameter $\\gamma$ and the corresponding cost.**\n\nThanks for your comments.\n\nAfter reading your comments, we remove the hyperparamter $\\gamma$ and instead use the EPIC bounds to narrow the range of tuning the number of student layers as follows.\n\n1. We compute the EPIC bounds $(u_{\\rm EPIC}(M;L))_{M=1}^{L-1}$ .\n2. We plot $u_{\\rm EPIC}(M;L)$ as a function of $M$, as shown in Figure 1.\n3. By observing the plot, we find the maximum value $M_{\\max}$ such that $u_{\\rm EPIC}(M;L)$ decreases slowly when $M>M_{\\max}$.\n4. We tune the hyperparamter $M$ in $[1, M_{\\max}]$.\n\nBecause $u_{\\rm EPIC}(M;L)$ converges exponentially to zero as $M$ increases, $M_{\\rm max} $ is usually much smaller than $L$. Thus, we can significantly reduce the range of tuning $M$.\n\n**2. The complexity of computing the EPIC bound is high.**\n\nThanks for your comments.\n\nPlease note that we do not need to compute EPIC bounds in the inference stage of GNNs, hence the computation of EPIC bounds do not affect the inference speed of GNNs, which is the focus of this paper and most knowledge distillation methods.\n\n**3. The relationship of our linear-based theory to non-linear graph models.**\n\nThanks for your comments.\n\nThe reason why we analyze the expressive power of GNNs from a linear perspective is that this is a standard practice [1, 2], and that existing works [1, 3] have demonstrated that the theoretical analysis under the linear assumption is applicable to general non-linear GNNs.\n\n[1] Xu, Keyulu, et al. \"Optimization of graph neural networks: Implicit acceleration by skip connections and more depth.\" *International Conference on Machine Learning*. PMLR, 2021.\n\n[2] Wang, Xiyuan, and Muhan Zhang. \"How powerful are spectral graph neural networks.\" *International Conference on Machine Learning*. PMLR, 2022.\n\n[3] Thekumparampil, Kiran K., et al. \"Attention-based graph neural network for semi-supervised learning.\" *arXiv preprint arXiv:1803.03735* (2018).\n\n**4. The comparison between EPIC and existing GNNs-to-MLPs distillation frameworks is unfair.**\n\nThanks for your comments.\n\nBecause we focus on how to select an appropriate student that is lightweight, while expressive, and MLP is currently a popular student structure for distilling GNNs, we compare our proposed EPIC with state-of-the-art GNNs-to-MLPs distillation frameworks. As mentioned in Section 6.2, we attribute the reason for the unsatisfactory performance of GNNs-to-MLPs frameworks to the weak expressive power of MLPs. The experiments support our claim in Introduction that \"MLPs are not 'good students' for distilling deep GNNs\".\n\n**5. Experiments of baselines that train the GNN with fewer layers from scratch.**\n\nThanks for your comments.\n\nWe conduct experiments of training student GNNs with fewer layers from scratch on the three OGB datasets. The results are as follows.\n\n| Arxiv  | 2     | 3     | 4     | 5     | 6     | 7     |\n| :------ | :-----: | ----- | ----- | ----- | ----- | ----- |\n| w/ KD  | 72.57 | 73.06 | 73.18 | 73.24 | 73.35 | 73.24 |\n| w/o KD | 72.25 | 72.91 | 73.00 | 72.65 | 72.34 | 72.15 |\n\n| Products | 2     | 3     | 4     | 5     | 6     | 7     |\n| :-------- | ----- | ----- | ----- | ----- | ----- | ----- |\n| w/ KD    | 76.80 | 76.99 | 77.70 | 78.32 | 78.26 | 78.58 |\n| w/o KD   | 76.67 | 76.86 | 76.85 | 77.29 | 77.56 | 77.71 |\n\n| Proteins | 10    | 20    | 30    | 40    | 50    | 60    |\n| -------- | ----- | ----- | ----- | ----- | ----- | ----- |\n| w/ KD    | 85.64 | 85.68 | 85.33 | 85.81 | 85.72 | 85.85 |\n| w/o KD   | 84.13 | 84.23 | 84.58 | 84.75 | 84.89 | 85.00 |"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7817/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700594152458,
                "cdate": 1700594152458,
                "tmdate": 1700594152458,
                "mdate": 1700594152458,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RvVSmC2nO0",
                "forum": "04UvXg4CvW",
                "replyto": "Ro1f6VPsyL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7817/Reviewer_ZSeG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7817/Reviewer_ZSeG"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response."
                    },
                    "comment": {
                        "value": "Thank you for your response, and I apologize for my delayed reply.\n\nUpon review, I find that the current revision does not adequately address my concerns. Specifically:\n\n1) Clarification on 'h' Removal: The implications of removing 'h' are not clear. Could you elaborate on how this affects the study's results?\n\n2) Rationale Behind Unfair Comparison: The justification for using an unfair comparison remains unconvincing. A more thorough explanation is needed to understand this choice.\n\n3) Overlooked Novelty Aspects: The issue of novelty in your study appears to have been overlooked. It's important to distinctly highlight what sets your research apart.\n\n4) Discrepancies in Revision: I did not find any changes in the revised paper that correspond to the issues raised in the response. \n\nGiven these unresolved issues, my evaluation of the paper remains unchanged."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7817/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700687847041,
                "cdate": 1700687847041,
                "tmdate": 1700687847041,
                "mdate": 1700687847041,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QzOoB7fURp",
            "forum": "04UvXg4CvW",
            "replyto": "04UvXg4CvW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7817/Reviewer_G7rf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7817/Reviewer_G7rf"
            ],
            "content": {
                "summary": {
                    "value": "The author(s) theoretically demonstrate that the shallow student model lacks sufficient expressive power to mimic the teacher;\nhence, distilling deep GNNs remains a tough challenge.\nThe derived upper bound allows for quantitative analysis of the gap, making it easy to determine an appropriate number of layers for student models."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Distilling very deep GNNs is challenging and important. \n2. This work provides some valuable suggestions on layer number selection."
                },
                "weaknesses": {
                    "value": "1. The conclusion obtained from Theorem 1 is obvious, and the upper bound derived therein appears difficult to apply in practice.\nWhile the hyperparameter $\\gamma$ can achieve this to some extent, the value of $\\gamma$ is not directly related to classification performance gap. Therefore, a search is also required to determine the appropriate value.\n2. The complexity of computing EPIC bounds should be analyzed (since SVD for large-scale graphs is expensive).\n3. Feature distillation (namely the EPIC loss presented in this paper) is a well-known technique."
                },
                "questions": {
                    "value": "Please refer to Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7817/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7817/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7817/Reviewer_G7rf"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7817/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698663519334,
            "cdate": 1698663519334,
            "tmdate": 1699636956726,
            "mdate": 1699636956726,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8BXWhBtlza",
                "forum": "04UvXg4CvW",
                "replyto": "QzOoB7fURp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7817/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7817/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer G7rf"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the insightful and valuable comments. We respond to your comments as follows and sincerely hope that our rebuttal could properly address your concerns. If so, we would deeply appreciate it if you could raise your score. If not, please let us know your further concerns, and we will continue actively responding to your comments and improving our submission.\n\n**1. The conclusion obtained from Theorem 1 is obvious.**\n\nThanks for your comments.\n\nTheorem 1 guarantees that we can distill a deep GNN into a shallow GNN, as there exist some shallow GNNs that have similar expressive power to the deep GNN. This conclusion is significant for the distillation of deep GNNs and is surprising in a way, as the numbers of neighbor hops included in the computation of each node in deep GNNs and shallow GNNs are very different.\n\nSpecifically, Theorem 1 shows that the minimum approximation error of $\\mathbf{H}_S^{(M)}$ to $\\mathbf{H}_T^{(L)}$ in terms of the Frobenius norm has an upper bound that decreases monotonically with respect to $M$. Furthermore, we empirically demonstrate that the upper bound converges exponentially to zero as $M$ increases.\n\n**2. How to apply the EPIC bound and select the hyperparameter $\\gamma$ in practice?**\n\nThanks for your comments.\n\nAfter reading your comments, we remove the hyperparamter $\\gamma$ and instead use the EPIC bounds to narrow the range of tuning the number of student layers as follows.\n\n1. We compute the EPIC bounds $(u_{\\rm EPIC}(M;L))_{M=1}^{L-1}$ .\n2. We plot $u_{\\rm EPIC}(M;L)$ as a function of $M$, as shown in Figure 1.\n3. By observing the plot, we find the maximum value $M_{\\max}$ such that $u_{\\rm EPIC}(M;L)$ decreases slowly when $M>M_{\\max}$.\n4. We tune the hyperparamter $M$ in $[1, M_{\\max}]$.\n\nBecause $u_{\\rm EPIC}(M;L)$ converges exponentially to zero as $M$ increases, $M_{\\rm max} $ is usually much smaller than $L$. Thus, we can significantly reduce the range of tuning $M$.\n\n**3. The complexity of computing the EPIC bound is high.**\n\nThanks for your comments.\n\nPlease note that we do not need to compute EPIC bounds in the inference stage of GNNs, hence the computation of EPIC bounds does not affect the inference speed of GNNs, which is the focus of this paper and most knowledge distillation methods."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7817/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700593798249,
                "cdate": 1700593798249,
                "tmdate": 1700593798249,
                "mdate": 1700593798249,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "a7b8m6ukE5",
                "forum": "04UvXg4CvW",
                "replyto": "QzOoB7fURp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7817/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7817/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We are looking forward to your further comments."
                    },
                    "comment": {
                        "value": "Dear Reviewer G7rf,\n\nThank you again for your careful reading and insightful comments, which are of great significance for improving our work. The deadline for the discussion stage is approaching, and we are looking forward to your feedback and/or questions. We sincerely hope that our rebuttal has properly addressed your concerns. If so, we would deeply appreciate it if you could raise your score. If not, please let us know your further concerns, and we will continue actively responding to your comments and improving our submission.\n\nBest,\n\nAuthors"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7817/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733187354,
                "cdate": 1700733187354,
                "tmdate": 1700733187354,
                "mdate": 1700733187354,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]