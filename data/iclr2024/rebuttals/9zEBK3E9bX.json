[
    {
        "title": "SPOT: Scalable 3D Pre-training via Occupancy Prediction for Autonomous Driving"
    },
    {
        "review": {
            "id": "9q3zp7yCkD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission496/Reviewer_FWLb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission496/Reviewer_FWLb"
            ],
            "forum": "9zEBK3E9bX",
            "replyto": "9zEBK3E9bX",
            "content": {
                "summary": {
                    "value": "The paper introduces SPOT (Scalable Pretraining via Occupancy prediction for learning Transferable 3D representations), a method for easing the annotation of 3D LiDAR point clouds, which is typically resource-intensive. The central idea is to leverage large-scale pre-training and then refine these models for different downstream tasks and datasets. Main contributions can be summarised as follows:\n\n- Occupancy Prediction: The paper underscores the potential of occupancy prediction as a means to learn general representations. The efficacy of this approach is validated through comprehensive experiments on numerous datasets and tasks.\n- Techniques for Point Cloud Augmentation: SPOT employs a beam re-sampling method to augment point clouds. It also implements class-balancing strategies to counteract the disparities arising from diverse LiDAR sensors and annotation methodologies across datasets.\n- Scalability of Pre-training: An interesting observation made is that as the amount of pre-training data increases, the performance on downstream tasks improves consistently."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper addresses a significant challenge in 3D LiDAR point cloud research, specifically the task of annotating 3D LiDAR point clouds for perception. The approach of performing large-scale pre-training and then fine-tuning the pre-trained backbone on various downstream datasets is novel.\n\n- Comprehensive experiments on multiple pre-training and downstream datasets (WOD, KITTI, SemanticKITTI, NuScenes, ONCE) are presented, along with thorough ablation studies.\n\n- The structure and writing of the manuscript are clear, making it easy to follow.\n\n- The figures, visualizations and illustrations are exemplary, with a particular appreciation for Fig. 1."
                },
                "weaknesses": {
                    "value": "- Some of the contributions highlighted by the authors appear to be not novel enough, e.g., the class-balancing strategies. Existing studies [1, 2], have showcased similar strategies. It would be good to acknowledge, cite, and compare their work with these prior studies.\n\n- In Sec. 4.1, the authors claim that \"our experiments are under label-efficiency setting.\" However, contemporary dataset subsampling techniques seem to encompass more than just the \"randomly selected\" method utilised in this paper. In fact, drawing from semi-supervised methodologies, the \"uniform sampling\" technique appears to be more prevalent. Moreover, Li et al. introduced the ST-RFD method, which aims to extract a more diverse subset of training data frame samples. I believe it would be beneficial for the authors to explore different sampling techniques and consider integrating the ST-RFD method to potentially achieve better downstream subset(s).\n\n- While the authors introduce a two-stage approach of first pre-training followed by fine-tuning, I'm still uncertain about the main advantages of that idea. For instance, current research employing semi-supervised or active learning techniques for point cloud semantic segmentation [3, 4] seems to achieve superior results  on 10% of the SemanticKITTI dataset (mIoU: 62.2 [3], mIoU: 60.0 [4]). Furthermore, Unal et al. [2] obtain a 61.3 mIoU on ScribbleKITTI (weakly annotated with 8% of the SemanticKITTI points). These methods also seem to efficiently utilise labels and alleviate the burden of extensive labelling.\n\n- Lack of related work, e.g., [1-4].\n\n[1] Zou, Y., Yu, Z., Kumar, B. V. K., & Wang, J. (2018). Unsupervised domain adaptation for semantic segmentation via class-balanced self-training. In Proceedings of the European conference on computer vision (ECCV) (pp. 289-305).\n\n[2] Unal, O., Dai, D., & Van Gool, L. (2022). Scribble-supervised lidar semantic segmentation. In\u00a0*Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*\u00a0(pp. 2697-2707).\n\n[3] Li, L., Shum, H. P., & Breckon, T. P. (2023). Less is more: Reducing task and model complexity for 3d point cloud semantic segmentation. In\u00a0*Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*\u00a0(pp. 9361-9371).\n\n[4] Kong, L., Ren, J., Pan, L., & Liu, Z. (2023). Lasermix for semi-supervised lidar semantic segmentation. In\u00a0*Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*\u00a0(pp. 21705-21715)."
                },
                "questions": {
                    "value": "Refer to Weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission496/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission496/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission496/Reviewer_FWLb"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission496/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697210478332,
            "cdate": 1697210478332,
            "tmdate": 1699635976346,
            "mdate": 1699635976346,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "S9cWw4Fuh9",
                "forum": "9zEBK3E9bX",
                "replyto": "9q3zp7yCkD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission496/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission496/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission496/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission496/Reviewers",
                    "ICLR.cc/2024/Conference/Submission496/Reviewers/Submitted"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission496/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer FWLb"
                    },
                    "comment": {
                        "value": "Dear Reviewer FWLb,\n\nThanks for your precious time and considerate suggestions to improve our manuscript.\n\n**Q1: Discussion about the class-balancing strategies with existing studies [1] [2].**\n\nA1:\u00a0 Thanks for your comments, both methods present excellent self-training category balancing methods. In the process of self-training, Class-balanced self-training (CBST) [1] sets different thresholds for different classes, and then balances classes according to the confidence of false tags, so that false tags with low confidence of rare classes can participate in the training. Based on the CBST, Class-range-balanced Self-training (CRB-ST) [2] adds range-based considerations,\u00a0 and sample pseudo-labels both from both category and range dimensions. Unlike them, we are mainly from a global perspective to alleviate the problem of class imbalance in the pre-training process. First, to ensure the integrity of occupancy labels in the scene, class sampling is based on frame-level. Secondly, considering that foreground information is more important than background information, and a large amount of background information can easily drown out scarce foreground information, we only rebalance for foreground categories, which can bring benefits to both downstream detection and segmentation tasks.\n\n**Q2: Discussion about using different sampling strategies in downstream task to potentially achieve better downstream subsets.**\n\nA2:\u00a0 We strongly agree with you that better sampling for downstream datasets leads to better performance. Since we do not use unlabeled data in the downstream dataset, we only perform fine-tuning experiments and fair comparison experiments on the same batch of randomly sampled data. Also, we have added some experimental results with uniform sampling. In the future, we will also integrate different sampling methods, such as the ST-RFD method [3], into the fine-tuning pipeline for better performance.\n\nTable 19. Different sampling strategies on downstream task.\n\n| Method | Sampling Strategy | F.D.A.  | mIOU |\n| --- | --- | --- | --- |\n| From Scratch (Cylinder3D) | random  | 10% NuScenes | 53.72 |\n| SPOT (Cylinder3D) | random  | 10% NuScenes | 56.10 |\n| From Scratch (Cylinder3D) | uniform  | 10% NuScenes | 52.96 |\n| SPOT (Cylinder3D) | uniform  | 10% NuScenes | 56.14 |\n\n**Q3: Discussion about task-specific semi-supervised learning/active learning [2,3,4].**\n\nA3:\u00a0 [2, 3, 4] are excellent semi-supervised or weakly supervised methods in the field of 3D point cloud segmentation, and they use the labeled data and other pseudo-labels of unlabeled data to achieve excellent performance, but most of these semi-supervised methods are task-specific or dataset-specific, e.g., they can only work in segmentation tasks or detection tasks. In contrast, SPOT focuses on the pre-training part, aiming to bring better transferable prior knowledge to all other tasks and datasets through just one pre-training process, which is decoupled from the downstream datasets and tasks. Moreover, we have not used downstream unlabeled data, meaning that we can combine SPOT with various semi-supervised or weakly-supervised methods in downstream fine-tuning to further improve the performance, by using SPOT as the downstream initialization. We think it's going to be a very effective combination.\n\n**Q4: Lack of related work, e.g., [1-4].**\n\nA4:\u00a0 Thank you for pointing these out! We has added all of them in the revision.\n\nWe hope our reply and additional experiment results address your concerns. Look forward to further discussion on any remaining concern about our paper.\n\nBest regards,\n\nAuthors of Paper 496 \n\n[1] Zou, Y., Yu, Z., Kumar, B. V. K., & Wang, J. (2018). Unsupervised domain adaptation for semantic segmentation via class-balanced self-training. In Proceedings of the European conference on computer vision (ECCV) (pp. 289-305).\n\n[2] Unal, O., Dai, D., & Van Gool, L. (2022). Scribble-supervised lidar semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 2697-2707).\n\n[3] Li, L., Shum, H. P., & Breckon, T. P. (2023). Less is more: Reducing task and model complexity for 3d point cloud semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 9361-9371).\n\n[4] Kong, L., Ren, J., Pan, L., & Liu, Z. (2023). Lasermix for semi-supervised lidar semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 21705-21715)."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission496/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700061709777,
                "cdate": 1700061709777,
                "tmdate": 1700061709777,
                "mdate": 1700061709777,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DUprGvFIpR",
            "forum": "9zEBK3E9bX",
            "replyto": "9zEBK3E9bX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission496/Reviewer_4p2k"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission496/Reviewer_4p2k"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a scalable pre-training method called SPOT for learning transferable representations for 3D perception tasks. SPOT pre-trains a model on the task of occupancy prediction, which is a general task that can be used to learn useful representations for various 3D perception tasks. To mitigate the gaps between pre-training and fine-tuning datasets, SPOT uses beam resampling augmentation and class-balancing strategies. The authors evaluate SPOT on various 3D perception tasks and datasets. SPOT outperforms training from scratch by a significant margin on all tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper is well-written and presents extensive studies on pre-training occupancy prediction on the Waymo Open Dataset and fine-tuning on Nuscenes, KITTI, ONCE, and SemanticKitti. The authors make the interesting observation that occupancy prediction outperforms detection pre-training, even on downstream detection tasks. This highlights the ability of occupancy prediction to mitigate the domain gap between pre-training and fine-tuning datasets."
                },
                "weaknesses": {
                    "value": "1. Unfair comparisons in Table 1 and 2: SPOT is compared against BEV-MAE and AD-PT, which are self-supervised and semi-supervised pre-training methods, respectively. This is an unfair comparison, as SPOT is a supervised pre-training method that benefits from human-labeled data. A more fair comparison would be to compare SPOT against other supervised pre-training methods.\n\n2. Supervised occupancy prediction is not new, and It is hard to argue scalability when pre-training is based on a labeled dataset.  On the other hand, self-supervised occupancy pretrainig was demonstrated in previous works, e.g. [1], which had shown unlabeled occupancy pretraining via MAE work. \n\n3. Another critical limitation on scalability is that the pre-training dataset must have a more expensive lidar setup (64-beam with high density) compared to the fine-tuning dataset. \n\n3. Low train from scratch baseline performance: As shown in Fig 7, with 100% nuScenes, training from scratch leads to mAP ~ 50, which is far from SOTA (>70) [2]. \n\n4.  Given the low train from scratch performance, could the baseline model be undertrained or that it is using a weaker data augmentation? \n\nPre-training 30 epochs on WOD requires a lot more computation resources compared to fine-tuning 30 epochs on NuScenes. \n\nIf you use the same computation resources ( as pre-training + fine-tuning) to train a model on NuScenes from scratch with a stronger data augmentation, I guess the performance gap between pre-training and training from scratch will be much smaller. \n\nAlso, previous studies have shown that with strong data augmentation during fine-tuning, the benefit from pretrianing diminishes [3]. \n\n[1] Min, Chen, et al. \"Occupancy-MAE: Self-Supervised Pre-Training Large-Scale LiDAR Point Clouds With Masked Occupancy Autoencoders.\" IEEE Transactions on Intelligent Vehicles (2023).\n\n[2] https://paperswithcode.com/sota/3d-object-detection-on-nuscenes\n\n[3] Zoph, Barret, et al. \"Rethinking pre-training and self-training.\" Advances in neural information processing systems 33 (2020): 3833-3845."
                },
                "questions": {
                    "value": "Please see weaknesses.\n\n\n\n-------------------------------------\nThank you for running the additional experiments. I increased my score. \n\nHowever, I cannot recommend acceptance \n1.  the improvement is marginal (only + 0.25 NDS) for a stronger baseline (VoxelNext), Table 17.\n2.  As reviewer y7L9 suggested, `the improvements and results showcased appear anticipated`."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission496/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission496/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission496/Reviewer_4p2k"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission496/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698607851087,
            "cdate": 1698607851087,
            "tmdate": 1700080905785,
            "mdate": 1700080905785,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IWT27orhYm",
                "forum": "9zEBK3E9bX",
                "replyto": "DUprGvFIpR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission496/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission496/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission496/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission496/Reviewers",
                    "ICLR.cc/2024/Conference/Submission496/Reviewers/Submitted"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission496/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 4p2k (Part1)"
                    },
                    "comment": {
                        "value": "Dear Reviewer 4p2k,\n\nThanks for your time and constructive suggestions! We provide additional experiments on semi-supervised pre-training via SPOT and make more fair comparisons to the baselines. Also a detailed discussion on the results are presented. \n\n**Q1:\u00a0Unfair\u00a0comparisons\u00a0with\u00a0BEV-MAE\u00a0and\u00a0AD-PT**\n\n**A1:** The motivation of SPOT is to pre-train a general backbone for different datasets and tasks, which is not achieved by BEV-MAE and AD-PT. For BEV-MAE, they are only able to pre-train and fine-tune on the same dataset. Tables 11, 12, 13 and 14 below show this phenomenon. For AD-PT, fine-tuning on semantic segmentation task harm the performance. Tables 12 and 13 show that when fine-tuning on LiDAR semantic segmentation task, AD-PT brings smaller improvement or even harm the performance.\n\nTo make more fair comparisons to AD-PT and also demonstrate the scalability of SPOT, we perform semi-supervised pre-training with SPOT. Results are provided in Tables 11, 12, 13 and 14 below. It can be found that pre-training with 5% sequence-level labeled data and 15% sequence-level unlabeled data achieves comparable\u00a0downstream\u00a0fine-tuning performance as that of pre-training with 20% sequence-level labeled data. Although 20% data in Waymo is much less than that used in AD-PT, it can be found that SPOT still achieve comparable or surpass AD-PT in various downstream datasets and tasks. We are also running experiments with more unlabeled data but due to the time limitation the experiments are not finished yet. We would update the results as soon as they finish. However, scalability of SPOT can also be discovered when we compare pre-trainings with different amount of unlabeled data (5% and 15%). It can be found that more unlabeled data brings better downstream performance. These results verify that (1) SPOT can learn general representations for different downstream datasets, tasks and architectures with small part of labels. (2) When incorporating more unlabeled data into pre-training phase, SPOT also shows scalability. \n\nTable 11. Comparison to different pre-training methods. The downstream task is 3D object detection on NuScenes. We provide additional semi-supervised pre-training via SPOT here.\n\n| Backbone    | Method | Pre-training Setting | Pre-training Data Amount | F.D.A.  | mAP  | NDS |\n| --- | --- | --- | --- | --- | --- | --- |\n| SECOND | From Scratch | - | - | 5% Nuscenes | 32.16 | 41.59 |\n| SECOND | BEV-MAE | Unsupervised | 100% | 5% Nuscenes | 32.09 | 42.88 |\n| SECOND | AD-PT | Semi-supervised | 100% | 5% Nuscenes | 37.69  | 47.95 |\n| SECOND | SPOT (SEMI) | Semi-supervised | 20% | 5% Nuscenes | 39.81 | 51.51 |\n| SECOND | SPOT | Fully-supervised | 20% | 5% Nuscenes | 39.63 | 51.63 |\n| CenterPoint | From Scratch | - | - | 5% Nuscenes | 42.37  | 52.01 |\n| CenterPoint | BEV-MAE | Unsupervised | 100% | 5% Nuscenes | 42.86  | 52.95 |\n| CenterPoint | AD-PT | Semi-supervised | 20% | 5% Nuscenes | 44.99  | 52.99 |\n| CenterPoint | SPOT (SEMI) | Semi-supervised | 20% | 5% Nuscenes | 45.18 | 54.98 |\n| CenterPoint | SPOT  | Fully-supervised | 20% | 5% Nuscenes | 44.94 | 54.95 |\n\nTable 12. Comparison to different pre-training methods. The downstream task is 3D object detection on KITTI. We provide additional semi-supervised pre-training via SPOT here.\n\n| Backbone    | Method | Pre-training Setting | Pre-training Data Amount | F.D.A.  | mAP  |\n| --- | --- | --- | --- | --- | --- |\n| SECOND | From Scratch | - | - | 20% KITTI | 61.70 |\n| SECOND | BEV-MAE | Unsupervised | 100% | 20% KITTI | 63.45 |\n| SECOND | AD-PT | Semi-supervised | 100% | 20% KITTI | 65.95 |\n| SECOND | SPOT (SEMI) | Semi-supervised | 20% | 20% KITTI | 65.45 |\n| SECOND | SPOT | Fully-supervised | 20% | 20% KITTI | 66.45 |\n| PV-RCNN | From Scratch | - | - | 20% KITTI | 66.71 |\n| PV-RCNN | BEV-MAE | Unsupervised | 100% | 20% KITTI | 69.91 |\n| PV-RCNN | AD-PT | Semi-supervised | 20% | 20% KITTI | 69.43 |\n| PV-RCNN | SPOT (SEMI) | Semi-supervised | 20% | 20% KITTI | 70.86 |\n| PV-RCNN | SPOT  | Fully-supervised | 20% | 20% KITTI | 70.85 |\n\nTable 13. Comparison to different pre-training methods. The downstream task is LiDAR semantic segmentation on NuScenes. We provide additional semi-supervised pre-training via SPOT here.\n\n| Backbone    | Method | Pre-training Setting | Pre-training Data Amount | F.D.A.  | mIOU |\n| --- | --- | --- | --- | --- | --- |\n| Cylinder3D | From Scratch | - | - | 5% Nuscenes | 45.85 |\n| Cylinder3D | BEV-MAE | Unsupervised | 100% | 5% Nuscenes | 46.94 |\n| Cylinder3D | AD-PT | Semi-supervised | 100% | 5% Nuscenes | 45.61 |\n| Cylinder3D | SPOT (SEMI) | Semi-supervised | 20% | 5% Nuscenes | 48.84 |\n| Cylinder3D | SPOT | Fully-supervised | 20% | 5% Nuscenes | 47.84 |"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission496/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700061316143,
                "cdate": 1700061316143,
                "tmdate": 1700061316143,
                "mdate": 1700061316143,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DPDf6tb2fn",
                "forum": "9zEBK3E9bX",
                "replyto": "DUprGvFIpR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission496/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission496/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission496/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission496/Reviewers",
                    "ICLR.cc/2024/Conference/Submission496/Reviewers/Submitted"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission496/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 4p2k (Part3)"
                    },
                    "comment": {
                        "value": "**Q5:\u00a0Low train from scratch baseline performance: As shown in Fig 7, with 100% NuScenes, training from scratch leads to mAP ~ 50, which is far from SOTA (>70).**\n\n**A5:** Thanks for pointing this out. Our motivation is to improve label-efficiency performance on various downstream tasks and datasets via **one** pre-trained model. We totally agree that it is also important to improve SOTA models. Most models that achieve >70 NDS performance on NuScenes Leaderboard is multi-modality methods with camera inputs but we focus on LiDAR perception. To demonstrate SPOT can improve SOTA model, we perform fine-tuning experiments with 100% full data on the VoxelNext [2] model, which is one of SOTA LiDAR-only models on NuScenes benchmark. And the results in Table 17 prove that SPOT can still achieve gains on SOTA models.\n\nTable 17. Fine-tune the pre-trained backbones for VoxelNext on 100% NuScenes 3D object detection dataset. \n\n| Method | P.D.A. | F.D.A.  | mAP | NDS |\n| --- | --- | --- | --- | --- |\n| From Scratch (VoxelNext) |  -  | 100% NuScenes | 60.53 | 66.65 |\n| SPOT  (VoxelNext) | 100% | 100% NuScenes | 61.05 | 66.90 |\n\n**Q6:\u00a0Could the baseline model be undertrained or that it is using a weaker data augmentation?**\n\n**A6:**  The data augmentations we used all follow the OpenPCDet settings, including gt sampling, random flip/rotate/translate, etc., which is a consistent approach for current 3D detection models. Besides, we conduct experiments on extending the training schedule on NuScenes dataset, and the results are shown as follows. From the results, it can be seen that even when the training schedule is extending by 5 times at least, the initialization by our pre-trained model can achieve higher performance. Especially for the Transformer-based DSVT detector, the result of just 20 epochs is even superior to the result of 150 epochs.\n\nTable 18. Extending training schedule on NuScenes dataset. \n\n| Method | P.D.A. | F.D.A.  | Training Schedule | mAP | NDS |\n| --- | --- | --- | --- | --- | --- |\n| From Scratch (SECOND) |  -  | 5% NuScenes | 30 epochs | 32.16  | 41.59 |\n| From Scratch (SECOND) |  -  | 5% NuScenes | 150 epochs | 36.79  | 51.01 |\n| SPOT (SECOND) | 20% | 5% NuScenes | 30 epochs | 39.63 | 51.63 |\n| SPOT (SECOND) | 100% | 5% NuScenes | 30 epochs | 42.57  | 54.28 |\n| From Scratch (CenterPoint) | - | 5% NuScenes | 30 epochs | 42.37 | 52.01 |\n| From Scratch (CenterPoint) | - | 5%\u00a0NuScenes  | 150 epochs | 41.01  | 53.92 |\n| SPOT (CenterPoint) | 20% | 5% NuScenes | 30 epochs | 44.94 | 54.95 |\n| SPOT (CenterPoint) | 100% | 5% NuScenes | 30 epochs | 47.47  | 57.11 |\n| From Scratch (DSVT) |  -  | 5% NuScenes | 20 epochs | 49.78 | 58.63 |\n| From Scratch (DSVT) |  -  | 5% NuScenes | 150 epochs | 54.30 | 63.58 |\n| SPOT (DSVT) | 20% | 5% NuScenes | 20 epochs | 56.65 | 63.52 |\n\nWe hope our additional experiment results and discussion address your concerns. And we are glad to discuss anything unclear about our paper.\n\nBest regards,\n\nAuthors of Paper 496\n\n[1] Chen Min, Liang Xiao, Dawei Zhao, Yiming Nie, and Bin Dai. Occupancy-mae: Self-supervised pre-training large-scale lidar point clouds with masked occupancy autoencoders. IEEE Transactions on Intelligent Vehicles, 2023.\n\n[2] Chen Y, Liu J H, Zhang X Y, et al. VoxelNeXt: Fully Sparse VoxelNet for 3D Object Detection and Tracking. arXiv 2023[J]. arXiv preprint arXiv:2303.11301."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission496/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700061474516,
                "cdate": 1700061474516,
                "tmdate": 1700061722929,
                "mdate": 1700061722929,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pUjHXN3H9H",
                "forum": "9zEBK3E9bX",
                "replyto": "DUprGvFIpR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission496/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission496/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission496/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission496/Reviewers",
                    "ICLR.cc/2024/Conference/Submission496/Reviewers/Submitted"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission496/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Updated Review (Part 1)"
                    },
                    "comment": {
                        "value": "Dear Reviewer ****4p2k,****\n\nThank you for the prompt reply and the acknowledgement of our additional experiments and discussions. We are writing to provide further discussion on your remaining concerns.\n\n**Q1: The seemingly marginal improvement for VoxelNext.**\n\nFirst of all, in our pre-training, both 3D and 2D (BEV) backbones are used for occupancy prediction but VoxelNext [1] only uses 3D backbone for encoding. Thus, we are only able to load weights of 3D backbone when fine-tuning on NuScenes, which losses useful information from pre-training. Under such circumstances, we also achieve 0.52\\% mAP improvement. It is a well-known phenomenon in previous works [2,3,4] that improving over SOTA models with 100% downstream labels is hard and 0.52\\% mAP is non-negligible improvement.\n\nTo further demonstrate the ability of SPOT to improve SOTA performance on NuScenes, we perform two additional experiments: (1) training VoxelNext on train and validation set and then test it on **Test** set. (2) fine-tuning on CenterPoint [5], which is another SOTA model on NuScenes, and it has both 3D and 2D backbone for embedding point clouds, leading to less information loss.\n\n(1) We train VoxelNext on train and validation set and then test it on **Test** set, which is a standard setting for **test set**. It can be seen from the following table that, although only pre-trained parameters of 3D backbone are loaded, SPOT can boost the performance of VoxelNext on **test set** by 1.0\\% mAP and 0.7\\% NDS, respectively. More than 200\\% improvement are gained compared to previous results and this improvement is significant on SOTA models as stated in previous works [2,3,4].\n\nTable. Fine-tune the pre-trained backbones for VoxelNext on 100% NuScenes 3D object detection trainval set and **evaluate on test set.** \n\n| Method | F.D.A.  | mAP | NDS |\n| --- | --- | --- | --- |\n| From Scratch (VoxelNext) | 100% NuScenes trainval Set | 64.5 | 70.0 |\n| SPOT  (VoxelNext) | 100% NuScenes trainval Set | **65.5** | **70.7** |\n\n(2) Furthermore, to avoid information loss, we employ the CenterPoint [5] (voxel_size=0.075), which utilizes both 3D and 2D backbones for encoding, for 3D object detection task on NuScenes. We load pre-trained weights for both 3D and 2D backbones from SPOT and fine-tune CenterPoint with full training data. In the Table below, it can be found that compared to training from scratch, initializing CenterPoint by SPOT brings 2.63\\% mAPs and 1.25\\% NDS improvement. Thus, without loss of pre-trained information, that is loading the full amount of pre-training parameters, SPOT is able to achieve far more significant performance improvement on SOTA models.\n\nTable. Fine-tune the pre-trained backbones for CenterPoint (voxel_size=0.075) on 100% NuScenes 3D object detection dataset. \n\n| Method | F.D.A.  | mAP | NDS |\n| --- | --- | --- | --- |\n| From Scratch (CenterPoint) | 100% NuScenes train set | 59.28 | 66.60 |\n| SPOT  (CenterPoint) | 100% NuScenes train set | **61.91** | **67.85** |\n\nWe hope these discussions and experiment results can cover your concerns on the improvement on SOTA models. Thanks.\n\n---\n\n---"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission496/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700213360758,
                "cdate": 1700213360758,
                "tmdate": 1700213360758,
                "mdate": 1700213360758,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "l0kJHM1KeP",
                "forum": "9zEBK3E9bX",
                "replyto": "DUprGvFIpR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission496/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission496/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission496/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission496/Reviewers",
                    "ICLR.cc/2024/Conference/Submission496/Reviewers/Submitted"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission496/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Look forward to further discussion."
                    },
                    "comment": {
                        "value": "Dear Reviewer 4p2k,\n\nThank you for your precious time on the review. As the deadline of discussion period is approaching, we sincerely hope that our response can address your concerns and we are looking forward to further discussion on any other questions or issues regarding the paper or our response.\n\nBest regards,\n\nAuthors of Paper 496"
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission496/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631584521,
                "cdate": 1700631584521,
                "tmdate": 1700631584521,
                "mdate": 1700631584521,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "fEV6rt330k",
            "forum": "9zEBK3E9bX",
            "replyto": "9zEBK3E9bX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission496/Reviewer_y7L9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission496/Reviewer_y7L9"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents SPOT (Scalable Pre-training via Occupancy Prediction), aimed at learning transferable 3D representations from LiDAR point clouds for autonomous driving tasks. SPOT leverages occupancy prediction as a pre-training task to learn general representations, employs beam re-sampling for point cloud augmentation, and class-balancing strategies to bridge domain gaps caused by varying LiDAR sensors and annotation strategies across different datasets. The authors extensively test SPOT across multiple datasets and 3D perception tasks, demonstrating its scalability and effectiveness."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Leveraging occupancy perception as a pretraining task is interesting.\n2. The tricks of beam re-sampling augmentation and class-balancing strategies are useful.\n3. The authors did large-scale experiments on five datasets."
                },
                "weaknesses": {
                    "value": "My major concern is that this pretraining is not self-supervised representation learning. Although the authors tout SPOT as a label-efficient solution, the labor-intensive nature of building a large-scale semantic occupancy dataset seems to contradict this claim. Furthermore, the improvements and results showcased appear anticipated, especially given the employment of the extensively annotated large-scale Waymo open dataset."
                },
                "questions": {
                    "value": "What will the results look like if only using binary occupancy labels without any human labeling? How can the pretraining be made fully self-supervised? Meanwhile, what will the results look like if pretraining on a different dataset (not Waymo)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission496/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission496/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission496/Reviewer_y7L9"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission496/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698649806214,
            "cdate": 1698649806214,
            "tmdate": 1699635976180,
            "mdate": 1699635976180,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ctrbWDv7Xw",
                "forum": "9zEBK3E9bX",
                "replyto": "fEV6rt330k",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission496/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission496/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission496/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission496/Reviewers",
                    "ICLR.cc/2024/Conference/Submission496/Reviewers/Submitted"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission496/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer y7L9 (Part 1)"
                    },
                    "comment": {
                        "value": "Dear Reviewer y7L9,\n\nThank you for your review. We conduct new experiments on semi-supervised pre-training and provide detailed analysis both in [General Response Part 2](https://openreview.net/forum?id=9zEBK3E9bX&noteId=mx7XgdJGbX) and here. We also provide experiment results on pre-training on other tasks and pre-training on NuScenes dataset via SPOT to cover your concerns. Besides, we provide discussion on supervised, semi-supervised and self-supervised pre-training here.\n\n**Q1: Label-intensive nature for pre-training via occupancy prediction.**\n\n**A1:** We thank you for acknowledging SPOT as a promising pre-training method for LiDAR perception and pointing out this aspect. We totally agree that using fewer or no labels for pre-training,\u00a0namely semi-supervised or\u00a0self-supervised\u00a0pre-training, is\u00a0essential and\u00a0more\u00a0practical. To address this concern, we conduct semi-supervised pre-training with SPOT. To be more specific, we\u00a0pre-train the backbones with SPOT using only 5% sequence-level labeled data and different amount of sequence-level unlabeled data (5% and 15%) via pseudo-labeling [1] and mean-teacher approach [2]. The results are shown in Tables 1, 2, 3, 4 and 5 in General Response Part 2.\n\nIt can be found in Tables 1, 2, 3, 4 and 5 that pre-training with 5% sequence-level labeled data and 15% sequence-level unlabeled data achieves comparable\u00a0downstream\u00a0fine-tuning performance as that of pre-training with 20% sequence-level labeled data. Meanwhile, when compared to pre-training with 5% sequence-level labeled data and 5% sequence-level unlabeled data, it can be found that **more unlabeled data brings better downstream performance**. These results verify that (1) SPOT can learn general representations for different downstream datasets, tasks and architectures with small part of labels. (2) When incorporating more unlabeled data into pre-training phase, SPOT also shows scalability.\n\nWe hope these results can cover your concerns on the label-intensive nature for fully-supervised pre-training via SPOT.\n\n**Q2: Given annotations on Waymo Dataset, the improvements and results are anticipated**\n\n**A2:** We would like to clarify\u00a0that\u00a0the\u00a0proposed\u00a0SPOT is\u00a0**a\u00a0more\u00a0general\u00a0pre-training\u00a0paradigm**. This\u00a0means\u00a0that\u00a0only one pre-trained is conducted via SPOT, and we applied the pre-trained backbones to improve performance in various datasets and tasks. As shown in Table 8 below, it can be found that pre-training via detection or segmentation\u00a0task\u00a0on\u00a0fully-annotated\u00a0Waymo\u00a0data suffers from two gaps when fine-tuned in downstream tasks (1) the gap between pre-training task and downstream task. Pre-training on semantic segmentation task harms the performance a lot. (2) the gap between different LiDARs used to collect datasets. When pre-training via detection task, the performance on downstream dataset also drops due to different sensors used in these two datasets. In comparison, SPOT achieves general improvements, which demonstrate the superiority of SPOT as pre-training task. \n\nTable 8. Pre-training with different tasks with annotations on Waymo Dataset and fine-tune the pre-trained backbones for 3D object detection on NuScenes dataset. \n\n| Pre-training\u00a0Task\u00a0 | P.D.A. | Downstream\u00a0Task | mAP | NDS |\n| --- | --- | --- | --- | --- |\n| From Scratch | - | 5%\u00a0NuScenes | 42.37 | 52.01 |\n| Detection\u00a0Task\u00a0on\u00a0Upstream | 100% | 5%\u00a0NuScenes | 40.89 | 49.75 |\n| Segmentation\u00a0Task\u00a0on\u00a0Upstream | 100% | 5%\u00a0NuScenes | 36.23 | 47.01 |\n| SPOT | 5%  | 5%\u00a0NuScenes | 43.56 | 53.04 |\n| SPOT (SEMI) | 5%  + Unlabeled 15% | 5%\u00a0NuScenes | 45.18 | 54.98 |\n\n**Q3: Using only binary occupancy labels for pre-training**\n\n**A3:** We conduct experiments where we pre-train backbones via SPOT with only binary occupancy labels. The results are as shown below:\n\nTable 9: Pre-training via SPOT with binary labels indicating Occupied and Empty.\n\n| Method | P.D.A. | F.D.A.  | mAP | NDS |\n| --- | --- | --- | --- | --- |\n| From Scratch (CenterPoint) |  -  | 5% NuScenes | 42.37 | 52.01 |\n| Binary occupancy pre-training (CenterPoint) | 20% | 5% NuScenes | 42.05 | 51.63 |\n| SPOT  (CenterPoint) | 20% | 5%\u00a0NuScenes  | 44.94  | 54.95 |\n\nIt can be found that pre-training only with binary labels harm the performance. This might stem from that pure occupancy information provides little semantic information and thus offer little help for downstream detection and semantic segmentation tasks."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission496/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700060956850,
                "cdate": 1700060956850,
                "tmdate": 1700061084093,
                "mdate": 1700061084093,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fBK7gzJTZO",
                "forum": "9zEBK3E9bX",
                "replyto": "fEV6rt330k",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission496/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission496/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission496/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission496/Reviewers",
                    "ICLR.cc/2024/Conference/Submission496/Reviewers/Submitted"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission496/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer y7L9 (Part 2)"
                    },
                    "comment": {
                        "value": "**Q4: How to make pre-training self-supervised via SPOT?**\n\n**A4:** We totally agree that using fewer or no labels for pre-training,\u00a0namely semi-supervised or\u00a0self-supervised\u00a0pre-training, is\u00a0essential and\u00a0more\u00a0practical. However, previous studies [3,4,5] that employ the fully self-supervised paradigm suffer from the inability to attain the task or dataset scalability: they perform the 3D pre-training and downstream on the same dataset. In contrast, SPOT is the first to explore the so-called one-for-all setting, meaning that the baseline model is only pre-trained once using the proposed SPOT and fine-tuned on different downstream datasets and tasks.\n\nAlso, as demonstrated in image domain, where the most popular pre-training model is fully-supervised pre-trained on ImageNet [6], it is not necessary for pre-training methods to be full self-supervised. Besides, for 3D pre-training on LiDAR point cloud, AD-PT [7] is a successful exploration on semi-supervised pre-training. We perform semi-supervised pre-training experiments with SPOT in [**Q1**](https://openreview.net/forum?id=9zEBK3E9bX&noteId=ctrbWDv7Xw) and [General Response Part 2](https://openreview.net/forum?id=9zEBK3E9bX&noteId=mx7XgdJGbX), which demonstrates the effectiveness of SPOT with few labeled data.\n\nWe hope these can cover your concerns.\n\n**Q5: Pre-training on a different dataset via SPOT?**\n\n**A5:** To verify that SPOT is able to pre-train on other datasets, we utilize the model which is pre-trained on Waymo to predict occupancy labels on 5% NuScenes dataset and generate pseudo occupancy labels. Next, we pre-train SPOT **from scratch on these NuScenes data**, and then fine-tune on the 20% KITTI data. As shown in Table 10 below, SPOT achieves significant gains compared to baseline results on KITTI dataset, demonstrating the effectiveness and generalization of SPOT.\n\nTable 10: Pre-training via SPOT on NuScenes and downstream to 3D object detection on KITTI.\n\n| Method | Pre-training dataset | F.D.A.  | mAP |\n| --- | --- | --- | --- |\n| From Scratch (SECOND) | - | 20% KITTI | 61.70 |\n| SPOT  (SECOND) | NuScenes | 20% KITTI | 64.39 |\n| From Scratch (PV-RCNN) | - | 20% KITTI | 66.71 |\n| SPOT  (PV-RCNN) | NuScenes | 20% KITTI | 69.58 |\n\nWe hope the discussion and experiments address your concerns. We are happy to discuss any remaining concerns about the paper.\n\nBest regards,\n\nAuthors of Paper 496\n\n[1] Dong-Hyun Lee et al. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. In Workshop on challenges in representation learning, ICML, volume 3, pp. 896. Atlanta, 2013.\n\n[2] Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems, 30, 2017.\n\n[3] Runsen Xu, Tai Wang, Wenwei Zhang, Runjian Chen, Jinkun Cao, Jiangmiao Pang, and Dahua Lin. Mv-jar: Masked voxel jigsaw and reconstruction for lidar-based self-supervised pre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13445\u201313454, 2023\n\n[4] Zhiwei Lin and Yongtao Wang. Bev-mae: Bird\u2019s eye view masked autoencoders for outdoor point cloud pre-training. arXiv preprint arXiv:2212.05758, 2022\n\n[5] Hanxue Liang, Chenhan Jiang, Dapeng Feng, Xin Chen, Hang Xu, Xiaodan Liang, Wei Zhang, Zhenguo Li, and Luc Van Gool. Exploring geometry-aware contrast and clustering harmonization for self-supervised 3d object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 3293\u20133302, 2021.\n\n[6] Deng J, Dong W, Socher R, et al. Imagenet: A large-scale hierarchical image database[C]//2009 IEEE conference on computer vision and pattern recognition. Ieee, 2009: 248-255.\n\n[7] Jiakang Yuan, Bo Zhang, Xiangchao Yan, Tao Chen, Botian Shi, Yikang Li, and Yu Qiao. Ad-pt: Autonomous driving pre-training with large-scale point cloud dataset. arXiv preprint arXiv:2306.00612, 2023"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission496/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700060978797,
                "cdate": 1700060978797,
                "tmdate": 1700061735288,
                "mdate": 1700061735288,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MqDRHhOPlk",
                "forum": "9zEBK3E9bX",
                "replyto": "fEV6rt330k",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission496/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission496/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission496/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission496/Reviewers",
                    "ICLR.cc/2024/Conference/Submission496/Reviewers/Submitted"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission496/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Look forward to further discussion."
                    },
                    "comment": {
                        "value": "Dear Reviewer y7L9,\n\nThank you for your precious time on the review. As the deadline of discussion period is approaching, we sincerely hope that our response can address your concerns and we are looking forward to further discussion on any other questions or issues regarding the paper or our response.\n\nBest regards,\n\nAuthors of Paper 496"
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission496/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631533467,
                "cdate": 1700631533467,
                "tmdate": 1700631533467,
                "mdate": 1700631533467,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]