[
    {
        "title": "GPS-SSL: Guided Positive Sampling to Inject Prior into Self-Supervised Learning"
    },
    {
        "review": {
            "id": "ygBbiuC2CK",
            "forum": "ExZ5gonvhs",
            "replyto": "ExZ5gonvhs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4387/Reviewer_sJCd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4387/Reviewer_sJCd"
            ],
            "content": {
                "summary": {
                    "value": "A method using prior knowledge to sample the positive data is proposed. It is supposed to mitigate the importance of data augmentation in self-supervised learning. The proposed GPS-SSL has shown superior capability over the methods with existing augmentation strategies."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ Studying new strategies that rely less on data augmentations in self-supervised learning is worthwhile to the representations learning fields.\n+ Exploring the pre-trained models (CLIP, Supervised models, VAE) for improving SSL might be interesting."
                },
                "weaknesses": {
                    "value": "+ The proposed method needs a heavier component (such as a neural network ResNet-50) to generate the positive data sample, which is significantly computational compared to a simple calculation of data augmentation even for strong augmentations with a series of cropping, color jittering, distortion, hue, etc...\n\n+ With the aid of a strong knowledge (and heavy) model trained on millions or hundred million of data (CLIP, ImageNet) the performance of the proposed method brings minimal advantage even worse than the existing SSL method such as VICReg in Table 2 with strong augmentation. In the weak augmentation setting, GPS-SSL may give better performance but still lag significantly behind the optimal setting (strong augmentation) of both streams, making it questionable about the contribution of the proposed method.\n\n+ SSL contains another branch that is also very promising with the fine-tuning accuracy on downstream tasks such as MAE [1], this approach also depends very little on data augmentation (only cropping or without any augmentation already made the very good performance). This example (MAE method) will challenge the proposed method in terms of dependency on augmentation because the proposed method could not work without augmentation. I believe that modern SSLs should include this metric (fine-tune accuracy) and compare both contrastive learning and MAE approaches.\n\n+ It should also include the linear evaluation of the only CLIP RN50 or supervised RN50 model when they have been used as the feature extractor for the downstream tasks on each considered dataset. It is to see without any training, how well these pre-trained model can perform, and based on that we can assess their contribution to the GPS-SSL (which is a combination of existing SSL + pre-trained CLIP/RN50).\n\n+ Another point is that the experimental setting is not practical and sufficient to demonstrate the effectiveness of GPS-SSL when evaluating self-supervised contrastive learn is that they only consider pretraining with 200 epochs, which is very few epochs required by SSL models to fully converge. As shown in SimSiam or many SSL (MoCo, BYOL, Barlow Twins, VICREG,... ) the performance is best achieved with long enough self-supervised pretraining (800-1000 epochs). As a result, the comparison in long training should be considered for both methods.\n\n+ It is not clear what is the metric they have shown in Table 1. Reading its caption, it is challenging to capture what metric they are comparing, top-1 ACC or error or something else.\n\n[1] Masked Autoencoders Are Scalable Vision Learners, CVPR 2022"
                },
                "questions": {
                    "value": "See weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4387/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698304725319,
            "cdate": 1698304725319,
            "tmdate": 1699636411664,
            "mdate": 1699636411664,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "I0rdyd97hy",
                "forum": "ExZ5gonvhs",
                "replyto": "ygBbiuC2CK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4387/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4387/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part 1/3"
                    },
                    "comment": {
                        "value": "- **The proposed method needs a heavier component (such as a neural network ResNet-50) to generate the positive data sample, which is significantly computational compared to a simple calculation of data augmentation even for strong augmentations with a series of cropping, color jittering, distortion, hue, etc\u2026**\n\n*Response:* Very good point; thank you for raising it. First of all, we will make sure to mention that explicitly in the introduction and when we derive our method. That being said, while the addition of another DN is clearly a more intense computational burden, we show that our method converges faster (thus needs fewer epochs, compensating for the extra cost). For example, we now provide a new experiment with a longer training schedule and obtain matching or improved results on both datasets with GPS-SimCLR and GPS-VICReg.\n\n|            | Cifar10 |          | FGVCAircraft |          |\n|------------|:-------:|:--------:|:------------:|:--------:|\n|            | 400 eps | 1000 eps |    400 eps   | 1000 eps | \n|   SimCLR   |  88.26  |   91.25  |     39.87    |   45.55  |\n| GPS-SimCLR |  89.57  |   91.10  |     50.08    |   51.64  |\n|   VICReg   |  89.34  |   90.61  |     33.21    |   41.19  | \n| GPS-VICReg |  89.68  |   89.84  |     45.48    |   49.29  |\n\n\n\nAlso, it is possible that the NN sampling, relying only on the latent space representations, can be made on a saved bank of representations obtained from the NN architecture, i.e., no need to forward each iteration. We will mention that in the conclusion.\n\n- **With the aid of a strong knowledge (and heavy) model trained on millions or hundred million of data (CLIP, ImageNet) the performance of the proposed method brings minimal advantage even worse than the existing SSL method such as VICReg in Table 2 with strong augmentation. In the weak augmentation setting, GPS-SSL may give better performance but still lag significantly behind the optimal setting (strong augmentation) of both streams, making it questionable about the contribution of the proposed method.**\n\n*Response:* We thank the reviewer for raising that concern. First, we would like to mention that one important contribution of our submission is in demonstrating a new path for SSL methods: improving the positive pair sampling strategy. To that end, we did employ, in a few places, some pretrained embedding that had access to more samples than the SSL baseline (although this was already clearly stated in our manuscript when such results were reported). However, we completely understand that this may limit the applicability and reach of our claims. As such, we have proposed two new sets of experiments to address that concern. The first is to actually let the SSL baseline benefit from that same additional information by replacing the randomly initialized backbone with the same one being used by GPS\u2019s embedding. In short, all methods now have access to the exact same extra information from the get-go:\n\n\n|            |              |  Cifar10 |            | FGVCAircraft |            |   |\n|------------|--------------|:--------:|:----------:|:------------:|:----------:|---|\n|            |              | Weak aug | Strong Aug | Weak aug     | Strong Aug |\n|   SimCLR   | RN50         |    46.69 |      87.39 |         5.67 |      27.36 |\n| GPS-SimCLR |              |     85.2 |      90.48 |        17.91 |      43.56 |\n|   SimCLR   | RN50_pt      |    43.99 |      94.02 |        17.91 |      59.92 |\n| GPS-SimCLR |              |     91.3 |      95.53 |        39.45 |      66.88 |\n|   SimCLR   | RN50_CLIP_pt |    45.57 |      90.26 |         6.21 |      41.04 |\n| GPS-SimCLR |              |    89.44 |      91.23 |        24.15 |      49.63 |\n\nWe observe that even in this setting, GPS is able to consistently improve the final SSL representations compared to the baseline (more details in our general answer). The second experiment we propose is to employ GPS with an embedding model that is trained on the same dataset being considered, i.e., no additional knowledge is present in GPS training:\n\n|            |    dataset   | prior knowledge BB | NN_trained_on | backbone | val_acc1 |\n|------------|:------------:|:------------------:|:-------------:|:--------:|:--------:|\n|   VICReg   | FGVCAircraft |          -         |       -       | ResNet50 |   39.99  |\n| GPS-VICReg | FGVCAircraft |      MAE_vitL      |  FGVCAircraft | ResNet50 |   42.87  |\n|   SimCLR   | FGVCAircraft |          -         |       -       | ResNet50 |   47.11  |\n| GPS-SimCLR | FGVCAircraft |      MAE_vitL      |  FGVCAircraft | ResNet50 |   46.93  |\n\n\n\nWhere we observe that GPS is again able to match or improve the SSL baseline."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4387/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700260066757,
                "cdate": 1700260066757,
                "tmdate": 1700260066757,
                "mdate": 1700260066757,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hI9eYjgeGj",
                "forum": "ExZ5gonvhs",
                "replyto": "ygBbiuC2CK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4387/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4387/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part 2/3"
                    },
                    "comment": {
                        "value": "- **SSL contains another branch that is also very promising with the fine-tuning accuracy on downstream tasks such as MAE [1], this approach also depends very little on data augmentation (only cropping or without any augmentation already made the very good performance). This example (MAE method) will challenge the proposed method in terms of dependency on augmentation because the proposed method could not work without augmentation. I believe that modern SSLs should include this metric (fine-tune accuracy) and compare both contrastive learning and MAE approaches.**\n\n*Response:* We thank the reviewer for this suggestion. Indeed, once allowing for fine-tuning, we believe that most of those variants should produce comparable performances. However, we focused on a frozen backbone as a means to precisely assess the quality of the learned representations, as guided by the employed training criterion. In fact, as part of our motivation is to argue on the current sub-optimal positive pair sampling strategy, we resorted to assessing such a performance gap without fine-tuning. We will, however, clearly state this limitation and also mention in our contributions that our goal is to improve frozen backbone SSL.\n\n\n- **It should also include the linear evaluation of the only CLIP RN50 or supervised RN50 model when they have been used as the feature extractor for the downstream tasks on each considered dataset. It is to see without any training, how well these pre-trained model can perform, and based on that we can assess their contribution to the GPS-SSL (which is a combination of existing SSL + pre-trained CLIP/RN50).**\n\n*Response:* That is an excellent point and we thank you for mentioning it. We added a table that depicts the linear probe evaluation of a pretrained RN50 with CLIP on Cifar10 and FGVCAircraft dataset:\n\n\n|                     | Cifar10\t | FGVCAircraft |\n|---------------------|:-------:|:------------:|\n| CLIP_RN50_LAION400M |   0.88  |     0.45     |\n|  MAE_vitB_ImageNet  |   0.86  |     0.28     |\n|  MAE_vitL_ImageNet  |   0.91  |     0.37     |\n| vae_RN50_objects365 |   0.29  |     0.02     |\n\n\nAs it can be seen, GPS-SSL indeed improves the classification performance on both datasets when trained on a RN50 with strong augmentations, showing that GPS-SSL indeed contributes. Furthermore, we show that even when using a CLIP pretrained RN50, GPS-SimCLR outperforms SimCLR in all states, showing that the contribution is due to the better \u201cpositive sampling\u201d, rather than using a pre-trained RN50.\n\n|           |    cifar10\t   |            | FGVCAircraft |            |\n|-----------|:------------:|:----------:|:------------:|:----------:|\n|           | linear probe | GPS-SimCLR | linear probe | GPS-SimCLR |\n| CLIP_RN50 |     87.85    |    91.17   |     44.55    | 53.81      |"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4387/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700260311034,
                "cdate": 1700260311034,
                "tmdate": 1700260311034,
                "mdate": 1700260311034,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aZWgeB7oQc",
                "forum": "ExZ5gonvhs",
                "replyto": "ygBbiuC2CK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4387/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4387/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part 3/3"
                    },
                    "comment": {
                        "value": "- **Another point is that the experimental setting is not practical and sufficient to demonstrate the effectiveness of GPS-SSL when evaluating self-supervised contrastive learn is that they only consider pretraining with 200 epochs, which is very few epochs required by SSL models to fully converge. As shown in SimSiam or many SSL (MoCo, BYOL, Barlow Twins, VICREG,... ) the performance is best achieved with long enough self-supervised pretraining (800-1000 epochs). As a result, the comparison in long training should be considered for both methods.**\n\n*Response:* We thank the reviewer for raising this limitation, we entirely agree with the need to provide a longer training schedule. We had previously reported results where most of the models were trained for 400 epochs, and some were trained for 200 epochs. The first change we have conducted following your suggestion is to make all the models in the reported tables in the submission with 400 training epochs. (Table 2 and 6).\n\nSecond, we performed a few additional experiments training for 1000 epochs with a ResNet18 using SimCLR (with and without GPS) which we report below (and in Table 3)\n\n|            | Cifar10 |          | FGVCAircraft |          |\n|------------|:-------:|:--------:|:------------:|:--------:|\n|            | 400 eps | 1000 eps |    400 eps   | 1000 eps | \n|   SimCLR   |  88.26  |   91.25  |     39.87    |   45.55  |\n| GPS-SimCLR |  89.57  |   91.10  |     50.08    |   51.64  |\n|   VICReg   |  89.34  |   90.61  |     33.21    |   41.19  | \n| GPS-VICReg |  89.68  |   89.84  |     45.48    |   49.29  |\n\nWhere we employ strong augmentations (the standard SSL setting). We observe that GPS consistently improves performances and arrives on par with SimCLR on Cifar10 after 1000 epochs. Interestingly (and we thank the reviewer again for the suggestion that led to quantifying that benefit) we observe that GPS converges in fewer epochs than SSL. This is again another possible benefit of opening new avenues for informed positive pair sampling: faster SSL training. We hope that those novel experiments answered the reviewer\u2019s empirical validation concerns.\n\n- **It is not clear what is the metric they have shown in Table 1. Reading its caption, it is challenging to capture what metric they are comparing, top-1 ACC or error or something else.**\n\n*Response:* We apologize for the confusion, the reported metrics are top-1 accuracy, we have made sure to state that clearly in the caption, thank you for raising that discrepancy."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4387/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700260339129,
                "cdate": 1700260339129,
                "tmdate": 1700260339129,
                "mdate": 1700260339129,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kOZH3jqVAN",
                "forum": "ExZ5gonvhs",
                "replyto": "ygBbiuC2CK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4387/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4387/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reminder"
                    },
                    "comment": {
                        "value": "Dear Reviewer sJCd,\n\nWe hope that you have had a chance to consider our rebuttal. Your comments greatly helped us improve our manuscript, we sincerely thank you for that. We would like to kindly remind you that ICLR won't have a second round of discussion. As such, we would deeply appreciate it if you could raise any remaining concern or comment that you may have. This way, we will have a chance to clarify any last issue before the end of the discussion period which is now very near. If you consider your concerns addressed, then we hope that you will find time to revise your score and review accordingly. \nThank you again for participating in the peer-review process.\n\nBest regards,\nThe authors"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4387/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700602313061,
                "cdate": 1700602313061,
                "tmdate": 1700602313061,
                "mdate": 1700602313061,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7MTwK8sUMB",
                "forum": "ExZ5gonvhs",
                "replyto": "ygBbiuC2CK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4387/Reviewer_sJCd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4387/Reviewer_sJCd"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks authors for their response. The rebuttal partly addressed my concerns. However, the additional results remain that the proposed method still lags behind the traditional SSL with the aid of the strong knowledge of the prior CLIP model (as mentioned in the weakness before). \n+ For example, in CIFAR-10 with optimal training (1000 ep), it clearly shows that SimCLR and VIGReg are still better than the proposed method (meaning that GPS failed to improve the SSL baselines in the optimal setting). \n+ For the FGVCAircraft dataset, although it can show some improvements, I think it is a very weak baseline, for example, paper [1] (Table 2) shows that SSL baselines such as MoCo-2 already achieved 52.54\\% and [1] even achieves 55.87\\% which is by far better than the results provided by GPS.\n\n[1] Learning Common Rationale to Improve Self-Supervised Representation for Fine-Grained Visual Recognition Problems, CVPR 2023\n\nI tend to keep my initial rating."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4387/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700639531449,
                "cdate": 1700639531449,
                "tmdate": 1700665823674,
                "mdate": 1700665823674,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0a57yF5cU7",
            "forum": "ExZ5gonvhs",
            "replyto": "ExZ5gonvhs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4387/Reviewer_uQnq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4387/Reviewer_uQnq"
            ],
            "content": {
                "summary": {
                    "value": "The authors proposed the Guided Positive Sampling (GPS) approach to\nfinding positive pairs in self-supervised learning, without data\naugmentation.  For each instance, a nearest neighbor is found in an\nembedding space pretrained with another dataset or with a variational\nautoencoder on the same dataset.  The corresponding instance becomes\nthe positive instance for self-supervised learning.\n\nIn their experiments, they consider using GPS with SIMCLR, BYOL,\nBarlow, and VICreg on five datasets.  For GPS, they use embeddings\nfrom supervised training, CLIP or VAE.  Generally, empirical\nresults indicate that using GPS outperforms, particularly with weak\naugmentations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Not relying on heavy handcrafting of data augmentation for\nself-supervised learning is interesting.  Using prior knowledge based\non a pretrained encoder, they propose to find a nearest neighbor to\nform a positive pair.  Generally, empirical results indicate that using\nGPS outperforms, particularly with weak augmentations."
                },
                "weaknesses": {
                    "value": "With prior knowledge, GPS seems to have an advantage over regular SSL,\nwhich generally does not use prior knowledge.  According to Figure 1,\ndata augmentation is used in GPS-SimCLR.  So GPS seems to differ only\nin the use of prior knowledge to find positive pairs.\n\nDetails are in questions below."
                },
                "questions": {
                    "value": "1.  Theorem 1: GPS-SSL: employing eq (2) or (3) into eq (1)?\n\n2.  Table 2: why are two different kinds of prior knowledge is used?\n\n3.  How is $Tau$ set in Equation 3?\n\n4.  With prior knowledge from another encoder, GPS has an advantage.\n    Hence, comparison with methods that don't have prior knowledge\n    might not be fair.  Could the regular SSL (with augmentation) also\n    use prior knowledge?  For example, the encoder is initialized by\n    prior knowledge and then regular SSL is performed.\n\n5.  Sec 4.1, how do you predict if the classes do not overlap in the\n    training and test sets (unseen classes branches/chains)?\n\n--------  after response from authors ---\n\nI think the authors performed experiments that remove the advantage of prior knowledge used in GPS and the results indicate GPS can improve performance over regular SSL."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "n/a"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4387/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4387/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4387/Reviewer_uQnq"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4387/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698777702068,
            "cdate": 1698777702068,
            "tmdate": 1700611312546,
            "mdate": 1700611312546,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "U7XYQ9va32",
                "forum": "ExZ5gonvhs",
                "replyto": "0a57yF5cU7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4387/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4387/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part 1/2"
                    },
                    "comment": {
                        "value": "-  **With prior knowledge, GPS seems to have an advantage over regular SSL, which generally does not use prior knowledge. According to Figure 1, data augmentation is used in GPS-SimCLR. So GPS seems to differ only in the use of prior knowledge to find positive pairs.**\n\n*Response:* We thank the reviewer for raising that concern (which was shared with other reviewers). We have provided two novel sets of experiments to demonstrate that GPS is able to help SSL learn richer representations by alleviating the sub-optimal positive pair generation employed by SSL that goes far beyond the use of additional knowledge that exists in the pretrained embedding. The first is to actually let the SSL baseline benefit from that same additional information by replacing the randomly initialized backbone with the same one being used by GPS\u2019s embedding. In short, all methods now have access to the exact same extra information from the get-go:\n\n|            |              |  Cifar10 |            | FGVCAircraft |            |\n|------------|--------------|:--------:|:----------:|:------------:|:----------:|\n|            |              | Weak aug | Strong Aug | Weak aug     | Strong Aug |\n|   SimCLR   | RN50         |    46.69 |      87.39 |         5.67 |      27.36 |\n| GPS-SimCLR |              |     85.2 |      90.48 |        17.91 |      43.56 |\n|   SimCLR   | RN50_sup_pt  |    43.99 |      94.02 |        17.91 |      59.92 |\n| GPS-SimCLR |              |     91.3 |      95.53 |        39.45 |      66.88 |\n|   SimCLR   | RN50_CLIP_pt |    45.57 |      90.26 |         6.21 |      41.04 |\n| GPS-SimCLR |              |    89.44 |      91.23 |        24.15 |      49.63 |\n\n\n\nWe observe that even in this setting, GPS is able to consistently improve the final SSL representations compared to the baseline (more details in our general answer). The second experiment we propose is to employ GPS with an embedding model that is trained on the same dataset being considered, i.e., no additional knowledge is present in GPS training:\n\n|            |    Dataset   | GPS-Backbone |  GPS-Dataset | Backbone | Val_acc1 |\n|------------|:------------:|:------------:|:------------:|:--------:|:--------:|\n|   VICReg   | FGVCAircraft |       -      |       -      | ResNet50 |   39.99  |\n| GPS-VICReg | FGVCAircraft |   MAE_vitL   | FGVCAircraft | ResNet50 |   42.87  |\n|   SimCLR   | FGVCAircraft |       -      |       -      | ResNet50 |   47.11  |\n| GPS-SimCLR | FGVCAircraft |   MAE_vitL   | FGVCAircraft | ResNet50 |   46.93  |\n\nWhere we observe that GPS is again able to match or improve the SSL baseline.\n\n- **Theorem 1: GPS-SSL: employing eq (2) or (3) into eq (1)?**\n\n*Response:* Eq (2) is the standard SSL solution to take a single sample, apply two different data-augmentations to it, and form a positive pair this way. Eq (3) is the proposed strategy obtained by applying the data-augmentation on two samples that are nearest neighbors in some given embedding space. Note that (3) can recover (2) in the limit case where we pick the sample itself as its nearest neighbor, we have put ``GPS\u2019\u2019 as part of eq (3) to make sure that the distinction was as clear as possible.\n\n- **Table 2: why are two different kinds of prior knowledge is used?**\n\n*Response:* We thank the reviewer for raising this concern. We have updated Table 2 accordingly to only use one prior knowledge (RN50_{CLIP} embeddings) for consistency and staying purely in the realm of SSL methods. In short, all employ GPS-SimCLR, the left table shows comparison using GPS with a few different embedding models, while on the right we use GPS-SimCLR with a few different hyper-parameters (to show its stability). All cases are on the same FGVCAircraft dataset.\n\n- **How is \\tau set in Equation 3?**\n\n*Response:* That is a very important point, and we thank the reviewer for raising it. Based on the quality of the metric space g_{\\gamma}, we can set \\tau to be larger or smaller. For example, if we know g_{\\gamma} is reliable, we can set \\tau to larger values. However, if we are unsure about the reliability of g_{\\gamma}, we can set \\tau to smaller values to make sure that the positive pairs convey useful information rather than noise. In fact (as mentioned in our original submission) as tau gets close tk 0 as GPS recovers vanilla SSL positive pair sampling (we made sure to emphasize that observation which enables a qualitative guess for the value of \\tau."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4387/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700259951399,
                "cdate": 1700259951399,
                "tmdate": 1700259951399,
                "mdate": 1700259951399,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jQVsJ6LOo0",
                "forum": "ExZ5gonvhs",
                "replyto": "0a57yF5cU7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4387/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4387/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part 2/2"
                    },
                    "comment": {
                        "value": "- **With prior knowledge from another encoder, GPS has an advantage. Hence, comparison with methods that don't have prior knowledge might not be fair. Could the regular SSL (with augmentation) also use prior knowledge? For example, the encoder is initialized by prior knowledge and then regular SSL is performed.**\n\n*Response:* We thank the reviewer for raising that concern. Thanks to your suggestion, we have provided that exact experiment in response 1 above (and in the general comment). We believe that this experiment has been extremely insightful to use as it clearly highlights that the current SSL positive pair sampling strategy (eq (2)) is indeed suboptimal, as it moves away from a potentially better-initialized model (as opposed to GPS).\n\n- **Sec 4.1, how do you predict if the classes do not overlap in the training and test sets (unseen classes branches/chains)?**\n\n*Response:* Thank you for raising this concern. The paper that introduces the dataset mentions how they divide the total data into the train and the multiple test splits. More specifically, they explain that first, a set of chains (along with all their branches) is reserved for the D_{UU} to make sure the chains (super-classes) and branches (classes) are not seen during training. Next, out of the remaining chains, a set of branches is chosen to add all of their images to the D_{SU} test split (since the training set will have other images from other branches from the same chain, but not the same branch images). Finally, out of the remaining branches, the images in each are split between D_{SS} and train, creating the final test split that has a subset of the branches seen during training. With this procedure, they ensure the table of overlapping below:\n\n|           | D_{SS}                                                     | D_{SU}                                                   | D_{UU}                                 |\n|-----------|------------------------------------------------------------|----------------------------------------------------------|----------------------------------------|\n| Train set | Shared branches (classes)and shared chains (super-classes) | Only shared chains (super-classes) branches are distinct | Both chains and branches are distinct. |"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4387/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700259996637,
                "cdate": 1700259996637,
                "tmdate": 1700259996637,
                "mdate": 1700259996637,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Hf7RgdVQr9",
                "forum": "ExZ5gonvhs",
                "replyto": "0a57yF5cU7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4387/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4387/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reminder"
                    },
                    "comment": {
                        "value": "Dear Reviewer uQnq,\n\nWe hope that you have had a chance to consider our rebuttal. Your comments greatly helped us improve our manuscript, we sincerely thank you for that. We would like to kindly remind you that ICLR won't have a second round of discussion. As such, we would deeply appreciate it if you could raise any remaining concern or comment that you may have. This way, we will have a chance to clarify any last issue before the end of the discussion period which is now very near. If you consider your concerns addressed, then we hope that you will find time to revise your score and review accordingly. \nThank you again for participating in the peer-review process.\n\nBest regards,\nThe authors"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4387/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700602282828,
                "cdate": 1700602282828,
                "tmdate": 1700602282828,
                "mdate": 1700602282828,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ve4LIBpiHH",
                "forum": "ExZ5gonvhs",
                "replyto": "jQVsJ6LOo0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4387/Reviewer_uQnq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4387/Reviewer_uQnq"
                ],
                "content": {
                    "title": {
                        "value": "comments on response"
                    },
                    "comment": {
                        "value": "Thanks for the response.\n\nI think the authors performed experiments that remove the advantage of prior knowledge used in GPS and the results indicate GPS can improve performance over regular SSL.   I plan to upgrade my score."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4387/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700611232784,
                "cdate": 1700611232784,
                "tmdate": 1700611232784,
                "mdate": 1700611232784,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "e2z6GozzAF",
            "forum": "ExZ5gonvhs",
            "replyto": "ExZ5gonvhs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4387/Reviewer_aLRr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4387/Reviewer_aLRr"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes Guided Positive Sampling Self-Supervised Learning (GPS-SSL), a method that integrates prior knowledge into Self-Supervised Learning (SSL) to improve positive sample selection and reduce reliance on data augmentations. Based on pretrained visual models and target dataset, GPS-SSL creates a metric space that facilitates nearest-neighbor sampling for positive samples. The method is applicable to various SSL techniques and outperforms baseline methods, particularly when minimal augmentations are used."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Extensive experiments show the effectiveness of the GPS strategy.\n- The paper is easy to follow."
                },
                "weaknesses": {
                    "value": "- The employment of prior knowledge, specifically in the form of a pretrained visual model and the target dataset, diverges from the fundamental principles of Self-Supervised Learning (SSL).\n- The incorporation of such prior knowledge raises concerns about the fairness of comparisons with existing SSL methods. There is a potential risk that the pretrained visual model and target dataset might leak additional information into the model, thereby skewing results and leading to issues of unfairness.\n- The difference between GSP-SSL and NNCLR lies primarily in their respective positive sampling strategies. However, the novelty of the proposed strategy is limited."
                },
                "questions": {
                    "value": "- It would be better to make prior knowledge in an unsupervised manner, except using pretrained visual model and target dataset.\n- The supervised results are supposed to be shown in Table 2."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4387/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4387/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4387/Reviewer_aLRr"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4387/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698822019420,
            "cdate": 1698822019420,
            "tmdate": 1699636411494,
            "mdate": 1699636411494,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IjrbkTD0dz",
                "forum": "ExZ5gonvhs",
                "replyto": "e2z6GozzAF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4387/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4387/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part 1/2"
                    },
                    "comment": {
                        "value": "- **The employment of prior knowledge, specifically in the form of a pretrained visual model and the target dataset, diverges from the fundamental principles of Self-Supervised Learning (SSL)**\n\t\n*Response:* We thank the reviewer for raising that concern. While we agree with the fact that using a pretrained model as part of SSL training is a shift, we ought to emphasize that one of our core contributions is in bringing forward the importance of positive sample selection (which we often illustrated employing pretrained models). We believe that this point carries a lot of weight to guide future research direction in SSL: improving the sampling and selection of positive pairs has the potential to greatly outpace progress made around data augmentation and architecture search.\n\nAdditionally, we emphasize that even when we employed, for example, CLIP pretrained embeddings for GPS, such models did not have access to labels during training. That is, the empirical result remains in the realm of SSL. We entirely agree with the reviewer\u2019s point for the cases where we used a supervised embedding, but again, our main motivation in doing so was to demonstrate how much richer SSL representations can be with a label-informed embedding. We made sure to clarify that point and use purely SSL embeddings, i.e., CLIP, in our manuscript in the main experiments (Table 2 and 6) and removed the supervised embeddings.\n\nFurthermore, we hope to further address the reviewer\u2019s concern with a new set of experiments (presented in Tables 8 and 9):\n\n|            |    dataset   | GPS-Backbone |  GPS-Dataset | backbone | val_acc1 |\n|------------|:------------:|:------------:|:------------:|:--------:|:--------:|\n|   VICReg   | FGVCAircraft |       -      |       -      | ResNet50 |   39.99  |\n| GPS-VICReg | FGVCAircraft |   MAE_vitL   | FGVCAircraft | ResNet50 |   42.87  |\n|   SimCLR   | FGVCAircraft |       -      |       -      | ResNet50 |   47.11  |\n| GPS-SimCLR | FGVCAircraft |   MAE_vitL   | FGVCAircraft | ResNet50 |   46.93  |\n\nWhich we hope demonstrate that even when no pre-trained embedding is accessible to GPS, and a new one needs to be re-trained on the given dataset (e.g., using MAE as in the above results), its use in GPS can yield final performances at least as good as the SSL baseline (SimCLR) or even greater (VICreg).\n\n- **The incorporation of such prior knowledge raises concerns about the fairness of comparisons with existing SSL methods. There is a potential risk that the pretrained visual model and target dataset might leak additional information into the model, thereby skewing results and leading to issues of unfairness.**\n\n*Response:* This is a great observation that needs to be addressed. First (as briefly mentioned in the above point), we have conducted new experiments where none of the DNN is pretrained, which showed great benefits of the proposed strategy. Another direction that we have also explored is to see what would happen if all the methods (GPS and the SSL baselines) had access to that extra information. To that end, we have conducted an experiment where we employed SSL and GPS training on CIFAR10 and FGVCAircraft starting from a backbone that is either random (realistic setting), Imagenet pretraining, or CLIP pretrained. That is, all methods have access to the same \u201cleaked\u201d information. We observe that even in that setting, GPS manages to produce greater results. This added experiment also further reinforces our main point: being able to correctly select positive samples is crucial to ensure that SSL training is successful, even when starting from a near-optimal initialization.\n\n|            |              |  Cifar10 |            | FGVCAircraft |            |   |\n|------------|--------------|:--------:|:----------:|:------------:|:----------:|---|\n|            |              | Weak aug | Strong Aug | Weak aug     | Strong Aug |\n|   SimCLR   | RN50         |    46.69 |      87.39 |         5.67 |      27.36 |\n| GPS-SimCLR |              |     85.2 |      90.48 |        17.91 |      43.56 |\n|   SimCLR   | RN50_pt      |    43.99 |      94.02 |        17.91 |      59.92 |\n| GPS-SimCLR |              |     91.3 |      95.53 |        39.45 |      66.88 |\n|   SimCLR   | RN50_CLIP_pt |    45.57 |      90.26 |         6.21 |      41.04 |\n| GPS-SimCLR |              |    89.44 |      91.23 |        24.15 |      49.63 |"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4387/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700259754870,
                "cdate": 1700259754870,
                "tmdate": 1700259754870,
                "mdate": 1700259754870,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3OBv0zpGpx",
                "forum": "ExZ5gonvhs",
                "replyto": "e2z6GozzAF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4387/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4387/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part 2/2"
                    },
                    "comment": {
                        "value": "- **The difference between GSP-SSL and NNCLR lies primarily in their respective positive sampling strategies. However, the novelty of the proposed strategy is limited.**\n\n*Response:* Our method is indeed inspired by NNCLR. However, GPS-SSL is a more general form of NNCLR. As mentioned in our general answer and in the theoretical part of our submission, GPS should be thought of as a general formulation of positive pair sampling using guidance that takes the form of prescribing the space in which nearest neighbors are sampled. That space, however, need not be given from another deep neural network but can be given from known data augmentations (thereby recovering baseline SSL), or from external knowledge such as querying an oracle. Last but not least, our formulation is identical for any SSL method employed (as opposed to NNCLR, which has been derived and tested uniquely on SimCLR). Our hope is that GPS can be seen as a general add-on akin to data augmentation that can be employed with any SSL loss.\n\n- **It would be better to make prior knowledge in an unsupervised manner, except using pretrained visual model and target dataset**\n\n*Response:* We entirely agree with that concern and have now provided numerous experiments to address it, including in the above answer to concern 1.,  and the general comment.\n\n- **The supervised results are supposed to be shown in Table 2**\n\n*Response:* We are sorry for the confusion, we have updated the caption for Tables 1 and 2 to make sure that we clearly state which method and which metric is being presented (classification accuracy)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4387/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700259802518,
                "cdate": 1700259802518,
                "tmdate": 1700259802518,
                "mdate": 1700259802518,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hRgoiZSYdB",
                "forum": "ExZ5gonvhs",
                "replyto": "e2z6GozzAF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4387/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4387/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reminder"
                    },
                    "comment": {
                        "value": "Dear Reviewer aLRr,\n\nWe hope that you have had a chance to consider our rebuttal. Your comments greatly helped us improve our manuscript, we sincerely thank you for that. We would like to kindly remind you that ICLR won't have a second round of discussion. As such, we would deeply appreciate it if you could raise any remaining concern or comment that you may have. This way, we will have a chance to clarify any last issue before the end of the discussion period which is now very near. If you consider your concerns addressed, then we hope that you will find time to revise your score and review accordingly. \nThank you again for participating in the peer-review process.\n\nBest regards,\nThe authors"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4387/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700602267243,
                "cdate": 1700602267243,
                "tmdate": 1700602267243,
                "mdate": 1700602267243,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PQpy4IkOLA",
                "forum": "ExZ5gonvhs",
                "replyto": "3OBv0zpGpx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4387/Reviewer_aLRr"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4387/Reviewer_aLRr"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer Response"
                    },
                    "comment": {
                        "value": "Thanks for the response, which somehow addressed some of my concerns. However, the additional experiment cannot solve my concerns about the plausibility and fairness of using pretrained visual model. The novelty is also limited in comparison with NNCLR. Additionally, it's important to note that the CLIP model does have access to the labels, even though these labels might be noisy. I am afraid after taking into account these issues, I may not be able to increase my score."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4387/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634420780,
                "cdate": 1700634420780,
                "tmdate": 1700634420780,
                "mdate": 1700634420780,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]