[
    {
        "title": "Decoupled Actor-Critic"
    },
    {
        "review": {
            "id": "c5RKFIQfnK",
            "forum": "op19LjpHkH",
            "replyto": "op19LjpHkH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8610/Reviewer_HF8k"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8610/Reviewer_HF8k"
            ],
            "content": {
                "summary": {
                    "value": "This is an empirical paper well-motivated by solving the conservative policy update and optimistic exploration problem that exists in deep RL. The proposed method achieves significant performance compared with the presented baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The motivation is clear, and the method looks solid.\nThe authors provide extensive experiments and put great effort into enhancing reproducibility."
                },
                "weaknesses": {
                    "value": "The paper does not compare against the ensemble-based methods [e.g., REDQ], which I believe is relevant and necessary. \n\nOn the general applicability of the idea, will this idea work with different value-to-policy generation rules? \n\nThere is no text pointing to Figure 1 and Figure 2. Also, many of the abbreviations are used since the beginning of the paper but are introduced at very late stages. The overall presentation of the paper can be improved.\n\nThe authors do not disclose any pitfalls of the DAC algorithm. Is it consistently better than SAC/TD3/other baselines? This is a huge claim, and would definitely be a huge strength of the work if it is true.\n\n\n\n\n\nReferences:\n\n[REDQ] Chen, Xinyue, et al. \"Randomized ensembled double q-learning: Learning fast without a model.\"\u00a0arXiv preprint arXiv:2101.05982\u00a0(2021)."
                },
                "questions": {
                    "value": "It is known in the literature that off-policy learning can suffer from the [tandem problem]. How do you solve such difficulty when updating the conservative actor using the data generated by the optimistic actor?\n\nWhat is the backbone of DAC? How do the authors explicitly model the variance and mean of the policy?\n\n\nReferences:\n\n[Tandem] Ostrovski, Georg, Pablo Samuel Castro, and Will Dabney. \"The difficulty of passive learning in deep reinforcement learning.\"\u00a0Advances in Neural Information Processing Systems\u00a034 (2021): 23283-23295."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8610/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8610/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8610/Reviewer_HF8k"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8610/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698601304969,
            "cdate": 1698601304969,
            "tmdate": 1700772196065,
            "mdate": 1700772196065,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rb4PWRBoYX",
                "forum": "op19LjpHkH",
                "replyto": "c5RKFIQfnK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8610/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8610/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer HF8k (1)"
                    },
                    "comment": {
                        "value": "We thank the Reviewer for their time writing the review. We are happy that the Reviewer finds the proposed approach \u201cwell-motivated\u201d and \u201csolid\u201d, the performance gains \u201csignificant\u201d and the experiments \u201cextensive\u201d. We would like to point the Reviewers attention to new experimental results described in our meta-comment. The new results further indicate that DAC achieves very strong performance on locomotion tasks, significantly outperforming both model-based (TD-MPC) and model-free (SR-SAC) SOTA on the dog domain. Below, we respond to each question/weakness listed by the Reviewer. \n\n> The paper does not compare against the ensemble-based methods [e.g., REDQ], which I believe is relevant and necessary. \n\nWe kindly ask the Reviewer to investigate Figures 1, 9 & 10 of the original manuscripts to find the evaluation of DAC against exactly REDQ. As follows, REDQ achieves significantly worse results than other considered algorithms in the high replay benchmark. This result is consistent with [1], where SR-SAC (high replay SAC with resets) massively outperforms REDQ across various replay regimes. For completeness, we included both REDQ and SR-SAC in the evaluation presented in Figure 1, but dropped it in most of the latter evaluations because of substantial performance differences between REDQ and other algorithms.\n\n> On the general applicability of the idea, will this idea work with different value-to-policy generation rules? \n\nOur approach is trivially applicable to other value-based continuous actions algorithms like DDPG, TD3 or Dreamer. It would require some accommodations to be applied to discrete actions algorithms like Rainbow or BBF. We are happy to expand on the answer if the Reviewer shares which particular algorithm the Reviewer is interested in. \n\n> There is no text pointing to Figure 1 and Figure 2. Also, many of the abbreviations are used since the beginning of the paper but are introduced at very late stages. The overall presentation of the paper can be improved.\n\nWe thank the Reviewer for this suggestion! We added text in the main body that describes why Figures 1 & 2 are important for our narrative. We make some additional changes to make sure that abbreviations are explained earlier in the text.  \n\n> The authors do not disclose any pitfalls of the DAC algorithm.\n\nWe ask the Reviewer to note that our original manuscript contains a \u201cLimitations\u201d section, where we describe pitfalls of our proposed DAC algorithm. Those limitations include:\n\n1. Complexities of the tandem setting - \u201cDAC divergence minimization presents unique optimization challenges. Unlike typical uses of KL divergence, where the target distribution remains fixed (eg. Variational Autoencoders (VAE)), DAC deals with a constantly evolving policy that is continually improving. Consequently, the optimistic actor needs to keep up with the conservative actor\u2019s changes. As depicted in Figure 4, DAC heavily relies on maintaining a low divergence between the actors. While DAC adjustment mechanisms proved effective in the tested environments, there is no guarantee that they will suffice in more complex ones.\u201d\n2. Optimistic actor architecture - \u201cThe second drawback of DAC lies in its inherent use of two actor networks, which results in slightly increased memory and computational demands compared to the standard Soft Actor-Critic (SAC) approach. In practice, the wall-clock time of DAC is around 10% greater than that of SAC and is indistinguishable from the overhead induced by OAC, which requires additional backpropagation through the critic ensemble. Moreover, since DAC initializes both actors with identical parameters, they must share the same network architecture. However, as indicated by Figure 4, simply copying parameters between them offers only minimal performance enhancement. In light of this, we believe that the necessity for identical architectures can be mitigated by employing techniques like delayed policy updates or by learning rate scheduling.\u201d \n\nAs DAC builds on SAC, we focus on limitations stemming from components specific to DAC. We add a line in the limitations section mentioning that DAC inherits limitations of SAC."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8610/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700001026724,
                "cdate": 1700001026724,
                "tmdate": 1700156739753,
                "mdate": 1700156739753,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "piO2NfXQLS",
            "forum": "op19LjpHkH",
            "replyto": "op19LjpHkH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8610/Reviewer_7ruN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8610/Reviewer_7ruN"
            ],
            "content": {
                "summary": {
                    "value": "The optimistic have different impact on actor and critic update, i.e., the overestimation of critic may result in sub-optimal policy while the optimistic policy can yield lower regret. Thus motivated, this work propose to decouple the actor-critic by using different actors for TD learning and exploration. The proposed method is tested on locomotion tasks with various replay ratio and achieve better performance than previous work."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper is well-motivated by the issues in the single policy regime in the conventional actor-critic.\n2. The proposed method shows less sensitivity to the hyperparameter thanks to the adaptive adjustment of the optimistic level, which is different from the related work Optimistic Actor-Critic (OAC)."
                },
                "weaknesses": {
                    "value": "See the questions below"
                },
                "questions": {
                    "value": "1. It is unclear how does Figure 2b and 2c shows the critic disagreement for different actors?\n2. If it is possible to use non-linear approximation of Q-value in Eqn. (6)\n3. What is the impact of the ensemble size as in Eqn. (4)\n4. Some notations can be confusing, i.e., $|\\mathcal{A}|$ is used to denote the action dimensionality where normally it is used as the cardinality of the action space."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8610/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698738099727,
            "cdate": 1698738099727,
            "tmdate": 1699637077399,
            "mdate": 1699637077399,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "R35eA94nZo",
                "forum": "op19LjpHkH",
                "replyto": "piO2NfXQLS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8610/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8610/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 7ruN"
                    },
                    "comment": {
                        "value": "We thank the Reviewer for their time writing the review. We are happy that the Reviewer finds our approach \u201cwell-motivated\u201d and appreciated the proposed adaptive mechanisms that address OAC shortcomings. We would like to point the Reviewer\u2019s attention to the new results added to our experiments (described in our meta-comment). In those results, DAC appears to significantly outperform model-based DMC SOTA (TD-MPC) and to the best of our knowledge represents the best recorded sample efficiency on the dog/humanoid domains of a model-free RL algorithm . Please find our responses to the Reviewer\u2019s questions below.\n\n> It is unclear how does Figure 2b and 2c shows the critic disagreement for different actors?\n\nFigure 2b & 2c show conservative and optimistic policies and related sampled state-action samples (denoted by different dot colors - red for optimistic and black for conservative). As follows, the policies are not massively different which is due to a small KL divergence between the two policies which is enforced by DAC. Despite similarity of the policies, the optimistic policy allocates some of the allowed KL budget to achieve bigger variance than the baseline conservative policy (indicated by the smaller likelihoods of the optimistic policy); secondly, the optimistic policy leads the agent to the corners of the state-action space which as shown in Figure 2a yields most critic disagreement. If the Reviewer wishes, we are happy to create a new graph where the optimistic policy is less penalized for the divergence and should thus lead to more contrasting policies.  \n\n> If it is possible to use non-linear approximation of Q-value in Eqn. (6)\n\nEqn. 6 describes the optimistic policy rule used in OAC (not in our proposed DAC), which as described by the authors of OAC used a linear approximation. To the best of our knowledge, some of the closed-form solutions used in OAC require the model to be linear. In fact, one of the key novelties of DAC that contrast to previous work is that it uses a non-linear model of Q-value upper-bound.\n\n> What is the impact of the ensemble size as in Eqn. (4)\n\nWith more than two critics, the minimum of the ensemble stops being equivalent to Eqn. 5 with $\\beta^{lb} = -1$. However, it can be easily calculated what $\\beta^{lb}$ corresponds to a clipped double Q-learning rule with more than two critics (this is discussed in [1]). We believe that extending DAC to a bigger ensemble is an interesting direction. Approaches like REDQ take a subset of the ensemble for every TD calculation and we think that this is a design choice that would have to be thoroughly ablated in the context of DAC. As such, we believe that it mandates an independent research endeavor. \n\n> Some notations can be confusing, i.e., |A| is used to denote the action dimensionality where normally it is used as the cardinality of the action space.\n\nWe used the notation inherited from the SAC manuscript, which uses |A| to denote control dimensions. We though that such notation is fitting, as we consider control over continuous space. We are happy to change the notation according to the Reviewer's proposition. \n\nWe hope that the above responses anwer the Reviewers' doubts about our proposed method. Furthermore, we hope that the new experimental results presenting DAC performance on complex DMC environments, as well as changes done to the manuscript increase the Reviewers confidence in DAC. If so, we kindly ask the Reviewer to consider adjusting the score of our work. \n\n[1] - Ciosek, Kamil, et al. \"Better exploration with optimistic actor critic.\" Advances in Neural Information Processing Systems. 2019."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8610/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700001701191,
                "cdate": 1700001701191,
                "tmdate": 1700045124975,
                "mdate": 1700045124975,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dsw15YIfOW",
                "forum": "op19LjpHkH",
                "replyto": "R35eA94nZo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8610/Reviewer_7ruN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8610/Reviewer_7ruN"
                ],
                "content": {
                    "title": {
                        "value": "Thank the authors for the response"
                    },
                    "comment": {
                        "value": "I appreciate the authors addressing my concerns and I will keep my original score."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8610/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700531419447,
                "cdate": 1700531419447,
                "tmdate": 1700531419447,
                "mdate": 1700531419447,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Mw5fsxlJ6J",
            "forum": "op19LjpHkH",
            "replyto": "op19LjpHkH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8610/Reviewer_XQ6P"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8610/Reviewer_XQ6P"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes the Decoupled Actor-Critic (DAC) algorithm that leverages two actors, one optimistic for efficient environment exploration and one conservative for stable learning. DAC further features a adaptive mechanism for setting the optimism trade-off to better account for the impact of reward scales. The performance of DAC is evaluated on a variety of tasks from the DeepMind Control Suite and compares favorably to the selected baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "-\tThe approach of combining optimistic exploration with conservative updating is very neat\n-\tEvaluation on 10 seeds with a multitude of baselines is great\n-\tOverall well written / structured paper that is easy to follow\n-\tPromising results, while some adjustments should be made regarding the experimental evaluation as discussed below"
                },
                "weaknesses": {
                    "value": "-\tDreamer results in Figure 11 are looking good, while harder tasks such as Quadruped Run and Humanoid Walk are missing \u2013 Dreamer-v2 is able to solve these tasks well even for visual-control, why not compare on tasks like Quadruped/Humanoid/etc. (or even extend to visual control)?\n-\tMore complex Control Suite tasks (Figures 12 & 13) like Humanoid Walk/Run should be run for longer as they have not converged, yet, while recent papers have also evaluated on the Dog domain\n-\tSAC is in general a good baseline, however, it would be nice to also compare performance to a more \u201cDMC-native\u201d baseline such as D4PG or (D)MPO to provide another reference point\n-\tThere are quite a few missing articles / words + typos that should be fixed\n-\tIt would be good to extend the discussion to model-based exploration agents, e.g. the works in [1] and [2] leveraged Dreamer/RSSM-based agents for visual control that explored via ensemble disagreement over rewards, where [1] maintained an optimistic upper confidence bound exploration policy as well as a distinct exploitation policy. [3] also explores uncertain returns with access to the nominal reward functions.\n\n[1] T. Seyde, et al. \u201eLearning to plan optimistically: Uncertainty-guided deep exploration via latent model ensembles\u201d, CoRL 2021.\n\n[2] R. Sekar, et al. \u201cPlanning to explore via self-supervised world models,\u201d ICML 2020.\n\n[3] P. Ball, et al. \"Ready policy one: World building through active learning,\" ICML, 2020."
                },
                "questions": {
                    "value": "-\tCould your provide an exemplary calculation of how the maximum average performance is calculated in Section F.1 \u2013 is the argmax over time? Why average over the tasks?\n-\tFish and Swimmer are very stochastic tasks due to random goal placement, but the results in Figure 12 & 13 still look extremely variable with confidence intervals barely visible. Could you double check how the evaluations are computed? Do all runs use the same evaluation seed (e.g. same eval goal across all seeds)?\n-\tIt might be worth briefly discussing the general impact of replay ratios across algorithm implementation, as the \u201clow replay\u201d regime with 3 gradient steps per 1 environment steps seems to be significantly higher than e.g., Acme\u2019s MPO default of 1 gradient steps per 8 environment steps (or 1/1 for Acme\u2019s SAC). The impact of replay ratios can be wild.\n-\tThe caption of Table 2 mentions 10 HARD DMC tasks, while Pendulum Swingup and Cartpole Swingup Sparse would not be considered hard (arguably, Cheetah, Quadruped, Fish aren\u2019t hard either)?\n-\tHave you also tried DroQ as an even more recent addition / alternative to REDQ?\n-\tHave you tried an ablation study on the number of critics? What patterns would you expect?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8610/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8610/Reviewer_XQ6P",
                        "ICLR.cc/2024/Conference/Submission8610/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8610/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698787568258,
            "cdate": 1698787568258,
            "tmdate": 1700510227160,
            "mdate": 1700510227160,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xYB9Qz9jgG",
                "forum": "op19LjpHkH",
                "replyto": "Mw5fsxlJ6J",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8610/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8610/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer XQ6P (1)"
                    },
                    "comment": {
                        "value": "We thank the Reviewer for a thorough and insightful review. We are excited that the Reviewer finds our approach \u201cneat\u201d, the paper \u201cwell written\u201d and the results \u201cpromising\u201d. We extended our experiments by a setting suggested by the Reviewer - 3mln steps on humanoid and dog tasks. Furthermore, we added an experiment verifying our claim regarding overestimation and plan on number of changes in the manuscript. We are excited to share that the new results further indicate that DAC is a substantial improvement over the existing model-free approaches. We described the new additions in detail in our meta-response. Below, we directly respond to the Reviewers\u2019 questions. \n\n> Dreamer results in Figure 11 are looking good, while harder tasks such as Quadruped Run and Humanoid Walk are missing \u2013 Dreamer-v2 is able to solve these tasks well even for visual-control, why not compare on tasks like Quadruped/Humanoid/etc.?\n\nWe used the results provided by the authors of DreamerV3 in their official repository. As such, we restricted evaluation to the 18 DMC tasks considered in DreamerV3 prio evaluation. DreamerV3 has pretty substantial compute requirements as compared to model-free algorithms. To that end, we are unsure if the potential small changes to the evaluation (there are already 18 tasks in the benchmark) mandate the cost of running Dreamer on additional environments.\n\n> (or even extend to visual control)?\n\nWe agree that prio-based control is not where Dreamer shines. However, we also think that using DAC in visual tasks would require non-trivial architectural choices. Such non-trivial choices stem from the intersection of using two actor networks, the common practice in visual control to use a shared encoder for actor-critic structure, and the recently researched impact of gradient weighting between actor-critic in visual tasks [1]. As such, we would have to ablate on various design choices regarding architecture and gradient passing in our extended actor-critic context. As such, we think that extending DAC to visual control validates a separate research endeavor. We kindly ask the Reviewer to note that it is a common practice to evaluate a continuous-control RL algorithm on a prio-based benchmark alone [2, 3, 4].\n\n> More complex Control Suite tasks (Figures 12 & 13) like Humanoid Walk/Run should be run for longer as they have not converged yet, while recent papers have also evaluated on the Dog domain\n\nDMC 500k/1mln has become a popular evaluation protocol used in recent works [1,5,6]. We think that running many tasks of varying difficulty for a consistent amount of timesteps makes sense from the aggregate evaluation perspective. However, we also agree that harder, unsolved tasks are more interesting than well researched propositions like cheetah. To this end, we added a new experiment: dog {walk, run, trot} and humanoid {walk, run} on 3mln environment steps - following the Reviewer\u2019s expectations. We describe the experiment in our meta-response. There, we compare SR-SAC, DAC and a recent baseline pointed by the Reviewer (TD-MPC). Please find a summary table below:\n\n|    task   | SR-SAC |  DAC |   DAC+  | TD-MPC | DAC+ | TD-MPC |\n|:---------:|:------:|:----:|:-------:|:------:|:----:|:------:|\n|    mean   |   214  |  439 | **607** |   546  |  **756** |   725  |\n| env steps |  1mln  | 1mln |   1mln  |  1mln  | 3mln |  3mln  |\n\nIn particular, SR-SAC does not seem to work on dog domain. This result is consistent with the evaluation performed in the TD-MPC manuscript. \n\n> SAC is in general a good baseline, however, it would be nice to also compare performance to a more \u201cDMC-native\u201d baseline such as D4PG or (D)MPO to provide another reference point\n\nWe are happy to share that we added D4PG and MPO to the Dreamer evaluation. The results are consistent with the Dreamer manuscript (ie. DAC > DreamerV3 > D4PG > MPO)\n\n> There are quite a few missing articles / words + typos that should be fixed\n\nWe made a sweep through the manuscript, we will make sure that the camera-ready version is fixed!\n\n> It would be good to extend the discussion to model-based exploration agents (...)\n\nThank you for pointing us to those works! We have added a paragraph to the Related Work section detailing how DAC related to this literature."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8610/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700002465687,
                "cdate": 1700002465687,
                "tmdate": 1700157120953,
                "mdate": 1700157120953,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Vo0XGLZ0c7",
                "forum": "op19LjpHkH",
                "replyto": "aX7H4kVxud",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8610/Reviewer_XQ6P"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8610/Reviewer_XQ6P"
                ],
                "content": {
                    "title": {
                        "value": "Response to Rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for your extensive replies. I think the additional experiments and explanations improved the paper and I'm happy to increase my rating. Regarding the computation of aggregate scores, it might make sense to add a brief variation of the above explanation to the manuscript for ease of readability."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8610/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700510184383,
                "cdate": 1700510184383,
                "tmdate": 1700510184383,
                "mdate": 1700510184383,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9O3zH08SHq",
            "forum": "op19LjpHkH",
            "replyto": "op19LjpHkH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8610/Reviewer_SCgY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8610/Reviewer_SCgY"
            ],
            "content": {
                "summary": {
                    "value": "The paper claims that there seems to be a conflicting demand in actor-critic architecture: the critic tends to overestimate so conservatism is needed when computing bootstrap target; however, the actor should act optimistically to improve sample efficiency/reduce regret. The authors propose dual actor-critic to reconcile the problem: there are both conservative actor and optimistic actor, and an ensemble of critics used to compute the lower/upper bound of value estimates. The basic idea is to let actor acts optimistically while the other actor act is used for maximize lower bound of the Q values. The algorithm also adds some heuristic designs such as minimizing the KL divergence between the two actors, learning the optimism control parameter and the KL divergence weight. Empirical results are provided to show the effectiveness of the algorithm."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper includes many experiments, which might provide heuristics for application-oriented tasks; \n\n2. The paper presents its algorithm clearly. \n\n3. The studied problem regarding balancing optimism and potential overestimation is interesting."
                },
                "weaknesses": {
                    "value": "The proposed algorithm is mostly designed by heuristics and the implementation details are not theoretically justified. Although I do not think theoretical support is necessary for a good paper, I expect empirical evidence to verify the critical claims/algorithmic designs of this paper (see below). \n\nFurthermore, since the proposed algorithm is basically a synthesis of different intuitive designs, a discussion of where the algorithm would converge to should be provided. Currently the paper is written in a way that different updating rules are introduced; I expect to see a clear objective function (maybe with constraints) of Algorithm 1, so readers can easily see what it is optimizing, can the authors write it down in the rebuttal? \n\nWhen simultaneously maximizing both lower and upper bound of the Q values, would it squash all action values higher and still result in overestimation? \n\nempirically: \n1. Verify the proposed method indeed mitigate overestimation comparing with an algorithm without using any correction, e.g., compare the estimate value and MC estimation; the current version of the paper directly using evaluation return as a performance measure, which, I think lacks justification, as it is unclear where the improved performance results from; \n\n2. The added optimism is essentially an exploration strategy, some baselines aiming at exploration should be also compared. \n\n3. Ablation study should be provided to justify the following design choices: the effect of ensemble (and do those baselines use ensemble too?), the necessity of optimizing optimism and KL weight (can you use some intuitive choices instead of learning them)? \n\nAny comments how do you decide the order of the updating rules 10-15? And how their learning rates are chosen? \n\n4. The algorithm introduces many more hyper parameters comparing with commonly seen SAC or TD3 due to the added components in the losses, I would not consider the comparison to be fair with other baselines, unless evidence of similar efforts have been made to thoroughly sweep baseline\u2019s hyper-parameters is provided. \n\n5. Eq 7, how do you calculate the gradient w.r.t optimistic actor parameters, it appears the gradient should be also propagated through/to the first two Q functions and these Qs are interdependent with actor. \n\n6. The motivation of the paper is to avoid overestimation while keep good exploration. Isn\u2019t it quite intuitive to combine some exploration method with methods that mitigate overestimation?  it shouldn\u2019t be difficult to design such a baseline as mitigating overestimation typically require multiple critics and uncertainty estimate could be derived from the ensemble for exploration purpose. What is the proposed algorithm\u2019s advantage? \n\n7. The experimental design of studying replay ratio appears to disconnect with the primary motivation of this paper; this should be put in the appendix."
                },
                "questions": {
                    "value": "see above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8610/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8610/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8610/Reviewer_SCgY"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8610/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698789645620,
            "cdate": 1698789645620,
            "tmdate": 1700531307575,
            "mdate": 1700531307575,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DnziC9mnp8",
                "forum": "op19LjpHkH",
                "replyto": "9O3zH08SHq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8610/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8610/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer SCgY (1)"
                    },
                    "comment": {
                        "value": "We thank the Reviewer for their time. We are happy that the Reviewer finds our algorithm presentation clear, the considered problem interesting and that the Reviewer appreciates the amount of experiments performed within our work. We particularly thank the Reviewer for the suggestion regarding the overestimation experiment. Furthermore, we perform additional experiments on the dog domain. There, we record that DAC achieves significantly better performance than model-based and model-free DMC SOTA. We describe the experiments in our meta-response. Below, we respond in detail to each of the Reviewer\u2019s questions.\n\n> The paper claims that there seems to be a conflicting demand in actor-critic architecture\n\nThis conflict is not something that we discuss first - it has been thoroughly discussed in [1, 2]. We hope that our writing does not imply otherwise. \n\n> The proposed algorithm is mostly designed by heuristics and the implementation details are not theoretically justified.\n\nWe would like to note that DAC follows fundamental concepts in RL theory, such as the actor-critic framework (stemming from the Policy Gradient Theorem), optimism in the face of uncertainty and off-policy learning. Its design incorporates widely researched principles of policy and value iteration in the context of maximum entropy formulation, which all are foundational for the field of RL. Even the automatic optimization of optimism and KL regularization can easily be linked to dual optimization [3], used in similar contexts in many other RL algorithms [4, 5]. However, we realize that our manuscript focuses on evaluating implementation of theoretically founded principles in practice, and as such could be considered an empirical work. \n\n>  I expect empirical evidence to verify the critical claims/algorithmic designs of this paper \n\nFigures 4 & 5 extensively evaluate the performance impact of many design decisions (10 DAC simplifications & 14 hyperparameter configurations). We note that 7/10 DAC simplifications and 14/14 hyperparameter configurations perform better than baseline SAC.\n\n> Furthermore, since the proposed algorithm is basically a synthesis of different intuitive designs, a discussion of where the algorithm would converge to should be provided.\n\nIn Section 3.2 we write: \u201cThis mechanism allows for separate entropy for TD learning and exploration while retaining standard convergence guarantees of AC algorithms. In fact, (...) it follows that in the limit both actors recover a policy that differs only by the level of entropy\u201d. Furthermore, since SAC is an off-policy algorithm and DAC differs only by samples that land in the experience buffer, we believe it trivially follows that DAC retains convergence guarantees of SAC. \n\n> Currently the paper is written in a way that different updating rules are introduced; I expect to see a clear objective function (maybe with constraints) of Algorithm 1, so readers can easily see what it is optimizing, can the authors write it down in the rebuttal?\n\nThe current version of the paper features a pseudo-code with links to every equation used in the calculation. Furthermore, Sections 3 & A2 detail what and why DAC is optimizing. If the Reviewer points us to what particularly the Reviewer finds confusing, we are happy to adjust the text. \n\n> When simultaneously maximizing both lower and upper bound of the Q values, would it squash all action values higher and still result in overestimation? \n\nWe think that there is a misunderstanding. Overestimation is a property of the critic and is believed to stem from TD learning [2,5,6]. DAC features no \u201csimultaneous maximization of lower and upper bound of the Q-values\u201d and the critic used in DAC learns using a traditional clipped-double Q-learning [4,6]. The lower and upper bounds policies are optimized by conservative and optimistic actors respectively (not \u2018simultaneously\u2019), and the optimistic actor does not interact with TD learning in DAC design. As such, there is no reason for overestimation in DAC beyond overestimation that occurs in SAC. This is underlined in Section 1 of our manuscript: \u201cDAC employs two actors, each independently optimized using gradient backpropagation with different objectives. The optimistic actor is trained to maximize an optimistic Q-value upper-bound while adjusting optimism levels automatically. This actor is responsible for exploration (sampling transitions added to the experience buffer). In contrast, the conservative actor is trained using standard lower-bound soft policy learning and is used for sampling temporal-difference (TD) targets and evaluation.\u201d"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8610/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700003118211,
                "cdate": 1700003118211,
                "tmdate": 1700044940353,
                "mdate": 1700044940353,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ptUkkzBoxM",
                "forum": "op19LjpHkH",
                "replyto": "9O3zH08SHq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8610/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8610/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer SCgY (3)"
                    },
                    "comment": {
                        "value": "> The algorithm introduces many more hyper parameters comparing with commonly seen SAC or TD3 due to the added components in the losses, I would not consider the comparison to be fair with other baselines, unless evidence of similar efforts have been made to thoroughly sweep baseline\u2019s hyper-parameters is provided. \n\nWe would like to respond to the above assertion in three points:\n\n1. SAC/TD3 are arguably the most popular RL algorithms at the time of writing this response. Both were evaluated numerous times on DMC locomotion, which is one of the most popular RL continuous-control benchmarks. As such, we believe that the hyperparameter settings used for SAC/TD3 were arguably more \u201csweeped\u201d than those of DAC. The hyperparameter choices used for our implementations of SAC and SR-SAC are used in the current DMC locomotion SOTA (SR-SAC) [9].\n\n2. We kindly ask the Reviewer to note that reusing hyperparameters proposed in the earlier works is a common practice, especially in the case of such well-known benchmarks as DMC/gym [1,2,4,5,6,7,8,9,10,11].\n\n3. Finally, we would like to point towards experiments presented in Figure 5 of our original manuscript. There, we evaluate 14 hyperparameter configurations of DAC (our entire sweep), as well as the reference SAC performance. According to the results, all 14/14 DAC configurations significantly outperform SAC. \n\nConsidering the above arguments, we kindly ask the Reviewer to reconsider his \u201cunfair comparison\u201d assertion. \n\n> Eq 7, how do you calculate the gradient w.r.t optimistic actor parameters, it appears the gradient should be also propagated through/to the first two Q functions and these Qs are interdependent with actor. \n\nThe Eq 7 is implemented the same way a standard backprob through critic is implemented in DDPG, D4PG, SAC, TD3, OAC, TOP etc. It is not required to update the critics parameters as a result of this update, we can just use the gradient calculation to propagate it further to the actor. If the Reviewer wishes, we can further expand on how it is usually implemented. \n\n> The motivation of the paper is to avoid overestimation while keep good exploration. Isn\u2019t it quite intuitive to combine some exploration method with methods that mitigate overestimation? it shouldn\u2019t be difficult to design such a baseline as mitigating overestimation typically require multiple critics and uncertainty estimate could be derived from the ensemble for exploration purpose. What is the proposed algorithm\u2019s advantage? \n\nWe evaluate against OAC, TOP, SAC, REDQ and TD3 which all \u201ccombine some exploration method with methods that mitigate overestimation\u201d and all use multiple critics to derive an uncertainty estimate \u201cfor exploration purpose\u201d. Furthermore, in Figure 4 we consider 10 DAC simplifications, which all as well fit the Reviewer\u2019s description. We discuss the advantage of our approach in Section 3. \n\n> it shouldn\u2019t be difficult to design such a baseline\n\nWe design and evaluate a variety of baselines common to DAC. The results are presented in Figures 4 & 6. We are happy to add more baseline designs to our evaluations, if the Reviewer shares more details about the Reviewer\u2019s envisioned baseline. \n\n> The experimental design of studying replay ratio appears to disconnect with the primary motivation of this paper; this should be put in the appendix.\n\nWe argument why we study two replay ratio regimes in Section 1 & 4. In our view it is an important aspect in the light of modern RL studies, which was repeatedly shown to play a substantial role in the performance of RL algorithms. Also, as Reviewer XQ6P wanted us to expand the discussion of replay ratios, we would like to keep the analysis in the main body of the paper.\n\nWe thank the Reviewer for their time and valuable input. In the light of new experimental results, we would like to ask the Reviewer to reconsider his score of the manuscript."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8610/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700003580724,
                "cdate": 1700003580724,
                "tmdate": 1700003811472,
                "mdate": 1700003811472,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AHh6V0z3fh",
                "forum": "op19LjpHkH",
                "replyto": "ptUkkzBoxM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8610/Reviewer_SCgY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8610/Reviewer_SCgY"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your efforts. I updated my score."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8610/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700531290341,
                "cdate": 1700531290341,
                "tmdate": 1700531290341,
                "mdate": 1700531290341,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]