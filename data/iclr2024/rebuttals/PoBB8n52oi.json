[
    {
        "title": "SummaryMixing: A Linear-Complexity Alternative to Self-Attention for Speech Recognition and Understanding"
    },
    {
        "review": {
            "id": "goWrxaDWTQ",
            "forum": "PoBB8n52oi",
            "replyto": "PoBB8n52oi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5692/Reviewer_gqu7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5692/Reviewer_gqu7"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a linear-complexity based architecture as a replacement of self-attention for speech recognition to improve the efficiency. The proposed block has a local branch and a global branch to take both information into account. The local branch is based on a MLP, and the global branch has a MLP and then an average pooling among all the fram"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* Paper fits ICLR scope well.\n* The idea of reducing model complexity through replacing self-attention with linear layers is interesting.\n* The proposed method is solid."
                },
                "weaknesses": {
                    "value": "* Presentations need improvement. See question sections for details.\n* Literature reviews on ASR are very limited. Please have an individual paragraph in Section 1.1 to describe those.\n* Novelty is a concern. The proposed work is an incremental work on the top of BranchFormer and HyperMixer. Although there is a paragraph describing the relationship to HyperMixer, the difference seems to be more on the choice of linear functions. Please explain more on this.\n* The improvements from the proposed method seems to depend on the use of a transformer decoder (with self-attention). Need more experiments to verify the parity of the proposed method to self-attention in other conditions. See question sections for details."
                },
                "questions": {
                    "value": "Abstract: \u201cHowever, attention layers in trained speech recognizers tend to not capture fine-grained pair-wise information.\u201d Please explain what \u201cfine-grained pair-wise information\u201d is and why it is important (although they are explained in the following sections, abstract should be self-explanatory).\n\nFigure 1: The diagrams are a bit confusing. I understand that \u201cT\u201d refers to the total number of frames. However, audiences may likely think of them as different attention heads. Please consider a better way for the diagrams.\n\nSection 2.1 - Multi-head SummaryMixing. Please better explain the heads in SummaryMixing. In self-attention, heads are operating in parallel with different sets of parameters. However, here heads help to reduce the number of parameters. Please consider adding a diagram on this if it helps. In addition, by dividing x_t into n chunks, is it similar to chuck-wise attention or attentions with a limited context window? If that\u2019s the case, please avoid using \u201chead\u201d here.\n\nSection 3.3.1: The results shown in this section have a transformer decoder of 6-blocks-MHSA paired with each system. In this setup, the decoder will capture the global context, even if the encoder doesn\u2019t. However, we can only conclude that self-attention is redundant when self-attention based decoders are used.\nThe authors also conducted experiments without a transformer decoder (i.e., CTC only) on Branchformer in the supplemental materials. However, there are two more problems to figure out. 1. How does vanilla Conformer/Transformer with CTC performance in this setup? They are the most common architectures for productions than Branchformer. 2. These results are obtained with LM shallow fusion, which can be expensive for deployment. How would the proposed approach perform without shallow fusion? If the language information is needed, please also consider RNN-T decoder."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5692/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5692/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5692/Reviewer_gqu7"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5692/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698101740720,
            "cdate": 1698101740720,
            "tmdate": 1700605294951,
            "mdate": 1700605294951,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GNTsowZRMd",
                "forum": "PoBB8n52oi",
                "replyto": "goWrxaDWTQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5692/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5692/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you 1/2."
                    },
                    "comment": {
                        "value": "First, we wish to thank the reviewer for mentioning that SummaryMixing is a solid method that fits well the scope of ICLR. We wish to mention that we can provide an anonymized version to the reviewer if necessary. Questions, remarks, and concerns from the reviewer are addressed in the next paragraphs. \n\n\n**Weaknesses**\n\n*Presentations need improvement. See question sections for details.*\n\nThanks to the reviewer for helping clarify a few aspects of the manuscript. The detailed answers are given in the question section and the paper has been updated accordingly. \n\n*Literature reviews on ASR are very limited. Please have an individual paragraph in Section 1.1 to describe those.*\n\nAs suggested, we added a paragraph in Section 1.1 to summarize the ASR domain and how it relates to this work. We invite the reviewer to have another look at this Section.\n\n*Novelty is a concern. The proposed work is an incremental work on the top of BranchFormer and HyperMixer. Although there is a paragraph describing the relationship to HyperMixer, the difference seems to be more on the choice of linear functions. Please explain more on this.*\n\nFirst of all, we modified Section 2.2 to clarify the use of SummaryMixing both in Conformer and Branchformer as well as Section 2 and the Appendix to clarify the link with HyperMixer. Indeed, we also apply SummaryMixing with Conformer and achieve SOTA, but this was not clearly expressed in the first version of the manuscript. Now, if we see things from the Branchformer point of view, we agree with the reviewer, in proportion, the Sumformer implies only a few changes (and this is a strength from our point of view). However, we would like to first highlight that SummaryMixing is also applied to the Conformer architecture i.e. (not only Branchformer) as shown by Table 1 and the CTC-only experiments. Conformer with SummaryMixing beats the MHSA Conformer as well. As a more general answer, SummaryMixing can replace MHSA in any speech encoder.  Indeed, and as mentioned by the reviewer in the strength of this manuscript, SummaryMixing is about \u201creplacing self-attention with linear layers [...]\u201d not only for Branchformer. In this article we demonstrate that this is feasible with the two most used architectures for ASR: Conformer and Branchformer. Finally, we wish to highlight the fact that Branchformer was accepted at ICML while its contribution was to make two blocks of layers parallel instead of sequential \u2013 from our very own point of view, this does not represent much more novelty than SummaryMixing (quantitatively speaking), yet it was accepted and is now used by the community thanks to the easy open-access to the code, which is also true for SummaryMixing.\n\n*The improvements from the proposed method seems to depend on the use of a transformer decoder (with self-attention). Need more experiments to verify the parity of the proposed method to self-attention in other conditions. See question sections for details.*\n\nWe thank the reviewer for raising this excellent remark. We invite the reviewer to have a look at the new results added in the top comment of the reviews as well as the last paragrap of Section 3.3.1 We decided to train ASR systems with CTC only, i.e. without any attentional decoder, and reached basically the same performance between MHSA and SummaryMixing on dev-clean and tes-clean. Therefore, we can say that, in the specific context of this experimental protocol, MHSA is not needed to achieve SOTA ASR performance and that SummaryMixing can do just as well WER-wise while drastically speeding up the training and decreasing the VRAM consumption."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5692/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700153776256,
                "cdate": 1700153776256,
                "tmdate": 1700153776256,
                "mdate": 1700153776256,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1SY2n0a9rH",
                "forum": "PoBB8n52oi",
                "replyto": "bHWx2XgRI4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5692/Reviewer_gqu7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5692/Reviewer_gqu7"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the clarifications!"
                    },
                    "comment": {
                        "value": "I would like to appreciate the clarifications and improvements made to the manuscript. I updated my scores accordingly for the improvements."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5692/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700605269320,
                "cdate": 1700605269320,
                "tmdate": 1700605269320,
                "mdate": 1700605269320,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "h3YnKoBBZt",
            "forum": "PoBB8n52oi",
            "replyto": "PoBB8n52oi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5692/Reviewer_yNSy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5692/Reviewer_yNSy"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel method to replace self-attentions with a more computationally efficient summary mixing model for speech recognition and spoken language understanding. Rather than computing an NxN attention matrix, the authors propose to first compute the temporal mean of the sequence and combine that with local information extracted using a cgMLP branch. Experiments on multiple corpora show that the proposed approach can match the performance of standard self-attention and Branchformer."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper uses a very simple approach to substitute self-attention with attention between a temporal summary and the original vector that appears to work well in practice.\n\n2. Experiments are performed on multiple corpora for ASR (CommonVoice-Dutch, Italian, French), Librispeech, AISHELL, TED-LIUM 2, GSC and SLURP for SLU."
                },
                "weaknesses": {
                    "value": "1. The paper makes the assumption that MHSA in the encoder is not necessary based on Zhang et. al. and Peng. et. al. However, I believe the assumption may not be well supported because (a) Zhang et al point out that you don't need self-attentions in higher layers, not that you don't need self-attentions at all, and (b) Peng. et al which uses self-attentions to model global context while using cgMLP to model local context. Therefore, the rationale behind this fundamental assumption made in the paper seems unclear. \n\n2. SLURP uses read speech - the impact of temporal averaging here may be minimal, but may be very harmful in spontaneous conversational speech. Such explorations in my view are important in this work to claim that SummaryMixing can reach top-of-the-line in speech understanding. \n\n3. Writing could be more self-contained and clear in certain places - for example, HyperMixer is not well described which makes it hard to assess the relationship between the methods."
                },
                "questions": {
                    "value": "1. The authors claim that a score of 0.5 diagonality implies a uniform distribution and therefore, a simple average could achieve a similar token mixing. However, this is not obvious or clear to me. Could the authors explain?\n\n2. I was interested to know how the performance of summary mixing is impacted during inference by temporal averaging. As the sequence length increases, the sum vector becomes a mixture of more frames, degrading the ability to discriminate individual frames/tokens. Do the authors have any numbers showing ASR performance (WER) as a function of sequence length across models with self-attention and the proposed SummaryMixing ?\n\n3. As SLURP uses read speech, I was wondering if the authors had performed experiments on other corpora with spontaneous speech, for example, SLUE-VoxPopuli [1] for Named Entity Recognition (NER)?\n\n4. The paper mentions KWS results, but I don't see any in the results section.\n\n5. There are some papers [2,3,4] that show linear transformers can obtain comparable performance to MHSA for ASR contrary to what the authors claim. These works are also relevant and must be acknowledged in the paper. \n\n[1] \"SLUE Phase-2: A Benchmark Suite of Diverse Spoken Language Understanding Tasks\",\nSuwon Shon, Siddhant Arora, Chyi-Jiunn Lin, Ankita Pasad, Felix Wu, Roshan S Sharma, Wei-Lun Wu, Hung-yi Lee, Karen Livescu, Shinji Watanabe\n\n[2] Jingyu Sun, Guiping Zhong, Dinghao Zhou, Baoxiang Li, Yiran Zhong, \"Locality Matters: A Locality-Biased Linear Attention for Automatic Speech Recognition\"\n\n[3] \"Conformer-Based Speech Recognition with Linear Nystr\u00f6m Attention and Rotary Position Embedding\", \nLahiru Samarakoon, Tsun-Yat Leung\n\n[4] \"Fast Conformer with Linearly Scalable Attention for Efficient Speech Recognition\", Dima Rekesh et. al."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5692/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5692/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5692/Reviewer_yNSy"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5692/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698616019331,
            "cdate": 1698616019331,
            "tmdate": 1700609672440,
            "mdate": 1700609672440,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cFzVxTZEvv",
                "forum": "PoBB8n52oi",
                "replyto": "h3YnKoBBZt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5692/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5692/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you 1/2."
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for taking the time to read and review this article. We are glad to read that the reviewer found SummaryMixing to be a simple yet effective approach to replace MHSA. We wish to mention that we can provide an anonymized version to the reviewer if necessary. The concerns and remarks of the reviewer are addressed in the following: \n\n**Weaknesses**\n\n*The paper makes the assumption that MHSA in the encoder is not necessary based on Zhang et. al. and Peng. et. al. [...] Zhang et al point out that you don't need self-attentions in higher layers, not that you don't need self-attentions at all [...] Peng. et al which uses self-attentions to model global context while using cgMLP to model local context. Therefore, the rationale behind this fundamental assumption made in the paper seems unclear.*\n\nWe agree with the reviewer that this motivation is key. This is why we changed section 1 to better detail this motivation. To be more precise and answer this question, we would like to highlight that with this article, our motivation is that both works seem to indicate that self-attention **may** not be useful under certain conditions that the reviewer mentioned. Before experimenting, we did not know that self-attention could be entirely replaced by SummaryMixing \u2013 the literature just gave us a hint in that direction, and that is why the paper decided to propose and explore SummaryMixing. In the Branchformer paper, Figure 6 clearly indicates that the self-attention weights are close to being uniformly distributed from the very first layer to the last layer. Based on the definition of \u201cdiagonality\u201d in Figure 6 of the Branchformer paper, diagonality 0.5 implies weights are uniformly distributed, and almost all layers have diagonality between 0.5 and 0.6. Also, uniformly distributed attention weights imply that the weighted sum w.r.t. weights is equal to (up to a constant factor) an unweighted sum. Although the attention weights are not strictly uniformly distributed based on Figure 6, it gives us hints to propose the SummaryMixing operation. Also, although the diagonality is measured for Branchformer, we also test SumamryMixing for Conformer in our paper. \n\n*SLURP uses read speech - the impact of temporal averaging here may be minimal, but may be very harmful in spontaneous conversational speech. Such explorations in my view are important in this work to claim that SummaryMixing can reach top-of-the-line in speech understanding.*\n\nWe would like to address this point in two parts. First, and following the reviewer\u2019s request, we conducted an experiment on a very noisy dataset with conversational speech: AMI. We wish to mention that there is no AMI ASR recipe available in SpeechBrain yet, and we tried our best based on the literature and the results are not far away from the numbers in other tools, like K2 or ESPnet.  From the results, the reviewer can see that SummaryMixing still works competitively compared to MHSA. It also performs much better than the previous best baseline: the Fastformer linear operation baseline fails to reach the same level of performance.\n\n **WERs on AMI dataset (conversational and noisy speech)**\n| Model (Conformer Encoder-Decoder CTC+Attention) | Dev | Test |\n| -------- | -------- | -------- |\n| MHSA (41.4M)    | 21.9 | **18.2**|\n| SummaryMixing (39.9M) | **21.8**| 19.4|\n| Fastformer (39.9M) | 25.1 | 21.3|\n\nSecond, ASR is different from SLU, and we therefore can not affirm that such findings could be extended to SLU. As a matter of fact, we decided to follow the same experimental protocol as the [Branchformer paper](https://arxiv.org/pdf/2207.02971.pdf) accepted at ICML, hence the choice of the SLURP and Google Speech Command datasets. AMI was the only available (commercially exploitable and in SpeechBrain) dataset that we could use to try quickly SummaryMixing on very noisy and conversational speech."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5692/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700153693367,
                "cdate": 1700153693367,
                "tmdate": 1700153693367,
                "mdate": 1700153693367,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4J0tuUamMR",
                "forum": "PoBB8n52oi",
                "replyto": "MTFooAxEax",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5692/Reviewer_yNSy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5692/Reviewer_yNSy"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the reviewers for their responses. \n\n\nRe. motivation, the following is not clear to me - could you explain ? \n\n```\nAlso, uniformly distributed attention weights imply that the weighted sum w.r.t. weights is equal to (up to a constant factor) an unweighted sum. Although the attention weights are not strictly uniformly distributed based on Figure 6, it gives us hints to propose the SummaryMixing operation. \n```\nI understand the progression of events better now, and it seems like this motivation only supports a modification of the Branchformer. \n\nI appreciate the additional AMI results, but it would have been ideal to do such experiments on speech understanding corpora. I encourage authors to try and do this experiment and add it to the camera ready if accepted. \n\nI appreciate the additional analysis on temporal averaging and the note on KWS. The authors have also added some information on related work in ASR. \n\nAbout linear transformers: The cosformer used by Sun. et. al does, in fact, appears to match or beat the standard conformer results from their paper, and in my mind, is a state-of-the-art baseline for speech[1] and non-speech[2] tasks. The code for the self-attention implementation is also public here[3]. This would have been good to compare to. \n\nBased on the provided responses, I have updated my score. \nI would like to encourage the authors if given a chance to conduct more expansive tests across speech tasks like speech summarization, and non-speech tasks like Wiki-LM to make this paper more relevant and useful to broader audiences. \n\n\n[1] Jingyu Sun, Guiping Zhong, Dinghao Zhou, Baoxiang Li, Yiran Zhong, \"Locality Matters: A Locality-Biased Linear Attention for Automatic Speech Recognition\"\n[2] \"cosFormer: Rethinking Softmax in Attention\", Zhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan, Lingpeng Kong, Yiran Zhong, ICLR 2022\n[3] https://github.com/OpenNLPLab/cosFormer"
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5692/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700609644958,
                "cdate": 1700609644958,
                "tmdate": 1700609644958,
                "mdate": 1700609644958,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mrGvEyzx2G",
                "forum": "PoBB8n52oi",
                "replyto": "h3YnKoBBZt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5692/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5692/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answering the last few questions"
                    },
                    "comment": {
                        "value": "Dear reviewer,\n\nWe wish to thank you for acknowledging the changes and updating your score.\n\nFirst, we would like to clarify the motivation issue. Branchformer is not the only architecture showing MHSA layers behave like linear operations. In the third paragraph of Section 1, we mention works that show some layers in Transformers and Conformer also behave as linear operations. This is what motivated us to try SummaryMixing with both Branchformer and Conformer which are, currently, the two state-of-the-art and most common speech encoders. We hope that this clarifies the motivation. If not, please let us know and we will extend the answer further.\n\nSecond, we wish to address the request for more experiments across tasks. Another work submitted to ICLR 2024 trying to speed up ASR training (and ASR only,) under a much more restricted experimental scenario (only 3 datasets, and we use 2 of them) than SummaryMixing, has received excellent scores without any mention of a potential scope issue or audience issue. Both models are not strictly comparable -- their is much more complex to implement and use and still has a quadratic-time complexity while achieving similar WER performance to ours. However, we must remark that the datasets and tasks that we already cover in SummaryMixing are more numerous than this other submission which will most likely be accepted according to the reviewers.  With that being said, we are now targeting a few more experiments as mentioned by the reviewer, but we of course can't give any results before the deadline of the discussion period. \n\nFinally, we agree with the reviewer on the cosFormer -- and we will try it for the camera-ready version (if accepted). We also wish to highlight that the new Related Work now includes a mention to \"Locality Matters: A Locality-Biased Linear Attention for Automatic Speech Recognition\".\n\nWe thank again the reviewer for the discussion, and we remain available until the end of the rebuttal period (24 hours away).\n\nThe authors."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5692/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700654630464,
                "cdate": 1700654630464,
                "tmdate": 1700673109172,
                "mdate": 1700673109172,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QHRErrrolL",
            "forum": "PoBB8n52oi",
            "replyto": "PoBB8n52oi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5692/Reviewer_fkrd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5692/Reviewer_fkrd"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposed a global summarization layer as a replacement for MHSA in several ASR architectures such as branchformer and Conformer. This layer or module is introduced and compared with several parallel implementation in an open source project SpeechBrain. \nThe newly proposed component dubbed summaryMixing has a linear computation cost on the encoder ASR components.\nExpectation is that this will close the gap with MHSA but actually authors found this surpasses it in their tests."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper proposes a generalization of hyperMixer and apply it to Speech tasks. \nThe generalization is simple but its effectiveness is surprising. \nThe proposed architecture works very well in comparison with many baselines and architectures (Conformer, Branchformer, ContextNet, Branchformer +fastattention, Branchformer w/o attention, ...)\nThe obtained and reported results are very competitive with various ASR benchmarks and setups.\nIt is worth noting the surprisingly good performance of one of the simplest baseline, the branchformer w/o attention, which authors acknowledge and highlight.\nThey consider encoder architecture only to show the unnecessary MHSA complexity for ASR tasks is not coming exclusively from the top decoder.."
                },
                "weaknesses": {
                    "value": "There is a couple of points where the newly proposed component experimentation could have been improved.\nThe authors should clarify the ASR architecture in the final version since the current description of the ASR architecture is vague and prone to confusions-- see  questions below --.  I think this is something that authors might be aware of but prefer refer readers to the SpeechBrain recipes and experimentation.  This might leave a shallow reader without immediate interest in reproducing or delving into the code and recipes with a confusing architecture.\n\nThe approach has some limitations, so it would be intriguing to examine the performance of the proposed encoder in long-context ASR. \nSince this is a leaning representation conference, some experiments on NLP tasks, as in the original HyperMixer paper, would have attracted more readers.\n\nGiven the nature of speed optimization and comparison with MHSA, it would be interesting for the readers to know the specifics of the MHSA implementation as there are many very efficient implementations available nowadays.\n\nFinally, it would have been nice to compare the proposed approach in both batch and online ASR."
                },
                "questions": {
                    "value": "The following questions might need some attention into the manuscript:\n* which is the global architecture and loss ? It is clear that it is a encoder decoder based architecture with CTC loss, but this is an important detail mentioned in 1 line. How many encoder layers each arch has ? which are the details of the decoder ? \n* Could the MHSA improve as the context becomes larger ? \n* which is the MSHA implementation have you used ?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5692/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698880675714,
            "cdate": 1698880675714,
            "tmdate": 1699636595495,
            "mdate": 1699636595495,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "koEDLBUkNS",
                "forum": "PoBB8n52oi",
                "replyto": "QHRErrrolL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5692/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5692/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you 1/2."
                    },
                    "comment": {
                        "value": "We wish to thank the reviewer for highlighting the effectiveness of the proposed method and mentioning that the results are very competitive with various ASR benchmarks and setups. We are glad to see that the reviewer finds the results interesting and appealing to the community. We wish to mention that we can provide an anonymized version to the reviewer if necessary. In the following, we address the remaining comments and questions of the reviewers.\n\n**Weaknesses**\n\n*I think this is something that authors might be aware of but prefer refer readers to the SpeechBrain recipes and experimentation. This might leave a shallow reader without immediate interest in reproducing or delving into the code and recipes with a confusing architecture.*\n\nWe totally agree with the reviewer. The reason for this design choice is that, for instance, the yaml (hyperparameters definition) of the model trained on Librispeech that we have released on GitHub (please do not look for it, it\u2019s not anonymized, we can provide it here if necessary) contains 260 lines of model definition. Hence, we initially thought that this would be better for the reader to just have access to our parameter files (**available on Github for all experiments**) as well as the SpeechBrain recipes would be sufficient. However, we added a simplified version of the model definition that hopefully will help the reader to get a better understanding of the model architecture in the Appendix. We invite the reviewer to have a read at the Appendix and let us know if any more information should be added. \n\n*The approach has some limitations, so it would be intriguing to examine the performance of the proposed encoder in long-context ASR. Since this is a learning representation conference, some experiments on NLP tasks, as in the original HyperMixer paper, would have attracted more readers.* \n\nWe agree with the reviewer and invite them to have a look at the top comment illustrating the new results on the evolution of the WER as a function of the sequence length of SummaryMixing vs Fastformer vs MHSA. We also added these results to the Appendix of the manuscript. From the results, it is clear that SummaryMixing does not suffer more than MHSA from longer sentences, it actually looks like the opposite. We also share the reviewer\u2019s view on the application domain. However, our primary domain of expertise is speech processing, and it is unclear if 10 days would be sufficient for us to build a trustworthy experimental protocol on NLP tasks. \n\n*Given the nature of speed optimization and comparison with MHSA, it would be interesting for the readers to know the specifics of the MHSA implementation as there are many very efficient implementations available nowadays.*\n\nWe agree that this is not clear from the current form of the manuscript. We invite the reviewer to have a look at section 3.1 to make sure that the revised article solves this issue. In practice, the used MHSA implementation is the one from SpeechBrain (with relative positional encoding for better ASR performance) implemented in PyTorch [here](https://github.com/speechbrain/speechbrain/blob/bd27e99119edfe6b4f530be16d46cce28af260b3/speechbrain/nnet/attention.py#L362). For the speed perspective, we compared it against the regular MHSA from PyTorch (visible [here](https://github.com/speechbrain/speechbrain/blob/bd27e99119edfe6b4f530be16d46cce28af260b3/speechbrain/nnet/attention.py#L642)  in SpeechBrain) and found no significant degradation in speed. We however did not compare this to other forms of very efficient MHSA implementation such as FlashAttention as they benefit from optimisations linked to the compilation and it would be unfair to compare this with SummaryMixing that is a raw PyTorch implementation.  \n\n*Finally, it would have been nice to compare the proposed approach in both batch and online ASR.*\n\nFor now, we agree that only offline ASR is addressed. The primary reason for this choice is that, at the time of writing, the only available online ASR pipeline in SpeechBrain is still under Pull Request. We also fully understand that this is not a fully valid answer for not trying it on K2 for instance, but the latter would also require a complete change of environment, with different baselines (e.g. fastformer, hypermixer, the branchformer, and context net does not exist in K2). This is not realistic for a 9 page articles and within 10 days."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5692/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700153590979,
                "cdate": 1700153590979,
                "tmdate": 1700153590979,
                "mdate": 1700153590979,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bAK54Iydlj",
            "forum": "PoBB8n52oi",
            "replyto": "PoBB8n52oi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5692/Reviewer_wGqz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5692/Reviewer_wGqz"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel neural network architecture called SummaryMixing for speech recognition. The motivation of this work is to replace the heavy computational cost yielded by transformer self-attention blocks with a SummaryMixing block on top of the branchformer architecture. SummaryMixing shows effectiveness by offering a competitive performance from the original branchformer architecture, especially with the low computational cost, mainly within the CTC framework."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Simple and yet another neural network architecture by extending the branchformer architecture to capture global characteristics via SummaryMixing and local characteristics via cgMLP.\n- Showing the low computational cost compared with the original branchformer\n- Good reproducibility (open source implementation and the use of public data)."
                },
                "weaknesses": {
                    "value": "- The survey of the efficient transformer (conformed) is not sufficient. There are a lot of efficient transformers in the NLP and ML field (e.g., https://arxiv.org/pdf/2009.06732.pdf). Even if we limit the discussions to speech applications, there are a lot of methods (e.g., squeezeformer, efficient conformer, emformer, etc.). \n- The performance improvement from the conventional method (e.g., fastformer) is marginal.\n- The method is on top of branchformer, which already uses Fastformer, and its technical novelty is weak.\n- Several descriptions (e.g., the surveys, distinction from HyperMixer/MLP Mixer, and detailed architectures) are unclear.\n  - Frankly, I could not understand the distinction of the SummaryMixing block from HyperMixer/MLP Mixer only from Section 2.1, mainly due to the lack of an explanation of what HyperMixer/MLP Mixer is.\n  - It is difficult to understand the detailed architectures only from this description: \"In particular, the transformation (f), summary (s), and combiner (c) functions are all implemented as a dense linear layer followed by a GeLU activation function.\" Similarly, how the model size of the proposed and other methods is adjusted was unclear."
                },
                "questions": {
                    "value": "- Can you expand the discussion of how this novel architecture and findings would attract the general AI and ML researchers in ICLR? This paper specializes in speech recognition (and spoken language understanding, which is a very similar task to speech recognition), and it is a narrow scope for me.\n- Besides the above expansion, can you explain why you selected Fastformermer and ContextNet?\n- Can you apply this to RNN-T?\n\nOther minor comments\n- Abstract: ASR --> automatic speech recognition (ASR), speech understanding --> spoken language understanding\n- Page 3, last paragraph \"Hence, $X \\in \\mathbb{R}$ becomes $X \\in \\mathbb{R} ^{T \\times D/n}$\": Is $X \\in \\mathbb{R}$ scalar? I think you missed adding some domains."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5692/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699212530067,
            "cdate": 1699212530067,
            "tmdate": 1699636595390,
            "mdate": 1699636595390,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zEkA9xXDtz",
                "forum": "PoBB8n52oi",
                "replyto": "bAK54Iydlj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5692/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5692/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you 1/2."
                    },
                    "comment": {
                        "value": "We first would like to thank the reviewer for highlighting the good reproducibility of SummaryMixing as well as its low computational cost. We wish to mention that we can provide an anonymized version to the reviewer if necessary. In the following, we address every single comment made by the reviewer and answer the given questions. \n\n**Weaknesses**\n\n*The survey of the efficient transformer (conformed) is not sufficient. [...] Even if we limit the discussions to speech applications, there are a lot of methods*\n\nWe agree with the reviewer that comparing with more \u201clinear attention\u201d is important and we modified the related work (Section 1.1) of the manuscript accordingly to incorporate many relevant methods. We invite the reviewer to take a look at this revised section. Experiment-wise, however, It is worth mentioning that our SummaryMixing was compared with two \u201clinear-attention\u201d alternatives - Fastformer and Hypermixer. In the original paper, Fastformer has been compared to longformer, Linformer, linear transformers, BigBird and poolingformer. All these methods are state-of-the-art alternatives to self-attention. In this original published work, Fastformer was better than all of them and has been replicated in ESPNet. This is why it appeared as a strong baseline in our case. Indeed, if Fastformer beats most existing linear alternatives, our goal should be to beat Fastformer.  Then, we wish to highlight that there exists no available implementation of any linear-attention claiming SOTA performance on ASR benchmarks -- SummaryMixing is the first one. Fastformer is implemented in ESPnet, but does not reach SOTA \u2013 SummaryMixing does on most if not all ASR datasets. \n\n*The performance improvement from the conventional method (e.g., fastformer) is marginal.*\n\nThe Fastformer also can not catch the performance of MHSA, hence compute and memory costs were coming at the cost of accuracy. This is not true with SummaryMixing as it reaches or even beats MHSA. We agree with the reviewer that absolute improvements do not seem vastly game-changing. However, we wish to highlight that the BranchFormer, which simply makes the conformer parallel instead of sequential and was published at ICML, improved the performance over the Conformer by .1% on Librispeech dev-clean and .2% on test-clean. Compared to Fastformer; SummaryMixing improves the performance by .2% .1% .3% on Librispeech dev-clean, test-clean and test-others respectively. if we consider other ASR tasks, SummaryMixing always beats Fastformer, with an average absolute gain of 0.75%. The latter improvement is more significant than the one originating from Branchformer vs Conformer from the reference ICML paper. The performance might appear marginal in the reviewer\u2019s perspective, which might be true, yet it is consistent, and more importantly, totally aligned with what we see from the improvements in ASR from the literature.\n\n*The method is on top of branchformer, which already uses Fastformer, and its technical novelty is weak.*\n\nWe modified Section 2.2 to clarify the use of SummaryMixing both in Conformer and Branchformer. Also, we wish to bring our perspective to the potential lack of novelty. If we see things from the Branchformer point of view, we agree with the reviewer, in proportion, the Sumformer implies only a few changes (and this is a strength from our point of view). However, we would like to first highlight that SummaryMixing is also applied to the Conformer in this paper (Table 1 and CTC only results) and it also beats or matches MHSA. SummaryMixing is expected to replace MHSA in any speech encoder equipped with it, and this article demonstrates that it works just better with the two most used architectures: Conformer and Branchformer. Second, we wish to mention that Branchformer was accepted at ICML while its contribution was to make two blocks of layers parallel instead of sequential \u2013 from our very own point of view, this does not represent much more novelty than SummaryMixing (quantitatively speaking), yet it was accepted and is now used by the community.\n\n*Several descriptions (e.g., the surveys, distinction from HyperMixer/MLP Mixer, and detailed architectures) are unclear.*\n\nWe thank the reviewer for mentioning these issues. We revised the paper according to the three problems mentioned by the reviewer, see Section 1.1 for the survey, Section 2 and the Appendix for the Hypermixer, and Section 3.1 and the Appendix for the detailed architecture. Please let us know if the reviewer still finds the definitions confusing or lacking precision, as this only concerns text, we can easily fix it and align it with the reviewer's perspective."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5692/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700153472439,
                "cdate": 1700153472439,
                "tmdate": 1700153540496,
                "mdate": 1700153540496,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]