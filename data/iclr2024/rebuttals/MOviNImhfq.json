[
    {
        "title": "Effective Graph Representation Learning via Smoothed Contrastive Learning"
    },
    {
        "review": {
            "id": "CGGTZtMvYb",
            "forum": "MOviNImhfq",
            "replyto": "MOviNImhfq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8116/Reviewer_V43w"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8116/Reviewer_V43w"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on tackling the equal treatment issue of misclassified false negative nodes in conventional GCL approaches. Specifically, the paper presents a Smoothed Graph Contrastive Learning model which leverages the geometric structure of augmented graphs to exploit proximity information associated with positive/negative pairs in contrastive loss. This enables the significance of node pairs to be adjusted. Furthermore, a graph batch-generating strategy that partitions the given graphs into multiple subgraphs is also proposed to facilitate efficient training in separate batches. Experiments show the superiority of the proposed framework."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper is well-motivated. Equal treatment of misclassified false negative nodes and the lack of a mechanism to differentiate misclassified nodes based on proximity can be harmful to graph contrastive learning.\n2. Applying smoothing approaches to pair matrices is novel and interesting."
                },
                "weaknesses": {
                    "value": "1. The writing of the paper should be improved. There are many minor mistakes in the paper:\n  - \"These methods, including , including Deep Graph Infomax\" in paragraph 2 of Section 2.\n  - \"Therfore\" in paragraph 2 of Section 3.1.\n  - In caption of Figure 2, g(j) is not a positive pair.\n  - \"distinguishe\" in paragraph 1 of Section 4.2.4.\n2. A persuasive demonstration of why misaligning negative pairs is harmful should be provided.\n3. The smoothing method only employs the original graph information. However, the node relationship can be highly changed after augmentation. For example, two highly related nodes can be dissimilar when one of them is dropped. In such cases, is the proposal still efficient?\n4. The proposal seems incremental - the smoothing technique, loss function and subgraph generating can be easily detached from the framework.\n5. Can previous contrastive objectives be used in the proposal? There is a lack of ablation studies to exclude the effect of the proposed contrastive objective.\n6. A time analysis should be provided to verify the efficiency of the proposal."
                },
                "questions": {
                    "value": "See Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8116/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697567039090,
            "cdate": 1697567039090,
            "tmdate": 1699637005627,
            "mdate": 1699637005627,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0L4D2EjoGP",
                "forum": "MOviNImhfq",
                "replyto": "CGGTZtMvYb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8116/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8116/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part 1"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for his/her time and effort spent on evaluating our work and their constructive comments. We find the suggestions to be very helpful in improving the quality of our work, making it clearer and more convincing. We are pleased to hear that the reviewer found our idea novel and interesting and is convinced that the paper is well-motivated.\n\n>The writing of the paper should be improved\n\nWe appreciate your insights and acknowledge the need for improvement in the writing of the paper. We will carefully review and address the minor mistakes to enhance the overall quality of the manuscript.\n\n>A persuasive demonstration of why misaligning negative pairs is harmful should be provided\n\nWe present the following paragraphs in Section 2.1 to demonstrate the detrimental effects of misaligning negative pairs.\n\nIn contrastive learning, the misalignment of negative pairs adversely affects the learning process due to its inadvertent impact on the objective function. Consider the following contrastive loss function, designed for each anchor node  $v_t^{(i)}$ with feature embedding $\\mathbf{h}_t^i$. \n\nThe objective is to minimize the distance between embeddings of positive pair $\\( v_t^{(i)},v_t^{(i)} \\)$ and simultaneously maximize the distance between embeddings of negative pairs $\\( v_t^{(i)},v_q^{(j)}\\)_{q=1,q\\neq t}^{N-1}$ (Please see Eq. 1 in the paper).\n\nMisalignment in negative pairs $\\(v_t^{(i)},v_k^{(j)}\\)$ detrimentally impacts the learning process by introducing errors in the loss computation. The misalignment leads to an undesired increase in the loss, hindering the optimization process. Specifically, the GCL model increases the distance between misaligned negative pairs, and inadvertently separates semantically similar samples, leading to a degradation of overall performance."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8116/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700604078959,
                "cdate": 1700604078959,
                "tmdate": 1700604078959,
                "mdate": 1700604078959,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "14QJ7DrO1e",
                "forum": "MOviNImhfq",
                "replyto": "CGGTZtMvYb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8116/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8116/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part 2"
                    },
                    "comment": {
                        "value": ">The smoothing method only employs the original graph information. However, the node relationship can be highly changed after augmentation. For example, two highly related nodes can be dissimilar when one of them is dropped. In such cases, is the proposal still efficient?\n\nAccording to the proposed framework, the smoothing method incorporates graph information from each augmented view. Following the generation of two augmented views, we apply the smoothing method to each view independently, aiming to capture every change in the original graph. We sincerely apologize for any confusion that may have arisen from the initial presentation of the framework. In the new version, we have made efforts to provide a clearer explanation.\n\n>The proposal seems incremental....\n\nRespectfully, we adhere to the common framework established for GCL models. Inspired by the DGI framework that proposed an objective based on MI maximization in the graph domain, the majority of GCL models adopt this consistent framework, which encompasses graph augmentation, decoder computation, contrastive objective computation in the pre-training step, and subsequent evaluation in downstream tasks. However, what distinguishes our proposed framework from other GCL models is the intuitive incorporation of proximity information between nodes in positive and negative pairs. \n\nWe would like to emphasize that in the conventional GCL framework, there is a lack of information about the proximity of positive/negative pairs, and all N\u22121 negative pairs are uniformly treated. In other words, the conventional GCL approach treats all misclassified nodes equally, regardless of whether the misclassification occurs near the true positive or at a significant distance from it (since a 1-hop error is just as \u201cexpensive\u201d as a k>>1 hop error).\nA common approach for considering proximity involves computing pairwise distances among all nodes in the graph. However, our approach is more efficient, avoiding the need to compute or store large dense matrices. This efficiency is achieved through the integration of three developed smoothing approaches embedded in our loss function.\n\n>Can previous contrastive objectives be used in the proposal? \n\nWe can further generalize the smoothed positive/negative pairs to some of the other contrastive objectives with similar settings, which apply positive/negative pairs at the node level. Additionally, the augmented generation strategy should maintain the nodes from the original graph to serve as corresponding positive pairs. For example, we applied the smoothed positive/negative matrices to the contrastive objective used in the GRACE method. However, considering that GRACE utilizes two types of negative pairs (inter-view and intra-view), we constrained it to only inter-view negative pairs. While results on some benchmarks showed improvements over the original GRACE, our approach consistently outperformed it.\n\n> There is a lack of ablation studies to exclude the effect of the proposed contrastive objective.\n\nTo perform an ablation study on the contrastive objective, we evaluate the significance of each individual term and subsequently combine them with hyperparameter $\\lambda$.\nThe following table provides the accuracies of different variants of SGCL achieved by different components of the contrastive objective on three benchmarks of varying scales: small (Cora), medium (CoauthorCS), and large (ogbn-arxiv). Initially, we observe that the exclusion of any term from our loss function results in deteriorated or collapsed solutions, aligning with our expectations. Subsequently, we investigated the influence of the combination of two individual terms using an optimal value of $\\lambda$.\n\n|Model | Benchmark  |  First term | Second-term | Loss (Eq. 5) | ($\\lambda)$ |\n|----------|----------|----------|----------|----------|----------|\n| | small (Cora) | 84.13\u00b13.1 | 83.54\u00b11.0 | 86.54\u00b11.4 | (4e-4) |\n|SGCL-T | medium (CoauthorCS) | 91.36\u00b10.7 | 90.87\u00b10.4 | 92.99\u00b10.4 | (1e-4) |\n| | large (ogbn-arxiv) | 68.92\u00b10.0 | 67.05\u00b10.0 | 69.30\u00b10.5 | (1e-4) |\n| | small (Cora) | 85.26\u00b12.9 | 85.54\u00b11.3 | 87.50\u00b11.7 | (4e-4) |\n|SGCL-B | medium (CoauthorCS) | 93.04\u00b10.1 | 91.45\u00b10.3 | 93.15\u00b10.4 | (1e-4) |\n| | large (ogbn-arxiv) | 68.73\u00b10.3 | 68.29\u00b10.4 | 69.24\u00b10.3 | (1e-4) |\n| | small (Cora) | 84.91\u00b11.8 | 82.81\u00b12.1 | 85.22\u00b10.8 | (4e-4) |\n|SGCL-D | medium (CoauthorCS) | 92.4\u00b10.52 | 91.09\u00b10.2 | 93.2\u00b10.4 | (1e-4) |\n| | large (ogbn-arxiv) | 68.40\u00b10.3 | 68.29\u00b10.3 | 69.03\u00b10.4 | (1e-4) |\n\nTo select the value of $\\lambda$, we initially set it as $\\lambda =1/2N$\u200b. However, in the experiments, we determined its optimal value through grid search. For instance, on the Photo dataset, the optimal value for $\\lambda$ was found to be around $2.3e\u22124$. This value aligns with our first initialization when considering the batch size of $ N=2000$ in the experiments."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8116/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700604619661,
                "cdate": 1700604619661,
                "tmdate": 1700604619661,
                "mdate": 1700604619661,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sYiTyBAycS",
                "forum": "MOviNImhfq",
                "replyto": "CGGTZtMvYb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8116/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8116/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part 3"
                    },
                    "comment": {
                        "value": ">A time analysis should be provided to verify the efficiency of the proposal.\n\nThe computational cost of graph contrastive learning models is analyzed through two distinct components: pre-training and downstream task evaluation. In the pre-training phase, the process involves augmentation generation, encoder computation, and computation of the contrastive objective for each batch. In the downstream task phase, the model learns two input/output MLP layers and evaluates the model for node classification. \n\nWe conducted the computation analysis to evaluate the runtime performance of three variants of the SGCL model and compared these variants with several baseline methods on graphs of different scales, ranging from small to medium and large-scale graphs. The results of these experiments are summarized in the following table.\n\nThe table results indicate that during pre-training, SGCL-T on the Cora dataset outperforms MVGRL in running time. However, in other experiments, the computational cost of the proposed model is slightly increased compared to the baselines, primarily attributed to the computation associated with the smoothing approach. Specifically, its computational load is approximately twice that of MVGRL.\nIt's noteworthy to highlight that the computational costs in the downstream evaluation phase across all models are nearly identical on each benchmark. This implies that, despite the more computations during the pre-training phase, our model demonstrates efficiency during the downstream evaluation phase.\n\n|Model | Phase | Small (Cora) | Medium (CoauthorCS) | Large (ogbn-arxiv)|\n|----------|----------|----------|----------|----------|\n|DGI | pre-training | 0.0391 | 0.0916 | 0.0732 |\n|| downstream | 0.0024 | 0.0148 | 0.0837 |\n|GRACE | pre-training | 0.0713 | 0.3186 | 0.4233 |\n| | downstream | 0.0024 | 0.0148 | 0.0845  |\n|MVGRL | pre-training | 0.2266 | 0.7824 | 0.9407 |\n| | downstream | 0.0024 | 0.0148 | 0.0833  |\n|BGRL | pre-training | 0.0927 | 0.1849 | 0.1755 |\n| | downstream | 0.0024 | 0.0149 | 0.0846  \n|GBT | pre-training | 0.0343 | 0.1387 | 0.5388 |\n| | downstream | 0.0024 | 0.0148 | 0.0844 |\n|SGCL-T | pre-training | 0.1916 | 1.5012 | 2.1334 |\n || downstream | 0.0025 | 0.0149 | 0.0841  |\n|SGCL-B | pre-training | 1.3484 | 3.3491 | 3.9549| \n| | downstream | 0.0025 | 0.0151 | 0.0848  |\n|SGCL-D | pre-training | 1.3771 | 3.4538 | 4.0496 |\n| | downstream | 0.0024 | 0.0151 | 0.0841  |"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8116/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700604707476,
                "cdate": 1700604707476,
                "tmdate": 1700604707476,
                "mdate": 1700604707476,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BSCmhwQydv",
            "forum": "MOviNImhfq",
            "replyto": "MOviNImhfq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8116/Reviewer_jBXi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8116/Reviewer_jBXi"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a method called Smoothed Graph Contrastive Learning (SGCL) that tries to use the graph structure to spread out the weights for positive and negative pairs. The matrix $\\tilde\\Pi_{pos}^{(i,j)}$ is the smoothed out matrix of positive weights (smoothing is done for example using the graph Laplacian matrix). Positive pairs between graph views $\\mathcal{G}^{(i)}, \\mathcal{G}^{(j)}$ are encouraged to have embeddings with a cosine similarity close to 1. Negative pairs are encouraged to have orthogonal embeddings.\n\nThe main contribution of the paper is the idea to smooth positive/negative weights based on graph structure. Experimental results are compellingly in favor of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The main idea to smooth out weights is simple. \n* The proposed method performs really well in experiments."
                },
                "weaknesses": {
                    "value": "None that I can come up with."
                },
                "questions": {
                    "value": "What are the final embedding dimensions in the experiments?\n\nTypo:\n* page 3, Section 3.2, second paragraph: ${0,1}$ should be $\\{0,1\\}$ \n* Section 4.2.3: Is $\\tilde D_{ii}$ the degree + 1?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8116/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698636079588,
            "cdate": 1698636079588,
            "tmdate": 1699637005517,
            "mdate": 1699637005517,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5oKIj9TvyQ",
                "forum": "MOviNImhfq",
                "replyto": "BSCmhwQydv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8116/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8116/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We express our gratitude to the reviewer for dedicating time and effort to evaluate our work and providing constructive comments. \n\n>The main idea to smooth out weights is simple.\n\nWe would like to strongly emphasize that adapting smoothing correspondences to graph contrastive loss is highly non-trivial. Note that in GCL models, in the absence of labeled information, numerous incongruent nodes for each anchor node are potentially treated as false negatives. Specifically, nodes that are close to the anchor and thus have the potential to be semantically similar are inevitably categorized as negative pairs. However, in the conventional GCL methods, which lack information about the proximity of these nodes, all negative pairs are handled uniformly. In other words, the conventional contrastive learning approach treats all misclassified nodes equally, regardless of whether the misclassification occurs near the true positive or at a significant distance from it. \nAs outlined in the paper, our approach addresses this limitation by promoting local consistency. A common approach for considering proximity involves computing pairwise distances among all nodes in the graph. However, our approach is more efficient, avoiding the need to compute or store large dense matrices. We effectively integrated three developed smoothing approaches embedded in our loss function. Subsequently, the loss function intuitively incorporates proximity information between nodes in positive and negative pairs.\n\n>What are the final embedding dimensions in the experiments?\n\nIn our experiments, we utilize a 512-hidden channel as the dimension of feature embedding learned using the encoder. Consequently, the final embedding dimension in the pre-training step is 512. However, during the evaluation step in downstream tasks, the final embedding dimension aligns with the number of classes in each benchmark. For instance, in the CoauthorCS dataset, it corresponds to 15.\n\n>Typo\n\nWe appreciate your insights and recognize the need for corrections, including typos in the writing of the paper. We will carefully review and address the minor mistakes to enhance the overall quality of the paper. \n$D_{ij}$ is the degree matrix computed using the augmented adjacency matrix $\\hat{A}=A+I$, which $\\hat{A} $ represents the adjacency matrix with a self-loop for each node."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8116/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700597047316,
                "cdate": 1700597047316,
                "tmdate": 1700597047316,
                "mdate": 1700597047316,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QyDY5UdVVa",
            "forum": "MOviNImhfq",
            "replyto": "MOviNImhfq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8116/Reviewer_EGjE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8116/Reviewer_EGjE"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces Smoothed Graph Contrastive Learning to address the issues of false positives and false negatives in graph contrastive learning. The primary idea is to leverage the structural information of the graph to obtain pairwise proximity information and assign weights to each pair. Experimental results demonstrate the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The approach presented in this paper, using graph structural information to smooth the contrastive learning loss, is both intriguing and innovative.\n- The proposed method has a solid theoretical foundation.\n- The paper provides ample background knowledge to assist readers who may not be familiar with graph smoothing."
                },
                "weaknesses": {
                    "value": "- The structure and organization of this paper appear quite impractical. The author dedicates a substantial portion of Section 3 to background knowledge, occupying a significant amount of space, and only begins to introduce the proposed method towards the end of Page 5. This has resulted in an insufficiently detailed experimental section. It is advisable for the author to trim down the content in Section 3 and allocate more space to the experimental aspects of the paper.\n- The author fails to provide the rationale and intuition behind using the loss function as depicted in Eq.4. This loss function does not appear to be particularly innovative. Additionally, I believe that the choice of lambda is crucial, but the author does not explain how lambda is selected.\n- The experiments in this paper seem overly simplified, and the dataset splits chosen do not align with commonly used splits in self-supervised learning (public split). The selection of baselines appears outdated, and the reported results in the paper do not align with the results reported for these baseline methods in their original sources."
                },
                "questions": {
                    "value": "I have some questions about the definition of Equation 4.\n\n- In Equation 4, the author claims that $C$ is the cross-correlation matrix of the embedding matrix. However, according to the definition of cross-correlation, $C$ should be an $F\\times F$ matrix rather than an $N\\times N$ matrix, which contradicts Equation 5. I would recommend the author to double-check this issue.\n- In Equation 4, when $i\u2260j$ and $\\hat{\\pi}(i, j) = 1$, Eq.4 assigns a high weight to minimize $c_{ij}$. This seems counterintuitive because $\\hat{\\pi}(i, j) = 1$ should imply that nodes i and j are very likely to be false negatives, so $c_{ij}$ should be maximized rather than minimized."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8116/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8116/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8116/Reviewer_EGjE"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8116/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698728074133,
            "cdate": 1698728074133,
            "tmdate": 1699637005384,
            "mdate": 1699637005384,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EUMLl8tVSM",
                "forum": "MOviNImhfq",
                "replyto": "QyDY5UdVVa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8116/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8116/Authors"
                ],
                "content": {
                    "title": {
                        "value": "The rationale and intuition behind using the loss function"
                    },
                    "comment": {
                        "value": "We express our gratitude to the reviewer for dedicating time and effort to evaluate our work and providing constructive comments. The suggestions are invaluable for enhancing the quality of our work, improving clarity, and making it more convincing. We are pleased to learn that the reviewer finds the theoretical foundation of our model to be novel and is convinced that the proposed method is intriguing and innovative.\n\n>The structure and organization of this paper appear quite impractical\n\nYour insights are appreciated. We moved section 2 (related work) to the appendix and revised the paper accordingly to ensure a better balance between sections which allocates more space to the experimental aspects as suggested.\n\n>The author fails to provide the rationale and intuition behind using the loss function\n\nand \n\n>Question1: .... I would recommend the author to double-check this issue.\n\nAs per your insightful recommendation, matrix $C \\in \\{0,1\\}^{(N\\times N)}$ accurately represents the similarity between node pairs in two augmented graphs, despite our inadvertent reference to it as a cross-correlation matrix.\nWe sincerely apologize for any confusion resulting from the unclear presentation and typographical error in Eq. 4. In the first term of this equation, please note that $\\textbf{I}$ should be substituted with the scalar \"1,\" and the operator between matrices should be interpreted as the **element-wise** operator.\n\nKindly refer to the accurate formulation (Eq. 5 in the new paper) provided below:\n\n$L_{SGCL}^{(i,j)} ={ \\parallel {\\tilde{\\Pi}_{p}^{(i,j)}\\odot (1-\\mathbf{C}^{(i,j)})} \\parallel_F^2 +\\lambda \\parallel (1-{\\tilde{\\Pi}_p^{(i,j)}})\\odot \\mathbf{C}^{(i,j)} \\parallel_F^2 }$\n\nIt appears to us that the rationale behind utilizing the loss function has become more insightful now.\n\nThe intuition is straightforward. Generally, matrix $C^{(i,j)}$ comprises similarity values between pairs of nodes from two augmented graphs. Our expectation is to maximize $C^{(i,j)}$ for positive pairs and minimize $C^{(i,j)}$\u200b for negative pairs, equivalent to simultaneously minimizing $1\u2212C^{(i,j)}\u200b$ for positive pairs and $C^{(i,j)}$\u200b for negative pairs.\n\nThe first term minimizes the discrepancy between '1' and each element of $C^{(i,j)}$, aligning with the values in the smoothed positive pairs matrix $\\hat{\\Pi}_{p}^{(i,j)}$. This term enforces stability and preservation in the embeddings of positive pairs. \n\nLikewise, the second term minimizes each element of $C^{(i,j)}$ in accordance with the values in the smoothed negative pairs matrix  $\\hat{\\Pi}_{n}^{(i,j)}=1-\\hat{\\Pi}_p^{(i,j)}$. This term actively promotes a substantial diversity in the embeddings of negative pairs.\n\n\n>Questions 2:\n\nAccording to our definition, $\\hat{\\pi}_{p} (i,j) = 1$ for each $i=j$. \n\nTo clarify, if we hypothetically consider that two nodes $i$ and $j$ ($i \\neq j$) are very likely to be false negatives, such that $\\hat{\\pi}_{p}(i,j) = 1$, then the first term assigns a weight of '1' to minimize $1 - C^{(i,j)}$ which is equivalent to maximizing $ C^{(i,j)}$, as you rightly mentioned. However, the second term is redundant since it assigns a weight of '0' to minimize $ C^{(i,j)}$."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8116/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700603275386,
                "cdate": 1700603275386,
                "tmdate": 1700603275386,
                "mdate": 1700603275386,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "D4lMnseuwo",
                "forum": "MOviNImhfq",
                "replyto": "QyDY5UdVVa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8116/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8116/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">I believe that the choice of lambda is crucial, but the author does not explain how lambda is selected.\n\nTo select the value of $\\lambda$, we initially set it as $\\lambda =1/2N$\u200b. However, in the experiments, we determined its optimal value through grid search. For instance, on the Photo dataset, the optimal value for $\\lambda$ was found to be around $2.3e\u22124$. This value aligns with our first initialization when considering the batch size of $N=2000$ in the experiments.\n\nAdditionally, we performed an ablation study on the contrastive objective. We evaluated the significance of each individual term and subsequently combined them with hyperparameter $\\lambda$.\nThe following table provides the accuracies of different variants of SGCL achieved by different components of the contrastive objective on three benchmarks of varying scales: small (Cora), medium (CoauthorCS), and large (ogbn-arxiv). Initially, we observe that the exclusion of any term from our loss function results in deteriorated or collapsed solutions, aligning with our expectations. Subsequently, we investigated the influence of the combination of two individual terms using an optimal value of $\\lambda$.\n\n|Model | Benchmark  |  First term | Second-term | Loss (Eq. 5) | ($\\lambda)$ |\n|----------|----------|----------|----------|----------|----------|\n| | small (Cora) | 84.13\u00b13.1 | 83.54\u00b11.0 | 86.54\u00b11.4 | (4e-4) |\n|SGCL-T | medium (CoauthorCS) | 91.36\u00b10.7 | 90.87\u00b10.4 | 92.99\u00b10.4 | (1e-4) |\n| | large (ogbn-arxiv) | 68.92\u00b10.0 | 67.05\u00b10.0 | 69.30\u00b10.5 | (1e-4) |\n| | small (Cora) | 85.26\u00b12.9 | 85.54\u00b11.3 | 87.50\u00b11.7 | (4e-4) |\n|SGCL-B | medium (CoauthorCS) | 93.04\u00b10.1 | 91.45\u00b10.3 | 93.15\u00b10.4 | (1e-4) |\n| | large (ogbn-arxiv) | 68.73\u00b10.3 | 68.29\u00b10.4 | 69.24\u00b10.3 | (1e-4) |\n| | small (Cora) | 84.91\u00b11.8 | 82.81\u00b12.1 | 85.22\u00b10.8 | (4e-4) |\n|SGCL-D | medium (CoauthorCS) | 92.4\u00b10.52 | 91.09\u00b10.2 | 93.2\u00b10.4 | (1e-4) |\n| | large (ogbn-arxiv) | 68.40\u00b10.3 | 68.29\u00b10.3 | 69.03\u00b10.4 | (1e-4) |"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8116/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700603310677,
                "cdate": 1700603310677,
                "tmdate": 1700603310677,
                "mdate": 1700603310677,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IoqEWQIRE2",
                "forum": "MOviNImhfq",
                "replyto": "QyDY5UdVVa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8116/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8116/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Experiments"
                    },
                    "comment": {
                        "value": ">The experiments in this paper seem overly simplified...\n\nIn our experiments, we employed the PyGCL library to explore a mini-batch scenario, aligning with the setting of our proposed model. Considering the diverse settings across various models developed for contrastive learning and the absence of comprehensive experiments on several Ogbn benchmarks in the published paper, to ensure fair comparisons, we reproduced the results of the state-of-the-art models available within this library, as detailed in Table 1 of the paper.  We will publish the code of our model as well as the SOTA models for easy reproducibility of the results once the paper is accepted.\n\nFor further investigation, we conducted additional experiments involving a range of state-of-the-art methods, notably CGRA [1] and GRLC [2]. To establish a fair and robust comparison with the existing SOTA methods, these experiments are conducted in a full-batch scenario, adhering to the **commonly employed data split in self-supervised learning** as provided in the OGB benchmarks. The results are summarized in the following table.\n\n[1 H. Duan et al., \"Self-supervised contrastive graph representation with node and graph augmentation,\" Neural Networks, (67), 2023\n\n[2] L. Peng et al., \"GRLC: Graph Representation Learning With Constraints,\" in IEEE Transactions on Neural Networks and Learning Systems, 2023\n\n|Model | Cora | Citeseer | Pubmed | CoauthorCS | Computers | Photo |\n|---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- |\n|DGI   | 76.28\u00b10.04 | 69.33\u00b10.14 | 83.79\u00b10.08 | 91.63\u00b10.08 | 71.96\u00b10.06 | 75.27\u00b10.02 |\n|GRACE  | 81.80\u00b10.19 | 71.35\u00b10.07 | 85.86\u00b10.05 | 91.57\u00b10.14 | 84.77\u00b10.06 | 89.50\u00b10.06|\n|MVGRL  | 84.98\u00b10.11 | 71.29\u00b10.04 | 85.22\u00b10.04 | 91.65\u00b10.02 | 88.55\u00b10.02 | 91.90\u00b10.08 |\n|BGRL  | 80.21\u00b11.14 | 66.33\u00b12.10 | 81.78\u00b11.06 | 90.19\u00b10.82 | 84.24\u00b11.32 | 89.56\u00b11.01 |\n|GBT | 79.32\u00b10.31 | 65.78\u00b11.33 |**86.35\u00b10.48** | 91.87\u00b10.07 | **90.43\u00b10.18** | 92.23\u00b10.18 |\n|CGRA  |82.71\u00b10.01  |69.23\u00b11.19  |82.15\u00b10.46  |91.26\u00b10.27  |89.76\u00b10.36  |91.54\u00b11.06|\n|GRLC  |83.50\u00b10.24 | 70.02\u00b10.16 | 81.20\u00b10.20 | 90.36\u00b10.27 | 88.54\u00b10.23 | 91.80\u00b10.77|\n|**SGCL-T** | 84.45\u00b10.04| 71.26\u00b10.06 |84.11\u00b10.08 |92.14\u00b10.09 | 86.81\u00b10.01 |**92.71\u00b10.05** |\n|**SGCL-B** | **85.08\u00b10.12**| **72.77\u00b10.33** |83.67\u00b10.06 |    **92.16\u00b10.15** | 88.24\u00b10.05| 92.43\u00b10.03 |\n|**SGCL-D** | 84.47\u00b10.25| 70.32\u00b10.04 |85.22\u00b10.02 |    92.04\u00b10.05 |84.98\u00b10.34 |90.09\u00b10.11 |\n\nAdditionally, we have extended the evaluation of the model by conducting new experiments for the graph classification task, using five commonly used graph classification benchmarks: MUTAG, PTC, IMDB-BINARY, PROTEINS, and ENZYMES. In this experiment, we followed the InfoGraph setting for graph classification and compared the accuracy with the self-supervised state-of-the-art methods, including InfoGraph, GraphCL, MVGRL, BGRL, AD-GCL, LaGraph, and CGRA (Please see section 4.3).\n\nThe results reported in the following table (Table 3 in the paper) indicate that, in comparison to the best-performing state-of-the-art methods, the proposed approach demonstrates enhanced accuracy for IMDB-BINARY, PROTEINS, and ENZYMES, while maintaining comparable accuracy on other benchmarks. \n\nIt's worth mentioning that the accuracies of all models are reported from their respective published papers, except for the BGRL results, which we reproduced under the same experimental setting. \n\nModel | IMDB-Binary | PTC | MUTAG | PROTEINS | ENZYMES \n|----------|----------|----------|----------|----------|----------|\n|InfoGraph | 73.0\u00b10.9 | 61.7\u00b11.4 | 89.0\u00b11.1 | 74.4\u00b10.3 | 50.2\u00b11.4 |\n|GraphCL | 71.1\u00b10.4 | 63.6\u00b11.8 | 86.8\u00b11.3 | 74.4\u00b10.5 | 55.1\u00b11.6 |\n|MVGRL  | 74.2\u00b10.7 | 62.5\u00b11.7 | 89.7\u00b11.1 | 71.5\u00b10.3 | 48.3\u00b11.2 |\n|AD-GCL  | 71.5\u00b11.0 | 61.2\u00b11.4 | 86.8\u00b11.3 | 75.0\u00b10.5 | 42.6\u00b11.1 |\n|BGRL  | 72.8\u00b10.5 | 57.4\u00b10.9 | 86.0\u00b11.8 | 77.4\u00b12.4 | 50.7\u00b19.0 |\n|LaGraph  | 73.7\u00b10.9 | 60.8\u00b11.1 | 90.2\u00b11.1 | 75.2\u00b10.4 | 40.9\u00b11.7|\n|CGRA  | 75.6\u00b10.5 | **65.7\u00b11.8** | **91.1\u00b12.5** | 76.2\u00b10.6 | 61.1\u00b10.9 |\n|**SGCL-T** | 75.2\u00b12.8 | 64.0\u00b11.6 | 89.0\u00b12.3 | 79.4\u00b11.9 | **65.3\u00b13.6**|\n|**SGCL-B** | 73.2\u00b13.7 | 62.5\u00b11.8 | 87.0\u00b12.8 | **81.6\u00b12.3** | 63.7\u00b11.6 |\n|**SGCL-D** | **75.8\u00b11.9** | 62.6\u00b11.4 | 86.0\u00b12.6 | 81.5\u00b12.3 | 64.3\u00b12.2 |"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8116/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700603629589,
                "cdate": 1700603629589,
                "tmdate": 1700603629589,
                "mdate": 1700603629589,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "s45X8MHBlz",
            "forum": "MOviNImhfq",
            "replyto": "MOviNImhfq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8116/Reviewer_4ysC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8116/Reviewer_4ysC"
            ],
            "content": {
                "summary": {
                    "value": "The paper tackles the challenge in Graph contrastive learning (GCL), particularly the problem of uniformly incorporating negative samples in the contrastive loss, which may not account for the proximity of the true positive nodes. The authors introduced a new method Smoothed Graph Contrastive Learning model (SGCL), aiming to consider the geometric structure of augmented graphs and exploit proximity information for better representation learning."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The presentation is clear.\n2. The studied problem is interesting."
                },
                "weaknesses": {
                    "value": "1. The newest baseline is published in 2022. Therefore the paper misses a lot SOTA methods.\n2. The paper doesn't involve computational cost analysis. Moreover, the cost should be compared with baselines.\n3. Smoothing for graph contrastive learning seems to be a little trivial to me. \n4. Can the proposed methods be adopted for graph-level tasks [1,2]? \n\n[1] Supervised Contrastive Learning with Structure Inference for Graph Classification, TNNLS\n\n[2] Self-supervised Graph-level Representation Learning with Adversarial Contrastive Learning, TKDD"
                },
                "questions": {
                    "value": "See above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8116/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8116/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8116/Reviewer_4ysC"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8116/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698818667287,
            "cdate": 1698818667287,
            "tmdate": 1699637005237,
            "mdate": 1699637005237,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hS1fdhBkV7",
                "forum": "MOviNImhfq",
                "replyto": "s45X8MHBlz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8116/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8116/Authors"
                ],
                "content": {
                    "title": {
                        "value": "New experiment with newest baselines"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for his/her time and effort spent on evaluating our work and their constructive comments. We find the suggestions to be very helpful for improving the quality of our work, making it more convincing. We are pleased to learn that the reviewer is interested in the problem and has found the presentation of our method to be clear.\n\n>The newest baseline is published in 2022. Therefore the paper misses a lot SOTA methods.\n\nIn our experiments, we employed the PyGCL library to explore a mini-batch scenario, aligning with the setting of our proposed model (https://github.com/PyGCL/PyGCL). Considering the diverse settings across various models developed for contrastive learning and the absence of comprehensive experiments on several Open Graph Benchmarks in the published paper, to ensure fair comparisons, we reproduced the results of the state-of-the-art models available within this library, as detailed in Table 1 of the paper. We will publish the code of our model as well as the SOTA models for easy reproducibility of the results once the paper is accepted.\nFor further investigation, we conducted additional experiments involving a range of state-of-the-art methods, notably CGRA [1] and GRLC [2]. To establish a fair and robust comparison with the existing SOTA methods, these experiments are conducted in a full-batch scenario, adhering to the **commonly employed data split in self-supervised learning as provided in the OGB benchmarks**. The results are summarized in the following table.\n\n[1] H. Duan et al., \"Self-supervised contrastive graph representation with node and graph augmentation,\" Neural Networks, (67), 2023\n\n[2] L. Peng et al., \"GRLC: Graph Representation Learning With Constraints,\" in IEEE Transactions on Neural Networks and Learning Systems, 2023\n\n|Model | Cora | Citeseer | Pubmed | CoauthorCS | Computers | Photo |\n|---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- |\n|DGI   | 76.28\u00b10.04 | 69.33\u00b10.14 | 83.79\u00b10.08 | 91.63\u00b10.08 | 71.96\u00b10.06 | 75.27\u00b10.02 |\n|GRACE  | 81.80\u00b10.19 | 71.35\u00b10.07 | 85.86\u00b10.05 | 91.57\u00b10.14 | 84.77\u00b10.06 | 89.50\u00b10.06|\n|MVGRL  | 84.98\u00b10.11 | 71.29\u00b10.04 | 85.22\u00b10.04 | 91.65\u00b10.02 | 88.55\u00b10.02 | 91.90\u00b10.08 |\n|BGRL  | 80.21\u00b11.14 | 66.33\u00b12.10 | 81.78\u00b11.06 | 90.19\u00b10.82 | 84.24\u00b11.32 | 89.56\u00b11.01 |\n|GBT | 79.32\u00b10.31 | 65.78\u00b11.33 |**86.35\u00b10.48** | 91.87\u00b10.07 | **90.43\u00b10.18** | 92.23\u00b10.18 |\n|CGRA  |82.71\u00b10.01  |69.23\u00b11.19  |82.15\u00b10.46  |91.26\u00b10.27  |89.76\u00b10.36  |91.54\u00b11.06|\n|GRLC  |83.50\u00b10.24 | 70.02\u00b10.16 | 81.20\u00b10.20 | 90.36\u00b10.27 | 88.54\u00b10.23 | 91.80\u00b10.77|\n|**SGCL-T** | 84.45\u00b10.04| 71.26\u00b10.06 |84.11\u00b10.08 |92.14\u00b10.09 | 86.81\u00b10.01 |**92.71\u00b10.05** |\n|**SGCL-B** | **85.08\u00b10.12**| **72.77\u00b10.33** |83.67\u00b10.06 |    **92.16\u00b10.15** | 88.24\u00b10.05| 92.43\u00b10.03 |\n|**SGCL-D** | 84.47\u00b10.25| 70.32\u00b10.04 |85.22\u00b10.02 |    92.04\u00b10.05 |84.98\u00b10.34 |90.09\u00b10.11 |"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8116/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700598912416,
                "cdate": 1700598912416,
                "tmdate": 1700598912416,
                "mdate": 1700598912416,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2cl7eSonwf",
                "forum": "MOviNImhfq",
                "replyto": "s45X8MHBlz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8116/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8116/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Computational cost analysis"
                    },
                    "comment": {
                        "value": "The computational cost of graph contrastive learning models is analyzed through two distinct components: pre-training and downstream task evaluation. In the pre-training phase, the process involves augmentation generation, encoder computation, and computation of the contrastive objective for each batch. In the downstream task phase, the model learns two input/output MLP layers and evaluates the model for node classification. \n\nWe conducted the computation analysis to evaluate the runtime performance of three variants of the SGCL model and compared these variants with several baseline methods on graphs of different scales, ranging from small to medium and large-scale graphs. The results of these experiments are summarized in the following table.\n\nThe table results indicate that during pre-training, SGCL-T on the Cora dataset outperforms MVGRL in running time. However, in other experiments, the computational cost of the proposed model is slightly increased compared to the baselines, primarily attributed to the computation associated with the smoothing approach. Specifically, its computational load is approximately twice that of MVGRL.\nIt's noteworthy to highlight that the computational costs in the downstream evaluation phase across all models are nearly identical on each benchmark. This implies that, despite the more computations during the pre-training phase, our model demonstrates efficiency during the downstream evaluation phase.\n\n|Model | Phase | Small (Cora) | Medium (CoauthorCS) | Large (ogbn-arxiv)|\n|----------|----------|----------|----------|----------|\n|DGI | pre-training | 0.0391 | 0.0916 | 0.0732 |\n|| downstream | 0.0024 | 0.0148 | 0.0837 |\n|GRACE | pre-training | 0.0713 | 0.3186 | 0.4233 |\n| | downstream | 0.0024 | 0.0148 | 0.0845  |\n|MVGRL | pre-training | 0.2266 | 0.7824 | 0.9407 |\n| | downstream | 0.0024 | 0.0148 | 0.0833  |\n|BGRL | pre-training | 0.0927 | 0.1849 | 0.1755 |\n| | downstream | 0.0024 | 0.0149 | 0.0846  \n|GBT | pre-training | 0.0343 | 0.1387 | 0.5388 |\n| | downstream | 0.0024 | 0.0148 | 0.0844 |\n|SGCL-T | pre-training | 0.1916 | 1.5012 | 2.1334 |\n || downstream | 0.0025 | 0.0149 | 0.0841  |\n|SGCL-B | pre-training | 1.3484 | 3.3491 | 3.9549| \n| | downstream | 0.0025 | 0.0151 | 0.0848  |\n|SGCL-D | pre-training | 1.3771 | 3.4538 | 4.0496 |\n| | downstream | 0.0024 | 0.0151 | 0.0841  |"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8116/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700599441719,
                "cdate": 1700599441719,
                "tmdate": 1700599441719,
                "mdate": 1700599441719,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pCuEA40bAg",
                "forum": "MOviNImhfq",
                "replyto": "s45X8MHBlz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8116/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8116/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Contrastive learning model"
                    },
                    "comment": {
                        "value": ">Smoothing for graph contrastive learning seems to be a little trivial to me.\n\nWe would like to strongly emphasize that adapting smoothing correspondences to graph contrastive loss is highly non-trivial. Note that in GCL models, in the absence of labeled information, numerous incongruent nodes for each anchor node are potentially treated as false negatives. Specifically, nodes that are close to the anchor and thus have the potential to be semantically similar are inevitably categorized as negative pairs. However, in the conventional GCL methods, which lack information about the proximity of these nodes, all negative pairs are handled uniformly. In other words, the conventional contrastive learning approach treats all misclassified nodes equally, regardless of whether the misclassification occurs near the true positive or at a significant distance from it. \n\nAs outlined in the paper, our approach addresses this limitation by promoting local consistency. A common approach for considering proximity involves computing pairwise distances among all nodes in the graph. However, our approach is more efficient, avoiding the need to compute or store large dense matrices. We effectively integrated three developed smoothing approaches embedded in our loss function. Subsequently, the loss function intuitively incorporates proximity information between nodes in positive and negative pairs."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8116/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700599549393,
                "cdate": 1700599549393,
                "tmdate": 1700599549393,
                "mdate": 1700599549393,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "l1vRuzt1JP",
                "forum": "MOviNImhfq",
                "replyto": "s45X8MHBlz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8116/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8116/Authors"
                ],
                "content": {
                    "title": {
                        "value": "graph-level tasks"
                    },
                    "comment": {
                        "value": ">Can the proposed methods be adopted for graph-level tasks [1,2]?\n\nWe acknowledge that our proposed model is able to be adopted for graph-level tasks. We have evaluated the proposed model for the graph classification task using five commonly used graph classification benchmarks: MUTAG, PTC, IMDB-BINARY, PROTEINS, and ENZYMES. In this experiment, we followed the InfoGraph setting for graph classification and compared the accuracy with the self-supervised state-of-the-art methods, including InfoGraph, GraphCL, MVGRL, BGRL, AD-GCL, LaGraph, and CGRA. \n\nThe results reported in the following table (Table 3 in the paper)  indicate that, in comparison to the best-performing state-of-the-art methods, the proposed approach demonstrates enhanced accuracy for IMDB-BINARY, PROTEINS, and ENZYMES, while maintaining comparable accuracy on other benchmarks. \nIt's worth mentioning that the accuracies of all models are reported from their respective published papers, except for the BGRL results, which we reproduced under the same experimental setting. \n\nModel | IMDB-Binary | PTC | MUTAG | PROTEINS | ENZYMES \n|----------|----------|----------|----------|----------|----------|\n|InfoGraph | 73.0\u00b10.9 | 61.7\u00b11.4 | 89.0\u00b11.1 | 74.4\u00b10.3 | 50.2\u00b11.4 |\n|GraphCL | 71.1\u00b10.4 | 63.6\u00b11.8 | 86.8\u00b11.3 | 74.4\u00b10.5 | 55.1\u00b11.6 |\n|MVGRL  | 74.2\u00b10.7 | 62.5\u00b11.7 | 89.7\u00b11.1 | 71.5\u00b10.3 | 48.3\u00b11.2 |\n|AD-GCL  | 71.5\u00b11.0 | 61.2\u00b11.4 | 86.8\u00b11.3 | 75.0\u00b10.5 | 42.6\u00b11.1 |\n|BGRL  | 72.8\u00b10.5 | 57.4\u00b10.9 | 86.0\u00b11.8 | 77.4\u00b12.4 | 50.7\u00b19.0 |\n|LaGraph  | 73.7\u00b10.9 | 60.8\u00b11.1 | 90.2\u00b11.1 | 75.2\u00b10.4 | 40.9\u00b11.7|\n|CGRA  | 75.6\u00b10.5 | **65.7\u00b11.8** | **91.1\u00b12.5** | 76.2\u00b10.6 | 61.1\u00b10.9 |\n|**SGCL-T** | 75.2\u00b12.8 | 64.0\u00b11.6 | 89.0\u00b12.3 | 79.4\u00b11.9 | **65.3\u00b13.6**|\n|**SGCL-B** | 73.2\u00b13.7 | 62.5\u00b11.8 | 87.0\u00b12.8 | **81.6\u00b12.3** | 63.7\u00b11.6 |\n|**SGCL-D** | **75.8\u00b11.9** | 62.6\u00b11.4 | 86.0\u00b12.6 | 81.5\u00b12.3 | 64.3\u00b12.2 |\n\nWe would like to mention that among the two references you recommended, we compared the accuracies using the second reference, CGRA, while the first reference was supervised, making the comparison inherently unfair."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8116/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700599918976,
                "cdate": 1700599918976,
                "tmdate": 1700599918976,
                "mdate": 1700599918976,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]