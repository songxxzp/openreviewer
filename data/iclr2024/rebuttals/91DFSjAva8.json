[
    {
        "title": "SERA: Sample Efficient Reward Augmentation in offline-to-online Reinforcement Learning"
    },
    {
        "review": {
            "id": "PK641r1mHF",
            "forum": "91DFSjAva8",
            "replyto": "91DFSjAva8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2188/Reviewer_NgYG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2188/Reviewer_NgYG"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates sample-efficient offline-to-online reinforcement learning with reward augmentation technique. Specifically, this paper enhances VCSE with Q conditioned state entropy, deriving initially successful empirical findings on D4RL benchmark."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The perspective of improving sample efficiency for offline-to-online RL seems interesting."
                },
                "weaknesses": {
                    "value": "Overall, I think this paper does not meet the basic bar of ICLR, especially in terms of writing and experiments. I strongly suggest the authors proof-reading the paper thoroughly to make it a stronger submission. See detailed comments below.\n- From Fig. 1, it seems that CQL-SERA > Cal-QL-SERA > Cal-QL baseline > CQL baseline, which contradicts to the empirical findings in Fig. 4.\n- Is the unbiasedness of SERA theoretically guaranteed by replacing V function by Q function? If not, it is kind of over-claiming in Introduction.\n- Please conduct sufficient research investigation on offline-to-online RL. A lot of related works are not appropriately referenced:\n\n[1] Offline-to-online reinforcement learning via balanced replay and pessimistic q-ensemble, CoRL\u201922.\n\n[2] Adaptive policy learning for offline-to-online reinforcement learning, AAAI\u201923.\n\n[3] Policy Expansion for Bridging Offline-to-Online Reinforcement Learning, ICLR\u201923.\n\n[4] Sample Efficient Offline-to-Online Reinforcement Learning, TKDE\u201923.\n\n[5] Actor-Critic Alignment for Offline-to-Online Reinforcement Learning, ICML\u201923.\n\n[6] Fine-tuning offline policies with optimistic action selection, NeurIPS workshop.\n\n[7] A Simple Unified Uncertainty-Guided Framework for Offline-to-Online Reinforcement Learning, arXiv preprint.\n\n[8] PROTO: Iterative Policy Regularized Offline-to-Online Reinforcement Learning, arXiv preprint.\n\n[9] Ensemble-based Offline-to-Online Reinforcement Learning: From Pessimistic Learning to Optimistic Exploration, arXiv preprint.\n\n[10] Towards Robust Offline-to-Online Reinforcement Learning via Uncertainty and Smoothness, arXiv preprint.\n\n- Exploration has been discussed a lot by previous works on offline-to-online RL [3,4,6,7]. Please discuss advantages of SERA over them.\n- In Section 3.1:\n\n(1) $d_\\mathcal{D}$ is not defined.\n\n(2) Should not J(Q) be a MSE loss?\n\n(3) In $\\mathcal{B}_{\\mathcal{M}}^{\\pi}Q(s,a)$, the condition of the expectation is $s \\sim \\mathcal{D}$?\n\n(4) Eq.(1) seems incorrect. Check Eq. (3.1) in Cal-QL paper.\n\n(5) what is $s_i^{knn}$ in Eq.(2)?\n\n- In Section 3.2:\n\n(1) Eq.(4) seems incorrect. Please double-check.\n\n(2) Overall, I cannot follow details in Section 3.2. Please provide step-by-step instructions in Appendix to make it more clear.\n\n- In Section 4.1, Isn\u2019t SERA a generic offline-to-online RL algorithm? Why the training objective is constrained to the framework of CQL and Cal-QL?\n- Moreover, this paper claims to have an appendix pdf, but I cannot find the appendix in openreview.\n- Why experiments are only conducted on 8 selected tasks. In general, MuJoCo has random/medium/medium-replay/medium-expert/etc. datasets. Consider these settings.\n- It seems that there are only one random seed throughout the paper. Please repeat all the experiments with at least three different random seeds to control the randomness. Also, please report the mean and std value.\n- Please consider more sufficient comparison in Fig. 5. Besides, in ant-medium, where is TD3+BC? In ant, halfcheetah, and walker2d, IQL seems performs better than IQL-SERA. Could you provide more explanations?\n- Why only two tasks are selected in Fig. 6 (a)?\n- Why only IQL is selected in Fig. 6 (b) on only two tasks?\n- There are no sufficient ablation studies on each component of SERA. For example, you claim that condition on Q is better than V, thus, please derive some empirical findings to support this claim.\n- Some typos:\n\n(1) Reference format is not well-handled throughout the paper. In ICLR template: xxx (Author, et al., Year)\n\n(2) In page 2: by maximizing no-conditioned -> non-conditioned; Anther reason -> Another.\n\n(3) In page 3: some researches penalty the -> penalize; both offline and online RL., we -> delete the comma; improving Model-free offline-to-online RL -> model-free; \n\n(4) In page 4: given N i.i.d samples -> $N$; consists of samples -> revise this sentence; Add , in Eq.(4); Equation. 4 -> Equation 4;\n\n(5) In page 5: params -> parameters;\n\n(6) In page 6: Differing -> Different;"
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2188/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2188/Reviewer_NgYG",
                        "ICLR.cc/2024/Conference/Submission2188/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2188/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698395774956,
            "cdate": 1698395774956,
            "tmdate": 1700711547139,
            "mdate": 1700711547139,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NXVNnKO7Fe",
                "forum": "91DFSjAva8",
                "replyto": "PK641r1mHF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2188/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2188/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer NgYG (Part 1, Overall responses)"
                    },
                    "comment": {
                        "value": "$A1:$We thank the reviewers for the analysis of the initial version of the manuscript. There are significant differences between the current version and the initial version. We have addressed most of the issues based on the comparison between the two versions of the manuscript. Additionally, in Part 1, we will provide overall responses to select questions. In the subsequent parts, we will reorganize the reviewer's questions and provide detailed responses.\n\n$Q2:$ Should not J(Q) be a MSE loss?\n\n$A2:$ Many thanks to the reviewers for pointing out the shortcomings. It was a writing error.\n\n$Q3:$In $\\mathcal{B}_{M}^{\\pi}Q(s,a)$, the condition of the expectation is $s\\sim \\mathcal{D}$?\n\n$A3:$$\\mathcal{B}$ is bellman operator, and $\\mathcal{B}_{M}^{\\pi}Q(s,a)=r(s,a)+\\gamma\\times Q(s',\\pi(s'))$ where $(s,a,s')\\sim \\mathcal{D}$\n\n$Q4:$ Overall, I cannot follow details in Section 3.2. Please provide step-by-step instructions in Appendix to make it more clear.\n\n$A4:$ The latest version has reorganized Section 3.2 in the main text and provided relevant proofs\n\n$Q5:$In Section 4.1, Isn\u2019t SERA a generic offline-to-online RL algorithm? Why the training objective is constrained to the framework of CQL and Cal-QL?\nMoreover, this paper claims to have an appendix pdf, but I cannot find the appendix in openreview.\nWhy experiments are only conducted on 8 selected tasks. In general, MuJoCo has random/medium/medium-replay/medium-expert/etc. datasets. Consider these settings.\nIt seems that there are only one random seed throughout the paper. Please repeat all the experiments with at least three different random seeds to control the randomness. Also, please report the mean and std value.\n\n$A5:$ Firstly, our theorem 4.1 proves that SERA can ensure the policy optimization of soft Q, so we tend to test it on algorithms based on soft Q, thus we choose Cal-QL and CQL to conduct the test. Secondly, in Figure 4, we tested the performance of SERA combined with various model-free algorithms (TD3+BC, AWAC, IQL), and the improvement brought by SEAR is particularly evident for AWAC. Additionally, we used AWAC as the base algorithm, and in Figure 5, we compared the performance differences between various exploration algorithms (RND, SE) and SEAR. SERA performs the best overall. Additionally, the latest version we submitted has added an appendix and expanded the D4RL experiments to $\\texttt{medium-replay}$.\n\n$Q6:$ Please consider more sufficient comparison in Fig. 5. Besides, in ant-medium, where is TD3+BC? In ant, halfcheetah, and walker2d, IQL seems performs better than IQL-SERA. Could you provide more explanations?\n\n$A6:$ Firstly, IQL combined with SERA generally achieves slightly better performance in most cases. We provide the reason for the poor performance of IQL on $\\texttt{walker2d-medium}$. Beta in IQL is a crucial hyperparameter, where a small beta makes the training target close to behavioral cloning, and a large beta makes the training target close to Q learning. However, when testing SERA, we did not intentionally adjust the hyperparameters of various algorithms. Therefore, a too-small beta may limit the exploratory nature of SERA, thereby restricting the potential improvement in sample efficiency brought by SERA.\n\n$Q7:$ Why only IQL is selected in Fig. 6 (b) on only two tasks?\n\n$A7:$ In the new version, we have added AWAC and compared the performance of IQL and AWAC when combined with RND, SE, and SERA on these two tasks. SERA performs the best.\n\n$Q8:$ There are no sufficient ablation studies on each component of SERA. For example, you claim that condition on Q is better than V, thus, please derive some empirical findings to support this claim.\n\n$A8:$ The current version has expanded with more ablation experiments compared to the initial version (section.5.2). In the next few days, we will further expand necessary ablation experiments based on the experimental progress, such as comparing Q condition with V condition.\n\n$Q9:$ Some typos $\\cdots$\n\n$A9:$ Thanks for the reviewer's suggestions. We have submitted a new version of the manuscript and will systematically address the details based on the writing advice from the reviewers."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2188/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700410050693,
                "cdate": 1700410050693,
                "tmdate": 1700543504495,
                "mdate": 1700543504495,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tvfOUPDkwI",
                "forum": "91DFSjAva8",
                "replyto": "NXVNnKO7Fe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2188/Reviewer_NgYG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2188/Reviewer_NgYG"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your feedback. While I acknowledge considerable enhancements in the writing of this paper, it's important to note that my concerns have not been addressed in a point-by-point manner. Therefore, I have opted to maintain my original score."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2188/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700541995813,
                "cdate": 1700541995813,
                "tmdate": 1700541995813,
                "mdate": 1700541995813,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xj3T0NpHVE",
                "forum": "91DFSjAva8",
                "replyto": "PK641r1mHF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2188/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2188/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer NgYG (Part 2: New extended and ablation studies )"
                    },
                    "comment": {
                        "value": "We thank the reviewer's question of the deficiencies in our initial version of the ablation experiments. In the response of Part 2, we will carefully organize and answer the ablation study-related question and provide more detailed descriptions of the various ablation and extended experimental results we have added in the new version manuscript. \n\n---\n## Extended experiments\n\n$Q1:$ It seems that there are only one random seed throughout the paper. Please repeat all the experiments with at least three different random seeds to control the randomness. Also, please report the mean and std value.\n\n$A1:$ In Table 1, we report results from at least three repeated experiments, with multiple runs tested for each repetition. The new experimental results include both the mean and variance. To facilitate simultaneous presentation, we have included the results below.\n\n|Task Name| CQL | CQL+SERA | CalQL | CalQL+SERA\n|:---------:|:---------:| :---------:|:---------:|:---------:|\n| antmaze-large-diverse |89.2  |89.8$\\pm$3.2|86.3$\\pm$0.2|94.5$\\pm$1.7|\n| antmaze-large-play    |91.7  |&92.6$\\pm$ 1.3|83.3$\\pm$9.0|95.0$\\pm$1.1|\n| antmaze-medium-diverse|89.6 |98.9$\\pm$0.2|96.8$\\pm$1.0 |99.6$\\pm$0.1|\n| antmaze-medium-play   |97.7 |99.4$\\pm$0.4|95.8$\\pm$0.9|\t98.9$\\pm$0.6|\n| halfcheetah-meidum    | 69.9  |87.9$\\pm$2.3|45.6$\\pm$0.0|46.9$\\pm$0.0|\n| walker2d-medium       |123.1 |130.0$\\pm$0.0 |80.3$\\pm$0.4 |90.0$\\pm$3.6|\n| hopper-medium         |56.4|62.4$\\pm$ 1.3|55.8$\\pm$0.7|61.7$\\pm$2.6|\n| ant-medium            |123.8|136.9$\\pm$1.6|96.4$\\pm$0.3|104.2$\\pm$3.0|\n\nAdditionally, in Figure 3, we refer to [1] to conduct a statistical analysis of Table 1, and the improvement brought by SERA is statistically significant.\n\n$Q2:$ Why only two tasks are selected in Fig. 6 (a)?\n\n$A2:$ We thank the reviewer's questions. Figure 6(a) serves as further verification of whether SERA can be applied to soft-Q-based algorithms. We have conducted extensive experiments based on CQL-SAC and Cal-QL, which also demonstrate that SERA can be used in conjunction with Soft-Q. \n\n$Q3:$ Why only IQL is selected in Fig. 6 (b) on only two tasks?\n\n$A3:$ In the new version, we have added AWAC and compared the performance of IQL and AWAC when combined with RND, SE, and SERA on these two tasks in Figure 5 (b). SERA performs the best. \n\n$Q4:$ Why experiments are only conducted on 8 selected tasks.\n\n$A4:$ We have supplemented the experimental results for medium-replay in Figure 1 (training curve) and Table 9 (fine-tuned results). \n\n$\\text{Comparisons 1}:$ We choose CQL as base algorithm, and compare APL, PEX, BR, SERA in Figure 7 and Table 10, we also provide average performance below:\n\n| CQL+APL | CQL+PEX | CQL+BR | CQL+SERA\n|:---------:| :---------:|:---------:|:---------:|\n|56.2 |27.6  |50.4|83.8 |\n\n---\n\n## New Ablation \n\n$Q1:$ Is the unbiasedness of SERA theoretically guaranteed by replacing V function by Q function? If not, it is kind of over-claiming in Introduction. $Q2:$For example, you claim that condition on Q is better than V, thus, please derive some empirical findings to support this claim.\n\n$A1,A2:$ Thank you for pointing out the shortcomings. Our previous manuscript indeed provided overly affirmative descriptions on this matter (Q condition is better than V condition). Therefore, in the supplementary Appendix (F.3), we have added additional experiments to compare the performance differences between Q condition and V condition. Specifically, In Appendix F.3, we chose AWAC as the base algorithm and added a comparison of SERA based on Q condition and V condition to calculate intrinsic rewards. We found that, overall, Q condition performs better than V condition.\n\n$\\text{Pre-trained Q vs. Random Initialized Q when conduct SERA: }$ In the Figure 10 (Appendix F2), we added new ablation experiments using AWAC as the base algorithm on $\\texttt{hopper-medium}$, $\\texttt{walker-medium}$, $\\texttt{halfcheetah-medium}$, and $\\texttt{ant-medium}$ tasks. Specifically, we compared the performance of SERA when using pretrain-Q networks and randomly initialized Q networks to calculate intrinsic rewards, and found that using pre-trained Q networks for intrinsic reward computation yields better performance than using un-pretrained Q networks, thus Q condition is effective, and more carefully analysis, please see the response to Reviewer zoZX (Question.1)\n\n[1] Rishabh Agarwal, et.al. Deep reinforcement learning at the edge of the statistical precipice, 2022"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2188/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700542946565,
                "cdate": 1700542946565,
                "tmdate": 1700558376442,
                "mdate": 1700558376442,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WYV3TRCFeG",
                "forum": "91DFSjAva8",
                "replyto": "PK641r1mHF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2188/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2188/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for actively participating in the rebuttal process and providing numerous suggestions, and we believe the current version has addressed most of the reviewers' concerns. To facilitate communication and streamline responses, we will reply point-by-point under this section (total three part). Additionally, we have added a new section to categorize and organize responses to different kinds of issues. Thanks a lot.\n\n---\n\n$Q1:$ From Fig. 1, it seems that CQL-SERA > Cal-QL-SERA > Cal-QL baseline > CQL baseline, which contradicts to the empirical findings in Fig. 4.\n\n$A1:$ We thank the reviewer's question. Firstly, this is a schematic diagram, and its primary purpose is to illustrate the training phase of SERA. Additionally, we will adjust the positions of the curves based on the final average experimental results.\n\n$Q2:$ Is the unbiasedness of SERA theoretically guaranteed by replacing V function by Q function? If not, it is kind of over-claiming in Introduction.\n\n$A2:$ Thank you for pointing out the shortcomings. Our previous manuscript indeed provided overly affirmative descriptions on this matter (Q condition is better than V condition). Therefore, in the supplementary Appendix (F.3), we have added additional experiments to compare the performance differences between Q condition and V condition. Specifically, In Appendix F.3, we chose AWAC as the base algorithm and added a comparison of SERA based on Q condition and V condition to calculate intrinsic rewards. We found that, overall, Q condition performs better than V condition.\n\n$Q3:$ Please conduct sufficient research investigation on offline-to-online RL. A lot of related works are not appropriately referenced:\n\n$A3:$ We thank reviewer for pointing out the shortcomings. The referenced papers are indeed crucial recent studies. However, due to limited space in the main text, we prefer to introduce studies most directly related to our research.  If necessary, we may choose to add a section on advances in offline-to-online learning in the supplementary material.\n\n$Q4:$ Exploration has been discussed a lot by previous works on offline-to-online RL [3,4,6,7]. Please discuss advantages of SERA over them.\n\n$A4:$ From the experimental results (average results on gym-mujoco (medium, medium-replay) and antmaze), our method outperforms PEX[3], BR[.1], APL[.2], SUNG[7]\n\n| CQL+APL | CQL+PEX | CQL+BR |  CQL+SUNG|CQL+SERA\n|:---------:| :---------:|:---------:|:---------:|:---------:|\n|56.2 |27.6  |50.4|82.4 |83.8|\n\nNext, we will compare SERA with [3, 4, 6, 7] in terms of methods and theories:\n\n- Compared to PEX[3], SERA does not require training an additional policy, and experimental results show that SERA outperforms PEX.\n\n- Compared to [4], SERA does not incorporate meta-adaptation into the framework. Instead, it focuses solely on enhancing offline-to-online performance from the perspective of exploration. Therefore, in theoretical terms, this represents an essential and unique approach.\n\n- SERA and [6] represent two completely opposite approaches. SERA adopts an aggressive exploration strategy during online training, while [6] aims to explore only slightly beyond the replay buffer during online fine-tuning. Therefore, intuitively, SERA can collect a more diverse dataset more quickly.\n\n- SERA and SUNG[7] share a commonality in that SUNG tends to favor optimistic exploration strategies, while SERA also encourages exploration by the agent. The difference lies in the fact that SERA does not require additional training, and SUNG's VAE may be influenced by data quality during pretraining. Additionally, experimental results show that SERA slightly outperforms SUNG\n\n$Q5:$ typos(1) to (5); Q6: Errors of Equation:\n\nWe thank the reviewers for pointing out the shortcomings in our writing. The latest version of our manuscript has shown significant improvement compared to the initial version, and we will further enhance our expression."
                    },
                    "title": {
                        "value": "Response to Reviewer NgYG (Point by point manner, Part 1/3)"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2188/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700544755415,
                "cdate": 1700544755415,
                "tmdate": 1700588118424,
                "mdate": 1700588118424,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sVDRywL08A",
                "forum": "91DFSjAva8",
                "replyto": "PK641r1mHF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2188/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2188/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer NgYG (Point by point manner, Part 2/3)"
                    },
                    "comment": {
                        "value": "$Q6:$ In Section 4.1, isn't SERA a generic offline-to-online RL algorithm? Why the training objective is constrained to the framework of CQL and Cal-QL?\n\n$A6:$ We thank the reviewer's suggestions. In the main text, we have expanded the experiments to popular algorithms such as TD3+BC, SAC, and AWAC. SERA shows overall good performance across these algorithms.\n\n$Q9:$ Moreover, this paper claims to have an appendix pdf, but I cannot find the appendix in openreview.\n\nThe supplementary material has been added after the main text. Specifically, \n\n- In Appendix B, we provide some theoretical proofs relevant to this paper. Specifically,  Appendix B.1 explains the connection between State entropy maximization and exploration. Appendix B.2 contains Theorems 4.1 and 4.2. Theorem 4.1 ensures policy improvement in Soft-Q algorithms when combined with SERA.\n\n- In Appendix C$\\sim$D, we have systematically organized the implementation details of SERA, including implementation, computing resources and hyperparameters.\n\n- In Appendix E, We have added the performance of the algorithm on medium-replay, and further comparisons were made between SERA and various efficient offline-to-online algorithms.\n\n$Q10:$ Why experiments are only conducted on 8 selected tasks. In general, MuJoCo has random/medium/medium-replay/medium-expert/etc. datasets. Consider these settings.\n\n$A10:$ We have supplemented the experimental results for medium-replay in Figure 1 (training curve) and Table 9 (fine-tuned results). \n\n$Q11:$ It seems that there are only one random seed throughout the paper. Please repeat all the experiments with at least three different random seeds to control the randomness. Also, please report the mean and std value.\n\n$A11:$ In Table 1, we report results from at least three repeated experiments, with multiple runs tested for each repetition. The new experimental results include both the mean and variance. To facilitate simultaneous presentation, we have included the results below.\n\n|Task Name| CQL | CQL+SERA | CalQL | CalQL+SERA\n|:---------:|:---------:| :---------:|:---------:|:---------:|\n| antmaze-large-diverse |89.2  |89.8$\\pm$3.2|86.3$\\pm$0.2|94.5$\\pm$1.7|\n| antmaze-large-play    |91.7  |&92.6$\\pm$ 1.3|83.3$\\pm$9.0|95.0$\\pm$1.1|\n| antmaze-medium-diverse|89.6 |98.9$\\pm$0.2|96.8$\\pm$1.0 |99.6$\\pm$0.1|\n| antmaze-medium-play   |97.7 |99.4$\\pm$0.4|95.8$\\pm$0.9|\t98.9$\\pm$0.6|\n| halfcheetah-meidum    | 69.9  |87.9$\\pm$2.3|45.6$\\pm$0.0|46.9$\\pm$0.0|\n| walker2d-medium       |123.1 |130.0$\\pm$0.0 |80.3$\\pm$0.4 |90.0$\\pm$3.6|\n| hopper-medium         |56.4|62.4$\\pm$ 1.3|55.8$\\pm$0.7|61.7$\\pm$2.6|\n| ant-medium            |123.8|136.9$\\pm$1.6|96.4$\\pm$0.3|104.2$\\pm$3.0|\n|average                |92.7 | 94.7        | 78.8       | 86.4        |\n\n$Q12:$ Please consider more sufficient comparison in Fig. 5. Besides, in ant-medium, where is TD3+BC? In ant, halfcheetah, and walker2d, IQL seems performs better than IQL-SERA. Could you provide more explanations?\n\n$A12:$ We thank the reviewer's suggestions. Firstly, IQL combined with SERA generally achieves slightly better or the same performance in most cases. In particular, we provide the reason for the poor performance of IQL on $\\texttt{walker2d-medium}$. $\\beta$ in IQL is a crucial hyperparameter, where a small $\\beta$ makes the training target close to behavioral cloning, and a large $\\beta$ makes the training target close to Q learning. However, when testing SERA, we did not intentionally adjust the hyperparameters of various algorithms. Therefore, a too-small $\\beta$ may limit the exploratory nature of SERA, thereby restricting the potential improvement in sample efficiency brought by SERA."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2188/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700586572486,
                "cdate": 1700586572486,
                "tmdate": 1700638827250,
                "mdate": 1700638827250,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VqVXIMlzxB",
                "forum": "91DFSjAva8",
                "replyto": "PK641r1mHF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2188/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2188/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer NgYG (Point by point manner, Part 3/3)"
                    },
                    "comment": {
                        "value": "$Q13:$ Why only two tasks are selected in Fig. 6 (a)?\n\n$A13:$ We thank the reviewer's questions. Figure 6(a) serves as further verification of whether SERA can be applied to soft-Q-based algorithms (theorem 4.1 has the guarantee of soft-Q optimization). We have conducted extensive experiments based on CQL-SAC and Cal-QL, which demonstrate that SERA can be used in conjunction with Soft-Q.\n\n$Q14:$ Why only IQL is selected in Fig. 6 (b) on only two tasks?\n\n$A14:$ In the new version, we have added AWAC and compared the performance of IQL and AWAC when combined with RND, SE, and SERA on these two tasks. SERA performs the best.\n\n$Q15:$ There are no sufficient ablation studies on each component of SERA. For example, you claim that condition on Q is better than V, thus, please derive some empirical findings to support this claim.\n\n$A15:$We thank the reviewer's suggestions, and we have incorporated a substantial number of ablation experiments in the latest version of the manuscript. We have also systematically organized the previous experimental results in the $\\textbf{Response to Reviewer NgYG (Part 2: New extended and ablation studies )}$ section, facilitating the reviewers in their examination and comparative analysis.\n\n---\n\n$Q16:$ Some typos: $\\cdots$\n\n$A16:$ We thank reviewers for pointing out these shortcomings. Your feedback is very helpful in improving our writing.\n\n[.1] Lee S, Seo Y, Lee K, et al. Offline-to-online reinforcement learning via balanced replay and pessimistic q-ensemble[C]//Conference on Robot Learning. PMLR, 2022: 1702-1712.\n\n[.2] Adaptive policy learning for offline-to-online reinforcement learning, AAAI\u201923."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2188/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700586592003,
                "cdate": 1700586592003,
                "tmdate": 1700638786258,
                "mdate": 1700638786258,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Gxu6aa6WN7",
                "forum": "91DFSjAva8",
                "replyto": "PK641r1mHF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2188/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2188/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Dear Reviewer NgYG, we kindly and sincerely invite you to provide further feedback."
                    },
                    "comment": {
                        "value": "Dear Reviewer  NgYG,\n\nAs the rebuttal stage is nearing its conclusion, we kindly invite you to provide feedback on our latest manuscript and the current responses. Thank you.\n\nBest regards."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2188/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700703949581,
                "cdate": 1700703949581,
                "tmdate": 1700705673660,
                "mdate": 1700705673660,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Buj6jpeRRd",
                "forum": "91DFSjAva8",
                "replyto": "Gxu6aa6WN7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2188/Reviewer_NgYG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2188/Reviewer_NgYG"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your comprehensive response addressing each point individually. However, I still harbor reservations about certain concerns, particularly Q1, Q3, Q6, and Q11. Additionally, I'd like to highlight that the substantial revision and response, especially near the author-reviewer discussion deadline, indeed place a significant burden on the reviewers. Despite these considerations, I am raising my score to 5."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2188/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700711506920,
                "cdate": 1700711506920,
                "tmdate": 1700711506920,
                "mdate": 1700711506920,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uqXpLq0VyT",
                "forum": "91DFSjAva8",
                "replyto": "PK641r1mHF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2188/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2188/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Further discussion of Q1 and Q3"
                    },
                    "comment": {
                        "value": "Further discussion of Q1\n\nIn our new manuscript, we have revised Figure 1, which now serves two main purposes: 1) Figure 1 succinctly illustrates the process of SERA. 2) Figure 1 demonstrates that SERA can benefit certain algorithms.\n\n---\n\nFurther discussion of Q3: We supplement related work in this section, and we have added such content in the Appendix of new manuscript. \n\nIn the past, efforts have primarily focused on enhancing offline-to-online performance from two perspectives.\n\nThe first perspective involves adopting a conservative policy optimization during online fine-tuning, typically achieved through the incorporation of policy constraints. Specifically, there are three main approaches within this category. The first approach constrains the predictions of the fine-tuning policy within the scope of offline support during online fine-tuning [1]. While this method contributes to achieving stable online fine-tuning performance, it tends to lead to overly conservative policy learning, and the accuracy of the estimation of offline support also influences the effectiveness of online fine-tuning. The second approach utilizes an offline dataset to constrain policy learning [2,3,4,5]. However, the effectiveness of fine-tuning cannot be guaranteed if the dataset quality is poor. This method is sensitive to the quality of the dataset. The third approach employs pre-trained policies to constrain online fine-tuning, but this paradigm is influenced by the quality of the pre-trained policy [6,7].\n\nThe second perspective involves adopting a conservative approach during offline training, specifically using pessimistic constraints to learn Q to avoid OOD (Out-of-Distribution) issues. Research in this category primarily includes: Learning a conservative Q during offline pretraining and employing an appropriate experience replay method during online learning or using Q ensemble during offline pretraining to avoid OOD problems [8,10,11]. However, as this approach introduces conservative constraints during critic updates, the value estimates between offline and online are not aligned, leading to a decrease in performance during early online fine-tuning. Therefore, Cal-QL introduces a calibrated conservative term to ensure standard online fine-tuning [9].\n\nAddtionally, there are also some other methods, such that ODT[12] combined sequence modeling with Goal conditioned RL to conduct offline-to-online RL.\n\nreference:\n\n(Policy constrain)\n\n[1] Supported Policy Optimization for Offline Reinforcement Learning. (SPOT)\n\n[2] AWAC: Accelerating Online Reinforcement Learning with Offline Datasets. (AWAC)\n\n[3] Offline Reinforcement Learning with Implicit Q-Learning. (IQL)\n\n[4] The In-Sample Softmax for Offline Reinforcement Learning. (InAC)\n\n[5] ine-Tuning Offline Policies With Optimistic Action Selection. (O3F)\n\n[6] Policy Expansion for Bridging Offline-to-Online Reinforcement Learning. (PEX)\n\n[7] Actor-Critic Alignment for Offline-to-Online Reinforcement Learning. (ACA)\n\n(Pessimitic critic)\n\n[8] Offline-to-Online Reinforcement Learning via Balanced Replay and Pessimistic Q-Ensemble .\n\n[9] Cal-QL: Calibrated Offline RL Pre-Training for Efficient Online Fine-Tuning.\n\n[10] Mildly conservative q-learning for offline reinforcement learning.\n\n[11] Confidence-conditioned value functions for offline reinforcement learning.\n\n(Additional method)\n\n[12] Online Decision Transformer."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2188/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700731469905,
                "cdate": 1700731469905,
                "tmdate": 1700738319695,
                "mdate": 1700738319695,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CfIr4nQsZh",
            "forum": "91DFSjAva8",
            "replyto": "91DFSjAva8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2188/Reviewer_zoZX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2188/Reviewer_zoZX"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the problem of fine-tuning pre-trained offline RL agents. Specifically, it proposed a reward augmentation framework, named Sample Efficient Reward Augmentation (SERA), to encourage exploration in the fine-tuning stage with Q conditional state entropy. SERA further uses state marginal matching (SMM) and penalizes OOD state actions. Experiments on the D4RL benchmark tasks showed the proposed SERA outperformed other baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This paper investigates an important question in offline RL.\n- The proposed method outperformed other baseline in the D4RL benchmark task."
                },
                "weaknesses": {
                    "value": "- Firstly, the writing is not good enough. Many sentences are not rigorous or confusing. For example:\n    - In the first paragraph, \"such paradigm can only learn similar or slightly better performance than behavioural policy\" is not true. Because model-based offline RL methods can sometimes significantly improve the performance w.r.t. the behavioural policy.\n    - In the third paragraph, \"The second approach employs offline RL with policy regression\". What does the \"policy regression\" mean? Or it's a typo of \"policy regularization\".\n    - \"underestimate the value of the offline buffer in comparison to the ground truth returns\" => should be \"underestimate the value of OOD samples in the offline buffer\"\n\n- There are too many typos and grammar errors: \n    1. \"some researches penalty the Q values\" ==> penalize\n    2. Missing period after \"or implicitly regularize the bellman equation\"\n    3. \"It similarly train agent\" ==> trains\n    4. \"high sampling efficiency\" ==> sample\n    5. extra period \"on both offline and online RL., we \"\n    6. \"as a Markov decision Process\" ==> Decision\n    7. \"A denotes the actions space\" ==> action\n    8. missing comma in \"tau = {s0, a0, r0, ..., st, at rt}\"\n    9. missing \"the\" in \"in offline-to-online RL problem setting\"\n    10. \"Bellman equation iteration\"  ==> \"Bellman iteratio equation\"\n    11. \"it always suffer from\" ==> suffers\n    12. missing norm notation in the one step Bellman equation\n    13. missing right bracket in \"if (s', pi(\\cdot | s') \\notin D\"\n    14. \"to penalty the OOD state actions\" ==> penalize\n    15. \"expected regression\" ==> expectile regression\n    16. \"by rollout behavioural policy\" ==> unrolling\n    17. \"thus has the\" ==> having\n    18. \"only maximize\" ==> maximizing\n    19. \"rather E[H[s]]\" ==> rather than\n    20. \"where Tanhs see\" ==> sees\n\n- There are some missing SOTA baselines for offline-to-online fine-tuning in the experiments: Reincarnating RL [1], PEX [2], InAC [3]\n\n[1] (Agarwal et al., NeurIPS' 22) Reincarnating reinforcement learning: Reusing prior computation to accelerate progress\n\n[2] (Zhang et al., ICLR' 23) Policy Expansion for Bridging Offline-to-Online Reinforcement Learning\n\n[3] (Xiao et al., ICLR' 23) The In-Sample Softmax for Offline Reinforcement Learning"
                },
                "questions": {
                    "value": "- \"which is unbiased in the early online process\" => why it's unbiased?\n\n- Since the main argument of this work is a new exploration method for fine-tuning offline RL agents. I think it should compare to other  intrinsic reward baselines, i.e, state entropy, RND, ICM."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2188/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2188/Reviewer_zoZX",
                        "ICLR.cc/2024/Conference/Submission2188/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2188/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698511993731,
            "cdate": 1698511993731,
            "tmdate": 1700742113170,
            "mdate": 1700742113170,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "r4OcLJ5nUS",
                "forum": "91DFSjAva8",
                "replyto": "CfIr4nQsZh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2188/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2188/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer zoZX (Part of Questions)"
                    },
                    "comment": {
                        "value": "$Q1:$ \"which is unbiased in the early online process\" => why it's unbiased?\n\n$A1:$  We thank the essiential question of Reviewer, and we would like to further clarify our use of 'un-biased' in reviwer's comments.  Such 'un-biased' are not related to the concept of distribution shift between offline and online. Instead, such 'un-based' are specifically referring to whether the Q used in calculating the state entropy of the Q condition is pre-trained. In VCSE [.1], the authors considered a purely online setting and used a value network with random initialization to compute the condition of value-conditioned entropy. However, a value network with random initialization may not accurately estimate the values of all states in the early stages of online training. Consequently, when calculating the value-conditioned state entropy, there may be situations where states with different true values correspond to the same output from the value network. \n\nTherefore, we use a value network with random initialization to calculate the value-conditioned entropy as a 'biased' prediction in the early stages of online training. Such biased prediction doesn't align with the original intention of VCSE because low-value states and high-value states may be assigned the same value condition in the early stages of online training, it could result in states from low-value regions being included in high-value regions, or states from high-value regions being included in low-value regions.\n\nHowever, we can't immediately conclude that an offline pre-trained Q network is necessarily better than a randomly initialized Q network. There is a distribution shift issue between offline and online. Therefore, we chose AWAC as the base algorithm and combined with SERA (pretrained Q condition), SERA (from-scratch Q condition) to test on various offline-to-online tasks such as $\\texttt{ant-medium}$, $\\texttt{hopper-medium}$, $\\texttt{walker-medium}$, and $\\texttt{halfcheetah-medium}$. As shown in Figure 10 (Appendix F.2), using an offline-pretrained Q as a condition performs better compared to using a randomly initialized Q as a condition and consistently outperforms the baseline. In particular, using a randomly initialized Q as a condition even achieves lower performance than the baseline in $\\texttt{walker-medium}$. These experimental results all demonstrate the advantage of using a pre-trained Q network when computing conditions\n\n$Q2:$ Since the main argument of this work is a new exploration method for fine-tuning offline RL agents. I think it should compare to other intrinsic reward baselines, i.e, state entropy, RND, ICM.\n\n$A2:$ Thanks for the valuable suggestions from the reviewers. We chose AWAC and IQL as base algorithms, combined with SERA, RND, and state entropy (SE), and conducted offline-to-online testing on $\\texttt{walker-medium}$ and $\\texttt{hopper-medium}$. As shown in Figure 5 (b), we found that the algorithm combined with SERA performs the best.\n\n[.1] Dongyoung Kim, et al. Accelerating reinforcement learning with value-conditional state entropy exploration, 2023"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2188/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700401565832,
                "cdate": 1700401565832,
                "tmdate": 1700575019943,
                "mdate": 1700575019943,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZEugYkn327",
            "forum": "91DFSjAva8",
            "replyto": "91DFSjAva8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2188/Reviewer_sGsS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2188/Reviewer_sGsS"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on offline-to-online RL and proposes improving the performance by enhancing exploration during online fine-tuning with a reward augmentation framework, SERA. The intrinsic rewards are calculated by implementing State Marginal Matching (SMM) and penalizing out-of-distribution (OOD) state actions."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The proposed method is easy to understand.\n- The technique seems sound."
                },
                "weaknesses": {
                    "value": "- See the questions."
                },
                "questions": {
                    "value": "==Major concerns==\n- The authors are strongly advised to revise this paper carefully. There are so many typos in this paper, which affects the normal comprehension of this paper.\n- I do not understand how to calculate Equation (3) in practice when the state is high dimensional continuous variables. Can the authors provide the analysis?\n- What is the relation between Equation (2) and Equation (4) when calculating the critic-conditioned intrinsic reward?\n- Every intrinsic reward calculation must be calculated by KNN, so the efficiency of physic time consumption may be a little poor.\n- I can not find the appendix mentioned in this paper.\n- Can the author provide the whole comparisons about D4RL datasets?\n- The format of some citations is wrong.\n- What about the random seed in the experiments?\n\n==Minor concerns==\n- The authors should explain all symbols that appear in this paper, e.g., in Section 3.1, the authors do not introduce $d_D(.|s)$ and $\\mathcal{G}_{\\mathcal{M}}$.\n- In Definition 1, why the critic-conditioned entropy does not contain \u201c-\u201d. Besides, if there are N states that are used for calculating the critic conditioned entropy, \n- In Equation (3), the initial state distribution is $\\rho_0(S)$, but in Section 3.1, the initial state distribution is defined as $p(s_0)$. Besides, \n- In Equation (4), the symbols of the left side and the right side are very different. Can the authors provide a detailed derivation?\n- The authors should provide the derivation about \u201cAnother reason is that maximizing Es\u223c\u03c1(s)[H\u03c0[s]] is equivalent to minimize DKL(\u03c1\u03c0(s)||p\u2217(s)) thus has the mathematical guarantee.\u201d\n- Different reference expressions about figures.\n\n\n\n==Typos==\n- Section 3.1 \u201cModel-free Offline RL\u201d: \u201cIn particular, Model-free\u201d-> \u201cIn particular, model-free\u201d\n- Section 3.1 \u201cModel-free Offline RL\u201d: \u201cSpecifically, Model-free\u201d -> \u201cSpecifically, model-free\u201d\n- Section 3.1 \u201cModel-free Offline RL\u201d: \u201cone step bellman equation i.e. \u2026. which\u201d -> \u201cone step bellman equation, i.e. xxxx, which\u201d\n- Section 3.1 \u201cModel-free Offline RL\u201d: \u201cPrevious studies have extensively studied such a problem, such that CQL was proposed to penalty the OOD state actions by conservative term (Equation 1), and IQL implicitly learns Q function with expected regression without explicit access to the value estimation of OOD state-actions.\u201d\n- Section 3.1: \u201cstate entropy(Seo et al., 2021)\u201d -> \u201cstate entropy (Seo et al., 2021)\u201d\n- Section 3.1: \u201ci.i.d\u201d-> \u201c\u201ci.i.d.\u201d\u201d\n- Section 3.2: grammatical mistake: \u201cSpecifically, we first use the offline methods to \u2026..\u201d\n- Section 3.2: \u201c\\pi_{beta}\u201d -> \u201c\\pi_{\\beta}\u201d\n- Section 3.2: \u201cEquation. 4\u201d -> \u201cEquation (4)\u201d\n- Section 3.2: \u201cSMM,i.e.\u201d -> \u201cSMM, i.e.\u201d\n- Section 3.2: \u201cOnly maximize\u201d -> \u201cOnly maximizing\u201d\n- Section 4.1: \u201c\u2026 are the params of double Q Networks\u201d -> \u201c\u2026 are the parameters of double Q Networks\u201d\n- Section 4.1: \u201cin addition to testing SERA\u201d -> \u201cin addition to test SERA\u201d\nThere are so many typos, so I suggest the authors check this paper carefully."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2188/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698676853944,
            "cdate": 1698676853944,
            "tmdate": 1699636152460,
            "mdate": 1699636152460,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XLn5qm7Mnz",
                "forum": "91DFSjAva8",
                "replyto": "ZEugYkn327",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2188/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2188/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer sGsS (Part of Major Concerns)"
                    },
                    "comment": {
                        "value": "$Q1:$ The authors are strongly advised to revise this paper carefully. There are so many typos in this paper, which affects the normal comprehension of this paper.\n\n$A1:$ We appreciate the valuable suggestions provided by the reviewers. The current version differs significantly from the initial version. Specifically,\n\n- (1) in Section 3 and 4, we have redefined the fundamental concepts required for this paper, and we have provided a mathematical proof (Appendix B.1 and B.2) demonstrating that SERA can be used in conjunction with Soft-Q. Thus, the experimental performance of SERA can be interpreted. \n\n- (2) In Section 5, we extended experiments related to d4rl from medium to medium-replay (Figure 1, Table 9). Additionally, we reported the averaged results of experiments conducted multiple times in Table 1. Furthermore, in Figure 3, we conducted a statistical analysis of Table 1, and the statistical results indicate that the improvement brought by SERA to the algorithm is significant.\n\n- (3) In Section 6, we have included a substantial number of ablation experiments to comprehensively validate the effectiveness of SERA from multiple perspectives. These ablation experiments include: in Appendix F.3, the performance of SERA under V condition is better than under Q condition, which confirms the point made in the last paragraph of Section 3. Additionally, the comparison between pre-train Q and un-pretrain Q, as added in the supplementary pages, attests that SERA can effectively utilize the Q network pre-trained offline to calculate intrinsic rewards.\n\n$Q2:$ I do not understand how to calculate Equation (3) in practice when the state is high dimensional continuous variables. Can the authors provide the analysis?\n\n$A2:$ Thanks for reviewer's question. Firstly, although this formula is in discrete form, it is independent of whether the variables are continuous or not. Additionally, this formula calculates entropy for a batch and does not cover all samples. However, with a sufficient number of training iterations, it is equivalent to maximizing the state entropy over the entire space. \n\n$Q3:$ What is the relation between Equation (2) and Equation (4) when calculating the critic-conditioned intrinsic reward?\n\n$A3:$ Thanks for the reviewer's suggestions. Firstly, we have removed Equation 2 in the main text and systematically defined the concept and mathematical form of state entropy under the joint probability distribution (new version manuscript, Definition 2). We have also provided the implementation form of Q-conditioned state entropy (new version manuscript, Equation 2). In addition, based on the initial version, Formula 2 in the initial version cannot directly calculate Q conditioned state entropy. The initial version manuscript adopts the same method as the current version to calculate Q-conditioned state entropy, which is same as Equation 2 in new version manuscript.\n\n$Q4:$ Every intrinsic reward calculation must be calculated by KNN, so the efficiency of physic time consumption may be a little poor.\n\n$A4:$ We thank the reviewer's question. Firstly, the main advantage of mathematically approximating entropy is that it does not require training. Secondly, in the new version, we have supplemented the calculation method based on VAE (Equation 21). We plan to compare the speed and cost between the VAE-based and mathematical methods in the future. \n\n$Q5:$ I can not find the appendix mentioned in this paper.\n\n$A5:$ The current version includes the supplementary material directly appended to the end of the main text. (introduction see Response to Reviewer NgYG (Point by point manner, Part 2/3), Q9)\n\n\n$Q6:$ Can the author provide the whole comparisons about D4RL datasets? \n\n$A6:$  We have added experimental results for \"medium-replay\" in Figure 2 and Table 9. Additionally, we refer [1.] to perform statistical analysis on the results from Table 1 in Figure 3. SERA brings significant improvements to Cal-QL and CQL.\n\n$Q7:$ The format of some citations is wrong.\n\n$A7:$ We thank the reviewer for pointing out the shortcomings of writing. The current version of the writing has seen significant improvement compared to the initial version. The suggestions from the reviewers have been instrumental in enhancing the quality of the manuscript.\n\n$Q8:$ What about the random seed in the experiments?\n\n$A8:$ Thanks, we have presented the mean and variance of the final performance for each task in Table 1 (new version), we have included the results below (see: Response to Reviewer NgYG (Part 2: New extended and ablation studies ))."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2188/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700384113235,
                "cdate": 1700384113235,
                "tmdate": 1700626810245,
                "mdate": 1700626810245,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aEr9Dh18eU",
                "forum": "91DFSjAva8",
                "replyto": "ZEugYkn327",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2188/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2188/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer sGsS (Part of Minor Concerns)"
                    },
                    "comment": {
                        "value": "$Q1:$ in Section 3.1, the authors do not introduce and In Definition 1, $Q2:$why the critic-conditioned entropy does not contain \u201c-\u201d $\\cdots$.\n\n$A1,A2:$ We thank suggestions provided by the reviewer, and we believe these questions and recommendations will further refine our definitions and writing. In the new manuscript, we have organized and redefined several concepts essential for understanding this paper. Some symbols and previously defined content have been removed, but we still provide explanations for certain symbols used previously. $\\mathcal{G}$ means the greedy operator,ie, $\\pi \\leftarrow argmax_{\\pi} E_{s\\sim D}[Q(\\pi(s),s)]$, and $d(\\cdot|s)$ denotes state-marginal distribution in dataset $D$.\n\n$Q3:$ The authors should provide the derivation about \u201cAnother reason is that maximizing $E_{s\\sim\\rho(s)}[H_{\\pi}[s]]$ is equivalent to minimize $D_{KL}(\\rho_{\\pi}(s)||p^*(s))$.\u201d\n\n$A3:$ Good questions! In the new version paper, we have systematically defined and demonstrated that maximizing entropy serves two main purposes (Appendix.B.1): (1) Maximizing state entropy encourages the agent to explore the entire state space, enabling the acquisition of states not initially included in the dataset through increased exploration. (2) Maximizing state entropy establishes a trade-off, where one aspect aids in approximating any target density. Furthermore, our Theorem 4.1 ensures that, when combined with SERA in soft Q algorithms, policy improvement is guaranteed. Certainly, we acknowledge that in the past, our expression regarding this matter may have been too affirmative. Merely maximizing entropy cannot guarantee the absolute achievement of SMM (State Marginal Matching). Achieving SMM in an absolute sense requires combining term 3, ie, $\\max E_{s\\sim \\rho_{\\pi}}[\\log p^*(s)]$. This term can offset term1 in the Appendix.B.1 (page 16, Equation.7), thereby achieving SMM.\n\n[1] Rishabh Agarwal, et.al. Deep reinforcement learning at the edge of the statistical precipice, 2022"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2188/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700560647512,
                "cdate": 1700560647512,
                "tmdate": 1700560715346,
                "mdate": 1700560715346,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EPDpYEho5e",
                "forum": "91DFSjAva8",
                "replyto": "ZEugYkn327",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2188/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2188/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer sGsS (Part of Typos)"
                    },
                    "comment": {
                        "value": "We thank the reviewers for pointing out the shortcomings in our initial manuscript. The suggestions from the reviewers are very helpful in improving the quality of our manuscript. The current version has shown significant improvement in writing compared to the initial version."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2188/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700587387068,
                "cdate": 1700587387068,
                "tmdate": 1700587387068,
                "mdate": 1700587387068,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4wLzCfeSYi",
                "forum": "91DFSjAva8",
                "replyto": "ZEugYkn327",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2188/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2188/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Dear Reviewer sGsS, we kindly and sincerely invite you to provide further feedback, thank you"
                    },
                    "comment": {
                        "value": "Dear Reviewer sGsS,\n\nAs the rebuttal stage is nearing its conclusion, we kindly invite you to provide feedback on our latest manuscript and the current responses. Thank you.\n\nBest regards."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2188/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738358991,
                "cdate": 1700738358991,
                "tmdate": 1700738426450,
                "mdate": 1700738426450,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "D6rExE5Z15",
            "forum": "91DFSjAva8",
            "replyto": "91DFSjAva8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2188/Reviewer_pDE4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2188/Reviewer_pDE4"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a generalized reward enhancement framework known as SERA, which aims to boost online fine-tuning performance by designing intrinsic rewards, thereby improving the online performance of offline pre-trained policies. SERA achieves this by implicitly enforcing state marginal matching and penalizing out-of-distribution state behaviors, encouraging the agent to cover the target state density, resulting in superior online fine-tuning outcomes. Experimental results consistently demonstrate the effectiveness of SERA in enhancing the performance of various algorithms in offline-to-online settings."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The exploration of the offline-to-online problem in this study holds great relevance and is imperative for practical implementations, aligning seamlessly with the demands of real-world situations.\n2. The fundamental idea at the core of this study is firmly grounded. While the concept presented in this paper is rather straightforward, involving the introduction of an exploration strategy during the online phase to enhance performance, the specific exploration technique employed is quite novel and has demonstrated favorable results in the experiments."
                },
                "weaknesses": {
                    "value": "1. In the experimental section, the author conducted experiments solely on the medium dataset in MuJoCo. However, according to the consensus in the field of offline-to-online research, it is generally recommended to perform experiments on at least three types of datasets: medium, medium-replay, and medium-expert, in order to validate the effectiveness of the method.\n2. The method proposed in this paper is primarily an extension of CQL and Cal-QL. However, in the context of the offline-to-online field, the actual compared baselines are limited to AWAC and Cal-QL. It is advisable for the authors to consider comparing their method with other more efficient algorithms such as Balanced Replay[1], PEX[2], and ENOTO[3].\n3. The SERA algorithm, proposed in this paper, primarily enhances online performance by designing intrinsic rewards to encourage exploration. This concept has been mentioned in previous works such as O3F[4] and ENOTO, although SERA employs different exploration methods. While introducing exploration during the online phase can enhance performance, it may introduce another challenge: instability due to distribution shift, which can lead to performance degradation in the early stages of online learning. This issue has been discussed in many offline-to-online works and is a critical metric in this field. However, it might not be very evident on the medium dataset. Therefore, the authors should consider conducting additional experiments on the medium-replay and medium-expert datasets to verify whether performance degradation occurs.\n4. In Figure 4, the experimental results for the Antmaze environment are challenging to discern, as the curves for various algorithms are intertwined and unclear. The author should consider optimizing the representation of these experimental results for better clarity.\n5. In Table 1, only the mean values of the algorithm results are presented, with a lack of information regarding the errors or variances associated with these results.\n\n[1] Lee S, Seo Y, Lee K, et al. Offline-to-online reinforcement learning via balanced replay and pessimistic q-ensemble[C]//Conference on Robot Learning. PMLR, 2022: 1702-1712.\n\n[2] Zhang H, Xu W, Yu H. Policy Expansion for Bridging Offline-to-Online Reinforcement Learning[J]. arXiv preprint arXiv:2302.00935, 2023.\n\n[3] Zhao K, Ma Y, Liu J, et al. Ensemble-based Offline-to-Online Reinforcement Learning: From Pessimistic Learning to Optimistic Exploration[J]. arXiv preprint arXiv:2306.06871, 2023.\n\n[4] Mark M S, Ghadirzadeh A, Chen X, et al. Fine-tuning offline policies with optimistic action selection[C]//Deep Reinforcement Learning Workshop NeurIPS 2022. 2022."
                },
                "questions": {
                    "value": "See weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2188/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2188/Reviewer_pDE4",
                        "ICLR.cc/2024/Conference/Submission2188/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2188/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698719691647,
            "cdate": 1698719691647,
            "tmdate": 1700444522307,
            "mdate": 1700444522307,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GqivgjDDUt",
                "forum": "91DFSjAva8",
                "replyto": "D6rExE5Z15",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2188/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2188/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer pDE4"
                    },
                    "comment": {
                        "value": "$Q1:$ In the experimental section, the author conducted experiments solely on the medium dataset in MuJoCo. However, according to the consensus in the field of offline-to-online research, it is generally recommended to perform experiments on at least three types of datasets: \"medium\", \"medium-replay\", and \"medium-expert\", in order to validate the effectiveness of the method.\n\n$A1:$ We thank the reviewer for the suggestion to supplement the experiments (D4RL). In the new version of the manuscript, we have added tests on \"medium-replay\" in D4RL. As shown in Table.9 (SERA improve CQL 21%, Cal-QL 12.2%), and CQL-SERA has the best overall performance (Compared with various baselines IQL, AWAC, TD3+BC, CQL, Cal-QL). \n\n$Q2:$ The method proposed in this paper is primarily an extension of CQL and Cal-QL. However, in the context of the offline-to-online field, the actual compared baselines are limited to AWAC and Cal-QL. It is advisable for the authors to consider comparing their method with other more efficient algorithms such as Balanced Replay, PEX, and ENOTO.\n\n$A2:$ Good suggestions! We have supplemented relevant tests. As shown in Figure 7 (new manuscript) and Table 10 in the appendix (new manuscript), CQL-SERA outperforms CQL-APL, CQL-PEX, and CQL-BR on (antmaze, and gym-mujoco {medium, medium-replay} domains). At the same time, for convenience, we have provided the average performance of the compared algorithms on all tasks below:\n\n| CQL+APL | CQL+PEX | CQL+BR | CQL+SERA\n|:---------:| :---------:|:---------:|:---------:|\n|   56.2   |    27.6   |   50.4  |  83.8     |\n\n$Q3:$ The SERA algorithm, proposed in this paper, primarily enhances online performance by designing intrinsic rewards to encourage exploration. This concept has been mentioned in previous works such as O3F[4] and ENOTO, although SERA employs different exploration methods. While introducing exploration during the online phase can enhance performance, it may introduce another challenge: instability due to distribution shift, which can lead to performance degradation in the early stages of online learning. This issue has been discussed in many offline-to-online works and is a critical metric in this field. However, it might not be very evident on the medium dataset. Therefore, the authors should consider conducting additional experiments on the medium-replay and medium-expert datasets to verify whether performance degradation occurs\n\n$A3:$ Nice suggestions! In question 1, we supplemented experiments related to D4RL. Additionally, in the supplementary pages, we provided mathematical proofs demonstrating that SERA ensures policy improvement during soft Q optimization (Theorem B.3 in Appendix.B.2) and encourages the agent to uniformly explore the entire observation space (Appendix.B.1 \"Why does ASMM encourage covering the target density?\"). Additionally, [.2] explains that diverse data can address the shortcomings of conservative policies, and SERA can encourage the agent to explore the environment as much as possible, facilitating the collection of more diverse dataset, therefore, from a theoretical perspective, SERA also helps improve various soft Q-based algorithms. \n\n$Q4:$ In Figure 4 (Figure 2 in new manuscript), the experimental results for the Antmaze environment are challenging to discern, as the curves for various algorithms are intertwined and unclear. The author should consider optimizing the representation of these experimental results for better clarity.\n\n$A4:$ Good suggestions! According to the reviewer's suggestion, we further optimized this figure using uniform sampling. The distinguishability between curves has significantly improved compared to the previous version. The trends of the curves remain consistent with the statements in the paper, showing that CQL-SERA and Cal-QL-SERA converge faster and achieve better fine-tuned results.\n\n$Q5:$ In Table 1, only the mean values of the algorithm results are presented, with a lack of information regarding the errors or variances associated with these results.\n\n$A5:$ Thanks to the reviewer's suggestion, we have reported the results of experiments conducted in at least three repetitions, with multiple runs for each repetition. We have presented the mean and variance of the final performance for each task in Table 1. Additionally, we employed the statistical methods described in [.1] and further analyzed the experimental results from Table 1 in Figure 3. The results demonstrate that SERA significantly enhances the performance of CQL and Cal-QL (higher median, IQM, and mean scores are better).\n\n[.1] Rishabh Agarwal, et.al. Deep reinforcement learning at the edge of the statistical precipice, 2022\n\n[.2] Yicheng Luo, et.al. Finetuning from offline reinforcement learning: Challenges, trade-offs and practical solutions, 2023."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2188/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700370071993,
                "cdate": 1700370071993,
                "tmdate": 1700407403508,
                "mdate": 1700407403508,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FQsnDPaeoG",
                "forum": "91DFSjAva8",
                "replyto": "GqivgjDDUt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2188/Reviewer_pDE4"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2188/Reviewer_pDE4"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your detailed feedback and improvement to the paper. I believe most of my concerns are addressed and I have raised my score to 6."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2188/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700444504126,
                "cdate": 1700444504126,
                "tmdate": 1700444504126,
                "mdate": 1700444504126,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]