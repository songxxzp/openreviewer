[
    {
        "title": "Efficient Heterogeneous Meta-Learning via Channel Shuffling Modulation"
    },
    {
        "review": {
            "id": "rrF7YdzbYe",
            "forum": "QiJuMJl0QS",
            "replyto": "QiJuMJl0QS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8171/Reviewer_KAtC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8171/Reviewer_KAtC"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses the issue of meta-learning methods assuming a uniform task distribution, which hinders their performance on multi-modal task distributions. To address this, the authors introduce a new strategy for heterogeneous meta-learning that modulates the routing between convolution channels in neural networks. This strategy, called the Gumbel-Benes layer, offers a more efficient parameter complexity compared to existing methods and shows improved performance in terms of accuracy and runtime on multi-modal meta-learning benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "**Originality**: The paper innovatively tackles the homogeneous task distribution assumption in meta-learning by introducing a modulation mechanism between convolution channels, showcasing a fresh approach to the problem. \n\n**Quality**: The proposed Gumbel-Benes layer is robustly validated on multiple multi-modal meta-learning benchmarks, evidencing its effectiveness and superiority over existing methods.\n\n**Clarity**: The presentation of the novel strategy and its underlying concepts is coherent, making the methodology and results easily understandable.\n\n**Significance**: By addressing the limitations of previous meta-learning methods and offering a more efficient parameter complexity solution, this work holds substantial potential to influence future research and applications in the field of meta-learning."
                },
                "weaknesses": {
                    "value": "Overall, the paper exhibits commendable novelty, particularly in utilizing meta-routing to learn task-specific activations for diverse tasks. However, I have several concerns related to the experiments:\n\n1. What is the impact of the lambda in front of the prototype loss on the model's performance?\n2. Did the authors experiment with adding routers only in shallow or deep layers of the network? I'm curious about the influence of routers on the model across different layers.\n3. Concerning the complexity introduced by the Router to the model, could the authors provide comparisons in terms of parameter count and training efficiency with HSML?"
                },
                "questions": {
                    "value": "1. Given that $\\phi(D_1)$  produces the prototype for the entire task rather than for individual classes, I'm curious: is $L_{PROTO}$'s loss computed for each class's prototype? \n\n2. A more in-depth theoretical analysis would undoubtedly solidify the paper further and provide a more comprehensive view.\n\n3. It would greatly benefit readers if the authors could include an algorithm in the appendix, offering clearer insight into the paper's approach."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8171/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698702908259,
            "cdate": 1698702908259,
            "tmdate": 1699637013390,
            "mdate": 1699637013390,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "c95c7UbnJ3",
                "forum": "QiJuMJl0QS",
                "replyto": "rrF7YdzbYe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8171/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8171/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your comments"
                    },
                    "comment": {
                        "value": "1. We provide below the results of varying the $\\lambda$ parameter in the MRN-GB/MRN-GS objective function. We used the 1-shot 5-way Omniglot dataset and 200 training epochs for each experiment (Note that in the full experiment in our manuscript we used 2000 training epochs, hence the accuracy values reported below are somewhat lower).\n\n| $\\lambda$         | 0.2   | 0.5   | 1.0       | 2.0       | 5.0   |\n|-------------------|-------|-------|-----------|-----------|-------|\n| MRN-GB (Omniglot) | 0.827 | 0.829 | **0.888** | 0.861     | 0.840 |\n| MRN-GS (Omniglot) | 0.843 | 0.848 | 0.861     | **0.885** | 0.867 |\n\nThe above result seems to suggest that the parameter $\\lambda$ cannot be too large, causing the loss function to behave similarly to the ProtoNet loss; or too small, causing the task embedding module to capture unmeaningful patterns.\n\n2.  In all of our experiments, we used a base architecture with 4 convolution blocks. For our methods, we added one routing layer after every block, hence the routers are distributed at various depths of the network. The influence of the routers at each depth is an interesting experiment to conduct. We have provided some preliminary results below (also on the 5-way 1-shot Omniglot dataset), in which we compared the performance of our methods using all 4 routers vs. the performance of using only one router at different depths (e.g., larger depth means being closer to the output layer). \n\n| Router Depth      | 1     | 2     | 3     | 4     | All   |\n|-------------------|-------|-------|-------|-------|-------|\n| MRN-GB (Omniglot) | 0.873 | 0.875 | 0.869 | 0.747 | 0.888 |\n| MRN-GS (Omniglot) | 0.851 | 0.867 | 0.835 | 0.805 | 0.861 |\n\nOur result suggests that having the routing layer placed earlier on in the architecture is the most effective practice (e.g., putting a routing layer after the second convolution block gives the best performance). However, combining many routing layers might improve performance, such as in the case of MRN-GB. We will investigate this behavior further in future work.\n\n3. Parameter count and training efficiency comparison have been provided in Table 3 of our manuscript.\n\n4. The ProtoNet loss ($\\mathcal{L}_{proto}$) is computed for each class in a meta task. The embedding vector phi is the **average** of the ProtoNet embeddings for all classes in the task. This is a well-known strategy to compute task embedding. Thank you for pointing it out, we will add an algorithm box to clarify this in our revision."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8171/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700443371385,
                "cdate": 1700443371385,
                "tmdate": 1700443371385,
                "mdate": 1700443371385,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OTR9AwGMXi",
            "forum": "QiJuMJl0QS",
            "replyto": "QiJuMJl0QS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8171/Reviewer_d54Q"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8171/Reviewer_d54Q"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a new meta-learning approach for heterogeneous sets of tasks. The method is presented as a permutation learning problem in which convolution channels are shuffled via a continuous switch network. This trainable routing network is parameterized by input embeddings learned via the prototypical loss (Snell et al., 2017), in which the centroids encode the prototype shuffling parameters corresponding to each task. The method is favorable compared to using Gumbell-Sinkhorn layers (Mena et al., 2018) which use routing embedding dimensions quadratic in the number of channels, hence requiring larger networks with more parameters and longer inference times. The proposed method is also empirically compared to a few other baseline algorithms that do not account for heterogeneity, matching SOTA performance in single task regime and outperforming alternatives in the heterogeneous task setup."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Originality and significance: The presented method shows clear improvements over the studied baselines by showing how a task-based ordering can be derived from a smaller dimensional embeddings: $C \\log C$ v.s. $C^2$. The empirical analysis uses both problems with synthetically generated tasks (JIGSAW), as well as problems formulated by grafting multiple tasks. The results show the effectiveness in heterogeneous meta-learing domains.\n\nQuality and clarity: The paper is easy to follow and the experiments are explained in sufficient detail."
                },
                "weaknesses": {
                    "value": "The paper is not self-contained and requires prior knowledge of prototypical networks and Gumbel-Sinkhorn layers. A reader not familiar with both topics will have to consult the references for basic definitions of key components of the proposed method (namely $L_{proto}$ and  $GS(.)$). A minimal overview of these should have been included in the main body of the paper, or at least in the appendix."
                },
                "questions": {
                    "value": "Even though the method reduces the nominal parameter complexity of the required routing network, it might still be over parametrizing the space by a log factor. Specifically, in order to define an ordering of $C$ channels one should require no more than $C$ values (each with at least $\\log_2C$ bits). I suspect that one can derive a (continuous) shuffling using embeddings of size $C$ instead of $C \\log C$ by using a sorting network (e.g. Bitonic sort) in a similar fashion as the Benes network is used in this work, but with a different switch type: $(U_j, channel_j)$ pairs are sorted by the key values and deltas between key values (passed a sigmoid) can form continuous switches. Can such a mechanism work in the use cases presented by the paper, and how would it compare to using the MRM-GB in terms of parametrization and evaluation speed?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8171/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698734221968,
            "cdate": 1698734221968,
            "tmdate": 1699637013269,
            "mdate": 1699637013269,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gALHIqb6kM",
                "forum": "QiJuMJl0QS",
                "replyto": "OTR9AwGMXi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8171/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8171/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your comments"
                    },
                    "comment": {
                        "value": "1. We will provide an overview of related concepts in the revision of our paper.\n\n2. Thank you for the interesting suggestion regarding the use of sorting networks. It is true that other sorting networks can be used in place of the Benes routing network for our strategy. This approach would be very similar in spirit to our method. Whereas our Gumbel Benes layer is conditioned on the task embedding vector, a sorting layer would theoretically derive contextual information from learning a value function that scores the convolution channels. Furthermore, a bitonic sorting network will also require $C\\log^2 C$ comparators, so we would expect this mechanism to have, at best, similar complexity to our method."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8171/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700442517778,
                "cdate": 1700442517778,
                "tmdate": 1700443416833,
                "mdate": 1700443416833,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "T8KLpScdEq",
                "forum": "QiJuMJl0QS",
                "replyto": "gALHIqb6kM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8171/Reviewer_d54Q"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8171/Reviewer_d54Q"
                ],
                "content": {
                    "title": {
                        "value": "Updates"
                    },
                    "comment": {
                        "value": "1. IIUC no revision was uploaded for this submission that adds missing parts mentioned in my review.\n2. Note that even though the complexity of the sorting network is the same as Benes, its parameter count is smaller by a log factor. Hence the model that generates the routing parameters can have lower output dimension and hence lower complexity."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8171/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700521901660,
                "cdate": 1700521901660,
                "tmdate": 1700521901660,
                "mdate": 1700521901660,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Lxf7sMFduW",
            "forum": "QiJuMJl0QS",
            "replyto": "QiJuMJl0QS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8171/Reviewer_aRVA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8171/Reviewer_aRVA"
            ],
            "content": {
                "summary": {
                    "value": "**Edit: I have raised my score to 6 to reflect the author's updates to the submission.**\n\n\nThe paper considers the problem of meta-learning across heterogeneous tasks when the model is expected to extract and generalize the meta-knowledge and transfer it to quickly learn the novel tasks. Whereas, many meta-learning models focus on the tasks coming from the same distribution (homogeneous), the authors consider the directly the tasks coming from a multi-modal task distribution (heterogeneous setting). Recently, many appeared many methods that tackle the heterogeneous meta-learning, but they suffer from a significant increase in parameter complexity. As an alternative approach, the authors propose a novel strategy which incorporates modulating the routing between convolution channels in the network. The paper introduce a novel neural neural permutation layer based on the classical Benes routing network. Finally, the proposed method is compared against various meta-learning benchmarks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper has a few significant strengths overall, which I will outline below:\n1. Considering directly the heterogeneous tasks setting which I found especially significant within the Meta-Learning.\n2. The main idea of introducing the Gumbel-Benes routing layer is very interesting and seems to be novel in Meta-Learning.\n3. Leveraging the complexity of permutation layer inspired by the Benes network is fair.\n4. I like experiment presented in Figure 3.\n5. The presentation is clear. Overall, the flow of the manuscript is well-organized."
                },
                "weaknesses": {
                    "value": "However, despite the strengths, the paper has a few major and minor weaknesses. I will focus on the experiments section especially, because I recognize it as insufficient: \n\n1. The results of all methods, presented in the Table 1 and Table 2, are usually within their standard deviations. Because of that, I will not support the claim that the presented method is significantly better than others.\n2. The methods that are chosen for the comparison are not a current state-of-the-art methods, having a few years. I strongly encourage authors to include comparison with other methods like [1] or [2].\n3. The comparison are considered only across a single neural network architecture. I admit that the scenarios of using the Gumbel-Benes routing layers are limited into CNN architectures. However, there is not the reason to skip the comparison on highly popular in Meta-Learning, ResNet architectures. I will suggest for sure comparison on at least ResNet-10 (as in, e.g., [3]).\n4. The lack of more challenging 1-shot 5-way classification on Mini-Imagenet dataset. \n5. As far as I know, there is the first example of the Benes networks-inspired Meta-Learning approach. However, there were others works based on the Benes routing, e.g, [4]. I would like to see the Related Work section given credits to that works and comparing the proposed method with them.\n\n**References:**\n\n[1] Sun, Z., Wu, J., Li, X., Yang, W., and Xue, J.-H. Amortized bayesian prototype meta-learning: A new probabilistic meta-learning approach to few-shot image classification. In International Conference on Artificial Intelligence and Statistics, pp. 1414\u20131422. PMLR, 2021.\n\n[2] Ye, H.-J., Hu, H., Zhan, D.-C., Sha, F.: Few-shot learning via embedding adaptation with set-to-set functions. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 8808\u20138817 (2020)\n\n[3] Patacchiola, M., Turner, J., Crowley, E. J., O\u2019Boyle, M., and Storkey, A. J. Bayesian meta-learning for the few-shot setting via deep kernels. Advances in Neural Information Processing Systems, 33, 2020.\n\n[4] Freivalds, K., Ozoli\u0146\u0161, E., & \u0160ostaks, A. (2019). Neural shuffle-exchange networks-sequence processing in o (n log n) time. Advances in Neural Information Processing Systems, 32."
                },
                "questions": {
                    "value": "I would like to see especially the following experiments and improvements:\n1. Provide results for the 1-shot 5-way classification setting on Mini-Imagenet.\n2. Please, compare with state-of-the-art methods as suggested in Weaknesses section. \n3. The work will be better if you could test the Gumbel-Benes layers in ResNets also.\n4. Add the Related Work section regarding other deep learning works utilizing the Benes networks/routing algorithm.\n\n**Questions:**\n\n1. Another place where the Gumbel-Benes routing layers might be beneficial are standard methods working on the Meta-Dataset [1], which is a created really heterogeneous tasks dataset. The best methods working on Meta-Dataset are often ResNets with some adapting layers within, so they are really within the scope of the Gumbel-Benes layers. For the example methods, please see the rank tables in [2].\n\n\n**References:**\n\n[1] Triantafillou, E., Zhu, T., Dumoulin, V., Lamblin, P., Evci, U., Xu, K., ... & Larochelle, H. (2019). Meta-dataset: A dataset of datasets for learning to learn from few examples. arXiv preprint arXiv:1903.03096.\n\n[2] https://github.com/google-research/meta-dataset"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8171/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8171/Reviewer_aRVA",
                        "ICLR.cc/2024/Conference/Submission8171/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8171/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698771925053,
            "cdate": 1698771925053,
            "tmdate": 1700665953899,
            "mdate": 1700665953899,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8Pyq608b1B",
                "forum": "QiJuMJl0QS",
                "replyto": "Lxf7sMFduW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8171/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8171/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your comments"
                    },
                    "comment": {
                        "value": "1. Regarding significance, even though the margin of improvement is not as large as you would prefer, we would like to highlight that our contribution also includes a parameter efficient meta learning formulation, as demonstrated in Table 3.\n\n2. Following your suggestion, we have conducted follow up experiments on the more difficult 1-shot, 5-way Mini-ImageNet setting and its Jigsaw counterpart. We adopt similar training settings to the 5-shot, 5-way experiments in the main manuscript. That is, we trained every baseline for 2000 epochs, sampling 32 training tasks of 16 images per epoch (1-shot, 15-queries). For testing, we sampled 5 test tasks with 1 training and 15 test images per task. Due to the limited rebuttal time, we only focus on comparing with other heterogeneous Meta learning baselines (i.e., MMAML, HSMAML, URL). We have also provided the result using ResNet-18 on these scenarios to answer your other question. The results of these experiments are detailed in the Table below:\n\n| Dataset/Method                   | MRN-GB    | MRN-GS | MMAML | HSML      | URL   |\n|----------------------------------|-----------|--------|-------|-----------|-------|\n| Mini-ImageNet                    | **0.504** | 0.496  | 0.432 | 0.472     | 0.496 |\n| Jigsaw Mini-ImageNet             | **0.480** | 0.464  | 0.432 | 0.408     | 0.448 |\n| Mini-ImageNet (ResNet-18)        | 0.547     | 0.526  | 0.541 | **0.555** | 0.470 |\n| Jigsaw Mini-ImageNet (ResNet-18) | **0.525** | 0.299  | 0.490 | 0.448     | 0.501 |\n\n3. We have compared our method to a lot of benchmarks in the manuscript, one of which (i.e. the URL method) was published very recently in 2022. Due to the limited rebuttal time, we may not be able to provide results to the suggested methods. In particular, we couldn\u2019t find an implementation of reference [1]. On the other hand, FEAT (reference [2]) adopts a Transformer architecture that does not immediately fit with our permutation framework, thus it would require significant changes to provide an apples-to-apples comparison. That being said, we will try our best to provide such results in our future revision.\n\n4. Thank you for your suggestion. We will provide a discussion of other DL works that use the Benes Network in our revision. We however hope you agree that our work is the first that uses this concept in a meta-learning context."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8171/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700442457330,
                "cdate": 1700442457330,
                "tmdate": 1700442457330,
                "mdate": 1700442457330,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7rxGgbdzxm",
                "forum": "QiJuMJl0QS",
                "replyto": "8Pyq608b1B",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8171/Reviewer_aRVA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8171/Reviewer_aRVA"
                ],
                "content": {
                    "comment": {
                        "value": "I want to thank the authors for their response and including additional results. In the following paragraphs, I will go through the points from the authors response to my review.\n\n\n1. Thank for this clarification. However, I have two concerns regarding this point: \n    \n\n- The presented results don\u2019t compare with the methods having better results - e.g., in response to my review, the authors provide the comparison on mini-ImageNet 5-way 1-shot task, achieving 50.4% of accuracy while even in [1] (from my review, see Table 1), we have at least 5 methods achieving better accuracy (using the same backbone).\n\n\n- Regarding the results from Table 3 - I agree that the authors need smaller number of parameters and get lower runtime than other presented methods. However, my main concern is that the comparison is unfair. The authors proposed to use 4-layers CNN networks and increase the number of channels, which is strongly favorable for their method. However, the most common setting for, e.g., MAML, is to use different backbone - usually some deeper ResNet to get the better results. Because of that, the usual runtimes are not that big as provided (since other methods rather not use such architectures). From my perspective, the comparison of the methods in Table 3 is provided on artificial task, being far from standard setting.\n\n\n2. Thank for providing the additional results. I assume that including them into manuscript would make the work better. Regarding results, please look my comment above.\n\n\n3. I will be looking forward to see additional baselines in the revised version of the manuscript, as promised by the authors.\n\n\n4. Thank you for promising of adding the discussion in the next version of the manuscript. I agree with the authors that, as far as I know, there is the first usage of this concept in the context of meta-learning.\n\n\nOverall, I think that the authors are solving unrealistic setting - i.e., their experiments on heterogeneous meta-learning tasks are usually not a problem for many meta-learning approaches (e.g., the methods compared on Meta-Dataset). I\u2019m also concern with the lack of comparison with the methods achieving better accuracy - you can see, e.g., methods presented in Tables as baselines in [1] (from my review).\n\n\nFinally, I thank the authors for the response and including the additional results. I agree that the proposed method is interesting. However, I think that my concerns were not addressed properly and lack of including other baselines might be confusing for many readers coming from the few-shot learning community and knowing the typical baselines results on the presented tasks. Unfortunately, in this situation, I just cannot raise my score."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8171/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700501466565,
                "cdate": 1700501466565,
                "tmdate": 1700501466565,
                "mdate": 1700501466565,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KgxyEjkKN2",
                "forum": "QiJuMJl0QS",
                "replyto": "Lxf7sMFduW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8171/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8171/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Extra results of our methods trained for 12000 epochs to compare with Table 1 of Reference [1]"
                    },
                    "comment": {
                        "value": "Dear reviewer,\n\nWe have re-run our method for 12000 training epochs. The table below presents our results side by side with the results quoted from Table 1 of Ref [1].\n\n| Method                                      | MRN-GB (CNN) | MRN-GS (CNN) | BMAML | PLATIPUS | ABML | Amortized VI | VERSA | Meta Mixture | VAMPIRE | DKT   | ABPML (Ref [1]) |\n|---------------------------------------------|--------------|--------------|-------|----------|-------|--------------|-------|--------------|---------|-------|-----------------|\n| Mini-ImageNet (1-shot, 5-way, 12000 epochs) | **0.606**       | 0.591        | 0.538 | 0.501    | 0.450 | 0.441        | 0.534 | 0.512        | 0.515   | 0.497 | 0.533           |\n\nAs you can see, given similar training budget our method is capable of achieving much better performance than the quoted results. We hope this would give you sufficient evidence to support our method's potential for now. Given time, we will thoroughly compare with these previous methods on the exact same setting (train/test split, learning rate, layer size, etc.)"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8171/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700580484892,
                "cdate": 1700580484892,
                "tmdate": 1700580497350,
                "mdate": 1700580497350,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TUYIZwQDxK",
                "forum": "QiJuMJl0QS",
                "replyto": "KgxyEjkKN2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8171/Reviewer_aRVA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8171/Reviewer_aRVA"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to thank the authors for their response for my concerns. In the following paragraphs, I will go through all the issues mentioned by authors in the last two responses.\n\n\n1. Many thanks for adding the number of the training epochs between the methods and for the rerunning their code also for the full 12k epochs. I will go to the achieved results later in the response.\n\n\n2. Regarding the ResNet and Conv-4 backbones. I totally agree that they are both used in the few-shot learning papers. However, I just want to mention that my comment regarding the used backbone in Table 3 was not intended to criticize the Conv-4 backbone in the few-shot learning settings, but was to emphasize the artificially of this comparison. If it was not crystal clear, I will add explanation. I think that the Table 3 looks good up to the 64 number of channels. Why I think that? Since in the Meta-Learning architecture the most popular backbone is the Conv-4 with the 64 channels in each layer. This particular backbone with larger number of channels per layer is being seldom used just because it doesn\u2019t scale well (what we can see in the last two rows of Table 3). In practice, for the harder tasks, demanding the more powerful backbone, the few-shot methods usually go for the ResNets. Now, my concerns about the Table 3 was the following. For a reader of this paper, the results now presented in Table 3 might suggest that for harder tasks, the MAML-based methods just cannot be used in practice, because they barely fit in the GPU. Whereas, in practice, the different backbone (popular for more demanding tasks) might be successfully used with different backbone. I want also to apologize the authors if my concerns were not crystal clear at the beginning and I hope that they are better understandable now.\n\n\n3. I agree with the authors that the presented datasets were previously known in the few-shot learning field. My concern was that comparing to the mentioned, e.g., Meta-Dataset or Meta-Dataset+VTAB (which also contains most of the datasets from the paper) the experimental setting seems to be reasonably easier. That\u2019s why I would prefer experiments on the mentioned datasets. Moreover, for the future, I strongly encourage the authors to try their method + e.g., Transformer/ResNet backbone on Meta-Dataset, because I suppose that it might be interesting and possibly beneficial for the field.\n\n\n4. Regarding lack of baselines, I\u2019ve just believe that presenting the baselines that are significantly worse than the current SOTA, might provide some misunderstanding for the readers. That\u2019s why I mentioned just a few stronger baselines to show the proposed method in wider context.\n\n\n5. I would like to congratulate the authors on the outstanding improvement in results presenting on miniImageNet 1-shot 5-ways task. I hope that in the revised version of the manuscript, the authors will provide the reruned experiments to show what are the real practical limits of their method. Just for the quick test could the authors check their method rerun for 12k epochs for the Omniglot task from Table 1 to show if the method can do significantly better than MAML (I\u2019ve selected the easiest dataset because running time of the discussion)? Regarding the presented new results on the miniImageNet and last response of the authors, I am still a little bit concerned about the fairness of comparison with other methods. In the last response, the authors wrote that: \"Given time, we will thoroughly compare with these previous methods on the exact same setting (train/test split, learning rate, layer size, etc.)\", which I don\u2019t know how to exactly understand. I want to ask what backbone architecture was used in this particular setting? I assume it was CNN but could the authors explain how many layers and channels were used there? I\u2019m curious what the \"exact same setting\" means here since all of these methods were compared on exactly same setting - e.g., exactly same CNN backbone. I hope that the authors could explain it to me and dispel my last concerns."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8171/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700621066672,
                "cdate": 1700621066672,
                "tmdate": 1700621066672,
                "mdate": 1700621066672,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VDSglsnN03",
                "forum": "QiJuMJl0QS",
                "replyto": "Lxf7sMFduW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8171/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8171/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response"
                    },
                    "comment": {
                        "value": "First of all, thank you for clarifying your points. We do understand your concerns better now, and appreciate your constructive feedback. To add some discussion:\n\nWe acknowledge that ConvNet layers with more than 64 channels are not as practical in practice, and there are better architectures to try our method on. Some, such as ResNet, are immediately compatible as shown in the rebuttal experiments and achieve reasonably good performance. We will heed your advice and experiment with Transformer/ViT architectures in the future, possibly by permuting the multi-head structure of transformer. We would also like to highlight that the heterogeneous meta-learning baselines that adopt the weight modulation strategy will become more impractical with increasing model size, regardless of the architecture type. In our manuscript we chose to increase the number of channels to achieve this effect. Although it was not meant to favor our method, we do understand how confusion can arise.\n\nRegarding the rerun experiments on 12k epochs, the backbone that we used is the CNN architecture with 4 conv blocks and 4 routing layers as described in the paper. Each conv block has a 64 channel conv layer, followed by BatchNorm, ReLU and MaxPool, which is exactly similar to [1]. What we meant in our previous response was that we couldn't find their implementation, so we cannot reproduce exactly their learning rate, train/test split, weight initialization (our apology, we meant to say layer weight, not layer size). Reference [1] also did not describe how many test images are there in one test task, and how many query images are there in one training task. In our experiments, each task consists of 1-shot/15-query or 1-shot/15-test. Due to limited time, we also did not go into each method listed in Table 1 of reference [1] to check their experimental setting to make sure our results are as fair as possible. \n\nFinally, we will try our best to provide the accuracy of our methods on the Omniglot dataset with 12k epochs in the remaining time of the rebuttal."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8171/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700630831331,
                "cdate": 1700630831331,
                "tmdate": 1700630883511,
                "mdate": 1700630883511,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0MJAxpMwfa",
                "forum": "QiJuMJl0QS",
                "replyto": "VDSglsnN03",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8171/Reviewer_aRVA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8171/Reviewer_aRVA"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors! \n\n\nThank you for your response and better understanding to my concerns. I strongly encourage you to include my suggestions stated in the previous responses in the revised version of the manuscript. I suppose that including parts of our discussion (regarding backbone architectures) as an ablation study might be beneficial for the paper. \n\n\nIn this situation, I would like to increase my score to 6."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8171/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700664904711,
                "cdate": 1700664904711,
                "tmdate": 1700664904711,
                "mdate": 1700664904711,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WiElYPockq",
                "forum": "QiJuMJl0QS",
                "replyto": "k2NeQ5bkIW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8171/Reviewer_aRVA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8171/Reviewer_aRVA"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to thank the authors for adding the additional results in the very limited time before the end of discussion. It's good to see that the proposed method achieves better results than presented in the first version of the paper. I suggested this experiment on the easy task just to see if there might be any consistency between additional training time and achieving better results than the ones presented at the beginning.\nOnce again, I hope to see in the final version the additional experiments and ablation studies that were suggested during our discussion.\n\nBest regards,\nReviewer aRVA"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8171/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700706765011,
                "cdate": 1700706765011,
                "tmdate": 1700706765011,
                "mdate": 1700706765011,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7u0JYrSaVL",
            "forum": "QiJuMJl0QS",
            "replyto": "QiJuMJl0QS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8171/Reviewer_ppvH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8171/Reviewer_ppvH"
            ],
            "content": {
                "summary": {
                    "value": "This paper deals with the heterogeneous meta-learning problem from the perspective of channel shuffling modulation.   \nInstead of the conventional fixed backbone + modulation layer scheme for multiple task distributions, this paper seeks to find a task-specific channel permutation routing mechanism.   \n\nThe idea is motivated by ShuffleNet and implemented with a task-specific prototypical vector to learn the shuffling operation. For the permutation matrix, this paper first adopts the Gumbel-Sinkhorn layer to generate the permutation. Then, the classical Benes\u02c7 routing network is utilized to improve the efficiency. \n\nExperiments are conducted on several heterogeneous datasets and compared with various MAML-based methods. As a parameter-efficient method, the proposed method also performed generally better than baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- (1) This paper provides a different perspective for heterogeneous meta-learning, i.e., channel shuffling by permutation learning. It can be a good alternative to the modulation-based methods. \n- (2) The utilization of the traditional Bene\u02c7s network can improve the efficiency of the proposed method. \n- (3) Experiments are conducted on the meta-learning benchmark, and the results are fair and generally better than other MAML-variant methods."
                },
                "weaknesses": {
                    "value": "- (1) The authors describe the heterogeneous meta-learning problem from the MAML perspective. However, similar to heterogeneous meta-learning, researchers also used the terminology multi-domain/cross-domain to describe the diverse task distribution when dealing with few-shot meta-learning problems, e.g., [R1]. Besides Li et al. (2022), other related works applying feature modulation can also be discussed, such as [R2, R3]. \n- (2) I am wondering why permutation and channel shuffling work compared with modulation. The intuition of modulation assumes a strong shared backbone, and each dataset/distribution performs task-specific adjustments for its data. For the permutation case, it is better to visualize what permutation can be learned for different distributions. \n\n\n[R1] Triantafillou, E., Zhu, T., Dumoulin, V., Lamblin, P., Evci, U., Xu, K., Goroshin, R., Gelada, C., Swersky, K., Manzagol, P.A. and Larochelle, H., 2019. Meta-dataset: A dataset of datasets for learning to learn from a few examples. arXiv preprint arXiv:1903.03096.\n\n[R2] Triantafillou, E., Larochelle, H., Zemel, R., & Dumoulin, V. (2021, July). Learning a universal template for few-shot dataset generalization. In International Conference on Machine Learning (pp. 10424-10433). PMLR.\n\n[R3] Liu, Y., Lee, J., Zhu, L., Chen, L., Shi, H., & Yang, Y. (2021). A multi-mode modulator for multi-domain few-shot classification. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 8453-8462)."
                },
                "questions": {
                    "value": "Please see above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8171/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699098017644,
            "cdate": 1699098017644,
            "tmdate": 1699637013015,
            "mdate": 1699637013015,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0GgYbl86Y3",
                "forum": "QiJuMJl0QS",
                "replyto": "7u0JYrSaVL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8171/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8171/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your comments"
                    },
                    "comment": {
                        "value": "1. Thank you for your suggestion; we will provide a discussion of the suggested related works in our revision.\n\n2. We strongly agree that the intuition of modulation assumes a strong shared backbone, and each dataset/distribution performs task-specific adjustments for its data. However, the degree of adjustments will depend on the heterogeneity of the task distribution. Additive modulations might suffice for task distributions consisting of closely related tasks. On the other hand, more challenging distribution of tasks tend to require more significant modifications, as explored in previous studies of heterogeneous meta learning via modulation. Our hypothesis is that structural modulations, such as a shuffling of the embedded information channels, would be more effective at modulating between tasks of different modalities. In fact, this is not an arbitrary design choice as it is also supported by previous empirical evidence. For example, random shuffling has been used in the context of compact neural architecture due to its efficiency in encoding information. \n\n3.  It would be possible to visualize the learned permutations. However, we would like to note that the identity of the permutations themselves are not quite important. We believe that it is more critical to show that the network can learn to assign similar tasks to similar permutations, and different tasks to different permutations, and thus improve knowledge organization. To show this, we have provided the t-SNE plots of the learned Benes switch configurations for 1000 tasks in two different scenarios (see our Appendix A), which show distinct clusters that correspond to the task identities."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8171/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700441425470,
                "cdate": 1700441425470,
                "tmdate": 1700441425470,
                "mdate": 1700441425470,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3Dm4nw7MTL",
                "forum": "QiJuMJl0QS",
                "replyto": "0GgYbl86Y3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8171/Reviewer_ppvH"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8171/Reviewer_ppvH"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response. My concerns are addressed. \nI will keep my Rating as 6."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8171/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700707283472,
                "cdate": 1700707283472,
                "tmdate": 1700707283472,
                "mdate": 1700707283472,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]