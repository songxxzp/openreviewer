[
    {
        "title": "A New, Physics-Based Continuous-Time Reinforcement Learning Algorithm with Performance Guarantees"
    },
    {
        "review": {
            "id": "OaL6JtGF0N",
            "forum": "Cdng6X2Joq",
            "replyto": "Cdng6X2Joq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8465/Reviewer_SQkk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8465/Reviewer_SQkk"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a new continuous-time reinforcement learning (CTRL) algorithm for control of affine nonlinear systems. The key idea is to use reference command input (RCI) as probing noise in learning. The simulations show RCI leads to better results than fitted value iteration."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper has a good review of the existing ADP methods."
                },
                "weaknesses": {
                    "value": "1. The methodology introduced in this paper is an extension of the RADP method, with the primary modification being the linearization of the nonlinear system. However, the implications of such linearization are not distinctly outlined, nor is there a clear comparative analysis with the traditional RADP method. The absence of a detailed examination of the linearization's impact raises questions about the method's efficacy and novelty.\n\n2. The authors suggest that the rationale behind employing the RCI framework is its potential to enhance the PE condition. Nevertheless, the explanation as to why this approach is effective is insufficiently substantiated. Furthermore, the connection between the RCI and the employed linearization technique is ambiguous, resulting in a fragmented logical flow in the methodology's presentation.\n\n3. The proposed methodology presupposes a comprehensive understanding of system dynamics. However, with known system dynamics, one could conduct policy iteration directly using a \"differential\" formulation as opposed to the \"integral\" formulation, which seems unnecessarily convoluted. For instance, a comparison could be made with the \"Relaxed Actor-Critic\" method detailed in [1], which offers a solution to the HJB equation through policy iteration in the context of fully understood system dynamics.\n\nReference: [1] J. Duan et al., \"Relaxed Actor-Critic With Convergence Guarantees for Continuous-Time Optimal Control of Nonlinear Systems,\" in IEEE Transactions on Intelligent Vehicles, vol. 8, no. 5, pp. 3299-3311, May 2023, doi: 10.1109/TIV.2023.3255264.\n\n4. Unfortunately, the link provided for the open-source code corresponding to the paper's methodology is inaccessible, which hinders peer verification and replicability of the results presented.\n\n5. The proof presented for Theorem 2.1 is unconvincing. It employs the Closed-Loop Stability attribute of Kleinman\u2019s Algorithm, but the narrative fails to clarify why this particular inference is applicable to nonlinear systems as well. The proof lacks a thorough explanation, making the applicability of Kleinman\u2019s Algorithm to nonlinear systems questionable."
                },
                "questions": {
                    "value": "Why use the proposed method using linearization? What is the intuition behind it?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8465/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8465/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8465/Reviewer_SQkk"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8465/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698168206852,
            "cdate": 1698168206852,
            "tmdate": 1700499034478,
            "mdate": 1700499034478,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oIqRQeRyg2",
                "forum": "Cdng6X2Joq",
                "replyto": "OaL6JtGF0N",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8465/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8465/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer"
                    },
                    "comment": {
                        "value": "This Reviewer's claims are addressed in the General Points of our rebuttal, and in our responses to the other two Reviewers. Please review."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8465/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700449969029,
                "cdate": 1700449969029,
                "tmdate": 1700449969029,
                "mdate": 1700449969029,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sw9J3lfXTi",
                "forum": "Cdng6X2Joq",
                "replyto": "OaL6JtGF0N",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8465/Reviewer_SQkk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8465/Reviewer_SQkk"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for you response"
                    },
                    "comment": {
                        "value": "Thank you very much for the explanation! It helps me a lot to understand the paper in depth.\n\nI can see my comments are similar to those of Reviewer QT7M.\n\nHowever, I still find it confusing and struggle to accept the claimed contributions, because compared to methods like RADP and Integral RL, the approach in this paper requires knowledge of system dynamics. From my understanding, when system dynamics are known, we can directly solve ADP using differential forms, utilizing the system's derivative information. Additional derivative information can make differential-form ADP more stable and can be directly solved using neural networks. The authors did not discuss this aspect, nor did they provide a performance comparison. I still believe that the ADP method with known system dynamics needs at least a comparison with other methods that also use known system dynamics, otherwise the performance comparison is not fair. I can accept that your method might be inferior to differential-form results, but I would like to see some discussion and experimental comparisons.\n\nAdditionally, from a theoretical perspective, I still find the proof of algorithmic convergence provided by the authors to be tricky, at least not convincing enough for me.\n\nI see the effort put into this work and the authors' code is very clean, which is commendable. However, I still hope the authors will carefully consider the issues I've mentioned. Therefore, I do not recommend acceptance for publication this time."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8465/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700661979088,
                "cdate": 1700661979088,
                "tmdate": 1700662015257,
                "mdate": 1700662015257,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OYIUXeanYr",
                "forum": "Cdng6X2Joq",
                "replyto": "rUmwX2NLDT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8465/Reviewer_SQkk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8465/Reviewer_SQkk"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for the author's prompt reply. This is my last sincere attempt to impart knowledge of ADP to the author.\n\u00a0\n\nFirstly, in the context of continuous-time ADP, the solution of PEV in its differential form is just as mainstream as the integral form. If the authors are skeptical, they can refer to Chapter 9 in the textbook [1] and Section 3 in the review by Sutton [2]. For instance, in Section 3 of [2], the author divides PI into differential PI and integral PI.\u00a0\n\nI am quite familiar with the review by Wallace & Si, 2022 that you mentioned, but its focus is on integral PI. To my understanding, the biggest advantage of Integral PI is that it does not require knowledge of internal dynamics, which is repeatedly mentioned in Lewis's original paper. However, your method requires knowledge of internal dynamics, making the comparison inherently unfair.\n\u00a0\nSecondly, if you insist that only integral PI is mainstream, then I would also like you to recognize what the real SOTA algorithms in top-tier computer conferences are. Please refer to [3][4] from major ML venues on model-based CTRL that you have not mentioned.\n\u00a0\n\n[1] Liu, Derong, et al. Adaptive dynamic programming with applications in optimal control. Berlin: Springer International Publishing, 2017.\n\n[2] Lee, Jaeyoung, and Richard S. Sutton. \"Policy iterations for reinforcement learning problems in continuous time and space\u2014Fundamental theory and methods.\" Automatica 126 (2021): 109421.\n\n[3] Yildiz, Cagatay, Markus Heinonen, and Harri L\u00e4hdesm\u00e4ki. \"Continuous-time model-based reinforcement learning.\" International Conference on Machine Learning. PMLR, 2021\n\n[4] Holt, Samuel, et al. \"Neural Laplace Control for Continuous-time Delayed Systems.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2023."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8465/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700697770349,
                "cdate": 1700697770349,
                "tmdate": 1700697770349,
                "mdate": 1700697770349,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uN5rVynTyv",
            "forum": "Cdng6X2Joq",
            "replyto": "Cdng6X2Joq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8465/Reviewer_UKDb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8465/Reviewer_UKDb"
            ],
            "content": {
                "summary": {
                    "value": "This work introduces physics-based CT-RL algorithm for affine systems using reference command input.  It aims at providing theoretical guarantees while showing good performance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Careful comparisons and evaluations (if the presentations become better, those should become clearer)\n\nTheorem 2.1 could be a potential strength; but I could not quite follow the details here.\nTo be honest, it was very hard to parse the overall algorithm.\nWhy for nonlinear systems the policy K is introduced in the algorithm?  The author also mention mu as a policy.\nProposition A.1 is referred at several places but without clear connections.\nFor nonlinear systems, the results should only be satisfied locally?\nI may be missing something here, but I believe improving presentations should largely help clarifying the strength of the theoretical statements."
                },
                "weaknesses": {
                    "value": "1. From the cost 2, the system must stabilizes on a zero cost point and stays there without control input so that the cost exists: Although there is a comparison to other methods, I honestly think this is a strong assumption for practical purposes that this work claims to target.\n2. The presentation is not well structured; perhaps it is better to present a conceptual procedures first with figures, pseudo algorithm etc., and then go into the details.  The authors also use some notations and concepts and describe them later; which make it harder to track; those should be mentioned at the conceptual presentation stage.\nAlso for experimental sections, I guess it is because of page limit, it is a bit hard to parse what is going on (no indent, no new line...).\n3. More explanations around A, B (nominal linearization terms that are known) are needed.\n4. Table 2 is hard to parse.  Table 4 could be improved to show which case works better for RCI.\n5. For all of the tables (and some figures) in the appendix, they should have more descriptions in the captions and they could be improved so that it becomes easier to get the ideas."
                },
                "questions": {
                    "value": "1. I don\u2019t get what \u201cThus, RCI can improve learning of existing CT-RL algorithms\u201d mean from the paragraph.  Can you elaborate on this?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8465/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8465/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8465/Reviewer_UKDb"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8465/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698457554991,
            "cdate": 1698457554991,
            "tmdate": 1699637056474,
            "mdate": 1699637056474,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "55Lu8biaUJ",
                "forum": "Cdng6X2Joq",
                "replyto": "uN5rVynTyv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8465/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8465/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer"
                    },
                    "comment": {
                        "value": "> From the cost 2, the system must stabilizes on a zero cost point and stays there without control input so that the cost exists: Although there is a comparison to other methods, I honestly think this is a strong assumption for practical purposes that this work claims to target.\n\n* We greatly appreciate the Reviewer's effort in reading our paper.\n\n* Respectfully, your assertion that ``the system must stabilizes on a zero cost point and stays there without control input so that the cost exists\" is simply incorrect: The system need not reach the equilibrium in finite time for the cost to be finite. These sorts of considerations are addressed in any standard optimal control text; see, e.g., Frank Lewis's \"Optimal Control.\"\n\n* Q-R cost (2) is far from a \"strong assumption\" -- In control problems it is **the standard** cost structure that has been assumed in optimal control all the way back to its inception with Kalman in the 1960s. \n\n* Q-R cost is also the predominant cost structure chosen by leading ADP CT-RL algorithms; see, e.g., (Vrabie \\& Lewis, 2009), (Vamvoudakis \\& Lewis, 2010), (Jiang \\& Jiang, 2014), (Bian \\& Jiang, 2022). The deep RL FVI works (Lutter et al., 2023a), (Lutter et al., 2022) also accommodate Q-R cost.\n\n* General RL settings oftentimes can accommodate different and flexible reward functions. We emphasize that we are addressing a continuous-time RL **dynamical control** problem, not a discrete decision making/reward problem.\n\n> For nonlinear systems, the results should only be satisfied locally? I may be missing something here [...]\n\n* As stated in Theorem 2.1: The convergence, optimality, and closed-loop stability results of RCI apply to the full nonlinear system (1). \n\n* These results are not global in nature, which is **by design.** We would kindly like to remind this Reviewer that global asymptotic stability results for nonlinear systems generally require extremely stringent theoretical assumptions on both the structure of the environment dynamics and the knowledge of the dynamics. For instance, we discuss the theoretical assumptions required for the global results of the ADP CT-VI method (Bian \\& Jiang, 2022) in detail in Appendix Remark C.3 (pp. 16 of manuscript). These include:\n\n  * Existence and uniqueness of solutions to an uncountable family of finite-horizon HJB equations\n  * Properness of each solution to the finite-horizon HJB equation\n  * Convergence of family of solutions of finite-horizon HJB equation to the infinite-horizon HJB solution\n  * Invariance of closed-loop state/action trajectory to compact set with respect to the probing noise $d$\n  * Initial *globally asymptotically stabilizing* policy\n  * PE assumption on various learning signals\n  * Chosen basis functions approximate optimal value and its gradient uniformly on compact sets\n  * Chosen basis functions approximate optimal policy uniformly on compact sets\n  * Chosen basis functions approximate optimal Hamiltonian uniformly on compact sets\n  * Basis functions for critic network are linearly-independent\n  * Basis functions for actor network are linearly-independent\n  * Basis functions for Hamiltonian network are linearly-independent\n\n* We emphasize: The proposed method requires **not one** of these above theoretical assumptions.\n\n> I don\u2019t get what \"Thus, RCI can improve learning of existing CT-RL algorithms\" mean from the paragraph. Can you elaborate on this?\n\n* In regards to this question, the associated Equation (13) (pp. 4) provides a direct means to accommodate reference command inputs for standard-formulation CT-RL algorithms. Thus, the empirical issues observed for CT-RL algorithms which only insert probing noise (Wallace \\& Si, 2022) can be overcome by applying reference command input via (13)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8465/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700449894934,
                "cdate": 1700449894934,
                "tmdate": 1700449894934,
                "mdate": 1700449894934,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "v3knMobVwo",
                "forum": "Cdng6X2Joq",
                "replyto": "55Lu8biaUJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8465/Reviewer_UKDb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8465/Reviewer_UKDb"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response"
                    },
                    "comment": {
                        "value": "Thank you for the responses;\n\n1. I did not mean \"reach ... in [finite time]\" by \"stabilizing\".\n2. The cost is indeed standard if it is linear systems.  For nonlinear system that may not stabilize with certain control policy, there should be discount factor or other forms of cost.\n3. What confused me about local vs global is about the claims of Theorem 2.1.  It is partially due to the presentation, but it says about optimality of the solutions for nonlinear system although the original Kleinman talks about it for linear systems.  Can you elaborate more on this?\n\nOther parts are satisfactory to me; thank you"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8465/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700527511116,
                "cdate": 1700527511116,
                "tmdate": 1700527511116,
                "mdate": 1700527511116,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "12QuZ0heAP",
                "forum": "Cdng6X2Joq",
                "replyto": "KFspqwRZVY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8465/Reviewer_UKDb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8465/Reviewer_UKDb"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response again"
                    },
                    "comment": {
                        "value": "Thank you for your response again.\nI wanted to let you know that I have read the response.\nIt might be partially due to my understanding, but I still believe the current presentation is a bit confusing.\nI keep my score because of it, but I appreciate the authors' clarifications on the work."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8465/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700649558969,
                "cdate": 1700649558969,
                "tmdate": 1700649558969,
                "mdate": 1700649558969,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "hFVZxerVFY",
            "forum": "Cdng6X2Joq",
            "replyto": "Cdng6X2Joq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8465/Reviewer_QT7M"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8465/Reviewer_QT7M"
            ],
            "content": {
                "summary": {
                    "value": "The authors proposed a new method exclusively for solving LQR problems (restricted to Q-R cost functionals without cross terms) by leveraging input/output insights and the underlying control problem structure. This enables the proposed method to have theoretical foundation which is currently lacking in more general purpose methods including ADP and DeepRL. In several benchmark tasks, the proposed method outperforms or matches existing practice."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors specifically studied an important class of control problem, namely the affine nonlinear LQR problem, in continuous time. By leveraging the linear-quadratic property of the underlying problem structure, and utilizing Kleinman's method, the authors arrived at a theoretical guarantee unsurprisingly. The proposed method indeed outperform in tasks where underlying dynamics are known and deterministic."
                },
                "weaknesses": {
                    "value": "The study of linear-quadratic problems has formed a long list, while the manuscript only mentioned a few general-purpose methods such as ADP and FVI. The weakness of this work hence can be summarized as follows.\n\n1. This work failed to mention other similar works in continuous-time LQR setting where different exploitation of the same linear-quadratic structure (as Kleinman's method) leads to different theoretical guarantees and efficient algorithms. The authors may want to conduct a thorough survey on existing works and compare their approaches with other model-based continuous-time LQR methods. A few examples can be found like:\n\n[1] Jeongho Kim, Jaeuk Shin, and Insoon Yang. Hamilton-jacobi deep q-learning for deterministic continuous-time systems with lipschitz continuous controls. The Journal of Machine Learning Research, 22(1):9363\u20139396, 2021.\n\n[2] Haoran Wang, Thaleia Zariphopoulou, and Xun Yu Zhou. Reinforcement learning in continuous time and space: A stochastic control approach. The Journal of Machine Learning Research, 21(1): 8145\u20138178, 2020.\n\n2. It is questionable if the method in this work can be fairly compared to other general purpose RL methods or ADP methods, since the latter typically won't consider the specific underlying structure of the control problem. The authors may want to proceed more carefully when utilizing FVI as the benchmark and perform comparison for tasks like pendulum for which model-based LQR-type algorithm can easily excel."
                },
                "questions": {
                    "value": "The experiment provided in the work is only restricted to very low-dimensional control problem, i.e., pendulum. Since this work has exploited the underlying linear-quadratic structure to a great extent, it is more worth looking at the capacity and efficiency of the algorithm on high-dimensional tasks, with both state and action space in large dimensions. Otherwise, the contribution is limited."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8465/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8465/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8465/Reviewer_QT7M"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8465/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698723620906,
            "cdate": 1698723620906,
            "tmdate": 1699637056344,
            "mdate": 1699637056344,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1pBEcGXemb",
                "forum": "Cdng6X2Joq",
                "replyto": "hFVZxerVFY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8465/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8465/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer"
                    },
                    "comment": {
                        "value": "> The authors proposed a new method exclusively for solving LQR problems [...]\n\n> The authors specifically studied an important class of control problem, namely the affine nonlinear LQR problem\n\n> It is questionable if the method in this work can be fairly compared to other general purpose RL methods or ADP methods, since the latter typically won't consider the specific underlying structure of the control problem.\n\n* Thank you for reviewing our paper. We are afraid that the classification of the proposed method as an \"affine *nonlinear Linear* Quadratic Regulator problem\" is inaccurate, as there is no such thing in either RL or optimal control.\n\n* Please see our General Point 1 above for further discussion of this point.\n\n> The proposed method indeed outperform in tasks where underlying dynamics are known and deterministic.\n\n* Please see our General Point 2 above for further discussion of this point.\n\n> The authors may want to proceed more carefully when utilizing FVI as the benchmark and perform comparison for tasks like pendulum for which model-based LQR-type algorithm can easily excel.\n\n* Respectfully, we struggle to fathom how more carefully we could choose a system to benchmark FVI than to select the **identical** pendulum system chosen by the FVI authors for benchmarking in **both** the original cFVI study (Lutter et al., 2023a) and rFVI study (Lutter et al., 2022).\n\n* We explicitly discuss this point in Table 2 of the manuscript.\n\n* We would like to point out that our work studies swing-up of the pendulum, where the system nonlinearities are at their strongest -- far from chosing a comparison for which our algorithm can \"easily excel.\"\n\n> The experiment provided in the work is only restricted to very low-dimensional control problem, i.e., pendulum.\n\n* Our choice of environments is objectively SOTA in CT-RL upon comparison to the leading deep RL FVI environments in Table 2, including with respect to system order:\n\n  * **SOTA Deep RL FVIs:**\n    * 2nd order: Pendulum \n    * 4th order: Cart Pendulum\n    * 4th order: Furatura Pendulum\n\n  * **RCI:**\n    * 2nd order: Pendulum -- identical to FVIs as benchmark \n    * 4th order: Jet Aircraft (model parameters from NASA wind tunnel data)\n    * 4th order: Ground Robot (model parameters from system ID on actual hardware)\n\n> This work failed to mention other similar works in continuous-time LQR [...]\n\n* Again, we emphasize that RCI is a full nonlinear learning algorithm, not an LQR learning algorithm. Thus, the respective LQR subsections of the two works referenced do not constitute \"similar works\" to this one. \n\n* However, even the nonlinear control sections do not furnish direct comparability to the proposed method, for the following reasons:\n\n  * Regarding J. Kim, et al. (2021): This algorithm studies a discounted cost formulation, which requires an entirely different theoretical framework to prove convergence, optimality, and closed-loop stability and does not furnish direct numerical comparability to the undiscounted problem studied. Furthermore, the algorithm requires discretizing the system in order to apply its Q-learning framework. Thus, this is not a properly *continous-time* learning algorithm.\n\n  * Regarding H. Wang, et al. (2020): As is stated in the title, this algorithm requires a stochastic control formulation, which also requires an entirely separate theoretical structure and machinery than the deterministic optimal control problem studied. As with J. Kim, et al. (2021), this work also studies a discounted problem. Furthermore, this algorithm also requires applying limiting arguments to a discretized system and control problem. Thus, this is not a properly *continous-time* learning algorithm."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8465/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700449791396,
                "cdate": 1700449791396,
                "tmdate": 1700449791396,
                "mdate": 1700449791396,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yYYeNaM1Tg",
                "forum": "Cdng6X2Joq",
                "replyto": "1pBEcGXemb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8465/Reviewer_QT7M"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8465/Reviewer_QT7M"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the reply from the authors to my raised concerns and the two general points in the rebuttal. It still seems to me the equation (1) is not a fully nonlinear system; the change of state dynamics depends linearly on control u, even if both f and g can be fully nonlinear. In addition, the objective function (2) is clearly quadratic in u. This system qualifies for the well-studied linear-quadratic (in u) paradigm, for which, as I mentioned, extensive researches and methods have been proposed. In other words, the nonlinearity in x is not as challenging as the nonlinearity in u, thereby, if the authors consider solving a standard LQR problem as in (1)-(2), a more in-depth discussion of various methods may be necessary."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8465/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700455016603,
                "cdate": 1700455016603,
                "tmdate": 1700455016603,
                "mdate": 1700455016603,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WwTj1aw9Rk",
                "forum": "Cdng6X2Joq",
                "replyto": "hFVZxerVFY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8465/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8465/Authors"
                ],
                "content": {
                    "title": {
                        "value": "In short, we solving a nonlinear optimal control problem, not \"a standard LQR problem as in (1)-(2)\". Please check out any optimal control textbook."
                    },
                    "comment": {
                        "value": "> I appreciate the reply from the authors to my raised concerns and the two general points in the rebuttal. It still seems to me the equation (1) is not a fully nonlinear system; the change of state dynamics depends linearly on control u, even if both f and g can be fully nonlinear.\n\n> In other words, the nonlinearity in x is not as challenging as the nonlinearity in u, thereby, if the authors consider solving a standard LQR problem as in (1)-(2), a more in-depth discussion of various methods may be necessary.\n\nWe thank the reviewer for the follow up questions.\n\n* In short, we solving a nonlinear optimal control problem, not \"a standard LQR problem as in (1)-(2)\".\n\n* In regards to what \u201cnonlinearity\u201d the reviewer considers \"challenging\" - 1) The reviewer probably did not realize that continuous time RL (CT-RL) cannot be mixed up with discrete-time (DT-RL). CT-RL is truly more challenging than DT-RL. 2) The reviewer probably has not read our manuscript about our clear justifications and discussions that our results are SOTA, and these results are based on SOTA CT-RL problem formulation (1)-(2).\n\n* More specifically, as we state in boldface in the manuscript, and in our General Points 1: We address the **same** affine nonlinear system structure $(f, g)$ as the SOTA CT-RL works in ADP and Deep RL FVIs. This structure is the **standard** in CT-RL.\n\n* Now to shed some new light on the reviewer\u2019s question, given that the SOTA ADP and Deep RL works address the same affine nonlinear system $(f, g)$, should we tell them all that they should be solving LQR problems instead, too?\n\n* As for \"a more in-depth discussion of various methods may be necessary\" -- We are afraid that we have been thorough, from system under consideration, method, and environments to results, our work is SOTA. If the reviewer has specific references that \"require further discussion in the manuscript\", please do inform us.\n\n> In addition, the objective function (2) is clearly quadratic in u. This system qualifies for the well-studied linear-quadratic (in u) paradigm, for which, as I mentioned, extensive researches and methods have been proposed. \n\n* We would like to emphasize again that Q-R cost on nonlinear system $\\neq$ LQR.\n\n* For the Reviewer's confusion of linear and nonlinear control problems, we respectfully encourage that they revisit a standard optimal control text; e.g., Frank Lewis's \"Optimal Control\".\n\n* Please see our response to Reviewer 2 regarding Q-R cost. This quadratic cost structure is also the standard in optimal control (linear and nonlinear) dating back to Kalman in the 1960s.\n\n* Q-R cost is also the predominant cost structure chosen by leading ADP CT-RL algorithms; see, e.g., (Vrabie \\& Lewis, 2009), (Vamvoudakis \\& Lewis, 2010), (Jiang \\& Jiang, 2014), (Bian \\& Jiang, 2022). The deep RL FVI works (Lutter et al., 2023a), (Lutter et al., 2022) also accommodate Q-R cost."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8465/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700525841264,
                "cdate": 1700525841264,
                "tmdate": 1700526703982,
                "mdate": 1700526703982,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]