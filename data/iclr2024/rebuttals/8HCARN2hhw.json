[
    {
        "title": "Learning with a Mole: Transferable latent spatial representations for navigation without reconstruction"
    },
    {
        "review": {
            "id": "JhDRy14AEy",
            "forum": "8HCARN2hhw",
            "replyto": "8HCARN2hhw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission365/Reviewer_SbcL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission365/Reviewer_SbcL"
            ],
            "content": {
                "summary": {
                    "value": "This work is about learning an abstract latent representation $\\mathbf{r}_t$ for navigation based on end-to-end learning, without a metric map.\nThe representation is inferred by a recurrent neural network from past RGB-D images and PointGoal-task information.\nA policy outputs actions based on this representation.\n\nThe main contribution is in how the representation is learned: instead of training it through the main policy directly, the authors use an auxiliary \"blind\" policy.\nIn training, the auxiliary policy has to navigate to randomly picked subgoals branching off of the main policy path, but it has to do so using only the last inferred representation $\\mathbf{r}_t$ from the main policy loop, with no access to new RGB-D observations.\nIt is argued that this would force the representation to be better suited for navigation in unseen environments (which is debatable, see my comments about this below).\n\nThe approach is trained and evaluated in simulation (Habitat, Gibson scenes), and sim2real transfer is studied on a small scale (11 runs on a real robot)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The method appears sound on the whole, and most design choices are motivated.\n\n- The presentation was easy to follow, but there is some room for improvement (e.g. see comment on missing environment examples below).\n\n- The results indicate the method works in the considered environments, with success rates of ca. 90% in simulation and 80% in sim2real. To the best of my knowledge this is reasonable, compared to the prior art on end-to-end learned navigation. \n\n- Ablations of the main method aspects are included, which is appreciated.\n\n- There is some indication of improved sim2real transfer (e.g. compared to PPO), but there are also limitations of the evaluation (see weaknesses)."
                },
                "weaknesses": {
                    "value": "- The number of real-world experiments is rather low (11 episodes). While I can see how three baselines + the method itself add up in terms of workload, with only 11 runs I cannot tell if the sim2real results are statistically significant. I understand that it is unlikely this can be corrected on a short term during the review process, but it is an important point to consider for future revisions.\n- Related to the previous point, the paper is currently missing information about the complexity of the environments and the length of the evaluated navigation runs in them. Some qualitative examples in 2D would be nice, particularly of the real office environment. I am primarily concerned about this because navigation success metrics depend very much on a) the geodesic length of the navigation trajectory and b) how maze-like the layout of the scene is. Revealing the complexity of the considered navigation tasks will make the paper stronger.\n- I am not certain if the blindness property (a forefront contribution of the paper) is as useful compared to data augmentation:\n    - The results seem to support that subgoal spawning is beneficial. However, I am not convinced that the removal of observations in the auxiliary policy matters a lot. Simple behavior cloning + the subgoal data augmentation ((c) in Table 1) performs very well both in Sim and in NoisySim, better than plain navigability training ((d) in Table 1).\n    - I understand that the number of RGB-D observations with the proposed method is reduced, but this is a choice. The number of env interactions remains the same, so the comparison is on even terms.\n    - In the sim2real experiment, was the BC baseline ((c) in Table2) trained with the data augmentation regime, or regular? I believe sim2real results with BC + subgoal augmentation compared against the proposed method would have been ideal here.\n- The main text should specify the exact data fed into the networks clearly, this would make it much easier to assess the applicability of the solution. If I understand appendix A.2 correctly, the representation GRU takes both RGB-D images (not just RGB) and positional data (GPS + compass orientation), whereas the main text only talks about visual observations. This is somewhat misleading, navigation based on RGB data alone is very different from navigation with RGB-D images (from which collision information is easy to extract) with known poses.\n- The probing network is not directly related to the policy, so I am skeptical about the interpretation of the probing results at the very end of sec. 5. I think the paper would be better off by highlighting that this investigation is speculative, and should be taken with a grain of salt.\n\nOverall, I find that some of the main claims are stronger than what the experiments show (e.g. sec. 4) and that the evaluation could have been more precise. Note that I understand and can agree with the intuitive arguments in sec. 4, but these need to be supported better by the results. Still, the proposed training scheme does have some merit and the method seems to work, so I am leaning slightly positive in my assessment."
                },
                "questions": {
                    "value": "- In the sim2real experiments, which BC variant was used as a baseline? With augmentation (i.e. subgoals) or without?\n- I am not sure I agree with the interpretation of the probing experiments. Sure, they indicate the reconstructed occupancy grids from the navigability representation are more optimistic, but then again can this be interpreted at all? There is no indication that the policy uses the representation in the same way, not to mention that absolute optimism w.r.t. collisions is not a desirable property per se."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission365/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission365/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission365/Reviewer_SbcL"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission365/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698674296779,
            "cdate": 1698674296779,
            "tmdate": 1700663597435,
            "mdate": 1700663597435,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "40S7UjxwbE",
                "forum": "8HCARN2hhw",
                "replyto": "JhDRy14AEy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission365/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission365/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the constructive and detailed feedback!"
                    },
                    "comment": {
                        "value": "> The number of real-world experiments is rather low (11 episodes). While I can see how three baselines + the method itself add up in terms of workload, with only 11 runs I cannot tell if the sim2real results are statistically significant. \n\nWe agree that the more real-world experiments, the better. As rightfully noted by the reviewer, these experiments are time-consuming, and benchmarking four different methods leads to considerable engineering effort. In this paper we conducted 44 robot experiments, and we would like to stress that most papers in this space present significantly fewer experiments, for example:\n- \u201cObject Goal Navigation using Goal-Oriented Semantic Exploration\u201d. NeurIPS 2020: 20 robot experiments.\n- \u201cSuccess weighted by completion time: A dynamics-aware evaluation criteria for embodied navigation\u201d. IROS 2021: 12 runs, i.e. 2 episodes, each one repeated 3 times, for 2 methods.\n- \u201cNavigating to Objects Specified by Images\u201d. ICCV 2023: 8 experiments.\n\n> The paper is currently missing information about the complexity of the environments and the length of the evaluated navigation runs in them. [...] I am primarily concerned about this because navigation success metrics depend very much on a) the geodesic length of the navigation trajectory and b) how maze-like the layout of the scene is. Revealing the complexity of the considered navigation tasks will make the paper stronger.\n\nThanks, the revised manuscript features a top-down map of the real environment with the ground-truth test trajectories in Table 1 (previously Table 2, we swapped Table 1 and Table 2 for readability). We also include the average geodesic length of the test episodes, which is 8.9 meters - for comparison Gibson val episodes have an average length of 5.9 meters. One room of the test environment is shown in Fig. 1 and features thick carpets, uneven floor and large windows that disturb the robot sensing.\n\n> I am not certain if the blindness property [...] is as useful compared to data augmentation: [...] I am not convinced that the removal of observations in the auxiliary policy matters a lot. Simple behavior cloning + the subgoal data augmentation ((c) in Table 1) performs very well both in Sim and in NoisySim, better than plain navigability training ((d) in Table 1).\n\nDifferences between methods (c) and (d)  are indeed small in simulation (Table 2 in the revised manuscript, previously Table 1). However we argue that the navigability loss improves sim2real transfer because it relies on non-visual navigability information that has reduced sim2real gap compared to RGB-D, used with data augmentation. Thus it is not surprising that performance in simulation is similar for the two approaches, while we observe significant gains in real-world experiments, Table 2 of the revised manuscript (previously Table 1), where Navigability improves Success Rate and SPL with respect to data augmentation, model (c), by about 10%.\n\n> I understand that the number of RGB-D observations with the proposed method is reduced, but this is a choice. The number of env interactions remains the same, so the comparison is on even terms.\n\nThis is indeed true, however the navigability loss brings a significant computational advantage at training because the simulator does not have to render RGB-D observations, allowing to speed-up training and reduce hardware and energy requirements. \n\n> In the sim2real experiment, was the BC baseline ((c) in Table2) trained with the data augmentation regime, or regular? I believe sim2real results with BC + subgoal augmentation compared against the proposed method would have been ideal here.\n\nIndeed, the BC baseline in the sim2real experiment (Table 1 in the revised manuscript, previously Table 2) is the one with subgoal augmentation. We modified the notation on the table to \"(c) BC (L) (S)\" to clarify this.\n\n> The main text should specify the exact data fed into the networks clearly, this would make it much easier to assess the applicability of the solution. If I understand appendix A.2 correctly, the representation GRU takes both RGB-D images (not just RGB) and positional data (GPS + compass orientation), whereas the main text only talks about visual observations.\n\nThe inputs are indeed RGB-D and Goal vector, we clarify this in the revised manuscript.\n\n> The probing network is not directly related to the policy, so I am skeptical about the interpretation of the probing results at the very end of sec. 5. I think the paper would be better off by highlighting that this investigation is speculative, and should be taken with a grain of salt.\n\nThis was a design choice: the probing experiments are not directly related to the policy, they are designed to probe the representation built in Phase 1 and gain intuitions of what is captured at that stage of training. We agree that we can only speculate about these experiments, this is added to the paper. We also tone down claims related to these experiments."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission365/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700495891538,
                "cdate": 1700495891538,
                "tmdate": 1700500564614,
                "mdate": 1700500564614,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "S6ExetXZBr",
                "forum": "8HCARN2hhw",
                "replyto": "JhDRy14AEy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission365/Reviewer_SbcL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission365/Reviewer_SbcL"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for responding! The top-down map figure and the BC (L) + (S) clarification in the new Table 1 were much needed improvements from my perspective, thanks for addressing them. The rest of the clarifications also help.\n\nMy original assessment was already leaning positive, the above raises my confidence in that assessment. My current score would be between 6 and 7."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission365/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700663480098,
                "cdate": 1700663480098,
                "tmdate": 1700663793225,
                "mdate": 1700663793225,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JMLPqKpT3z",
            "forum": "8HCARN2hhw",
            "replyto": "8HCARN2hhw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission365/Reviewer_hBzC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission365/Reviewer_hBzC"
            ],
            "content": {
                "summary": {
                    "value": "This work presents an approach to learn an actionable representation of the scene without optimizing for the reconstruction objective. The key idea is to use the learned representation to navigate on multiple short sub-episodes without any direct visual observations. The learned representation is optimized by this blind auxiliary agent for navigability and not reconstruction. Extensive experiments in both simulation and real world demonstrate the effectiveness of the proposed approach over several baselines, eg. BC, PPO, Map+Plan, especially in noisy settings."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The learned representation is optimized for its usability in navigation rather than reconstruction objective. This makes it suitable for any downstream task involving navigation.\n- The latent cognitive map can be learned via a blind auxiliary agent without the use of direct visual observations.\n- The paper is well written. The related work section is quite comprehensive and provides a good overview of the literature. Sec. 4 provides a good discussion on the importance of blindness and differences with data augmentation.\n- Extensive experiments (Table 1) in simulation and NoisySim settings show the benefits of Navigability when combined with policy learning approaches, eg. BC and PPO. Sim2real performance (Table 2) on a physical robot shows its effectiveness over BC, PPO & mapping+planning in real world.\n- Ablation study on the continuity of hidden state (Table 3), connection between the representation and blind policy (Table 4), and reconstruction performance (Fig. 6) provide valuable insights into the capabilities of the proposed approach."
                },
                "weaknesses": {
                    "value": "- Sec. 4 reasoning makes sense for GRUs & LSTMs since the gating mechanism in GRUs & LSTMs explicitly encourages the specified behavior. Would this also hold for other architectures, eg. transformer? It'd be useful to provide some insights into this.\n- Sec. 4 argues that the training will prioritize learning `r1` and neglect `r2` and provides two reasons. Is there any empirical evidence for this in the context of navigation?\n- For BC(L)(S)(data augm.) baseline in Table 1, when training on the short routes, is the hidden state copied from the long trajectory or initialized separately?\n- How are the c.1 & c.3 variants in Fig. 6 different from Table 3? The c.3 variant in Fig. 6 is worse than other baselines but it works the best in Table 3. It seems like `Always continue` works well with navigability but not with BC. It'd be helpful to clarify why the performances are so different.\n- What does `sym-SPL` represent? why is the minimum of the two ratios taken?"
                },
                "questions": {
                    "value": "The questions are mentioned in the weaknesses above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission365/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698710418887,
            "cdate": 1698710418887,
            "tmdate": 1699635963478,
            "mdate": 1699635963478,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "13OQi97lwv",
                "forum": "8HCARN2hhw",
                "replyto": "JMLPqKpT3z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission365/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission365/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the constructive and detailed feedback!"
                    },
                    "comment": {
                        "value": "Thank you for the constructive and encouraging remarks!\n\n> Sec. 4 reasoning makes sense for GRUs & LSTMs since the gating mechanism in GRUs & LSTMs explicitly encourages the specified behavior. Would this also hold for other architectures, eg. transformer? It'd be useful to provide some insights into this.\n\nThe reasoning in section 4 is not limited to any specific form of representing the history of observations of the agent. It can be compressed into a recurrent memory, as in our experiments (GRU or LSTM), or be self-attention over time, like in a decision transformer.\n\n> Sec. 4 argues that the training will prioritize learning r1 and neglect r2 and provides two reasons. Is there any empirical evidence for this in the context of navigation?\n\nWhile compression mechanisms in recurrent memory have been studied in the past (cf. the references we provide in section 4.), up to our knowledge, we are the first to investigate them in the context of navigation.\n\nOn the other hand, short-cut learning is a well established concept in the literature in a very general context, establishing that, if the training data contains spurious correlations, trained models tend to learn simpler (mostly unwanted) decision strategies:\n- Leon Bottou. \u201cFrom machine learning to machine reasoning\u201d. Machine learning, 94(2):133\u2013149, 2014\n- Geirhos et al. \u201cShortcut learning in deep neural networks\u201d. Nature Machine Intelligence, pages 665\u2013673, 2020.\n\nIn terms of empirical work specifically on navigation, we can refer to the well known \u201ccopy-cat\u201d behavior in autonomous driving, where agents with memory tend to behave less well than agents taking only the current observation, e.g. as in \n- Wen et al., Fighting Copycat Agents in Behavioral Cloning from Observation Histories, NeurIPS 2020.\n\nIn the following work it has been empirically shown that an agent attends to the relevant latent memory slot when a searched (and previously seen) object is not in view, and that attention to memory disappears once the object is in view. While this has only been qualitatively shown (Figure 4 and associated explanations) and not quantitatively evaluated, it provides further evidence for the behavior:\n- Beeching et al., EgoMap: Projective mapping and structured egocentric memory for Deep RL. ECML-PKDD, 2020.\n\n> For BC(L)(S)(data augm.) baseline in Table 1, when training on the short routes, is the hidden state copied from the long trajectory or initialized separately?\n\nThis is variant (c.3), \u201calways continue\u201d, where the hidden state is copied from the long trajectory.\n\n> How are the c.1 & c.3 variants in Fig. 6 different from Table 3? The c.3 variant in Fig. 6 is worse than other baselines but it works the best in Table 3. It seems like `Always continue` works well with navigability but not with BC. It'd be helpful to clarify why the performances are so different.\n\nThe reasoning requirements for agents in these scenarios are different. We can only speculate on the reasons: We expect the \u201calways continue\u201d variant to perform less well because of the inconsistency of the hidden state at the sub goals, and this is corroborated by the experiments in Figure 6, which *only* uses the spatial representation, per construction. However, the full agent presented in Table 3 uses the spatial representation but also reactive components from visual observations, which can learn short-cuts in reasoning. This agent will largely benefit from having more information integrated in its hidden memory, which in this case is not reset at each sub goal.\n\nThese are conjectures and it is hard to derive conclusive insights from these probing experiments.  We stress this in the revised paper and we will tone down the claims related to the probing experiments.\n\n> What does `sym-SPL` represent? why is the minimum of the two ratios taken?\n\nsym-SPL is a symmetric version of SPL that we introduce in paragraph \u201cProbing the representations\u201d, page 9: $$\\textit{Sym-SPL} = \\sum_{i=1}^D \\sum_{n=1}^N S_{i,n} \n\\min  \\left (\\frac{\\ell_{i,n}}{\\ell_{i,n}^*},\\frac{\\ell_{i,n}^*}{\\ell_{i,n}} \\right ).$$ \nThe reason to use this measure is that the reconstructed map used by the agent to navigate might erroneously indicate non-navigable space as navigable, leading to SPL larger than 1. For this reason, besides computing the ratio between the predicted path $\\ell_{i,n}$ and ground-truth shortest path $\\ell_{i,n}^*$ as in standard SPL, we also compute $\\frac{\\ell_{i,n}^*}{\\ell_{i,n}}$ and take the minimum to penalize planned trajectories that go through non navigable spaces.\n\nIn other words, and to give an intuitive explanation, compared to standard SPL, to be optimal, not only needs the agent\u2019s path approach the ground truth (GT) \u201cfrom below\u201d, it should also not \u201covershoot\u201d."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission365/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700495888022,
                "cdate": 1700495888022,
                "tmdate": 1700495888022,
                "mdate": 1700495888022,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gXhQ7qK4ae",
                "forum": "8HCARN2hhw",
                "replyto": "13OQi97lwv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission365/Reviewer_hBzC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission365/Reviewer_hBzC"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the clarifications. It helps me understand the paper better."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission365/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700666296191,
                "cdate": 1700666296191,
                "tmdate": 1700666296191,
                "mdate": 1700666296191,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "jfe8XqAsw7",
            "forum": "8HCARN2hhw",
            "replyto": "8HCARN2hhw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission365/Reviewer_sYMN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission365/Reviewer_sYMN"
            ],
            "content": {
                "summary": {
                    "value": "The authors address the task or learning an informative latent state representation for downstream tasks. Instead of a conventional scene reconstruction loss they opt to learn a representation that optimizes navigability by introducing a time-series behavior cloning loss for a blind auxiliary policy (with no future timestep observations or reconstructions) on generated short-term sub-goals (section 3-4, page 4).\n\nThe authors discuss the importance of the \u201cblindness property\u201d with a toy example in section 4 (Figures 4 and 5) where they explain that the conventional behavior cloning method may learn to adopt a memory-less latent representation whereas their approach involving future timestep predictions based on past observations encourages memory in the latent representation.\n\nThe method is evaluated in simulation and real-world experiments. They report raw navigation success rate and SPL (success weighted by the optimality of planned paths) versus variants of behavior cloning and PPO (Table 1 and 2) where they method outperforms baselines. They further examine how to continue the latent state at the end of short sub-goal episodes (Table 3) and how to set the initial latent for the auxiliary blind policy (Table 4). The authors also examine the representative ability of their latent state by training a scene map reconstruction network and doing subsequent path planning with it (Figure 6 and Table 5). Although other baselines obtain a better raw map reconstruction, their method generally outperforms others in the navigation planning score (especially in noisy environments)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Discussion and experiments are detailed and analyze the different components of the method. Experiments test many ablations and variants of baselines to illustrate the impacts of their additions.\n- Both simulation and real-world results.\n- The work is well written and clear."
                },
                "weaknesses": {
                    "value": "I feel the prediction of future labels without reconstruction is not a very novel architecture. I am also not sure if the improvement in results is because of the \u201cblindness property\u201d in the navigability loss (equation 5) or simply the addition of training data from generated sub-goals. There is an experimental discussion section addressing this concern (\u201cIs navigability reduced to data augmentation?\u201d, page 8) where they compare against a behavior cloning agent with added sub-goals. However, the difference in performance ((c) versus (e) in Table 1) seems relatively small (about 1% for success rate and even smaller point margins for SPL) and there is no reported confidence intervals or variance for these results over repeated seeds or trials. Thus, I am unsure how convincing these results are.\n\n__Minor Criticisms:__\n- Table 2 seemed to be discussed in detail before Table 1 in the experimental section which I found a bit disorienting for readability purposes.\n- I believe the methods relation to a mole is only explained in appendix  A.2. (The auxiliary agent \u03c1 (a.k.a. the \u201cmole\u201d)) and so I was initially a bit confused by the title."
                },
                "questions": {
                    "value": "- On page 8, \u201cImpact of navigability\u201d, the text reads: \u201cWe see that PPO (a) outperforms classical BC (b), which corroborates known findings in the literature.\u201d However, the results in Table 1 appear to indicate the opposite, with BC (b) having a higher success rate. Is this a typo or a misunderstanding on my part?\n\n- In Table 3, the \u201cc.3. Always continue\u201d variant performs best. However, in Figure 6, now c.1 outperforms c.3. Is there any intuition behind this result?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission365/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission365/Reviewer_sYMN",
                        "ICLR.cc/2024/Conference/Submission365/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission365/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698828270200,
            "cdate": 1698828270200,
            "tmdate": 1700518901117,
            "mdate": 1700518901117,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "skZIqKd49P",
                "forum": "8HCARN2hhw",
                "replyto": "jfe8XqAsw7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission365/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission365/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your feedback!"
                    },
                    "comment": {
                        "value": "Thanks for the constructive remarks!\n\n> I feel the prediction of future labels without reconstruction is not a very novel architecture. I am also not sure if the improvement in results is because of the \u201cblindness property\u201d in the navigability loss (equation 5) or simply the addition of training data from generated sub-goals. \n\nThe main difference between adding the navigability loss and standard data augmentation comes from the fact that the auxiliary agent is blind: the steps on short episodes improve the representation integrated over visual observations on long episodes, whereas classical data augmentation would generate new samples and train them with the same loss. We see this as generating a new learning signal for the existing samples (i.e. on long episodes) using privileged navigability information from the simulator. The experiments with the real robot also clearly show the advantage of this method compared to data augmentation.\n\n> There is an experimental discussion section addressing this concern (\u201cIs navigability reduced to data augmentation?\u201d, page 8) where they compare against a behavior cloning agent with added sub-goals. However, the difference in performance ((c) versus (e) in Table 1) seems relatively small (about 1% for success rate and even smaller point margins for SPL) and there is no reported confidence intervals or variance for these results over repeated seeds or trials. Thus, I am unsure how convincing these results are.\n\nOur hypothesis is that the navigability loss improves sim2real transfer because it relies on non-visual navigability information that has reduced sim2real gap compared to RGB-D, used with data augmentation. Therefore it is not surprising that performance in simulation is similar for the two approaches (Table 2 of the revised manuscript - previously Table 1), while we observe significant gains in real-world experiments (Table 1 of the revised manuscript - previously Table 2), where Navigability (e) improves Success Rate and SPL with respect to (c) by about 10%.\n\n> Table 2 seemed to be discussed in detail before Table 1 in the experimental section which I found a bit disorienting for readability purposes.\n\nThanks a lot for the comment, in the revised manuscript we swapped Table 1 and Table 2 to improve readability.\n\n> I believe the methods relation to a mole is only explained in appendix A.2. (The auxiliary agent \u03c1 (a.k.a. the \u201cmole\u201d)) and so I was initially a bit confused by the title.\n\nThe name \u201cmole\u201d was chosen because moles are blind, as our auxiliary agent is, so we found it to be an evocative image. \n\n> On page 8, \u201cImpact of navigability\u201d, the text reads: \u201cWe see that PPO (a) outperforms classical BC (b), which corroborates known findings in the literature.\u201d However, the results in Table 1 appear to indicate the opposite, with BC (b) having a higher success rate. Is this a typo or a misunderstanding on my part?\n\nThanks a lot for the remark, this was indeed a typo and it is corrected in the revised version of the manuscript.\n\n> In Table 3, the \u201cc.3. Always continue\u201d variant performs best. However, in Figure 6, now c.1 outperforms c.3. Is there any intuition behind this result?\n\nTable 3 shows results for the complete agents on the PointGoal navigation task, e.g. navigation in 3D photorealistic environments, whereas Figure 6 reports results for the experiments probing the representations after Phase 1 of training, e.g. \u201cnavigation\u201d in estimated 2D maps. The reasoning requirements for agents in these scenarios are different. We can only speculate on the reasons: We expect the \u201calways continue\u201d variant to perform less well because of the inconsistency of the hidden state at the sub goals, and this is corroborated by the experiments in Figure 6, which *only* uses the spatial representation, per construction. However, the full agent presented in Table 3 uses the spatial representation but also reactive components from visual observations, which can learn short-cuts in reasoning not related to the spatial structure. This agent will largely benefit from having more information integrated in its hidden memory, which in this case is not reset at each sub goal.\n\nThese are conjectures and it is hard to derive conclusive insights from these probing experiments.  We stress this in the revised paper and we will tone down the claims related to the probing experiments."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission365/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700495883190,
                "cdate": 1700495883190,
                "tmdate": 1700495883190,
                "mdate": 1700495883190,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JZemtn7cch",
                "forum": "8HCARN2hhw",
                "replyto": "skZIqKd49P",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission365/Reviewer_sYMN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission365/Reviewer_sYMN"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for replying to my questions. I have raised my score slightly."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission365/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700518862572,
                "cdate": 1700518862572,
                "tmdate": 1700518862572,
                "mdate": 1700518862572,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]