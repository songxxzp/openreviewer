[
    {
        "title": "BTBS-LNS: A Binarized-Tightening, Branch and Search Approach of Learning Large Neighborhood Search Policies for MIP"
    },
    {
        "review": {
            "id": "tul4oRzHr0",
            "forum": "Qwxe8WKSgy",
            "replyto": "Qwxe8WKSgy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5004/Reviewer_uy5T"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5004/Reviewer_uy5T"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors develop a machine-learning-guided large neighborhood search (LNS) procedure that provides a heuristic approach to general MIPs. This includes several improvements over existing (LNS) procedures including:\n- A new approach to handling general binary variables, wherein a learning model is used to predict the tightness of bounds placed on these variables\n- A new way of encoding MIP problems as graphs, wherein the graph includes nodes for the objective function\n- A method wherein a learning model is used to generate local branching constraints\nThe authors provide extensive computational results on a range of problems."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The paper features some novel improvements to machine-learning-guided large neighborhood search approaches to MIP. This is a timely topic with high potential significance. The computational results are extensive and impressive."
                },
                "weaknesses": {
                    "value": "Overall, I found this paper to be extremely unclear and poorly written. For example:\n- The authors state that \"we represent each general integer variable with $d=\\lceil \\log_2(ub - lb) \\rceil$ variables, where ub and lb are original variable upper and lower bounds, respectively. The subsequent optimization is applied to the substitute variables\". The authors should show this substitution more explicitly. The authors seem to indicate that the bound-tightening scheme is then fixing some of these substitute variables. I am aware of some common binarization schemes (e.g. Owen & Mehrota 2002), but the bound-tightening scheme given in algorithm 1 does not seem to correspond with a variable-fixing approach to any binarization scheme that I can think of.\n-  I was completely unable to understand even at a high-level what their \"branching\" approach entailed until I read Algorithm 3 in the appendix. Readers should not have to read the appendix to make sense of the main text. In fact, their algorithm does not really \"branch\" as far as I can tell. \"Branching\" typically means building a search tree that divides the space of solutions. Instead, the algorithm adds local branching constraints similar to Fischetti and Lodi (2003) or Liu, Fischetti and Lodi (2022), but doesn't actually carry out the branching part of this procedure. Note: the authors might disagree with the statement \"the algorithm adds local branching constraints\", as the authors differentiate between \"global branching\" and \"local branching\", but in either case the constraint that is added takes the form $\\sum\\_{i \\in D} |x^{t+1}\\_i - x_i^t| \\leq rn$, which is a local branching constraint as defined in Fischetti and Lodi 2003.\n- In order to understand how the authors defined a neighborhood for their large neighborhood search procedure, I again had to consult the appendix. This is a critical detail of a large neighborhood search algorithm, and should be made much more clear.\n- I cannot tell what data was used to train these models in any of the experiments.\n\nThe three improvements (bound tightening scheme, graph encoding & learning approach, \"branching\" learning approach) in the paper each seem somewhat marginal. However, I don't view this weakness as something that would necessarily prevent publication of this work, as the combination of these improvements does seem to be a significant advancement in the design of heuristic procedures for MIPs."
                },
                "questions": {
                    "value": "What binarization scheme did you use?\nWhat training data did you use in your computational experiments?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5004/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5004/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5004/Reviewer_uy5T"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5004/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698444552277,
            "cdate": 1698444552277,
            "tmdate": 1699636488563,
            "mdate": 1699636488563,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YzS3ipA2pX",
                "forum": "Qwxe8WKSgy",
                "replyto": "tul4oRzHr0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5004/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5004/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer uy5T"
                    },
                    "comment": {
                        "value": "We sincerely thank Reviewer uy5T for the positive feedback and precious suggestions! Below we address each comment in detail:\n\n> **Q1**: Detail explanation on the \"Binarize and Tighten\" approach.\n\nThanks for your comments. For general integer variables, we propose a \"Binarize and Tighten\" scheme. At first, in the binarization phase, general integer variables were encoded to $d = \\lceil{\\log_{2}{(ub-lb)}}\\rceil$ substitution variables. For example, a general integer variable $x$ with range $[0, 7]$ can be encoded to 3 variables ($x_1, x_2, x_3$) to represent the original variable with decreasing significance. In this way, we transform the LNS decision for the original variable to 3 different decisions on substitution variables. Then, when either variable is fixed by LNS, it indicates current solution reliable at corresponding significance. The decisions are utilized to tighten the bounds of original variables sequentially, rather than directly fixed, at current step illustrated in Alg.1.\n\n\n> **Q2**: Explanation on the branching components, as it doesn't actually carry out the branching part of this procedure.\n\nThanks for your insightful comments. The meaning of branching in this paper mainly refers to the one used in the previous work [1]. In other words, we identify the potentially wrongly decided variables by LNS and optimize them by adding the local branching constraints. It takes effect on top of LNS, making them closer to global optimum and serves as a further optimization scheme to the learned LNS policy.\n\n\n> **Q3**: In order to understand how the authors defined a neighborhood for their large neighborhood search procedure, I again had to consult the appendix. This is a critical detail of a large neighborhood search algorithm and should be made much more clear.\n\nThanks for your suggestions. We will update the paper and place some critical descriptions back to the main text if page permits.\n\n\n\n> **Q4**: I cannot tell what data was used to train these models in any of the experiments.\n\nAs illustrated in Sec 4.1, for SC, MIS, CA and MC, we generate several training instances following the same distribution with the testing set, except for different problem sizes. Then the LNS and branching policy will be sequentially trained on these instances. The features and states for the instances are given in Table 7 in the appendix.\n\nAs for heterogeneous instances like MIPLIB, we performed cross-validation. Specifically, at each round, we split them into training, validation, and testing sets by 70\\%, 15\\%, and 15\\%. In this way, the whole dataset can be tested sequentially to make fair comparison.\n\n\n**References:**\n\n[1] Liu D, Fischetti M, Lodi A. Learning to search in local branching[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2022, 36(4): 3796-3803."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5004/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700054469894,
                "cdate": 1700054469894,
                "tmdate": 1700054469894,
                "mdate": 1700054469894,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ay9GhcvAoc",
                "forum": "Qwxe8WKSgy",
                "replyto": "YzS3ipA2pX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5004/Reviewer_uy5T"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5004/Reviewer_uy5T"
                ],
                "content": {
                    "title": {
                        "value": "Response to author response"
                    },
                    "comment": {
                        "value": "Thank you for the clarifications. I think that the changes have made many things about the paper more clear, and I would revise my overall rating to an 8 (accept)."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5004/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700495991112,
                "cdate": 1700495991112,
                "tmdate": 1700495991112,
                "mdate": 1700495991112,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nAgb0XqfC1",
            "forum": "Qwxe8WKSgy",
            "replyto": "Qwxe8WKSgy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5004/Reviewer_zCZB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5004/Reviewer_zCZB"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors propose the BTBS-LNS technique to solve the MIP problem in order to cope with the problem that the LNS technique often falls into local optimality. The authors claim that their technique effectively escapes local optimality, and extensive experiments on a large number of instances show that it leads the SCIP and LNS baseline, is comparable to Gurobi, and even performs better early in the run."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The article is praiseworthy for its extensive experimental data and significant findings. The authors have selected a large number of baselines for comparative experiments on different benchmarks, which are demonstrated by a large number of figures and tables. These experimental results strongly prove the effectiveness of the new technology.\n\n2. The paper is laudable for its well-structured and logical presentation, providing a comprehensive understanding of the research topic."
                },
                "weaknesses": {
                    "value": "1. On page 4, in \u2018THE BINARIZED TIGHTENING SCHEME\u2019 chapter, the authors mention that the analysis for MIPLIB shows that all unbounded variables have either upper or lower bounds, and the authors have therefore designed virtual upper/lower bounds. Could there be meaningful MIP instances other than MIPLIB where variables exist simultaneously with no upper or lower bounds? If it exists, can BTBS-LNS handle it? Are there serious robustness problems with ignoring such instances? \n\n2. On page 8, in 'Hyperparameters' under section 4.1, the authors mention the use of SCIP as the default solver, noting that this is the blue box part of Fig. 1. In my understanding the SCIP solver should correspond to the 'MIP solver' on the right hand side of Fig.1, and the boxes for Neighborhood Search and Branching Policy on the left hand side are similarly blue, is this a minor graphing and writing error, or am I misunderstanding that SCIP is equally involved in these stages?\n\n3. As described in the experiment part, the authors conduct experiments on those MIP instances that are encoded from other combinatorial problems, including set covering, maximal independent set, combinatorial auction, and maximum cut. Actually, there are specific optimization solvers for each of those combinatorial optimization problems, and MIP solvers do not represent the state of the art in solving those problems. As a submission to a top-tier conference, it is required to compare their proposed method against the real state of the art.\n\n4. In fact, there exist standard benchmarks for evaluating MIP solvers, i.e., Hans Mittelmann's benchmarks (https://plato.asu.edu/bench.html). Why don\u2019t the authors test their proposed method and baseline solvers on those standard benchmarks?"
                },
                "questions": {
                    "value": "Please see my comments in \"Weaknesses\"."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "I do not have ethic concern about this paper."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5004/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5004/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5004/Reviewer_zCZB"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5004/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698565720652,
            "cdate": 1698565720652,
            "tmdate": 1699636488442,
            "mdate": 1699636488442,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "97YFFqEKTt",
                "forum": "Qwxe8WKSgy",
                "replyto": "nAgb0XqfC1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5004/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5004/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer zCZB"
                    },
                    "comment": {
                        "value": "We sincerely thank Reviewer zCZB for their valuable suggestions. Summary of responses and revisions are as follows:\n\n> **Q1**: Could there be meaningful MIP instances other than MIPLIB where variables exist simultaneously with no upper or lower bounds? If it exists, can BTBS-LNS handle it? Are there serious robustness problems with ignoring such instances?\n\nWe really appreciate your insightful comments. Although rare, there may be instances that contain variables simultaneously with no upper or lower bounds (called \"free\" variables). In our current implementation, we will leave them unfixed (untightened). We argue that it was not serious robustness problems, as the presolve technique in MIP solvers can handle most of the cases into single unbounded or totally bounded. The few remaining variables being unfixed will not contribute to heavy computational costs. To be more convincing, we will test our approach on those instances to further evaluate its generalization ability, in our next round of response with updated experiments.\n\n\n>**Q2**: On page 8, in 'Hyperparameters' under section 4.1, the authors mention the use of SCIP as the default solver, noting that this is the blue box part of Fig.1. In my understanding the SCIP solver should correspond to the 'MIP solver' on the right hand side of Fig.1, and the boxes for Neighborhood Search and Branching Policy on the left hand side are similarly blue, is this a minor graphing and writing error, or am I misunderstanding that SCIP is equally involved in these stages?\n\nSorry for some misunderstanding. SCIP only corresponds to the 'MIP solver' on the right hand side of Fig.1. We will make it clear in the updated paper version.\n\n\n> **Q3**: As described in the experiment part, the authors conduct experiments on those MIP instances that are encoded from other combinatorial problems, including set covering, maximal independent set, combinatorial auction, and maximum cut. Actually, there are specific optimization solvers for each of those combinatorial optimization problems, and MIP solvers do not represent the state of the art in solving those problems. As a submission to a top-tier conference, it is required to compare their proposed method against the real state of the art.\n\n\nThanks for your insightful suggestions. Similar to the MIP solvers, our proposed approach was designed for general instances, rather than specific problems. We don't finetune our structures or frameworks specifically for each problem. In this respect, except for the specific problems, we also compared our BTBS-LNS on more heterogeneous instances (MIPLIB) from diverse problems, and it also delivers superior performance compared with the MIP solvers, further demonstrating the generalization ability. However, we really appreciate your valuable suggestions. Comparison with specific optimization solvers may be an interesting topic to further improve and demonstrate the performance of our proposed BTBS-LNS, and we will try to supplement some experiments, in our next round of response with updated experiments.\n\n\n> **Q4**: In fact, there exist standard benchmarks for evaluating MIP solvers, i.e., Hans Mittelmann's benchmarks (https://plato.asu.edu/bench.html). Why don\u2019t the authors test their proposed method and baseline solvers on those standard benchmarks?\n\nThanks for your comments. We have evaluated our proposed approaches on these standard benchmarks. The anonymous MIPLIB (AMIPLIB) tested in Table 3 contains of a curated set of instances from MIPLIB, which are benchmark instances to test MIP solvers from Hans Mittelmann (https://plato.asu.edu/ftp/milp.html). And to make full comparison, we have also tested the whole 240 benchmark instances from MIPLIB in Appendix A.5, and detailed performance analysis are reported in Appendix B. In general, our proposed BTBS-LNS can achieve competitive performance even on these standard cross-distribution instances."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5004/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700054420782,
                "cdate": 1700054420782,
                "tmdate": 1700054420782,
                "mdate": 1700054420782,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zr3IaVJ0hU",
            "forum": "Qwxe8WKSgy",
            "replyto": "Qwxe8WKSgy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5004/Reviewer_38AJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5004/Reviewer_38AJ"
            ],
            "content": {
                "summary": {
                    "value": "A deep learning-based approach for providing primal solutions to mixed-integer programming (MIP) problems is described. The primary contributions are given as follows. First, the paper proposes \"binarized tightening\" for variable encoding / adjusting bounds during search. Second, a tripartite graph model is proposed in which the \"standard\" bipartite graph model is extended with nodes representing the objective function. Third, the paper uses a large neighborhood search fix-and-resolve strategy and claims this is a variable branching strategy. Fourth, they compare the approach to SCIP and Gurobi on the several MIP datasets and claim good performance.\n\nOverall, this is a paper that unfortunately is not good enough for acceptance. This is especially disappointing because it has some fascinating ideas; I think both the tripartite graph and the bound tightening, if properly explained, could be important contributions.\n\n=====\nI have raised my score from a 3 to a 5. See my comment for more information."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The tripartite graph structure is an interesting idea. One might argue that the objective information was already present in the graph, so this isn't necessary to add in this way, however I think the objective nodes could provide an interesting flow of information between parts of the problem that are connected through objective components.\n\n2. The binarized tightening is indeed new for the learning to optimize literature. It reminds me of tightening schemes in constraint programming, so it would be nice if the authors could make that connection in their work. There is an imitation learning aspect to this that I am not sure is really so new, but in general the encoding of the variables is interesting.\n\n3. The performance of the approach is quite promising."
                },
                "weaknesses": {
                    "value": "1. This paper is not well written and unfortunately this results in a variety of issues. Please have a professional proofreader check for typos, there are several including in the abstract (e.g., there must be a \"the\" before \"Mixed\").\n\na. Key contributions of the paper are barely even discussed in the main text. Information about the branch and search method, tripartite graph, etc., is all in the back. And these are not just details, in my opinion. This paper basically abuses the page limit with lots of redundant text (BTBS is defined several times...) and then expects the reviewer to read the appendix to actually understand what is going on. Sorry, I am not willing to do that. Furthermore, the bound tightening scheme seems somewhat arbitrary -- there are many tightening/filtering strategies, see the CP literature.\n\nb. There are experiments at the start of the paper before the reader can even really understand what is being tested, on what instances, etc. This is essentially a visualization for marketing purposes rather than science.\n\nc. The conclusion is essentially non-existent, nor do the authors discuss weaknesses of their work.\n\n\n2. I take great issue with the big claim in the abstract that the approach performs competitively with Gurobi and SCIP. As best I can tell, this approach is a heuristic, meaning this is an apples and oranges comparison. If this approach actually finds optimal solutions and provides a proof, then I truly have no idea how, which only furthers my first argument.\n\n3. The paper makes several claims/arguments that I just do not follow/support.\n\na. Backdoor variables: I see no theoretical or empirical proof that this paper finds them. It is just a conjecture that it is finding backdoors; we do not really know. The seminal paper on backdoors from Williams, Gomes and Selman (\"Backdoors to typical case complexity\") is not even cited. \n\nb. The paper argues that current approaches \"becom[e] ensnared in local optima\". The implication is that this method does not, but of course, it does. This is a sentence that can be written about essentially any heuristic method and is thus not a valid argument for this paper about why it is novel or better than anything else. Table 1 continues this misconception and basically gets everything wrong about metaheuristics and \"addressing local optima\". Succinctly put, LNS itself is a high level strategy for avoiding local optima, the adaptive neighborhood size of other approaches is also a mechanism for doing so. The header just makes no sense for categorizing the techniques.\n\nIn the example with (3), I do not view this issue as a fundamental flaw of LNS. LNS is just a framework that can be implemented however the user likes. They are correct that fixing integer variables to specific values in their domains is ineffective (again, see the past 20 years of the constraint programming literature), but this has nothing to do with LNS as a framework.\n\n4. The experiments are all given 200 seconds for solving. What a surprise that SCIP and Gurobi do not solve many instances that well in that time frame. It is totally unreasonable and completely disconnected with the time limits that one would have when solving a general MIP problem. Generally ten minutes is considered a reasonable amount of time for a heuristic on a large-scale real-world problem. For solving to optimality with SCIP or Gurobi, one would usually allow for 24 (or more) hours.\n\nFurthermore, the experiments do not provide sensitivity analysis or ablation in the main text beyond comparing different branching schemes. Maybe there is more in the appendix, but those are critical details for the main text."
                },
                "questions": {
                    "value": "1. Can this method prove optimality?\n2. Is there an inherent reason why the bound tightening works in powers of two?\n3. Why do you claim that MIPs are more difficult than IPs? Please cite the CS theory that says this is the case."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5004/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5004/Reviewer_38AJ",
                        "ICLR.cc/2024/Conference/Submission5004/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5004/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698661502893,
            "cdate": 1698661502893,
            "tmdate": 1700559884216,
            "mdate": 1700559884216,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "olEGd5R8Ly",
                "forum": "Qwxe8WKSgy",
                "replyto": "zr3IaVJ0hU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5004/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5004/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the reviewer 38AJ (Part 1)"
                    },
                    "comment": {
                        "value": "We thank reviewer 38AJ so much for the careful review and insightful comments! Below we would like to illustrate some details.\n\n> **Q1**: This paper is not well written and results in a variety of issues and typos.\n\nThanks for your suggestions. We will check the overall expressions and writing in detail and correct some typos in the updated paper version.\n\n> **Q2**: Key contributions of the paper are barely even discussed in the main text. Furthermore, the bound tightening scheme seems somewhat arbitrary -- there are many tightening/filtering strategies, see the CP literature.\n\nThanks for your valuable suggestions. We will place some important components, e.g., branch and search method, tripartite graph, back to the main text if page permits. \n\nAs for the bound tightening scheme, it was designed for general integer variables. The intuition is to balance between complexity and exploration scope. In this respect, we designed a binarize-then-tighten scheme. The binarization was utilized to encode them with several substitute decision variables, utilized to evaluate the reliability of current solution at different significance. And afterwards, the bitwise LNS decisions were applied to the bound tightening sequentially. When either bit indicates the current solution reliable, we tighten the bounds that maintains a pivotal position close to (or precisely at) the midpoint of the recalibrated bounds. We argue that when the current solution sits precisely at the midpoint of variable bounds, no further bound tightening should be performed as it shows no preference for either direction, which drives us to design the current bound tightening scheme, tightening the bounds on the far side iteratively.\n\n> **Q3**: There are experiments at the start of the paper before the reader can even really understand what is being tested, on what instances, etc. This is essentially a visualization for marketing purposes rather than science.\n\nWe really appreciate your insightful comments. The existence of Fig.2 was utilized to intuitively illustrate the potential limitations and drawbacks of current learning based LNS approaches on instances from Maxcut and MIPLIB. However, it may be confusing for the readers to some extent in the methodology section. It may be better to illustrate the limitations in a clear way.\n\n> **Q4**: The conclusion is essentially non-existent, nor do the authors discuss weaknesses of their work.\n\nThanks for your suggestions, and currently the conclusion and weaknesses given in Sec 5 were not sufficient. In general, we have proposed a binarized tightening branch and search approach to learn LNS policies in this paper. It was designed to efficiently deal with general MIP problems, and delivers superior performance over competing baselines on ILP, MIP datasets and even heterogeneous instances from MIPLIB. Sufficient ablation studies demonstrate the effectiveness of each component, including the tripartite graph, binarize and tighten scheme, and the extra branching at each step.\n\nHowever, the proposed BTBS-LNS is only a primal heuristic to search for better feasible solutions, while cannot prove optimality, which are also common limitations of LNS-based approaches. Implementing them into MIP solvers as primal heuristics may be a possible solution. However, interaction with current existed primal heuristics, and the rule to take effect, are key challenges in practical implementation. In general, applications of the learning-based approach in real-world scenarios will be our future directions.\n\n> **Q5**: I take great issue with the big claim in the abstract that the approach performs competitively with Gurobi and SCIP. As best I can tell, this approach is a heuristic, meaning this is an apples and oranges comparison.\n\nThanks for your valuable comments. Our proposed approach is an improved primal heuristic to search for better feasible solutions. Similar to all the heuristic based LNS methods, it cannot prove optimality. As you mentioned, MIP solvers, like Gurobi and SCIP, will simultaneously find feasible solutions and prove the optimality. To make fair comparison, both Gurobi and SCIP were fine-tuned with the aggressive mode to focus on improving the objective value, rather than proving optimality. It follows the same protocol as previous publications [1, 2, 3]. Compared with the hybrid multiple primal heuristics implemented in MIP solvers, our proposed BTBS-LNS can deliver better solutions within the same timelimit, demonstrating its effectiveness.\n\n**Note that owing to the limited characters of only one response and we want to answer your questions in detail, we have to add another reply in the following!**"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5004/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700054065836,
                "cdate": 1700054065836,
                "tmdate": 1700054872670,
                "mdate": 1700054872670,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "M2F7dT6nZu",
                "forum": "Qwxe8WKSgy",
                "replyto": "zr3IaVJ0hU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5004/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5004/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the reviewer 38AJ (Part 2)"
                    },
                    "comment": {
                        "value": "**Owing to the limited characters of only one response and we want to answer your questions in detail, we have to add another reply in the following!**\n\n> **Q6**: The paper makes several claims/arguments that I just do not follow/support, e.g., backdoor variables.\n\nThanks for your insightful comments. Backdoor variables refer to a set of variables, such that when set correctly, the sub-solver can solve the remaining problem [4]. LNS may benefit from the backdoor variables due to its partial fixing mechanism. If we can find the backdoors and fix them correctly as soon as possible, the LNS may deliver significantly better performance. In this paper, we extend the meaning of backdoor variables to those with different solutions compared with global optimum (**BTBS-LNS-G**), which may be confusing to some extent. We have clarified its meaning in the abstract. To identify and optimize those variables efficiently, we propose an extra branching policy on top of LNS at each step.\n\n> **Q7**: The paper argues that current approaches \"become ensnared in local optima\", which has to be explained\n\nWe really appreciate your comments, and we will update some expressions to make it clear. As you mentioned, the descriptions for LNS and local optimum may be inaccurate. In fact, the drawbacks and limitations of current learning based LNS approaches may derive from limited learning accuracy or performance. Driven by the learning complexity of MIP problems, current approaches may be easy to be ensnared in local optima. It has nothing to do with LNS as a framework. In other words, **what we claimed \"ensnared in local optima\" totally refers to the non-optimal learning based LNS policies, rather than the LNS itself**. Hybrid policies or adaptive neighborhood size were both strategies to deal with the limitations of learning-based approaches. Specifically for our BTBS-LNS, it can be regarded as an improved LNS approach by identifying and optimizing the potentially wrongly decisions by the learned LNS policy. And therefore, it can help the LNS decisions getting closer to global optimum, jumping out of local optimum in some cases.\n\n> **Q8**: The experiments are all given 200 seconds for solving, which may be unreasonable.\n\nThanks for your comments. As illustrated above, our BTBS-LNS was an improved primal heuristic to search for better feasible solutions, while not prove optimality. In MIP solvers, multiple primal heuristics are called iteratively at each branch and bound node, with strong time/iteration/node constraints to ensure efficiency. They try to find feasible solutions of good quality in a reasonably short period of time [5]. And to make fair comparison, both Gurobi and SCIP was fine-tuned with the aggressive mode to focus on improving the objective value, rather than proving optimality. The time limit 200s was set following similar protocols with the previous publication [1], and within 200s, some approaches can already deliver quite close gaps compared with the global optimum on most of the instances. To make comparisons in detail, anytime performance within 200s timelimit was reported in Fig.4 and Fig.6, 7, 8, 9 in the appendix, revealing the superior performance over competing baselines almost anytime from 0s to 200s.\n\n> **Q9**: The experiments do not provide sensitivity analysis or ablation in the main text beyond comparing different branching schemes. Maybe there is more in the appendix, but those are critical details for the main text.\n\nThanks for your valuable suggestions. In Sec 4.4, we make sensitivity analysis on the branching variable ratios. And detailed sensitivity analysis on other important hyperparameters will be supplemented in our next round of response.\n\nAs for the ablation study, we tested the degraded versions of our BTBS-LNS by removing each component separately, including\n- **LNS-TG**: where we replace the tripartite graph with the widely used bipartite graph.\n- **LNS-Branch**: where we remove the branching policy.\n- **LNS-IBT**, **LNS-IT**: where we remove the binarize & tighten mechanism separately.\n- **LNS-ATT**: where we replace our attention-based graph attention network with the widely used GNN.\n\nAs can be seen from Table 1, 2, 3, they all performed slightly worse, revealing the effectiveness and necessity of each component.\n\n**Note that owing to the limited characters of only one response and we want to answer your questions in detail, we have to add another reply in the following!**"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5004/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700054260232,
                "cdate": 1700054260232,
                "tmdate": 1700055607363,
                "mdate": 1700055607363,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pCaUoTKMcC",
                "forum": "Qwxe8WKSgy",
                "replyto": "zr3IaVJ0hU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5004/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5004/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the reviewer 38AJ (Part 3)"
                    },
                    "comment": {
                        "value": "**Owing to the limited characters of only one response and we want to answer your questions in detail, we have to add another reply in the following!**\n\n> **Q10**: Is there an inherent reason why the bound tightening works in powers of two?\n\nWe really appreciate your valuable comments. Intuitively, we utilized a similar binarization encoding scheme to efficiently control the encoded variable counts, avoiding heavy computational costs. And on the other hand, considering that the general integer variables may deliver different ranges, an adaptive bound tightening scheme may be more suitable to deal with the dynamic ranges across variables. With the current scheme, each encoded variable can represent the original integer variable at different significance, utilized to sequentially tighten the bounds.\n\n> **Q11**: Why do you claim that MIPs are more difficult than IPs? Please cite the CS theory that says this is the case.\n\nSorry for some confusing expressions, and we will update them to make it clear. In this paper, the difficulty of MIP compared with IPs mainly lie in that the continuous variables may require different treatment in learning-based approaches, as they can rarely be fixed. While in general, the difficulty of both MIP and IPs can vary depending on the characteristics of the specific problems being considered. Different problem instances may present unique challenges that impact the complexity and feasibility of finding optimal solutions.\n\n**References**:\n\n[1] Wu, Yaoxin, et al. \"Learning large neighborhood search policy for integer programming.\" Advances in Neural Information Processing Systems 34 (2021): 30075-30087.\n\n[2] Song J, Yue Y, Dilkina B. A general large neighborhood search framework for solving integer linear programs[J]. Advances in Neural Information Processing Systems, 2020, 33: 20012-20023.\n\n[3] Huang T, Ferber A M, Tian Y, et al. Searching large neighborhoods for integer linear programs with contrastive learning[C]//International Conference on Machine Learning. PMLR, 2023: 13869-13890.\n\n[4] Williams R, Gomes C P, Selman B. Backdoors to typical case complexity[C]//IJCAI. 2003, 3: 1173-1178.\n\n[5] Achterberg T. Constraint integer programming[J]. 2007."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5004/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700054349793,
                "cdate": 1700054349793,
                "tmdate": 1700055582272,
                "mdate": 1700055582272,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YlIIr5l3WU",
                "forum": "Qwxe8WKSgy",
                "replyto": "pCaUoTKMcC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5004/Reviewer_38AJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5004/Reviewer_38AJ"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the effort the authors have taken to try to respond to the paper's weaknesses, but let me first emphasize that I believe the comment field has a character limit for a good reason.\n\nThere is no revision process here, and I cannot possibly re-read the paper to see if it is actually well-written now. The best I can offer is that I have looked through it, and it honestly does not look that changed. The contributions at the beginning are a great change, but you still mix experiments into the beginning of the paper. \n\nI am not satisfied with the connection to constraint programming here; bounds tightening was not invented in your paper, and I see no connection to the existing literature.\n\nI am willing to increase my score to a 5, acknowledging some improvements. But I am not an advocate for this paper at this time, sorry. I believe you have some nice ideas in here, and I am confident that when you have the time to really re-work the paper you will have a very successful submission."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5004/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700559841626,
                "cdate": 1700559841626,
                "tmdate": 1700559841626,
                "mdate": 1700559841626,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HYMVfWOgis",
            "forum": "Qwxe8WKSgy",
            "replyto": "Qwxe8WKSgy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5004/Reviewer_5yM1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5004/Reviewer_5yM1"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents an ML-guided LNS framework for MIPs. It combines a binarize-and-tighten scheme to address general integer variables and a branching policy to help escape local optima. It also has a search policy to guide destroy variables. In the experiment, the new method is compared against a variety of ML-guided approaches and heuristic approaches. The presented results show that the proposed method finds better solutions at a faster speed."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper proposes a combination of ML-guided search policy and branching policy for LNS in MIP solving. Various techniques and engineering designs are proposed. The novelty of the paper comes from those details. \n\n2. The effectiveness is demonstrated in experiments with different settings and ablation studies."
                },
                "weaknesses": {
                    "value": "1. The writing for some important parts is difficult to understand. I don\u2019t follow the argument for the BTBS scheme. Though intuitively the method itself makes sense to me. In figure 1, are the neighborhood search and branching policy two separate local searches in one iteration? From the pseudocode in Appendix, it looks like you destroy and repair the variables you got by taking the intersection of the branching and LNS policies.  But in other places, you said the branching policy is on top of the LNS.\n\n2. Details on how you implement the branching policy are totally missing, e.g., how you collect data and train. The variant BTBS-LNS-L looks very similar to Sonnerat et al. The design for the branching policy is not well-justified conceptually.\n\n3. Minor weakness: The experiment is comprehensive with lots of information, but without a summary it makes readers get lost. It would be nice to summarize the takeaways, especially for those ablation studies."
                },
                "questions": {
                    "value": "See the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5004/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698817245382,
            "cdate": 1698817245382,
            "tmdate": 1699636488242,
            "mdate": 1699636488242,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MPT4n7uRLe",
                "forum": "Qwxe8WKSgy",
                "replyto": "HYMVfWOgis",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5004/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5004/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the reviewer 5yM1 (Part 1)"
                    },
                    "comment": {
                        "value": "We sincerely thank Reviewer 5yM1 for the detailed feedback and precious suggestions! Below we address every comment in detail.\n\n> **Q1**: The writing for some important parts is difficult to understand. In Fig.1, are the neighborhood search and branching policy two separate local searches in one iteration?\n\nThanks for your comments. The large neighborhood search (LNS) and branching are two separate learned policies. At each iteration, current problem states will be firstly fed into the learned LNS policy network, and obtain the fix/unfix decisions, denoted as $n_i^t$. Afterwards, the neighborhood search decisions, along with the problem states, will serve as inputs for the branching networks (see Sec 3.4 for detail), and the branching decisions for the potentially wrongly fixed variables by LNS can be obtained, denoted as $b_i^t$. In this respect, branching policy are indeed on top of LNS to optimize some potentially wrong LNS decisions. Then as illustrated in Alg.2, $n_i^t$ and $b_i^t$ are utilized to jointly optimize the current solution.\n\n> **Q2**: Details on how you implement the branching policy are totally missing, e.g., how you collect data and train. The variant BTBS-LNS-L looks very similar to Sonnerat et al. The design for the branching policy is not well-justified conceptually.\n\nThanks for your valuable suggestions. Training pipeline for the branching policy was briefly introduced in Alg.3 in the appendix. Specifically, we propose to learn the branching policy with an offline manner. The inputs are tripartite graph-based features (same as LNS policy learning), where we additionally append the LNS decisions at each step as variable features, as we only focused on the fixed variables for extra branching. In this respect, branching can be seen on top of LNS, which was significantly different from Sonnerat et al [1]. Then the graph-based features are fed into a similar graph attention network as described in Sec 3.3 to update the node/edge representations. We finally process the variable nodes by a multi-layer perceptron to obtain the branching probability for each variable at this step. Cross-entropy loss was utilized to train the branching network to bring the outputs closer to the collected labels\n\nSpecifically for the data collection, with the learned LNS policy, we solve the training instances for each problem again. The tripartite graph-based features were collected at each step, along with the LNS decisions. Label collection were different between the two branching variants. In the local branching, labels are collected by incorporating the following constraint at each step:\n\n$$\\sum\\limits_{i \\in {\\mathcal B} \\cap {\\mathcal F}} {|x_i^{t + 1} - x_i^t| \\le k}$$\n\nwhere ${\\mathcal F}$ denotes the fixed variables selected by LNS. With this extra constraint, the re-defined sub-MIP can be solved, and up to $k$ branching variables are selected from ${\\mathcal F}$, serving as branching labels. In the global branching variant, it gathers labels also from the fixed variables by LNS at each step and contrast them with the global optimal solution. Variables that exhibit differing values are indicative of potentially misclassified variables within the current LNS decisions.\n\nIn general, the intuition of the branching component is to select and optimize those wrongly fixed variables by the learned LNS policy at each step. The global and local branching variants are designed at a global (local) view, to identify the variables that sit far from the global optimum and optimize them with an extra branching process. To further evaluate its effectiveness, we compared our BTBS-LNS with LNS-Branch, where we remove the extra branching policy at each step. Below are the results on complex MIP problems (item placement and AMIPLIB), and the full comparison results can refer to Table 1, 2, and 3.\n\n+ **Balanced item placement**\n\n|Methods|Obj|Gap%|Primal Integral|\n|:--:|:--:|:--:|:--:|\n|LNS-Branch|20.12|43.90|3537.0|\n|BTBS-LNS-L|13.82|16.82|2030.3|\n|BTBS-LNS-G|**13.45**|**15.78**|**1912.5**|\n\n+ **Anonymous MIPLIB (AMIPLIB)**\n\n|Methods|Gap%|\n|:--:|:--:|\n|LNS-Branch|9.32|\n|BTBS-LNS-L|**4.19**|\n|BTBS-LNS-G|4.35|\n\nAs can be seen, our BTBS-LNS achieves consistently and significantly superior performance over LNS-Branch, revealing the necessity of extra branching on top of LNS.\n\n**Note that owing to the limited characters of only one response and we want to answer your questions in detail, we have to add another reply in the following!**"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5004/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700053796064,
                "cdate": 1700053796064,
                "tmdate": 1700053796064,
                "mdate": 1700053796064,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]