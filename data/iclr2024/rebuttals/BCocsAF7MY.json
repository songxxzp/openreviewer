[
    {
        "title": "Fine-tune Language Models to Approximate Unbiased In-context Learning"
    },
    {
        "review": {
            "id": "cIcouTFKbU",
            "forum": "BCocsAF7MY",
            "replyto": "BCocsAF7MY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2203/Reviewer_NqVg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2203/Reviewer_NqVg"
            ],
            "content": {
                "summary": {
                    "value": "This paper uses learning-based approaches to approximate the un-biased in-context learning (ICL). The performance of ICL is highly dependent on quality of demonstrations such as imbalanced or biased demos; this paper investigates the phenomenon in the context of SoftMax self-attention that is built on prior works. Authors also performed experiments to compare their RICL and LARICL algorithms against other fine-tuning and prefix tuning methods on different distribution prefixes."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Different from previous in-context learning (ICL) that is focused on linear self-attention (LSA) with linear tasks, based on the pioneering work on ICL for attention scheme (Gao et al 2023), this work focuses on the training process of the attention mechanisms and decompose it to SoftMax regression following Deng et al 2023. Authors attempt to establish that ICL achieves implicit parameter optimization by minimizing the SoftMax regression loss."
                },
                "weaknesses": {
                    "value": "The reviewer concerns on unclear presentation of both methods and experiments. In the former, it is not clear to me what\u2019s their contribution given the cited prior works by Gao et al 2023 and Deng et al 2023. Is it application of the reweighting algorithms on the demonstration examples? In the latter, it is not clear to me about their experimental setup, data set generation, pre-training etc. In Section D of the Appendix, it appears that they assume the input-output demo pairs are composed of a soft prompt with the corresponding output, is it class label or continuous output? What is the real-world task it can correspond to? Change-of-Thought? Using a real-world data set/task with hard prompts can be more meaningful since soft prompt might not make semantic sense when mapping back to hard prompts or cannot even be mapped back to meaningful hard prompts."
                },
                "questions": {
                    "value": "What\u2019s the original contribution of the SoftMax regression objective proposed here compared to Deng et al?\n\nWhat are the ICL learning tasks you have used in your experiments? If soft prompts have been used, how do you ensure it maps to semantically meaningful hard prompts?\n\nWhat do you mean by imbalance or bias at all? How does your reweighting approach compare with the simple resampling approaches that simply make the ICL demos balanced and/or unbiased?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2203/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2203/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2203/Reviewer_NqVg"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2203/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698786038904,
            "cdate": 1698786038904,
            "tmdate": 1699636154226,
            "mdate": 1699636154226,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "gL2yu5kQZz",
            "forum": "BCocsAF7MY",
            "replyto": "BCocsAF7MY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2203/Reviewer_A8do"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2203/Reviewer_A8do"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes re-weighted in-context learning (ICL) to address the biased and imbalanced input prompts for ICL. The main idea is to fine-tune language models on an unbiased validation set, and to learn the optimal weight for each few-shot example."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper points out an interesting problem, i.e., the quality or weight of the prompt / few-shot examples might affect the ICL performance, and it remains an interesting study about how to weight / select the optimal prompts for efficient ICL.\n\n- The paper proposes a neat method of reweighing the embedding vectors of the prompts, where the weight is learned via an unbiased validation set.\n\n- The method is proved promising on some synthetic datasets."
                },
                "weaknesses": {
                    "value": "- I found the paper is super unclear, which makes it difficult to understand the main contribution. Most of the part is devoted to understand the relationship of ICL and softmax regression, which is based on existing work. If I understand it correctly, it basically then studies the re-weighted softmax regression (both theoretically and empirically) instead, to somehow equivalently showing that the study is valid for ICL. Though I appreciate the simplification here, I doubt the applicability of the method on large language models.\n\n- The empirical results are purely based on synthetic numerical datasets, instead of any language models, I am not fully convinced by the empirical results here. It would be great if the author could perform more empirical studies based on existing language models, instead of the \"toy examples\". Also, it would be great to compare the proposed methods with other PEFT methods on language tasks as well. \n\n- The paper discusses the \"unbiased\" all the time, but i found even a definition of \"unbiasedness\" is missing in the paper. What unbiasedness refers to in language models? \n\n- The motivation of the paper is also not well-supported. It would be great to add some understanding on how existing language models assigns weights on various few-shot examples? how the diversity/quality of these examples make a difference for ICL?"
                },
                "questions": {
                    "value": "See Weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2203/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699137466708,
            "cdate": 1699137466708,
            "tmdate": 1699636154159,
            "mdate": 1699636154159,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "iboZh7rnyJ",
            "forum": "BCocsAF7MY",
            "replyto": "BCocsAF7MY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2203/Reviewer_sUUz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2203/Reviewer_sUUz"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents RICL and LARICL, algorithms to fine-tune language models to estimate the optimal weights for each in-context example. In-context learning is highly susceptible to the input-output examples, leading to bias and imbalanced learning. This paper addresses this issue by learning parameters implicitly for in-context learning, reweighting the input vectors appropriately to mitigate the biases and imbalances."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper addresses an important problem of ICL being noisy. It proposes learning weights for input vectors to enable unbiased ICL. \n2. The authors also extend their algorithm to linear approximation to minimize the training cost of fine-tuning language models.\n3. They also study the convergence of their proposed algorithm by first establishing the smoothness of the gradients."
                },
                "weaknesses": {
                    "value": "1. Notation clarification: lot of notations have been used before defining them. For example, on page 2, it is not clear what is R, n and f_i? \nSimilarly in Theorem 3.4, what does x_t \\leq R and x_{t+1} \\leq R mean? \n2. The bounds presented in theorems seem to be pretty loose, seems like the approximation error will increase with dimension of examples? Also, how does it behave with number of examples?\n3. From Fig. 2, prefix-tuning looks like a great contender to proposed algorithms and if I understand correctly, prefix-tuning is less computationally expensive than proposed algorithms. \n4. Experiments in the paper are on data generated by the authors. It will be more effective if RICL was able to demonstrate bias mitigation on some public dataset (as simple as few-shot classification tasks)."
                },
                "questions": {
                    "value": "1. Just so that I understand better, what if the in-context examples were repeated, A_1=A_2=....A_m. How does RICL would result in more unbiased performance compared to say vanilla ICL? Or does it need some \"diversity\" in examples?\n2. How valid are the assumptions in real-world scenario, especially the ones needed for Lipschitz-smooth gradients"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2203/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2203/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2203/Reviewer_sUUz"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2203/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1700537742523,
            "cdate": 1700537742523,
            "tmdate": 1700537742523,
            "mdate": 1700537742523,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]