[
    {
        "title": "Replay across Experiments: A Natural Extension of Off-Policy RL"
    },
    {
        "review": {
            "id": "tHBxwttqn4",
            "forum": "Nf4Lm6fXN8",
            "replyto": "Nf4Lm6fXN8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5980/Reviewer_gauC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5980/Reviewer_gauC"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes Replay across Experiments (RaE) which is based on the simple concept of reusing experience from previous experiments to improve exploration and bootstrap learning. The proposed approach is employed with a number of existing algorithms in locomotion and manipulation tasks and shown to improve the learning efficiency."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The described approach is simple. It could be impactful as it is mostly algorithm-agnostic."
                },
                "weaknesses": {
                    "value": "The main drawback is the lack of explicit explanations for the improvements observed. The limitations, perhaps such as increased storage memory could also be emphasized."
                },
                "questions": {
                    "value": "1.\tWhat motivated the reuse of previous experimental data? There is no explicit explanation for this.\n\n2.\tDo the age of trajectories matter? Eg: Suppose there is a limit on the data storage, would it be more valuable to add older trajectories to the data buffer or relatively newer ones?\n\n3.\tIn the interest of comprehensiveness, I would have liked to see the benefits of this approach in Atari games as well. Does the approach improve the performance of say, DQN in Atari environments? Performance with methods like prioritized experience replay would also be interesting.\n\n4.\tThe results in Table 1 are interesting. However, the reasons for the observed trends are not clearly explained.\n\n5.\tIn Fig 1, the size of the \u2018Data\u2019 blocks is increasing, but is not immediately noticeable. It would be better to exaggerate the increase in block size to bring a reader\u2019s attention to the data accumulation mechanism."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5980/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5980/Reviewer_gauC",
                        "ICLR.cc/2024/Conference/Submission5980/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5980/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698584557326,
            "cdate": 1698584557326,
            "tmdate": 1700692221660,
            "mdate": 1700692221660,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Xs0oZjmo8U",
                "forum": "Nf4Lm6fXN8",
                "replyto": "tHBxwttqn4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5980/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5980/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer gauC"
                    },
                    "comment": {
                        "value": "Thank you for your helpful and positive feedback. We have updated the paper with suggested changes highlighted in red and are confident that the presentation is stronger thanks to them. We address specific questions below and have added a general comment for points shared across reviews. Please do not hesitate to ask for further clarifications.\n\n* What motivated the reuse of previous experimental data\n\nWe demonstrate how reusing previous data improves performance across a range of diverse domains and algorithms. Related work has demonstrated a similar effect to some degree albeit with additional algorithmic complexities that render them difficult to implement. Our focus in this work is on analyzing an underlying simple core idea more directly. With the increasing proliferation of offline datasets for robotics domains, RaE is an effective tool that can be incorporated into existing algorithms and fits naturally into existing RL experimentation workflows.\n\n* \u201c..would it be more valuable to add older trajectories to the data buffer or relatively newer ones?\u201d\n\nThe results of Table 1 indicate that the best way to use RaE with limited data is to combine trajectories sampled randomly throughout training (Mixed return) with a higher ratio of online to offline data (70-80 % works quite well). We have emphasized this point in the text.\n\n* Benefits of RaE on Atari games\n\nThe `Random resets\u2019 baseline we consider in our work is closely related to the work of Nikishin et al. [1]. They demonstrate an improvement of performance on Atari domains albeit with different underlying algorithms. We demonstrate RaE outperforms random resets across a range of diverse environments and algorithms with continuous action spaces; a finding which is likely to carry over to the Atari domain.\n\n* Reasons for results in Table 1.\n\nTable 1 presents an analysis of choosing different mixing ratios when running RaE with smaller datasets (10,000 and 100,000 episodes). Importantly, for all the main results shown in Figure 3 and 4 a fixed 50-50 ratio works well across domains. However, when the amount of data is restricted, using more online data is beneficial and prevents overfitting to a narrow distribution of offline trajectories. We have clarified this point in the description of Table 1.\n\n* \u201c..exaggerate the increase in block size to bring a reader\u2019s attention to the data accumulation mechanism.\u201d\n\nThank you for the suggestion. We have updated the paper with a new version of the figure that highlights the increasing block size of the data on each iteration. Please let us know if further improvements can be made.\n\n[1] Nikishin, Evgenii, et al. \"The primacy bias in deep reinforcement learning.\" International conference on machine learning. PMLR, 2022."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5980/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700141840468,
                "cdate": 1700141840468,
                "tmdate": 1700141840468,
                "mdate": 1700141840468,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FotPagR8iU",
                "forum": "Nf4Lm6fXN8",
                "replyto": "Xs0oZjmo8U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5980/Reviewer_gauC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5980/Reviewer_gauC"
                ],
                "content": {
                    "title": {
                        "value": "Thanks"
                    },
                    "comment": {
                        "value": "Thanks for your responses. \n\nRegarding the reuse of old/new trajectories - As the authors pointed out, using a mixed strategy works well. However, mixed return samples data uniformly. My question was - how would the performance be affected if this sampling was not done uniformly, but proportional (or inversely proportional) to recency? Perhaps this is an additional set of ablations that the authors could consider including.\n\nRegarding Atari experiments - Although RaE shows superior performance over the `Random resets' baseline (which shows relatively improved performances in Atari), it would be better to show RaE's actual performances in Atari, as the attributes of that environment (with image-based observations and discrete actions) are fundamentally distinct from the ones shown in the current paper. However, I understand if the experiments cannot be added at this stage, given the limited time frame.\n\nRegarding my point on bringing attention to the data accumulation mechanism, my suggestion was to increase the sizes of the second and third `Data' blocks relative to the first one, but I leave this choice to the authors."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5980/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700440436866,
                "cdate": 1700440436866,
                "tmdate": 1700440436866,
                "mdate": 1700440436866,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yCSNkGITsu",
                "forum": "Nf4Lm6fXN8",
                "replyto": "cn5gYwcZnq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5980/Reviewer_gauC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5980/Reviewer_gauC"
                ],
                "content": {
                    "title": {
                        "value": "Inaccurate rephrasing"
                    },
                    "comment": {
                        "value": "I noticed that Reviewer wVcc's comments are slightly misrepresented here. The reviewer's comment was that it is great that this paper studied an understudied topic, and not that this is a great paper that discusses an understudied topic. It is just a subtle difference in wording, but I believe it misrepresents the original comment. I assume this was unintentional, but I just wanted to point it out to give the authors an opportunity to correct it."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5980/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700441374982,
                "cdate": 1700441374982,
                "tmdate": 1700441374982,
                "mdate": 1700441374982,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Y9RYcLaf9J",
                "forum": "Nf4Lm6fXN8",
                "replyto": "ko9yKT8Wk3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5980/Reviewer_gauC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5980/Reviewer_gauC"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your responses and clarifications. You are right - some of the environments are indeed image-based. Apologies for my oversight."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5980/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700616805916,
                "cdate": 1700616805916,
                "tmdate": 1700616805916,
                "mdate": 1700616805916,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QYQNUkoobt",
            "forum": "Nf4Lm6fXN8",
            "replyto": "Nf4Lm6fXN8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5980/Reviewer_u3fY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5980/Reviewer_u3fY"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a method to reuse experience from prior RL experiments and shows benefits across a variety of RL algorithms and experiments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The authors present a very simple idea that leads to improved performance across a variety of environments.\n- The proposed method works well even with a small amount of prior data.\n- The proposed method works well even with low return offline data, which makes the method much more useful in practice."
                },
                "weaknesses": {
                    "value": "- It is not clear to me if \"Total online steps\" in the figures includes the steps from prior experiments or not, so I'm concerned about the fairness of the comparison between RaE and baselines. If the authors can clarify this point then I may be willing to raise my score."
                },
                "questions": {
                    "value": "- I'm a bit confused about the difference between Random Weight Resetting and RaE. The authors write that \"Reloading data for an experiment restart implicitly involves resetting network weights\", which makes me think that RaE and Random Weight Resetting are very similar. However, this clearly isn't the case since RaE performs better. Can the authors clarify the difference?\n- Across all figures does \"Total online steps\" include the steps from prior experiments for RaE? If not, I'm concerned that it may not be a fair comparison.\n- The authors write, \"At the beginning of each training run, policy and value-function are re-initialized in line with stand-alone experiments\". I'm curious if re-initializing vs not re-initializing at the start of each training run makes a difference in performance?\n- Under \"Potential Limitations and Strategies for Mitigation\" the authors write that \"changes in dynamics or experimental settings might invalidate previously collected data.\" Have the authors actually tried experimenting with changing dynamics across training runs. I'd be curious to see how much of a negative effect this would actually have in practice."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5980/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5980/Reviewer_u3fY",
                        "ICLR.cc/2024/Conference/Submission5980/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5980/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698714635899,
            "cdate": 1698714635899,
            "tmdate": 1700711549332,
            "mdate": 1700711549332,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Zez0l6hjin",
                "forum": "Nf4Lm6fXN8",
                "replyto": "QYQNUkoobt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5980/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5980/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer u3fY"
                    },
                    "comment": {
                        "value": "Thank you for your helpful and constructive feedback. We have updated the paper with suggested changes highlighted in red and are confident that the presentation is stronger thanks to them. We address specific questions below and have added a general comment for points shared across reviews. Please do not hesitate to ask for further clarifications.\n\n* Total online steps\n\nWe have updated Figure 4 to include the data from the offline dataset. The relative performance of RaE against the baselines remains unchanged and importantly, RaE continues to show improved asymptotic performance on challenging domains.  We hope this addresses your original comment and will help improve your original score. If there are further questions, please let us know.\n\n* Difference between Random resets and RaE\n\nWhen restarting an experiment with RaE, in addition to resetting the weights of the network, the data distribution throughout learning utilizes a fixed ratio of mixing between offline and online data. In contrast, `Random resets\u2019 only reload network weights and then use data from the replay buffer for learning in which older data is overwritten with time. In addition, RaE is more practical to implement since a period to reset does not have to be chosen and fixed a-priori; experiments can be restarted and data mixed naturally during the course of experimentation with repeated iteration.\n\n* \u201cI'm curious if re-initializing vs not re-initializing at the start of each training run makes a difference in performance?\n\nThis is a very good point. The choice of re-initializing weights is orthogonal to RaE and can be incorporated on top of RaE. Figure 7 of Appendix C.2 highlights this by comparing the performance of RaE with and without reloading weights on the `Locomotion\u2019 domains. We find reinitializing weights can improve learning speed without hurting asymptotic performance.\n\n* \u201cHave the authors actually tried experimenting with changing dynamics across training runs.\u201d\n\nOur comment regarding changing dynamics alludes to a theoretical limitation where an extreme change in the dynamics could render the transitions in the offline data invalid. However, as you note, this may not be much of an issue in practice. To test this, we conducted an experiment presented in Appendix C.4 where we transfer data from the `Locomotion Soccer (State)\u2019 task to a new setting where a mass (of 300g) is randomly attached to the legs of the walker to perturb its motion. Surprisingly we find RaE works well in practice in this setting and can learn a partial solution to the task while learning online from scratch fails to learn any solution."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5980/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700141774289,
                "cdate": 1700141774289,
                "tmdate": 1700141774289,
                "mdate": 1700141774289,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "raRfbFyfAR",
                "forum": "Nf4Lm6fXN8",
                "replyto": "CsDmK6qzrr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5980/Reviewer_u3fY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5980/Reviewer_u3fY"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the clarifications! My main concern has been addressed, so I am raising my score."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5980/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700711681641,
                "cdate": 1700711681641,
                "tmdate": 1700711681641,
                "mdate": 1700711681641,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "B5kC603qE2",
            "forum": "Nf4Lm6fXN8",
            "replyto": "Nf4Lm6fXN8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5980/Reviewer_fgij"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5980/Reviewer_fgij"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose Replay across Experiments (RaE), in which all past experiment data is stored in the replay memory for training. In essence, data is never discarded. Data from all past experiment trials is stored and reused, not just the trajectories left in the buffer at the end of the previous experiment. The authors recommend a default mixture of 50-50 offline/online data (where \u201conline\u201d data is from the current experiment) to obtain good performance without tuning this hyperparameter. The authors compare RaE against other common strategies (fine-tuning, AWAC, and parameter resetting) combined with algorithms such as DMPO and D4PG in control benchmarks including Locomotion Soccer, Manipulation RGB Stacking, and RL Unplugged.\n\n***Rebuttal: score raised from 5 to 6**"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The method is simple to implement compared to many existing data-reuse RL methods.\n- The experiments in the paper are comprehensive, testing a multitude of diverse methods in a number of high-dimensional control environments. Several strong baselines from the literature are compared against. In spite of this simplicity, RaE can achieve strong performance in these tasks. The breadth of the results demonstrate the generality of RaE.\n- The paper is well organized and includes nice discussions for related work and practical use cases."
                },
                "weaknesses": {
                    "value": "- The main insight of the work, while interesting, is a small contribution. As the authors note in the background section, other methods already use data stored from previous experiments, so the only novelty here is that *data is not discarded*. There is no theoretical analysis in the paper. Without significant novelty or theory, the paper depends solely on its empirical results.\n- Storing all previous data is memory intensive, which is why offline RL generally uses more complicated techniques to learn from limited data. The authors do not discuss this drawback of their method, but I could see it being a bottleneck in long experiments with high-dimensional observations.\n- It is unclear that replaying offline data from a long time ago is as beneficial as the authors claim. Table 1 seems to indicate that performance improves almost monotonically as the proportion of online data increases to 90%. The authors claim that \u201cas more data becomes available, a lower ratio [of 70-80%] works better,\u201d but this only happens occasionally, and the performance improvement is small (about 2-5%). Without any measure of significance provided, it does not seem that more than 10% offline data actually helps much.\n- The results are rather noisy and would benefit from increasing the number of trials (which is currently only 5). I would recommend that the authors use a 95% confidence interval instead of standard deviation and apply a 100-episode moving average (if they are not already doing so already) to make the results easier to read. Currently, some of the standard deviations are overlapping, but I think the results would be significant if confidence intervals are used instead.\n\n**Minor edits:**\n- In-text citations are not rendered correctly on the last line of page 1.\n- In the background section, the range of discount values should be mentioned: $\\gamma \\in [0,1]$.\n- I think it would be helpful to the reader to define the scientific notation shorthand the first time it is used, e.g., $\\text{4e5} = 4 \\times 10^{-5}$."
                },
                "questions": {
                    "value": "1. What is meant by \u201cAccumulated Reward\u201d in the y-axes of Figures 3, 4? Is this the undiscounted episode return?\n1. What is meant by \u201cfrom scratch to convergence\u201d in Table 1\u2019s caption? Does that mean that exactly 100% refers to the final performance obtained by a pure online-data agent?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5980/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5980/Reviewer_fgij",
                        "ICLR.cc/2024/Conference/Submission5980/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5980/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698784335674,
            "cdate": 1698784335674,
            "tmdate": 1700721778066,
            "mdate": 1700721778066,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ILOXBRFl5C",
                "forum": "Nf4Lm6fXN8",
                "replyto": "B5kC603qE2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5980/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5980/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer fgij"
                    },
                    "comment": {
                        "value": "Thank you for your helpful and constructive feedback. We have updated the paper with suggested changes highlighted in red and are confident that the presentation is stronger thanks to them. We address specific questions below and have added a general comment for points shared across reviews. Please do not hesitate to ask for further clarifications.\n\n* \u201cThe main insight of the work, while interesting, is a small contribution.\u201d\n\nWe respectfully disagree on the scale of contribution of our work. As you note, we present a highly-effective, simple method that can easily be introduced to RL methods and provide a comprehensive evaluation showing good performance compared to several strong baselines across a breadth of domains. As we discuss in the main text, other works that use a similar idea do so in an incidental manner, and commonly focus on additional algorithmic changes that add considerable complexity. By focusing on the core approach and combining its simplicity with a strong empirical contribution we believe our work can translate to practical real world impact across a range of RL domains.\n\n* \u201cStoring all previous data is memory intensive\u201d \n\nWe agree that memory is not free, but as Reviewer wVcc points out, and as witnessed by the increasing availability of large datasets in RL and robotics [1], memory is relatively cheap. Large, shared datasets for robotics are becoming more widely available and are likely to be integrated into existing RL workflows. As we show, RaE improves asymptotic performance on many domains: in many cases it may be cheaper to scale up data storage rather than compute to achieve a similar final performance. And as illustrated by Table 1, we can achieve strong results by storing relatively small subsets of data randomly throughout training.\n\n* Confidence intervals instead of standard deviations.\n\nWe agree that the visual presentation of data is important.  We have replaced the error bars for the plots in Figure 3 (and Figure 6 in the Appendix) to instead show a 95% confidence interval over the mean after smoothing over 1000 episodes and have updated the description in Section 3.3 to reflect this.\n\n* 90% online data works best in Table 1\n\nThe results in Table 1 discuss how best to adapt RaE to regimes where data is sparse. As you point out, using more online data is a reasonable strategy in this setting. However note that mixing some offline data is preferable to learning purely offline and all the results in Figure 3 and 4 show an improvement in asymptotic performance with a simple 50/50 ratio of mixing data.\nAdditionally, we have updated the paper to include an additional result in the Appendix C.3 that illustrates the effect of mixing different data ratios when using the full dataset (of 4e5 episodes). This result represents many project settings and shows that when more data is available, a mixing ratio of 50 to 70 percent of online data works better than a mix with too much online data.\n\n* Accumulated Reward\n\nYes, by accumulated reward we mean the undiscounted episode return achieved by the agent. We have updated the captions of Figures 3 and 4 to reflect this.\n\n* Table 1 caption \u2018from scratch to convergence\u2019\n\nThe results in Table 1 present the performance of RaE with small subsets of data as a percentage of learning from scratch. By \u2018from scratch to convergence\u2019, we want to highlight that the agent performance being compared against is not at the 10,000 or 100,000 episode mark but with an online agent trained until it converges.\n\n* Minor edits\n\nThank you for pointing these out to us. We have made corrections to the text.\n\n\n[1] Open X-Embodiment: Robotic learning datasets and RT-X models. https://robotics-transformer-x.github.io."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5980/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700141694267,
                "cdate": 1700141694267,
                "tmdate": 1700141694267,
                "mdate": 1700141694267,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "e8mtFOw48q",
                "forum": "Nf4Lm6fXN8",
                "replyto": "ILOXBRFl5C",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5980/Reviewer_fgij"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5980/Reviewer_fgij"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for making the suggested paper updates. I do agree that the simplicity of the approach is valuable, and I also see that argument that memory is cheaper than compute. The method is very general and can be combined with most (off-policy) algorithms, so I think it can be impactful.\n\nGiven that the paper does not advance any new theories or extremely new ideas, though, I think it is really important to demonstrate beyond reasonable doubt that RaE is effective in practice. The current empirical evaluation is already quite extensive and I think there is a lot of potential in this regard. However, there are a lot of theoretical issues with off-policy training, like the effect of mixing stale data distributions on the fixed points of the function approximation, that worry me that the observed results will not generalize. See, for example, Figure 5 of [1] which shows the state weighting greatly impacts the solution quality, and the on-policy weighting usually leads to lower approximation error.\n\nWhat would help convince me is a clearer ablation study. Based on the results in Table 1 / Figure 8a, it is still not clear to me that mixing offline data is always helpful against pure online agent. This may stem from a misunderstanding on my part, but why is a 100% online-data mix not compared in this study, or is it? Is that what is meant by the 100% score relative to \"from scratch to convergence\"?\n\nWould it also be possible to show learning curves instead of summarized final performance (hopefully this isn't too hard if you still have the data)? In my opinion, the most interesting case is the mixed-data regime, since that is used by your main experiments. It also concerns me that the datasets used in these ablations are smaller than the ones used in your main experiment. In off-policy RL, it is not always the case that more data is better, due to the reasons I mentioned above. For example, see this paper that showed that a DQN-like agent can actually perform worse as the memory size increases [2]. I think the ablation would be much stronger if it closely matches your main experiment setup and clearly demonstrates that a variety of data mixtures are better than a pure online agent, throughout training and not just the final performance.\n\n[1] A Generalized Projected Bellman Error for Off-policy Value Estimation in Reinforcement Learning. Patterson, White, and White, 2022.\nhttps://arxiv.org/pdf/2104.13844.pdf\n\n[1] A Deeper Look at Experience Replay. Zhang and Sutton, 2017.\nhttps://arxiv.org/pdf/1712.01275.pdf"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5980/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700342496648,
                "cdate": 1700342496648,
                "tmdate": 1700342496648,
                "mdate": 1700342496648,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PIuc9sSLAI",
                "forum": "Nf4Lm6fXN8",
                "replyto": "DVjXrNge2k",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5980/Reviewer_fgij"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5980/Reviewer_fgij"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you again for the clarifications. I appreciate the new ablation results in Figure 8a. The comparison to 100% in the bar chart makes the performance improvement much clearer.\n\nSimilarly, would it be possible to add another row to Table 1 to show the 100% online performance in the same manner? This would obviously just be a row of 100%'s, which may seem redundant, but I think it would make the table's purpose clearer. Part of my initial skepticism of the method was because it appeared the 100% case was being intentionally ignored. The 100% row's values could be indicated in a different color (e.g., black) to make it very clear that it is the baseline. The caption could also updated to emphasize that the agent trained \"from scratch to convergence\" is a 100% online-data mix. All of these small changes would make the figure more readable in my opinion. \n\nI also think it would be great to briefly discuss some of the theoretical issues related to off-policy learning that I talked about above. The empirical results show that these are not too much of an issue in practice, but I think it is important to mention them.\n\nIn the meantime, I appreciate the improvements made to the paper. I have raised my score."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5980/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700721671621,
                "cdate": 1700721671621,
                "tmdate": 1700721671621,
                "mdate": 1700721671621,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "tjFfAv883U",
            "forum": "Nf4Lm6fXN8",
            "replyto": "Nf4Lm6fXN8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5980/Reviewer_wVcc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5980/Reviewer_wVcc"
            ],
            "content": {
                "summary": {
                    "value": "The paper discusses a new setting, where data is shared across experimental runs to improve the performance of the policy. A simple strategy, adding data from previous experiments to the initial replay buffer, is explored and shown to be an effective approach. Various factors are investigated including the quality of the data used and the amount of data kept. These experiments are done with a variety of policy optimizatino algorithms and robotics benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The approach of keeping data from previous experiments seems to be very relevant in practical scenarios where our main goal is to train an agent with strong performance. In that case, given the cheap cost of memory, it would be sensible to keep data from previous runs for the benefit of future experiments. \nThis seems like an understudied topic and it's great that this paper discusses it.\n\n- There's a nice variety of environments that are used, including some more complex ones with a good mix of algorithms too.\n\n- The writing and organization is clear, making the paper easy to read. The paper has a distinct focus which helps gets the message across too."
                },
                "weaknesses": {
                    "value": "- In the current paper, most of the experiments run the same learning agent on the same environment with the RaE algorithm but the method is pitched as being helpful for boosting learning between different experiments with potential differences in experimental conditions.\nSee Questions.\n\n- There are other simple algorithms for this across-experiment setting that would be interesting to investigate. See Questions."
                },
                "questions": {
                    "value": "- Aside from RaE, another natural baseline for this across-experiment setting would be to use behaviour cloning (or distillation) on the previously trained agents. Have you considered doing so? \n\n- RaE only incorporates the previous data at the beginning of optimization. What about training on the data throughout the optimization process? For example, keeping the buffer of previous data and sampling from it occasionally or mixing in samples into minibatches. \n\n- As I mentioned in strengths section, I think training across-experiments could be relevant for practical purposes. How do you see it being used in a scientific context? It could be difficult to fairly assess algorithms if the data they have access to depends on the sequence of previous experiments done. i.e. an advantage of discarding previous data is that algorithms start on equal ground in different papers, allowing fair comparisons. \n\n- I'm a bit surprised that doing some offline learning first is so detrimental to the policy (Fig. 4, finetuning and AWAC). Intuitively, I would guess that there should be a jumpstart in the performance due to the additional offline training. Could you clarify this?\nDo you have any hypotheses why these methods don't perform very well here?\n\n- Currently, the experiments that use RaE are quite similar to simply using resets but at the end of each experimental run. It would be interesting to see some experiments where the same algorithm was not used in consecutive experiments or, at least, changing hyperparameters. This could better simulate the development process of an RL algorithm. \nAs a suggestion, I would be curious to see if hyperparameter optimization could be made easier. Since RL agents can be sometimes hard to run initially on a new problem, we might be able to more easily identify which hyperparameter settings are promising by giving the agent additional data."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5980/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5980/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5980/Reviewer_wVcc"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5980/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699240513014,
            "cdate": 1699240513014,
            "tmdate": 1700689380987,
            "mdate": 1700689380987,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ORkTD8RI1I",
                "forum": "Nf4Lm6fXN8",
                "replyto": "tjFfAv883U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5980/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5980/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer wVcc"
                    },
                    "comment": {
                        "value": "Thank you for your helpful and constructive feedback. We have updated the paper with suggested changes highlighted in red and are confident that the presentation is stronger thanks to them. We address specific questions below and have added a general comment for points shared across reviews. Please do not hesitate to ask for further clarifications.\n\n* \u201cWhat about training on the data throughout the optimization process?\u201d\n\nRaE mixes online data and offline data in a 50/50 ratio for each minibatch sampled throughout training (and not just at the beginning). We have updated the text (page 3) to clarify this. \n\n* RaE in the context of the scientific method for reproducible and comparable results.\n\nThis is a very good point; thank you for highlighting it. Reproducibility in RL is already a challenging open problem [1]. If RaE is more widely adopted, it may add another dimension to this issue: subtle changes in the ordering of data when mixing may lead to changes in final performance. One solution to this could be to standardize and open source offline data sources for benchmark domains (e.g. RL Unplugged) and define deterministic orderings when reading from the data using, for example, fixed random seeds. We have included a brief discussion on this point in the Discussion section (Section 5).\n\n\n* \u201cWhy does offline training work so poorly?\u201d\n\nIn Figure 4, fine-tuning and AWAC learn on the Humanoid Run domain but perform poorly  on the Manipulator domains of RL Unplugged. Humanoid run uses a dense reward signal that incentivizes the agent for moving forward. In contrast, the Manipulator domains provide sparse rewards that are triggered in stages when a robotic arm moves to a peg (or ball) and inserts it into a hole. Learning offline from limited data in sparse reward settings is particularly challenging which is why we see a strong benefit of using RaE to mix in online data to improve asymptotic performance.  Additionally offline learning may also overfit to the limited set of trajectories in the dataset, making further improvement through fine-tuning difficult.We have added a sentence to Section 3.3 to highlight this.\n\n* \u201cAdditional baseline: BC and distillation to previous agent.\u201d\n\nWhen fine-tuning, we consider two baseline agents trained with the offline data: one that uses CRR and another that uses BC. We then select the best performing variant among these to finetune. We believe this addresses your point but are happy to clarify and run another baseline (time permitting) if required.\n\n* \u201c.. more easily identify which hyperparameter settings are promising by giving the agent additional data.\u201d\n\nYes, the faster iteration time when using RaE could be useful to search for better hyperparameters to use in new settings from scratch. However, it is important to note that hyperparameters that are tuned in a setting that reuses data may not directly transfer over to settings where data is not reused. For example, learning rates and optimizer parameters may not be directly transferable.\n\n\n\n[1] Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D., & Meger, D. (2018). Deep Reinforcement Learning That Matters. Proceedings of the AAAI Conference on Artificial Intelligence, 32(1). https://doi.org/10.1609/aaai.v32i1.11694"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5980/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700141582192,
                "cdate": 1700141582192,
                "tmdate": 1700141582192,
                "mdate": 1700141582192,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pkhn1I2FI1",
                "forum": "Nf4Lm6fXN8",
                "replyto": "ORkTD8RI1I",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5980/Reviewer_wVcc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5980/Reviewer_wVcc"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the clarifications. I had misunderstood the RaE algorithm slightly. It maybe nice to emphasize this difference between RaE and random resets as done in the response to reviewer gauC \"When restarting an experiment with RaE, in addition to resetting the weights of the network, the data distribution throughout learning utilizes a fixed ratio of mixing between offline and online data.\"  Based on the responses to the other reviewers and this one, I'd be willing to increase the score."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5980/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700689363610,
                "cdate": 1700689363610,
                "tmdate": 1700689363610,
                "mdate": 1700689363610,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]