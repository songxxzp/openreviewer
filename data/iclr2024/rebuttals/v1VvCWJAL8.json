[
    {
        "title": "Towards Characterizing Domain Counterfactuals for Invertible Latent Causal Models"
    },
    {
        "review": {
            "id": "j5GuwBvQNU",
            "forum": "v1VvCWJAL8",
            "replyto": "v1VvCWJAL8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6182/Reviewer_KSip"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6182/Reviewer_KSip"
            ],
            "content": {
                "summary": {
                    "value": "This work focuses on domain counterfactuals: *What a sample would have looked like if it had been generated in a different environment*.\nIt provides a characterization of *domain counterfactually equivalent* models. They show that sparsity of domain interventions as an inductive bias can help reduce the search space and generate more accurate counterfactuals."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The problem that this work considers has practical significance, e.g., in combining datasets in domains where data collection is expensive."
                },
                "weaknesses": {
                    "value": "* I think the writing and structure of the paper can be improved. A large fraction of space is used for many theorems, some of which are not that important. I suggest moving these theorems to the appendix and only keeping Theorem 2 in section 3.2. Furthermore, the simulated experiments look very contrived to me. The data generation mechanism used for creating data in these experiments is too simplistic. It consists of linear transformations followed by a leaky relu (I had to dig into a very long appendix to find this). The model also seems to have knowledge of the precise form of data generation mechanism (linear + relu), and only fits that form.\nTo me, the interesting experiments come in section 5.2. However, I can't find an explanation of how sparsity is enforced in these experiments.\nIn summary, I think authors can restructure the paper to provide more room for experiments in 5.2., and explain the methodology in detail."
                },
                "questions": {
                    "value": "* How does one set the number of intervention (k)?\n* If we don't have the right k, or if the form of the ILD is not precisely known, how off could be our counterfactual estimates? Do we have identifiability in this case?\n* How does this work compare with a prior line of work in using sparsity for identifiable representations, e.g., [Synergies between Disentanglement and Sparsity: Generalization and Identifiability in Multi-Task Learning](https://proceedings.mlr.press/v202/lachapelle23a.html) or [On the Identifiability of Nonlinear ICA: Sparsity and Beyond](https://proceedings.neurips.cc/paper_files/paper/2022/hash/6801fa3fd290229efc490ee0cf1c5687-Abstract-Conference.html), just to name a few?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6182/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6182/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6182/Reviewer_KSip"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6182/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698328270426,
            "cdate": 1698328270426,
            "tmdate": 1700581825979,
            "mdate": 1700581825979,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MEy0FSVXYE",
                "forum": "v1VvCWJAL8",
                "replyto": "j5GuwBvQNU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6182/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6182/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer KSip (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive feedback. We respond to the questions and suggestions individually below. We have also modified the paper accordingly.\n\n\n> \u201cThe simulated experiments look very contrived to me. The data generation mechanism used for creating data in these experiments is too simplistic.\u201d\n\nWe apologize for any lack of clarity regarding the data-generating process in the main paper, and we have revised the simulated experiment section to clarify the form of the observation function (marked in blue). To address the concern about the simplicity of our simulated experiments, we have added additional experiments where we use a more complicated normalizing flow (based on RealNVP) for the observation function in the ground truth dataset, and our model is misspecified as either a different normalizing flow structure or a VAE structure. More details and relevant figures can be found in Appendix J.2. In summary, we observe a similar trend as that in Figure 1, using the ILD-Relax-Can model results in lower counterfactual error than the ILD-Dense model, regardless of whether the structures of $g^*$ and $g$ match. \n\n> Motivation of the simulated experiment.\n\nThe primary aim of our experimental section is to demonstrate the practical utility of our theoretical framework. We begin with scenarios where all our assumptions are met, then progressively explore cases where these assumptions are not fully satisfied, such as model misspecifications in the intervention size. Following the reviewer's suggestion, we have further extended our investigation to include scenarios with misspecified functional classes of $g$. Finally, in Section 5.2, we test our model design in a more practical setting where we relax the invertibility assumption.\n\nWe also wish to highlight that our simulated experiments encompass a variety of distinct latent causal mechanisms. We have investigated 10 different SCMs for all of our experiments as discussed in Appendix G.1.\n\n> \u201cHowever, I can't find an explanation of how sparsity is enforced in these experiments. In summary, I think authors can restructure the paper to provide more room for experiments in 5.2., and explain the methodology in detail.\u201d\n\nIn Section 5.2, sparsity is enforced by using the sparse $f_\\cdot$ structure (i.e. only $k$ mechanisms in $f_d$ are allowed to change across different domains) as the final layer of the VAE encoder and the initial layer of the VAE decoder. Informally, this can be thought of as the encoder of the VAE generating the Gaussian parameters in a sparse fashion and the decoder of the VAE is sparse due to its initial layer being sparse. An overview of this VAE setup can be seen in the encoder/decoder graphics in Figure 17. \n\n\n\n\n>\u201cI think the writing and structure of the paper can be improved. A large fraction of space is used for many theorems, some of which are not that important. I suggest moving these theorems to the appendix and only keeping Theorem 2 in section 3.2.\u201d\n \nThank you for your feedback. We agree that the writing of the theoretical sections can be simplified and have substantially revised Section 2 accordingly. Regarding the importance of Theorem 1 and 4, we summarize how they play critical roles in our paper below.\n\n**Theorem 1**: This theorem establishes both necessary and sufficient conditions for counterfactual equivalence. It forms the foundational basis for subsequent theorems in our paper as it allows for easy verification of counterfactual equivalence in ILDs and can facilitate the rapid construction of counterfactual equivalence models for any given ILD.\n\n**Theorem 4**: Together with Corollary 5, Theorem 4 demonstrates that all counterfactually equivalent ILDs share the same sparsity in intervention. This insight is crucial as it shows that constraints on the intervention set size do not exclude any equivalent ILDs, and it significantly narrows the functional space in practical applications, which leads to our model design in practice."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6182/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700193659411,
                "cdate": 1700193659411,
                "tmdate": 1700193659411,
                "mdate": 1700193659411,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VZE5z0iBnS",
                "forum": "v1VvCWJAL8",
                "replyto": "xepKj28EhC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6182/Reviewer_KSip"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6182/Reviewer_KSip"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer response"
                    },
                    "comment": {
                        "value": "Thanks for the detailed answers. I increased my score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6182/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700581907547,
                "cdate": 1700581907547,
                "tmdate": 1700581907547,
                "mdate": 1700581907547,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mCy2p4mOWl",
            "forum": "v1VvCWJAL8",
            "replyto": "v1VvCWJAL8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6182/Reviewer_jRXL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6182/Reviewer_jRXL"
            ],
            "content": {
                "summary": {
                    "value": "This work considers the situation that different domains have the same causal graph but different structural causal functions; the \u201cintervention\u201d in this paper is the \u201c*domain difference of causal mechanisms*\u201d, and the \u201ccounterfactual\u201d is the \u201c*domain adaptation of observations*\u201d under a pre-defined domain difference. More specifically, the \u201cintervention\u201d means changing a structural function (from one domain to another) but leaving any other things untouched, and the \u201ccounterfactual\u201d means simply the new observation generated by the \u201cintervention\u201d.\n\nAn Invertible Latent Domain Causal Model (ILD) is a set of SCMs whose structural functions are invertible and autoregressive plus an Invertible Observation Function which connects the latent and observable. The equivalence class of ILDs is characterized, and it is proved that, for any CLDs, there exist equivalent \u201ccanonical forms\u201d whose domain difference is described by the last k structural functions under the topological ordering. Based on this, the paper makes the point that \u201cif we know the number k of different structural functions between the domains, then we can improve the performance\u201d.\n\nThere are experimental supports for the effectiveness of the above idea."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "It is an interesting idea to see the difference between domains as invertible transformations.\n\nThe theoretical analysis seems serious (but it is quite impossible to check the proof in detail as a conference submission). \n\nIf a real-world situation satisfies the theoretical setting, then it is possible to largely reduce the complexity of finding domain differences."
                },
                "weaknesses": {
                    "value": "*The idea of \u201cintervention\u201d and \u201ccounterfactual\u201d in this paper is nonstandard and confusing*. As standard concepts in causal inference, an \u201cintervention\u201d roughly means setting a variable to a specific value but leaving any other things untouched, and \u201ccounterfactual\u201d means imagining an intervention with the *past values* of exogenous variables untouched. In the above, *the structural functions are unchanged*. However, as indicated in my summary, the \u201cintervention\u201d and \u201ccounterfactual\u201d in this paper refer to totally different things, in particular, the \u201ccounterfactual\u201d does *not* consider the past values of exogenous variables which are the gist of \u201ccounterfactual\u201d. Explanations regarding these are necessary, and I strongly suggest *not* using the terms \u201cintervention\u201d and \u201ccounterfactual\u201d in this paper.\n\n*The \u201cgeneralized causal mechanisms\u201d in Prop 2 are only a subclass of \u201cinvertible SCMs\u201d*. There is no formal definition of \u201cinvertible SCMs\u201d in this paper; I think the definition should be like: writing the whole system of SCMs as X=f(\\epsilon) and function f is invertible. In the proof of Prop 2, it assumed that each $\\hat{f}^{(j)}$ is invertible. But this is not implied by the general concept/definition of \u201cinvertible SCMs\u201d I mentioned above, because there is no guarantee that the *subsystem* involving only $z_{<j}$ is invertible; information from other variables might be needed to invert the whole system, and missing any piece of information might render the subsystems non-invertible. Hereafter, I name this class of models *\u201cinvertible autoregressive SCM (IASCM)\u201c*.\n\n*The claim on the generality of the model class (Prop 1) seems problematic*. First, here the model class is IASCM. In the proof of Prop 1, the Rosenblatt transformation should be constructed for a fixed *sample point* of p(X), because each conditional CDF F_j should depend on fixed x_{<j}. Thus, we need a Rosenblatt transformation for each sample point, and we cannot construct the F_p and F_q for the whole distributions p(X) and q(\\epsilon). \n\nI have spent 3 hours to get around the confusion in the listed Weaknesses, and I can only skim the rest of the paper and ask some questions as below. *I will read the rebuttal and the revised paper again and update my review accordingly*."
                },
                "questions": {
                    "value": "*Do we have any ideas on the identifiability of the model*? This is an important question because we discuss causality. Although the theory in the paper converts IASCMs on a set of domains into an easy-to-deal-with form that is the canonical ILD. There are no discussions on the identifiability of canonical ILDs, that is, when we really try to learn the canonical ILDs, can we identify the single eq class of canonical ILDs, which contains the true one? The practical application of the proposed idea depends critically on this question. \n\n*How can we understand if a practical dataset satisfies the theoretical setting*? Both high-level discussions and examples are highly desirable. For example, for MNIST datasets, how can we understand that the observable (image) is related to the latent by an injective mapping? Why the latent variables in different domains are related by invertible mappings? And in the end, what are the latent variables? Note that I do not require any precise claims on the causal structure (it is latent anyway), but even reasonable \u201cguesses\u201d would be very helpful.\n\nI think the 2nd equality of eq2 is also by definition?\n\nIn Def 3, point 3, I am not sure Gaussian exogenous noises are without loss of generality, any references and/or reasonings?\n\nShared observation function g should be explained right after Def 3.\n\nThe claim of \u201cboth counterfactually and distributionally equivalent\u201d in Theorem 2 looks strange to me. Can\u2019t we prove that counterfactual equivalence implies distributional equivalence?\n\nThere is a related work [1] that is worth mentioning; it also considers invertible causal mechanisms, shared function between domains, and transferability between domains. See Sec 2.3, 2.4 and 3.1 in that paper.\n\n[1] Wu, Pengzhou, and Kenji Fukumizu. \"Causal mosaic: Cause-effect inference via nonlinear ICA and ensemble method.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2020.\n\nMinor\n\nThere are two different defs of autoregressive functions, at the end of page 1 and in Def 1. Def 1 seems to be correct.\n\nIn Prop 2, generalize \u2192 generalized\n\nIn Corollary 3, you mentioned \u201cProp 8\u201d, but I think it should be just Def 8."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6182/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6182/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6182/Reviewer_jRXL"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6182/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698766672328,
            "cdate": 1698766672328,
            "tmdate": 1699636672410,
            "mdate": 1699636672410,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8weUBRjZ8M",
                "forum": "v1VvCWJAL8",
                "replyto": "mCy2p4mOWl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6182/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6182/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer jRXL (1/3)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the insightful review. We respond to the questions and suggestions individually below. We have also modified the paper accordingly (highlighted in blue).\n\n>The idea of \u201cintervention\u201d and \u201ccounterfactual\u201d in this paper is nonstandard and confusing \u2026 in particular, the \u201ccounterfactual\u201d does not consider the past values of exogenous variables which are the gist of \u201ccounterfactual\u201d.\n\nThanks for the question. \n\n*Intervention*: We would like to first clarify the intervention considered in this paper. Here, we mainly consider soft intervention which corresponds to the change of functions in the structural assignment [1] [2]. We note this is different than a perfect intervention (e.g., a *do* intervention), as this does not completely eliminate the causal effect of the parents.\n\n*Counterfactual*: In this work, we are specifically interested domain counterfactual queries (i.e. what would $\\mathbf{x}$ look like if it had come from domain $d\u2019$ instead of domain $d$?), which are a specific type of the general counterfactual query. Similar to the general counterfactual query, a domain counterfactual query exactly follows the three steps: abduction, action, and prediction [3]. For example, given an image $\\mathbf{x}$ from domain $d$, we want to answer the question what this image would look like if it is from domain $d\u2019$. We first use the inverse of our observation function $g$ and our latent SCM for domain $d$ to recover the exogenous noise $\\epsilon$ given the evidence (abduction) as follows: $\\epsilon =  f^{-1}\\_{d}\\circ g^{-1}(\\mathbf{x})$. Then, we perform the domain intervention (action) by exchanging the original $f_{d}$ with $f\\_{d\u2019}$. Finally, we use the recovered noise and intervened SCM to predict the counterfactual $\\mathbf{x}\\_{d\\rightarrow d\u2019}$ (prediction). Together this approach follows: $\\mathbf{x}\\_{d \\rightarrow d\u2019} = g\\circ f\\_{d\u2019}\\circ f^{-1}\\_{d}\\circ g^{-1}(\\mathbf{x})$. Additionally, in our empirical analysis, we pay particular attention to the variables that should remain constant during counterfactual estimation. For instance, in our ColorRMNIST experiment, a key focus is on evaluating whether the color attribute is preserved when we alter the rotation aspect. \n\n[1] Sch\u00f6lkopf, B., Locatello, F., Bauer, S., Ke, N. R., Kalchbrenner, N., Goyal, A., & Bengio, Y. (2021). Toward causal representation learning. Proceedings of the IEEE, 109(5), 612-634.\n\n[2] Peters, J., Janzing, D., & Sch\u00f6lkopf, B. (2017). Elements of causal inference: foundations and learning algorithms (p. 288). The MIT Press.\n\n[3] Pearl J Glymour M Jewell NP. Causal Inference in Statistics : A Primer. Chichester West Sussex UK: John Wiley & Sons; 2016. , p96.\n\n\n\n> The \u201cgeneralized causal mechanisms\u201d in Prop 2 are only a subclass of \u201cinvertible SCMs\u201d \u2026 missing any piece of information might render the subsystems non-invertible. \n\nThanks for pointing this out. We agree that if we only assume the invertibility of $f$, we could not guarantee that the subsystem of $\\widehat{f}^{(i)}$ is invertible to $\\epsilon_i$ given only $z_{<i}$. However, without loss of generality, we could assume the partial order in $z$ such that parent nodes have smaller indices, then the overall function $f$ is invertible autoregressive. We believe the confusion is that we define the invertible SCM on the subfunction $\\widehat{f}$, rather than on $f$ directly. \n\n\n> The claim on the generality of the model class (Prop 1) seems problematic\u2026Thus, we need a Rosenblatt transformation for each sample point, and we cannot construct the F_p and F_q for the whole distributions p(X) and q(\\epsilon). \n\nWe might be misunderstanding this concern, please forgive any misinterpretation if that is the case. The Rosenblatt transformation is constructed based on a *joint distribution*, not a single fixed point [1]. But perhaps the concern stems from the fact that in prop 1, $\\mathbf{x}$ represents a random vector rather than a specific instantiation. It is under this case, where $\\mathbf{x}$) is a *random* vector, that the Rosenblatt transformation is well-defined. Additionally, we note that the conditional CDFs are invertible w.r.t. their first argument given previous values $x_{<j}$, but the whole function is invertible because it is autoregressive. Intuitively, this means that you can recover the first variable using the 1D invertible CDF, and then recover the 2nd variable given the first recovered value, etc. Thus, we believe our original proof is correct. Does this answer your concern (perhaps we misunderstood)?\n\n[1] Rosenblatt, Murray. \u201cRemarks on a Multivariate Transformation.\u201d The Annals of Mathematical Statistics, vol. 23, no. 3, 1952, pp. 470\u201372. JSTOR, http://www.jstor.org/stable/2236692. Accessed 17 Nov. 2023."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6182/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700199710897,
                "cdate": 1700199710897,
                "tmdate": 1700199909398,
                "mdate": 1700199909398,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SeKSedQXgx",
                "forum": "v1VvCWJAL8",
                "replyto": "8weUBRjZ8M",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6182/Reviewer_jRXL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6182/Reviewer_jRXL"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the rebuttal. My main concerns remain.\n\n**Soft intervention**\n\nThe definition should be clearly spelled out in a revision. The references you gave are from the ML community; I suspect this concept is widely accepted outside the ML community, or even inside the ML community. I tend to agree that, if we accept the \u201csoft intervention\u201d, then your \u201ccounterfactual\u201d can be seen as a counterpart of the standard counterfactual; but all these need careful explanation in the paper.\n\n**Generalized causal mechanisms**\n\nI cannot understand how a re-ordering of nodes can make non-invertible subsystems invertible.\n\n**Rosenblatt transformation**\n\nAfter another look at the original reference and your paper, it seems to me that, in the original reference, F_n is seen as a function of x_{\u2264n}, thus is *non*-invertible. But in your proof, you see F_n as invertible, then this necessarily means that *your* F_n should be a function of x_n, with x_{<n} *fixed* by a sample value; this was also my thought when reading your paper."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6182/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700578441399,
                "cdate": 1700578441399,
                "tmdate": 1700578441399,
                "mdate": 1700578441399,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "70M2CzQQbR",
            "forum": "v1VvCWJAL8",
            "replyto": "v1VvCWJAL8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6182/Reviewer_4MQs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6182/Reviewer_4MQs"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers the problem of making counterfactual predictions, in a multi-domain setting of latent causal models: In each domain, the latent variables form an SCM, the observed variables are computed from the latent ones by a deterministic function that is shared by all domains, and the counterfactual query asks, given observed data from one domain, what that data would have been had it been generated in another domain."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is clearly structured, guiding the reader through the theoretical setup.\n\nCompared to existing similar work, this paper provides a significant and novel contribution, both in terms of theoretical results and their application in an algorithm."
                },
                "weaknesses": {
                    "value": "I have a concern that there might be an unstated assumption; see my main question below.\n\nThe text could benefit from more proofreading."
                },
                "questions": {
                    "value": "One thing that is unclear to me about the problem setting is the following. Suppose $g = Id$, for some domains $f_d = Id$, and for others $f_d = -Id$. Then in all domains, the observed variables are independently distributed as standard normals. (The same would be true for variations where e.g. each domain flips the signs of a subset of the variables.) For a given counterfactual query, how can your method know whether to predict according to $x_{d'} = x_d$, or according to $x_{d'} = -x_d$? There is no signal in the available data to determine this. Is there an (implicit?) assumption somewhere to rule out cases like this?\n\nOther comments / questions:\n\n* Abstract, \"all non-intervened variables have non-intervened ancestors\": I suggest \"... have no intervened ancestors\". The current sentence can be interpreted as \"have some intervened ancestors\".\n\n* Paragraph before C1-4, \"Given our assumption ... distribution equivalence\": This is apparently without assumptions (1) and (2), which were stated in the preceding sentence. Please make clearer in the text that you're now considering assuming *only* (3).\n\n* In corollary 3, \"Prop. 8\" should refer to definition 8.\n\n* Section 3.3: \"establish on\" should be something else. These sentences should emphasize that you'll now be assuming continuity of $f$ and $g$ (so remove \"the\" and rewrite). In corollary 5, no requirement of continuity is currently stated, but I think it is needed. Also, just above the corollary, what do you mean with \"and not ill-defined\"?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6182/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698833608782,
            "cdate": 1698833608782,
            "tmdate": 1699636672295,
            "mdate": 1699636672295,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TFHEagsN7o",
                "forum": "v1VvCWJAL8",
                "replyto": "70M2CzQQbR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6182/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6182/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 4MQs"
                    },
                    "comment": {
                        "value": "We appreciate the insightful feedback provided by the reviewer. We responded to the questions and suggestions you made individually below, with pointers to locations in the manuscript where the corresponding changes have been made (highlighted in blue).\n\n> Suppose $g = Id$, for some domains $f_d = Id$ and for others $f_d = -Id$. For a given counterfactual query, how can your method know whether to predict according to $x_{d\u2019}$ or according to $x_{d\u2019}=-x_d$? Is there an (implicit?) assumption somewhere to rule out cases like this?\n\nThank you for this insightful question. Our work does not rely on unstated assumptions, and your example pinpoints a crucial aspect of our theoretical framework. Our paper's theoretical contribution lies in characterizing the relationship between counterfactually and distributionally equivalent Invertible Latent Domain (ILD) models. We demonstrate that all counterfactually and distributionally equivalent ILDs share a common feature: the size of the intervention set. In the scenario you described, it is indeed impossible to infer the counterfactual query directly from the interventional data. Based on our distribution equivalence definition (Definition 2), in this case, the intervention set size is $m$ because all the variables are intervened on. By fitting the domain distributions, the counterfactual error (defined in the new Section 3.4 of our revised paper) is bounded by a distribution fit term and a worst-case error. In your example, because the intervention set size is $m$, we will incur a worst-case counterfactual error of $O(\\sqrt{m})$. However, if we assume the sparse mechanism shift (SMS) hypothesis about the true model, then we can reduce this worst-case counterfactual error to $O(\\sqrt{k})$ because only $k$ dimensions can be the negative of the original. \n\nWe expect that future work will be able to more fully explore and analyze counterfactual risk to improve domain counterfactual estimation building upon our results in this paper.\n\n\n\n> [In the] abstract, \"all non-intervened variables have non-intervened ancestors\": I suggest \"... have no intervened ancestors\".\n\nThanks for the suggestion. We rewrote this part in the revised manuscript marked in blue.\n\n> \"Given our assumption ... distribution equivalence\": This is apparently without assumptions (1) and (2), which were stated in the preceding sentence. Please make clearer in the text that you're now considering assuming only (3).\n\nThis is based on Assumption 3. We have revised the paper accordingly. \n\n> In corollary 3, \"Prop. 8\" should refer to definition 8.\n\nWe have revised the paper accordingly.\n\n> Section 3.3: \"establish on\" should be something else\u2026In corollary 5, no requirement of continuity is currently stated, but I think it is needed.\n\nWe have edited these sections to be more clear (e.g., we added a constraint that $(g, f_d)$ for all $d$ are continuous in Corollary 5)."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6182/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700192811439,
                "cdate": 1700192811439,
                "tmdate": 1700233303523,
                "mdate": 1700233303523,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "agn1JWFrqW",
                "forum": "v1VvCWJAL8",
                "replyto": "70M2CzQQbR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6182/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6182/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up"
                    },
                    "comment": {
                        "value": "Hi Reviewer 4MQs, thank you for your helpful feedback. Have we adequately addressed your questions and concerns?"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6182/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700603347095,
                "cdate": 1700603347095,
                "tmdate": 1700603347095,
                "mdate": 1700603347095,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ba0PFjSPk6",
            "forum": "v1VvCWJAL8",
            "replyto": "v1VvCWJAL8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6182/Reviewer_ji9w"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6182/Reviewer_ji9w"
            ],
            "content": {
                "summary": {
                    "value": "The paper focuses on the problem of domain counterfactuals in the context of latent causal models and proposes a practical yet theoretically grounded approach to address this problem, aiming to improve the estimation of domain counterfactuals while making minimal assumptions about the true model and available data.  Experiments on extensive simulated and image-based data show the advantages of domain counterfactual estimation both theoretically and practically."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The problem setting, which revolves around domain counterfactual estimation, is a novel and highly intriguing area of research. I believe it has the potential to make a valuable contribution to the research community.\n\n---------\n\nMy main concerns are as follows:\n\n1) I am experiencing confusion regarding domain counterfactual equivalence. While I agree that it may not be necessary to fully identify the true latent causal model for domain counterfactual estimation, I would appreciate a more intuitive explanation of Theorem 1 and its implications. For instance, it would be helpful to know which specific aspects of the true latent causal model need to be identified to enable domain counterfactual estimation. This could include considerations like the identifiability of latent noise variables, the size of the intervention set, or any other relevant factors. \n\n2) How should we precisely define the size of the intervention set, denoted as 'k'? In discussions about interventions or the number of variables subject to intervention, there is usually a reference to a latent causal model where no interventions have occurred. How do we accurately establish this reference latent causal model?\n\n3) One of the main contributions, in my view, pertains to the definition of the canonical domain counterfactual model. However, this definition might appear somewhat stringent, particularly with its requirement that only the last variables be intervened. Even though it's surprising that any Invertible Latent Domain (ILD) model can be transformed into an equivalent canonical ILD, could you provide some real-world applications to illustrate and justify the relevance and utility of the canonical domain counterfactual model?\n\n4) It appears that in order to enhance domain counterfactual estimation, one needs to have prior knowledge of the intervention sparsity, denoted as 'k.' However, in practical scenarios, obtaining this information can be challenging. While the experiments do offer some insights and analysis regarding the mismatch of sparsity between generative and inference models, could you provide further justification or reasoning for the selection of the appropriate value for 'k'?\n\n\nI would be willing to increase my rating if these concerns are addressed."
                },
                "weaknesses": {
                    "value": "See above"
                },
                "questions": {
                    "value": "See above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6182/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6182/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6182/Reviewer_ji9w"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6182/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698839947090,
            "cdate": 1698839947090,
            "tmdate": 1700700614241,
            "mdate": 1700700614241,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jvKBXFaoOr",
                "forum": "v1VvCWJAL8",
                "replyto": "ba0PFjSPk6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6182/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6182/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ji9w (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the helpful feedback. We responded to the questions and suggestions you made individually below, with pointers to locations in the manuscript where the corresponding changes have been made (highlighted in blue).\n\n>\u201cI would appreciate a more intuitive explanation of Theorem 1 and its implications.\u201d\n\nThanks for the question. The significance of theorem 1 is to formally characterize the set of model\u2019s which are counterfactually equivalent to each other. Intuitively, this means that any model within this set produces *exactly the same counterfactual output*  (i.e. $x_{d_1 \\rightarrow d_2}$), *despite* different underlying causal mechanisms, such as variations in recovered exogenous noise, observation functions, or implied distributions (e.g., it is possible to be counterfactually equivalent even though not distributionally equivalent, see a section discussing the relationship between distribution equivalence and counterfactual equivalence in the Response to All Reviewers.).\n\nKey implications of Theorem 1 include:\n\n1. This theorem reveals that, surprisingly, despite the true ILD, vastly different ILDs could generate equivalent counterfactuals even if they might not be distributionally equivalent.\n2. This theorem proves that recovering the original causal model, which is a much more challenging task, is unnecessary for estimating domain counterfactuals.\n3. It provides a framework to test whether two Invariant Latent Domain (ILD) models are counterfactual equivalents. It also facilitates the development of new counterfactual equivalent ILDs. Utilizing this construction ability, the proof of Theorem 2, Proposition 3, Theorem 4 and Proposition 5 all directly use this theorem. \n\n\n> \u201cHow should we precisely define the size of the intervention set, denoted as 'k'? \u2026there is usually a reference to a latent causal model where no interventions have occurred. How do we accurately establish this reference latent causal model?\u201d\n\nThank you for your insightful question. To clarify, we categorize a node as 'intervened' when there's a variation in its causal mechanism across different domains in our ILD model. This determination can be made using any domain distribution as a reference point if the intervention is soft [1]. For simplicity and without any loss of generality, we often assume domain 1 as this reference. In our revised manuscript, this concept is elaborated in Equation 9 under Definition 9, where the second equality reflects this reference-based perspective. \n\nAlternatively, we can assess intervention by comparing pairwise causal mechanisms. This means that a latent node $j$ is considered intervened if the functions $\\widetilde{f}\\_{d}^{(j)}$ and $\\widetilde{f}_{d\u2019}^{(j)}$ differ for any domain pair $d,d\u2019$. This approach is represented by the first equality in Equation 9. \n\n[1] Kocaoglu, Murat, et al. \"Characterization and learning of causal graphs with latent variables from soft interventions.\" Advances in Neural Information Processing Systems 32 (2019)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6182/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700195897041,
                "cdate": 1700195897041,
                "tmdate": 1700195897041,
                "mdate": 1700195897041,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fDBr4TEgGZ",
                "forum": "v1VvCWJAL8",
                "replyto": "ba0PFjSPk6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6182/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6182/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up"
                    },
                    "comment": {
                        "value": "Hi Reviewer ji9w, thank you for your helpful feedback. Have we adequately addressed your questions and concerns?"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6182/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700603284818,
                "cdate": 1700603284818,
                "tmdate": 1700603284818,
                "mdate": 1700603284818,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EqJNp2D2KN",
                "forum": "v1VvCWJAL8",
                "replyto": "fDBr4TEgGZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6182/Reviewer_ji9w"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6182/Reviewer_ji9w"
                ],
                "content": {
                    "title": {
                        "value": "Comments"
                    },
                    "comment": {
                        "value": "Thanks for your replies.\n\nWhile I acknowledge that it may not be essential to fully identify the true latent causal model for domain counterfactual estimation, I remain uncertain about which specific aspects of the true latent causal model need to be identified to facilitate domain counterfactual estimation. Could you please provide further clarification?"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6182/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700629399382,
                "cdate": 1700629399382,
                "tmdate": 1700629399382,
                "mdate": 1700629399382,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bCP4Qc0mQn",
                "forum": "v1VvCWJAL8",
                "replyto": "ba0PFjSPk6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6182/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6182/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Comment by Reviewer ji8w"
                    },
                    "comment": {
                        "value": "Thank you for your response. Succinctly, we need to know $k^*$ for better counterfactual estimation if we want to achieve counterfactual equivalence by fitting the observational data distribution.\n\nTheorem 4 suggests that any distributional and counterfactual equivalent models share the same intervention set size $k$. To aid in selecting $k$ in practice, we have introduced Theorem 6 which shows that misspecifying of $k$ will lead to a bias-variance tradeoff scenario, where if the estimated $k$ is larger than $k^*$, there will be a large variance on the estimation. However, if the estimated $k$ is smaller than $k^*$, you will not be able to find a distributionally equivalent model, which leads to a biased estimation. For a detailed explanation regarding the impact of misspecifying $k$ and how to choose it in practice, we refer the reviewer to the **Choosing intervention set size $k$ and new bound** paragraph in our response to all reviewers. Of all of our assumptions, the sparse mechanism shift assumption plays an important role here as it, together with our new Theorem 6, guides us to choose a relatively smaller $k$ unless we fail to achieve distribution equivalence, which is justified by our empirical study."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6182/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634233131,
                "cdate": 1700634233131,
                "tmdate": 1700668485864,
                "mdate": 1700668485864,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iZHElbieHG",
                "forum": "v1VvCWJAL8",
                "replyto": "bCP4Qc0mQn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6182/Reviewer_ji9w"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6182/Reviewer_ji9w"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for further response.\n\nI have a better understanding of this part now.  I have increased my score accordingly and will engage in discussion with reviewers further."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6182/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700700591407,
                "cdate": 1700700591407,
                "tmdate": 1700700591407,
                "mdate": 1700700591407,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]