[
    {
        "title": "Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion"
    },
    {
        "review": {
            "id": "F899EMDr1f",
            "forum": "Psl75UCoZM",
            "replyto": "Psl75UCoZM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2827/Reviewer_4daR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2827/Reviewer_4daR"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a point cloud forecasting-based world model for autonomous driving. The model first tokenizes point clouds into discrete BEV tokens (codebook/vocabulary) following VQVAE and UltraLiDAR (Xiong et al., 2023). Then tokens are decoded to reconstruct the point clouds with an implicit representation depth rendering branch and a classical coarse voxel reconstruction branch. MaskGIT is further leveraged to a discrete diffusion model, with different masking conditions and history information (the classifier-free diffuision) guidance applied, to realize the future prediction ability. The point cloud forecasting method is evaluated on three datasets and achieves state-of-the-art results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The presentation of the paper is good, especially in the methodology part. Symbols and figures are clear and helpful for understanding.\n- The model architecture is detailed in the appendix. The authors also provide details besides the model structure, such as the K-means clustering strategy to solve codebook collapse and LayerNorm to stabilize training, which are valuable empirical findings for future research.\n- MaskGIT with diffusion is interesting. It could probably be applied to other tasks as well."
                },
                "weaknesses": {
                    "value": "- The reviewer is confused about the motivation of discrete tokenization and masked image modeling.\n  -  The proposed method adopts a VQVAE-like model to capture the complex 3D world, as mentioned in the introduction challenge (i). Classic BEV (the method in BEVFusion, BEVFormer, etc.,) can also realize this ability IMO. This undermines the motivation to use discrete tokenization and the necessity to use a discrete diffusion model in the after.\n  -  Table 4 presents the ablation of the discrete diffusion algorithm. The motivation to use MaskGIT seems its parallel decoding strategy. How about the masked image modeling? What will the results or inference time be like if no masked image modeling (or even MaskGIT) is applied?\n  -  The intuition of using different masking strategies for world model training comes from the robotics field. It would be valuable if ablations on this could be presented as well.\n  -  In light of the above points, a naive baseline should be simple diffusion modeling with simple BEV features.\n- The gain of point cloud forecasting mostly comes from CFG, which involves past poses and actions. Without this, the results are close to 4D-Occ. \n  - Taking the current action as input is reasonable for a world model, but much information about history could make the prediction rely heavily on the past. If the prediction horizon is longer, such a long history is also weird as poses at the very beginning are intuitively unhelpful. \n  - As this task implicitly involves ego-planning, this could lead to causal confusion though impressive results are obtained under the open-loop scenario. The authors have stated that combining the world modeling approach with model-based RL is a future direction, yet, it is important to demonstrate its effectiveness for the decision-making task."
                },
                "questions": {
                    "value": "- In the introduction, the task definition of point cloud forecasting is 'to predict future point cloud observations given past observations and future ego vehicle poses'. In which paper, the future ego vehicle pose is provided?\n- Why not report full results under all metrics in ablation study tables?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2827/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698675425390,
            "cdate": 1698675425390,
            "tmdate": 1699636226394,
            "mdate": 1699636226394,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iUm8X1an1D",
                "forum": "Psl75UCoZM",
                "replyto": "F899EMDr1f",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2827/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2827/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response (Part 1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their feedback. We are glad that the reviewer finds our approach to be interesting and the presentation of methodology to be clear and helpful. We will be addressing your questions and concerns below.\n\n*Q: \u201cThe proposed method adopts a VQVAE-like model to capture the complex 3D world, as mentioned in the introduction challenge (i). Classic BEV (the method in BEVFusion, BEVFormer, etc.,) can also realize this ability IMO.\u201d*\n\nA: Methods like BEVFusion and BEVFormer are image encoders and tackle supervised object detection; they do not have the ability to generate Lidar point clouds. \n\n*Q: \u201cHow about the masked image modeling? What will the results or inference time be like if no masked image modeling (or even MaskGIT) is applied?\u201d*\n\nA: Masked image modeling is mostly used for representation learning, and often cannot serve as a generative model on its own. The task of world modeling is a generative task. Regarding the connection between masked image modeling and MaskGIT, it is true that after tokenization, MaskGIT is very similar to masked image modeling, but MaskGIT can serve as a generative model. \n\n*Q: \u201cThe intuition of using different masking strategies for world model training comes from the robotics field. It would be valuable if ablations on this could be presented as well.\u201d*\n\nA: We only use two types of attention masks during training, and both are necessary. The masks are either a causal mask, or an identity mask that only allows each frame to attend to itself. The causal attention mask is standard in GPT language models (the original Transformer paper [1] says that they \u201cmodify the self-attention sub-layer \u2026 to prevent positions from attending to subsequent positions\u201d). The identity (attention) mask is necessary for classifier-free diffusion guidance (CFG). Without it, CFG cannot be applied. To better illustrate how CFG works in our world model and why the second type of mask is necessary, we have included an additional diagram Figure 10 in the Appendix. \n\n*Q: \u201cA naive baseline should be simple diffusion modeling with simple BEV features.\u201d*\n\nA: Simple BEV features from supervised object detectors such as BEVFusion and BEVFormer cannot generate point clouds. In addition, assuming that our proposed tokenizer is used, other diffusion models such as Gaussian diffusion models are not necessarily simpler, and often require many more diffusion steps than our discrete diffusion model (which only requires 10 diffusion steps) in the absence of additional distillation techniques. Besides, our proposed method for classifier-free diffusion guidance in the world modeling context is likely important regardless of the continuous/discrete nature of the diffusion world model. \n\n*Q: \u201cTaking the current action as input is reasonable for a world model, but much information about history could make the prediction rely heavily on the past.\u201d*\n\nA: Since a self-driving environment is a partially observable rather than a fully observable environment, conditioning on both past observations and past actions is a reasonable thing to do. The more information from the past that is provided to the world model, the better it can estimate the current state of the world and make predictions about the future. The Transformer can learn to decide which part of information is most relevant or irrelevant, but it can only learn to do so when the necessary information is available. \n\n*Q: \u201cAs this task implicitly involves ego-planning, this could lead to causal confusion though impressive results are obtained under the open-loop scenario.\u201d \u201cIn which paper, the future ego vehicle pose is provided?\u201d*\n\nA: For a world model in self-driving scenes, the actions are where the vehicle plans to drive, represented by future ego vehicle poses. The previous state-of-the-art method 4D Occupancy [2] also uses future ego vehicle poses. Since the future ego poses are provided to the world model, the world modeling task alone does not involve ego-planning. We acknowledge that transferring this to a closed-loop setting while mitigating causal confusion is an interesting and open research problem, but the goal of our paper is to provide a more accurate world model that can be learned from unlabeled data, as we believe this is the major current bottleneck.\n\n*Q: \u201cWhy not report full results under all metrics in ablation study tables?\u201d*\n\nA: In our updated draft, we have updated the ablation study tables to include all metrics within the ROI."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2827/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700371812465,
                "cdate": 1700371812465,
                "tmdate": 1700612263510,
                "mdate": 1700612263510,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6U9AXWTzGa",
                "forum": "Psl75UCoZM",
                "replyto": "F899EMDr1f",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2827/Reviewer_4daR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2827/Reviewer_4daR"
                ],
                "content": {
                    "title": {
                        "value": "Post-rebuttal Response from Reviewer"
                    },
                    "comment": {
                        "value": "Thanks for the clarifications. I have also detailedly gone over other reviewers' comments and authors' feedback. I have some remaining concerns below.\n\n- The motivation of discrete tokenization, in my original review. Maybe I didn't clearly state my thoughts. This point is somehow similar to the concern of Reviewer 2kSt. The method mainly includes three main components besides the neural rendering decoder, VQVAE-like BEV tokenizer, MaskGIT-based discrete diffusion model, and the spatio-temporal world model. My original concern is why use VQVAE-like BEV tokenizer (discrete BEV tokens) instead of continuous BEV features like normal BEV methods (BEVFormer, etc) or even voxel-based methods? The point is continuous vs discrete. I cannot fully get the idea of why they do not have the ability to generate Lidar point clouds. They should behave similarly to the tokenized one for the reconstruction decoder.\n  - If thus, an ideal ablation table should be (the last two rows are current Tab.4). They are likely inferior and inefficient, but the comparisons as well as proper motivation in the introduction could make the overall project more solid and convincing:\n| Method         | Metrics    | FPS | Others |\n| -------------- | :-------: | :--------: | :------: |\n| continuous BEV + naive diffusion          |    -   |    -     |   -   |\n| tokenized BEV + naive diffusion       |    -   |    -     |   -   |\n| tokenized BEV + MaskGIT |    -   |    -     |   -   |\n| tokenized BEV + MaskGIT-based discreate diffusion |    -   |    -     |   -   |\n\n- I agree with the authors that conditioning on past observations and actions is reasonable for a world model. My concern is that this could lead to an unfair comparison with previous methods, as suggested by Reviewer amws as well. The gain of point cloud forecasting mostly comes from CFG. Without this, the results are close to 4D-Occ."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2827/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700635743767,
                "cdate": 1700635743767,
                "tmdate": 1700638358373,
                "mdate": 1700638358373,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4PwVINe278",
            "forum": "Psl75UCoZM",
            "replyto": "Psl75UCoZM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2827/Reviewer_amws"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2827/Reviewer_amws"
            ],
            "content": {
                "summary": {
                    "value": "The study focuses on the development of unsupervised world models to enhance an autonomous agent's understanding of its environment. Though world models are a form of sequence modeling, their adoption in robotic applications like autonomous driving hasn't scaled as rapidly as language models such as GPT. Two primary challenges identified are the complex nature of observation spaces and the need for a scalable generative model. To address these, the research proposes a novel approach: (1) Tokenization of Sensor Observations. (2) Prediction using Discrete Diffusion. Applying this method to point cloud observations (which play a crucial role in autonomous driving) showed a significant improvement. The proposed model reduced the Chamfer distance by more than 65% for a 1-second prediction and over 50% for a 3-second prediction on major datasets like NuScenes, KITTI Odometry, and Argoverse2."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. The contribution is clear and important to the autonomous driving society. Developing a driving world model is recognized as a critical step for scene understanding and decision-making.\n\n2. This paper is well-written and easy to follow. The figures are intuitive and informative. \n\n3. The experimental results are surprisingly good, which improves a lot over existing SOTA methods."
                },
                "weaknesses": {
                    "value": "1. The metrics for evaluating the performance of the world model may not be reasonable enough. For point cloud, most of the points describe the background, which is usually static and irrelevant to the downstream task. The prediction of the motion of dynamic objects is more important. Maybe the author can also report the comparison results for dynamic objects or show the advantage of using such a model for some downstream tasks.\n\n2. The conclusion says \u201cOne particularly exciting aspect of our approach is that it is broadly applicable to many domains. We hope that future work will combine our world modeling approach with model-based reinforcement learning to improve the decision-making capabilities of autonomous agents.\u201d  It is unclear to me what the advantage of using such a world model is for MBRL, for example, compared to object segmentation and tracking pipelines.\n\n3. There are many complex modules in the pipeline, including VQ-VAE, diffusion model, neural feature grid, and transformer. Not sure if it is easy to reproduce the results and extend it to other datasets or tasks."
                },
                "questions": {
                    "value": "1. Could the authors elaborate more on \u201cusing past agent history as CFG conditioning improves world modeling\u201d. What agent history is used here and how is it used? Does it introduce additional knowledge and make the comparison unfair?\n\n2. Since the proposed method introduces a world model, could the authors demonstrate some qualitative examples of different future predictions with different actions? A longer horizon could be helpful to check the consistency of generated frames. I wonder how diverse the future prediction is and how accurate the prediction matches the given action."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2827/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2827/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2827/Reviewer_amws"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2827/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698709114584,
            "cdate": 1698709114584,
            "tmdate": 1700445967714,
            "mdate": 1700445967714,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "D9gDjQCG9j",
                "forum": "Psl75UCoZM",
                "replyto": "4PwVINe278",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2827/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2827/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their feedback. We are glad that the reviewer finds the paper\u2019s contribution to be clear and important, and the writing to be easy-to-follow. We will be addressing your questions and concerns below.\n\n*Q: \u201cThe metrics for evaluating the performance of the world model may not be reasonable enough.\u201d*\n\nA: The metrics we use follow the evaluation protocol from previous point cloud forecasting literature [1]. In addition, our qualitative results clearly show that our model drastically outperforms the previous SOTA on dynamic objects. See the second scene in Figure 5, the third scene in Figure 11, the second and third scene in Figure 15; those examples clearly show that our method can predict the motion of dynamic objects well even in complex scenes, while prior methods are unable to. In fact, our approach seems to be the first point-cloud forecasting method that can make reasonable predictions for dynamic objects without any labels. \n\n*Q: \u201cIt is unclear to me what the advantage of using such a world model is for MBRL, for example, compared to object segmentation and tracking pipelines.\u201d*\n\nA: The main advantage is that it is an unsupervised learning algorithm, so that it can learn from any unlabeled data. The traditional object segmentation and tracking pipelines require annotations and labels to learn, which are very expensive.\n\n*Q: \u201cThere are many complex modules in the pipeline, including VQ-VAE, diffusion model, neural feature grid, and transformer. Not sure if it is easy to reproduce the results and extend it to other datasets or tasks.\u201d*\n\nA: We have included all the technical details in the appendix for reproducibility. We have also added two additional diagrams, Figure 6 and Figure 7, about the architecture of VQ-VAE and spatio-temporal Transformer. It is important to recognize that our approach is conceptually simple: turning sensor observations into discrete tokens, and then applying discrete diffusion. It only has two models: the tokenizer and the world model. The complex designs are about specific architecture choices inside those two models: the neural feature grid is an architecture choice for the tokenizer decoder, and the transformer is an architecture choice for the world model. To extend the approach to other datasets and tasks (beyond self-driving), the specific architectural choices might need to change, but the general approach of tokenization and discrete diffusion are broadly applicable. \n\n*Q: \u201cCould the authors elaborate more on \u2018using past agent history as CFG conditioning improves world modeling\u2019. What agent history is used here and how is it used? Does it introduce additional knowledge and make the comparison unfair?\u201d*\n\nA: To better illustrate how CFG is applied in our world model, we have included an additional diagram Figure 8 in the Appendix. Section 4.3 also covers this part in detail. The agent history means the past observations and action history of the ego vehicle. We assigned a specific symbol to denote agent history: $c^{t-1}$. It is used as classifier-free diffusion guidance. The past observations and action history are already inputs in the world model; they do not require any additional knowledge. \n\n*Q: \u201cSince the proposed method introduces a world model, could the authors demonstrate some qualitative examples of different future predictions with different actions?\u201d*\n\nA: In our updated draft, we have included visualizations of predicted futures under counterfactual actions in Figure 10 (in the appendix). Here we modify the future trajectories of the ego vehicle (which are action inputs to the world model), and we demonstrate that the world model is able to imagine alternative futures given different actions. The imagined futures are consistent with the counterfactual action inputs.\n\n*References:*\n\n[1] Khurana, Tarasha, et al. \"Point Cloud Forecasting as a Proxy for 4D Occupancy Forecasting.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2827/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700371465549,
                "cdate": 1700371465549,
                "tmdate": 1700371465549,
                "mdate": 1700371465549,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "w4xmmVP5Yi",
                "forum": "Psl75UCoZM",
                "replyto": "D9gDjQCG9j",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2827/Reviewer_amws"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2827/Reviewer_amws"
                ],
                "content": {
                    "title": {
                        "value": "Reply to authors"
                    },
                    "comment": {
                        "value": "Thanks to the authors for providing more explanation and experimental results. All my concerns are resolved so I am glad to raise my score."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2827/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700445947540,
                "cdate": 1700445947540,
                "tmdate": 1700445947540,
                "mdate": 1700445947540,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1TgRv4Osxm",
            "forum": "Psl75UCoZM",
            "replyto": "Psl75UCoZM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2827/Reviewer_2kSt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2827/Reviewer_2kSt"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes to use a VQ-VAE+diffusion approach (similar to certain image diffusion pipelines) for the task of learning point-cloud world models for autonomous driving. They design task-specific encoder and decoder architectures to encode point-cloud observations as a sequence of discrete tokens, apply an interleaved spatial-temporal transformer and a discrete diffusion model to predict discrete codes for future frames, and decode with a model based on neural occupancy representations. The proposed model compares favorably to SOTA baselines for the task on standard metrics."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The proposed approach gives strong empirical performance. The architecture takes advantage of structure in the problem at several points in useful and interesting ways (in particular the combination of localized neural occupancy and BEV tokenization is quite interesting, and seems novel). I think the backbone (transformer+discrete diffusion) is comparatively less novel, but this is the first time I've seen it applied to the autonomous driving setting and it is interesting to see that it still gives strong performance.\n\nSeparately the authors propose several improvements to MaskGIT. These modifications seem to be crucial for the performance of their algorithm, but it would be interesting to see if these improvements generalize to the original image-based setting or to other discrete diffusion settings (though doing this on anything other than a toy problem would probably be outside of the scope of this paper).\n\nThe authors also identify an issue with standard point-cloud prediction metrics and propose a simple modification. Though this part may be less relevant to the ICLR community, it is an important observation for the self-driving community and should not be overlooked."
                },
                "weaknesses": {
                    "value": "The proposed architecture is highly specific to point-cloud occupancy prediction (the novelty lies largely in the encoder and decoder, which are task-specific architectures).\n\nThe introduction/related work are somewhat intermixed, which is fine, but leads to confusing presentation. It's not entirely clear from the introduction/related work how the proposed method relates to MaskGIT, and although MaskGIT is heavily referenced it is never clearly described. Background of discrete diffusion could also be better described.\n\nAblations of differences to MaskGIT are good, but it would be useful to also present ablations of the other task-specific model components (encoder/decoder, and maybe also the BEV token grouping?) as compared to general-purpose versions.\n\nMinor note: point cloud prediction visualizations are a little difficult to parse - I'm not sure how they could be improved but it's quite difficult to analyze results/see what's changed between two different images, both in Fig. 1 and Fig. 5."
                },
                "questions": {
                    "value": "- It would be interesting to conduct a more thorough analysis of the modifications to MaskGIT, perhaps on a simpler discrete diffusion problem.\n - Why is L1 median reported inside the ROI but L1 mean reported for the full scene?\n - How important are the novel task-specific encoder and decoder (including the rendering and reconstruction losses on the encoder) to the final performance?\n - It's mentioned that the model is relatively small; how much do the results change when scaling the model (up/down)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2827/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2827/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2827/Reviewer_2kSt"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2827/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698833309192,
            "cdate": 1698833309192,
            "tmdate": 1699636226243,
            "mdate": 1699636226243,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4dYgRh1V3e",
                "forum": "Psl75UCoZM",
                "replyto": "1TgRv4Osxm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2827/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2827/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response (Part 1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their feedback. We are glad that the reviewer finds our proposed method to be interesting and novel, and that the strong empirical performance of our method is recognized. We will be addressing your questions and concerns below.\n\n*Q: \u201cThe proposed architecture is highly specific to point-cloud occupancy prediction (the novelty lies largely in the encoder and decoder, which are task-specific architectures)\u201d*\n\nA: It is true that our tokenizer is highly specific to point clouds. After tokenization, however, the specific 3D representations are abstracted away from the world model. The architecture of the world model is very general-purpose: it is a spatio-temporal Transformer, and both its inputs and outputs are discrete tokens. \n\nAdditionally, the concept of Bird-Eye View (BEV) is quite general and goes beyond point clouds, since many popular approaches for camera-based perception such as Lift-Splat-Shoot [1] and BEVFormer [2] also lift camera data to BEV. Therefore, camera data can potentially be tokenized using BEV tokens as well. In this case, the spatio-temporal transformer acting as the world model can remain unchanged.\n\n*Q: \u201cIt's not entirely clear from the introduction/related work how the proposed method relates to MaskGIT, and although MaskGIT is heavily referenced it is never clearly described.\u201d*\n\nA: In our updated draft, we have included a clear and concise description of MaskGIT in the background section. To include it here: \u201cMaskGIT (Chang et al., 2022) has significantly simpler training and sampling procedures based on BERT alone: for training, it masks a part of the input tokens (with an aggressive masking schedule) and then predicts the masked tokens from the rest; for sampling, it iteratively decodes tokens in parallel based on predicted confidence.\u201d\n\n*Q: \u201cIt would be useful to also present ablations of the other task-specific model components (encoder/decoder, and maybe also the BEV token grouping?) as compared to general-purpose versions\u201d*\n\nA: Due to the cost and compute time required to train a tokenizer from scratch, it is too costly to ablate all components in the tokenizer. That being said, the individual components in the tokenizer are mostly off-the-shelf: the encoder follows standard designs in point-cloud based object detection; the vector quantization layer follows VQVAE; the decoder follows NeRF-like differentiable depth rendering already known to work well. Each off-the-shelf component was designed for general-purpose use; the contribution of our tokenizer is to demonstrate that combining the three components together can allow us to tokenize an entire self-driving scene with detailed reconstructions. \n\n*Q: \u201cI'm not sure how they could be improved but it's quite difficult to analyze results/see what's changed between two different images, both in Fig. 1 and Fig. 5.\u201d*\n\nA: In our updated draft, we have modified Figure 1 to provide a more zoomed-in view. We have also updated Figure 5 to provide zoomed-in orange boxes, which highlight that our method is significantly better in terms of not just the overall structure of the scene but also geometric details, especially at 3 seconds into the future. We will continue to find better ways to visualize the point clouds for the camera-ready version.\n\n*Q: \u201cIt would be interesting to conduct a more thorough analysis of the modifications to MaskGIT, perhaps on a simpler discrete diffusion problem.\u201d*\n\nA: We agree with the reviewer that this would be interesting, but the focus of this work is unsupervised world modeling in large-scale self-driving scenes. Given that our experiment section is centered around point cloud forecasting, we think that perhaps it makes the most sense to leave such investigation to future work. \n\n*Q: \u201cWhy is L1 median reported inside the ROI but L1 mean reported for the full scene?\u201d*\n\nA: As explained in the paper, L1 median within the ROI is a much better metric because (1) the median is more robust to outliers than the mean, and (2) in most self-driving applications predictions within the ROI is what matters for downstream tasks such as planning. For a more direct comparison with the baselines we still computed L1 mean since that is what was reported in their original work."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2827/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700371045895,
                "cdate": 1700371045895,
                "tmdate": 1700371045895,
                "mdate": 1700371045895,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "g71pJI9SrH",
            "forum": "Psl75UCoZM",
            "replyto": "Psl75UCoZM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2827/Reviewer_9NMX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2827/Reviewer_9NMX"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a state-of-the-art world model for driving data. Among several major contributions, the paper describes a new way to tokenize point clouds using a VQVAE combined with a PointNet; the paper also proposes a combination of generative masked modeling and discrete diffusion for learning a world model. The proposed method is tested on three commonly used lidar datasets and is shown to achieve state-of-the-art on 1s and 3s time horizon prediction."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. The paper proposes a tokenizer for point clouds, which could have major applications across robotics.\n2. The combination of MaskGIT with discrete diffusion and classifier-free guidance is novel. The idea of both decoding and denoising tokens is very interesting.\n3. The proposed model outperforms prior state-of-the-art by a large margin.\n4. The methods section is clear even though it proposes several novel models and losses."
                },
                "weaknesses": {
                    "value": "The unnumbered first equation in Section 3 should be explained better.\n\nMinor:\n* It is not fully clear to me what \u201cSE(3) ego poses\u201d mean.\n* Figure 1 and 5 might be easier to read if you zoom in on the circled areas."
                },
                "questions": {
                    "value": "1. \u201cWe hope that future work will combine our world modeling approach with model-based reinforcement learning to improve the decision making capabilities of autonomous agents.\u201d \u2013 Are you planning to release your code?\n2. What hardware is required to train your model?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "10: strong accept, should be highlighted at the conference"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2827/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699024143538,
            "cdate": 1699024143538,
            "tmdate": 1699636226171,
            "mdate": 1699636226171,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "i9PXTkqmuq",
                "forum": "Psl75UCoZM",
                "replyto": "g71pJI9SrH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2827/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2827/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their feedback. We are glad that the novelty of our method and the significance of our results are recognized, and that the reviewer finds our method section to be clear. We will be addressing your questions and concerns below.\n\n*Q: \u201cThe unnumbered first equation in Section 3 should be explained better\u201d*\n\nA: We have added additional explanations in the updated draft. This equation is the commonly used diffusion objective (Equation 13 in the original diffusion paper [1]). \n\n*Q: \u201cIt is not fully clear to me what \u2018SE(3) ego poses\u2019 mean\u201d*\n\nA: We express the ego vehicle actions as ego pose transformations of the ego vehicle during two consecutive time steps, where each action is a 3-D homogeneous transformation matrix consisting of a translation and rotation (i.e., SE(3)). Since the world model is action-conditioned, we need to tell the world model where the agent plans to drive. As the ego vehicle can be assumed to have a rigid body, we do so in the form of rigid transformations. As mentioned in Section 4.4, we simply flatten those 4x4 matrices and feed the 16-dim vector as additional inputs to the spatio-temporal Transformer. \n\n*Q: \u201cFigure 1 and 5 might be easier to read if you zoom in on the circled areas.\u201d*\n\nA: In our updated draft, we have modified Figure 1 to provide a more zoomed-in view and better viewing angles. We have also updated Figure 5 to provide zoomed-in orange boxes, which highlight that our method is significantly better in terms of not just the overall structure of the scene but also geometric details. \n\n*Q: \u201cAre you planning to release your code?\u201d*\n\nA:  We are unable to release the code, but we have included all the technical details of our model in the appendix. We have also added two additional diagrams, Figure 6 and Figure 7, about our model architectures to improve reproducibility. \n\n*Q: \u201cWhat hardware is required to train your model?\u201d*\n\nA: The tokenizer is trained on 16 T4 GPUs; and the world model is trained on 8 A10 GPUs. \n\n*References:*\n\n[1] Sohl-Dickstein, Jascha, et al. \"Deep unsupervised learning using nonequilibrium thermodynamics.\" International conference on machine learning. PMLR, 2015."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2827/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700370659808,
                "cdate": 1700370659808,
                "tmdate": 1700370659808,
                "mdate": 1700370659808,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "G70Ymv1lpm",
                "forum": "Psl75UCoZM",
                "replyto": "i9PXTkqmuq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2827/Reviewer_9NMX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2827/Reviewer_9NMX"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you for answering my question. I am in favor of accepting this paper."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2827/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700695822061,
                "cdate": 1700695822061,
                "tmdate": 1700695822061,
                "mdate": 1700695822061,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rArc0WbxO5",
            "forum": "Psl75UCoZM",
            "replyto": "Psl75UCoZM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2827/Reviewer_2hr7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2827/Reviewer_2hr7"
            ],
            "content": {
                "summary": {
                    "value": "The work presents a groundbreaking technique for learning world models in an unsupervised manner, with a particular application to autonomous driving. It addresses the complexity of interpreting unstructured sensor data by implementing a Vector Quantized Variational AutoEncoder (VQ-VAE) to tokenize this data, followed by the prediction of future states through a discrete diffusion process. This technique modifies the Masked Generative Image Transformer (MaskGIT) into a discrete diffusion model, which leads to a substantial increase in prediction accuracy. The proposed approach stands out for its ability to tokenize sensor inputs and utilize a spatio-temporal Transformer for the efficient decoding of future states, which has demonstrated an improvement in prediction accuracy over existing methods on autonomous driving datasets. The model achieves a significant reduction in prediction errors and also shows competence in generating both precise short-term forecasts and diverse long-term predictions, thereby holding great promise for the application of GPT-like learning paradigms in robotics."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper introduces a novel approach by combining VQ-VAE tokenization with a discrete diffusion process, which is kind of innovating. The idea of simplifying the observation space and tokenizing the observation space makes it much easier to model the complex observation space that are usually the case for self-driving.The proposed method's improvement is demonstrated through rigorous experimental validation, showing significant improvements in prediction accuracy over existing methods.The reduction in Chamfer distance for both short-term and long-term predictions indicates a high-quality advancement in the field of point cloud predictions. The paper is also well-structured, with a clear exposition of the methodology, which includes tokenization of sensor data and the subsequent prediction process."
                },
                "weaknesses": {
                    "value": "The paper mostly addresses the prediction of near term future states but it is not clear if we go much further, how would the accuracy be? With a diffusion model, the inference could be slow, so this model may not be suitable for use on board but mostly would be useful for simulations and other tasks that don't require real time feedback or predictions. This may limit the application of this approach."
                },
                "questions": {
                    "value": "How is the result if we predict much further, like 9s? In other dataset, like WOMD, the prediction horizon tends to be slightly longer so it would be great to know if the performance would drop significantly if we predict much further away states. Secondly, could you also share some insights on how this model could exactly be integrated within modern self-driving systems and work with other modules such as planning? How would noise in perception, like Lidar affect the prediction accuracy?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2827/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699258125735,
            "cdate": 1699258125735,
            "tmdate": 1699636226082,
            "mdate": 1699636226082,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ruRV1WbAKG",
                "forum": "Psl75UCoZM",
                "replyto": "rArc0WbxO5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2827/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2827/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response (Part 1)"
                    },
                    "comment": {
                        "value": "Author Response:\n\nWe thank the reviewer for their feedback. We are glad that the novelty of our approach and the rigor of our experimental evaluation are recognized, and that the reviewer finds our exposition to be clear and well-structured. We address the questions and concerns below.\n\n*Q: \u201cThe paper mostly addresses the prediction of near term future states but it is not clear if we go much further, how would the accuracy be?\u201d \u201cHow is the result if we predict much further, like 9s?\u201d*\n\nA: We answer this question from several perspectives:\n - While we agree with the reviewer that long horizon prediction is an interesting problem, we believe finding a good metric to evaluate such capability will require extensive research. Evaluating sensor predictions 9s ahead via reconstruction metrics is not very meaningful. In autonomous driving scenarios, vehicles often cover a considerable distance within 9 seconds, leading to substantial transformations in the scene. Many foreground actors (like vehicles) and background elements (like trees) present in the new scene might not have been visible just 9 seconds earlier. Consequently, observations from 9 seconds prior often become insufficient, if not almost irrelevant, for making accurate future predictions. As such, unsupervised world modeling has a significantly greater degree of uncertainty than supervised object-level trajectory prediction, especially for long-horizon prediction. Note that supervised object-level trajectory prediction systems circumvent this issue by focusing solely on the trajectories of existing foreground actors that are visible at the current scene. \n - Generating coherent long-horizon futures will likely require training the model to predict longer than 3s. While our framework allows us to train a model to predict 9s ahead, this will require significantly more compute resources, and is beyond the scope of our work. \n - We note that our approach already significantly outperforms existing SOTA in 1s and 3s predictions, which have been the common benchmarks for unsupervised point cloud forecasting. Predicting 9s is very challenging, and there are no baselines to compare to, but we will consider it for future work.\n\n*Q: \u201cwith a diffusion model, the inference could be slow\u201d*\n\nA: It is true that our current model requires 10 diffusion steps for generating each frame. However, we also point out the following:\n - Inference speed is not our current focus in this work. The goal of our work is to showcase what results can be obtained with a discrete diffusion world model. As a pioneering work studying how to apply diffusion to unsupervised world modeling on real-world data, our model can already achieve good results using a reasonably small number of diffusion steps (10 steps).\n - The field is actively working on improving the speed of diffusion models. Until recently, the field has primarily focused on Gaussian diffusion models such as DDPM [1], which initially required 1000 diffusion steps per sample; later on, DDIM [2] required 50 steps, progressive distillation [3] required around 8 steps, and Consistency Models [4] only required 1-2 diffusion steps. Even though discrete diffusion has not yet received as much attention from the field, we expect that similar distillation techniques can likely be applied in the future.\n\n\n*Q: \u201ccould you also share some insights on how this model could exactly be integrated within modern self-driving systems and work with other modules such as planning\u201d*\n\nA: With a world model, the problem of autonomy can be seen as a model-based reinforcement learning (MBRL) problem. The autonomy system is an RL agent taking actions in the world. Previously, to apply MBRL, the bottleneck was about whether such a world model can be learnt on real-world observations in the self-driving scenes. Our work is a contribution towards removing this bottleneck. With an effective world model at hand, research can start to apply MBRL approaches such as Dreamer-v2 [5] to both learn a driving policy during training and plan with model-predictive control (MPC) at test time. While Dreamer-v2 assumes a given reward function, we can learn such a reward function from real-world driving data via techniques such as GAIL [6]. This is just one example of how this model can be integrated with modern autonomy systems."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2827/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700370244482,
                "cdate": 1700370244482,
                "tmdate": 1700370244482,
                "mdate": 1700370244482,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uWEzfT4piv",
                "forum": "Psl75UCoZM",
                "replyto": "rArc0WbxO5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2827/Reviewer_2hr7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2827/Reviewer_2hr7"
                ],
                "content": {
                    "title": {
                        "value": "thanks for the rebuttal, I'll keep my rating."
                    },
                    "comment": {
                        "value": "Thanks for clarifying some of my questions. I think the notion of using a discrete diffusion process is indeed novel and improves the prediction accuracy nontrivially, so I keep my rating. But predicting 9s is still meaningful, though the author claimed they don't have the computing resources to do that which is reasonable. However, that does not mean that predicting 9s or some horizon longer than 3s isn't important. Since the vehicle speed could vary, in some cases, horizon longer than 3s is still important to predict to get a sense how far the vehicle is able to predict without making too much errors. There are indeed industry models that can predict long horizon trajectories, though some are not open sourced, which may make it hard to compare with."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2827/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700699772471,
                "cdate": 1700699772471,
                "tmdate": 1700700192621,
                "mdate": 1700700192621,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]