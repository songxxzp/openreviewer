[
    {
        "title": "Parameter-Efficient Multi-Task Model Fusion with Partial Linearizeation"
    },
    {
        "review": {
            "id": "q9sbKYO1po",
            "forum": "iynRvVVAmH",
            "replyto": "iynRvVVAmH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2767/Reviewer_vkjS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2767/Reviewer_vkjS"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel approach to enhance multi-task fusion in large pre-trained models. The authors introduce partial linearization of adapter modules combined with task arithmetic, improving the fusion of multiple tasks while maintaining efficient fine-tuning and inference. Experimental results demonstrate that this method outperforms standard techniques, especially as the number of tasks increases. The contribution lies in its ability to construct unified multi-task models effectively and efficiently fuse fine-tuned task vectors, highlighting the benefits of partial linearization for scalable multi-task model fusion."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper exhibits a clear and logical structure, making it easy to comprehend.\n\n- The proposal's effectiveness is demonstrated through comprehensive experiments conducted on both NLP and image classification tasks. The visualization provided in Figure 6 offers intriguing insights into disentanglement error, further substantiating the proposal's efficacy."
                },
                "weaknesses": {
                    "value": "- The absence of experiments conducted on larger-scale models diminishes the significance of the proposal."
                },
                "questions": {
                    "value": "- Can the proposed methods demonstrate effectiveness on SOTA models?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2767/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698164100806,
            "cdate": 1698164100806,
            "tmdate": 1699636219583,
            "mdate": 1699636219583,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "noRYyNTUZg",
                "forum": "iynRvVVAmH",
                "replyto": "q9sbKYO1po",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2767/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2767/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer vkjS"
                    },
                    "comment": {
                        "value": "Thank you for your comprehensive review and the acceptance recommendation for our paper. Your feedback is greatly appreciated and will help us improve the final version of our paper.\n\n**W1: The absence of experiments conducted on larger-scale models diminishes the significance of the proposal.**\n\n**The scale of the experiments.** Regarding the scale of the experiments, we acknowledge that including larger-scale models could further validate the robustness and scalability of our proposal. While our current results already demonstrate the effectiveness of our method on CLIP and Flan-T5 models, we are eager to extend our experiments to SOTA models to provide a more thorough validation of our approach. Due to resource limitations, conducting experiments on larger models was challenging as they require substantial GPU resources, which were difficult to support. We futher report results of extra small-scale experiments of semantic segmentation task in response to Q1, which indicate our method can be extended to larger model and pixel-level vision task.\n\n---\n\n**Q1: Can the proposed methods demonstrate effectiveness on SOTA models?**\n\n**Effectiveness on SOTA models.** We are confident that our partial linearization method can be applied to SOTA models to enhance their multi-task learning capabilities. The positive results we observed in CLIP and Flan-T5 models, which are themselves highly competitive, give us a strong indication that our method would be beneficial when applied to other advanced models. We will take your question as a valuable suggestion for future work and aim to include such results in subsequent publications. \n\nTo demonstrate this, we conducted a small-scale segmentation experiment. We utilized the SAM model ('sam_vit_b_01ec64.pth') to test the segmentation tasks with both LoRA and L-LoRA on the Pascal VOC 2012 and NYUD v2 datasets. We set the batch size to 4 and the LoRA r hyperparameter to 32. *Due to time constraints and resource limitations, the models were still far from convergence at this point, but the results were sufficient to demonstrate the effectiveness of L-LoRA (The code for this experiment will be put on GitHub together with a clean version of the original code).* We report the performance of the pre-trained models, the fine-tuned models, and the fused models obtained through weight averaging on the validation sets of Pascal VOC and NYUD v2 in terms of mIoU in the table below.\n\nTable: reported mIoU of LoRA and L-LoRA.\n\n| Model                             | fine-tuning method | VOC 2012 | NYUD v2 | Mean mIoU |\n|-----------------------------------|--------------------|----------|---------|-----------|\n| Pretrained Model                  |                    | 0.65     | 0.75    | 0.70      |\n| fine-tuned (far from converge)    | LoRA               | **4.51**     | **2.42**    | **3.47**      |\n|                                   | L-LoRA             | 4.09     | 1.93    | 3.01      |\n| two-model fusion (simple average) | LoRA               | 1.92(43%)     | 1.15(47%)    | 1.53      |\n|                                   | L-LoRA             | **4.05(99%)**     | **1.51(78%)**    | **2.78**      |\n\nNotably, the two-model fusion using simple averaging yields different results for each method. The LoRA fusion only retains 43% and 47% of the pretrained model's performance on VOC 2012 and NYUD v2, respectively. In contrast, the L-LoRA fusion maintains a remarkable 99% and 78% of the fine-tuned LoRA's performance on the respective datasets, resulting in a mean mIoU of 2.78."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2767/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699949078140,
                "cdate": 1699949078140,
                "tmdate": 1699952427018,
                "mdate": 1699952427018,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iY9w7Mx4jD",
                "forum": "iynRvVVAmH",
                "replyto": "noRYyNTUZg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2767/Reviewer_vkjS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2767/Reviewer_vkjS"
                ],
                "content": {
                    "title": {
                        "value": "I will keep my rating"
                    },
                    "comment": {
                        "value": "I would like to thank the authors for their rebuttal efforts to make the paper more solid. I will suggest the author add the results to the paper. My concerns are addressed and I will keep my rating of the paper."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2767/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631607625,
                "cdate": 1700631607625,
                "tmdate": 1700631607625,
                "mdate": 1700631607625,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "eomIzzF9zb",
            "forum": "iynRvVVAmH",
            "replyto": "iynRvVVAmH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2767/Reviewer_HkFi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2767/Reviewer_HkFi"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel method to improve multi-task fusion for parameter-efficient fine-tuning techniques like LoRA fine-tuning. Specifically, their approach partially linearizes only the adapter modules and applies task arithmetic over the linearized adapters. This allows us to leverage the advantages of model fusion over linearized fine-tuning, while still performing fine-tuning and inference efficiently. Extensive experiments are conducted."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The code is provided.\n2. This paper proposes a new linearized LoRA method.\n3. Extensive experiments are conducted."
                },
                "weaknesses": {
                    "value": "1. The novelty of this paper is limited. This paper simply adapts the proposed method in [1] by replacing full fine-tuning with LoRA. Thus, the method in this paper is obviously more efficient than [1] since LoRA is more efficient than full fine-tuning.\n2. The definition of the task vector of LoRA in this paper seems to be unreasonable.\n3. The L-LoRA has a large performance drop on single-task fine-tuning (Figure 7) and a slight increase on the merged case (Table 1, especially in the NLP domain) compared to LoRA. Thus, it is unclear what are the advantages of L-LoRA compared to LoRA."
                },
                "questions": {
                    "value": "### Major Concerns:\n1. The novelty of this paper is limited. [1] studies the linearized full fine-tuning while this paper simply replaces full fine-tuning with LoRA. Although the authors emphasize the proposed method is more efficient than [1], it is obvious since LoRA is more efficient than fully fine-tuning.\n2. What's the meaning of $\\phi_0$? For the full fine-tuning, $\\theta_0$ is the pre-trained model weight and is shared for every task $\\tau_i$. However, for LoRA, $\\phi_i$ is newly added parameters for each task $\\tau_i$. Thus, what is $\\psi_0$? Is $\\psi_0$ a shared initialization for LoRA matrixes of every task $\\tau_i$?\n3. In [2], the task vector of full fine-tuning is defined as $\\nu_i=\\theta_i-\\theta_0$, which means the parameter change of $\\theta_0$. Thus, why the task vector of LoRA in this paper is defined as the change of LoRA parameters $\\phi_i-\\phi_0$ rather than the change of $\\theta_0$ as in [3], i.e., $A_iB_i$, where $\\phi_i=[A_i, B_i]$ is the LoRA parameters.\n4. Table 1 only shows the average normalized scores over multiple datasets, so how about the performance of each dataset? Could you provide it in the Appendix?\n5. It seems the proposed L-LoRA method does not perform well on the NLP domain, according to the results in Table 1 and the similarity heatmap in Figure 8.\n6. Why use the proposed L-LoRA rather than the existing LoRA? The single-task fine-tuning results in Figure 7 show that L-LoRA has a large performance drop compared to LoRA in many datasets.  \n\n### Minor Concerns:\n1. The caption of Figure 3(c): not linearized?\n\n\n**References**\n\n[1] Task Arithmetic in the Tangent Space: Improved Editing of Pre-Trained Models. arXiv:2305.12827.\n\n[2] Editing Models with Task Arithmetic. ICLR, 2023.\n\n[3] Effective and Parameter-Efficient Reusing Fine-Tuned Models. arXiv:2310.01886."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2767/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2767/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2767/Reviewer_HkFi"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2767/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698391084121,
            "cdate": 1698391084121,
            "tmdate": 1700670182097,
            "mdate": 1700670182097,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uWxz07EAGH",
                "forum": "iynRvVVAmH",
                "replyto": "eomIzzF9zb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2767/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2767/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer HkFi (1)"
                    },
                    "comment": {
                        "value": "Thank you for your detailed evaluation of our manuscript and for raising several insightful points. We appreciate the opportunity to address your concerns and clarify aspects of our work.\n\n**W1 & Q1: The novelty of this paper is limited. This paper simply adapts the proposed method in [1] by replacing full fine-tuning with LoRA. Thus, the method in this paper is obviously more efficient than [1] since LoRA is more efficient than full fine-tuning.** \n\n**Novelty of the paper.** We understand your concern regarding the perceived incremental nature of our contribution. However, we must emphasize that the essence of our contribution extends beyond mere efficiency improvements. Our work's innovation lies in the application of first-order information during the partial linearization of LoRA modules. This use of first-order information is pivotal; it captures the most critical aspects of model variability, which is the fundamental reason our method is effective, not merely an incremental extension.\n\nIn our approach, we carefully considered the specific aspects of the LoRA fine-tuned model that would benefit most from linearization. Rather than applying a blanket linearization across the entire model, we strategically targeted the adapter modules for this process. This focused application of linearization to the adapters is a deliberate choice, driven by the understanding that these modules play a crucial role in task-specific adaptations within the larger model architecture. By linearizing only these components, we aim to enhance their ability to integrate and retain task-specific information without incurring the computational costs associated with linearizing the entire model. This method allows us to maintain the overall efficiency of the LoRA framework while reaping the benefits of linearization where it counts the most\u2014within the adapters that are key to the model's fine-tuning for distinct tasks.\n\n[1] Task Arithmetic in the Tangent Space: Improved Editing of Pre-Trained Models. arXiv:2305.12827.\n\n---\n\n**Q2: What's the meaning of\u00a0$\\phi_0$? For the full fine-tuning,\u00a0$\\theta_0$\u00a0is the pre-trained model weight and is shared for every task\u00a0$\\tau_i$. However, for LoRA,\u00a0$\\phi_i$\u00a0is newly added parameters for each task\u00a0$\\tau_i$. Thus, what is\u00a0$\\phi_0$? Is\u00a0$\\phi_0$\u00a0a shared initialization for LoRA matrixes of every task\u00a0$\\tau_i$?**\n\n**Meaning of $\\phi_0$.** Yes, $\\phi_0$ represents a shared initialization for LoRA matrixes across all tasks in our work. As detailed in Appendix B, we have employed a consistent random seed\u2014specifically, seed 42\u2014for initializing the parameter-efficient models in both the vision and language domains. \n\nThe rationale behind this specific condition is that when defining the task vector for PEFT models, it is necessary to have a common starting point. In practice, when using only LoraHub to perform model fusion, it is possible to relax this constraint. This is because LoraHub computes a weighted sum of the parameters across the entire LoRA modules to produce a merged LoRA module $\\phi = \\sum_i w_i \\phi_i$, thus avoiding the introduction of the concept of task vector."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2767/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699948733581,
                "cdate": 1699948733581,
                "tmdate": 1699948744779,
                "mdate": 1699948744779,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZoUA0hCtAm",
                "forum": "iynRvVVAmH",
                "replyto": "eomIzzF9zb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2767/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2767/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer HkFi (2)"
                    },
                    "comment": {
                        "value": "**W2 & Q3: The definition of the task vector of LoRA in this paper seems to be unreasonable. 1. In [2], the task vector of full fine-tuning is defined as\u00a0$\\nu_i=\\theta_i - \\theta_0$, which means the parameter change of\u00a0$\\theta_0$. Thus, why the task vector of LoRA in this paper is defined as the change of LoRA parameters $\\phi_i-\\phi_0$\u00a0rather than the change of\u00a0$\\theta_0$\u00a0as in [3], i.e.,\u00a0$A_i B_i$, where\u00a0$\\phi_i=[A_i, B_i]$\u00a0is the LoRA parameters.**\n\n**Task vector definition.** Our definition of the task vector as $\\phi_i - \\phi_0$ (the trainable parameter space, LoRA parameter space) rather than the change of $\\theta$ (the original parameter space) as done in [3, 4] is a deliberate and calculated decision that aligns with the nature of our fine-tuning approach.\n\n1. When partially linearizing a LoRA model, we concentrate on the dynamics of the trainable parameters and compute the Jacobian-verctor product for all trainable parameters, specifically the LoRA parameters denoted by $\\phi$. The original model parameters, denoted by  $\\theta_0$, are treated as constants during fine-tuning\u2014akin to model buffers that provide a stable foundation upon which the adaptable components of the model, the LoRA parameters, can exert their influence.\n2. From a mathematical standpoint, we perform a first-order Taylor expansion on the trainable parameters, which does not involve the parameter merging operations of the LoRA module.\n\n   $$f^{\\text{lin}}\\_{\\theta_0}(x;\\phi)=f\\_{\\theta_0}(x; \\phi_0) + \\nabla_\\phi f\\_{\\theta_0}(x;\\phi_0)^\\top (\\phi -\\phi_0)$$\n\n   It's important to note that this process of Taylor expansion is applied exclusively to the parameters that are designated as trainable\u2014those that we actively adjust during the fine-tuning phase.\n3. Although our experiments have solely utilized LoRA fine-tuning, such a definition is helpful for generalize our method to other parameter-efficient fine-tuning techniques, such as adapter-tuning.\n\n[2] Editing Models with Task Arithmetic. ICLR, 2023.  \n[3] Effective and Parameter-Efficient Reusing Fine-Tuned Models. arXiv:2310.01886.  \n[4] Composing Parameter-Efficient Modules with Arithmetic Operations. http://arxiv.org/abs/2306.14870.  \n\n\n---\n\n**Q4: Table 1 only shows the average normalized scores over multiple datasets, so how about the performance of each dataset? Could you provide it in the Appendix?**\n\n**Performance on Each dataset.** We agree that a detailed breakdown of performance across individual datasets would be beneficial. We will include these results in the Appendix to provide a more comprehensive view of our method's performance.\n\nTable: normalized scores of merged CLIP-ViT-B/16 across individual datasets.\n\n|     Method      | Fine-tuning Mode |   Cars   |   DTD    | EuroSAT  |  GTSRB   | RESISC45 |  SUN397  |   SVHN   |   Mean   |\n| :-------------: | :--------------: | :------: | :------: | :------: | :------: | :------: | :------: | :------: | :------: |\n| Simple Average  | full fine-tuning |   0.77   |   0.62   | **0.76** |   0.49   |   0.71   |   0.87   | **0.56** | **0.68** |\n|                 |       LoRA       |   0.81   |   0.69   |   0.39   |   0.40   |   0.62   |   0.93   |   0.16   |   0.57   |\n|                 |      L-LoRA      | **0**.87 | **0.86** |   0.41   | **0.58** | **0.74** | **0.97** |   0.19   |   0.66   |\n| Task Arithmetic | full fine-tuning |   0.80   |   0.63   | **0.87** |   0.73   |   0.78   |   0.86   | **0.85** |   0.79   |\n|                 |       LoRA       |   0.86   |   0.72   |   0.51   |   0.50   |   0.73   | **0.91** |   0.48   |   0.67   |\n|                 |      L-LoRA      | **0.95** | **0.91** |   0.65   | **0.74** | **0.86** |   0.96   |   0.78   | **0.84** |\n|  Ties-Merging   | full fine-tuning |   0.78   |   0.65   | **0.92** |   0.69   | **0.83** |   0.88   | **0.93** | **0.81** |\n|                 |       LoRA       |   0.81   |   0.74   |   0.54   |   0.50   |   0.71   |   0.92   |   0.40   |   0.66   |\n|                 |      L-LoRA      | **0.89** | **0.91** |   0.60   |   0.69   |   0.82   | **0.97** |   0.51   |   0.77   |\n|     LoraHub     | full fine-tuning |   0.93   | **0.90** |   0.32   |   0.30   |   0.47   |   0.77   |   0.13   |   0.55   |\n|                 |       LoRA       |   0.83   |   0.68   |   0.38   |   0.76   |   0.70   |   0.86   |   0.64   |   0.69   |\n|                 |      L-LoRA      | **0.94** |   0.88   | **0.73** | **0.85** | **0.85** | **0.91** | **0.80** | **0.85** |"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2767/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699948839653,
                "cdate": 1699948839653,
                "tmdate": 1699948839653,
                "mdate": 1699948839653,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cFTSpiz81i",
                "forum": "iynRvVVAmH",
                "replyto": "eomIzzF9zb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2767/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2767/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer HkFi (3)"
                    },
                    "comment": {
                        "value": "**Q5: It seems the proposed L-LoRA method does not perform well on the NLP domain, according to the results in Table 1 and the similarity heatmap in Figure 8.**\n\n**Performance in the NLP Domain.** The observed underperformance in the NLP domain could be due to our initial choice of prompt templates not being optimal for fine-tuning the language models\u2014a slight oversight in our experimental design. Nevertheless, we are confidence that this does not skew the comparison of multi-task model fusion performance between various methods, given that the same prompt template was employed throughout the experiments. \n\nTo enhance the model's output, we should provide more explicit cues within the input. For instance, as suggested in Appendix E.1.6, a more instructive prompt would be \"{sentence}. Is this sentence 'positive' or 'negative'?\" This type of prompt would clearly signal to the language model that the response should be either 'positive' or 'negative', potentially resulting in increased accuracy.\n\nTo demonstrate this, we carried out a series of small-scale multi-task model fusion experiments focusing on three downstream tasks: CoLA, MNLI, and RTE. For all fine-tuning processes, we utilized the Adam optimizer, standardizing the batch size to 16 and setting the number of fine-tuning steps to 2000 for all models. The learning rate was configured to 1e-5 for full fine-tuning, while for both LoRA and L-LoRA, we increased the learning rate to 4e-5. Table 1 presents a comparison between the prompt templates originally used in our study and an improved version of these templates. Table 2 details the individual performance of fine-tuned models employing different prompt templates. In Table 3, we assess the 'exact-match' accuracy on validation datasets through a simple model averaging technique for three-model fusion. Our findings indicate that a better and more predictable prompt yields superior results, with L-LoRA outperforming LoRA in terms of both absolute scores and normalized scores.\n\nTable 1: Comparison of the templates used in our paper and a better version of prompt templates.\n\n| Task | Prompt Templates        | input text                                                                                                                                                                      | target text                                                                 |\n| ---- | ----------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------- |\n| CoLA | Templates in Appendix E | \"cola sentence: {sentence}\"                                                                                                                                                     | \"unacceptable\" if label=0 else \"acceptable\"                                 |\n|      | Better Prompt Templates | \"Indicate if the following sentence is grammatically correct or not:   \\\"{sentence}\\\". **Answere 'acceptable' or 'unacceptable'.**\"                                             | \"unacceptable\" if label=0 else \"acceptable\"                                 |\n| MNLI | Templates in Appendix E | \"mnli hypothesis: {hypothesis} premise: {premise}\"                                                                                                                              | \"entailment\", \"neutral\", \"contradiction\" if   label is 0, 1,2, respectively |\n|      | Better Prompt Templates | \"Does the premise: '{premise}' logically imply, contradict, or is   neutral to the hypothesis: '{hypothesis}'? **Answere with 'entailment',   'contradiction', or 'neutral'.**\" | \"entailment\", \"neutral\", \"contradiction\" if   label is 0, 1,2, respectively |\n| RTE  | Templates in Appendix E | \"rte sentence1: {sentence1} sentence2: {sentence2}\"                                                                                                                             | \"entailment\" if label=0 else \"not_entailment\"                               |\n|      | Better Prompt Templates | \"Does the text: '{sentence1}' entail that '{sentence2}' is true? **Provide   'yes' or 'no'.**\"                                                                                  | \"yes\" if label=0, else \"no\"                                                 |"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2767/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699948899331,
                "cdate": 1699948899331,
                "tmdate": 1699948899331,
                "mdate": 1699948899331,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KKuMZ3A5xR",
                "forum": "iynRvVVAmH",
                "replyto": "eomIzzF9zb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2767/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2767/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Table 2 and Table 3 of 'Response to reviewer HkFi (3)'"
                    },
                    "comment": {
                        "value": "Table 2: individual performance of fine-tuned models with different prompt templates\n\n|                                | Fine-tuning Method | CoLA | MNLI       | RTE        | Mean       |\n| ------------------------------ | ------------------ | ---- | ---------- | ---------- | ---------- |\n| Prompt Templates in Appendix E | full fine-tuning   | 0.75 | 0.82       | 0.85       | 0.81       |\n|                                | LoRA fine-tuning   | 0.69 | 0.76       | 0.83       | 0.76       |\n|                                | L-LoRA fine-tuning | 0.69 | 0.46       | 0.75       | 0.63       |\n| Better Prompt Templates        | full fine-tuning   | 0.75 | 0.83(+1%)  | 0.86(+1%)  | 0.81       |\n|                                | LoRA fine-tuning   | 0.69 | 0.83(+9%)  | 0.84(+11%) | 0.79(+4%)  |\n|                                | L-LoRA fine-tuning | 0.69 | 0.82(+78%) | 0.81(+29%) | 0.77(+22%) |\n\nTable 3: three-model fusion using simple model averaging, we evaluate 'exact-match' accuracy on validation datasets.  These results show that a better and more predictable prompt yields superior results, with L-LoRA outperforming LoRA in terms of both absolute scores and normalized scores. (v0: prompt templates in Appendix E; v1: better prompt templates.)\n\n|                  |                    | v0 |          |          |          |    | v1 |          |          |          |\n|------------------|--------------------|--------------------------------|----------|----------|----------|----|-------------------------|----------|----------|----------|\n|                  | fine-tuning method | CoLA                           | MNLI     | RTE      | Mean     | \\| | CoLA                    | MNLI     | RTE      | MEAN     |\n| Absolute Score   | full-finetuning    | **0.67**                       | **0.37** | **0.54** | **0.53** | \\| | **0.70**                | **0.78** | **0.82** | **0.76** |\n|                  | LoRA               | 0.35                           | 0.00     | **0.42** | **0.25** | \\| | 0.69                    | 0.63     | 0.82     | 0.71     |\n|                  | L-LoRA             | 0.35                           | 0.00     | 0.23     | 0.20     | \\| | 0.69                    | **0.73** | 0.81     | **0.74** |\n| Normalized Score | full-finetuning    | **0.90**                       | **0.45** | **0.63** | **0.66** | \\| | 0.93                    | **0.94** | 0.95     | 0.94     |\n|                  | LoRA               | 0.50                           | 0.00     | **0.50** | **0.34** | \\| | 1.00                    | 0.76     | 0.98     | 0.91     |\n|                  | L-LoRA             | **0.51**                       | 0.00     | 0.31     | 0.28     | \\| | 1.00                    | **0.89** | **1.00** | **0.96** |\n\n\nAdditionally, the performance discrepancies might also stem from inherent differences in task characteristics and dataset nature within the NLP field."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2767/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699948937676,
                "cdate": 1699948937676,
                "tmdate": 1699950005708,
                "mdate": 1699950005708,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "S267oHNOGB",
                "forum": "iynRvVVAmH",
                "replyto": "eomIzzF9zb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2767/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2767/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer HkFi (4)"
                    },
                    "comment": {
                        "value": "**W3 & Q6: The L-LoRA has a large performance drop on single-task fine-tuning (Figure 7) and a slight increase on the merged case (Table 1, especially in the NLP domain) compared to LoRA. Thus, it is unclear what are the advantages of L-LoRA compared to LoRA. Why use the proposed L-LoRA rather than the existing LoRA?**\n\n**Advantages of L-LoRA over LoRA.** Despite the performance drop in single-task fine-tuning observed in Figure 7, L-LoRA demonstrates improved performance in the multi-task fusion setting. The increase in performance in the merged case, especially in the NLP domain, suggests that L-LoRA may offer advantages in scenarios where multi-task performance is prioritized.\n\nThe multi-task domain is particularly challenging due to the need to balance and optimize across various objectives. The performance gains achieved by L-LoRA in this setting suggest that our method can effectively navigate the trade-offs between tasks, finding what could be described as a Pareto optimal point\u2014a solution where no task's performance can be improved without compromising another's. The advantages of L-LoRA become evident when considering the broader context of its application. The advantages of L-LoRA become evident when considering the broader context of its application. While single-task performance is important, the ability to excel in multi-task environments is increasingly becoming a benchmark for success in advanced machine learning systems. Our method contributes to this goal by enhancing multi-task model fusion capabilities, which is a significant step forward in developing more capable and efficient AI systems.\n\nBy the way, based on the small-scale model fusion experiments we conducted in response to Q5, we observe that superior prompt templates can significantly narrow the performance gap between L-LoRA and LoRA. But searching for better prompt templates is not the focus of our research in this paper.\n\n---\n\n**Minor concern 1: caption of figure 3(c)**\n\nThis is a typo, thanks for pointing it out and we will correct it. \"(c) Linearized...\" should be \"(d) Linearized...\""
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2767/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699949030689,
                "cdate": 1699949030689,
                "tmdate": 1699949030689,
                "mdate": 1699949030689,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "I6BZeedfKF",
                "forum": "iynRvVVAmH",
                "replyto": "S267oHNOGB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2767/Reviewer_HkFi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2767/Reviewer_HkFi"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the authors for the rebuttal. I still have some concerns about my initial comments.\n\n1. Novelty: If my understanding is correct, this paper uses the method in [1] in LoRA fine-tuning instead of the full fine-tuning in [1]. Are there any technical challenges when adapting the method in [1] to LoRA?\n\n2. Task vector of LoRA: Since the definition of the task vector for LoRA in this paper is different from other works like [3, 4], I suggest the authors discuss it in the paper. And $\\phi_0$ should be clearly defined in the paper.\n\n3. Since LoRA matrixes across all tasks share the same initialization $\\phi_0$, do you conduct repeated experiments with different $\\phi_0$ or explore the effect of $\\phi_0$? \n\n3. The feedback of Q4: (1) why the mean results in this table are different from the one in Table 1 of the paper? (2) why do the results of L-LoRA/LoRA have a large difference with full fine-tuning in different datasets? For example, using the simple average method, the results of L-LoRA in DTD and SVHN are 0.86 and 0.19, respectively, but the results of full fine-tuning are 0.62 and 0.56. (3) have you added this table to the revised paper?\t\n\n4. The NLP experiments: I suggest the authors add the results with new prompt templates to the paper."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2767/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700645365012,
                "cdate": 1700645365012,
                "tmdate": 1700645365012,
                "mdate": 1700645365012,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vfYKXeRjku",
                "forum": "iynRvVVAmH",
                "replyto": "HZj0nT8kk3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2767/Reviewer_HkFi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2767/Reviewer_HkFi"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the authors for further feedback, which addresses most of my concerns and enhances my understanding of this paper. I am willing to raise my score now.\n\nBut I still would like to emphasize the novelty of this paper is limited: this paper adapts an existing method in [1] to the LoRA parameters rather than the original whole model parameters in [1] and thus, the efficiency advantage in this paper is apparent because the LoRA parameters are much fewer than the original model parameters."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2767/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670158101,
                "cdate": 1700670158101,
                "tmdate": 1700670158101,
                "mdate": 1700670158101,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3i61Guje6Y",
            "forum": "iynRvVVAmH",
            "replyto": "iynRvVVAmH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2767/Reviewer_iJMj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2767/Reviewer_iJMj"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new method for Parameter-Efficient Fine Tuning (PEFT) of large pre-trained foundational models for multi-task models. The authors build on prior work in weight disentanglement (Ortiz-Jimenez et al. 2023) and extend it to LoRA for better fusion of models. The authors hypothesise that partial linearisation of a model through the LoRA modules during fine-tuning can improve weight disentanglement, which is conducive to better task arithmetic. Results are shown on a variety of experiments whereupon the proposed models outperforms others on vision classification tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This is a very well written paper, motivations are clear, results are (mostly) well presented and it is clear to understand the results.\n\n2. The idea to perform partial linearisation on LoRA modules is interesting and nicely presented. The results, especially on CLIP-ViT-B-16 (Figure 5a) are compelling argument to the original hypothesis of the work being correct."
                },
                "weaknesses": {
                    "value": "1. A key result of the paper is the result from Appendix A which allows the authors to hypothesize that partial linearization of a subset of module parameters (here LoRA) can improve weight disentanglement.\n\n    1a. This result needs to be in the main body of the text and properly explained. Without, it is difficult to \n    understand exactly why the authors claim this.\n\n    1b. Having checked the derivation in Appendix A, the authors show that the model output of a linearised \n    model is only determined by the gradient of the loss in the non-linearised model on task $t_i$. How \n    exactly then does this allow the authors to make the central hypothesis, which guides the presented \n    method?\n\n2. The results on the NLP task (Flan T-5-Base) need to be better explain. The method (L-LoRA) not only performs worse than full fine-tuning (so does LoRA fine-tuning) but also worse on average than LoRA. Why is this? Is the presented model only applicable to vision tasks?"
                },
                "questions": {
                    "value": "1. Section 4.2 introduces parameter scaling laws for weight disentanglement. However, what does this have to do with the method or key results? The scaling laws suggest over-parameterisation is necessary for weight disentanglement. I struggle to see the connection between this and the need to partially linearize a model for fine-tuning.\n\n2. A remark in the manuscript is made bottom of page 8 that \"...higher cosine similarity...implies greater redundancy and overlap...This results in more destructive task interference with na\u00efve merging\". I don't follow this as it seems to be opposite to most work in multi-task training. In that setting, na\u00efve training would favour similar tasks as it would not require methods to mitigate task interference (see GradNorm method). Why is this then opposite in the context of this paper?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2767/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2767/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2767/Reviewer_iJMj"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2767/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698797496849,
            "cdate": 1698797496849,
            "tmdate": 1700707730033,
            "mdate": 1700707730033,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DOtQ6FpuyM",
                "forum": "iynRvVVAmH",
                "replyto": "3i61Guje6Y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2767/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2767/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer iJMj (1)"
                    },
                    "comment": {
                        "value": "We are grateful for your insightful comments and the opportunity to clarify the aspects of our work that you found unclear. Your feedback is invaluable in helping us refine our paper. Thank you for your thorough review and for considering our work.\n\n---\n\n**W1: A key result of the paper is the result from Appendix A which allows the authors to hypothesize that partial linearization of a subset of module parameters (here LoRA) can improve weight disentanglement.**\n1. **W1a: This result needs to be in the main body of the text and properly explained. Without, it is difficult to understand exactly why the authors claim this.**\n\n**Key result placement.** We acknowledge your suggestion regarding the placement of the key result from Appendix A. We will provide a more detailed explanation in section 4.\n\n2. **W1b: Having checked the derivation in Appendix A, the authors show that the model output of a linearized model is only determined by the gradient of the loss in the non-linearized model on task $\\tau_i$. How exactly then does this allow the authors to make the central hypothesis, which guides the presented method?**\n\n**Connection between linearization and weight disentanglement.** We agree that the derivation is central to our hypothesis and warrants more visibility. We will clarify this explanation in the manuscript to make the connection more explicit.\n\nThe derivation in Appendix A shows that the change of the linearized model output is determined by the gradient of the loss. The connection to central hypothesis is that this property of linearized models enables a more predictable and controlled update during fine-tuning. By fine-tuning in the linearized space, updates of trainable parameters are perpendicular to the loss contours, this orthogonality promotes that the updates contribute to the separation of the weight representations corresponding to distinct tasks, a concept known as weight disentanglement.\n\nMathematically, the change of linearized model output for a input sample $x$ around the initialization $\\phi(0)$ can be written as:\n\n$$\nf^{\\text{lin}}_{\\theta_0}(x; \\theta(\\Delta t))-f\\_{\\theta_0}(x;\\theta(0))\n=\\nabla\\_{\\theta_0}f(x;\\theta_0)^\\top (\\theta(\\Delta t)-\\theta(0))\n=-\\eta \\mathbb{E}\\_{(x\\_{\\tau_i},y\\_{\\tau_i})\\sim D\\_{\\tau_i}} [\n\\boldsymbol{K}(x, x\\_{\\tau_i}; \\theta(0))\n\\nabla_f \\mathcal{L}\\_{\\text{CE}}(f\\_{\\theta_0}(x\\_{\\tau_i}; \\theta(0)), y\\_{\\tau_i})]\n$$\n\nThus we have the following formula that captures the relationship between task-specific gradient of the loss and weight disentanglement for linearized model\n\n$$\\theta(\\Delta t)-\\theta(0) \\propto \\mathbb{E}\\_{(x\\_{\\tau_i},y\\_{\\tau_i})\\sim D\\_{\\tau_i}} \n[\\boldsymbol{K}(x, x\\_{\\tau_i}; \\theta(0))\n\\nabla_f \\mathcal{L}\\_{\\text{CE}}(f\\_{\\theta_0}(x_{\\tau_i}; \\theta(0)), y_{\\tau_i})]$$\n\n---"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2767/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699948255259,
                "cdate": 1699948255259,
                "tmdate": 1699948255259,
                "mdate": 1699948255259,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BFQXq1t9nj",
                "forum": "iynRvVVAmH",
                "replyto": "3i61Guje6Y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2767/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2767/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer iJMj (2)"
                    },
                    "comment": {
                        "value": "**W2: The results on the NLP task (Flan T-5-Base) need to be better explain. The method (L-LoRA) not only performs worse than full fine-tuning (so does LoRA fine-tuning) but also worse on average than LoRA. Why is this? Is the presented model only applicable to vision tasks?**\n\n**Performance on NLP Tasks.** Our selection of prompts was rather naive, following the approach used by (Raffel et al. in 2020), which was a minor oversight in our experimental design. Nonetheless, we are confident that this does not compromise the comparative analysis of multi-task model fusion performance across different methods, as the same prompt template was consistently applied in all experiments. We find that with fine-tuning of the prompt selection and other task-specific adjustments, L-LoRA can be made effective in the NLP domain. \n\nTo demonstrate this, we carried out a series of small-scale multi-task model fusion experiments focusing on three downstream tasks: CoLA, MNLI, and RTE. For all fine-tuning processes, we utilized the Adam optimizer, standardizing the batch size to 16 and setting the number of fine-tuning steps to 2000 for all models. The learning rate was configured to 1e-5 for full fine-tuning, while for both LoRA and L-LoRA, we increased the learning rate to 4e-5. Table 1 presents a comparison between the prompt templates originally used in our study and an improved version of these templates. Table 2 details the individual performance of fine-tuned models employing different prompt templates. In Table 3, we assess the 'exact-match' accuracy on validation datasets through a simple model averaging technique for three-model fusion. Our findings indicate that a better and more predictable prompt yields superior results, with L-LoRA outperforming LoRA in terms of both absolute scores and normalized scores.\n\nTable 1: Comparison of the templates used in our paper and a better version of prompt templates.\n\n| Task | Prompt Templates | input text | target text | \n| ---- | ----------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------- |\n| CoLA | Templates in Appendix E | \"cola sentence: {sentence}\"                                                                                                                                                     | \"unacceptable\" if label=0 else \"acceptable\" |\n| | Better Prompt Templates | \"Indicate if the following sentence is grammatically correct or not:   \\\"{sentence}\\\". **Answere 'acceptable' or 'unacceptable'.**\" | \"unacceptable\" if label=0 else \"acceptable\" |\n| MNLI | Templates in Appendix E | \"mnli hypothesis: {hypothesis} premise: {premise}\"                                                                                                                              | \"entailment\", \"neutral\", \"contradiction\" if   label is 0, 1,2, respectively |\n| | Better Prompt Templates | \"Does the premise: '{premise}' logically imply, contradict, or is   neutral to the hypothesis: '{hypothesis}'? **Answere with 'entailment',   'contradiction', or 'neutral'.**\" | \"entailment\", \"neutral\", \"contradiction\" if   label is 0, 1,2, respectively |\n| RTE  | Templates in Appendix E | \"rte sentence1: {sentence1} sentence2: {sentence2}\"                                                                                                                             | \"entailment\" if label=0 else \"not_entailment\" |\n| | Better Prompt Templates | \"Does the text: '{sentence1}' entail that '{sentence2}' is true? **Provide   'yes' or 'no'.**\"                                                                                  | \"yes\" if label=0, else \"no\" |"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2767/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699948488836,
                "cdate": 1699948488836,
                "tmdate": 1699948488836,
                "mdate": 1699948488836,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CK0WmIeoje",
                "forum": "iynRvVVAmH",
                "replyto": "3i61Guje6Y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2767/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2767/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Table 2 & Table 3 of 'Response to reviewer iJMj (2)'"
                    },
                    "comment": {
                        "value": "Table 2: individual performance of fine-tuned models with different prompt templates\n\n| | Fine-tuning Method | CoLA | MNLI | RTE | Mean |\n| ------------------------------ | ------------------ | ---- | ---------- | ---------- | ---------- |\n| Prompt Templates in Appendix E | full fine-tuning   | 0.75 | 0.82       | 0.85       | 0.81|\n| | LoRA fine-tuning| 0.69 | 0.76| 0.83 | 0.76|\n| | L-LoRA fine-tuning | 0.69 | 0.46| 0.75 | 0.63|\n| Better Prompt Templates| full fine-tuning   | 0.75 | 0.83(+1%)  | 0.86(+1%)  | 0.81|\n|| LoRA fine-tuning   | 0.69 | 0.83(+9%)  | 0.84(+11%) | 0.79(+4%)  |\n|| L-LoRA fine-tuning | 0.69 | 0.82(+78%) | 0.81(+29%) | 0.77(+22%) |\n\nTable 3: three-model fusion using simple model averaging, we evaluate 'exact-match' accuracy on validation datasets. (v0: prompt templates in Appendix E; v1: better prompt templates.)\n\n|                  |                    | v0 |          |          |          |    | v1 |          |          |          |\n|------------------|--------------------|--------------------------------|----------|----------|----------|----|-------------------------|----------|----------|----------|\n|                  | fine-tuning method | CoLA                           | MNLI     | RTE      | Mean     | \\| | CoLA                    | MNLI     | RTE      | MEAN     |\n| Absolute Score   | full-finetuning    | **0.67**                       | **0.37** | **0.54** | **0.53** | \\| | **0.70**                | **0.78** | **0.82** | **0.76** |\n|                  | LoRA               | 0.35                           | 0.00     | **0.42** | **0.25** | \\| | 0.69                    | 0.63     | 0.82     | 0.71     |\n|                  | L-LoRA             | 0.35                           | 0.00     | 0.23     | 0.20     | \\| | 0.69                    | **0.73** | 0.81     | **0.74** |\n| Normalized Score | full-finetuning    | **0.90**                       | **0.45** | **0.63** | **0.66** | \\| | 0.93                    | **0.94** | 0.95     | 0.94     |\n|                  | LoRA               | 0.50                           | 0.00     | **0.50** | **0.34** | \\| | 1.00                    | 0.76     | 0.98     | 0.91     |\n|                  | L-LoRA             | **0.51**                       | 0.00     | 0.31     | 0.28     | \\| | 1.00                    | **0.89** | **1.00** | **0.96** |"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2767/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699948513577,
                "cdate": 1699948513577,
                "tmdate": 1699949957653,
                "mdate": 1699949957653,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fNCd888puo",
                "forum": "iynRvVVAmH",
                "replyto": "3i61Guje6Y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2767/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2767/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer iJMj (3)"
                    },
                    "comment": {
                        "value": "**Q1: Section 4.2 introduces parameter scaling laws for weight disentanglement. However, what does this have to do with the method or key results? The scaling laws suggest over-parameterisation is necessary for weight disentanglement. I struggle to see the connection between this and the need to partially linearize a model for fine-tuning.**\n\n**Parameter Scaling Law.** The parameter scaling law is an empirical phenomenon suggesting that models with a larger number of trainable parameters tend to exhibit better performance in multi-task model fusion. Consequently, a logical deduction would be that parameter-efficient fine-tuning could potentially diminish the performance of multi-task model fusion. Indeed, our experimental results corroborate this (as illustrated in Figure 5). \n\nHowever, our method stands out in that the performance of L-LoRA's multi-task model fusion remains on par with that of full fine-tuning, despite its parameter efficiency. This indicates that partial linearization, with a very limited increase in inference cost overhead, effectively bridges the gap typically observed between parameter efficiency and multi-task fusion performance.\n\n---\n\n**Q2: A remark in the manuscript is made bottom of page 8 that \"...higher cosine similarity...implies greater redundancy and overlap...This results in more destructive task interference with na\u00efve merging\". I don't follow this as it seems to be opposite to most work in multi-task training. In that setting, na\u00efve training would favour similar tasks as it would not require methods to mitigate task interference (see GradNorm method). Why is this then opposite in the context of this paper?**\n\n**cos similarity and task interference.** You've raised an insightful point. When we discuss the impact of high cosine similarity on model performance, it's important to distinguish between the contexts of multi-task training and model fusion. *In the former, tasks with similar characteristics often benefit from the ability to share representations, as this commonality can facilitate learning by leveraging the same features or patterns.*\n\nHowever, in the context of model fusion, which is the focus of our paper, the dynamics differ from simultaneous multi-task training. *Here, we are dealing with models that have been independently trained on separate tasks and are subsequently being merged.* In such a post-hoc fusion, a high cosine similarity between task weights is indicative of overlap in the information encoded by the models, thus be problematic. While this might suggest a common underlying structure, it also raises the risk that merging these similar weights could amplify shared errors or redundancies, rather than complementing each model's strengths. It can signal redundancy, where multiple models encode similar information without adding new insights, or even conflict, where the shared information is not entirely compatible across tasks.\n\nIn summary, while shared representations are advantageous in simultaneous multi-task learning, our work suggests that for the fusion of independently trained models, distinctiveness in task representations\u2014achieved through orthogonality\u2014is key to preserving the integrity and performance of the fused model. This distinction is crucial for understanding the different implications of cosine similarity in multi-task training versus model fusion contexts."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2767/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699948583003,
                "cdate": 1699948583003,
                "tmdate": 1699948583003,
                "mdate": 1699948583003,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "j5qhbo0pGg",
                "forum": "iynRvVVAmH",
                "replyto": "DOtQ6FpuyM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2767/Reviewer_iJMj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2767/Reviewer_iJMj"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your response. I\u2019ve read the rebuttal across all reviews.\n\nIn view of the rebuttal - I will change my score from 5 to 6"
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2767/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700707707604,
                "cdate": 1700707707604,
                "tmdate": 1700707707604,
                "mdate": 1700707707604,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "aBi4FrnLpB",
            "forum": "iynRvVVAmH",
            "replyto": "iynRvVVAmH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2767/Reviewer_tr5D"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2767/Reviewer_tr5D"
            ],
            "content": {
                "summary": {
                    "value": "Efficient finetuning on the pretrained large model has been an important topic. In this work, a partial linearization method (L-Lora) is proposed under the context of PERF(parameter-efficient finetuning).  The key idea is applying linearization to adapter modules and applies task arithmetic over the linearized adapters. In practice, first-order Tayler expansion is used to linearize the model dynamics at time $t$. Based on the derivation from a neural tangent kernel theory, the hypothesis is that partial linearization of a subset of model parameters during fine-tuning can also improve weight disentanglement compared to full non-linear fine-tuning. CLIP and Flan-T5 are used to verify the hypothesis in vision-language and language domains."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Evaluations are conducted on both vision-language and language tasks. \n- The proposed method achieved significant performance improvement, compared with the standard LoRA strategy."
                },
                "weaknesses": {
                    "value": "- In vision domain, only the high-level vision task like image classification tasks evaluated, the mid-level and low level task are missing, for example, semantic segmentation."
                },
                "questions": {
                    "value": "- From table 1, seems the L-LoRA method are outperforming full-finetuning under some model fusion settings, do we have some possible illustrations?\n- Is it possible to evaluate L-LoRA on image segmentation foundational models like SAM?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2767/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698822207887,
            "cdate": 1698822207887,
            "tmdate": 1699636219356,
            "mdate": 1699636219356,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZznfQQRh5z",
                "forum": "iynRvVVAmH",
                "replyto": "aBi4FrnLpB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2767/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2767/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer tr5D"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful review and constructive feedback on our paper. We are grateful for your positive assessment of our work and appreciate the strengths you've highlighted.\n\n**Q1: From table 1, seems the L-LoRA method are outperforming full-finetuning under some model fusion settings, do we have some possible illustrations?**\n\n**Results from Table 1.** Indeed, as observed in Table 1, our L-LoRA (Linearized Low-Rank Adaptation) appears to outperform full fine-tuning in terms of normalized score under certain fusion settings. \n\nOn one hand, the partially linearization likely leads to better weight disentanglement*, meaning that the task-specific knowledge encoded in the weights is more separable. \n\nOn the other hand, we should consider *the data requirement of full fine-tuning versus parameter-efficient fine-tuning. As for fine-tuning, the amount of data in downstream tasks is sometimes not enough. However, full fine-tuning generally demands extensive data to optimize the large number of parameters effectively. In scenarios where abundant data is available, full fine-tuning is expected to excel as it can leverage the comprehensive dataset to refine a large amount of model's weights for each specific task. However, in situations with limited data, full fine-tuning may not perform optimally due to overfitting concerns or insufficient information for the model.\n\nL-LoRA, on the other hand, with its focus on updating only a subset of the model's parameters, is less dependent on large amounts of data. This characteristic could explain why L-LoRA sometimes outperforms full fine-tuning in multi-task fusion settings, especially when data availability of downstream dataset is a limiting factor.\n\n---\n\n**W1 & Q2: In vision domain, only the high-level vision task like image classification tasks evaluated, the mid-level and low level task are missing, for example, semantic segmentation. Is it possible to evaluate L-LoRA on image segmentation foundational models like SAM?**\n\n**Extending evaluation on mid-level and low-level task (Evaluating on image segmentation models).** We acknowledge the limitation you pointed out regarding the scope of our evaluations in the vision domain. Our decision to focus on high-level vision tasks like image classification was driven by the desire to establish a strong foundational understanding of multi-task model fusion in a well-studied context, and we agree that extending the evaluation to lower-level vision tasks would provide a more comprehensive understanding.\n\nEvaluating our method on mid-level and low-level task, such as image segmentation, is indeed possible. This is due to the task-agnostic nature of our approach, which does not impose specific requirements on the loss function. The only prerequisite for our method is that the tuned parameter-efficient modules must be capable of computing the Jacobian-vector product, thereby enabling it to be efficiently linearized. \n\nTo demonstrate this, we conducted a small-scale experiment. We utilized the SAM model ('sam_vit_b_01ec64.pth') to test the segmentation tasks with both LoRA and L-LoRA on the Pascal VOC 2012 and NYUD v2 datasets. We set the batch size to 4 and the LoRA r hyperparameter to 32. *Due to time constraints and resource limitations, the models were still far from convergence at this point, but the results were sufficient to demonstrate the effectiveness of L-LoRA (The code for this experiment will be put on GitHub together with a clean version of the original code).* We report the performance of the pre-trained models, the fine-tuned models, and the fused models obtained through weight averaging on the validation sets of Pascal VOC and NYUD v2 in terms of mIoU in the table below.\n\nTable: reported mIoU of LoRA and L-LoRA.\n\n| Model                             | fine-tuning method | VOC 2012 | NYUD v2 | Mean mIoU |\n|-----------------------------------|--------------------|----------|---------|-----------|\n| Pretrained Model                  |                    | 0.65     | 0.75    | 0.70      |\n| fine-tuned (far from converge)    | LoRA               | **4.51**     | **2.42**    | **3.47**      |\n|                                   | L-LoRA             | 4.09     | 1.93    | 3.01      |\n| two-model fusion (simple average) | LoRA               | 1.92(43%)     | 1.15(47%)    | 1.53      |\n|                                   | L-LoRA             | **4.05(99%)**     | **1.51(78%)**    | **2.78**      |\n\nNotably, the two-model fusion using simple averaging yields different results for each method. The LoRA fusion only retains 43% and 47% of the pretrained model's performance on VOC 2012 and NYUD v2, respectively. In contrast, the L-LoRA fusion maintains a remarkable 99% and 78% of the fine-tuned LoRA's performance on the respective datasets, resulting in a mean mIoU of 2.78."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2767/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699947665414,
                "cdate": 1699947665414,
                "tmdate": 1699947665414,
                "mdate": 1699947665414,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AHPPWAhKGx",
                "forum": "iynRvVVAmH",
                "replyto": "ZznfQQRh5z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2767/Reviewer_tr5D"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2767/Reviewer_tr5D"
                ],
                "content": {
                    "title": {
                        "value": "response to rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for the detailed response and the new experiment results. I will keep my rating and it will be great if you can add these results to the paper"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2767/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700638336734,
                "cdate": 1700638336734,
                "tmdate": 1700638336734,
                "mdate": 1700638336734,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]