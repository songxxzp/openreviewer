[
    {
        "title": "Assessing the Impact of Distribution Shift on Reinforcement Learning Performance"
    },
    {
        "review": {
            "id": "0ziTjbAwLt",
            "forum": "esh9JYzmTq",
            "replyto": "esh9JYzmTq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4448/Reviewer_okG3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4448/Reviewer_okG3"
            ],
            "content": {
                "summary": {
                    "value": "Reinforcement learning (RL) contains unique challenges in fixing its reproducibility problem. Current displays of evaluation take attention away from other important factors such as model overfitting and experimental design. RL researchers have developed various reliability evaluation metrics to understand the strengths and weaknesses of each RL algorithm, but these metrics do not take out-of-distribution observations into account. The authors propose time series analysis tools to measure model robustness under the presence of distribution shift. They apply these analytical tools in both single-agent and multi-agent environments to show the effect of introducing distribution shifts during test time."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "I appreciate the plot showing the flaw of solely relying on point estimates to perform model evaluation."
                },
                "weaknesses": {
                    "value": "The authors don't list potential weaknesses of their current method in the main text."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4448/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4448/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4448/Reviewer_okG3"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4448/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698729380271,
            "cdate": 1698729380271,
            "tmdate": 1699636420169,
            "mdate": 1699636420169,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kQR3JWPRmW",
                "forum": "esh9JYzmTq",
                "replyto": "0ziTjbAwLt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4448/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4448/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We thank the reviewer for the feedback"
                    },
                    "comment": {
                        "value": "> **The authors don't list potential weaknesses of their current method in the main text.**\n\nThank you for pointing that out. The second paragraph in section 6 of the improved version now mentions some possible weaknesses of our methodology.\n\nWe also invite reviewer okG3 to read the improved version of the main article and appendix. There are some significant changes that we believe enhance the quality of the paper."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4448/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700277458105,
                "cdate": 1700277458105,
                "tmdate": 1700277458105,
                "mdate": 1700277458105,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9kxykQDlvi",
            "forum": "esh9JYzmTq",
            "replyto": "esh9JYzmTq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4448/Reviewer_8p4f"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4448/Reviewer_8p4f"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the evaluation of RL algorithms against distribution shifts.\nIn particular, it proposes the usage of evaluation techniques from the time series literature to take into account changes in the environment."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- The paper is written in an intuitive way, which would help the adoption of the proposed methodology by the RL community.\n\n- The evaluation of RL algorithms is certainly an important issue"
                },
                "weaknesses": {
                    "value": "- It did not come across to me what is the precise problem the paper is trying to solve. The problem formulation is somewhat vague and relies on artificial examples that make it difficult to connect it with the application of RL algorithms.\n\n- The methodology proposed is somewhat scattered. It is unclear how this evaluation methodology would be applied. Perhaps the paper could include a pseudo-code or a flowchart to ground all the steps of the proposed methodology.\n\n- Although the paper provides several examples, it does not provide a proper interpretation of the results. For example, in Figure 5, which agent is more robust? In Figure 3, what is the conclusion regarding the performance of A2C and PPO? Should we favor one of them in practice?\n\n- The related work section only lists the contribution of related papers but does not provide a description of how this paper distinguishes from those."
                },
                "questions": {
                    "value": "1. Could you describe more formally the assumption that \"the trained agents achieved a clear trend in performance\"? In particular, how is this related to the convergence of an RL agent? Does it mean the agent has reached an optimal performance? Furthermore, as the assumption does not consider a distribution shift, does it mean the agent continues to update its behavior?\n2. Could you comment on the connections from this methodology with non-stationary MDPs?\n3. Considering the change in the dynamics of the environment, I think we cannot always conclude that the change in performance is due to the agent's behavior. For instance, in some cases, although the dynamics change, the optimal policy may remain the same. Could this methodology help identify if the agent is underperforming?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4448/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4448/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4448/Reviewer_8p4f"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4448/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698827009196,
            "cdate": 1698827009196,
            "tmdate": 1699636420058,
            "mdate": 1699636420058,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "f3JXl6YYnr",
                "forum": "esh9JYzmTq",
                "replyto": "9kxykQDlvi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4448/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4448/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We thank the reviewer for their valuable feedback"
                    },
                    "comment": {
                        "value": "> **It did not come across to me what is the precise problem the paper is trying to solve. The problem formulation is somewhat vague and relies on artificial examples that make it difficult to connect it with the application of RL algorithms.**\n\nSorry for not making this clearer. Accounting for distribution shift has not been previously investigated in reliable RL. This is a more realistic setting for agents that will be deployed in the real world. Hence, we wanted to provide a methodology for this under-researched problem that measures RL performance under distribution shift using time series. Past RL reliability papers rely on point estimates alone. Time series analysis is also needed check if a well-trained agent continues to perform well over a period of time. For example, you would want a methodology in controlled settings that shows how a RL-trained self-driving car performs in the presence of many distribution shifts over a long-enough period of time before releasing it out to the public. The new protocol at the end of section 4 should clarify our proposed methodology. The new analyses in section 5 show that our methodology does not rely on artificial examples. \n\n> **It is unclear how this evaluation methodology would be applied. Perhaps the paper could include a pseudo-code or a flowchart to ground all the steps of the proposed methodology.**\n\nThank you for pointing this out. To fix this, we included a clear step-by-step protocol at the end of section 4.\n\n> **In Figure 5, which agent is more robust? In Figure 3, what is the conclusion regarding the performance of A2C and PPO? Should we favor one of them in practice?**\n\nFigure 5 alone is just meant to show what it looks like when there is a significant difference is and what it looks like when the difference is not significant in the observational experiments. As mentioned, section 5 shows some analyses regarding what conclusions to draw from our methodology. We hope the new Atari analysis in section 5, which uses Figure 5 as part of the story, gives a sufficient interpretation of the results in Atari games. \n\n>**The related work section only lists the contribution of related papers but does not provide a description of how this paper distinguishes from those.**\n\nThank you for pointing this out. We included a few more sentences at the end of the last paragraph of section 2 of the improved version to distinguish ourselves from past RL reliability work.\n\n> **Could you describe more formally the assumption that \"the trained agents achieved a clear trend in performance\"? In particular, how is this related to the convergence of an RL agent? Does it mean the agent has reached an optimal performance? Furthermore, as the assumption does not consider a distribution shift, does it mean the agent continues to update its behavior?**\n\nThank you for catching that. What we should have said was \"a flat (slope = 0) trend in raw performance.\" We updated the article to fix this issue. Here, we just assume some baseline performance (not necessarily optimal) for the causal impact plots. We do not assume the agent updates its behavior. That might be interesting for future research to see how agents with adaptive learning can fare under this methodology.\n\n> **Could you comment on the connections from this methodology with non-stationary MDPs?**\n\nSure! With some modifications, our methodology should still hold if the parallel trends assumption holds for the treatment and control groups. This would require verification that both groups show parallel trends in the current environment, which I don't expect to hold in all cases. If this line of thinking were to continue, I expect we would need to use other sophisticated methods of causal inference to draw conclusions about agents in non-stationary environments. One drawback of going outside of stationary MDPs is the possible need to use environment-specific knowledge (or covariates besides rewards), which we deliberately avoid in this paper to keep it simple and general. This is out of the scope of this paper, but interesting nonetheless.\n\n> **Considering the change in the dynamics of the environment, I think we cannot always conclude that the change in performance is due to the agent's behavior. For instance, in some cases, although the dynamics change, the optimal policy may remain the same. Could this methodology help identify if the agent is underperforming?**\n\nWe do not claim our methodology necessarily assigns blame between agent and environment for the decrease in agent performance. Yes, it could be that the agent was able to reach optimal behavior in the training environment, but that does not necessarily mean it will still perform optimally when the test environment is different in some way. Our framework can identify if the agent is underperforming if you know what distribution shift to use during the causal impact experiments. Hopefully, things will become clearer if you read the protocol in section 4."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4448/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700276703626,
                "cdate": 1700276703626,
                "tmdate": 1700276703626,
                "mdate": 1700276703626,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "idBXRQ2QCQ",
            "forum": "esh9JYzmTq",
            "replyto": "esh9JYzmTq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4448/Reviewer_mUDk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4448/Reviewer_mUDk"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a set of evaluation methods that measure the robustness of Reinforcement Learning (RL) algorithms under distribution shifts. The paper argues to account for performance over time while the agent is acting in its environment. The authors recommend time series analysis as a method of observational RL evaluation and show that the unique properties of RL and simulated dynamic environments supports their additional assumptions needed to measure the causal impact in their experimental evaluation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. Detailed background of the various causal inference topics is provided. Even though most of the information can be moved to the appendix, this amount of information makes the paper easy to read for someone new to the field."
                },
                "weaknesses": {
                    "value": "1. Lack of novelty: The paper does not have a novel contribution. The idea of using simulators to perform interventional analysis is not new [1-5]. \n\nThough they talk about the focus in the paper being on \"adversarial attacks on images (Atari game observations) and agent switching in multi-agent environments\", there is nothing that is specific to the adversarial nature of the distribution shifts. The paper as is will be applicable if distribution shifts happen due to any other factor.\n\n2. Verbose description: The setup is unnecessary and made complex to justify the simple idea of using a simulator to perform interventions. Simple things like an average of sampled data is dedicated a definition and equation (eq. 1), which I think is not needed. Only Section 4.2 is something that talks about a new approach, everything before that is motivation or background. \n\n3. Insufficient experimental analysis: Experimental evaluation is not sufficient. Figures 3 and 4 are not analyzed in detail, and the inferences from these experiments are not explained properly. Even in the appendix, only the plots are added without any analysis.\n\n[1] Lee et al., SCALE: Causal Learning and Discovery of Robot Manipulation Skills using Simulation. CoRL 2023.\n\n[2] Verma et al., Learning Causal Models of Autonomous Agents using Interventions. KEPS 2021.\n\n[3] Lee et al., Causal Reasoning in Simulation for Structure and Transfer Learning of Robot Manipulation Policies. ICRA 2021.\n\n[4] Ahmed et al., CausalWorld: A Robotic Manipulation Benchmark for Causal Structure and Transfer Learning. ICLR 2021.\n\n[5] Verma et al., Autonomous Capability Assessment of Black-Box Sequential Decision-Making Systems. KEPS 2023."
                },
                "questions": {
                    "value": "1. How would you comment on the related literature showing that simulators can be used for interventions? How is your idea new compared to them? My failure to understand this is the biggest reason for my score. Maybe I am missing something, and I would appreciate it if you could comment on it. \n\n2. Is the choice of adversarial nature of distribution shift important to the ideas presented in the paper? If we perform an interventional analysis on this paper and make the distribution shifts non-adversarial (something simply changed in the environment), then would the analysis you present still hold? If not, why?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4448/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4448/Reviewer_mUDk",
                        "ICLR.cc/2024/Conference/Submission4448/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4448/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699145773551,
            "cdate": 1699145773551,
            "tmdate": 1700724499675,
            "mdate": 1700724499675,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "q9UWACpjwb",
                "forum": "esh9JYzmTq",
                "replyto": "idBXRQ2QCQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4448/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4448/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We thank the reviewer for their valuable feedback"
                    },
                    "comment": {
                        "value": "> **How would you comment on the related literature showing that simulators can be used for interventions? How is your idea new compared to them? My failure to understand this is the biggest reason for my score. Maybe I am missing something, and I would appreciate it if you could comment on it.**\n\nWe apologize for the confusion. Our paper is a RL reliability/robustness paper. We are following past RL reliability papers like [Measuring the Reliability of Reinforcement Learning Algorithms](https://openreview.net/forum?id=SJlpYJBKvH) and [Deep Reinforcement Learning at the Edge of the Statistical Precipice](https://openreview.net/forum?id=uqv8-U4lKBe). The strength and originality of these papers is not in the metrics themselves, but the proposed protocols that ensure reproducible and reliable RL. We contribute a methodology that includes counterfactual time series analysis, using the impact visualization strategy from Brodersen et al. (2015), as a way to measure the impact of distribution shift after training.  For clarity, we explicitly state in the first paragraph of section 2 of our improved version of the article that it is not a causal machine learning paper and does not claim to be the first to use simulators for interventions. \n\n\n\n> **Verbose description: The setup is unnecessary and made complex to justify the simple idea of using a simulator to perform interventions. Simple things like an average of sampled data is dedicated a definition and equation (eq. 1), which I think is not needed. Only Section 4.2 is something that talks about a new approach, everything before that is motivation or background.**\n\nThank you for the suggestion. In the updated article, we moved eq. 1 and section 4.1 to the appendix.\n\n\n> **Insufficient experimental analysis: Experimental evaluation is not sufficient. Figures 3 and 4 are not analyzed in detail, and the inferences from these experiments are not explained properly. Even in the appendix, only the plots are added without any analysis.**\n\nThank you for pointing this out. In section 5, we included more in-depth analyses in section 5. In the Atari games, we conclude that RL Baselines3 Zoo PPO agents tend to perform better but the impact plots show clear signs of overfitting in some environments. On the other hand, A2C agents tend to be relatively more robust against adversarial attacks in Atari games.\n\n\n> **Is the choice of adversarial nature of distribution shift important to the ideas presented in the paper? If we perform an interventional analysis on this paper and make the distribution shifts non-adversarial (something simply changed in the environment), then would the analysis you present still hold? If not, why?**\n\nSorry about the confusion. No, the choice of using adversarial examples was mainly because it is one of the more well-known types of distribution shift. It was not our intent to focus exclusively on adversarial distribution shifts. In ad hoc agent switching, our original motivation was benign switching of robots that work in a group (e.g. switching teammates in robot soccer/football). We will explicitly state that any distribution shift can be used in our methodology in the second paragraph of section 2 in the updated article."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4448/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700277132318,
                "cdate": 1700277132318,
                "tmdate": 1700277132318,
                "mdate": 1700277132318,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PgaXkb9KOI",
                "forum": "esh9JYzmTq",
                "replyto": "q9UWACpjwb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4448/Reviewer_mUDk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4448/Reviewer_mUDk"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their detailed response. Your response clarifies some aspects, but I still feel the paper itself is not clear enough and can improve with another pass of rewriting. For instance, I gave an example of verbose writing, but it is still verbose in many places, where the same sentence or concept is repeated in multiple places. I do believe the direction is promising, and the modified version is better than the original version. I will increase my score to reflect the changes and better clarity."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4448/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700724477742,
                "cdate": 1700724477742,
                "tmdate": 1700724477742,
                "mdate": 1700724477742,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]