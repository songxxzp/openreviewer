[
    {
        "title": "Time-LLM: Time Series Forecasting by Reprogramming Large Language Models"
    },
    {
        "review": {
            "id": "eMBnjIFJfJ",
            "forum": "Unb5CVPtae",
            "replyto": "Unb5CVPtae",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4366/Reviewer_baDP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4366/Reviewer_baDP"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a pioneering approach, Time-LLM, which harnesses the reprogramming and Prompt-as-Prefix techniques to repurpose large language models for time series forecasting while keeping the backbone LLM intact. The method innovatively bridges input time series to optimized text prototypes, making the time series inputs more digestible for language models. This integration enables large language models to seamlessly tackle time series forecasting. Furthermore, Time-LLM incorporates the Prompt-as-Prefix mechanism to facilitate the LLM's reasoning capabilities over time series. By adding natural language prompts, the approach enriches time series input and presents task directives in a comprehensible language format. In my view, this holds substantial promise for controlled time series analysis across diverse applications. The authors have conducted extensive experiments to demonstrate the effectiveness and efficiency of the proposed reprogramming framework, showing considerable potential in leveraging LLMs for time series tasks."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1.\tThe paper is articulately composed and well-organized, with most concepts clearly presented. It offers an in-depth exploration of the proposed concepts related to LLM reprogramming and Prompt-as-Prefix.\n\n2.\tThe proposed reprogramming framework is novel and has proven to be highly effective. Instead of directly feeding original time series into LLMs, the innovative approach of converting time series into text prototype representations for language model comprehension stands out. This strategy might set the foundation for a broader method of cross-modality adaptation in LLMs.\n\n3.\tThe augmentation of input context with declarative prompts, such as domain expertise and task guidelines, to steer LLM reasoning is notably promising. It offers significant potential for controlled time series analysis in a variety of applications.\n\n4.\tThe architectural design of Time-LLM is logical and underpinned by clear motivations. Dividing time series into patches and reprogramming each into text prototype representations is a wise choice, which aligns well with the use of natural language prompts to guide LLM in time series reasoning.\n\n5.\tComprehensive experimental results are provided to evaluate the proposed reprogramming framework from various aspects. Indeed, Time-LLM demonstrates promising results, especially under the few-shot and zero-shot protocols, showing considerable sample efficiency and applicability in real-word applications. There are also abundant ablation and other side experiments to study the proposed method from various aspects."
                },
                "weaknesses": {
                    "value": "1.\tWhile this paper is generally well-written, there are areas that could benefit from further refinement. For instance, Fig. 5 lacks clarity and might benefit from being displayed at a larger scale to improve visibility. In patch reprogramming (Sec. 3.1), it would be beneficial to illustrate how the linear projection aligns the hidden dimensions in Fig. 2. Within the Prompt-as-Prefix discussion, reference is made to the inclusion of statistics within the prompt. Elaborating on the specific content and the calculation methods employed would enhance clarity.\n\n2.\tThe paper's foundational concept hinges on the idea of reprogramming. However, it omits references to several pivotal reprogramming studies from recent years (as listed below). Integrating these pertinent studies in the introductory or related works section could provide a more robust understanding of the concept.\n\n3.\tThere are some writing issues. For instance, there should be uniformity in the references' formatting, with a preference for citing formally published works from conferences or journals over preprints.\n\n[1] Vinod, R., Chen, P. Y., & Das, P. (2020). Reprogramming Language Models for Molecular Representation Learning. In Annual Conference on Neural Information Processing Systems.\n\n[2] Melnyk, I., Chenthamarakshan, V., Chen, P., Das, P., Dhurandhar, A., Padhi, I., & Das, D. (2022). Reprogramming Pretrained Language Models for Antibody Sequence Infilling. International Conference on Machine Learning."
                },
                "questions": {
                    "value": "1.\tCould you elucidate the mechanism behind \"Output Projection\" (Sec. 3.1), particularly the aspects related to flattening and the linear projection? A formulaic representation would greatly aid in understanding.\n\n2.\tIn the \"Prompt-as-Prefix\" paragraph, there is a reference to computing the trend and lag in relation to time series, with several potential implementations hinted at. Could the authors detail the methodology used to determine the trend and lag information within the prompts?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4366/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698758915179,
            "cdate": 1698758915179,
            "tmdate": 1699636408999,
            "mdate": 1699636408999,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gDztX3ia05",
                "forum": "Unb5CVPtae",
                "replyto": "eMBnjIFJfJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4366/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4366/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer baDP"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer for their valuable suggestions to further improve our paper. We are appreciative of the reviewer's recognition of our contributions as innovative and significant in illuminating future research across a wide range of applications. Below, we address the primary concerns raised by the reviewer.\n\n> W1. While this paper is generally well-written, there are areas that could benefit from further refinement. For instance, Fig. 5 lacks clarity and might benefit from being displayed at a larger scale to improve visibility. In patch reprogramming (Sec. 3.1), it would be beneficial to illustrate how the linear projection aligns the hidden dimensions in Fig. 2. Within the Prompt-as-Prefix discussion, reference is made to the inclusion of statistics within the prompt. Elaborating on the specific content and the calculation methods employed would enhance clarity.\n\n+ We agree with the reviewer's suggestion to highlight Fig. 5 in a larger size. However, due to space constraints in the main text, scaling up Fig. 5 poses challenges as it may compromise the presentation of other results and discussions.\n+ In response to the suggestion of depicting the transformation from $\\mathbf{Z}^{(i)}$ to $\\mathbf{O}^{(i)}$ in Fig. 2, **we have slightly revised this figure in the part of \"Patch Reprogram\"**.\n+ To construct the statistical prompting block shown in Fig. 4, we compute the following representative statistics: (1) the minimum, maximum, and median values of the given time series; (2) the overall trend of the time series, determined by calculating the sum of differences between consecutive time steps. A sum greater than 0 indicates an upward trend, while a lesser sum denotes a downward trend; (3) the top-5 lags of the time series, identified by computing the autocorrelation using fast Fourier transformation and selecting the five lags with the highest correlation values. Please refer to our code for more details, which has been made available to AC and all reviewers. **We have also revised the paper accordingly and included this technical detail in the appendix.**\n\n> W2. The paper's foundational concept hinges on the idea of reprogramming. However, it omits references to several pivotal reprogramming studies from recent years (as listed below). Integrating these pertinent studies in the introductory or related works section could provide a more robust understanding of the concept.\n\nThank you for the suggestion. To enhance comprehension of model reprogramming and more related works, **we have revised and expanded the discussion on cross-modality adaptation in Appendix A**.\n\n> W3. There are some writing issues. For instance, there should be uniformity in the references' formatting, with a preference for citing formally published works from conferences or journals over preprints.\n\nThank you for the suggestion. **We have thoroughly reviewed and updated the reference list, including both the format and venues of the references**. Additionally, considering the rapid growth in LLMs and related studies, many references in our work are recent preprints from 2023. These references are crucial in offering the necessary background knowledge for a better understanding of the proposed method.\n\n> Q1. Could you elucidate the mechanism behind \"Output Projection\" (Sec. 3.1), particularly the aspects related to flattening and the linear projection? A formulaic representation would greatly aid in understanding.\n\nUpon packing and feedforwarding the prompt and patch embeddings $\\mathbf{O}^{i}$ through the frozen LLM, we discard the prefixal part and obtain the output representations, denoted as $\\tilde{\\mathbf{O}}^{i}$. Subsequently, we follow PatchTST [1] and flatten $\\tilde{\\mathbf{O}}^{i}$ into a 1D tensor with the length $P \\times D$, which is then linear projected as $\\hat{\\mathbf{Y}}^{i} \\in \\mathbb{R}^H$. Due to space constraints in the main text, **we have revised the paper accordingly and included this technical detail in Appendix B.1**.\n\n> Q2. In the \"Prompt-as-Prefix\" paragraph, there is a reference to computing the trend and lag in relation to time series, with several potential implementations hinted at. Could the authors detail the methodology used to determine the trend and lag information within the prompts?\n\nSee our response above to W1.\n\nWe hope our responses provided above can adequately address the reviewer's concerns and questions."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4366/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700265879680,
                "cdate": 1700265879680,
                "tmdate": 1700265879680,
                "mdate": 1700265879680,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GlHsTUrRP1",
                "forum": "Unb5CVPtae",
                "replyto": "gDztX3ia05",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4366/Reviewer_baDP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4366/Reviewer_baDP"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the authors for their great effort in clarifying my questions. The substantial addition of new experimental results further strengthens the paper. I will maintain my rating, and I believe a score of 8 is well-deserved."
                    }
                },
                "number": 32,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4366/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700563740768,
                "cdate": 1700563740768,
                "tmdate": 1700563740768,
                "mdate": 1700563740768,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FB48naOU2d",
            "forum": "Unb5CVPtae",
            "replyto": "Unb5CVPtae",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4366/Reviewer_cwaB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4366/Reviewer_cwaB"
            ],
            "content": {
                "summary": {
                    "value": "This work casts time series modeling as yet another \"language\" task by addressing the problem of casting continuous-valued data into a discrete representation and leveraging prompts to the language model. Here the authors draw from previous works, including PatchTST, to patch the continuous valued time series data and project them onto word embeddings via an attention mechanism. The new discretized embeddings and a word prompt are fed into a pretrained LLM which predicts future values. This work produces state-of-the-art results on both short-term and long-term predictions on the M4 and electricity transformer temperature (ETT) datasets, respectively. The work also produces the state-of-the-art results on zero and few-shot learning evaluated on ETT."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Instead of creating a new representation of the continous-valued time series data the authors discretized the data by projecting it onto existing word embeddings via an attention mechanism. This approach is exciting in it's use of the word embeddings as the discretization medium rather than using a linear layer in the final calculation of the embeddings. In doing so, this work shows how using not only trained LLMs but also the learned embeddings can transfer to time series problems.\n\nBy building the final embeddings from word embeddings, the authors cast the time-series problem into a \"language\" problem. This allows the authors to further utilize language model features by employing a prompt which is shown to improve time series predictions. One could imagine expanding further on this new property by trying different prompts and seeing if such prompts can illicit adversarial results. Ultimately, this approach may be utilized for new experiments to both probe LLMs and further exploit their capabilities."
                },
                "weaknesses": {
                    "value": "The authors chose a limited set of time series data to benchmark this model on. Many of the models they compare against have been benchmarked against other data sets: weather, traffic, electricity consumption, and the spread of influenza. This confuses the objective of the paper. If the objective is to show this model gives the state-of-the-art results for the two data sets then that is clear. If the authors wish to claim this model outperforms others across many domains then more evidence is needed. If the authors wish to introduce this model and benchmark it on a single data set to demonstrate its potential capabilities then they provide many examples."
                },
                "questions": {
                    "value": "My primary concern is that the evidence provided, the stated intentions, and the claims of this work do not fully line up. I would like to know the primary objective and claims of this work. Depending on that answer I feel there might be more evidence required. \n\na) If this work is meant to show that the model is state-of-the-art across many potential time series tasks then comparisons across other datasets (mentioned above) would be much more convincing. This work only uses ETT and M4.\n\nb) If this work is meant to benchmark this model's capabilities on a single data set then please make that clear. The state-of-the-art claims should only be made with regard to the data sets the model was benchmarked against."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4366/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4366/Reviewer_cwaB",
                        "ICLR.cc/2024/Conference/Submission4366/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4366/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698807464788,
            "cdate": 1698807464788,
            "tmdate": 1700898570308,
            "mdate": 1700898570308,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4NlQVHfdLN",
                "forum": "Unb5CVPtae",
                "replyto": "FB48naOU2d",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4366/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4366/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer cwaB (Part 1)"
                    },
                    "comment": {
                        "value": "We extend our sincere thanks to the reviewer for their constructive feedback on our paper. We are deeply grateful for the recognition of our contributions and the significant potential of this work. Below, we have addressed the main concern raised by the reviewer.\n\n> W1. The authors chose a limited set of time series data to benchmark this model on. Many of the models they compare against have been benchmarked against other data sets: weather, traffic, electricity consumption, and the spread of influenza. a) If this work is meant to show that the model is state-of-the-art across many potential time series tasks then comparisons across other datasets (mentioned above) would be much more convincing. This work only uses ETT and M4. b) If this work is meant to benchmark this model's capabilities on a single data set then please make that clear. The state-of-the-art claims should only be made with regard to the data sets the model was benchmarked against.\n\n+ Our primary objective in this work is extending the capabilities of large language models to practical time series forecasting within a model reprogramming space. We also mean to evidence that Time-LLM consistently outperforms or matches the state-of-the-art performance in mainstream forecasting tasks, especially in few-shot and zero-shot scenarios. We believe this falls into the category (a) mentioned by the reviewer.\n\n+ We have performed additional experiments on four datasets: (1) Weather, (2) Electricity, (3) Traffic, and (4) Influenza-like Illness (ILI). **Our detailed results can be found in the main text (i.e., Tables 1,3, and 4) and appendices (i.e., Tables 10,11,14, and 15) of the revised paper**. For the reviewer's convenience, *a very brief summary* is provided below, highlighting the overall performance on a set of most representative methods on additionaly added four datasets.\n\n  + A concise summary of additional long-term forecasting results in average on Weather, Electricity, Traffic, and Influenza-like Illness (ILI):\n\n  | Method      | Time-LLM  |                     | GPT4TS |       | DLinear |       | PatchTST            |                     | TimesNet |       | FEDformer |       |\n  | ----------- | --------- | ------------------- | ------ | ----- | ------- | ----- | ------------------- | ------------------- | -------- | ----- | --------- | ----- |\n  | Metric      | MSE       | MAE                 | MSE    | MAE   | MSE     | MAE   | MSE                 | MAE                 | MSE      | MAE   | MSE       | MAE   |\n  | Weather     | **0.225** | **0.257**           | 0.237  | 0.270 | 0.248   | 0.300 | **0.225**           | $\\underline{0.264}$ | 0.259    | 0.287 | 0.309     | 0.360 |\n  | Electricity | **0.158** | **0.252**           | 0.167  | 0.263 | 0.166   | 0.263 | $\\underline{0.161}$ | **0.252**           | 0.192    | 0.295 | 0.214     | 0.327 |\n  | Traffic     | **0.388** | $\\underline{0.264}$ | 0.414  | 0.294 | 0.433   | 0.295 | $\\underline{0.390}$ | **0.263**           | 0.620    | 0.336 | 0.610     | 0.376 |\n  | ILI         | **1.435** | $\\underline{0.801}$ | 1.925  | 0.903 | 2.169   | 1.041 | $\\underline{1.443}$ | **0.797**           | 2.139    | 0.931 | 2.847     | 1.144 |"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4366/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700265416609,
                "cdate": 1700265416609,
                "tmdate": 1700265416609,
                "mdate": 1700265416609,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dtsrprOJHP",
                "forum": "Unb5CVPtae",
                "replyto": "Dwl4lk56mS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4366/Reviewer_cwaB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4366/Reviewer_cwaB"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for thoroughly addressing my concerns. I would like for the authors to the statement above in the main text as it would have helped improve clarity: \"Our primary objective in this work is extending the capabilities of large language models to practical time series forecasting within a model reprogramming space. We also mean to evidence that Time-LLM consistently outperforms or matches the state-of-the-art performance in mainstream forecasting tasks, especially in few-shot and zero-shot scenarios.\"\n\nAt this time I do not have any more concerns related to my review and I am inclined to change my referral to accept. I would like to discuss with reviewer L3pe about their concerns with regard to selecting the best test score. Therefore my final referral will be contingent on that."
                    }
                },
                "number": 35,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4366/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700681741669,
                "cdate": 1700681741669,
                "tmdate": 1700681741669,
                "mdate": 1700681741669,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5j35okXerl",
            "forum": "Unb5CVPtae",
            "replyto": "Unb5CVPtae",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4366/Reviewer_MAkL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4366/Reviewer_MAkL"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents Time-LLM, a framework designed to harness Large Language Models (LLMs) for time series forecasting. By converting time series data into text-like prototypes and introducing the Prompt-as-Prefix (PaP) method to supplement this data with additional context, the framework effectively aligns time series data with the modalities of natural language. The empirical results indicate Time-LLM's superiority in comparison to other leading models, particularly in few-shot and zero-shot contexts."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is articulate and systematically structured, making the motivation and methodology behind the proposed solution evident.\n2. The approach of modality alignment from time series to natural language is both innovative and promising, offering a new perspective for future research.\n3. The empirical evaluation is thorough, encompassing an analysis of different LLM variations, an ablation study, computational efficiency considerations, and model interpretation."
                },
                "weaknesses": {
                    "value": "**Major**\n1. The choice of datasets for evaluation is restrictive, as the ETT datasets involve similar metrics monitored under different conditions. Their mutual similarities might overinflate the perceived performance of Time-LLM. Inclusion of diverse datasets such as Weather, Electricity, and Traffic, commonly featured in literature, would offer a more holistic assessment. Moreover, there's an emerging consensus that long-term forecasting benchmarks have a preference for univariate models, potentially bypassing the capability of handling cross-variate correlations  ([1], [2]). As Time-LLM also processes each channel separately, this limitation should be discussed.\n2. The mechanism used to produce the next H steps remains unclear, warranting a more detailed explanation.\n\n**Minor**\n1. Figure 3 Ambiguities:\n    1. Fig 3(a): Initially, the patches seem to represent input patches $X_P$. To eliminate any confusion, clearly labeling associated variables like $E, Z$ would be helpful.\n    2. Fig 3(b): The figure raises questions about whether the model outputs only the subsequent step or the next $H$ steps. Additionally, the function of the intermediate layer remains undefined.\n2. Error in Fig 3(b): There seems to be a need for a one-step left shift in the output of Patch-as-Prefix to render it an auto-regressive model.\n\nI would be glad to raising my score if the aforementioned weaknesses are addressed.\n\n[1] Chen, Si-An, et al. \"TSMixer: An All-MLP Architecture for Time Series Forecasting.\" Transactions on Machine Learning Research. 2023\n\n[2] Das, Abhimanyu, et al. \"Long-term Forecasting with TiDE: Time-series Dense Encoder.\" Transactions on Machine Learning Research. 2023"
                },
                "questions": {
                    "value": "1. Does the model operate in an autoregressive manner, predicting only the subsequent step, or does it directly forecast the next $H$ steps?\n2. How is $E'$ derived from $E$?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4366/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4366/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4366/Reviewer_MAkL"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4366/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698824026069,
            "cdate": 1698824026069,
            "tmdate": 1700451996872,
            "mdate": 1700451996872,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uTj2XzKI2J",
                "forum": "Unb5CVPtae",
                "replyto": "5j35okXerl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4366/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4366/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer MAkL (Part 1)"
                    },
                    "comment": {
                        "value": "We express our sincere gratitude to the reviewer for providing valuable feedback on our paper. We are deeply appreciative of the acknowledgment of the contributions and significance of our work. To our best effort, we have addressed the concerns and questions raised by the reviewer below.\n\n> W1.1. Inclusion of diverse datasets such as Weather, Electricity, and Traffic, commonly featured in literature, would offer a more holistic assessment.\n\nWe have performed additional experiments on four datasets: (1) Weather, (2) Electricity, (3) Traffic, and (4) Influenza-like Illness (ILI). **Our detailed results can be found in the main text (i.e., Tables 1,3, and 4) and appendices (i.e., Tables 10,11,14, and 15) of the revised paper**. For the reviewer's convenience, *a very brief summary* is provided below, highlighting the overall performance on a set of most representative methods on additionaly added four datasets.\n\n+ A concise summary of additional long-term forecasting results in average on Weather, Electricity, Traffic, and Influenza-like Illness (ILI):\n\n  | Method      | Time-LLM  |                     | GPT4TS |       | DLinear |       | PatchTST            |                     | TimesNet |       | FEDformer |       |\n  | ----------- | --------- | ------------------- | ------ | ----- | ------- | ----- | ------------------- | ------------------- | -------- | ----- | --------- | ----- |\n  | Metric      | MSE       | MAE                 | MSE    | MAE   | MSE     | MAE   | MSE                 | MAE                 | MSE      | MAE   | MSE       | MAE   |\n  | Weather     | **0.225** | **0.257**           | 0.237  | 0.270 | 0.248   | 0.300 | **0.225**           | $\\underline{0.264}$ | 0.259    | 0.287 | 0.309     | 0.360 |\n  | Electricity | **0.158** | **0.252**           | 0.167  | 0.263 | 0.166   | 0.263 | $\\underline{0.161}$ | **0.252**           | 0.192    | 0.295 | 0.214     | 0.327 |\n  | Traffic     | **0.388** | $\\underline{0.264}$ | 0.414  | 0.294 | 0.433   | 0.295 | $\\underline{0.390}$ | **0.263**           | 0.620    | 0.336 | 0.610     | 0.376 |\n  | ILI         | **1.435** | $\\underline{0.801}$ | 1.925  | 0.903 | 2.169   | 1.041 | $\\underline{1.443}$ | **0.797**           | 2.139    | 0.931 | 2.847     | 1.144 |\n\n+ A concise summary of additional 10% few-shot learning results in average on Weather, Electricity, Traffic, and Influenza-like Illness (ILI):\n\n  | Method      | Time-LLM  |                     | GPT4TS              |                     | DLinear |       | PatchTST            |           | TimesNet |       | FEDformer |       |\n  | ----------- | --------- | ------------------- | ------------------- | ------------------- | ------- | ----- | ------------------- | --------- | -------- | ----- | --------- | ----- |\n  | Metric      | MSE       | MAE                 | MSE                 | MAE                 | MSE     | MAE   | MSE                 | MAE       | MSE      | MAE   | MSE       | MAE   |\n  | Weather     | **0.234** | **0.273**           | $\\underline{0.238}$ | $\\underline{0.275}$ | 0.241   | 0.283 | 0.242               | 0.279     | 0.279    | 0.301 | 0.284     | 0.324 |\n  | Electricity | **0.175** | $\\underline{0.270}$ | $\\underline{0.176}$ | **0.269**           | 0.180   | 0.280 | 0.180               | 0.273     | 0.323    | 0.392 | 0.346     | 0.427 |\n  | Traffic     | **0.429** | $\\underline{0.306}$ | 0.440               | 0.310               | 0.447   | 0.313 | $\\underline{0.430}$ | **0.305** | 0.951    | 0.535 | 0.663     | 0.425 |\n\n+ A concise summary of additional 5% few-shot learning results in average on Weather, Electricity, Traffic, and Influenza-like Illness (ILI):\n\n  | Method      | Time-LLM            |                     | GPT4TS              |                     | DLinear |       | PatchTST  |           | TimesNet |       | FEDformer |       |\n  | ----------- | ------------------- | ------------------- | ------------------- | ------------------- | ------- | ----- | --------- | --------- | -------- | ----- | --------- | ----- |\n  | Metric      | MSE                 | MAE                 | MSE                 | MAE                 | MSE     | MAE   | MSE       | MAE       | MSE      | MAE   | MSE       | MAE   |\n  | Weather     | **0.260**           | 0.309               | $\\underline{0.263}$ | **0.301**           | 0.263   | 0.308 | 0.269     | 0.303     | 0.298    | 0.318 | 0.309     | 0.353 |\n  | Electricity | $\\underline{0.179}$ | **0.268**           | **0.178**           | $\\underline{0.273}$ | 0.176   | 0.275 | 0.181     | 0.277     | 0.402    | 0.453 | 0.266     | 0.353 |\n  | Traffic     | $\\underline{0.423}$ | $\\underline{0.298}$ | 0.434               | 0.305               | 0.450   | 0.317 | **0.418** | **0.296** | 0.867    | 0.493 | 0.676     | 0.423 |"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4366/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700264814611,
                "cdate": 1700264814611,
                "tmdate": 1700264814611,
                "mdate": 1700264814611,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JcFgOuClK7",
                "forum": "Unb5CVPtae",
                "replyto": "dLU4DoJew7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4366/Reviewer_MAkL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4366/Reviewer_MAkL"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for addressing my concerns in your rebuttal. The additional experiments and detailed responses have significantly improved the clarity and persuasiveness of the paper. I have accordingly increased my score to reflect this positive development.\n\nRegarding the handling of cross-variate features, I agree that the utilization of LLMs for forecasting presents a valuable contribution. As you mentioned, the added complexity of incorporating cross-variate features into the model may not always translate into improved performance on academic benchmarks. However, in real-world and industrial settings, time series data often exhibits sparsity and intermittency while also encompassing multiple variables. The ability to effectively handle cross-variate features is crucial for addressing these challenges. Therefore, I still encourage the authors to discuss on this limitation in the paper or consider elaborate it as a potential direction for future work, advancing the community awareness about this issue."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4366/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700451973321,
                "cdate": 1700451973321,
                "tmdate": 1700451973321,
                "mdate": 1700451973321,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rdaH21r4Q7",
            "forum": "Unb5CVPtae",
            "replyto": "Unb5CVPtae",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4366/Reviewer_Z9NA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4366/Reviewer_Z9NA"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes an approach to reprogram pre-trained large language models such that they can effectively do forecasting in zero-shot, few-shot, and fully supervised settings. The authors propose two strategies two forecast time-series: (1) by reprogramming patches of time-series by grounding them in text prototypes via cross attention, and (2) by using descriptions of the data, instruction, statistics of time-series as  the prefix. The authors demonstrate promising performance on short and long horizon tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is very well written, clear, with sufficient details to ensure reproducibility. I really liked that desiderata that the authors identified to enable LLMs to produce forecast. \n2. The experiments were well designed with some limitations in rigour which I will discuss in the next section.\n\nI really liked the paper, it was well written, well motivated and performant."
                },
                "weaknesses": {
                    "value": "Following are some things to improve in the paper. I think in general the experiments can be made more rigorous.\n1. **Baselines**:  I understand that the authors are following the experiment protocol followed by TimesNet, but there are several limitations: (1) Statistical methods such as AutoARIMA, AutoTHETA, AutoETS, Naive and Seasonal Naive, etc. were not compared with. These methods are important and very performant in practice, (2) N-BEATS and N-HITS were only compared during short-horizon forecasting, (3) Recent papers on using LLMs for time-series forecasting were not compared against, for e.g. LLM4TS [1] and PromptCast [2] (4) I am aware that the paper \"LLMs are zero-shot forecasters\" [3] only got recently published, but it would improve the experiments if the authors were able to compare with it. I should emphasize that this is completely optional. \n2. **Datasets:** Increasing the amount of datasets for experimentation will improve the results. The current set of datasets is pretty limited, even for long horizon forecasting datasets, where datasets such as Influenza-like Illnesses, Exchange Rate, Tourism, and Weather etc. (see PatchTST) were missing. For short-horizon datasets, M3 at the very least, and the Monash time-series forecasting archive can be added to improve results. \n\n**References:**\n[1] Chang, Ching, Wen-Chih Peng, and Tien-Fu Chen. \"Llm4ts: Two-stage fine-tuning for time-series forecasting with pre-trained llms.\" arXiv preprint arXiv:2308.08469 (2023).\n\n[2] Xue, Hao, and Flora D. Salim. \"PromptCast: A New Prompt-based Learning Paradigm for Time Series Forecasting.\" (2022).\n\n[3] Gruver, Nate, et al. \"Large Language Models Are Zero-Shot Time Series Forecasters.\" arXiv preprint arXiv:2310.07820 (2023)."
                },
                "questions": {
                    "value": "I do not have any questions at that would change my opinions regarding the paper. I think that the rigour of the experiments must be improved."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4366/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4366/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4366/Reviewer_Z9NA"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4366/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698879943036,
            "cdate": 1698879943036,
            "tmdate": 1700491631979,
            "mdate": 1700491631979,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "n0gDHOW69l",
                "forum": "Unb5CVPtae",
                "replyto": "rdaH21r4Q7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4366/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4366/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Z9NA (Part 1)"
                    },
                    "comment": {
                        "value": "We express our gratitude to the reviewer for providing constructive feedback on our paper, and we greatly appreciate the acknowledgement of our contributions. We have addressed the specific concerns raised by the reviewer as detailed below.\n\n> W1.1. Statistical methods, such as AutoARIMA, AutoTHETA, and AutoETS, were not compared with.\n\nWe have included AutoARIMA, AutoTheta, and AutoETS as baseline methods. **The detailed results can be found in Table 11 of the revised paper**. For the reviewer's convenience, we provide a summary of the relevant (averaged) results below:\n\n| Method      | Time-LLM  |           | AutoARIMA |       | AutoTheta |       | AutoETS |       |\n| ----------- | --------- | --------- | --------- | ----- | --------- | ----- | ------- | ----- |\n| Metric      | MSE       | MAE       | MSE       | MAE   | MSE       | MAE   | MSE     | MAE   |\n| ETTh1       | **0.408** | **0.423** | 0.952     | 0.656 | 1.319     | 0.797 | 1.286   | 0.784 |\n| ETTh2       | **0.334** | **0.383** | 0.635     | 0.532 | 1.635     | 0.678 | 1.981   | 0.505 |\n| ETTm1       | **0.329** | **0.372** | 1.145     | 0.697 | 1.268     | 0.741 | 1.529   | 0.790 |\n| ETTm2       | **0.251** | **0.313** | 3.205     | 0.634 | 0.975     | 0.523 | 2.430   | 0.459 |\n| Weather     | **0.225** | **0.257** | 1.080     | 0.448 | 0.477     | 0.373 | 1.137   | 0.406 |\n| Electricity | **0.158** | **0.252** | 0.597     | 0.510 | 0.797     | 0.596 | 0.755   | 0.570 |\n| Traffic     | **0.388** | **0.264** | 1.344     | 0.765 | 3.510     | 1.284 | 4.395   | 1.285 |\n| ILI         | **1.435** | **0.801** | 4.530     | 1.346 | 5.232     | 1.431 | 4.323   | 1.317 |\n\n> W1.2. N-BEATS and N-HITS were only compared during short-horizon forecasting.\n\nWe have compared to N-BEATS and N-HITS in long-term forecasting tasks, following the reviewer's suggestion. **The detailed results are in Table 11 of the revised paper**. We have provided a summary of the relevant (averaged) results below:\n\n| Method      | Time-LLM  |           | N-HiTS |       | N-BEATS |       |\n| ----------- | --------- | --------- | ------ | ----- | ------- | ----- |\n| Metric      | MSE       | MAE       | MSE    | MAE   | MSE     | MAE   |\n| ETTh1       | **0.408** | **0.423** | 0.473  | 0.462 | 0.568   | 0.525 |\n| ETTh2       | **0.334** | **0.383** | 0.487  | 0.464 | 0.564   | 0.529 |\n| ETTm1       | **0.329** | **0.372** | 0.395  | 0.412 | 0.451   | 0.445 |\n| ETTm2       | **0.251** | **0.313** | 0.337  | 0.376 | 0.355   | 0.402 |\n| Weather     | **0.225** | **0.257** | 0.235  | 0.286 | 0.256   | 0.306 |\n| Electricity | **0.158** | **0.252** | 0.205  | 0.296 | 0.259   | 0.351 |\n| Traffic     | **0.388** | **0.264** | 0.427  | 0.338 | 0.620   | 0.454 |\n| ILI         | **1.435** | **0.801** | 2.997  | 1.171 | 6.839   | 1.882 |\n\n> W1.3. Recent papers on using LLMs for time-series forecasting were not compared against for, e.g. LLM4TS and PromptCast. I am aware that the paper \"LLMs are zero-shot forecasters\" only got recently published, but it would improve the experiments if the authors were able to compare with it.\n\nThis is a good suggestion. Following the reviewer's recommendation, we have also included a comparison with LLMTime, even though this is completely optional according to the author guide this year. Our considerations are twofold: (1) LLM4TS is not yet open-sourced, and (2) LLMTime has shown better performance compared to PromptCast and operates directly on time series data. **The additional results we have incorporated can be found in Tables 5 and 16**. For ease of reference for the reviewer, we have summarized the relevent (averged) results below:\n\n| Method         | Time-LLM  |           | LLMTime |       |\n| -------------- | --------- | --------- | ------- | ----- |\n| Metric         | MSE       | MAE       | MSE     | MAE   |\n| ETTh1 to ETTh2 | **0.353** | **0.387** | 0.992   | 0.708 |\n| ETTh1 to ETTm2 | **0.273** | **0.340** | 1.867   | 0.869 |\n| ETTh2 to ETTh1 | **0.479** | **0.474** | 1.961   | 0.981 |\n| ETTh2 to ETTm2 | **0.272** | **0.341** | 1.867   | 0.869 |\n| ETTm1 to ETTh2 | **0.381** | **0.412** | 0.992   | 0.708 |\n| ETTm1 to ETTm2 | **0.268** | **0.320** | 1.867   | 0.869 |\n| ETTm2 to ETTh2 | **0.354** | **0.400** | 0.435   | 0.443 |\n| ETTm2 to ETTm1 | **0.414** | **0.438** | 0.769   | 0.567 |\n\nNote that to ensure a fair comparison, we have aligned the backbone LLM used in LLMTime with that of our method in terms of model size, specifically employing Llama2-7B. It should also be noted that LLMTime does not explicitly involve a transfer step but directly inferences on target datasets, resulting in some entries sharing same results. In a nutshell, Time-LLM demonstrates a substantial improvement over 75% and 53% in terms of the averged MSE and MAE."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4366/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700264612193,
                "cdate": 1700264612193,
                "tmdate": 1700264612193,
                "mdate": 1700264612193,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JS6JWShZTg",
                "forum": "Unb5CVPtae",
                "replyto": "Pk01FXwRTx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4366/Reviewer_Z9NA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4366/Reviewer_Z9NA"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the rebuttal!"
                    },
                    "comment": {
                        "value": "Dear Authors, \nThank you so much for running these experiments in a short amount of time. I really appreciate it. \nIn the light of these experiments, I have updated my score. \n\nThanks!"
                    }
                },
                "number": 27,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4366/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700491616372,
                "cdate": 1700491616372,
                "tmdate": 1700491616372,
                "mdate": 1700491616372,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8B8VVcLty5",
            "forum": "Unb5CVPtae",
            "replyto": "Unb5CVPtae",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4366/Reviewer_L3pe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4366/Reviewer_L3pe"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes the use of pre-trained large language models for time series prediction. As claimed, the main contributions of the paper include introducing a novel concept of reprogramming large language models and augmenting the input context with declarative prompts, such as domain expert knowledge and task instructions, to guide LLM reasoning."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This paper provides a summary of the metrics for pre-trained large-language models, including generalizability, data efficiency, reasoning, and multimodal knowledge.\n- The details of the proposed method are presented clearly and are easy to follow."
                },
                "weaknesses": {
                    "value": "My main concerns include: \n- While LLM is a hot topic in the deep learning community, it is still unconvincing to directly transfer the knowledge of natural language in LLMs to time series tasks. Note that i) text and time series are distinct data modalities, and ii) the pre-trained LLMs are not pre-trained with text-time-series pairs. \n- Furthermore, the first contribution *\u201cintroducing a novel concept of reprogramming large language models for time series forecasting without altering the pre-trained backbone model\u201c* is not new, as previous works such as GPT4TS [1] have also explored this reprogramming approach, regardless of which parts of the LLMs are fine-tuned. Additionally, the proposed method requires training the input and output layers for adaptation, which means that it does involve some alteration of the pre-trained backbone model.\n- While using text to aid in time series prediction can be beneficial, as seen in applications such as stock prediction using financial news text mining, it's unclear how the shared declarative for a whole time series dataset can help understand complex temporal behaviors in different windows. Although these prompts may provide domain expert knowledge and task instructions, they do not introduce text information at each time step. Therefore, it remains unclear how these text prompts can benefit the understanding of complex temporal behaviors in time series.\n- Compared to related work such as [1], which has applied pre-trained LLMs to various time series tasks, the experiments in this paper are relatively limited. For instance, only prediction tasks are considered, and even for long-term prediction tasks, only the ETT datasets are included. This narrow scope of experiments limits the evaluation of the proposed approach to other time series tasks and datasets.\n- The link for the source code provided in the paper is empty. I cannot check for more details regarding the experiments.\n\nReference:\n[1] One Fits All: Power General Time Series Analysis by Pretrained LM"
                },
                "questions": {
                    "value": "More discussions:\n- Can you discuss the connections and differences between LLMs and traditional/existing deep-learning time series models? \n- It would be helpful to define what is a good time series representation expected to be and to provide a more in-depth discussion of why pre-trained LLMs are capable of producing such representations.\n- Regarding cross-domain adaptation in the zero-shot setup, I'm curious about what knowledge from the source domain in time series can be transferred to the target domain to achieve zero-shot prediction."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4366/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4366/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4366/Reviewer_L3pe"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4366/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699021068197,
            "cdate": 1699021068197,
            "tmdate": 1699673417244,
            "mdate": 1699673417244,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LYMP7VmFfK",
                "forum": "Unb5CVPtae",
                "replyto": "8B8VVcLty5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4366/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4366/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer L3pe (Part 1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for offering the valuable feedback. We have addressed each of the concerns raised by the reviewer as outlined below.\n\n> W1. It is unconvincing to directly transfer the knowledge of natural language in LLMs to time series tasks. Note that 1) text and time series are distinct data modalities, and 2) the pre-trained LLMs are not pre-trained with text-time-series pairs.\n\n+ While LLMs are typically pre-trained on extensive text corpora, they have been proven to be effective in pattern recognition and reasoning over complex sequences of tokens in approximating flexible distributions over numbers [1,3]. This capability can be well extended to time series data, as demonstrated in this work and in recent concurrent and follow-up studies [2-6]. As an emerging and promising field of study, a recent survey [7] also offers a comprehensive overview of research that utilizes LLMs not only for time series but also for more intricate spatio-temporal data mining. This further underscores the proficiency of LLMs in understanding and reasoning on temporal data.\n+ In addition to the evidence presented above, LLMs are known to encapsulate rich information and rules about the physical world through natural language modality, leading us to hypothesize that some inherent knowledge may also be relevant to understanding time series. Notable examples, such as those in references [2] and [3], have achieved a certain level of precision in zero-shot forecasting through direct prompting. Our contention is that to fully activate the LLMs' ability in time series understanding and reasoning, it is crucial to effectively align the modalities of time series and natural language, as discussed in the third paragraph of the introduction. This concept is further elaborated in the fourth paragraph of the introduction in two aspects: (1) the central idea is to reprogram the input time series into text prototype representations that are more naturally suited to language models\u2019 capabilities; (2) to further augment the model\u2019s reasoning about time series concepts, we introduce Prompt-as-Prefix (PaP), an innovative approach that enriches the input time series with additional context and provides task instructions in the natural language modality.\n+ Our established and newly added experimental results clearly demonstrate the fact that off-the-shelf LLMs are can be very effective in general time series forecasting, whether compared to SOTA time series models or the best available (open-sourced) related models like GPT4TS and LLMTime [3]. Although LLMs are often considered black-boxes, our analysis in Fig. 5 illustrates how the proposed LLM reprogramming framework bridges the gap between natural language and time series.\n\n[1] Mirchandani, S., Xia, F., Florence, P., Driess, D., Arenas, M. G., Rao, K., ... & Zeng, A. (2023, August). Large Language Models as General Pattern Machines. In 7th Annual Conference on Robot Learning.\n\n[2] Xue, H., & Salim, F. D. (2022). Prompt-Based Time Series Forecasting: A New Task and Dataset. arXiv preprint arXiv:2210.08964.\n\n[3] Gruver, N., Finzi, M. A., Qiu, S., & Wilson, A. G. (2023, November). Large Language Models Are Zero-Shot Time Series Forecasters. In Thirty-seventh Conference on Neural Information Processing Systems.\n\n[4] Chang, C., Peng, W. C., & Chen, T. F. (2023, August). Llm4ts: Two-stage fine-tuning for time-series forecasting with pre-trained llms. arXiv preprint arXiv:2308.08469.\n\n[5] Spathis, D., & Kawsar, F. (2023). The first step is the hardest: Pitfalls of Representing and Tokenizing Temporal Data for Large Language Models. arXiv preprint arXiv:2309.06236.\n\n[6] Cao, D., Jia, F., Arik, S. O., Pfister, T., Zheng, Y., Ye, W., & Liu, Y. (2023, October). TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting. arXiv preprint arXiv:2310.04948.\n\n[7] Jin, M., Wen, Q., Liang, Y., Zhang, C., Xue, S., Wang, X., ... & Xiong, H. (2023, October). Large models for time series and spatio-temporal data: A survey and outlook. arXiv preprint arXiv:2310.10196."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4366/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700263927573,
                "cdate": 1700263927573,
                "tmdate": 1700263927573,
                "mdate": 1700263927573,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LFWq7sHFBq",
                "forum": "Unb5CVPtae",
                "replyto": "4EwquWbtHQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4366/Reviewer_L3pe"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4366/Reviewer_L3pe"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your efforts in adding more experiments and responding to my questions.\n\n**+++ key errors in the code and unfair comparisons +++**\n\nAfter carefully reviewing the released code, I would like to highlight some key errors that I found. These errors have resulted in unfair comparisons:\n\ni) In the code (**line 194 in the file \"exp_long_term_forecasting.py\"**), the following line **uses test_loss for early stopping**: \n> **early_stopping(*test_loss*, self.model, path)**.\n\nHowever, in all previous works, the validation loss is used for model selection, and the test loss is reported for comparison. \nThis discrepancy in using the test loss for early stopping has led to **unfair comparisons**.\n\nii) In the code, the batch size is set to 2, and the value of \"drop_last\" in the test data loader is set to True. As a result, **the number of test samples differs from that used in previous baselines** (all other papers e.g., GPT4TS use a larger batch size (32) and use drop_last=True for test dataloader). This inconsistency in the test data handling further undermines the fairness of the comparisons.\n\nThese two errors indicate that the improvements showcased in the experiments lack convincing evidence due to the improper handling of the test loss and the inconsistency in the test data.\n\n**+++ regarding the responses +++**\n\nRegarding the responses provided, I would like to offer some additional suggestions:\n\ni) It is still challenging for me to grasp an intuitive motivation for why the proposed model outperforms other deep learning methods. In relation to the discussions on the differences between LLM-based and conventional time series forecasting models in the main text, I believe the authors should investigate and identify the specific temporal patterns or types of time series data that the proposed model can effectively leverage while existing methods struggle to handle them. \n\nii) The response regarding the \"model reprogramming\" not involving internal modifications to the backbone model, such as altering or fine-tuning its internal layers, is not entirely convincing to me. It should be noted that GPT4TS does fine-tune the positional embeddings and layer normalization layers only (while other main modules/layers are frozen). \n\niii) Thank you for explaining that the input time series statistics are dependent on the specific input time series. However, I believe this design aspect requires further investigation. Currently, as the statistics information is about the input window and the window is further fed into the self-attention-based reprogrammed part, there is a repetition of information. In other words, this design does not introduce new information that could enhance the training process. If the main contribution lies in introducing declarative prompts, it might be beneficial to consider incorporating **external text** information to provide additional context.\nOne more suggestion is that by exploring various combinations of input window lengths and corresponding statistics lengths, we can gain insights into the impact of different statistical information on model performance. \n\nI have no additional questions and I will finalize my score after discussing these points with the other reviewers."
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4366/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700456191672,
                "cdate": 1700456191672,
                "tmdate": 1700456191672,
                "mdate": 1700456191672,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kTAUidgvdw",
                "forum": "Unb5CVPtae",
                "replyto": "8B8VVcLty5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4366/Reviewer_L3pe"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission4366/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4366/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4366/Reviewers",
                    "ICLR.cc/2024/Conference/Submission4366/Reviewers/Submitted"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4366/Reviewer_L3pe"
                ],
                "content": {
                    "title": {
                        "value": "Regarding the authors' responses to the critical errors in the code"
                    },
                    "comment": {
                        "value": "Thanks to the authors for explaining the critical errors in the code. **However, these explanations are not convincing**. There are **several contradictory points in these explanations** that make it difficult for me to believe that the current version of the experimental results is accurate and reliable.\n\nFirstly, **there is a clear contradiction regarding the explanation** that *long-term, short-term, and few-shot forecasting \u2014 we implemented validation loss for early stopping, aligning with methodologies in related literature.* This is because the short-term prediction experiments are conducted using the M3 and M4 datasets, and according to the description in Table 8 of Appendix B.2 in the submission, **M3 and M4 datasets do not have validation sets**. Therefore, **it is impossible for the short-term predictions to be based on validation loss**. In the\u00a0`data_loader.py`\u00a0file, specifically in the\u00a0`Dataset_M4`\u00a0code, it can be observed that the M4 data indeed does not include a validation set \n(note that the\u00a0`exp_short_term_forecasting.py`\u00a0file also uses test loss to determine the best model, whereas the GPT4TS paper uses training loss for short-term forecasting). Based on this, the author's explanation lacks persuasiveness.\n\nSecondly, the description regarding using test loss for zero-shot forecasting is also problematic. Please refer to the code of GPT4TS (https://github.com/DAMO-DI-ML/NeurIPS2023-One-Fits-All/blob/main/Zero-shot_Learning/main_test.py). In their zero-shot experiments, they indeed use training loss for early-stopping, not test loss.\n\nFurthermore, in the provided [README.md](http://readme.md/) file, the author presents some training process information for the ETTm2 task. In these results, the author reports an MSE of 0.161 (and MAE of 0.253). This result is based on selecting the best test loss. **If early stopping were based on validation loss, the MSE result should be 0.1674806**. **Unfortunately, the aforementioned incorrect results of 0.161 and 0.253 have been reported in Table 10**. This deviation is significant and makes it difficult to determine the effectiveness of the proposed model. \n\n> **Training process provided by the authors in [README.md](http://readme.md/)**\n\n> Start training: long_term_forecast_ETTm2_512_96_TimeLLM_ETTm2_ftM_sl512_ll48_pl96_dm16_nh8_el2_dl1_df32_fc3_ebtimeF_dtTrue_Exp_0  \n- Epoch: 1 | Train Loss: 0.2048776 Vali Loss: 0.1279695 Test Loss: 0.1799820\n- Epoch: 2 | Train Loss: 0.1946422 Vali Loss: 0.1226610 Test Loss: 0.1738900\n- Epoch: 2 | Train Loss: 0.1946422 Vali Loss: 0.1226610 Test Loss: 0.1738900\n- Epoch: 4 | Train Loss: 0.2154310 Vali Loss: 0.1196599 Test Loss: 0.1697755\n- Epoch: 5 | Train Loss: 0.2022108 Vali Loss: 0.1181143 Test Loss: 0.1652272\n- Epoch: 6 | Train Loss: 0.1936386 Vali Loss: 0.1249270 Test Loss: 0.1682286\n- Epoch: 7 | Train Loss: 0.1956175 Vali Loss: **0.1149597** Test Loss: 0.1674806 > **based on best validation loss**\n- Epoch: 8 | Train Loss: 0.1864047 Vali Loss: 0.1160124 Test Loss: **0.1614796** > **based on best test loss**\n- Epoch: 9 | Train Loss: 0.1804296 Vali Loss: 0.1165261 Test Loss: 0.1651560\n- Epoch: 10 | Train Loss: 0.1817255 Vali Loss: 0.1188157 Test Loss: 0.1667696 \n\n> testing: long_term_forecast_ETTm2_512_96_TimeLLM_ETTm2_ftM_sl512_ll48_pl96_dm16_nh8_el2_dl1_df32_fc3_ebtimeF_dtTrue_Exp_0\nmse:**0.16147961839199066**, mae:0.25324239444732666\n\n**Based on the rigor of research, I strongly encourage the authors to reconsider the validity of the model** in subsequent versions and invest more time to **provide sufficient training details and correct experimental results** to verify the validity of the model. \n\nI appreciate the authors' efforts to add a lot of supplementary experiments during the rebuttal period. **However, due to bugs in the code and inconsistencies in the explanations provided, I cannot believe that the current version of the experimental results is accurate and reliable**."
                    }
                },
                "number": 34,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4366/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700648066137,
                "cdate": 1700648066137,
                "tmdate": 1700648563392,
                "mdate": 1700648563392,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]