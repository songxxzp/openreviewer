[
    {
        "title": "LogicBench: Towards Systematic Evaluation of Logical Reasoning Ability of Large Language Models"
    },
    {
        "review": {
            "id": "TpW1APE72e",
            "forum": "71kocBuhNO",
            "replyto": "71kocBuhNO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6607/Reviewer_Z2AS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6607/Reviewer_Z2AS"
            ],
            "content": {
                "summary": {
                    "value": "The authors present LogicBench, a natural language QA dataset covering 25 combinations of inference rules (e.g., modus ponens, disjuntive syllogism, etc.) and logics (propositional, first-order predicate, and non-monotonic). The generation procedure is clearly described and several examples are shown. A number of models are evaluated on the benchmark, and it is also used as a fine-tuning dataset to assess resultant improvements on other logical reasoning benchmarks. Related work is compared to and discussed throughout."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This is a very nicely designed benchmark that I am excited to use. I appreciate the effort the authors have taken on a number of fronts. 1) the authors unify a number of logics and inference rules into one global benchmark. 2) the materials are synthetically generated, but still reasonably naturalistic. 3) the paper is clear and well-written. 4) the contextualization with respect to other benchmarks breaks the space down very elegantly. \n\nThis paper is already marginally above the acceptance threshold and can be bumped to an accept if the below questions are addressed."
                },
                "weaknesses": {
                    "value": "There are several opportunities to improve the contribution and presentation of the work. I will address these more specifically below under questions. 1) There are a number of places where the reader would appreciate more detail as to the background behind decision points during the creation of this benchmark. 2) Aside from inference rules, there are additional correlated metadata that would be helpful to add and consider when comparing model performance across subsets. 3) it is not clear what the reader gains from the fine-tuning experiment."
                },
                "questions": {
                    "value": "- Since the synthetic generation process is completed partially via sampling from an LLM, there are of course concerns as to whether the generated problems actually follow the intended logical structure and are devoid of other errors. The authors dutifully note this, and as such, human-validate a subset of the data (500 instances), which they dub LogicBench[Eval] as opposed to the larger set (3750 instances) that are not validated in LogicBench[Aug]. It is perfectly reasonable to only validate a subset of the data given the size, but the authors should include a breakdown of the types of errors found when manually validating this subset. A table should be added describing sample statistics e.g., for a sample of 100 problems generated (as might be found in the raw Aug portion), how many need to be corrected for various discrepancies in logical formulation, typos, grammar, etc. This will help infer the ceiling on the Aug dataset.\n- In the description of the NL conversion, the authors note the following:\n`For instance, implication: \u201cp \u2192 q\u201d is\nexpressed as \u201cIf p, then q\u201d, conjunction: \u201cp \u2227 q\u201d is expressed as \u201cp and q.\u201d, and disjunction: \u201cp \u2228 q\u201d\nis expressed as \u201cAt least one of the following is true: (1) p and (2) q. Note that we do not know which\nof (1) and (2) is true. It is possible that only (1) is true, or only (2) is true, or both are true.\u201d`\nCan the authors comment on the unwieldy specification of \"or\"? I understand that the authors are attempting to explicitly distinguish \"or\" from \"xor\", but should make this clear and expand on why simpler versions were insufficient. Was a simpler specification attempted initially but resulted in extremely low performance across the board due to disambiguation difficulties? More information is needed here.\n- Some clarifying information on the metric is required. If I understand correctly, each instance has 4 variants? 1/4 of these shows the correct application of the rule, later measured by A(Yes), and 3/4 show variants that cannot satisfy the conclusion, later measured by A(No)? If this is not correct, perhaps this section can be clarified for other readers as well. For the A(No) problems, it appears to me that these contain both examples where the evidence provided cannot prove the conclusion or its negation, but also instances, where the evidence provided, can in fact prove the negation of the conclusion. For example, in Figure 2 Stage 3, Variation 2 implies \"not S1\", whereas Variations 3 and 4 can neither prove \"S1\" nor \"not S1\". Perhaps these patterns of variants should be noted differently in the dataset metadata. I would suspect certain audiences to care about the distinction between these types of errors.\n- Finally, it is unclear to me what we gain from the fine-tuning experiments. The performance increases on other benchmarks are minimal and it seems to me space would be better spent clarifying the benchmark details, such as those I've raised above, as these reflect the core contribution of the paper. I would suggest moving this to the appendix. If other reviewers disagree with me and find them informative, this suggestion may be disregarded and instead, the contribution explained to the reader more explicitly."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6607/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698695355055,
            "cdate": 1698695355055,
            "tmdate": 1699636753470,
            "mdate": 1699636753470,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ab2Js3EBFu",
                "forum": "71kocBuhNO",
                "replyto": "TpW1APE72e",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6607/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6607/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "R1: Clarification on why simpler versions were insufficient for \u2018or\u2019.\n\nA1: Thank you for this comment. Yes, indeed we want to make an explicit distinction between \u2018or\u2019 and \u2018xor\u2019. We found that understanding the logical implication of 'or' when integrated into logical formulations posed challenges to both humans - our human evaluators, mostly graduate students - and models in terms of understanding it. So with the focus of the paper on the inference rules, we decided to make the desired meaning of `or\u2019 explicit using the above construct, so that our human evaluations and annotations would be correct. See the below example from LogicBench(Aug) where the use of our template vs. the direct 'or' formulation:\n\n```\nContext: If Sam has not finished his homework, then he will not go to the party.\n\nContextual Template Question (from our dataset): \"Based on the context, can we ascertain that at least one of the following must always be true: (a) Sam has finished his homework, and (b) He will not attend the party?\"\n\nDirect 'or' Question: \"Can we conclude that Sam has finished his homework or he will not attend the party?\"\n```\n\nFrom the example provided above, it becomes evident that a question using a direct 'or' can be challenging to interpret and may cause confusion. Hence, we choose to go with the more expressed formulation of \u2018or\u2019.\n\n----\nR2: Additional information on the metric and about meta-data\n\nA2: Yes, your understanding is indeed correct. The question with the answer \u2018yes\u2019 has the correct application of the rule, and the question with the answer \u2018no\u2019 has a conclusion that is not supported by context (rule). Furthermore, we agree that different variation types can be added as metadata for a better understanding of data. Thanks for this valuable suggestion, and we will incorporate it in the revised version.\n\n-----\nR3: Details Related to fine-tuning experiments presented in Sec. 4.3\n\nA3: Thank you for this suggestion. With our exploration of augmenting data for training and showing that it is useful for generalizing to real-world logic datasets, we demonstrate an additional use case of LogicBench. Thus, we consider these experiments valuable. However, the core contribution remains to be systematical evaluation and highlighting flaws in the logical reasoning ability of LLMs.\n\n-----\nNote: During the rebuttal period, we conducted a human evaluation on a subset of LogicBench(Eval). We will share the results earliest possible."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6607/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700556999656,
                "cdate": 1700556999656,
                "tmdate": 1700556999656,
                "mdate": 1700556999656,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "chWIPX0FkL",
                "forum": "71kocBuhNO",
                "replyto": "TpW1APE72e",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6607/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6607/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "R4: Small-scale Human Evaluation on LogicBench(Eval)\n\nA4: Thank you for this suggestion. Please note that, in Sec. 3.3, we performed a systematic evaluation to ensure that generated instances follow the intended logical structure. To further support the integrity and reliability of the benchmark, we hired three graduate student volunteers to manually check the quality of generated data instances. Due to the limited rebuttal period, we randomly selected 5 instances from each inference rule, resulting in a total of 125 instances across 25 reasoning patterns for human evaluation. In particular, annotators are asked to provide binary answers (yes/no) for \u201cValidity of generated context\u201d, and \u201cValidity of generated question\u201d to make sure they adhere to the intended logical structure. Each instance is annotated by three different annotators. The inter-annotator agreement, measured with raw/observed agreement is 0.956. When there was a disagreement between annotators, a majority class was preferred. We provided example task instructions at https://anonymous.4open.science/r/LogicBench-EEBB/human_eval/readme.md. Results reveal that, after majority voting, annotators provided \u2018yes\u2019 for all 125 instances showing that all given instances adhere to the intended logical structure which further supports the quality of data checked by authors. We added this study in Appendix L."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6607/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634153537,
                "cdate": 1700634153537,
                "tmdate": 1700717966855,
                "mdate": 1700717966855,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mtb6iSWXuY",
            "forum": "71kocBuhNO",
            "replyto": "71kocBuhNO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6607/Reviewer_8r5S"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6607/Reviewer_8r5S"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a logical reasoning benchmark LogicBench that evaluates the logical reasoning abilities of large language models. LogicBench includes binary Yes/No questions of several propositional logic rules, first-order logic extended from the propositional logic rules and non-monotonic reasoning. The sentence pairs used to generate the dataset instances are generated by GPT-3. The authors observed that current models like FLAN-T5, Tk-instruct, and GPT series have better performance on the negative labels (No) than the positive labels (Yes). Further analysis demonstrates that ChatGPT has challenges in fully comprehend many reasoning problems and factors like negations and rule complexity bring challenges in logical understanding."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The logical reasoning problem is an important research direction in revealing the reasoning abilities of current pretrained language models. LogicBench includes several logical categories to picture the logical reasoning ability.\n- The analysis about unfolding the reasoning steps and investigating influential factors is helpful in diagnose the bottleneck of reasoning abilities."
                },
                "weaknesses": {
                    "value": "- In general, it is not easy to conclude the findings of the evaluation results from LogicBench and how it actually demonstrates the logical reasoning abilities. For example, whether the prompting method is the best approach to let the model speak out the reasoning steps that corresponds to the reasoning decision given the variances and limitations of generations of GPT-3/4. Similarly, other discussions need more evidences to provide enough information to support the suspensions."
                },
                "questions": {
                    "value": "- What are the source of the sentences used to generate the pairs in Figure 2 (e.g. 'Liam finishes his work' in Table 4)?\n- Are Yes and No questions balanced in the benchmark?\n- The authors indicate that the LogicQA and ReClor include richer reasoning formats, but will it necessarily undermine the impacts of finetuning on LogicBench?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6607/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698908354735,
            "cdate": 1698908354735,
            "tmdate": 1699636753325,
            "mdate": 1699636753325,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "W4y6VvPdzn",
                "forum": "71kocBuhNO",
                "replyto": "mtb6iSWXuY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6607/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6607/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "R1: Justification on why we use the prompting method to evaluate LLMs\u2019 reasoning ability\n\nA1: We note that the emergence of the prompt paradigm has shifted the assessment of LLMs, favoring zero-shot and few-shot evaluations over traditional supervised fine-tuning (SFT). Thus, our work also adapts a similar method and evaluates LLMs using Zero-shot-CoT, a standard practice for meaningful reasoning with LLMs. While evaluating LLMs such as GPT-3/4, we set the temperature hyper-parameter to 0 to control the variation in generation. Furthermore, we also conducted a manual qualitative analysis (section 4.3) to support our findings. \n\n---\nR2: What are the source of the sentences used to generate the pairs in Figure 2?\n\nA2: We prompt the model (GPT-3) to generate source sentences (propositions) as described in Sec 3.2.1. The prompt template is presented in Figure 3, and the comprehensive prompt is presented in Appendix A. \n\n----\nR3: Are Yes and No questions balanced in the benchmark?\n\nA3: No, they are not balanced. Thus, we presented a separate accuracy in Table 6, A(Yes) and A(No) to evaluate model performance. We updated this in the paper (Section 3.3).\n\n-----\nR4: The authors indicate that the LogicQA and ReClor include richer reasoning formats, but will it necessarily undermine the impacts of finetuning on LogicBench?\n\nA4: LogicBench is designed to evaluate LLMs on mainly single-step reasoning across a range of inference rules and show that they struggle with even single-step reasoning for complex inference rules. Furthermore, errors made during the initial single-step reasoning can be propagated through subsequent reasoning steps, potentially leading to wrong conclusions in multi-step reasoning scenarios. Thus, teaching models first to do better single-step reasoning via different approaches (fine-tuning in our case) and then utilizing them to perform reasoning over complex and longer formats (LogiQA, ReClor) can improve their performance (as shown in Section 4.3). Hence, we believe that indicating LogicQA and ReClor include richer reasoning formats will not necessarily undermine the impacts of finetuning on LogicBench."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6607/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700555661391,
                "cdate": 1700555661391,
                "tmdate": 1700555661391,
                "mdate": 1700555661391,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lBQl2nPYod",
            "forum": "71kocBuhNO",
            "replyto": "71kocBuhNO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6607/Reviewer_sePi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6607/Reviewer_sePi"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new benchmark for the evaluation of the logical reasoning ability of large language models. The main claim is that this new benchmark comparatively considers new types of reasoning including non-monotonic reasoning that is not considered in the existing ones. They evaluate multiple language models on this benchmark and provide further analysis of the difficulties those models have in reasoning. Finally, they show their synthesized data help models in improving logical reasoning when it is used in their LLM training (here T5) and tested on existing logical reasoning benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "They created a new benchmark/resource for QA when complex logical reasoning is needed to answer the questions.  \nCompared to the existing works, this paper is more extensive in its investigation of the types and patterns of logical reasoning needed for QA. They made sure to consider all different kinds including 25 inference rules.  The dataset can serve as a source of both evaluation and fine-tuning. The results are more detailed with respect to the difficulty of various reasoning rules and examined for a variety of language models."
                },
                "weaknesses": {
                    "value": "\u2014The work is incremental in extending the existing benchmarks and providing more results of LLM evaluations on logical reasoning. \n\u2014Sometimes, I am not sure if the provided conclusions are reliable in general.  For example, being more correct in NO answers might be due to having bias to saying more NOs than Yes\u2019s and not more than that. \nIntuitively, PL should be easier than FOL. The analysis does not bring any insight into why this is not the case with LLMs reasoning. Again the conclusion must be affected by the biases in the data creation. Often, making general conclusions is hard for this synthesized benchmarking practice and from that perspective, this work is not progressive and remains under the drawbacks of similar previous work."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6607/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6607/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6607/Reviewer_sePi"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6607/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699066947094,
            "cdate": 1699066947094,
            "tmdate": 1699636753216,
            "mdate": 1699636753216,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZJ7QG8F2sW",
                "forum": "71kocBuhNO",
                "replyto": "lBQl2nPYod",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6607/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6607/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "R1: \u201cbeing more correct in NO answers might be due to having bias to saying more NOs than Yes\u2019s and not more than that\u201d\n\nA1: Please note that we presented our results separately for A(No) and A(Yes). Moreover, we used CoT to evaluate the model which is the standard practice for meaningful reasoning with LLMs, and also make sure that the final answer is based on logical reasoning steps but not simple heuristics/shortcuts. Hence, we believe that incorrect/correct answers (Yes/No) produced by LLMs are not simply because of bias but because of the underlying reasoning process. To further support this, we also conducted a manual qualitative analysis (section 4.3) to present our findings. \n\n----\nR2: Justification of lower performance of GPT-4 on PL as compared to FOL\n\nA2: Thank you for raising this point. Please note that the results in Table 6 show an average improvement in FOL results because of LLMs\u2019 high accuracy on two axioms, EI and UI. However, when we compare the performance across the six common inference rules between PL and FOL (MT, HS, DS, BD, CD, DD), GPT-4 achieves an average of 49.16% A(Yes) for PL and 39.54% A(Yes) for FOL which shows that our results show the expected behavior.\n\nThe high accuracy of GPT-4 in handling EI and UI can be attributed to their simplicity.  While from human experience and complexity theory, FOL is harder than PL in general; in the LLM context, the crucial factor becomes what kind of logical sentences LLMs are pre-trained on. It seems that LLMs are pre-trained more on simple FOL sentences than on simple PL sentences. To get an indication of this we gave GPT-4 prompt_1 - \u201cGive twenty statements that have \u2018if\u2019 and \u2018then\u2019 in them\u201d and prompt_2 - \u201cGive twenty statements that have material implication \"or\" in them\u201d. From the results, we can observe that 11 of the 20 sentences in response to the first prompt were FOL kind and only 9 were propositional kind. On the other hand, all 20 statements in the response to the second prompt were FOL sentences. This shows that LLMs\u2019 comprehend simple FOL sentences, thus, showing high accuracy on simpler FOL axioms, i.e., EI and UI. However, LLMs exhibit consistently low performance across other complex/longer inference rules that are common to both PL and FOL (e.g., CD, BD, etc.)."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6607/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700558448978,
                "cdate": 1700558448978,
                "tmdate": 1700558448978,
                "mdate": 1700558448978,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PLenETdog8",
                "forum": "71kocBuhNO",
                "replyto": "lBQl2nPYod",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6607/Reviewer_sePi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6607/Reviewer_sePi"
                ],
                "content": {
                    "title": {
                        "value": ""
                    },
                    "comment": {
                        "value": "Thanks for the author's response, I read the responses and the new analysis provided in the rebuttal phase. I think the paper is a good one as mentioned in my initial review."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6607/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674541470,
                "cdate": 1700674541470,
                "tmdate": 1700674541470,
                "mdate": 1700674541470,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rbh1g1ihZP",
            "forum": "71kocBuhNO",
            "replyto": "71kocBuhNO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6607/Reviewer_1d9G"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6607/Reviewer_1d9G"
            ],
            "content": {
                "summary": {
                    "value": "This paper contributes Logic-Bench, a diagnostic dataset for systematic evaluation of logical reasoning capability of large language models. It covers 25 different reasoning patterns including propositional, first-order, and non-monotonic logics. The paper presents an evaluation for different LLMs in both zero-shot and few-shot settings. The authors also showed that LLMs trained with Logic-Bench yield better performance on several logical reasoning datasets such as LogiQA and ReClor."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper contributed a useful diagnostic dataset for evaluating the logical reasoning capability of large language models. \n- It covers a diverse range of logic reasoning types, especially the under-explored non-monotonic logics, which would be beneficial for future research. \n- The evaluation on different LLMs are comprehensive. \n- The paper is well-written and easy to follow."
                },
                "weaknesses": {
                    "value": "1) The main concern is that the dataset is synthetically generated with pre-defined logical rules and templates. Although this gives a comprehensive categorization of the logic types, it also leaves the question of how well it can generalize to real-world logical reasoning tasks? Although in Table 7 the authors showed that fine-tuning T5 with LogicBench leads to better performance, the performance improvement is quite marginal. \n2) A salient drawback is the synthetic nature of the dataset, crafted using predetermined logical rules and templates. This raises the question of its applicability and relevance to real-world logical reasoning challenges. While the authors showed in Table 7 that fine-tuning T5 with LogicBench leads to better performance, this enhancement was relatively slight. \n3) The few-shot learning performance from Table 13 hint at the potential of LogicBench to soon become less challenging. Remarkably, with few-shot prompting, GPT-4 already achieves an accuracy of roughly 90% in PL and FOL and about 85% for NM. \n4) The paper lacks the evaluation on modern open-sourced LLMs such as LLAMA, Alpaca, Vicuna, etc. \n5) Since the dataset is quite synthetic, I assume that when fine-tuning the model on a few examples, the model can achieve quite good performance. It would be beneficial for the authors to report this supervised performance as a reference for the potential upper-bound performance."
                },
                "questions": {
                    "value": "- Given the first weakness, could the authors elaborate on:\n  1. Whether high/low performance on LogicBench truly indicates high/low real-world logical reasoning capabilities for LLMs?\n  2. Is there evidence to support the premise that training LLMs on synthetic logic data such as those in LogicBench genuinely enhances their general logical reasoning capabilities?\n- In addition to reporting the break-down performance of each reasoning type, it would be intriguing to see if the model has the capability for cross-type generalization. For instance, when trained exclusively on one reasoning form (like PL), can the model generalize effectively to another, such as FOL?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6607/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699245601523,
            "cdate": 1699245601523,
            "tmdate": 1699636753099,
            "mdate": 1699636753099,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2uGsMFwLdk",
                "forum": "71kocBuhNO",
                "replyto": "rbh1g1ihZP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6607/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6607/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "R1: Discussion on how well LogicBench can generalize to real-world logical reasoning tasks\n\nA1: Thanks for this comment. Please note that the core contribution of our proposed work is the diagnostic benchmark for systematically evaluating the logical reasoning ability of LLM across 25 different reasoning inference rules. This benchmark goes beyond simple inference rules in classical logic and incorporates more complex inference rules including NM reasoning. Moreover, the evaluation of various LLMs on LogicBench(Eval) leads to interesting findings and shows shortcomings in their ability to accurately solve even single-step reasoning problems. Furthermore, with our exploration of augmenting data for training and showing that it is useful for generalizing to real-world logic datasets, we demonstrate an additional use case of LogicBench. However, the core contribution remains to be systematical evaluation and highlighting flaws in the logical reasoning ability of LLMs.\n\n----\nR2: Justification of high few-shot performance of GPT-4 on LogicBench\n\nA2: In the few-shot setting, we provide exemplars corresponding to the inference rule required to answer the test question. Bigger models such as GPT-4 have been shown to be remarkably good at following the in-context exemplars and mimicking the process to reach the correct conclusions. Thus, leveraging the in-context exemplars, GPT-4 achieves high accuracy in a few-shot setting (Table 13). In contrast, the zero-shot setting evaluates the true capabilities of these models in carrying out logical reasoning, as we can not expect that oracle exemplars corresponding to particular logical inference rules will always be available as part of prompts. We also believe that something as fundamental as logical reasoning ability should be an inherent capability of the model which is evaluated in a zero-shot setting. We discuss this point in Section 4.1.\n\n----\nR3: Including results of open-source LLaMa-2-7B on LogicBench\n\nA3: Thanks for this suggestion. We added an evaluation of LLaMa-2-7B on LogicBench(Eval) in Appendix G. Similar to the other LLMs such as GPT-3/4 in zero-shot setting, LLaMa-2 does not fair well on LogicBench(Eval).\n\n----\nR4: Supervised fine-tuning results on LogicBench using T5-large\n\nA4: As rightly mentioned by the reviewer, LogicT5 indeed achieves high accuracy (\u223c97%) on LogicBench(Eval).\n\n----\nR5: Whether high/low performance on LogicBench truly indicate high/low real-world logical reasoning capabilities for LLMs?\n\nA5: While we agree that real-world logical reasoning can be more complex and require multi-step reasoning, we would like to emphasize that our benchmark represents the first step in evaluating LLM across a spectrum of logical inference rules and shows that these LLMs struggle even with the single-step reasoning process when logical rules become complex.  Furthermore, errors made during the initial single-step reasoning can propagate through subsequent reasoning steps, potentially leading to wrong conclusions in multi-step reasoning scenarios (which is more human-like reasoning). While our benchmark primarily addresses single-step reasoning and doesn't specifically delve into multi-step human-like reasoning, evaluation on LogicBench is a strong indicator of real-world logical reasoning capabilities for LLMs.\n\n----\nR6: Is there evidence to support the premise that training LLMs on synthetic logic data such as those in LogicBench genuinely enhances their general logical reasoning capabilities?\n\nA6: Some recent works such as [1] show that training on synthetic data can improve the performance of out-of-domain logical reasoning datasets indicating enhanced general logical reasoning capabilities.\n\n[1] https://arxiv.org/pdf/2310.00836.pdf \n\n-----\nR7: Discussion on LLMs\u2019 capability for cross-type generalization\n\nA7: Thanks for this valuable suggestion. During the rebuttal phase, we conducted suggested experiments and included results in Appendix K in the revised version. Experimental results reveal that the model fine-tuned on PL performs fairly well on FOL (~ 86%), though it remains lower than the supervised model fine-tuned on FOL (~ 97%). A similar observation is made for the model fine-tuned on FOL which fairly does well on PL. However, both these models struggle to generalize on non-classical NM reasoning showing lower accuracy (~ 52%). Additionally, the model fine-tuned on NM reasoning struggles in generalizing to classical logic, PL, and FOL."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6607/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700555565793,
                "cdate": 1700555565793,
                "tmdate": 1700555565793,
                "mdate": 1700555565793,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0FNuxxOnhP",
            "forum": "71kocBuhNO",
            "replyto": "71kocBuhNO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6607/Reviewer_5vLY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6607/Reviewer_5vLY"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a new benchmark called LogicBench into the field of LLM reasoning. It consists of synthetically generated examples in natural language that follow a set of templated formulas derived from 3 different branches of deductive logic (propositional/first-order/non-monotonic). The examples are categorized into the various inference rules they are derived from and there is a 3 step generation procedure described. 2 sets of experiments are done: Evaluations on recent strong LM's are used to claim that LM's perform poorly on many of these structures, and the further evidence of the datasets utility is to show that it can be used as a training set for improving performance of downstream reasoning tasks."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "First of all, there is a sore need for better benchmarks that can help research in advanced LLM reasoning progress further. This dataset is aimed at that need, and in at least one sense it makes a useful contribution: it adds more complex reasoning schemas that  previously have not been used, especially on the non-monotonic side.  Non-monotonic reasoning is severely understudied in LLM's but it is a crucial part of human-like reasoning, so we will need more resources on this topic. \n\nThe rigorous characterization of the reasoning types by the inference rules used can be helpful in downstream uses and analysis. I also appreciate the use of characterizations of NM taken from the classical literature which have been under-used by modern NLP.\n\nThe variation generation part of the generation pipeline is a good idea and helps make the evaluation more robust. \n\nThe extra set of experiments showing that LogicBench(Aug) can be helpful in multi-task training of logic tasks is a useful conclusion.\n\nPerformance on some of these categories seem to go down with larger model, which if they hold up (see my concern below) would make them a good candidate for Inverse Scaling [1]. \n\n\n[1] https://arxiv.org/abs/2306.09479"
                },
                "weaknesses": {
                    "value": "Notwithstanding my #1 strength above, I have to say that I have mixed feelings about the overall utility of this work to reasoning. In a sense, LogicBench is a sort of advanced version of ProofWriter type datasets, and so while I think it will have some marginal utility in the field. it also takes it in the wrong direction in a sense (in my opinion) by focusing on a set of clearly delineated but complex and unusual inference steps, whereas advanced \"human-like\" reasoning for LLMs will require doing multiple steps of reasoning in a very complex and realistic context where there are multiple sources of ambiguity to contend with. Examples derived from templated inference rules don't solve this problem. \n\nMy biggest concern is that the results of GPT-4 on even the simpler PL problems don't even beat a majority baseline of 50%, which seems very unexpected and possibly a bug of some kind. The authors do not even mention this surprising finding, which makes me wonder if I have misunderstood the experiments (Each task is binary Y/N, so random should be 50% ?)\n\nWith a complex benchmark like this, one generally hopes that there have been systematic checks for the integrity and reliability of the benchmark as having sensible answers and tracking the phenomenon to be measured. The authors only offer a vague assurance in the end of sec 3.3 without concrete evidence; i would love to have seen something like a small-scale human eval  on a subset to confirm the validity of the benchmark. \n\nOverall the paper is fairly clear in what it is doing, but there are places where the authors are vague, esp. sec 4.3 first section. Most of the points in this section aren't big surprises."
                },
                "questions": {
                    "value": "1. You say this is the first benchmark for NMR, but there have been others e.g. BoardGameQA [1]\n\n2.  Table 2, i think the definition of EI is backwards.\n\n3. It seems when we generate natural language instantiations of each rule, we expect 'if' to always be treated as material implication, but it is well known that depending on the context 'if' can be interpreted in many other ways e.g. as a subjunctive, so it may be the case that the  more natural interpretation of a particular context sentence generated by the pipeline is such. Is this a problem for the reliability of the benchmark?\n\n4. Another surprise for me is that the PL benchmarks do worse with GPT-4 than NM, which again defies expectations. How do authors explain this?\n\n5. Is LogicT5 pre-trained from scratch on LogicBench(Aug) using the T5 model recipe or do you train on LogicBench starting from a pretrained T5-Large checkpoint? \n\n6. sec 3.2.1: the purpose of this step is to generate single atomic propositions. it seems we do this by generating entire triples for particular inference steps (like Modus Tollens) and then only extracting the individual atomic propositions. Why this complicated process?  \n\n\n[1] https://arxiv.org/abs/2306.07934"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6607/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699327861445,
            "cdate": 1699327861445,
            "tmdate": 1699636752959,
            "mdate": 1699636752959,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NpIhkshkGA",
                "forum": "71kocBuhNO",
                "replyto": "0FNuxxOnhP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6607/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6607/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "R1: Further Discussion on the role of LogicBench towards \u201chuman-like\u201d multiple steps reasoning\n\nA1:  We note that our benchmark represents a crucial first step in evaluating a variety of LLMs across a spectrum of logical inference rules and shows that these LLMs struggle even with the single-step reasoning process when logical rules become complex.  Furthermore, errors made during the initial single-step reasoning can be propagated through subsequent reasoning steps, potentially leading to wrong conclusions in multi-step reasoning scenarios (which is more human-like reasoning). While our benchmark primarily addresses single-step reasoning and doesn't specifically delve into multi-step human-like reasoning, it leads to valuable findings that LLMs struggle with even single-step reasoning for complex inference rules. Moreover, even in inference rules (e.g., HS, CD, DD, etc.), multiple sub-steps are involved in matching with the appropriate parts of the inference rule. \n\n--------\nR2: Clarification on task and further justification of GPT-4 results on simpler PL problems\n\nA2: Each task is a binary classification task with a Yes/No answer. Note that, the results in Table 6 show that GPT-4 demonstrates superior performance for simpler PL inference rules such as HS and MT as compared to others in PL which is expected. However, as we introduce more complex PL inference rules (i.e., longer inference rules that are not commonly used in human writing) such as DD, BD, and CD, GPT-4 starts to show a drop in performance. This is one of the findings in the paper as discussed in Section 4.3 (\u201cLonger inference rules are still challenging\u201d).\n\n------\nR3: Additional details on findings that are presented in Sec 4.3.\n\nA3:  Thanks for this comment. Please note that we performed qualitative analysis over selected samples (where the reasoning chain leads to incorrect predictions) to identify flaws in the reasoning process of LLMs, hence, low performance. We believe that different findings across various reasoning types are useful in terms of understanding mistake that is made by models in general. Moreover, this qualitative analysis supports the difficulty posed by negations and the challenges associated with longer inference rules. \n\n---------\nR4: Use of \u2018If\u2019 as material implication in LogicBench\n\nA4: Thanks for bringing up this interesting point. While the use of \u2018if\u2019 in natural language sentences in the wild could have other interpretations such as a subjunctive, we took care in our construction so that in our dataset \u2018if\u2019 meant material implication. Specifically, our data creation pipeline ensures the logical correctness of generated context and questions where \u2018if\u2019 is used as material implication, thus maintaining the reliability of our instances.\n\n--------------\nR5: Justification of lower performance of GPT-4 on PL as compared to NM\n\nA5: Thank you for making this observation. Its explanation, as we give below, is illuminating. In the development of AI, non-monotonic logics were partly developed to formalize natural language constructs, such as \u201cnormally birds fly\u2019\u2019, that were not formalizable in a straightforward manner using classical mathematical logics. Thus, while it was difficult for researchers to come up with non-monotonic logics and formalize non-monotonic reasoning, the fact that they were usually motivated by natural language examples, suggests that many of the non-monotonic reasoning aspects are present in the NL text in the wild that is used in the pre-training of the ultra-large LLMs such as GPT4. On the other hand, some of the PL features are counterintuitive to humans such as if we have contradiction (a and ~a)  then everything (even unrelated) is entailed. Also, some PL features are perhaps less prevalent in human writing (on which LLMs are trained) - such as Modes Tollens. Thus your observation is not a surprise and we will add this point to the paper.\n\n------------\nR6: Is LogicT5 pre-trained from scratch using the T5 model recipe or from a pretrained T5-Large checkpoint?\n\nA6: We train T5 on LogicBench(Aug) starting from a pre-trained T5-Large checkpoint.\n\n---\nWe continued the discussion in the next comment. Thank you!"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6607/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700555343859,
                "cdate": 1700555343859,
                "tmdate": 1700555458339,
                "mdate": 1700555458339,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zMsd05yK9w",
                "forum": "71kocBuhNO",
                "replyto": "kHpLpGoX38",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6607/Reviewer_5vLY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6607/Reviewer_5vLY"
                ],
                "content": {
                    "title": {
                        "value": "baseline"
                    },
                    "comment": {
                        "value": "A2: Each task is a binary classification task with a Yes/No answer. Note that, the results in Table 6 show that GPT-4 demonstrates superior performance for simpler PL inference rules such as HS and MT as compared to others in PL which is expected. However, as we introduce more complex PL inference rules (i.e., longer inference rules that are not commonly used in human writing) such as DD, BD, and CD, GPT-4 starts to show a drop in performance. This is one of the findings in the paper as discussed in Section 4.3 (\u201cLonger inference rules are still challenging\u201d).\n\nCan you tell us what the percentage of y/n labels were in general for these problems. There is such a divergence in accuracy between the results for Y v N, I suspect either strong class imbalance or that the prompts you used were not good (e.g. more few-shot Y examples than N).\n\n> Specifically, our data creation pipeline ensures the logical correctness of generated context and questions where \u2018if\u2019 is used > as material implication, thus maintaining the reliability of our instances.\n\nhow?\n\n>On the other hand, some of the PL features are counterintuitive to humans such as if we have contradiction (a and ~a) then >everything (even unrelated) is entailed. Also, some PL features are perhaps less prevalent in human writing (on which LLMs >are trained) - such as Modes Tollens. Thus your observation is not a surprise and we will add this point to the paper.\n\n This is a good point. Any way of bringing it out rigorously from your results?\n\n> Please note that we performed qualitative analysis over selected samples (where the reasoning chain leads to incorrect predictions) \n\nWhere is this qualitative analysis?\n\n> We follow the definition of EI from https://en.wikipedia.org/wiki/Existential_instantiation\n\nHow do you implement the notion of a \"new constant symbol\" in natural language?\n\n> During the rebuttal period, we conducted a human evaluation on a subset of LogicBench(Eval). We will share the results earliest possible\n\nthis would be helpful."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6607/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700584597023,
                "cdate": 1700584597023,
                "tmdate": 1700584597023,
                "mdate": 1700584597023,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "C4FEGoGpxS",
                "forum": "71kocBuhNO",
                "replyto": "0FNuxxOnhP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6607/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6607/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "R6: Small-scale Human Evaluation on LogicBench(Eval)\n\nA6: Thank you for this suggestion. Please note that, in Sec. 3.3, we performed a systematic evaluation to ensure that generated instances follow the intended logical structure. To further support the integrity and reliability of the benchmark, we hired three graduate student volunteers to manually check the quality of generated data instances. Due to the limited rebuttal period, we randomly selected 5 instances from each inference rule, resulting in a total of 125 instances across 25 reasoning patterns for human evaluation. In particular, annotators are asked to provide binary answers (yes/no) for \u201cValidity of generated context\u201d, and \u201cValidity of generated question\u201d to make sure they adhere to the intended logical structure. Each instance is annotated by three different annotators.  The inter-annotator agreement, measured with raw/observed agreement is 0.956. When there was a disagreement between annotators, a majority class was preferred. We provided example task instructions at https://anonymous.4open.science/r/LogicBench-EEBB/human_eval/readme.md. Results reveal that, after majority voting, annotators provided \u2018yes\u2019 for all 125 instances showing that all given instances adhere to the intended logical structure which further supports the quality of data checked by authors. We added this study in Appendix L."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6607/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634122540,
                "cdate": 1700634122540,
                "tmdate": 1700717981773,
                "mdate": 1700717981773,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wpYGYjnCBE",
                "forum": "71kocBuhNO",
                "replyto": "C4FEGoGpxS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6607/Reviewer_5vLY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6607/Reviewer_5vLY"
                ],
                "content": {
                    "title": {
                        "value": "imbalance"
                    },
                    "comment": {
                        "value": ">R1: Can you tell us what the percentage of y/n labels were in general for these problems?\n\n>A1: For LogicBench(Eval), out of 1720, 531 samples are for \u2018yes\u2019 and 1189 samples are for \u2018no\u2019. LogicBench(Aug) follows the >same ratio between yes and no questions. Thus, we presented a separate accuracy in Table 6, A(Yes) and A(No) to evaluate >model performance. We updated this in the paper (Sec. 3.3).\n\nThen why the severe difference in the accuracy numbrers ? I suppose you want to argue that models struggle to recognize a valid reasoning chain, but it could just as well have been poor templates in the CoT (does Flan-T5 even respond to cot? not sure). Different models might actually need different prompts, usually the right prompt can make a significant difference. It's important to know how these were chosen and how we can rule out the effect of the prompt. I see you averaged 3 prompts, but how were they chosen?\n\n\n    I think what's missing the most in your paper is good analysis of the results. For example, if you had broken down the results by \"length\" of inference rule, you could have clearly established the conclusion you draw about its effect rather than making a claim where the evidence needs to be inferred.\n\n> For inference rules with \u2018if\u2019 used in them, we created prompts such that context and questions provided in those exemplars >always contain the \u2018if\u2019 as material implication\n\nHow did you ensure this? I see this as a very hard problem since so much depends on context. e.g. 'If the butler did it, then the gardener is innocent.' would be interpreted by most speakers as either a suppositional or a subjunctive. how do you ensure that the pipeline never generates sentences like this ?"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6607/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700661527683,
                "cdate": 1700661527683,
                "tmdate": 1700661527683,
                "mdate": 1700661527683,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "u2En9T9dEW",
                "forum": "71kocBuhNO",
                "replyto": "0FNuxxOnhP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6607/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6607/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response.\n\n---\nR1: How were prompts chosen?\n\nA1: Please note that prompts presented in Appendix H.2 (used in our study) are Zero-shot-CoT prompts and we use the original paper for Zero-shot-CoT [1] as a reference to create those prompts manually.\n\n[1] https://arxiv.org/pdf/2205.11916.pdf \n\n---\nR2: Does Flan-T5 even respond to CoT?\n\nA2: Yes, FLAN generates basic explanations for the answer when given a CoT prompt instead of generating direct predictions. Thus, we also included FLAN-T5-3B (along with GPT-3/4, ChatGPT, and Tk-instruct) in our study for comprehensive evaluation. Please see the below example response generated by FLAN-T5-3B using our prompt 3 (Appendix H.2):\n```\nContext: If Liam finishes his work early, then he will order pizza for dinner.\nQuestion: If he won't order pizza for dinner, does this imply that Liam didn't finish his work early?\nFLAN (prediction): The answer is yes because if he didn't order pizza which means that he didn't finish his work early.\n```\nFrom the response, we can see that FLAN generates basic explanations given the CoT prompt, not a detailed step-by-step process.\n\n---\nR3: Analysis of results by \"Length\" of inference rules\n\nR3: Thank you for this suggestion. Context in the LogicBench(Eval) uses templates developed from formal expressions of inference rules, hence we use token length of context to measure the length of inference rules. The below table represents the performance of GPT-4 w.r.t. token length of context. Since inference rules from classical logic, PL and FOL, are formally expressed, we evaluate the effect of context length on the performance of GPT-4 on these inference rules.\n\n| Inference Rule  | Avg. Length | A(Yes) | A(No) |\n| ------------- | ------------- | ------------- | ------------- |\n| EG (FOL)\t        | 9.4\t              | 100.00%    | 100.00% |\n| MP (FOL)\t        | 13.8.            | 98.41%.     | 93.75% |\n| MI (PL)\t        | 16.55\t      | 46.03%\t   | 81.32% |\n| MT (FOL)\t        | 16.55\t      | 64.89%\t   | 81.56% |\n| UI (FOL)\t        | 16.9\t      | 100.00%\t   | 98.41% |\n| MT (PL)\t        | 17.7\t      | 85.19%\t   | 94.15% |\n| HS (FOL)\t        | 32.1\t      | 91.49%\t   | 97.93% |\n| HS (PL)\t| 35.65 | 96.45% | 98.76% |\n| DS (FOL)\t| 65.55\t| 33.33%\t| 75.32% |\n| CT (PL)\t| 68.05 |\t12.96% |\t76.55% |\n| DS (PL)\t| 68.45 | \t26.67%\t| 76.22% |\n| BD (FOL)\t| 93.05 |\t0.00% |\t76.56% |\n| CD (FOL)\t| 97.75 |\t33.33% |\t75.46% |\n| DD (PL)\t| 97.9\t| 33.33%\t| 75.93% |\n| BD (PL)\t| 97.9\t| 20.00%\t| 75.03% |\n| CD (PL)\t| 98.3\t| 33.33%\t| 76.13% |\n| DD (FOL) | 98.6\t| 14.29%\t| 78.30% |\n\nFrom the results, please note that the performance of GPT-4 decreases with the increasing token length of context. Specifically, GPT-4 often shows low performance on inference rules with longer context (e.g., CD, DD, BD, etc.).\n\n---\nR4: How did we ensure \u2018if\u2019 as material implication in prompts?\n\nR4: Our prompts consist of <sentences, context, question> triplets as exemplars, and we (authors) created those examples manually where we ensure the examples always contain \u2018if\u2019 as material implication. Furthermore, we manually validated LogicBench(Eval) to make sure that the generated context and question adhere to the intended structure of inference rules, and the \u2018if\u2019 in the inference rules that we use is the material implication \u2018if\u2019."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6607/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700715190756,
                "cdate": 1700715190756,
                "tmdate": 1700715496703,
                "mdate": 1700715496703,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]