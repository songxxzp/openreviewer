[
    {
        "title": "H-GAP: Humanoid Control with a Generalist Planner"
    },
    {
        "review": {
            "id": "Xmye6iPuQe",
            "forum": "LYG6tBlEX0",
            "replyto": "LYG6tBlEX0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7263/Reviewer_bwbh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7263/Reviewer_bwbh"
            ],
            "content": {
                "summary": {
                    "value": "This paper incorporates VQ-VAE, a prior transformer, and MPC planning to construct H-GAP, a state-action generative model for humanoid trajectories. Experiments show that H-GAP learns to generate a wide range of human behaviors through imitation learning and flexibly transfers to unseen downstream tasks through planning without retraining."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The proposed method outperforms MLP-backed behavior cloning in imitation learning by a large margin. Despite being a generalist policy, it also outperforms MPPI in most tasks by large margins in downstream tasks and performs reasonably well against offline RL specialist policies. \n\nWe also observe that the performance of imitation learning increases as the model and data scale up, paving the way to a possible future foundation model for humanoid control. However, we observe a drop in downstream task performance as the model size increases."
                },
                "weaknesses": {
                    "value": "1. The provided link in the abstract results in 404 not found, and this submission comes without supplementary videos. This, unfortunately, makes it difficult to assess the naturalness of the generated human motion, which is critical to humanoid motion generation. \n2. The paper claims that H-GAP provides a basis for a foundation model for humanoid control. However, only the performance of imitation learning improves with the model scale. The performance of downstream tasks drops for larger models. In my view, this does not constitute a foundation model's basis because supporting downstream tasks is the most important feature of a foundation model."
                },
                "questions": {
                    "value": "- What are the metrics used in table 1 and table 2?\n- How long does it take to train H-GAP?\n- I would assume that the optimal number of samples $N$ corresponds to the length of the intended trajectory $T$. What is the choice of $N$ for your experiments when $T=M=16$ and $L=4$?\n- Following up from the above question, how long does it for H-GAP to produce the reported results in table 2? How does it compare to the other methods?\n- Does the temperature change throughout algorithm 1? What are the choices of $\\Upsilon$ and $\\rho$?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7263/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7263/Reviewer_bwbh",
                        "ICLR.cc/2024/Conference/Submission7263/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7263/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698045297901,
            "cdate": 1698045297901,
            "tmdate": 1700640565360,
            "mdate": 1700640565360,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7qHxbwtPgH",
                "forum": "LYG6tBlEX0",
                "replyto": "Xmye6iPuQe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7263/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7263/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Link has been fixed"
                    },
                    "comment": {
                        "value": "> The provided link in the abstract results in 404 not found, and this submission comes without supplementary videos. This, unfortunately, makes it difficult to assess the naturalness of the generated human motion, which is critical to humanoid motion generation.\n\nWe apologize that the website didn\u2019t work during your review. It was due to a privacy setting that was not properly set. We fixed it as soon as we noticed the issue during the reviewing period. We can confirm it is working now.\n\nThis is a quick response to this particular issue and we'll address your other concerns and questions later this week (together with the response to other reviewers)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7263/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699894415811,
                "cdate": 1699894415811,
                "tmdate": 1699894415811,
                "mdate": 1699894415811,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jDAOJfLrFx",
                "forum": "LYG6tBlEX0",
                "replyto": "Xmye6iPuQe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7263/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7263/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "> The paper claims that H-GAP provides a basis for a foundation model for humanoid control. However, only the performance of imitation learning improves with the model scale. The performance of downstream tasks drops for larger models. In my view, this does not constitute a foundation model's basis because supporting downstream tasks is the most important feature of a foundation model.\n\nThank you for your insights. We acknowledge that the term 'foundation model' might have suggested our model is larger than it is. To clarify, we view H-GAP as an initial step towards a foundational model for humanoid control, given its generalist nature and training on a diverse dataset. We've adjusted our wording to 'an early attempt to build a foundation model for humanoid control' for clarity.\n\nThe scaling properties highlighted in our paper are part of our empirical contributions, underscoring the importance of data scaling relative to model scaling. This finding suggests prioritizing data enhancement to advance the generalist approach in humanoid control. Additionally, we delve into the reasons behind the limited impact of model scaling on downstream task performance in Section 3.3, providing a deeper understanding of these dynamics.\n\nWe want to highlight that our findings do not conflict with other empirical findings in foundation models for other modalities because:\nThe trend of imitation learning performance and validation loss of H-GAP do align with the scaling law reported in studies on large language models (LLMs) [1, 2].\n\nLarge foundation models are usually trained on a huge and diverse internet-scale dataset. For example, the whole training process of LLMs is usually only done for one or two epochs. If the training data is very limited, downstream tasks are not guaranteed to be improved just by scaling the model [3, 4].\n\n\n\t[1] Hoffmann et al. \u201cTraining Compute-Optimal Large Language Model\u201d. 2022\n\t[2] Henighan et al. \u201cScaling Laws for Autoregressive Generative Modeling\u201d. 2022\n\t[3] Muennighoff et al. \u201cScaling Data-Constrained Language Models\u201d. Neurips 2023\n\t[4] Radford et. al. Robust Speech Recognition via Large-Scale Weak Supervision, 2022\n\n> What are the metrics used in table 1 (imitation performance) and table 2 (downstream performance)?\n\nThe metrics used in table 1 are the average episodic clip tracking returns. The task is set up similar to the multi-clip tracking task [1], where the reward function measures the discrepancy between the agent\u2019s joint and body positions and the target configurations in a reference trajectory. We adapt our implementation from the ReferencePosesTask from the DeepMind Control Suite [2]. We have added more explanation to the numerical results in section 3.1 of the manuscript (highlighted red).\n\nThe metrics used in downstream control tasks (table 3 in the latest manuscript) are average episodic returns on downstream tasks. \n\n[1] Hasenclever et al. \u201cCoMic: Complementary task learning & mimicry for reusable skills.\u201d ICML 2020.\n[2] Tunyasuvunakool et al. \u201cdm_control: Software and tasks for continuous control\u201d 2020.\n\n> How long does it take to train H-GAP?\n\nThe walk clock training time depends on the model size, number of tokens used for training and system configuration. We report the training time of the models used in most of our experiments: training a 10M parameter VQ-VAE for 6.5M tokens on 1 V100 GPU takes 9 hours. Training a 40M parameter Prior Transformer for 6.5M tokens on 8 V100 GPU on a single node takes ~20 hours. For the larger model, say, a 300M parameter Prior Transformer trained on 8 V100 GPUs for 6.5M tokens extends to approximately 45 hours.\n\n> I would assume that the optimal number of samples N corresponds to the length of the intended trajectory T. What is the choice of N for your experiments when T=N=16 and L=4?\n\nWe use S=256 when reporting the results in downstream control tasks (table 3 in the latest manuscript). \n\n> Following up from the above question, how long does it take for H-GAP to produce the reported results in table 2 (downstream performance)? How does it compare to the other methods?\n\nThe planning time of a 10M parameter H-GAP takes 0.29s, on a single 4090 GPU, with sample size 256. For comparison, MPPI can take about 20 seconds for a single step of decision with a trajectory ensemble size of 32, but this is probably because we lack a good parallelizable implementation. We also tried to increase the ensemble size to 64 after paper submission, but didn't observe improvement.\n\n> Does the temperature change throughout algorithm 1? What are the choices of  \\upsilon and /rho\n\nThe temperature is fixed throughout algorithm 1. We use \\upsilon=4 and \\rho=0.99 when reporting the results in downstream control tasks (table 3 in the latest manuscript). We have included a table of the planning hyperparameters and additional ablation results in the appendix."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7263/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700086605163,
                "cdate": 1700086605163,
                "tmdate": 1700086605163,
                "mdate": 1700086605163,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0QfXHamrvv",
                "forum": "LYG6tBlEX0",
                "replyto": "jDAOJfLrFx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7263/Reviewer_bwbh"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7263/Reviewer_bwbh"
                ],
                "content": {
                    "comment": {
                        "value": "The authors' reponse have addressed my concerns. Based on the revised tone and the contribution of H-GAP, I am raising my rating to 8. Good work."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7263/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700640552405,
                "cdate": 1700640552405,
                "tmdate": 1700640552405,
                "mdate": 1700640552405,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "UyNcWz36Hl",
            "forum": "LYG6tBlEX0",
            "replyto": "LYG6tBlEX0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7263/Reviewer_kAwr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7263/Reviewer_kAwr"
            ],
            "content": {
                "summary": {
                    "value": "This paper tackles the challenging problem of general humanoid control, which is difficult due to high-dimensional action spaces and instability. The authors propose a Humanoid Generalist Autoencoding Planner (H-GAP) that learns a forward dynamics latent space model from the MocapACT dataset and propose model predictive control for downstream tasks. The forward dynamics model is trained using a transformer over discrete latent codes learned using a VQVAE that employs multiple codes per transition. The effectiveness of H-GAP is demonstrated through experiments on a 56 DoF humanoid across imitation benchmarks as well as analyses of its scaling properties on different model sizes."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Trains a single prior transformer for motion completion in a more principled way to jointly learn a motion prior and forward dynamics model that can be queried autoregressively and used for MPC planning. Introduces multi-code latent space modeling with VQVAE and transformer to accurately capture complex humanoid state-action sequences.\n    \n- Analyzes scaling properties on model sizes up to 300M parameters, revealing limitations in downstream task performance despite accuracy gains.\n    \n- Demonstrates strong imitation learning on a 56 DoF humanoid, validating the model's ability to represent diverse motor behaviors from the MocapAct dataset."
                },
                "weaknesses": {
                    "value": "- The paper could analyze the coverage of different motion skills by randomly sampling initial states and generating rollouts from the model. This would help quantify the diversity of motions that can be produced [1].\n    \n- For analyzing imitation abilities, the model could be conditioned on a small segment of reference trajectories rather than just initial states. This would better evaluate how well it can leverage larger context sequences. The current approach in Table 1 mainly reveals modeling/memorization capabilities rather than imitation abilities.\n    \n- Additional experiments could be designed to isolate the planning contributions of the model beyond just trajectory modeling. For example, a conditional generation task could be constructed using out-of-distribution initial states in order to see if the model is able to \u201ccatch-up\u201d to imitate a trajectory from a different initial polse from the original trajectory.\n    \n- More complex downstream tasks requiring long-term planning could be explored to go beyond trajectory imitation. The current tasks are limited to state distributions from the training data. New tasks like going to specified locations, or following a particular heading and and orientation[1] would require guiding the motion generation process. This could reveal how well the model can piece together behaviors.\n    \n- Overall, evaluating the model on tasks and scenarios that require leveraging the full context sequence in a generative way would provide better insight into the planning abilities. The current experiments focus primarily on trajectory modeling in different flavors. (imitation based Downstream tasks and imitation benchmark)\n    \n\n[1] Dou, Zhiyang et al. \u201cC\u00b7ASE: Learning Conditional Adversarial Skill Embeddings for Physics-based Characters.\u201d\u00a0*ArXiv*\u00a0abs/2309.11351 (2023): n. pag."
                },
                "questions": {
                    "value": "NA"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7263/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7263/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7263/Reviewer_kAwr"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7263/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698816370979,
            "cdate": 1698816370979,
            "tmdate": 1700735288031,
            "mdate": 1700735288031,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Qz67YZb2bX",
                "forum": "LYG6tBlEX0",
                "replyto": "UyNcWz36Hl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7263/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7263/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you for your valuable feedback. We'd like to clarify a potential misunderstanding regarding our experiments. The setups for our imitation experiments (Section 3.1 and Table 1) and downstream control tasks (Section 3.2 and Table 3) are distinctly different.\n\n**Initial State Configurations**: In the imitation tasks, the initial states are derived from the trajectories of the reference motions, varying across subtasks. For downstream tasks, however, a uniform initial state distribution is used across all tasks. This setup demonstrates that H-GAP's performance exceeds mere imitation, as evidenced by non-zero sum returns in forward and backward tasks.\n\n**Action Selection Approaches**: During imitation learning, we decode latent codes greedily, without employing any planning. Conversely, in downstream tasks, we utilize Model Predictive Control (MPC) to optimize the agent's behavior for achieving the highest task rewards, which further differentiates the two experimental setups.\n\n**Further experiments**: To further address your concern on this, we run an actual imitation H-GAP agent on downstream control tasks, and the average score across tasks are 22.04, which is significantly lower than H-GAP with MPC planning (46.18).\n\n> The paper could analyze the coverage of different motion skills by randomly sampling initial states and generating rollouts from the model. This would help quantify the diversity of motions that can be produced [1].\n\nIn response to your suggestion, we would appreciate further details regarding the specific distribution you have in mind for sampling.\nUniform or Gaussian sampling in a high-dimensional state space often leads to unnatural poses and ineffective rollouts. \nOn the other hand, if you're referring to sampling from the MocapAct dataset, our imitation learning experiments serve as an indirect assessment of the model's ability to capture a broad spectrum of behaviors. Here, initial states are drawn from a variety of clips, and Gaussian action noise is added for enhanced variability. If your suggestion falls outside these approaches, we'd appreciate further clarification to better address your point.\n\n> For analyzing imitation abilities, the model could be conditioned on a small segment of reference trajectories ... Table 1 mainly reveals modeling/memorization capabilities rather than imitation abilities.\n\nIn addressing your suggestion for conditioning the model on reference trajectories, we interpret this in two ways: conditioning on either a longer historical context or a target future trajectory. Using a longer historical context could enhance imitation performance, but may reduce the steerability for downstream control tasks. Our focus with H-GAP is on generalist control, where imitation learning primarily serves to validate model accuracy.\n\nConditioning on a target future trajectory, while potentially improving imitation, diverges from the norm in imitation learning, where policies typically access only current observations, not future trajectories. We believe our current approach maintains a fair balance in this context.\n\nAdditionally, to counteract mere memorization by the agent, we introduce Gaussian noise into the action selection process, ensuring that the model's performance extends beyond simple replication of data.\n\n> Additional experiments could be designed to isolate the planning contributions of the model beyond just trajectory modeling. ... is able to \u201ccatch-up\u201d to imitate a trajectory from a different initial pose from the original trajectory.\nMore complex downstream tasks ... This could reveal how well the model can piece together behaviors.\nOverall, evaluating the model on... provide better insight into the planning abilities. The current experiments focus primarily on trajectory modeling in different flavors. (imitation based Downstream tasks and imitation benchmark) \n\n\nThank you for your valuable suggestions. It appears that your concerns focus on whether H-GAP, coupled with Model Predictive Control (MPC), is only used for to imitation. To address this, we'd like to emphasize the diverse and perturbed nature of initial states used in our experiments, as detailed in Section 3.2.1. These states, often not directly aligned with the specific downstream task, necessitate adaptive behaviors from the agent. For instance, in the 'rotate y' task in this rollout video: , the agent starts from a forward-walking initial state and must adjust to execute the rotation, demonstrating the model's capability to \u201cpiece together behaviors\" in a non-trivial manner.\n\nLooking ahead, we are committed to exploring long-horizon, sparse-reward tasks to further probe our model's potential as you suggested. Our approach's strength in encoding compact, high-dimensional humanoid trajectories positions us well to tackle these more complex, long-horizon challenges, demonstrating the adaptability and broad applicability of our method."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7263/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700085576064,
                "cdate": 1700085576064,
                "tmdate": 1700085576064,
                "mdate": 1700085576064,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gRSOTZKgf5",
                "forum": "LYG6tBlEX0",
                "replyto": "UyNcWz36Hl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7263/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7263/Authors"
                ],
                "content": {
                    "title": {
                        "value": "New Reference Inclusion"
                    },
                    "comment": {
                        "value": "We would like to also thank you for your suggestion regarding additional references on humanoid control. We have incorporated the following reference into the revised version of our manuscript:\n\n>Dou, Zhiyang et al. \u201cC\u00b7ASE: Learning Conditional Adversarial Skill Embeddings for Physics-based Characters.\u201d ArXiv abs/2309.11351 (2023): n. pag."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7263/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700101409272,
                "cdate": 1700101409272,
                "tmdate": 1700101409272,
                "mdate": 1700101409272,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HdR0jYZriK",
                "forum": "LYG6tBlEX0",
                "replyto": "Qz67YZb2bX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7263/Reviewer_kAwr"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7263/Reviewer_kAwr"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for the detailed response. This was very helpful. The Authors commitment to exploring the potential of the direction is very welcome. I have updated my score.\n\nSome additional comments would be to include (1) explanations on the performance drop see in larger planning horizons, (2) the compute/latency tradeoff incurred in different choices of MPC parameters (3) some standard metrics like MPJPE [1] on the current imitation tasks as well as some noisy tasks such as video tracking data. This noisy task with partial matching goal further explores the flexibility/limitations afforded by the MPC framework. \n\n[1] Luo, Zhengyi et al. \u201cPerpetual Humanoid Control for Real-time Simulated Avatars.\u201d ArXiv abs/2305.06456 (2023): n. pag."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7263/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700735240138,
                "cdate": 1700735240138,
                "tmdate": 1700735240138,
                "mdate": 1700735240138,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iQD8hapGTL",
            "forum": "LYG6tBlEX0",
            "replyto": "LYG6tBlEX0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7263/Reviewer_oGKQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7263/Reviewer_oGKQ"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a Humanoid Generalist Autoencoding Planner (H-GAP), consisting of a VQ-VAE that maps state-action trajectories into potentially discrete codes based on MoCapAct, a transformer capable of modeling future distributions based on past observations, and an MPC planning algorithm based on multiple trajectories. Extensive experiments show that H-GAP can benefit from larger and more diverse datasets, demonstrating the potential of this line of work."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The proposed H-GAP is even simpler than the existing offline RL methods while it is general for different downstream control tasks, meaning that the algorithm does not need access to the simulator and train different high-level policies.\n\nExperimental results also show that the proposed H-GAP can outperform offline RL policy and other traditional MPC algorithms."
                },
                "weaknesses": {
                    "value": "Looking at the visualization on the website, I'm wondering why there is severe jitter compared to the ground truth sequence. I'm not sure if this is a result of the transformer not modeling temporal smoothness well, or if VQ-VAE doesn't represent state-motion clips well.\n\nI'm still not fully convinced of the ability of the proposed method to model a variety of humanoid motions (e.g., backflips) that have more complex patterns than locomotion. While the paper emphasizes data and model scaling experiments, which I appreciate, train data is a random sampling of the dataset, not of how performance changes when more action categories are added.\n\nI guess a potential limitation is the need to generate multiple trajectories. I would like to know how efficient the proposed method is, what the number of trajectories that typically need to be sampled, and whether these hyperparameters need to be changed to accept more trajectories as the motion database becomes more complex."
                },
                "questions": {
                    "value": "I'm not sure I understand correctly that the whole framework doesn't use a simulator at all. So what's the point of modeling action trajectories here, why not just state trajectories? And if so, why don't we just follow the existing kinematics-based human motion generation?\n\nOverall, the author's response to the concerns is needed to make the final decision. I am happy to increase the rating if my concerns are addressed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7263/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7263/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7263/Reviewer_oGKQ"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7263/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698910858270,
            "cdate": 1698910858270,
            "tmdate": 1699636866062,
            "mdate": 1699636866062,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RXgvqKWKDl",
                "forum": "LYG6tBlEX0",
                "replyto": "iQD8hapGTL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7263/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7263/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you for your constructive feedback. We're pleased that you recognize H-GAP's key features: its independence from a simulator during training and its general-purpose design.\n\nTo clarify, our primary focus is on continuous robotic control, not human motion animation generation. In the MPC planning process, we use predicted state rollouts to determine optimal actions, rather than to create motion animations. The videos on our website showcase this control policy in action within a simulator.\n\nWhile it's possible to use our model's trajectories for animation purposes, we view this as a separate and intriguing line of research. In a control context, these trajectories might appear natural but may not adhere closely enough to physical laws for effective action selection.\n\n\n> Looking at the visualization on the website, I'm wondering why there is severe jitter compared to the ground truth sequence. I'm not sure if this is a result of the transformer not modeling temporal smoothness well, or if VQ-VAE doesn't represent state-motion clips well.\n\nYour observation about the jitter in the visualizations is noteworthy and something we've also encountered in our research. These visualizations are simulator rollouts, not direct samples from the generative model. The jitter likely arises not from a lack of temporal smoothness in the sequence modeling but from control signal noise. We introduced action noise to enhance the challenge of the imitation task, preventing the agent from merely memorizing data. Additionally, the original Mocap data used in our model has inherent jitter, particularly in hand movements, due to tracking instabilities.\n\n> I'm still not fully convinced of the ability of the proposed method to model a variety of humanoid motions (e.g., backflips) that have more complex patterns than locomotion. While the paper emphasizes data and model scaling experiments, which I appreciate, train data is a random sampling of the dataset, not of how performance changes when more action categories are added.\n\nRegarding your concerns about our method's capability to model complex humanoid motions, we hope the updated video clips on our website will provide some reassurance. \nWe also want to clarify that the smaller datasets used in the data scaling experiments in Section 3.3 are constructed by sampling on the clip level, not on the transition or trajectory level. Each clip in the mocapact correspond to a certain type of behaviour so we believe the experiment does show the performance improves along with qualitatively more diversed actions are added.\n\n> I guess a potential limitation is the need to generate multiple trajectories. I would like to know how efficient the proposed method is, what the number of trajectories that typically need to be sampled, and whether these hyperparameters need to be changed to accept more trajectories as the motion database becomes more complex.\n\nFor all downstream control tasks and data scaling experiments, we sample S=256 trajectories. The planning process with a 10M parameter H-GAP model takes approximately 0.29 seconds on a single 4090 GPU. We acknowledge the potential for increased efficiency and are considering techniques like beam search to enhance the planning process in future developments.\n\n> I'm not sure I understand correctly that the whole framework doesn't use a simulator at all. So what's the point of modeling action trajectories here, why not just state trajectories? And if so, why don't we just follow the existing kinematics-based human motion generation?\n\nH-GAP is designed as a generative model for humanoid control, necessitating the modeling of actions, not just state trajectories, for effective control in both simulated and real-world environments. Unlike kinematics-based human motion generation methods, which lack output in executable actions, H-GAP fulfills the requirement for control tasks."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7263/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700085223050,
                "cdate": 1700085223050,
                "tmdate": 1700085223050,
                "mdate": 1700085223050,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]