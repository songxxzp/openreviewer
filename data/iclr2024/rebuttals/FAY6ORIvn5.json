[
    {
        "title": "How well does Persistent Homology generalize on graphs?"
    },
    {
        "review": {
            "id": "VbhLEsYFPA",
            "forum": "FAY6ORIvn5",
            "replyto": "FAY6ORIvn5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7697/Reviewer_yvKz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7697/Reviewer_yvKz"
            ],
            "content": {
                "summary": {
                    "value": "The authors analyze the generalization (prediction beyond training set) power of persistent homology on graphs. They also generalize existing vectorization techniques by adding non-linear layers and give experimental studies on graph classification tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors study an important problem how good the persistent homology is in prediction tasks in graph representation learning. They explore several approaches to answer this question and give experimental analysis."
                },
                "weaknesses": {
                    "value": "1. While the question addressed in this paper is important, its contribution appears to be relatively modest, especially when compared to the recent work by Morris et al. (2023). In that study, the authors delve into the predictive power of Graph Neural Networks (GNNs) through the lens of VC-dimension, and the authors of the current paper adapt their methods to the context of Persistent Homology. While the theoretical results are intriguing, their practical relevance in the machine learning domain remains questionable. \n\nThe provided bounds are often complex and abstract, making them challenging to compute in real-world applications. Given the extensive, nine-page proof section, which requires a thorough review, the paper might be better suited for a journal focused on statistics or applied topology rather than an ML venue. The heavy theoretical content, with limited applicability, calls into question the practical utility of the paper's findings in the ML community.\n\n2. The paper's readability and coherence could be significantly improved, as it currently suffers from the need for clearer definitions and explanations of key concepts. The exposition and the paper's overall objective should be more explicitly stated to facilitate a better understanding of its content."
                },
                "questions": {
                    "value": "Figures 3 and 4 are very interesting. In Figure 4, while the width is increasing, empirical values of the generalization gap stay very low. How do we explain this? On the other side, could you provide insights into the relationship between the generalization gap and the training set size?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7697/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7697/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7697/Reviewer_yvKz"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7697/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698523910209,
            "cdate": 1698523910209,
            "tmdate": 1699636937547,
            "mdate": 1699636937547,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "a3Ge1WcrlS",
                "forum": "FAY6ORIvn5",
                "replyto": "VbhLEsYFPA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7697/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7697/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your review. Below, we address your questions/comments.\n\n> \"contribution appears to be relatively modest, especially when compared to the recent work by Morris et al. (2023) [...] the authors of the current paper adapt their methods to the context of Persistent Homology.\"\n\n**Our focus and analysis are orthogonal to Morris et al. (2023).** Thanks for the chance to clarify the relationship between our Proposition 2 and the result by Morris et al. (in Proposition 1). We highlight that we only use the work by Morris to show an analogous result --- we don't build upon it or use it to prove anything. Also, our work reports a PAC-Bayesian analysis, which is unrelated to Morris et al. We added a plot to depict the dependence between the Lemmas used to prove our Proposition 2. We hope this clarifies the orthogonality of our work wrt to Morris'.\n\n> \"While the theoretical results are intriguing, their practical relevance in the machine learning domain remains questionable...The provided bounds are often complex and abstract, making them challenging to compute in real-world applications.\"\n\n**Our theoretical results are important in their own right.** Understanding the generalization behavior of ML algorithms, i.e., analyzing their ability to predict well once trained,  is one of the main goals of machine learning in general and statistical machine learning in particular. From that perspective, our theoretical results are already significant. \n\n**Practical implications of our work.** We introduce a new algorithm by regularizing PersLay (please see the new Section 3.4). The regularization term directly exploits the dependence of parameters established in our bounds. In our experiments with several real-world experiments, we show that this method generally improves performance over the standard (i.e., unregularized) PersLay algorithm across many real datasets, often significantly. Our empirical results strongly substantiate the practical benefits of our theoretical analysis that leads to a new algorithm with better performance.\n\n> \"Given the extensive, nine-page proof section, which requires a thorough review, the paper might be better suited for a journal focused on statistics or applied topology rather than an ML venue. The heavy theoretical content, with limited applicability, calls into question the practical utility of the paper's findings in the ML community.\"\n\n**\"Theoretical issues in deep learning\" is an explicitly identified relevant topic for ICLR (https://iclr.cc).** Although there has been a surge in PH-based methods for machine learning, our understanding of their theoretical properties is vastly under-explored, especially for graph-structured data. Therefore, we believe that our work represents an important contribution to the topological deep learning community. \n\n> \"The paper's readability and coherence could be significantly improved [...] and the paper's overall objective should be more explicitly stated to facilitate a better understanding of its content.\"\n\n**We have revised the presentation to make this work broadly accessible**. Thanks for your comment. We have rewritten sections of the paper for clarity. In particular, we have\n- clarified the relevance of the VC-Dimension bounds in the context of generalization and expressivity of any PH method (Section 3.1);\n- fixed typos and included a notation table in the Appendix;\n- expanded the notation, including the abbreviation (kFWL for k Folklore WL);\n- clarified which results apply to PH in general vs those that apply to PersLay (now called PersLay Classifier);\n-added a related works section (Section 1.2)\n- added the dependence between the different Lemmas for the VC-Dim bound (Figure 2);\n- clarified how our PAC-Bayes generalization bound for PersLay differs from the corresponding bounds for graph neural networks and feedforward networks;\n- refined the presentation by re-writing parts of sections 2 and 3 for clarity and precision.\n\n> \"Figures 3 and 4 are very interesting. In Figure 4, while the width is increasing, empirical values of the generalization gap stay very low. How do we explain this?\"\n\nFirst, we note that the empirical gap is a function of many quantities, including those other than width. More importantly, we have observed strong Pearson correlation coefficients between the empirical and theoretical gap across datasets, validating our analysis. \n\n> \"could you provide insights into the relationship between the generalization gap and the training set size?\"\n\nThe generalization gap decreases with, i.e., is inversely proportional to, the square root of the training set size. Theorem 2 reports this relationship where $m$ is the training set size.\n\n---\nThanks for your review. We improved the readability and coherence of our paper based on your feedback. If your concerns are sufficiently solved, we would appreciate if you consider raising your score. Otherwise, we will be happy to engage further and provide more clarifications."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7697/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700192160489,
                "cdate": 1700192160489,
                "tmdate": 1700192160489,
                "mdate": 1700192160489,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qJbiOJk302",
                "forum": "FAY6ORIvn5",
                "replyto": "a3Ge1WcrlS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7697/Reviewer_yvKz"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7697/Reviewer_yvKz"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the revision. While I appreciate the theoretical perspective in your results, I'm having trouble seeing their practical application in machine learning based on your experiments. I'm still concerned that the experimental section is relatively weaker, whereas the nine-page proof section constitutes the primary contribution of the paper. Therefore, I believe a thorough mathematical review for accuracy in the proofs is necessary. Due to these reasons, I'm inclined to keep my score."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7697/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700347900323,
                "cdate": 1700347900323,
                "tmdate": 1700347900323,
                "mdate": 1700347900323,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1GipuBSgLN",
                "forum": "FAY6ORIvn5",
                "replyto": "VbhLEsYFPA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7697/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7697/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you. Theoretical underpinnings of PH for ML have been lagging behind."
                    },
                    "comment": {
                        "value": "Many thanks for acknowledging our response, and sharing your perspective.  \n\nIt is our considered opinion that theoretical underpinnings provide a strong foundation that enables future empirical success stories. Good generalisation in particular has been one of the most important cornerstones and quests of Machine Learning from its very inception. In fact, generalization is at the core of the classical Structural Risk Minimization (SRM) principle that subsumes some of the most successful ML algorithms including SVMs (and kernel methods), Lasso, Ridge Regression, etc. SRM hinges on finding a good regularizer to guard against overfitting by controlling model complexity.  \n\nDespite a surge in applying PH methods for ML applications, the theoretical foundations of these methods are grossly underexplored [1, 2].  We presented here the first generalisation bounds for PH and introduced a novel structural risk minimisation algorithm (regularized PH method) based on our bounds that led to significantly improved performance on several real datasets, paving way for more comprehensive empirical investigations in domains such as drug discovery where PH descriptors are increasingly being employed [3]. \n\nBesides, our work has laid concrete foundations for analysing generalisation ability of other PH methods, and determining appropriate regularizers for principled design of novel practical algorithms for graph representation learning. We believe this shall provide a much-needed fillip to the topological data analysis and deep learning community.   \n\nAgain, thank you for your feedback. We'd love to discuss any further thoughts or comments you might have.  \n\nWe do hope for, and would greatly appreciate, your stronger support for this work once our theoretical proofs are vetted by some subset of reviewers during the ongoing review process.   \n\n[1] Turkes et al. On the Effectiveness of Persistent Homology, NeurIPS (2022).\n\n[2] Immonen et al. Going beyond persistent homology using persistent homology, NeurIPS (2023). \n\n[3] Demir et al. ToDD: Topological Compound Fingerprinting in Computer-Aided Drug Discovery, NeurIPS (2022)."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7697/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700359012558,
                "cdate": 1700359012558,
                "tmdate": 1700359032858,
                "mdate": 1700359032858,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9fRRypk9mJ",
            "forum": "FAY6ORIvn5",
            "replyto": "FAY6ORIvn5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7697/Reviewer_ys8K"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7697/Reviewer_ys8K"
            ],
            "content": {
                "summary": {
                    "value": "This paper provides first theoretical bounds for generalization of persistent homology (PH) on graphs. The results are supported with an experimental study on 5 real-world graph classification benchmarks. Moreover, additional experiment illustrates how the bounds can be used to regularize the PH pipeline."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Strengths are already described in the summary."
                },
                "weaknesses": {
                    "value": "I did not identify any major weaknesses in the paper, although I also did not check all the proofs in the supplementary material. However, the clarity of presentation could be significantly improved, see my questions and comments below."
                },
                "questions": {
                    "value": "(Q1) Related work: The related literature is missing. Does \u201cgeneralization capabilities of PH methods remain largely uncharted territory\u201d imply that there are no earlier works in this direction? If so, you can make it more explicit.  How is the recent paper [1] related to your work?\n\n[1] Immonomen, Souza and Garg, Going beyond persistent homology using persistent homology\n\n(Q2) Section 3.1: It is not very clear how Section 3.1 fits into the whole story of your paper, in particular since you write that expressivity and generalization can be at odds with each other, but you do not elaborate further. This issue is pronounced more in (the very nice) Figure 2, which is missing Proposition 1 and 2, and Lemma 2 and 3. What is the reason that Morris et al., 2023 and Rieck, 2023 that you rely on in Section 3.1, are not discussed in the Related work? Why do the experiments not validate these theoretical results too?\n\n(Q3) Table 1: In the Discussion, you write the following: \u201cIn Table 1, the study provides a valuable resource by depicting the resulting bound dependencies on various parameters. This information is instrumental in estimating the overhead introduced by PH in the generalization performance of conventional multi-layer perceptron.\u201d Immediately I was hoping to see some discussion in this direction, but I am not sure if the next two paragraphs are related to Table 1? Where does the $\\sqrt{\\ln b}$ appear, and where do we see $h \\sqrt{\\ln h}$? References to particular lemmas, theorems or tables can improve readability.\n\nIn general, can you provide some intuition about what makes the generalization bounds for PH different from other models, and/or what properties of PH do you use to obtain your theoretical results? Is it crucial that the input is a graph?\n\nMoreover, could you summarize the \u201ckey insights about the limits and power of PH methods\u201d? What do we learn from your paper about the generalization ability of PH?\n\n(Q4) \u201cWe report additional results across different epochs and hyper-parameters in the supplementary material.\u201d These results are not included?\n\n(Q5) Figure 5 is not very informative. Could it be replaced by including the results for the line and triangle point transformations (and their correlations) into Figure 3 and Figure 4?\n\n(Q6) Notation: The notation could be improved, what is the current logic? For example, you could e.g. use small case/capital case/Greek alphabet for graph nodes/sets/functions, and then be consistent. Often, you use the same notation for different things: e.g., S for both training set and the upper bound in Lemma 6, m for the size of training data and the maximal number of distinguishable graphs, omega for the PersLay weight function and for the hypothesis parameters, etc. Could you include a notation table? For instance, it took me quite some time to find what b is when seeing it appear at the end of Section 3. As you will see, a lot of the minor comments below would likely be resolved with a table summary of improved notation.\n\n\nMinor comments:\n\n-\tIn the paragraph on PAC-Bayesian Analysis, you define L_S, gamma and L_D, gamma before S, D, gamma and L are introduced. Also, for better clarity, the order of the formulas here should be reversed? Moreover, you use the notation L here, later within the line point transformation, and later also for a layer.\n-\tIn the paragraph on the Analysis Setup, when writing h_1=q, reminder the reader briefly what q is, or at least reference Figure 1.\n-\tThe acronym FWL is never introduced?\n-\tFor the node with Lemma 1 in Figure 2, you could reference Neyshabur (2018) to make it clearer that this is an earlier result and not the contribution of this paper.\n-\tIt is not clear to me when you use |x|, and when ||x||? Is ||.||_F from Table 1 ever described?\n-\tAt the end of statement of Lemma 4, I suggest to reference \u201c(see PersLay in Section 2)\u201d, so that the reader can easily find what AGG and Phi, Lambda, Gamma and L are.\n-\tAdd full stop at the end of Lemma 4 and Lemma 5.\n-\tWhich norm do you use for persistence diagrams, i.e., what is |D(G)|?\n-\tWhat is e in Lemma 6?  \n-\tWhy do we see L_D, 0 in Theorem 2, what scenario does gamma=0 reflect, can you provide some intuition? Also, you start this theorem with \u201cLet w  = \u2026\u201d, and then claim that \u201cfor any w, we have\u2026\u201d? On a related note, later in Section 4 you write that generalization gap is measured as L_D,0 \u2013 L_S, gamma=1, but you do not provide more info?\n-\tDoes the description of h, d and W_i in the caption of Table 1 reflect only the third row, or the complete table? In the latter case, make this description a separate sentence. Do you also want to mention again also what q, l, e, beta are?\n-\tWe compute correlation coefficients between -> We compute [specify which] correlation coefficients \\rho between\n-\t\u201calpha is a hyperparameter that balances the influence of the two terms\u201d: alpha should probably be replaced with lambda?\n-\t\u201cOur research highlights the significance of leveraging the principles of regularization to enhance the performance of machine learning models across diverse applications.\u201d I found this sentence rather surprising (not the focus of this work), could you rephrase or elaborate?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7697/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7697/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7697/Reviewer_ys8K"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7697/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698790325781,
            "cdate": 1698790325781,
            "tmdate": 1699636937337,
            "mdate": 1699636937337,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mNUXjY6fSS",
                "forum": "FAY6ORIvn5",
                "replyto": "9fRRypk9mJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7697/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7697/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response: Part 1/3"
                    },
                    "comment": {
                        "value": "Thanks for your detailed and thoughtful review. Below, we address your questions/comments.\n\n> \"Related work: The related literature is missing. Does \u201cgeneralization capabilities of PH methods remain largely uncharted territory\u201d imply that there are no earlier works in this direction? If so, you can make it more explicit. How is the recent paper [1] related to your work?\"\n\nThank you for your thoughtful feedback, and the opportunity to emphasize the contributions of this work.\n\n**First generalization results for PH-based methods.** Indeed, to the best of our knowledge, the generalization bounds established here are first such results for PH. Previously, PH has been used to analyze generalization behavior of other ML methods; however, the generalization of PH itself has not been considered in literature prior to this work.\n\n**Positioning wrt Immonen et al (NeurIPS'23).** Thank you for pointing us to the important work by Immonen et al (NeurIPS 2023). Our contributions are different from theirs. \n\nFrom a theoretical perspective, they focus on the {\\em expressivity} of PH methods,  characterizing the exact class of graphs that can be recognized by PH methods that rely on color-based filtrations.\nThey do not consider generalization of PH methods at all, which in contrast is the focus of this work. \n\nFrom a practical perspective, they propose a new, more expressive method RePHINE that yields improved performance on several real-world datasets. We leverage the dependence of parameters in our generalization bounds to propose a new method that regularizes PersLay, also achieving improvement on real data. \n\nIndeed, both expressivity and generalization are important from an ML perspective, and designing algorithms that strike a right balance between expressivity and generalization is a central goal of statistical learning theory. Thus, our work should be viewed as complementary to Immonen et al.\n\n> \"Section 3.1: It is not very clear how Section 3.1 fits into the whole story of your paper, in particular since you write that expressivity and generalization can be at odds with each other, but you do not elaborate further.\"\n\n**Expressivity vs. generalization, and the relevance of Section 3.1** Thank you for the opportunity to clarify this.\n\nIndeed, enhancing expressivity typically comes at the expense of  generalization. Morris et al. (2023) established such a result for Graph Neural Networks, showing that the  VC-dimension of GNNs with $L$ layers is lower bounded by the maximal number of graphs that can be distinguished by 1-WL (Weisfeiler-Leman test for isomorphism).  High VC-dimension directly translates to poor generalization, whereas by definition   greater the number of graphs that can be distinguished greater the expressivity. Thus, Morris et al. showed the tension between expressivity and generalization in the context of message passing GNNs that are at most as expressive as the 1-WL test.\n\nHowever, these results do not apply to PH. In section 3.1, we fill this gap by establishing the conflict between expressivity and generalization for any PH method, building on the results about expressivity of PH by another influential paper due to Rieck (2023). Importantly, we connect the generalization of PH via VC-dimension to expressivity in terms of the WL hierarchy (i.e., 1-WL and more expressive higher order WL tests). Our results in Section 3.1 hold particular significance since (topological descriptors obtained from) PH methods are increasingly being used to augment the capabilities of message-passing GNNs, so being able to analyze one in terms of the other is a step forward.\n\nThe result in Section 3.1 provides a lower bound on the generalization for any PH method, and thus complements the lower bound on expressivity by Rieck (2023) - both in terms of the WL/Folklore WL (FWL) hierarchy. Albeit important, these bounds do not take into account the underlying data distributions. Therefore, in the rest of the paper, we develop a data-dependent PAC-bayes bound on generalization of PersLay, a flexible and widely used PH method.\n\nImportantly, unlike the lower VC-dim bounds in Section 3.1 that are hard to estimate, we provide an upper bound on the generalization error in terms of more amenable parameters that directly lead to a new regularized PersLay method with demonstrable empirical benefits on real-world datasets.\n\n> \"This issue is pronounced more in (the very nice) Figure 2, which is missing Proposition 1 and 2, and Lemma 2 and 3.\"\n\nThanks for catching this. We have added a subplot to Figure 2, depicting the dependence between the Lemmas (2 and 3) used to prove Proposition 2.\n\n> \"What is the reason that Morris et al., 2023 and Rieck, 2023 that you rely on in Section 3.1, are not discussed in the Related work?\"\n\nThanks for pointing this out. We have added a related works sections (Section 1.2), where we also discuss these works."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7697/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700193181305,
                "cdate": 1700193181305,
                "tmdate": 1700195192438,
                "mdate": 1700195192438,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1IRTnC625o",
                "forum": "FAY6ORIvn5",
                "replyto": "9fRRypk9mJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7697/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7697/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response: Part 2/3"
                    },
                    "comment": {
                        "value": "> \"you write the following: \u201cIn Table 1, the study provides [...] bound dependencies on various parameters. This information is instrumental in estimating the overhead introduced by PH in the generalization performance of conventional multi-layer perceptron.\u201d Immediately I was hoping to see some discussion in this direction, but I am not sure if the next two paragraphs are related to Table 1?\"\n\n**Overhead due to PersLay.** Thanks for your bringing our attention to this. Based on your comment, we have improved the discussion regarding the discrepancy between our bound for the PersLay Classifier and the bounds for feedforward networks and GNNs. The dependence on width and spectral norms of the weights directly stems from Theorem 2 by absorbing non-related  quantities within the big-O notation. Analyzing the width dependence we conclude that the upper bound for PersLay increases from $O(\\sqrt{h \\ln h})$ (in feedforward networks and GNNs) to $O(h\\sqrt{\\ln h})$ when $q = \\Theta(h)$. So, to produce a tighter generalization bound,\nwe recommend choosing $q = o(h)$. We have also clarified this in the revised version.\n\n>  \"Where does the lnb appear, and where do we see hlnh? References to particular lemmas, theorems or tables can improve readability.\"\n\n**Regarding $\\ln b$ and $h \\ln h$.** Here, $b$ denotes the radius of the $\\ell_2$-ball in which the points of the persistence diagrams lie. The dependence on $b$ is subsumed within the quantity $M$ that appears in Theorem 2 (please, see Lemmas 4 and 5).  The additional dependence (compared to feedforward networks and GNNs) of our bound on the maximum input norm is proportional to $\\sqrt{\\ln b}$, which is upper bounded by $b$. Thus, PH does not incur any overhead due to this term, and so we have removed this now from the discussion section for improved exposition. \n\nFor $h \\ln h$, again, this appears in Theorem 2. Please note that, unlike the dependence on $b$, PersLay incurs an overhead proportional to $O(\\sqrt{h})$ when $q = \\Theta(h)$, as clarified above.\n\n> \"In general, can you provide some intuition about what makes the generalization bounds for PH different from other models, and/or what properties of PH do you use to obtain your theoretical results? Is it crucial that the input is a graph?\"\n\nThe main sources of difference come from the choice of the persistence diagram vectorization (Lemmas 4 and 5) and from the combination of perturbations in vectorization and linear layers (Lemma 6).\n\nRegarding the applicability to graphs, we note that our results in Section 3.1 regarding the VC dimension applies to graph data while  the PAC-Bayes bound applies more generally to diagrams extracted from any input.\n\n> \"Moreover, could you summarize the \u201ckey insights about the limits and power of PH methods\u201d? What do we learn from your paper about the generalization ability of PH?\"\n\nSome **key insights from our analysis** are:\n- There is an inherent tension between generalization and expressivity of any PH method in the context of graph representation learning (both can be lower bounded in terms of the WL hierarchy). In particular, enhancing expressivity leads to an increase in the VC-dimension, thereby worsening the ability to generalize (Section 3.1). \n- A flexible PH method, namely PersLay, has provably good generalization performance. In particular, the dependence of generalization on margin, sample complexity (i.e., size of the training dataset), and maximum norm across points in the persistence diagrams resembles and is comparable to that for feedforward neural networks (FNNs, Neyshabur et al., 2018) and graph neural networks (GNNs, Liao et al., 2020). \n- A key difference between generalization of PersLay and FNNs/GNNs is the dependence on the model width and the spectral norm of the model weights. Specifically, PersLay has slightly worse dependence in terms of these quantities; however, this gap can be regulated by modulating the dimensionality of the embedding of the persistence diagram before treating with the linear layers. \n- From a practical viewpoint, the theoretical dependence of generalization on parameters can be leveraged to introduce a regularized method in a structural risk minimization setup that leads to improved empirical performance on several real-world datasets.\n\n> \"\u201cWe report additional results across different epochs and hyper-parameters in the supplementary material.\u201d These results are not included?\"\n\nApologies for this. We did not find significant variability in these experiments and ended up not including them.\n\n> \"Figure 5 is not very informative. Could it be replaced by including the results for the line and triangle point transformations (and their correlations) into Figure 3 and Figure 4?\"\n\nThanks for the suggestion. We will remove Figure 5 and replace it with additional plots (like Fig. 3 and 4) for line and triangle transformations. We will update the paper with this change by the end of this discussion period."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7697/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700194441055,
                "cdate": 1700194441055,
                "tmdate": 1700194993410,
                "mdate": 1700194993410,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HjnjkQmwEM",
                "forum": "FAY6ORIvn5",
                "replyto": "9fRRypk9mJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7697/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7697/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response: Part 3/3"
                    },
                    "comment": {
                        "value": "> \"Notation: The notation could be improved, what is the current logic? For example, you could e.g. use small case/capital case/Greek alphabet for graph nodes/sets/functions, and then be consistent. Often, you use the same notation for different things: e.g., S for both training set and the upper bound in Lemma 6, m for the size of training data and the maximal number of distinguishable graphs, omega for the PersLay weight function and for the hypothesis parameters, etc. Could you include a notation table? For instance, it took me quite some time to find what b is when seeing it appear at the end of Section 3. As you will see, a lot of the minor comments below would likely be resolved with a table summary of improved notation.\"\n\n**We have revised the presentation**. We have rewritten sections of the paper for clarity. In particular, we have\n- clarified the relevance of the VC-Dimension bounds in the context of generalization and expressivity of any PH method (Section 3.1);\n- fixed typos and included a notation table in the Appendix;\n- expanded the notation, including the abbreviation (kFWL for k Folklore WL);\n- clarified which results apply to PH in general vs those that apply to PersLay (now called PersLay Classifier);\n- added a related works section (Section 1.2)\n- added the dependence between the different Lemmas for the VC-Dim bound (Figure 2);\n- clarified how our PAC-Bayes generalization bound for PersLay differs from the corresponding bounds for graph neural networks and feedforward networks;\n- refined the presentation by re-writing parts of sections 2 and 3 for clarity and precision.\n\n**Importantly, we have accepted all your minor suggestions/comments and implemented the changes in the revised manuscript.** We really appreciate your careful reading. \n\n---\n\nMany thanks for your thoughtful and incisive comments. We hope our response has elucidated several subtle points that you raised and allayed your concerns. We would greatly appreciate your stronger support for this work."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7697/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700194914164,
                "cdate": 1700194914164,
                "tmdate": 1700195420691,
                "mdate": 1700195420691,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TTCd4K4wQT",
                "forum": "FAY6ORIvn5",
                "replyto": "HjnjkQmwEM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7697/Reviewer_ys8K"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7697/Reviewer_ys8K"
                ],
                "content": {
                    "comment": {
                        "value": "I have read the responses to all of the reviewers, and I very much appreciate the effort that the authors made to address each of the concerns. I skimmed through the revised documents, but I haven't read them in detail; if most of the comments added here in the discussion are incorporated in the paper (it seems so), I think the work is definitely improved and I support its acceptance to ICLR.\n\nSome final minor comments: Consider revising the notation table once again, e.g. h_i (as a row in it's own right), beta, gamma, b, q ... are missing? I asked you what |D(G)| is, i.e. which norm do you use for persistence diagrams, but I think this information has not yet been included in the main text? I had to look it up in the proof of Lemma 4, where you write that, in case AGG=sum, \"A1 is the maximum size of the persistent diagram\", but what is size?"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7697/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700404361442,
                "cdate": 1700404361442,
                "tmdate": 1700404361442,
                "mdate": 1700404361442,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Fu4MS3A2Z0",
                "forum": "FAY6ORIvn5",
                "replyto": "cB2JoYLx0K",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7697/Reviewer_ys8K"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7697/Reviewer_ys8K"
                ],
                "content": {
                    "comment": {
                        "value": "\"Although the theory claims a theory of generalizability of the persistent homology of graphs, what the actual theorem shows is a generalization bound for maps that combine PersLay, ReLu, and DNN in the persistent homology.\" and the concern about the ReLU and DNN being fixed are very important, and I agree. Now that the authors differentiate between PH, PersLay and PersLay Classifier, I think the exposition is clearer and made more precise. Taking the relevance of PersLay into account, I think the first theoretical result about the generalization of a PH-based approach is nice (first step in the right direction)."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7697/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700406222652,
                "cdate": 1700406222652,
                "tmdate": 1700406222652,
                "mdate": 1700406222652,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4w2NOuctlX",
                "forum": "FAY6ORIvn5",
                "replyto": "TTCd4K4wQT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7697/Reviewer_ys8K"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7697/Reviewer_ys8K"
                ],
                "content": {
                    "comment": {
                        "value": "I would also like to add that I find this comment you made extremely useful:\n\n\"The main sources of difference come from the choice of the persistence diagram vectorization (Lemmas 4 and 5) and from the combination of perturbations in vectorization and linear layers (Lemma 6).\"\n\nbut I don't think you include this discussion in the revised paper? In my first round of feedback, I asked a lot about some intuitive summary about what we learn from your paper about the generalization of PH, and I think a paragraph about the influence of b, and the choice of point transformation, AGG and q would be very valuable."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7697/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700406551135,
                "cdate": 1700406551135,
                "tmdate": 1700406551135,
                "mdate": 1700406551135,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6VO6JXhibV",
                "forum": "FAY6ORIvn5",
                "replyto": "9fRRypk9mJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7697/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7697/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We are grateful for your active engagement and all the additional feedback. We summarize below how we've included all of your suggestions and comments.\n\n> Regarding the notation |D(G)| ---size of the diagram.\n\nWe have updated the paper to clarify this notation. Instead of using $|\\mathcal{D}(G)|$ to denote the number of points in the persistence diagrams, we are now using card($\\mathcal{D}(G)$) and call it the cardinality (instead of size) of the diagram $\\mathcal{D}(G)$. We added the following sentence to the first paragraph of page 3:\n\n*We denote the persistence diagram for a graph $G$ as $\\mathcal{D}(G)$ and use $\\text{card}(\\mathcal{D}(G))$ to represent its cardinality, i.e., number of (birth time, death time) pairs in $\\mathcal{D}(G)$.*\n\n We have also updated the notation table accordingly.\n\n> Consider revising the notation table once again, e.g. h_i (as a row in it's own right), beta, gamma, b, q ... are missing?\n\nThanks for catching this. We have included the following entries in the notation table:\n- $q$: dimensionality of the PersLay embeddings\n- $h_i$: dimensionality of the input to the $i$-th MLP layer\n- $\\gamma$: margin scalar in the margin-based loss\n- $b$: the maximum norm of the input to the feedforward network of the PersLay classifier\n- $\\beta$, $\\hat{\\beta}$: $\\max\\\\{||W_1||_2, ... ||W_l||_2, |W^{\\varphi}|_2 + 1\\\\}$, arbitrary approximation of $\\beta$\n\nWe've also improved some descriptions and the presentation of the Notation table.\n\n> I would also like to add that I find this comment you made extremely useful: \"The main sources of difference [...] combination of perturbations in vectorization and linear layers (Lemma 6).\" but I don't think you include this discussion in the revised paper?\n\nWe have now included this discussion in Section 3.4 (paragraph 'Comparison to other bounds').\n\n> I asked a lot about some intuitive summary about what we learn from your paper about the generalization of PH, and I think a paragraph about the influence of b, and the choice of point transformation, AGG and q would be very valuable.\n\nThanks to your comment, we have extended our discussion section by adding a paragraph about the 'Influence of PersLay' in Section 3.3 (Discussion). The new paragraph reads like this:\n\n**Influence of PersLay components.** *Our analysis shows that when $AGG= \\text{sum}$, it is hard to obtain reasonable generalization guarantees since $M$ depends on the cardinality of the persistence diagram, which can be large. Also, we can conclude that the generalization performance of PC using the line point transformation has a weaker dependence on $q$ ($M_1, M_2$ does not depend on $q$ in this case) than other point transformations. This is particularly relevant if one chooses a large value for $q$.*\n\nIn our previous answer, we have listed some key take-aways from our work. Here, we list them again alongside with pointers to where you can find the corresponding discussion in the paper:\n- There is an inherent tension between generalization and expressivity of any PH method in the context of graph representation learning (both can be lower bounded in terms of the WL hierarchy). In particular, enhancing expressivity leads to an increase in the VC-dimension, thereby worsening the ability to generalize (Section 3.1).  --- **We have added this right before Proposition 2**;\n- A flexible PH method, namely PersLay, has provably good generalization performance (analyzed here in the context of classification). In particular, the dependence of generalization on margin, sample complexity (i.e., size of the training dataset), and maximum norm across points in the persistence diagrams resembles and is comparable to that for feedforward neural networks (FNNs, Neyshabur et al., 2018) and graph neural networks (GNNs, Liao et al., 2020). --- **this discussion appears in the paragraph  'Comparison to other bounds' (Section 3.3)**;\n- A key difference between generalization of PersLay and FNNs/GNNs for classification is the dependence on the model width and the spectral norm of the model weights. Specifically, PersLay Classifier has slightly worse dependence in terms of these quantities; however, this gap can be regulated by modulating the dimensionality of the embedding of the persistence diagram before treating with the linear layers. --- **this discussion also appears in the paragraph 'Comparison to other bounds' (Section 3.3). We also added sentence about the main sources of differences.**\n- From a practical viewpoint, the theoretical dependence of generalization on parameters can be leveraged to introduce a regularized method in a structural risk minimization setup that leads to improved empirical performance on several real-world datasets. --- **we have added a separate subsection to introduce the regularized PersLay (Section 3.4).**\n\nMany thanks!"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7697/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700437945057,
                "cdate": 1700437945057,
                "tmdate": 1700438008456,
                "mdate": 1700438008456,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4VNRvJrdkU",
            "forum": "FAY6ORIvn5",
            "replyto": "FAY6ORIvn5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7697/Reviewer_qwyi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7697/Reviewer_qwyi"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the generalization performance of persistent homology is given in terms of PAC-Bayes. Normalized margine bounds are given via PersLay, a method that encompasses various vectorizations of the Persistent diagram. Normalized margine bounds has been theoretically proven and experiments have confirmed the theorem."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Theoretical derivation and proof of normalized margine bounds for persistent diagrams of graphs are given.\n- It is basically an analogy to Neyshabur et al. (2018), but combined with PersLay, which encompasses vectorization of various persistent diagrams to apply to persistent homology."
                },
                "weaknesses": {
                    "value": "The purpose of the main theorem seems unclear. Although the theory claims a theory of generalizability of the persistent homology of graphs, what the actual theorem shows is a generalization bound for maps that combine PersLay, ReLu, and DNN in the persistent homology. In fact, Neyshabur et al. (2018) explicitly states that it gives generalization bounds for DNNs. There seems to be a gap between the generalization performance of Persistent homology and the generalization bounds of PH, the mapping. Also, PH is only an example of one network and not for a general network.Currently, it appears to be a derivation of the generalization boundary of a self-defined network. Whether one is arguing for generalization bounds for persistent homology itself or for generalization bounds for networks using persistent homology, it seems to me that the arguments need to be organized and additional discussion is needed.\n\nThe biggest complaint is that it is extremely reader-unfriendly. For example, the definition of $gamma$-margine loss was written some time after its first appearance. It doesn't even say what k-FWL is; it may be Folklore Weisfeiler-Lehman, but it is not self-evidently recognizable to all readers of the subject. Map PH seems to be the entire architecture of Fig. 1, but the definition is unclear and the caption of Fig. 1 is sometimes described as PersLay's architecture, making it difficult to grasp."
                },
                "questions": {
                    "value": "Please comment on the above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7697/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7697/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7697/Reviewer_qwyi"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7697/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698834113600,
            "cdate": 1698834113600,
            "tmdate": 1700657977817,
            "mdate": 1700657977817,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pEL448vf4s",
                "forum": "FAY6ORIvn5",
                "replyto": "4VNRvJrdkU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7697/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7697/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your feedback. In the following, we address all your questions/comments.\n\n> \"Although the theory claims a theory of generalizability of the persistent homology of graphs, what the actual theorem shows is a generalization bound for maps that combine PersLay, ReLu, and DNN in the persistent homology.\"\n\n**Our VC-dimension results hold for any PH method on graphs.** We note that Proposition 2 sets a lower bound on the VC-dim of *any* PH method that distinguishes graphs based on their persistence diagrams obtained from arbitrary filtration functions. In particular, this establishes that enhancing expressivity of PH methods comes at the expense of generalization. Importantly, this result also connects the generalization of PH via VC-dimension to expressivity in terms of the WL hierarchy (i.e., 1-WL and more expressive higher order WL tests), which is the primary tool for analyzing and designing new GNN architectures. \n\n**Relevance of PersLay: it accommodates flexible vectorizations broadly used for PH.** Although important for showcasing the tension between generalization and expressivity, the VC-Dimension bounds do not take into account the underlying data distributions. Therefore, we establish the first data-dependent PAC-Bayesian generalization bounds fora flexible and widely used PH-based framework, namely Perslay. In particular, PersLay subsumes most persistence diagram vectorizations (e.g., persistence landscapes, persistence silhouette, deep sets, persistence images, and Gaussian-based vectorizations) proposed in the literature. Finally, our work also lays a strong foundation for analyzing other classes of PH methods, including those adopting learnable filtrations.\n\n> \"PH is only an example of one network and not for a general network.Currently, it appears to be a derivation of the generalization boundary of a self-defined network. [...] it seems to me that the arguments need to be organized and additional discussion is needed.\" \n\n**The network design is flexible.** Thanks for giving us the opportunity to clarify and improve the presentation. We analyzed multiple persistence diagram vectorization approaches in combination with a very general classification head (feedforward networks), following the same setup as PersLay. In fact, our choice of using PersLay for generalization analysis was motivated by the generality of the PersLay's vectorization scheme, which subsumes most of the previously proposed ones. These schemes have been also used in more recent methods, such as TOGL [Topological GNN, ICLR'22]. Moreover, PersLay is one of the most influential PH-based models for graph data, which is the focus of this work. Based on your feedback, we have clarified this in the revised version of the manuscript.\n\n> \"The biggest complaint is that it is extremely reader-unfriendly. For example, the definition of gamma-margine loss was written some time after its first appearance. It doesn't even say what k-FWL is; it may be Folklore Weisfeiler-Lehman\"\n\n**We have revised presentation to make this work broadly accessible.** Thanks for your comment. We have rewritten sections of the paper for clarity. In particular, we have\n - clarified the relevance of the VC-Dimension bounds in the context of generalization and expressivity of any PH method (Section 3.1); \n - fixed typos and included a notation table in the Appendix;\n - expanded the notation, including the abbreviation (kFWL for k Folklore WL);\n - clarified which results apply to PH in general vs those that apply to PersLay (now called PersLay Classifier);\n - added a related works section (Section 1.2)\n - added the dependence between the different Lemmas for the VC-Dim bound (Figure 2);\n - clarified how our PAC-Bayes generalization bound for PersLay differs from the corresponding bounds for graph neural networks and feedforward networks;\n - refined the presentation by re-writting parts section 2 and 3 for clarity and precision.\n\nWe thank you for your inputs, and believe the revised version is accessible for a broader audience and fully addresses the issues you pointed out.\n\n> \"Map PH seems to be the entire architecture of Fig. 1, but the definition is unclear and the caption of Fig. 1 is sometimes described as PersLay's architecture, making it difficult to grasp.\"\n\nApologies for the confusion. We have modified the caption to clarify that it depicts PersLay followed by multilayer perceptrons, now called PersLay Classifier (PC).\n\n---\nThank you for your constructive feedback. The quality of presentation has significantly improved based on your comments. We hope that our answers have sufficiently addressed your concerns, and will appreciate if you consider increasing your score. If you need any further clarification, please let us know."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7697/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700190049925,
                "cdate": 1700190049925,
                "tmdate": 1700190049925,
                "mdate": 1700190049925,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cB2JoYLx0K",
                "forum": "FAY6ORIvn5",
                "replyto": "pEL448vf4s",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7697/Reviewer_qwyi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7697/Reviewer_qwyi"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your revision and clarifications.\nI am aware from the outset that PersLay encompasses a lot of filtration and vectorization, and my concern was that ReLu and DNN were fixed. I understood this point to be arguing that generalization is only generalization as a classifier, and by showing this with respect to ReLu+DNN, the most classical method, it is possible to show that vectorization using TDA retains generalization ability, i.e., if you choose a good classifier, you can show generalization ability. I would like to raise my score by one as the writing is much improved. However, since the theorem itself is limited to fixed classifiers, the scope of this contribution is still narrow and its impact on many users of ML with TDA has not yet been felt. I feel further research on this point is needed."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7697/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700404341454,
                "cdate": 1700404341454,
                "tmdate": 1700404341454,
                "mdate": 1700404341454,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0QxsDPuRG9",
                "forum": "FAY6ORIvn5",
                "replyto": "Fu4MS3A2Z0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7697/Reviewer_qwyi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7697/Reviewer_qwyi"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your comment. Yes, I agree that this theoretical result is first step in the right direction. In this regard, I have a positive impression. I think it is a question of what level of quality is required as an ICLR accepted paper. This is not something that is clear-cut, though.Since this is a relative discussion, I would like to take into account the opinions of other reviewers, including AC."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7697/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700407748671,
                "cdate": 1700407748671,
                "tmdate": 1700407748671,
                "mdate": 1700407748671,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "97BhvF5685",
                "forum": "FAY6ORIvn5",
                "replyto": "6pvP3kFy9A",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7697/Reviewer_qwyi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7697/Reviewer_qwyi"
                ],
                "content": {
                    "comment": {
                        "value": "I think the discussion on ReLu further enhances the value of the proposed methodology. Since the discussion was not available at the time of submission, I leave it to the AC to decide whether to add this point to the evaluation. Generalization performance depends on the classifier, but I believe that if a simple linear classifier can be shown, it can be regarded as generalization performance at the feature level."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7697/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700548293454,
                "cdate": 1700548293454,
                "tmdate": 1700548293454,
                "mdate": 1700548293454,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "92c703mkv4",
                "forum": "FAY6ORIvn5",
                "replyto": "O4sA41I1Mk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7697/Reviewer_qwyi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7697/Reviewer_qwyi"
                ],
                "content": {
                    "comment": {
                        "value": "This paper needs to revise its claim points and revise its writing, but I would like to raise my score in hopes of improvement."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7697/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700657989576,
                "cdate": 1700657989576,
                "tmdate": 1700657989576,
                "mdate": 1700657989576,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ci6ChMngq7",
            "forum": "FAY6ORIvn5",
            "replyto": "FAY6ORIvn5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7697/Reviewer_1CoA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7697/Reviewer_1CoA"
            ],
            "content": {
                "summary": {
                    "value": "This paper analyzes the generalization power of PersLay and derives new generalization bound. In addition, the paper discusses a VC-dim lower bound for persistent homology (PH) in terms of the WL-test on graphs. Experimental results demonstrate that the theoretical bounds can well capture the trend observed in the empirical generalization gap."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Perslay is an excellent model in TDA and it boosts the incorporation of PH with GNNs. Due to its effectiveness, Perslay has no theoretical guarantees. This paper provides new insights about the generalization of Perslay and provides new upper bound.\n2. This paper extends the expressive power of PH in terms of WL to get a lower bound regarding the generalization ability of PH."
                },
                "weaknesses": {
                    "value": "1. 1.The proofs are based on some assumptions, e.g. the filtrations of PH are fixed. However, many recent works [1][2] are based on flexible filtration function when using Perslay. Perslay itself is a powerful tool to vectorize persistence diagrams (PD) and provides informative representations. It can be plugged into many other models when using PH, and these models already have strong generalization power, such as GNNs. Therefore, analyzing the bound of Perslay or PH may not be necessary.\n2. This paper merely investigated the generalization one vectorization tool of PD, i.e. Perslay, thus having limited contribution. Researchers who are interested in the generalization of PH on graphs may be more interested in other representations of PD, such as persistence images and deep sets [3], and in models with flexible filtrations [2].\n\n[1] Hofer Christoph, et al. \"Graph filtration learning.\"\u00a0ICML 2020.\n\n[2] Horn Max, et al. \"Topological graph neural networks.\"\u00a0ICLR 2022.\n\n[3] Manzil Zaheer, et al. \u201cDeep sets\u201d, NIPS 2017."
                },
                "questions": {
                    "value": "Please refer to the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7697/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7697/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7697/Reviewer_1CoA"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7697/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698843002008,
            "cdate": 1698843002008,
            "tmdate": 1700646436245,
            "mdate": 1700646436245,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PShDU9Cku4",
                "forum": "FAY6ORIvn5",
                "replyto": "ci6ChMngq7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7697/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7697/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your feedback. We reply to your comments/questions below.\n\n> \"The proofs are based on some assumptions, e.g. the filtrations of PH are fixed. However, many recent works [1][2] are based on flexible filtration function when using Perslay.\"\n\n**Our VC-dimension result does not make any assumptions and holds for all PH methods.** Please note that our VC-dimension analysis (section 3.1) is not limited to fixed filtration functions. In particular, our analysis establishes that enhancing expressivity of PH methods comes at the expense of generalization. Importantly, this result also connects the generalization of PH via VC-dimension to expressivity in terms of the WL hierarchy, which is the primary tool for analyzing and designing new GNN architectures. \n\n**Fixed filtration functions dominate the PH/ML literature.** The widespread use of learnable function is a relatively recent phenomenon in PH-based ML, and usually runs orders of magnitude slower compared to non-learnable ones. Arguably, applying non-learnable functions still represents the mainstream approach in TDA.\n\n**Some works have explicitly advocated for fixed filtration functions (with learnable vectorizations) over learnable filtrations.** Filtration functions can come in different flavors; for instance, they can rely on node degree [1], cliques [2], or node attributes [3]. Some of the popular options are parameter-free. Also, while some works showed gains using learnable filtrations [4], others have reported no benefits and adopted fixed functions instead [5,6]. There is still no consensus about the significance of the gains associated with learnable filtration in many application.\n\n[1] Deep learning with topological signatures. NeurIPS 2017.\n\n[2] Networks and cycles: A persistent homology approach to complex networks. ECCS 2013.\n \n[3] Going beyond persistent homology using persistent homology. NeurIPS 2023.\n\n[4] Topological GNNs. ICLR 2022.\n\n[5] PersLay. AISTATS 2020.\n\n[6] Improving Self-supervised Molecular Representation Learning using Persistent Homology. NeurIPS 2023.\n\n> \"It can be plugged into many other models when using PH, and these models already have strong generalization power, such as GNNs. Therefore, analyzing the bound of Perslay or PH may not be necessary.\"\n\n**Generalization guarantees of GNNs are not preserved when PH is incorporated.** While PH has been tightly integrated into graph base models (e.g., GNNs), this integration does not preserve the generalization capabilities of the base models. In particular, we know that the combination of GNNs and PH can distinguish a strictly larger class of graphs compared to GNNs alone. Therefore, we expect the VC-Dimension of GNN+PH to be greater than that of GNNs, and hence, significantly affecting the generalization guarantees of GNNs.\n\n**Understanding generalization of PH is essential to inform design choices (e.g., network architecture).** For instance, a key difference between generalization of PersLay and FNNs/GNNs is the dependence on the model width and the spectral norm of the model weights. Specifically, PersLay has slightly worse dependence in terms of these quantities; however, this gap can be regulated by modulating the dimensionality of the embedding of the persistence diagram before treating it with linear layers. Such important design choices are informed by our analyses.\n\n> \"This paper merely investigated the generalization one vectorization tool of PD, i.e. Perslay, thus having limited contribution. Researchers [...] may be more interested in other representations of PD, such as persistence images and deep sets [3]\"\n\n**Our analysis subsumes persistence images and deep sets.** We note that the vectorization methods introduced by PersLay are rather general and subsume the suggested persistence images (PI) and also DeepSet-based strategies. Thus, our analysis covers most of the vectorization schemes broadly used in the literature.\n\n**Our work lays a strong foundation for analyzing learnable filtrations.** One way to analyze PH with learnable filtration schemes could be to get upper bounds on perturbation of outputs in terms of the filtration function parameters. This would additionally require an analysis of Wasserstein distances between persistence diagrams obtained with different parameters. We believe that for a specific class of graphs we can get modified upper bounds for perturbation with respect to filtration function parameters that would depend on Wasserstein distance of same order. This additional analysis could be readily integrated into our framework to get generalization bounds for learnable filtrations.\n\n---\n\nThank you for your feedback. We believe that your comments have served to better position the importance of our contributions, including the significance of our results and the versatility of our analysis. We hope that our answers have sufficiently addressed your concerns, and would be grateful if they translate into an improved assessment of our work."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7697/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700189088832,
                "cdate": 1700189088832,
                "tmdate": 1700189088832,
                "mdate": 1700189088832,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GHSDZcRu8D",
                "forum": "FAY6ORIvn5",
                "replyto": "PShDU9Cku4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7697/Reviewer_1CoA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7697/Reviewer_1CoA"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. I have looked into other comments as well.\nIn general, my concerns are well addressed and I would like to raise my score.\nVery interesting paper."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7697/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700646401136,
                "cdate": 1700646401136,
                "tmdate": 1700646401136,
                "mdate": 1700646401136,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]