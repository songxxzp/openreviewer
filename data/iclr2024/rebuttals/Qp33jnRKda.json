[
    {
        "title": "Growing Tiny Networks: Spotting Expressivity Bottlenecks and Fixing Them Optimally"
    },
    {
        "review": {
            "id": "dCIuXUmDed",
            "forum": "Qp33jnRKda",
            "replyto": "Qp33jnRKda",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2716/Reviewer_qBe2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2716/Reviewer_qBe2"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a method to augment network architectures by finding layers with an \"expressivity bottlenecks\" and widening the network there. \n\nSpecifically, they calculate the functional derivative $v_\\text{goal} = - \\nabla_{u = f(x)} \\mathcal{L}(u)$ characterizing the best infinitesimal variation of the outputs of $f$ to decrease the loss at $x$. The derivative $v_\\text{goal}$ is then compared with its projection on the tangent space $T_\\mathcal{A}^{f_\\theta}$ of the manifold $F_\\mathcal{A}$ (networks with architecture $\\mathcal{A}$) at the point $f_\\theta$. The norm of the difference between these two directions is used to quantify the expressivity bottleneck. This gets also generalized not just for the logits $u$ but for all pre-activation values $a_l$. \n\nThe authors continue by providing a procedure how one calculates the best variation of the parameters for one layer $\\delta W_l^*$ (Proposition 3.1) in order to calculate the expressivity gap $\\Psi^l$, as well as a procedure to add neurons and initialize them optimally (Proposition 3.2). A series of Propositions (4.1 - 4.3) follow, shining light on the properties regarding the greediness of the approach. \n\nThe proposed approach is evaluated on the CIFAR-10 dataset. The authors start with an architecture consisting of two blocks of 2 convolutions and 2 MaxPooling each followed by two fully-connected layers using selu activation. The proposed method outperforms GradMax. The authors attribute this to the redundancy of GradMax. \n\nMinor comments:\n- in 2.2, paragraph \"Optimal move direction\" should $\\Theta$ be $\\Theta_\\mathcal{A}$? \n- Proof for 3.2 in the appendix: the first sentence seems incomplete"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- The idea is interesting\n- The approach appears to be sound, although the reviewer could not verify the proofs (maybe due to some misunderstandings, see more in the questions). \n- Overall well written, although the technical reasoning should be improved.\n- Mentioned Limitations where insightful.\n- Helpful Appendix."
                },
                "weaknesses": {
                    "value": "- The limitations regarding more complex datasets remains unclear. \n- There don't seem to be many experiments: How does the method perform on other seed architectures? \n- The math, especially the proofs should be more detailed."
                },
                "questions": {
                    "value": "In no particular order: \n\n- How does the approach compare to NEAT based techniques? What are differences / communalities? \n\n- How well would your approach work on datasets with lower signal to noise ratio compared to CIFAR-10? Would you expect to see overfitting? \n- How do you terminate the training procedure? Is there some schedule according to which you pick which layer gets widened? Do you pick the layer with the largest (normalized) expressivity gap? \n- what exactly do they improve?\n- Did you consider combination of standard gradient descent and your proposed method? How would they work out? \n\n- I could not find a formal definition for $\\partial / \\partial t$ in Section 2.2\n- Proposition 3.1 (Appendix):\n\t- What happens in the step where after \"$M^+$, we get:\"? To me it looks like you substituted $\\delta W_l$ with $\\delta W_l^*$ in $V_\\text{goal}^l B_{l-1}^T = \\delta W_l B_{l-1} B_{l-1}^T$, as this is where the gradient of $g(\\delta W_l)$ vanishes, and then multiplied $\\tfrac{1}{n} (B_{l-1} B_{l-1}^T)^+$ from the right. However, i am missing the reasoning why $B_{l-1} B_{l-1}^T \\tfrac{1}{n} (B_{l-1} B_{l-1}^T)^+ = I$. \n- Proposition 3.2: \n\t- As $S$ is just positive semi-definite ($S := \\tfrac{1}{n} B_{l-2} B_{l-2}^T$) and not necessarily positive definite (i.e. may not have full rank), how do we know that $S^{-1/2}$ exists in Proposition 3.2? \n\t- Of which matrix are $\\lambda_k$ the Eigenvalues? Currently i see that they are the singular values of $S^{-1/2} N$. \nOverall, especially Proposition 3.1 and 3.2 would really benefit from detailed explanations. \n\n- Does you method scale to ImageNet?\n- Did you compare the three different initialization approaches? Random initialization, zero initialization and your in Proposition 3.2 proposed initialization?  \n- Which hardware did you use to run your experiments?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2716/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698748455684,
            "cdate": 1698748455684,
            "tmdate": 1699636214031,
            "mdate": 1699636214031,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Qsun0xPzTQ",
                "forum": "Qp33jnRKda",
                "replyto": "dCIuXUmDed",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2716/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2716/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "$\\newcommand{\\mM}{\\mathbf{\\delta W}}$\n$\\newcommand{\\Ir}{\\begin{pmatrix} I_r & 0 \\\\\n                0 & 0\\end{pmatrix}}$\n\nThank you for your review and your good appreciation of our paper.\n\nTo answer the questions:\n> How does the approach compare to NEAT based techniques? What are differences / communalities?\n\nBoth approaches have the similar goals: the optimization of the architecture of a neural network, for a given machine learning task. However they proceed very differently. NEAT is based on evolutionary techniques, with a mix of exploration (random architecture changes), selection and cross-over. On the opposite we are driven by the gradient of the task loss, which enables us to directly know which part of the architecture we should change, and how (i.e. where to add neurons, and which ones). While NEAT needs to perform many random tries (and retrain each generated architecture) to find good moves, we exploit the information from the backpropagation to directly apply the optimal move. However our moves are optimal in a first order derivative meaning, and computing these moves is costly, while NEAT estimates the real impact of each tested move (but this comes at a computational cost as well, and arguably a higher one). We expect that in a near future, neural architecture optimization techniques involving both types of approaches will arise.\n\n> How well would your approach work on datasets with lower signal to noise ratio compared to CIFAR-10? Would you expect to see overfitting?\n\nDefinitely, yes, if the user forgot to add a regularizer to the loss to optimize. We suppose that the user provides a loss that they actually want to optimize. This is not the case though in current practice of neural networks, as architectures are selected for known good training biases, and as batteries of indirect regularizers are used (early-stopping, drop-out, batch-norm, data augmentation, etc.). In case the user forgot to add a regularizer to the loss, we could still use these indirect regularizers to fight against overfit (except for early-stopping). We also have other ways to detect of prevent overfitting, as we can estimate the statistical significance of the candidate neurons to add. If the eigenvalue associated to the candidate neuron is too low, the performance gain provided by that neuron is likely to be due to spurious correlations, so we skip it. We detail this in Appendices D.1 and D.3. We actually plan to work further on the design of statistical tests.\n\n> _How do you terminate the training procedure? Is there some schedule according to which you pick which layer gets widened? Do you pick the layer with the largest (normalized) expressivity gap?_\n\nThere are several possibilities to select the best layer to widen:\na) as you suggest, compute the (normalized) expressivity gap for each layer and pick the largest one;\nb) compute also for each layer the best neurons to add, and compare the associated eigenvalues accross layers (as these eigenvalues indicate the first order variation of the loss when adding such neurons). Such a comparison might need special normalization though.\nc) for each layer, also perform line-search to find the best way to add the neurons, and then compare the real loss gains.\nAs b) and c) have small computational cost compared to a), the last option is the most interesting, as it is the most reliable (being based on the real loss). This is what we have done for training the ResNet-18 architecture on CIFAR-100.\n\nRegarding termination, we stop when both conditions are met:\n1) classical gradient descent is not improving performance anymore\n2) no neuron is added anymore, that is, there is no way to add a neural anywhere to the architecture that would improve the loss.\n\n> what exactly do they improve?\n\nCould you please detail your question more?\n    \n> _Did you consider combination of standard gradient descent and your proposed method? How would they work out?_\n\nYes, this is what we do in practice, for instance with the new experiment on CIFAR-100: after each neuron addition we classically train the network for a certain time. This dramatically improves the results, compared to just adding neurons. However we noticed with a previous implementation that in some cases this would come with unstabilities due to the fact that different parts of the network might need different learning rates. These unstabilities have been tempered by better distributing the amplitude factor on added neurons: we now consider $(\\sqrt{\\varepsilon} \\alpha, \\sqrt{\\varepsilon} \\omega)$ instead of $(\\alpha, \\varepsilon \\omega)$, which makes future gradients more homogeneous."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2716/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700703466428,
                "cdate": 1700703466428,
                "tmdate": 1700703466428,
                "mdate": 1700703466428,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HexVZYJYDn",
            "forum": "Qp33jnRKda",
            "replyto": "Qp33jnRKda",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2716/Reviewer_vNAT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2716/Reviewer_vNAT"
            ],
            "content": {
                "summary": {
                    "value": "TINY is proposed to grow neural network architectures with the aim to remove expressivity bottlenecks. The authors propose a scheme to increase the width of a considered feed-forward neural network architecture (with either fully-connected or convolutional layers) by adding neurons and thus increasing the width of the network during the growth process. (No dynamic addition of layers or other modules is considered.) \nThe proposed method is similar to GradMax but tries to avoid adding redundant neutrons."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The authors work on a timely problem and try to reduce the computational requirements of deep learning by growing relatively small neural networks rather than training large ones from scratch.\n- The authors aim to reduce redundancy in the addition of neurons when neural networks are grown.\n- The proposal is theoretically motivated based on potential optimal additions of neurons in function space.\n- The experiments show improvements in test accuracy over GradMax on CIFAR10."
                },
                "weaknesses": {
                    "value": "- The exposition lacks a related literature discussion. While the introduction mentions different lines of research, it mostly focuses on early works in the related directions. Only Section 3 mentions a few related works on neural architecture growth and redundancy, which are easy to miss in the middle of the paper. As a result, an overview of the state of the art is missing.\n- A similar criticism also holds for the experiment section, which only compares with GradMax and not different types of approaches.\n- It is impossible to deduce from Section 3 what the actual algorithmic proposal is. The links in the algorithm to the supplement are broken. (The actual description is on page 15.) The actual update equations are not provided in the main paper. \n- Limitation: It appears that the number of neutrons that are added in each step is a hyper-parameter.\n- The update seems to involve a spectral decomposition to avoid neuron redundancy that is computationally costly. \n- The computational complexity of the full training process (including the network growth) has not been evaluated, even though it forms in integral part of the claimed contributions.\n- The method is not very flexible in adding layers or different kind of modules. It only grows the width of a chosen architecture.\n- The novelty of the method appears to be limited in comparison with GradMax.\n- Experiments are limited to CIFAR-10, a relatively small dataset of low complexity. \n(Note that GradMax was evaluated also on CIFAR-100 and ImageNet.)\n- The performance of the learned models on CIFAR-10 lacks far behind the test accuracy that can be achieved on this dataset with standard, relatively small ResNet architectures (like ResNet18).\n\n\nMinor points:\n- Broken figure link in Section 3 on page 4.\n- The supplement is not included in the main paper so that important links are broken (see algorithm, for instance)."
                },
                "questions": {
                    "value": "- How does the proposed method perform on CIFAR100 and ImageNet?\n- How is the matrix N on page 15 computed, since it depends on the (unknown?) $V_{goal}$?\n- What is the runtime complexity of an update step?\n- How do the computational requirements compare with training just a wider model from scratch once?\n- How does training the obtained end neural network from scratch compare to the proposed training + growing process? Is a real improvement in generalization performance observed?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2716/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698758089629,
            "cdate": 1698758089629,
            "tmdate": 1699636213917,
            "mdate": 1699636213917,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "y8i8fKebs3",
                "forum": "Qp33jnRKda",
                "replyto": "HexVZYJYDn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2716/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2716/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your detailed review and remarks that we answer below.\n\n### Answers to questions\n\n> How does the proposed method perform on CIFAR100 and ImageNet?\n\nWe explain in the common reply [\"About experiments and architectures\"](https://openreview.net/forum?id=Qp33jnRKda&noteId=bimw8pDivO) why we have not performed such tests, and that we are currently running them (on CIFAR-100 with a ResNet-style architecture), to show that our approach can scale to larger architectures and larger datasets. However we do not hope for competitive accuracy with the state-of-the-art as our current implementation does not include yet batch-norm, drop-out or data augmentation, which are needed to reach good scores.\n\n> How is the matrix N on page 15 computed, since it depends on the (unknown?) $V_{goal}$?\n\n$V_{goal}$ is actually known and easy to compute. We have rewritten the paper to make this more explicit (beginning of Section 2.2). The quantity $v_{goal}(x)$ is actually what the first step of the usual backpropagation computes, when the network has been shown sample $x$. So we get this quantity for free. We have added a detailed explanation in Appendix A.1 to show this property.\n\n> What is the runtime complexity of an update step?\n\nIn practice, running our approach takes about the same time as training from scratch with standard gradient descent the final architecture we find.\n\nWe detail the theoretical complexity in the common reply [\"Computational complexity\"](https://openreview.net/forum?id=Qp33jnRKda&noteId=wMiGwiU7Bs). In particular, we show that the computational cost of the SVD is neglectible relatively to standard gradient descent. This is because the matrices we apply SVD to are very small compared to other computations done to run the network (their size is the number of channels in a layer, and not  minibatch size, neither image size).\n\n> How do the computational requirements compare with training just a wider model from scratch once?\n\nThis comparison is performed in Appendix E.1.2 COMPARISON WITH BIG MODEL. The left figure shows the accuracy as a function of time, while middle figure plots the accuracy as a function of gradient step. We easily see a persistent gap between the two curves on the left plot. While we achieve convergence around 90% accuracy on the test after only 15 seconds, the large models do not reach this performance even after 250 seconds of training.\n\n\n\n> How does training the obtained end neural network from scratch compare to the proposed training + growing process? Is a real improvement in generalization performance observed?\n\nThis comparison is done in Appendix E.1.3 COMPARISON WITH THE SAME STRUCTURE RETRAINED FROM SCRATCH, where after obtaining an architecture with our method, we compare its performance with the same model randomly intialized and retrained from scratch. Regarding the left plot, which is a zoom of the final performances, the difference of performance is not significant (except for one model where the gap is 0.006 point of accuracy at our disadvantage). The take-away message is that we are able to obtain the same accuracy, in a single run with growing architecture, than the classical gradient descent run using an oracle to tell the best architecture, and this with similar training time. Consequently, we get architecture width tuning for free.\n\n### Answers to other remarks\n\n> The method is not very flexible in adding layers or different kind of modules. It only grows the width of a chosen architecture.\n\nOur approach naturally extends to the addition of new layers (or nodes) in general computational graphs (any differentiable DAG), as it turns out that layer creation is the same as adding neurons to an empty layer. We discuss it briefly in the last paragraph of [\"About experiments and architectures\"](https://openreview.net/forum?id=Qp33jnRKda&noteId=bimw8pDivO).\n\n> Limitation: It appears that the number of neutrons that are added in each step is a hyper-parameter.\n\nAs we discuss it in the common reply [\"Neuron addition strategy\"](https://openreview.net/forum?id=Qp33jnRKda&noteId=aJJH3Fi8jC), one can conceive neuron addition strategies that do really get rid of such hyper-parameters. In the paper we chose on purpose a very simple experimental setting (with fixed number of neuron additions) just to compare the quality of neurons added when using our approach or GradMax.\n\n> [About presentation]\n\nAs advised, we moved the Related Works paragraphs from Section 3 to the rest of the state of the art in the main introduction. We have put the appendix back to the end of the main paper, for links to work again. We have added a high-level description of the algorithm in the main paper, and we are adding the description of the sub-routines in the Appendix, with detailed complexity.\n\nWe hope we have answered your questions as well as adressed the weaknesses."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2716/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700616471796,
                "cdate": 1700616471796,
                "tmdate": 1700616471796,
                "mdate": 1700616471796,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LGiUx5YbB5",
                "forum": "Qp33jnRKda",
                "replyto": "y8i8fKebs3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2716/Reviewer_vNAT"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2716/Reviewer_vNAT"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their detailed response. At this point, I do not have any further questions.\n\nThe project does not seem to be at a completed and fully polished state, yet, as multiple experiments are still running and a neuron addition strategy is not worked out yet. For that reason, I keep my score for now, but would be open to raising it during a discussion with the other reviewers."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2716/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700692720844,
                "cdate": 1700692720844,
                "tmdate": 1700692720844,
                "mdate": 1700692720844,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8o5Nbl5adM",
            "forum": "Qp33jnRKda",
            "replyto": "Qp33jnRKda",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2716/Reviewer_XQVK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2716/Reviewer_XQVK"
            ],
            "content": {
                "summary": {
                    "value": "This paper shows how to grow tiny networks by leveraging the functional gradient to optimize the network architecture on the fly. They define the expressivity bottlenecks by the distance between the desired activity update and the reachable update from the current parameter space. And they greedy reduce the expressivity bottlenecks during training when neurons are added."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The problem is interesting and well-defined mathematically. Theoretically, they show how to solve the problem and have solid propositions and proofs, although I did not follow most of them. Empirically they compared their method to the previous method showing that they achieve better accuracy on cifar10 when growing from a tiny model."
                },
                "weaknesses": {
                    "value": "The paper is not easy to follow and there are a lot of typos in the paper, i.e., missing figure number in section 3, no caption for the algorithm. I do think we should have a main algorithm that describes the whole training process, like how function gradients are calculated and how the optimization problem is solved according to which proposition. Empirical results seems very limited even compared to the baseline methods, such as gradmax."
                },
                "questions": {
                    "value": "How do we add neurons to the convolutional layers? Are we structurally adding kernels or adding neurons treating them as fully connected layers?\n\nWhat are the benefits of the proposed method? Are we trying to have a method that tries to get the best model among a certain size or a method that can efficiently and effectively grow a tiny network to an arbitrary size? If it is the latter one, can we have some experiments with models that people use in practice?\n\nWhat is the computational cost of the proposed methods?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2716/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698804016870,
            "cdate": 1698804016870,
            "tmdate": 1699636213846,
            "mdate": 1699636213846,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Hu3V4HZtFA",
                "forum": "Qp33jnRKda",
                "replyto": "8o5Nbl5adM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2716/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2716/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your review and questions.\n\n### About presentation\nWe are very sorry for the typos, we hope we have corrected all of them in the revision, which we introduce in the common reply [\"New version of the paper, with improved clarity\"](https://openreview.net/forum?id=Qp33jnRKda&noteId=7ZWDNGZNl9). We have added a description of the main algorithm and are adding details of subroutines in an appendix. More generally we have reworked the paper to make it more clear and easier to follow.\n\nCalculating function gradients is actually very easy, it is part of what the standard backpropagation computes. We have added details in Section 2 and in Appendix A.1 to show this theoretically. We will also release the code (open-source). Technically one just has to call usual backpropagation and intercept an auxiliary computation, for instance with a [\"hook\"](https://pytorch.org/docs/stable/generated/torch.Tensor.register_hook.html) in the desired layer.\n\n\n> How do we add neurons to the convolutional layers? Are we structurally adding kernels or adding neurons treating them as fully connected layers?\n\nAdding one neuron to a convolutional layer $l$ consists in adding one kernel to layer \ud835\udc59 and consequently also one dimension to each kernel at layer $l+1$. We added a scheme, in dedicated Appendix A.6, to make this more clear.\n\n> What are the benefits of the proposed method? \n> Are we trying to have a method that tries to get the best model among a certain size or a method that can efficiently and effectively grow a tiny network to an arbitrary size? If it is the latter one, can we have some experiments with models that people use in practice?\n\nOur goal is to build models of the right size by iteratively growing the architecture, starting from a tiny model, instead of training large models and pruning them.\n\nSo we are interested in minimizing architecture size for a given target accuracy, or, equivalently, maximizing accuracy for a given size. We actually jointly optimize both size and accuracy. \n\nFor this we also need indeed to be able to grow models to potentially arbitrary-large sizes, in case the task at hand requires it. We are right now training a ResNet-18 model (with initially very few neurons per block), on CIFAR-100, to check hows the architecture grows (cf common reply [\"Experiments and architectures\"](https://openreview.net/forum?id=Qp33jnRKda&noteId=bimw8pDivO)).\n\n\n\n> What is the computational cost of the proposed methods?\n\nWe have addressed computational complexity in the common reply [\"Computational complexity\"](https://openreview.net/forum?id=Qp33jnRKda&noteId=wMiGwiU7Bs)."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2716/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700627464451,
                "cdate": 1700627464451,
                "tmdate": 1700627464451,
                "mdate": 1700627464451,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2K65enSvPC",
            "forum": "Qp33jnRKda",
            "replyto": "Qp33jnRKda",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2716/Reviewer_7xtb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2716/Reviewer_7xtb"
            ],
            "content": {
                "summary": {
                    "value": "The submission presents a novel method to increase the width of a network during optimization, inspired from a functional argument. The method starts from the gradient of the loss wrt the output of the network, and finds weights by trying to align the output to this desired change."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The method is novel and the problem of fitting both the weights and the architecture at the same time is relevant and very much open. I have heard the idea of \"let's do gradient descent on the architecture\" multiple time, but little in the way of actual attempts to define what is meant by that statement, which is welcome."
                },
                "weaknesses": {
                    "value": "$\\newcommand{\\R}{\\mathbb{R}}$$\\newcommand{\\L}{\\mathcal{L}}$\n\nThe paper presents an interesting idea and I am generally positive towards it, but I found it hard to get the intended message. I think it would greatly benefit from an update to improve the clarity of the message, especially on the following points\n- The presentation of the functional analysis viewpoint. I found it hard to follow, probably due to notation overload.\n- The submission seems to not directly address how to trade-off increasing number of parameters vs. updating the parameters we already have.\n- Given that a stated contribution of the submission is a definition of optimality, what is meant by that should be stated explicitly.\n- Some statements should be made more carefully to avoid overly general claims.\n\nI give more details and specific examples of each of the points below. I will increase my score if these points are adressed by a revision during the discussion period.\n\nAs my issues have to do with clarity, I tried giving specific and clear descriptions, leading to a possibly (overly long) review. The length of the section below should not be taken as a negative assesment of the submission. My hopes is that those can help the authors make the message of the paper clearer. \n\n---\n\n## Details\n\n**Clarity of the functional view**\n\nI might be missing some key background reference, but I struggle to follow section 2.2. My understanding of the high-level idea is that $v_{\\text{goal}}(x)$ indicates the desired change in output of the network by indicating what infinitesimal change in the output of the network is desired. This goal reasonable and I wouldn't have an issue if it had been stated as such, but I don't understand how it follows from the functional perspective outlined in \u00a72.2. \n\nThe notation seems overloaded to represent the functional and the \"standard\" ML notation. The lack of distinction makes the text hard to parse. For example, the expression $\\nabla_{f}\\L(f)$ implies that $\\L$ takes a function, but in $\\nabla_{u=f(x)} L(u)$, it is evaluated at the output of the network, a vector in $\\R^p$.\n\nThe text also implies that $\\nabla_f \\L(f) = \\nabla_{u=f(x)} \\L(u)$\" by the definition and evaluation of $v_{\\text{goal}}$. Assuming my interpretation above is correct, this equivalence is not obvious to me. It would benefit from an explanation as to why it holds, or a reference. It is also unclear to me why this holds without defining the space of functions, for example whether $\\mathcal{F}$ is the class of function representable by any width-$M$ networks and taking the union over all $M$s, some RKHS, or whether we any arbitrary pathologic discontinuous functions is allowed.\n\n**Balancing optimization and adding parameters**\n\nThe last contribution states that the submission \"naturally obtain[s] a series of compromises between performance and number of neurons, in a single run, thus removing the need for width hyper-optimization\". I would expect this contribution to refer to a particular result highlighting how the proposed methods trades-off (a) fitting the current architecture/doing more traditional GD steps vs. (b) adding neurons. Unless I missed something, the proposed method does not inherently address this tradeoff, and instead adds a fixed number of neurons. This seems to be replacing the width hyperparameter by a \"how-much-width-to-add\" hyperparameter. The method can still be an improvement by lowering the dependency of the performance on the hyper-parameter, but should be discussed more directly in the main text.\n\n**Definition of optimality**\n  \nThe submission uses the term optimal in many instances with what appears to be different meanings. It is not clear what criteria is used to establish optimality. To take an example from optimization, gradient descent being optimal could refer to the fact that it is the result of minimizing a surrogate quadratic problem, or to say that it attains the best rate of convergence among first-order algorithms in some problem class.\n\nAs the goal of the submission is to \"mathematically define the best possible neurons\" and fixing expressivity bottlenecks \"optimally\", it would be beneficial to be explicit about what is meant by \"optimal\". Especially as the submission can be interpreted as proposing two definitions; one implied in \u00a72.2 as minimizing the distance between $v_{\\text{goal}}$ and the actual update, and another looking at the layers independently in 2.3 to make the problem tractable (especially as the submission states in \u00a73.3 that \"this move is sub-optimal\").\n\nFor example, \"picking optimal directions that avoid redundancy in the pre-activation space\" at the end of the submission seems to reflect that \"optimal\" is taken to mean the optimal direction to decrease the first-order approximation of the loss, a concept that is missing from other instances such as \"Optimal functional move\", \"The optimal update of the weights at a given layer\", or the optimality in Prop 3.2.\n  \n**Overly broad claims**\n\n- (Intro) \"This removes the optimization issues (local minima) that usually arise when considering thin architectures\"; \"remove optimization issues\" is too broad, and might imply that local minima are the only optimization problem. The contribution should state that it is possible to avoid some local minima (with a forward reference to the specific result in \u00a74), or specify that this result applies to 1-hidden-layer networks.\n- (\u00a73.2) \"[adding random neurons] would not yield any guarantee regarding the impact on the system loss\"; I read this sentence as implying that this is in contrast with the proposed method, which then should have a guarantee that adding the proposed neurons decreases the loss. As no such results are presented, the description should be changed.\n- Going into \u00a72.3, I interpreted the description of \"recursive\" as implying that some invariant would be maintained, and specifically that the resulting update wouldn't change. To avoid this confusion, I would suggest being explicit are the start of \u00a72.3 that what follows is a an approximation to what is desired in \u00a72.2, as this only spelled out in \u00a73.1.\n\n\n**Related work**\nThe description of prior work could be more detailled to help readers unfamiliar with the literature. For example, it is not clear how the description of Net2Net, AdaptNet and MorphNet (\"propose different strategies to explore possible variations of a given architecture\") differs from the approach proposed here. \n\nI was also surprised to not see a citation to the classical works of neuron boosting/incrementally learning a neural network one neuron at a time (For example, Bengio, Le Roux, Vincent, Delalleau, and Marcotte. Convex neural networks. 2006, or other references found in the GradMax paper of Evci et al.), which I think would be relevant for historical context.\n\nAlthough focused on optimization of a fixed architecture, there is a line work in optimization for deep learning that takes a constrained optimization/Lagrangian view to obtain per-layer updates that look similar to the recursion argument in \u00a72.3--\u00a73.1. The following works might be of interest to the authors if they were previously unaware of those. _(to be explicit; although I do believe there is a connection and that some discussion could be beneficial, I am not requesting that the submission cite those works)_\n- Lecun. A Theoretical Framework for Back-Propagation. Proceedings of the 1988 connectionist summer school \n- Carreira-Perpi\u00f1\u00e1n and Wang. Distributed optimization of deeply nested systems. AISTATS 2014.\n- Taylor, Burmester, Xu, Singh, Patel and Goldstein. Training Neural Networks Without Gradients: A Scalable ADMM Approach. ICML 2016.\n- Frerix, Mollenhoff, Moeller and Cremers. Proximal Backpropagation. ICLR 2018.\n- Amid, Anil and Warmuth: LocoProp: Enhancing BackProp via Local Loss Optimization, AISTATS 2022\n\n---\n\n**Minor points (no need for a response):**\n- \"Under standard weak assumptions (A.1)\" made me think I should look for a an \"Assumption 1\", as this style of reference is common. I'd suggest spelling out \"(see Appendix A.1)\".\n- (after Prop 4.3) \"by requiring the added neuron to have infinitely small input weights\"; a literal interpretation of this sentence requires the inputs to be 0. I suggest rephrasing in terms of \"direction\" instead.\n- What \"time\" in $\\frac{\\partial\\theta}{\\partial t}$ is not defined, \n- \"shown empirically to be better optimized than small ones Jacot et al. 2018\" seems to imply that this is what Jacot et al. shown. I assume the citation should have been moved earlier in the sentence, for the theoretical part.  \n- There are multiple instances where \\citet and \\citep are mixed, leading to missing parentheses around citations, especially in paragraphs discussing related works (\"Notions of expressivity\" paragraph)\n- The \"amplitude factor $\\gamma$\" used in the Algorithm given in Fig. 6 seems undefined in the main paper."
                },
                "questions": {
                    "value": "The specifics of my points above do not require a response and can instead be adressed through a revision, although I am open to a discussion if the authors disagree with my comments.\n\nFor specific questions;\n\n- **Clarity of the functional view**\n\n  Are the notation issues identified above correct, or did I completely miss something? If so, what is the formal definitions of the objects used, and why is the functional gradient the same as the derivative wrt the output of the network?\n  (Those questions be adressed by a revision to the paper and need not be written in openreview posts)\n\n- **Balancing optimization and adding parameters**\n\n  Shouldn't the functional view provide a way to perform this trade-off, for example through some regularization parameter that could impact how much better the \"adding new weights\" step should be vs. updating existing weights?\n\n- **Complexity of the methods** \n\n  The introduction claims that the method is is \"competitive\" in computational complexity with standard training. However, it seems that the methods requires the computation of SVDs of matrices whose size dependent on the width of the network, and the complexity should scale (at least) with that width squared, which is much more than gradient descent. Could the authors clarify what was meant?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2716/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699595393244,
            "cdate": 1699595393244,
            "tmdate": 1699636213784,
            "mdate": 1699636213784,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JsuLhHVwLD",
                "forum": "Qp33jnRKda",
                "replyto": "2K65enSvPC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2716/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2716/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you a lot for your review and the raised points, which incited us to rework some parts of the paper and to add important details to the appendix.\n\nIn particular:\n> Clarity of the functional view\n\nWe apologize for the notation overload. We have corrected this, notably by distinguishing the sample-loss $\\ell$ from the global loss $\\mathcal{L}$, and by correcting a few typos. \nAs stated in the common reply [\"New version of the paper, with improved clarity\"](https://openreview.net/forum?id=Qp33jnRKda&noteId=7ZWDNGZNl9), we have added Appendix A.1 to get the paper self-contained and to explain what precisely is the functional gradient, as well as how to obtain the property $${v_{goal}(x)} = -\\left( \\nabla_f \\mathcal{L}(f) \\right)(x) = -\\nabla_{u} \\ell(u, y(x))\\big|_{u = f(x)}$$\nwhich is not trivial indeed. We hope that with the new version of the paper we have answered all of your concerns regarding the mathematical soundness of the paper.\n\n> Balancing optimization and adding parameters\n\nThis is of course a very important question. We provide a detailed answer in the common reply [\"Neuron addition strategy\"](https://openreview.net/forum?id=Qp33jnRKda&noteId=aJJH3Fi8jC), that we will also add to the paper as an appendix. \n\nTo answer the remark that the method \"adds a fixed number of neurons\", note that the algorithm (described in \"Algorithm 1\", page 9) for that experiment is actually more complex. A fixed number of candidate neurons are proposed indeed, but two selection steps take place afterwards: 1) a selection by statistical relevance based on the singular values (detection of spurious correlations, to avoid overfit), 2) a selection by performance gain (only consider neurons that do improve the loss on a validation set). Thus the number of added neurons may vary.\n\nThe number of neuron candidates is fixed in that experiment in order to compare with GradMax, but one can instead select this number in an adaptive manner based on the singular values (which are linked to expected loss improvement as stated by Proposition 3.2). A simple threshold on singular values enables indeed to make layers grow only when needed / useful. In that respect architecture growth can be not scheduled in advance but happen only on demand.\n\n> Complexity of the methods \n\nWe have addressed this point in the common reply [\"Computational complexity\"](https://openreview.net/forum?id=Qp33jnRKda&noteId=wMiGwiU7Bs), which is being included in a dedicated appendix as well. TL;DR: SVD cost is actually low.\n\n> Definition of optimality\n\nThank you for noticing this. Indeed we were using the same word \"optimal\" for different meanings. We have added a paragraph in Section 2 to make this more clear, and this was the opportunity to better introduce the criterion considered in the next sections. A supplementary appendix (A.3) details these concepts and their intertwinement.\n\n> Misc.\n\nThank you for the minor points and noticing formulations leading to \"overly broad claims\", that we have addressed. \nRegarding  \"[adding random neurons] would not yield any guarantee regarding the impact on the system loss\" however, what we mean is that our method yields the best neuron to add to decrease the loss at first order (in a sense now well defined), while a random neuron does not have this property.\n\nThank you also for the references, which we did not know and are very interesting and indeed related to our work. We will discuss them, in an appendix section though, as space is too limited in the main paper, especially with the imposed bibliographic citation style. Speaking of which:\n\n> I have heard the idea of \"let's do gradient descent on the architecture\" multiple time,\n\nAre you referring to DARTS and to the Lottery ticket hypothesis, or to other papers? in which case, we would be delighted if you could provide references."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2716/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700623542456,
                "cdate": 1700623542456,
                "tmdate": 1700623542456,
                "mdate": 1700623542456,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZSgduxlEbT",
                "forum": "Qp33jnRKda",
                "replyto": "JsuLhHVwLD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2716/Reviewer_7xtb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2716/Reviewer_7xtb"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your response. \n\nGiven the short time-frame, I did not have time to go through the entirety of the response and paper update yet, but am leaving this message before the end of the discussion period.\n\n**Clarity of the functional view**  \nFrom a short look, \u00a72 looks much better, but will need more time to go through it again. \n\n**Balancing optimization and adding parameters**  \nThe response claims that a \"neuron addition strategy\" is out of scope, but I would argue that the last contribution (\"naturally obtain a series of compromises between performance and number of neurons, in a single run, thus removing the need for layer width hyper-optimization\") is putting \"how to add neurons\" in scope. My understanding of this point is as a claim that the proposed methodology provides a way to measure the benefit of adding neurons vs. further optimizing the current ones. If this is not the case, and the core contribution is limited in scope to \"how to add a neuron when we need one\", this point needs clarification.\n\nI understand that the submission has a more subtle selection procedure, but it seems ad-hoc and disconnected from the formalism proposed in the earlier sections. This issue might be addressed by making the contribution point more explicit.\n\n**Misc**\n\n> Thank you also for the references\n\nI probably missed some relevant papers, and would recommend a deeper dive if the authors are interested (in my view, this is not strictly necessary for the current submission). Per-layer updates show up regularly in different contexts, and the  connection between those methods are often not obvious, leading to a very disconnected citation graph.\n\n> Are you referring to DARTS and to the Lottery ticket hypothesis, or to other papers? \n\nI was referring to informal conversations with colleagues at conferences/workshops, where I've heard the idea floated at the level of \"it would be great _if_ we could GD on the architecture\". DARTS or the RL-based methods referenced in the submission are the closest match I know."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2716/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700705424709,
                "cdate": 1700705424709,
                "tmdate": 1700705424709,
                "mdate": 1700705424709,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]