[
    {
        "title": "Why are hyperbolic neural networks effective? A study on hierarchical representation capability"
    },
    {
        "review": {
            "id": "UmY7pCAb54",
            "forum": "uEYxHVQJF4",
            "replyto": "uEYxHVQJF4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9418/Reviewer_4wYm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9418/Reviewer_4wYm"
            ],
            "content": {
                "summary": {
                    "value": "The paper carefully examines the effectiveness of various Hyperbolic Neural Networks (HNNs) by measuring their Hierarchical Representation Capabilities (HRC), an evaluation process named as Hierarchical Representation Capability Benchmark (HRCB). The four metrics developed for HRCB include Root Node Hierarchy, Coordinate Origin Hierarchy, Parent Node Hierarchy, and Sibling Node Hierarchy, which altogether measure how well the hierarchical structure is embedded in hyperbolic space. The paper also proposes pre-training strategies upon improving a model's HRC, and empirically assess the relationship between HRC and downstream performance. Experimental results show that HNNs' HRC has a significant impact on downstream performance, and pre-training HNNs towards enhancing HRC can improve its performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- [S1] The motivation of looking back upon hyperbolic neural networks and closely assessing their effectiveness on downstream tasks with respect to theoretical work is very interesting, and I believe any insights would greatly benefit the geometric deep learning community.\n- [S2] The scope of experimentation is fairly comprehensive, covering various downstream tasks, hyperbolic manifold spaces, and neural network architectures."
                },
                "weaknesses": {
                    "value": "- [W1] **The overall contribution is not clear within the writing and experiments.** Is the paper hoping to show why HNNs are effective? or that HNNs do not achieve theoretically optimal embeddings?. From the experiments, it seems the goal of the paper is that 1) there exist a gap in HRC between existing HNNs and theoretically optimal embeddings and 2) reducing the gap via pre-training helps boost downstream performance, yet the overall writing (e.g., abstract and introduction) makes it confusing on what to expect from the experiments: reasons behind HNNs' effectiveness or limitations of HNNs. \n- [W2] **The presented empirical observations in the text are unclear and somewhat misleading.** For instance, page 7 discusses how the LR target for NC \"does not need to distinguish the position relation\" among nodes, yet overfitting on LR helps improve HRC, which seems counterintuitive. Why is this the case? Furthermore, page 8 mentions how \"within the applicable scope of HNNs, performance can be improved by enhancing HRC\", with the node classification task being \"out of scope\". This is misleading considering that many previous work have shown performance boosts in node classification by leveraging hyperbolic models [A, B, C].\n- [W3] **The figures showing Friedman test and Nemenyi post-hoc tests are extremely hard to read.** It would be better to categorize results based on what the authors are hoping to convey through the experiment: as an example, for Figure 6(a), if the main observation is that GD, HR, and FD help HNNs learn position relation unlike LR, it would be better to simply draw a bar chart (or multiple bar charts, one for each manifold) with targets on the x-axis and the HRC values on the y-axis. That way, we can visually observe the orderings currently written as text within the plots.\n- [W4] **Downstream performance results are missing exact scores and are only compared in terms of rankings.** For Subsection 5.2.3, it would be better to simply present the downstream results in exact scores using the scoring metrics for each downstream task (F1 score for node classification, mAP for graph reconstruction and so on). This way, we can concretely estimate how much better/worse each method performed compared to another, and whether the results are within reasonable range compared to existing literature.\n\n[A] Chami et al., Hyperbolic Graph Convolutional Neural Networks. (NeurIPS 2019)\\\n[B] Liu et al., Hyperbolic Graph Neural Networks. (NeurIPS 2019)\\\n[C] Zhu et al., Graph Geometry Interaction Learning. (NeurIPS 2020)"
                },
                "questions": {
                    "value": "- [Q1] The Coordinate Origin Hierarchy metric $M_o$ seems to assume that the root node is located near the origin, while this constraint is not made explicitly during training of HNNs. Considering that Combinatorial Construction [D], on the other hand, trivially satisfies this assumption, would you still consider using this metric to be valid for fair comparison?\n- [Q2] Is the L strategy described in the beginning of subsection 5.2.3 equivalent to adding a weighted auxiliary loss to the downstream predictive loss? This seems very similar to how the HGCN paper used a link prediction regularization objective for node classification [A]. Thus adding a few discussion on this connection and giving the strategy a proper name rather than just \"L\" could help towards better clarity.\n\n[D] Sala et al., Representation Tradeoffs for Hyperbolic Embeddings. (ICML 2018)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9418/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698485440761,
            "cdate": 1698485440761,
            "tmdate": 1699637186743,
            "mdate": 1699637186743,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "Bo29FE2fma",
            "forum": "uEYxHVQJF4",
            "replyto": "uEYxHVQJF4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9418/Reviewer_u1No"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9418/Reviewer_u1No"
            ],
            "content": {
                "summary": {
                    "value": "The paper elucidates the scope and applicability of HNNs through quantitative analysis of their HRC. It provides guidance on enhancing HNNs by identifying factors that improve their hierarchical representation capability. In particular,\n\n- This paper proposes a benchmark (HRCB) to quantitatively evaluate the hierarchical representation capability (HRC) of HNNs. HRCB includes metrics to assess horizontal relationships (sibling nodes) and vertical relationships (parent-child nodes) in a hierarchy.\n\n- Experiments using HRCB show that HNNs do not achieve the theoretical optimal embedding in hyperbolic space. Their HRC is significantly lower than combinatorial construction methods.\n\n- Analysis reveals two key factors influencing HNNs' HRC: (1) Optimization objectives that help distinguish positional relationships between nodes, and (2) Training data structured as a complete n-ary tree.\n\n- The paper proposes pre-training strategies to enhance HNNs' HRC based on these insights. Experiments show improved downstream task performance from enhanced HRC, validating the analysis."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The author made a very interesting observation and conducted numerous experiments to support their findings."
                },
                "weaknesses": {
                    "value": "There are several points of concern from me:\n\n- The HRCB metrics presented presume the root node is positioned at the highest point. Additionally, the authors assume that the root node should be close to the origin. This isn't accurate for all HNNs, as detailed in [1]. If these assumptions are invalid, the four evaluation criteria proposed might not be accurate.\n\n- The research primarily uses two datasets (Disease and Animal) for analysis. Including a broader range and real-world datasets would provide more robust conclusions.\n\n- The author's description of pre-training strategies isn't very clear. Could this be elaborated more?\n\n- The comparison is made with the GCN model, but the HGCN isn't considered. This is an omission.\n\n- While the findings are intriguing, there are various forms of HNN currently available, such as those based on the tangent space or being fully hyperbolic. The author doesn't seem to address this aspect.\n\n[1] Menglin Yang et al. Hyperbolic Representation Learning: Revisiting and Advancing. ICML 2023."
                },
                "questions": {
                    "value": "1. Could you provide more details on the data generation process for the hierarchical structures?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9418/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698780009684,
            "cdate": 1698780009684,
            "tmdate": 1699637186628,
            "mdate": 1699637186628,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "jF50jOGkid",
            "forum": "uEYxHVQJF4",
            "replyto": "uEYxHVQJF4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9418/Reviewer_pJty"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9418/Reviewer_pJty"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies a benchmark for evaluating the hierarchical representation capacity (HRC) of the hyperbolic neural network (HNN). The empirical study shows the HRC can be affected by the optimization objectvie and the training data. This observation facilitate to develop pre-training strategies to enhace the HRC of HNN, improving the learning capacity and performance of the neural network. This paper shows some interesting observations, but it lacks of the generalization of HNN to other tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1-This paper first study the hierarchical representation capacity (HRC) of the hyperbolic neural network (HNN) and aim to answer how the HNN works.\n2-A benchmark, including data and metric, is proposed to evaluate the HRC.\n3-Three pre-training methods are proposed to improve the learning capacity and the performance."
                },
                "weaknesses": {
                    "value": "1-This paper only studies the HRC in the graph dataset, the observation on text and graph data is missed.\n\n2-The experiments on pre-training method misses formulation of the losses. In addition, why the GD as objective function can attain good performance, please explain?"
                },
                "questions": {
                    "value": "1-This paper study the HRC for the graph data, there is another metric, called delta-hyperbolicity, can be used to evaluate the hierarchiy of the data, I want to see does the value of delta-hyperbolic matches the value of the metrics proposed in this paper.\n\n2-This paper only studies the HRC in the graph dataset, does the conclusion is also hold in text and image data?\n\n3-The proposed pre-training method should also be evaluated in other datasets."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9418/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9418/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9418/Reviewer_pJty"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9418/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699196839762,
            "cdate": 1699196839762,
            "tmdate": 1699637186524,
            "mdate": 1699637186524,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]