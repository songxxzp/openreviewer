[
    {
        "title": "Video Decomposition Prior: Editing Videos Layer by Layer"
    },
    {
        "review": {
            "id": "XRJRDJcAxW",
            "forum": "nfMyERXNru",
            "replyto": "nfMyERXNru",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2017/Reviewer_6Tks"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2017/Reviewer_6Tks"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new framework that decomposes a video into multiple multiple RGB layers and associated opacity levels for various video editing tasks.\nThe proposed framework uses two neural network modules, RGB-net and \u03b1-net, to predict RGB layers and opacity/transmission layers for each video frame. \nThese layers are then composited to reconstruct the input video or achieve the desired effects during optimization.\nThe paper conducted experiments on video relighting, dehazing, and VOS tasks and achieves superior results compared to some existing baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Writing is overall clear.\n- Various tasks are conducted and better performance w.r.t to some baseline is reported.\n- No pre-training is required (test-time optimization)."
                },
                "weaknesses": {
                    "value": "- This paper proposes to unify various downstream tasks by video decomposition. However, the formulation of the video relighting task in this paper uses one RGB layer to represent the enhanced video and an alpha-layer to represent gamma correction. This formulation does not have any meaning of \u201cdecomposition\u201d. On the other hand, this paper carefully designs different decomposition definitions and corresponding constraints for different downstream tasks, but these constraints and decomposition definitions seem to lack a unified and general formulation. Therefore, I felt such kind of \u201cunification\u201d seems too artificial and forced.\n\n- The proposed formulation is not new (or at least has very few novelty).\nOverall, the proposed formulation can be summarized as a \"data-term + prior\" approach, which is a rather common formulation in optimization-based approach for image / video synthesis tasks. \nUsing neural networks as an implicit prior for data term is not new (as also mentioned in the paper - \"These approaches have highlighted the importance of formulating a loss function,\ncombined with the optimization of neural network parameters\") and the proposed paper extends this idea into other tasks.\nYet, the exact formulation designed for each task is dedicated but common (e.g., gamma-correction curves [1] for relighting; alpha-blending [2,3] for video-segmentation, dark-channel prior for dehazing).\n\n- Missing baselines for UVOS tasks: The paper compares with both pre-trained methods and test-optimization methods. For pre-trained methods, there are massive VOS methods [4,5,6] that has far better performance than compared methods in this paper; For test-optimization methods, there are also other video decomposition methods that targets this task [2,3].\n\n[1] Zhang, Mohan, et al. \"RT-VENet: a convolutional network for real-time video enhancement.\" Proceedings of the 28th ACM International Conference on Multimedia. 2020.\n\n[2] Kasten, Yoni, et al. \"Layered neural atlases for consistent video editing.\" ACM Transactions on Graphics (TOG) 40.6 (2021): 1-12. \n\n[3] Gu, Zeqi, et al. \"Factormatte: Redefining video matting for re-composition tasks.\" ACM Transactions on Graphics (TOG) 42.4 (2023): 1-14.\n\n[4] Cheng, Ho Kei, Yu-Wing Tai, and Chi-Keung Tang. \"Rethinking space-time networks with improved memory coverage for efficient video object segmentation.\" Advances in Neural Information Processing Systems 34 (2021): 11781-11794.\n\n[5] Yan, Kun, et al. \"Two-shot Video Object Segmentation.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n[6] Cheng, Ho Kei, and Alexander G. Schwing. \"Xmem: Long-term video object segmentation with an atkinson-shiffrin memory model.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022."
                },
                "questions": {
                    "value": "- While the proposed method does not require pre-training, it needs to optimization on each input sequence which takes time. How long does it take for optimizing a video sequence?\n\n- Is there a common insight or general guidance on applying this framework to downstream tasks? For example, what if applying this framework to tasks like denoising or super resolution?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2017/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2017/Reviewer_6Tks",
                        "ICLR.cc/2024/Conference/Submission2017/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2017/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697893600036,
            "cdate": 1697893600036,
            "tmdate": 1700450877980,
            "mdate": 1700450877980,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "g8N80i1aXm",
                "forum": "nfMyERXNru",
                "replyto": "XRJRDJcAxW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2017/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2017/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for taking their time out of a busy schedule and evaluating our submission. We are glad that reviewer 6Tks found our paper well-written. We greatly appreciate that reviewer 6Tks finds the test-time optimization factor to be a strength of our approach. We are happy to note that the reviewer found our approach performance better across different tasks. Before we address the specific weaknesses and questions of the reviewer, **we highly recommend R-6Tks to please go through the webpage of video results**, which was part of the supplementary.\n\n\n**Relighting Decomposition:** We understand the reviewer\u2019s concerns regarding the formulation of the relighting task. We would like to emphasize that the decomposition of a frame may not involve only additive decomposition($\\alpha$-blending). What we explored in this work (one of our major novel contributions) is to perform multiplicative decomposition on frames of the video sequence. We derived one such multiplicative decomposition utilizing the gamma correction formulation for the frame relighting task. The interesting outcome of this decomposition is we are able to achieve SOTA performance on the relighting task without requiring any task-specific data. Not only that, we are also able to achieve high temporal coherence in the processed relit videos. Such qualitative comparison can be found in videos attached with the webpage(supp.). \n\n**Unified Framework:** We hope to change the reviewer\u2019s viewpoint about the VDP framework being artificial. We have taken great pains to figure out this method. \n\nThe core of our framework lies in its ability to adapt to different tasks while maintaining a consistent structure. Basically, each task consists of three integral components: prior knowledge (decomposition formulation), motion modeling (via $\\alpha$-Net), and appearance modeling (via RGBnet). The **consistent architecture** of $\\alpha$-Net and RGBnet establishes a **unified** way of extracting the motion and appearance features from a video, across different tasks. These intermediate motion and appearance features are then utilized with task-specific adaptations that are reflected in the decomposition formulations. Hence, the unifying factor across different tasks is the consistency in the architecture used to extract motion and appearance cues from the videos across all tasks.\n\n\n**Novelty Issues:** We claim novelty on two terms.  First, The relighting task formulation derived from gamma correction is novel for low-lit video enhancements and is unique to our work. Such formulation is not utilized in the work[1] mentioned as a reference by the reviewer. \n\nSecond, The VDP framework\u2019s way of modeling motion and appearance in a video is unique to our work(utilizing $\\alpha$-Net and RGBnet). Specifically, to our knowledge, no prior test time optimization technique has utilized FlowRGBs to model the motion of the video, and it is unique to our methodology. \n\nWith regards to the mentioned test-time methods[2,3], they are not at all designed for the VOS task but more for edit propagation tasks. These methods require per-frame masks obtained by image segmentation baselines. So, we do not see any overlap of these techniques with our framework apart from edit propagation tasks.\n\nWe also would like to emphasize that maintaining spatio-temporal coherence is a big issue when it comes to tasks like dehazing and relighting, as can be seen from the qualitative results provided in the webpage(supplementary material). Our novelty also stems from the fact that our method can provide high-fidelity results with far superior spatio-temporal coherency. On top of this, we are able to achieve SOTA results for video dehazing and relighting tasks and achieve SOTA among the test-time optimization techniques in UVOS tasks. We hope this clarification will alleviate the reviewer\u2019s concern about the novelty issues.\n\n**Missing VOS baselines:** In Table 2, we only compare with other test-time optimization baselines for a fair comparison as the setting of each methodology is exactly the same, i.e., no prior segmentation annotation is given to the model during training. We provide a comprehensive evaluation of the task UVOS task in Table 5 in the appendix because of the limited availability of space in the main paper. We will definitely add the remaining baselines[4,5,6] in there for a more comprehensive comparison. Additionally, if ICLR allows one extra page after acceptance, we would move those comparisons to the main paper.\n\n**Time taken:** We refer the reviewer to our appendix section C. \n\n**General Guidance:** We have explored three major downstream tasks like the VOS, dehazing, and relighting for videos. [1]* utilizes a test-time optimization technique for tasks like denoising, super-resolution, etc. Our framework can be naively extended to such tasks by utilizing similar loss functions; however, we leave this extension for future work.\n\n[1]* `Video dynamics prior\u2019 - Neurips 2023"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2017/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699739528972,
                "cdate": 1699739528972,
                "tmdate": 1699739528972,
                "mdate": 1699739528972,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AWatMkh3Eb",
                "forum": "nfMyERXNru",
                "replyto": "g8N80i1aXm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2017/Reviewer_6Tks"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2017/Reviewer_6Tks"
                ],
                "content": {
                    "title": {
                        "value": "Further comments (1/2)"
                    },
                    "comment": {
                        "value": "I appreciate the authors detailed response. Here are my further comments:\n\n> What we explored in this work (one of our major novel contributions) is to perform multiplicative decomposition on frames of the video sequence. We derived one such multiplicative decomposition utilizing the gamma correction formulation for the frame relighting task.\n\nI agree that multiplicative decomposition is suitable for relighting task. That being said, the exact form of decomposition (i.e., whether it is multiplicative, or additive, or any other form) is highly dependent on the exact task, which actually means that the proposed framework is highly task based but not sufficiently general. That dedicated design for relighting is technically reasonable; yet I found it hard for me to judge whether it is a \"major novel\" contribution.\n\n> Not only that, we are also able to achieve high temporal coherence in the processed relit videos. Such qualitative comparison can be found in videos attached with the webpage(supp.).\n\nThe relighting quality looks great; the temporal coherence (as I played them again and again) looks very similar for both the proposed method, ZeroDCE++, and StableLLVE. Hence it is hard to say which one has \"bettter\" temporal coherency.\n(BTW one interesting point I noted is that ZeroDCE++ produces also temporal stable results but with noisy outputs. From my understanding, the noisy-eliminated result in the proposed method is mostly due to the usage of CNN as a prior during optimization (which was discussed in supp. materials), which are exactly the contribution of DIP.)\n\n> The core of our framework lies in its ability to adapt to different tasks while maintaining a consistent structure.\nThe consistent architecture of $\\alpha$-Net and RGBnet establishes a unified way of extracting the motion and appearance features from a video, across different tasks. These intermediate motion and appearance features are then utilized with task-specific adaptations that are reflected in the decomposition formulations. Hence, the unifying factor across different tasks is the consistency in the architecture used to extract motion and appearance cues from the videos across all tasks.\n\n(1) As you said, the intermediate features are utilized with task-specific adaptations with different formulations for different tasks. The formulation, prior loss functions, and the exact semantic meaning of the intermediate output from $\\alpha$-Net and RGBnet, are completely different. The author(s) seems argue that \"using the same network structure\" is enough to regard it as a \"unified framework\". I agree that they are the same; but I would respectfully disagree for claim it as a \"unified framework\" if the only consistency between tasks is that they share a same network structure for feature extraction.\n\n(2) The same thing across the proposed method for different tasks is the usage a neural network to extract intermediate representations. Again, this idea itself has been discussed in many papers (e.g., the DIP work and its extensions).\n\n> Second, The VDP framework\u2019s way of modeling motion and appearance in a video is unique to our work(utilizing \n-Net and RGBnet). Specifically, to our knowledge, no prior test time optimization technique has utilized FlowRGBs to model the motion of the video, and it is unique to our methodology.\n\nI really agree with this point, and I think utilizing FlowRGBs as input is indeed the key from my understanding. Hence this paper would become far more interesting if it had dense discussions regarding this part (which unfortunately it does not). \nThe most interesting part here is that the while the input optical flow models the \"relative\" motion between two frames, the output of FlowRGBs (from the $\\alpha$-net) becomes a per-frame $\\alpha$ that represents some \"absolute\" status for static frames. I believe having an in-depth discussion from the FlowRGB input perspective is far more interesting than saying the proposed framework is \"general\"."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2017/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700298075786,
                "cdate": 1700298075786,
                "tmdate": 1700298075786,
                "mdate": 1700298075786,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PUBBfkvtAP",
                "forum": "nfMyERXNru",
                "replyto": "XRJRDJcAxW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2017/Reviewer_6Tks"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2017/Reviewer_6Tks"
                ],
                "content": {
                    "title": {
                        "value": "Further comments (2/2)"
                    },
                    "comment": {
                        "value": "> With regards to the mentioned test-time methods[2,3], they are not at all designed for the VOS task but more for edit propagation tasks. These methods require per-frame masks obtained by image segmentation baselines. So, we do not see any overlap of these techniques with our framework apart from edit propagation tasks.\n\nI was misunderstanding that your method also requires mask input for first frame. Thanks for the clarification.\n\n> We also would like to emphasize that maintaining spatio-temporal coherence is a big issue when it comes to tasks like dehazing and relighting, as can be seen from the qualitative results provided in the webpage(supplementary material). Our novelty also stems from the fact that our method can provide high-fidelity results with far superior spatio-temporal coherency.\n\nSee comments above - I found it really hard to say the proposed method has \"far superior spatio-temporal coherency\" from the video results; For relighting task, it seems all methods except SDSD has similar level of temporal coherency.\nAlso, it is more important to understand why the proposed method has temporal coherency than just say \"we have good results\". For example, is the good temporal coherency comes from the $\\alpha$-map extracted from optical-flow inputs; or it is a directly outcome of applying optical-flow as a warping constraint as a loss function? The latter (warping constraints) is a common approach in video tasks, though.\n\n> In Table 2, we only compare with other test-time optimization baselines for a fair comparison as the setting of each methodology is exactly the same, i.e., no prior segmentation annotation is given to the model during training.\n\nYour setup is different from VOS as no initial mask is given. Thanks for clarification.\n\n> Time taken: We refer the reviewer to our appendix section C.\nI have read section C and still a little bit confused: \n- \"We achieve a processing rate of 60 frames/min for dehazing and relighting and 40 frames/min for VOS with one salient object\"\n\nIs this processing rate referring to optimization time or just inference time?\n- \"We use 100 epochs for a 60-frame sequence in VOS and 60 epochs for dehazing and relighting\"\n\nLet's make the answer simple - what exactly is the time for optimizing one video (60-frames, for example)?"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2017/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700298130343,
                "cdate": 1700298130343,
                "tmdate": 1700298187280,
                "mdate": 1700298187280,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TuIjxHOIay",
                "forum": "nfMyERXNru",
                "replyto": "skThPiKdQd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2017/Reviewer_6Tks"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2017/Reviewer_6Tks"
                ],
                "content": {
                    "title": {
                        "value": "Updating score to 6"
                    },
                    "comment": {
                        "value": "Thanks for the author(s) response and discussion. After reading the latest response I decided to update my score to 6 based on the following reason - \n\n(1) The author(s) has decided to eliminate the \"unified framework\" to make the paper positioned in a more accurate way in the revised version.\n\n(2) The optical-flow input coupled with DIP-style prior for test-time optimization is new and interesting, and the author(s) will discuss more on this.\n\n(3) I still have concerns regarding the novelty of relighting task formulation and the evaluation on temporal coherence; yet it is a minor issue that could be somehow addressed.\n\n(4) My other concerns have been clarified and addressed.\n\nGiven the current status I would update the score to 6; I would say it hard to further raise the score because this paper (even accepted) would require a (somehow major) revision of the overall flow and texts."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2017/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700450852455,
                "cdate": 1700450852455,
                "tmdate": 1700450852455,
                "mdate": 1700450852455,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cgPJtQurJw",
            "forum": "nfMyERXNru",
            "replyto": "nfMyERXNru",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2017/Reviewer_XPSu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2017/Reviewer_XPSu"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a new approach called the Video Decomposition Prior (VDP) framework. Unlike conventional methods, VDP leverages the motion and appearance of the input video, decomposing it into multiple RGB layers with associated opacity levels. These layers are then manipulated individually to achieve the desired results, addressing tasks like video object segmentation, dehazing, and relighting. The paper also introduces a logarithmic video decomposition formulation for relighting tasks. The approach is evaluated on standard video datasets, including DAVIS, REVIDE, and SDSD, demonstrating qualitative results on a diverse range of internet videos."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ The proposed video decomposition prior leverages the motion and appearance of the input video, decomposing it into multiple RGB layers with associated opacity levels.\n+ The proposed VDP is employed in different video-based tasks, including video object segmentation, video dehazing and video relighting."
                },
                "weaknesses": {
                    "value": "- In the paper, the limitations of the VDP is not discussed in the paper, and all the results are good cases. \n- In the introduction of Flow similarity loss, the VGG embeddings of masked flow-RGB and those of other layers are used to calculate the cosine similarity. It is unclear how to generate VGG embeddings from masked feature maps for cosine similarity calculation. \n- In equation (12), the behavior of the reconstruction layer loss resembles that of the L1 loss in the reconstruction loss. To validate the rationale behind the design, it is essential for the paper to elucidate the distinctions between them and elucidate the impact of the reconstruction layer loss."
                },
                "questions": {
                    "value": "Please refer to the questions in the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2017/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698681871396,
            "cdate": 1698681871396,
            "tmdate": 1699636132923,
            "mdate": 1699636132923,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FwXg0JETAv",
                "forum": "nfMyERXNru",
                "replyto": "cgPJtQurJw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2017/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2017/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer XPSu for your thoughtful and detailed review of our paper on the Video Decomposition Prior (VDP) framework. We appreciate your recognition of our novel approach and its applications in video processing tasks. We have addressed each of your concerns below, aiming to clarify the aspects you highlighted. We believe these clarifications and additional information will solidify the strength of our paper and align it more closely with the acceptance criteria for ICLR.\n\n**1. Limitation of VDP not discussed:**\nWe acknowledge the importance of discussing the limitations of our methodology. To address this, we have added a comprehensive discussion on the limitations of VDP in Section H of our appendix (part of the supplementary material). Due to space constraints, this section was not included in the main paper, but we are prepared to move it to the main body if ICLR allows an additional page in the final submission. This inclusion will provide a more balanced view of our methodology and its applicability.\n\n**2. Flow Similarity Loss:**\nWe appreciate your query regarding the formulation of our Flow Similarity Loss. This loss is crucial for differentiating the motion of the background from the foreground. The process involves three steps:\n\n- **Step 1:** We take the FlowRGB of the frame at time step $t$ (the process of obtaining FlowRGBs is detailed in Section K of the appendix included in supplementary material) and perform element-wise multiplication with both the foreground mask and the background mask, i.e., \n  - $FlowRGB_{Fg} = FlowRGB^t \\odot M^t$\n  - $FlowRGB_{Bg} = FlowRGB^t \\odot (1-M)^t$\n\n- **Step 2:** We use these obtained $FlowRGB_{Fg}$ and $FlowRGB_{Bg}$ and find their corresponding VGG embeddings. This is done by passing them as input to the VGG encoder (pretrained on ImageNet dataset and denoted as $\\phi(.)$) and taking the output of the last layer of the VGG encoder as the embedding vector. Mathematically, we can write these embedding vectors for the foreground and background layer as, \n  - $embed_{Fg} = \\phi( FlowRGB_{Fg})$\n  - $embed_{Bg} = \\phi( FlowRGB_{Bg})$\n\n- **Step 3:** In this final step, we calculate the loss value by taking a cosine value between these embeddings of the foreground and background layer.\n  - $L_{FlowSim} = \\frac{embed_{Fg} . embed_{Bg}}{|embed_{Fg}| |embed_{Bg}|}$\n\nWe minimize the cosine similarity between the VGG embeddings of the background layer and the foreground layer, which translates to the learning of a mask that separates the motion of the background and foreground. We hope this detailed explanation resolves any ambiguities about flow similarity loss.\n\n**3. Ablation Study Involving Reconstruction Layer Loss:**\nWe understand the concerns of the reviewer about the understanding the impact of reconstruction layer loss. We would refer the reviewer to section D.4 of the appendix(part of supplementary) where we perform extensive experiments on the DAVIS-16 dataset to study the impact of each loss term on the final segmentation results. We hope our clarifications have addressed the concerns of the reviewer.\n\nWe hope that our responses provide the necessary clarifications and insights into our methodology, reinforcing the potential impact and novelty of our work in the field of video processing. We are optimistic that these clarifications justify a rating upgrade from the current rating of \"6\"."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2017/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699751156778,
                "cdate": 1699751156778,
                "tmdate": 1699751156778,
                "mdate": 1699751156778,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yN9lSoPMjV",
            "forum": "nfMyERXNru",
            "replyto": "nfMyERXNru",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2017/Reviewer_8QGm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2017/Reviewer_8QGm"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces the Video Decomposition Prior (VDP) framework, a new approach for video editing tasks, including object segmentation, dehazing, and relighting. VDP decomposes videos into multiple layers and optimizes parameters without explicit training. The proposed logarithmic video decomposition enhances video relighting, resulting in state-of-the-art performance in downstream tasks: unsupervised video object segmentation, dehazing, and relighting."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Sound approach: The VDP framework presents an innovative approach to video editing, offering practicality and cost-effectiveness by not relying on extensive datasets or ground truth annotations. The ability to optimize parameters using the test sequence itself distinguishes VDP from traditional deep learning methods, which often require extensive training data.\n\n2. State-of-the-Art Performance: VDP demonstrates top-tier performance in key downstream tasks, including unsupervised video object segmentation, dehazing, and relighting.\n\n3. Good writing and representation."
                },
                "weaknesses": {
                    "value": "I am generally positive about this paper. my main concern lies in the lack of comprehensive comparison: The paper does not provide a comprehensive comparison with other video editing techniques, making it difficult to assess the VDP framework's performance against other state-of-the-art methods. Furthermore, video editing is a broad field, including tasks such as adding or removing objects. It seems that the proposed method may not be suitable for handling these scenarios. Therefore, I suggest that the authors consider refining the title and corresponding claims."
                },
                "questions": {
                    "value": "Please see the weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2017/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2017/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2017/Reviewer_8QGm"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2017/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698758549980,
            "cdate": 1698758549980,
            "tmdate": 1699636132858,
            "mdate": 1699636132858,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "f7aOcoVJtk",
                "forum": "nfMyERXNru",
                "replyto": "yN9lSoPMjV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2017/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2017/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer 8QGm for your thorough review and constructive feedback on our submission, \"Video Decomposition Prior (VDP).\" We appreciate your recognition of VDP's innovative approach and its state-of-the-art performance in unsupervised video object segmentation, dehazing, and relighting. Your insights are invaluable, and we have addressed your concerns below, hoping to strengthen our paper and align it more closely with your expectations.\n\n**Addressing the Comprehensive Comparison Concern:**\nYou rightly pointed out the necessity for a comprehensive comparison with existing video editing techniques. To address this, we have included additional comparative analysis in Section G of our appendix(supplementary material). This section details a unique proxy task developed specifically to evaluate VDP against contemporary baselines, considering the absence of a standardized comparison framework in video editing. The results of this comparative analysis are presented in Table 6 of the appendix. Additionally, we have prepared a supplementary webpage featuring video results and qualitative comparisons with these baselines, which should provide a more tangible understanding of VDP's capabilities in relation to existing methods.\n\n**Refining the Title and Scope:**\nIn response to your suggestion, we agree that refining the title and scope of our paper could more accurately represent the specific editing tasks that VDP excels in. We propose the revised title, \"Video Decomposition Prior and its Applications in Segmentation, Dehazing, and Relighting.\" This modification narrows down the scope and directly reflects the core strengths of our methodology. We are open to further suggestions from the reviewer regarding the title and scope adjustments.\n\nWe believe these updates and clarifications directly address your concerns and enhance the paper's relevance and contribution to the field. We hope these modifications and clarifications resonate with your assessment criteria and upgrade the paper's rating from a \u20186'."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2017/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699751461681,
                "cdate": 1699751461681,
                "tmdate": 1699751461681,
                "mdate": 1699751461681,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "t3VxCUNR8b",
            "forum": "nfMyERXNru",
            "replyto": "nfMyERXNru",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2017/Reviewer_u6Cn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2017/Reviewer_u6Cn"
            ],
            "content": {
                "summary": {
                    "value": "This paper attempts to propose a general framework for video editing. It starts by predicting the video into several individual layers and then utilizes task-specific knowledge to reconstruct the video. A reconstruction loss and a warping loss are utilized to train the network. This is an inference-time optimization framework that does not rely on external training tools. However, the framework is rather standard, and the choice of a few loss functions for the decomposition tasks is straightforward. Overall, the novelty is limited. I am inclined to reject this article. Please check the details in other sections."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The motivation to propose such a general framework is good."
                },
                "weaknesses": {
                    "value": "-: The reconstruction loss and warp loss used are very common loss functions and do not offer any novelty.\n\n-: The method is still not sufficiently general. For example, for the task of video segmentation, additional loss functions need to be designed as constraints. The three task-specific losses mentioned in Section 3.2 are the only ones provided. From this perspective, it is difficult to see what this framework proposed in the paper brings to this community.\n\n-: The comparison results of the experiments are not that fair. For instance, in Table 3, the comparison is made with the latest algorithm, CG-IDN (a 2021 algorithm). There are many dehazing algorithms that could be compared, including single-image hazing algorithms with stability processing (references [1][2]).\n\n-: Additionally, Table 2 lacks many baselines. Looking at the official website of DAVIS2016, the best baseline achieves an IOU score of over 82. Why wasn't this paper compared against that?\n\n-: Sometimes it is necessary to introduce additional prior knowledge, such as for the task of dehazing, where the effectiveness is actually limited.\n\n[1] Learning Blind Video Temporal Consistency\n\n[2] Blind Video Temporal Consistency via Deep Video Prior"
                },
                "questions": {
                    "value": "See weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2017/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2017/Reviewer_u6Cn",
                        "ICLR.cc/2024/Conference/Submission2017/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2017/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698759639502,
            "cdate": 1698759639502,
            "tmdate": 1700631896700,
            "mdate": 1700631896700,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2wBdw4O0zY",
                "forum": "nfMyERXNru",
                "replyto": "t3VxCUNR8b",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2017/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2017/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for taking their time out of a busy schedule and evaluating our submission. We are glad that reviewer u6Cn found our paper well-motivated. Before we address the specific weaknesses and questions of the reviewer, **we highly recommend reviewer u6Cn to please go through the webpage of video results**, which was attached in the supplementary file. \n\n\n**Novelty issues:**\n\nWe understand the concern raised by the reviewer over the novelty of the paper. Here is a more structured view of our model. The core of our framework for any task at hand has three components to it: prior knowledge(decomposition formulation), motion modeling (via $\\alpha$-Net), and appearance modeling (via RGBnet). The framework\u2019s way of modeling motion and appearance in a video is unique to our work. Specifically, to our knowledge, no prior test time optimization technique has utilized FlowRGBs to model the motion of the video. \n\nSecond, the utilization of prior knowledge(from gamma correction) to derive a novel decomposition formulation for the relighting task is a unique contribution of our work.\nLastly, we are able to achieve state-of-the-art temporally coherent results utilizing our framework for the task of dehazing and relighting. We would urge the reviewer to please check out the difference in the qualitative video results of our methodology and baselines(We have provided these in an HTML webpage as a part of our supplementary material). To the reviewer\u2019s point, our work derives the novelty from the above-mentioned points as opposed to the reconstruction or warping loss. We hope our clarification will alleviate the novelty concerns of the reviewer.\n\n\n**What is the consistency across tasks:**\n\nWe thank the reviewer for pointing out the difficulty in understanding the consistency between the tasks. Hopefully, our clarification about it will alleviate this difficulty. \n\nThe core of our framework lies in its ability to adapt to different tasks while maintaining a consistent structure. Basically, each task consists of three integral components: prior knowledge (decomposition formulation), motion modeling (via $\\alpha$-Net), and appearance modeling (via RGBnet). The **consistent architecture** of $\\alpha$-Net and RGBnet establishes a **unified** way of extracting the motion and appearance features from a video across different tasks. These intermediate motion and appearance features are then utilized with task-specific adaptations that are reflected in the decomposition formulations. Hence, the unifying factor across different tasks is the consistency in the architecture used to extract motion and appearance cues from the videos across all tasks.\n\nWe would put more effort into revamping the introduction section of our manuscript to emphasize this point of consistency our framework's architecture brings across this variety of tasks.\n \n\n**Baseline comparison for UVOS Task:**\n\nWe thank the reviewer for raising this point. In Table 2, we only compare with other test-time optimization baselines for a fair comparison as the setting of each methodology is exactly the same, i.e., no prior segmentation annotation is given to the model during training. We provide a comprehensive evaluation of the task UVOS task in Table 5 in the appendix because of the limited availability of space in the main paper. We would add the results to the main paper if ICLR allows one extra page in the final camera-ready paper.\n\n\n**Additional Experiments:**\n\nWe provide more comparisons with the baseline suggested by the reviewers for the dehazing task. We utilize the MSBDN as an image dehazing baseline. Please find the findings below.\n| Metrics | Blind[1] | DVP[2] | Ours |\n|----------|----------|----------|----------|\n| PSNR|22.98| 23.05 | 24.83|\n|SSIM| 0.877 | 0.882 | 0.916|\n\n\n**Additional Prior knowledge for the dehazing task:**\n\nWe agree with the reviewer that additional knowledge about the scene can improve the efficacy of our methodology. However, it is very difficult to acquire hazy video and clean video pairs in a real-world setting. Hence, we formulated a test time optimization technique that does not rely on task-specific data for performing the dehazing."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2017/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699740424252,
                "cdate": 1699740424252,
                "tmdate": 1699740424252,
                "mdate": 1699740424252,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "phxAzzOFrO",
                "forum": "nfMyERXNru",
                "replyto": "2wBdw4O0zY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2017/Reviewer_u6Cn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2017/Reviewer_u6Cn"
                ],
                "content": {
                    "title": {
                        "value": "Further comments"
                    },
                    "comment": {
                        "value": "Thank the authors for the detailed feedback. The rebuttal of the authors addresses most of my concerns, and I have some further questions regarding the feedback.\n\n- The novelty of FlowRGB: I am not quite clear about the argument \"no prior test time optimization technique has utilized FlowRGBs to model the motion of the video\". For example, it seems that [1] uses both flow and RGB for test-time optimization.\n\n- Are you going to remove the description about \"Unified Approach\". To me, your applications on three different tasks are still quite task-specific. It seems that you are going to revise the description in your feedback to other reviewers. \n\n- For VOS comparison, it is acceptable that you only compare with other test-time optimization approach. However, doesn't this go against your original intention? You should conduct experiments on some tasks that achieve better performance on test-time optimization approach to support your argument.\n\n- I appreciate the novelty on the relighting task and believe the your performance is better than other baselines.\n\n- Thanks for the results of experiments on dehazing tasks. It helps support your argument.\n\nThanks.\n\n[1] Deformable Sprites for Unsupervised Video Decomposition"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2017/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700558261266,
                "cdate": 1700558261266,
                "tmdate": 1700558261266,
                "mdate": 1700558261266,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Vyg1lxThCE",
                "forum": "nfMyERXNru",
                "replyto": "qDfnY5p3rZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2017/Reviewer_u6Cn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2017/Reviewer_u6Cn"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your time and explanations. I believe your explanations solve most of my concerns. In summary, I think the major problem of this problem is the argument about \"Unified\". As Reviewer 6Tks says, this paper requires substantial revision to emphasize its real contribution and avoid confused arguments. Now, I hold a positive attitude toward this paper, and my major concern is: can the current paper (at current status) be accepted? If the revision can be promised and all reviewers feel OK about it. I think this paper can be accepted. In summary, rejecting or accepting this paper are both OK for me."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2017/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631871438,
                "cdate": 1700631871438,
                "tmdate": 1700631871438,
                "mdate": 1700631871438,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]