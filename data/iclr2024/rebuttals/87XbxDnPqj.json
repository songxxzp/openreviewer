[
    {
        "title": "Gradient Descent Provably Solves Nonlinear Tomographic Reconstruction"
    },
    {
        "review": {
            "id": "78sGcGyRKu",
            "forum": "87XbxDnPqj",
            "replyto": "87XbxDnPqj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8267/Reviewer_TLot"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8267/Reviewer_TLot"
            ],
            "content": {
                "summary": {
                    "value": "The authors consider the computed tomography (CT) reconstruction problem. Specifically, they consider a case with strong reflectors in the scene of interest (like a metal implant). When such materials are present, they argue that a preprocessing step that is normally used to obtain a linear inverse problem is not numerically stable. In contrast, they propose to solve the resulting non-convex problem directly through gradient descent. They show that, if the observation matrix were a sample from a Gaussian distribution, and if the number of measurements exceed a lower bound, with high probability, gradient descent converges to the true solution at a geometric rate.\nThey also extend the results to the case where the number of measurements are fewer but this reduction is compensated by a regularizer that captures the structure in the family of signals that the solution belongs to.\nIn an experiments section, they compare their proposed reconstructions against the linearized model (where the Fourier-slice theorem applies)."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is clear, well-written, and points to a relevant sub-problem in CT reconstruction. They show that a simple approach like gradient descent, targeting a relatively simple reconstruction formulation, can provably solve the original problem under certain constraints. They also extend their results to the case where the number of measurements are reduced at the expense of making certain assumptions about the signal of interest, and come up with clear statements that they can prove. Reducing the number of measurements in CT is always welcome as that reduces the amount of radiation the patient is subjected to."
                },
                "weaknesses": {
                    "value": "The assumptions made by Theorems 1 and 2, which form the main contribution of the paper do not match well the CT measurement setup. Specifically, in CT, the beams are well-structured, and if we consider the image of a beam, it does not resemble at all a sample from a 2D Gaussian field. Therefore, it's not clear to me how much the results on provable convergence carry over. If the authors can show or argue that the local pseudo-convexity property (in some reasonable neighborhood of the solution) carries over to the deterministic concentrated-beam setting, the connection with CT would be stronger. Since the paper is not concerned with proposing a non-obvious algorithm but proving the usefulness of a rather well-known algorithm, this is a serious shortcoming.\n\nLess importantly, the experiments are not entirely convincing -- I think reconstructions with the linear model could have been improved by including regularizers."
                },
                "questions": {
                    "value": "Here are some comments/questions, some of which are minor.\n\n1. Page 2, \"We propose a Gaussian model of eqn 1.2\" : Please clarify what you mean by a Gaussian model? I guess you're referring to modelling $a_i$ to be samples of a Gaussian field, but that's not clear at all at this stage.\n\n2. Page 2, beginning of Section 2, \"(1) we model $a_i$ as a standard Gaussian...\" : This strongly contradicts the opening sentence of Section 2, where you mention \"$a_i$ are sparse, nonnegative, highly structured, dependent on $i$\". What is the motivation behind this modelling? Is it mainly to obtain provable results? I understand some deviation from an accurate model, but this is taking it a bit extreme.\n\n3. The second sentence of Definition 2 (\"Throughout...\") doesn't look like it's part of the definition.\n\n4. Page 5, footnote 1 : It's worth bringing in to the main text and clarifying what $m$ should exceed.\n\n5. In general, I would welcome concrete typical values for the constants $c_i$ that appear in theorems, for specific cases/resolution.\n\n6. Section 5.1 : Can you actually expect to see inside the metal? Here, I think the regularizer plays a more crucial role. What happens if you were to use a regularized formulation but with a linear measurement model (e.g., using one of the iterative solvers like FISTA)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8267/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698508712421,
            "cdate": 1698508712421,
            "tmdate": 1699637028331,
            "mdate": 1699637028331,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UIP69MyEN6",
                "forum": "87XbxDnPqj",
                "replyto": "78sGcGyRKu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8267/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8267/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response (part 1)"
                    },
                    "comment": {
                        "value": "Thanks for your thoughtful comments on our work! We respond to individual comments and questions inline below.\n\n>The assumptions made by Theorems 1 and 2, which form the main contribution of the paper do not match well the CT measurement setup. Specifically, in CT, the beams are well-structured, and if we consider the image of a beam, it does not resemble at all a sample from a 2D Gaussian field. Therefore, it's not clear to me how much the results on provable convergence carry over. If the authors can show or argue that the local pseudo-convexity property (in some reasonable neighborhood of the solution) carries over to the deterministic concentrated-beam setting, the connection with CT would be stronger. Since the paper is not concerned with proposing a non-obvious algorithm but proving the usefulness of a rather well-known algorithm, this is a serious shortcoming.\n\nThis is a great point that we have now addressed in the revised paper (and in this comment) in two ways. \n\nFirst, we have added an explanation of the logical steps we followed to reach our Gaussian theoretical model, and why each of these steps is well-supported by prior literature. In particular, our Gaussian model follows from two approximations: (1) A projection in real-space is equivalent to a slice through the origin in Fourier space, according to the Fourier slice theorem. Our first approximation involves sampling random Fourier coefficients rather than radial slices in Fourier space. This approximation is somewhat standard in the literature, for example in the seminal compressive sensing paper by Candes, Romberg, and Tao (https://arxiv.org/abs/math/0503066). (2) Our second approximation is to replace random Fourier coefficients with random Gaussian measurements. This approximation is also well supported by prior work, namely https://arxiv.org/abs/1506.03521 which shows that \u201cstructured random matrices behave similarly to random Gaussian matrices\u201d in compressive sensing type settings.\n\nSecond, and more importantly, we have independently verified that the key pseudoconvexity property we use in our proof\u2014namely, the positivity of the correlation quantity <gradient of loss at z, z-x>/||z-x||^2, where x is the true signal and z is the current iterate\u2014holds in practice even when we evaluate it using the true structure-preserving loss function instead of our Gaussian model. https://imgur.com/5rU3qgH (and the new Figure 1 in the revised manuscript) is a plot of the correlation value <gradient of loss at z, z-x>/||z-x||^2 where x is the synthetic phantom with ||x||= 7.4; the x-axis distance is normalized by this value so that the origin has normalized distance 1 relative to the global optimum. We sample 100 values of z at various distances from x, and show that this correlation is empirically positive for a wide range of distances including the distance between the origin and the true signal x. This experiment uses the true forward model that respects the ray structure, verifying that the same pseudoconvexity property we show in our Gaussian model also holds in the full forward model.\n\n>Less importantly, the experiments are not entirely convincing -- I think reconstructions with the linear model could have been improved by including regularizers.\n\nTo clarify, the baseline linearized method in the synthetic experiment is including exactly the same regularization (by total variation) as the nonlinear method; we also note that this synthetic experiment is not in the underdetermined regime, as more measurements are provided than there are voxels in the reconstruction. The baseline linearized method in the real experiment is not using gradient descent, but rather using a state-of-the-art proprietary algorithm provided by our industrial collaborators. This real-data linearized baseline is the closest we have to a ground truth volume. In the revised paper, we have added an additional comparison to a more standard (and open-source) linearized baseline algorithm, FDK, which also exhibits worse metal artifacts than our nonlinear reconstruction. The updated figure is in the revised manuscript, and at https://imgur.com/a/PmtZz2O for convenience."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8267/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700586295375,
                "cdate": 1700586295375,
                "tmdate": 1700586295375,
                "mdate": 1700586295375,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lOqtEBH4m4",
            "forum": "87XbxDnPqj",
            "replyto": "87XbxDnPqj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8267/Reviewer_qFK5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8267/Reviewer_qFK5"
            ],
            "content": {
                "summary": {
                    "value": "Due to a medical emergency, I am unable to assess this article."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "N/A"
                },
                "weaknesses": {
                    "value": "N/A"
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8267/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8267/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8267/Reviewer_qFK5"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8267/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698697736619,
            "cdate": 1698697736619,
            "tmdate": 1699637028228,
            "mdate": 1699637028228,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VAOgUOe2rA",
                "forum": "87XbxDnPqj",
                "replyto": "lOqtEBH4m4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8267/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8267/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We wish you all the best in recovering from this medical emergency!"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8267/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700585941496,
                "cdate": 1700585941496,
                "tmdate": 1700585941496,
                "mdate": 1700585941496,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "91mBmwLl65",
            "forum": "87XbxDnPqj",
            "replyto": "87XbxDnPqj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8267/Reviewer_MBT4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8267/Reviewer_MBT4"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies a nonlinear CT reconstruction model and provides global convergence for the proposed algorithms for unregularized and regularized problems. To justify the theoretical results, experiments are conducted on one synthetic data set and one real cone-beam CT data set. Some related works and further discussions are presented."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Detailed convergence analysis for the proposed algorithms would be the major strength. Besides, numerical experiments on CT data sets are conducted to justify the theoretical results."
                },
                "weaknesses": {
                    "value": "1. The paper does not seem to use the standard ICLR paper template. \n2. Regularization techniques have been widely used in solving inverse problems to address the ill-posedness. It is not clear about the contributions and novelty of the proposed approaches. \n3. In the experiments, there are no other related works for comparison. So, it is hard to see whether the proposed performance is standing out."
                },
                "questions": {
                    "value": "Why was the regularization term in the objective function in (2.3) recast as an inequality constraint in (4.1)? Will they give two different solutions? In addition, how was $x$ in (4.1) selected/constructed?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8267/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698777061001,
            "cdate": 1698777061001,
            "tmdate": 1699637028113,
            "mdate": 1699637028113,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "J6fEBVaAn3",
                "forum": "87XbxDnPqj",
                "replyto": "91mBmwLl65",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8267/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8267/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your time reviewing our paper! We respond to individual comments and questions inline below.\n\n>The paper does not seem to use the standard ICLR paper template. \n\nThis is interesting\u2013we are using the ICLR 2024 LaTeX template, but indeed it seems that the header line is missing in the original submission due to a package clash. Thanks for pointing this out; it has been resolved in the revised manuscript.\n\n>Regularization techniques have been widely used in solving inverse problems to address the ill-posedness. It is not clear about the contributions and novelty of the proposed approaches. \n\nThe main problem we are addressing is separate from the ill-posedness problem from undersampling. Rather, we are addressing the issue of sensitivity to absorptive media like metal, which arises because of the poorly conditioned logarithmic preprocessing used to linearize the measurements in the standard reconstruction pipeline. This problem would persist with the linearized forward model even in the case of unlimited measurements. For example, our synthetic experiments are not in the ill-posed regime; instead they are overdetermined and have no added measurement noise, yet linearized reconstruction fails because the logarithmic preprocessing is sensitive even to the noise of machine precision.\n\n>In the experiments, there are no other related works for comparison. So, it is hard to see whether the proposed performance is standing out. \n\nWe have added a standard linearized baseline method, FDK, in our real-data experiment, and we find that it also exhibits severe artifacts. The updated figure is in the revised manuscript, and at https://imgur.com/a/PmtZz2O for convenience. Please also note that the primary contribution of our work is the theoretical analysis, which is the first of its kind for this nonlinear forward model. The experiments are provided as a preliminary validation to support the theoretical results; they are not intended as a comprehensive comparison.\n\n>Why was the regularization term in the objective function in (2.3) recast as an inequality constraint in (4.1)? Will they give two different solutions? \n\nThese two formulations (2.3 and 4.1) are equivalent in the sense that for any value of R(x) in the inequality constraint (4.1) there is a value of lambda in the regularization term (2.3) that will yield exactly the same solution. We also note that our theoretical results are robust to the exact choice of this regularization weight or inequality constraint threshold (see e.g. section 6.4 in https://arxiv.org/pdf/1507.04793.pdf), but we have stated the results in this form (with a precise choice of regularization or constraint) for simplicity of exposition.\n\n>In addition, how was x in (4.1) selected/constructed?\n\nIn (4.1) the x in the constraint refers to the unknown ground truth signal. Even if the signal is unknown, this formulation assumes that some structure of x is known via the value of the function R(x). In practice this value R(x) can be viewed as a hyperparameter. \n\n\nPlease let us know if we've addressed your concerns, or if there are any followup comments or questions. Thanks for your time and thoughtful engagement with our revision!"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8267/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700585898265,
                "cdate": 1700585898265,
                "tmdate": 1700585898265,
                "mdate": 1700585898265,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "I4VSg50zB0",
            "forum": "87XbxDnPqj",
            "replyto": "87XbxDnPqj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8267/Reviewer_1R2J"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8267/Reviewer_1R2J"
            ],
            "content": {
                "summary": {
                    "value": "This article proposes a tomographic reconstruction method starting from the Beer-Lambert measurements on projections without linearisation. The resulting formulation is non-convex but the authors show its global optimum can be reached by gradient descent with high probability, with sufficient measurements, even in the presence of regularisation, which is the main result of the paper."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is interesting and relatively easy to read. The approach is also fairly straightforward, although non-trivial, and it is surprising that is has not been attempted before. The mathematical and optimisation approach is well carried out."
                },
                "weaknesses": {
                    "value": "Experiments are unconvincing, particularly Fig.2, where except around the high-density object (the crown), the quality of the linearised reconstruction is visibly better. Since the image is a phantom, a ground-truth should have been provided. The experiments on the Shepp-Logan phantom are too simple to be convincing.\n\nMaybe this is due to a poor choice of hyper-parameter or a sub-optimal regulariser.\n\nReference code does not seem to be provided, limiting reproducibility."
                },
                "questions": {
                    "value": "- How could you improve the fuzziness of Fig.2 ? This is essential because the current result does not sell your method well.\n- There are many other non-linear aspect to real tomography, for example the phenomenon of beam hardening (BH), due to the fact that the X-ray source is not monochromatic and that lower-energy photons are absorbed more than higher-energy ones. Can you carry over some of your methodology to reduce the artefacts caused by BH?\n- Could you compare with other non-linear tomography approaches."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8267/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699320488737,
            "cdate": 1699320488737,
            "tmdate": 1699637028003,
            "mdate": 1699637028003,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WarYnpbgNK",
                "forum": "87XbxDnPqj",
                "replyto": "I4VSg50zB0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8267/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8267/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your thoughtful comments on our work! We respond to individual comments and questions inline below.\n\n>Experiments are unconvincing, particularly Fig.2, where except around the high-density object (the crown), the quality of the linearised reconstruction is visibly better. Since the image is a phantom, a ground-truth should have been provided. The experiments on the Shepp-Logan phantom are too simple to be convincing.\n\nJust to clarify, when we describe the dental imaging target as a phantom, it is because it is not a living patient. This phantom is a real human skull, not a synthetic function, so we do not have a ground truth volume. We have removed the term \u201cphantom\u201d in the revised paper, when describing this skull dataset, to avoid any confusion for future readers. For this real dataset the linearized reconstruction is the closest we have to a ground truth; it is the state-of-the-art reconstruction provided by an industrial collaborator (using a proprietary method). We have also augmented our real-data experiment with a comparison to a more standard and open-source linearized baseline method, FDK, which likewise suffers metal artifacts as well as generally lower quality compared to either our nonlinear reconstruction or the commercial reference linearized reconstruction.\n\n>Maybe this is due to a poor choice of hyper-parameter or a sub-optimal regulariser.\n\nIndeed, by slightly reducing the total variation regularization we are able to reduce the blurriness in our nonlinear reconstruction. Thanks for the suggestion! The updated figure with our tuned results and the comparison to FDK is in the revised manuscript, and at https://imgur.com/a/PmtZz2O for convenience.\n\n>Reference code does not seem to be provided, limiting reproducibility.\n\nOur anonymized code has been uploaded as a supplemental file. We will also release the code publicly upon publication.\n\n>How could you improve the fuzziness of Fig.2 ? This is essential because the current result does not sell your method well. \n\nPlease refer to our response above.\n\n>There are many other non-linear aspect to real tomography, for example the phenomenon of beam hardening (BH), due to the fact that the X-ray source is not monochromatic and that lower-energy photons are absorbed more than higher-energy ones. Can you carry over some of your methodology to reduce the artefacts caused by BH? \n\nThis is a great suggestion that we will consider in followup work.\n\n>Could you compare with other non-linear tomography approaches. \n\nWe agree it\u2019s always preferable to compare to strong baselines. In this case, we are not aware of existing algorithms that use the nonlinear tomography forward model rather than the linearized version\u2013though if the reviewer has suggestions we would be happy to look into them. To be clear, when we say non-linear we are referring to the measurement model, not the reconstruction algorithm. For example, there are some nonlinear methods (e.g. Neural Adaptive Tomography https://dl.acm.org/doi/abs/10.1145/3528223.3530121) that use a nonlinear model as the volume representation (e.g. neural net), but to our knowledge these still use the same linearized forward model and thus face the same sensitivity to absorptive media like metal.\n\nPlease let us know if we've addressed your concerns, or if there are any followup comments or questions. Thanks again for your time and thoughtful engagement with our work!"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8267/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700585623975,
                "cdate": 1700585623975,
                "tmdate": 1700585623975,
                "mdate": 1700585623975,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]