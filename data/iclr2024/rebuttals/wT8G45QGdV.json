[
    {
        "title": "Consistent123: Improve Consistency for One Image to 3D Object Synthesis"
    },
    {
        "review": {
            "id": "zmtyYAO6Ri",
            "forum": "wT8G45QGdV",
            "replyto": "wT8G45QGdV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1971/Reviewer_xWyo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1971/Reviewer_xWyo"
            ],
            "content": {
                "summary": {
                    "value": "Consistent123 is an improved version of Zero123 by optimizing using extra cross-attention consistency with a progressive classifier-free guidance strategy. This cross-attention training also enables generating an arbitrary number of views during inference."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- A clear improvement in novel view synthesis. The video shared in the supplementary file clears shows the strength of Consistent123 over Zero123. \n- The cross-attention mechanism is simple yet effective."
                },
                "weaknesses": {
                    "value": "1. Smoothness in Results. I noticed that the Consistent123 approach generally produces smoother results and seems to miss out on some finer details compared to Zero123. This observation is particularly evident in the first row of Fig. 4, and in the hat geometry depicted in Fig. 7: both in sections (a) bottom and (b) up. Have the authors identified potential strategies or modifications to address this shortcoming?\n\n2. Concerns regarding arbitrary-length sampling. The methodology adopted uses a fixed number of views (8 views) during training. I'm concerned that this fixed view might adversely affect performance when dealing with arbitrary-length sampling at inference. This concern arises from a potential mismatch between training and test distributions. Any reason why using fixed number views during training? Is it only for simple implementation and faster training? An ablation study showcasing the performance with a random number of views as well during training would provide valuable insights and address this concern. \n\n3. Ablation on Zero123 pretraining. Could you present results when Consistent123 is trained from scratch without Zero123 pretraining?"
                },
                "questions": {
                    "value": "1. Presence of Ground Truth for clarity. I would recommend including the Ground Truth in Figs 1, 4, 5, and 6. Having a point of reference would greatly enhance the clarity and allow for a more informed evaluation of the results.\n\n2. Visualization of cross attention during training. The manuscript currently lacks visualizations for cross-attention dynamics during training. It would be beneficial for readers to see how these cross-attention maps evolve and converge throughout the training process. \n\n3. It makes the paper stronger if you can show better 3D reconstruction results. For example, you can use your Consistent123 inside RealFusion [1] and Magic123 [2] to show state-of-the-art image-to-3D results. \n\nOther minor suggestions:\n1. suggest to add more views in Fig 1 since there are empty space. You can also point out in the figure where Zero123 fails and you success to catch the audiences\u2019 attention quickly. \n2. Show back views in Fig 7 (a) bottom and (b) up. \n3. Better to add cross attention between views in Fig.2 (a) as well, like a few red lines across views at the denoised views.\n\n[1] Melas-Kyriazi, Luke, Iro Laina, Christian Rupprecht, and Andrea Vedaldi. \"Realfusion: 360deg reconstruction of any object from a single image.\" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8446-8455. 2023.\n[2] Qian, Guocheng, Jinjie Mai, Abdullah Hamdi, Jian Ren, Aliaksandr Siarohin, Bing Li, Hsin-Ying Lee et al. \"Magic123: One image to high-quality 3d object generation using both 2d and 3d diffusion priors.\" arXiv preprint arXiv:2306.17843 (2023)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1971/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698591703011,
            "cdate": 1698591703011,
            "tmdate": 1699636128752,
            "mdate": 1699636128752,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "i86aRvwkbR",
                "forum": "wT8G45QGdV",
                "replyto": "zmtyYAO6Ri",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1971/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1971/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer xWyo"
                    },
                    "comment": {
                        "value": "> Consistent123 may produce smooth texture in synthesized views for some cases. Have the authors identified potential strategies or modifications to address this shortcoming?\n\nIt is an important but challenging task for NVS methods to predict novel views with finer details. To further solve the smooth texture problem, the following techniques may help:\n\n- Lower the PCFG guidance scale and increase inference steps. For all cases in the paper, we use the fixed schedule for progressive classifier-free guidance without per-case tuning to perform fair comparisons.  For some specific cases as mentioned above, the starting and ending points of progressive classifier-free guidance can be tuned (lower for better texture) to alleviate the smoothness problem. Moreover, using larger DDIM sampling steps (e.g., 200) also helps to improve the synthesized texture.\n- Improve the quality of training data. As mentioned in other concurrent works (e.g., Instant3D), there is a lot of low-quality training data in Objaverse for novel view synthesis, which may result in the smoothness of the synthesized texture. A careful dataset pruning strategy may further alleviate this problem.\n\n>  What is the performance with a random number of views as well during training?\n\nIt is interesting to use a random number of views during training. We implement it by selecting a random number of views dynamically for each batch, which ranges from 8 views to 16 views (since we only render 18 views for each object). We evaluate the consistency score on GSO as shown in the following table. It shows that training at a fixed number of views is already effective, and training at a random view may degrade the performance. This is mainly because as the number of training views increases, the difference among views is smaller. It becomes easier for cross-view attention to predict the novel views, thus harming the robustness of the learned model and may lead to the overfitting issue.\n\n|           Method           | PSNR  | SSIM | LPIPS |\n| :------------------------: | :---: | :--: | :---: |\n|          Zero123           | 22.88 | 0.92 | 0.25  |\n|        Zero123 + SC        | 22.30 | 0.93 | 0.21  |\n|  Consistent123 (8 views)   | 27.98 | 0.98 | 0.11  |\n| Consistent123 (8~16 views) | 26.25 | 0.96 | 0.14  |\n\n> Can Consistent123 be trained from scratch?\n\nFor a fair comparison, we train Consistent123 from scratch on the same renderings as Zero123. We show the consistency score evaluated on GSO in the following table:\n\n|              Method               | PSNR  | SSIM | LPIPS |\n| :-------------------------------: | :---: | :--: | :---: |\n|   Zero123 (from scratch, 105k)    | 22.88 | 0.92 | 0.25  |\n|   Consistent123 (finetune, 10k)   | 25.82 | 0.96 | 0.15  |\n| Consistent123 (from scratch, 20k) | 23.59 | 0.92 | 0.17  |\n\nIn our experiments, we found that it is challenging to simultaneously maintain the synthesized view quality and consistency. Due to the limited scale of the existing 3D dataset, the model is easy to be trapped in overfitting when directly trained from scratch. To reduce the overfitting, it is reasonable to enable the model with novel view synthesis ability by training like Zero123, then improve the view consistency as Consistent123. Moreover, due to the slow training process (over 7 days for zero123), there is no further time to explore a more effective optimization strategy. A better training strategy (e.g., Efficient 3DiM) may make it possible to train Consistent123 from scratch. \n\n> It makes the paper stronger if you can show better 3D reconstruction results (e.g. use Magic123).\n\nBetter 3D reconstruction results using Magic123 are shown in the video supplementary (magic123.mp4) and Figure 17.\n\n> Minor suggestions mentioned in the review. \n\nWe thank you for all the kind suggestions and will consider them in the revised paper."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1971/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700538481761,
                "cdate": 1700538481761,
                "tmdate": 1700538481761,
                "mdate": 1700538481761,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ikTC1Kjv1O",
            "forum": "wT8G45QGdV",
            "replyto": "wT8G45QGdV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1971/Reviewer_n2pA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1971/Reviewer_n2pA"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors aim to improve the view consistency for the novel view synthesis method based on image-to-image translation (i.e., Zero123). Specifically, They incorporate Zero123 with shared self-attention layers and additional cross-view attention layers. In addition, they propose a progressive classifier-free guidance strategy to balance the texture and geometry during the denoising process. Experimental results show that the proposed Consistent123 achieves better view consistency on multiple benchmarks compared to Zero123. The authors also demonstrate the potential of Consistent123 on various downstream tasks, such as 3D Reconstruction and image-to-3D generation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed method allows flexible view numbers compared to concurrent work MVDream. Experiments show that using arbitrary-length sampling with more view numbers could boost view consistency, indicating the effectiveness of the proposed method.\n2. The proposed method is intuitive yet effective. By adding additional attention mechanisms, the authors improve the view consistency of Zero123.\n3. The proposed progressive classifier-free guidance is interesting and alleviates the trade-off between geometry and texture."
                },
                "weaknesses": {
                    "value": "1. The attention mechanisms are totally borrowed from previous work, such as shared self-attention from Cao et al. and cross-attention from video diffusion models.\n2. For the shared self-attention layers, when the views are totally orthogonal, how will this shared self-attention act? Can this self-attention find correct correspondence? For example, in Figure 5 (right), when there is no shared self-attention, the resulting first view seems much more interesting. It would be better to have self-attention visualization in this case.\n3. Considering Objaverse has 800K+ 3D models, the authors only picked up 100 objects for Table 1, which seems far from enough.\n4. The proposed Consistent123 loads pretrained weight from Zero123 and fixes these weights. It would be fair to also have a version training from scratch.\n5. Will the compromise solution introduce view inconsistency? It looks like there is no connection between the sampled views and the next round views.\n6. The results on 3D reconstruction seem poor, where the results from Neus are blurry and low-quality."
                },
                "questions": {
                    "value": "1. For the shared self-attention layers, is there any positional embedding? If not, does introducing a camera pose-aware positional embedding help? Do you have any insight on this? \n2. Since the current conditions on R and T are still geometry-free, I am worried that the proposed method's upper bound is limited."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1971/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1971/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1971/Reviewer_n2pA"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1971/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698741205147,
            "cdate": 1698741205147,
            "tmdate": 1699636128680,
            "mdate": 1699636128680,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "a8o1fd2X7j",
                "forum": "wT8G45QGdV",
                "replyto": "ikTC1Kjv1O",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1971/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1971/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer n2pA"
                    },
                    "comment": {
                        "value": "> The attention mechanisms in Consistent123 are borrowed from the video diffusion model and Cao et al.\n\nAlthough the high-level idea of attention mechanisms is inspired by video generation, they still remain unexplored in 3D generation and can not be directly applied for novel view synthesis. We make several key modifications to adapt these components for better view consistency.\n\n- Shared self-attention is an inference-only strategy in Cao et al., but we adapt it for both training and inference in Consistent123 to further enhance the alignment between novel views and input views.\n\n- To attend to the input view latent, we fix the first pose of the view sequence to be the same as the input image, thus avoiding additional inverse operations and making the training process more efficient.\n\n- We apply shared self-attention in all the diffusion timesteps, which only part of timesteps are used in Cao et al. We also conduct an ablation study in Table 3(b) to decide the position to apply for shared self-attention.\n\n- Finally, other important components of Consistent123 also significantly contribute to the view consistency, including the flexible number and pose at inference (while the frame length in video generation is typically fixed) and progressive classifier-free guidance. \n\n> How will the shared self-attention act if there exists a large pose difference between the input view and the synthesized view?\n\nIntuitively, with a larger pose difference between the input view and synthesized view, the shared self-attention would become less activated. For these poses, the non-shared self-attention (i.e., U-Net Encoder part) and cross-view attention layers can serve as complementary and the performance can be maintained. \n\nFor a better understanding of how shared self-attention works, we conducted an ablation study. For the poses whose azimuth angle with input view is larger than 90\u00b0, we set the shared self-attention to vanilla self-attention by force. We evaluate the consistency score on GSO in the following table. The table shows that the constrained version of shared self-attention achieves similar performance as the vanilla self-attention. This indicates that the performance will degrade when disabling shared self-attention when a large pose difference exists, which makes the model harder to learn when attending keys and values from different views (half of views attend to input view, half attend to themselves).\n\n|              Method               | PSNR  | SSIM | LPIPS |\n| :-------------------------------: | :---: | :--: | :---: |\n|      Vanilla Self-attention       | 24.64 | 0.94 | 0.17  |\n|       Shared Self-attention       | 27.98 | 0.98 | 0.11  |\n| Constrained Shared Self-attention | 24.67 | 0.94 | 0.17  |\n\n> The authors only picked up 100 objects for Table 1, which seems far from enough.\n\nWe also agree that more evaluation objects help to reduce the randomness of quantitative results. However, the evaluation of the consistency score is quite time-consuming (constructing a Nerf per object), preventing us from using a large number of evaluation objects. Moreover, our evaluation settings is mainly borrow from Zero123, which only use 20 objects in GSO (as well as most concurrent works). \n\nTo demonstrate more convincing results, we extend the number of evaluation objects to 1k in the following table, which shows that the proposed model consistently outperforms baselines by a large margin.\n\n|    Method     | PSNR (100) | SSIM (100) | LPIPS (100) | PSNR (1000) | SSIM (1000) | LPIPS (1000) |\n| :-----------: | :--------: | :--------: | :---------: | :---------: | :---------: | :----------: |\n|    Zero123    |   21.72    |    0.92    |    0.23     |    20.90    |    0.89     |     0.25     |\n| Zero123 + SC  |   22.09    |    0.92    |    0.21     |    20.78    |    0.89     |     0.24     |\n| Consistent123 |   24.98    |    0.96    |    0.14     |    23.89    |    0.94     |     0.16     |"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1971/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700538318553,
                "cdate": 1700538318553,
                "tmdate": 1700538318553,
                "mdate": 1700538318553,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xwmXznLOV3",
            "forum": "wT8G45QGdV",
            "replyto": "wT8G45QGdV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1971/Reviewer_4aQC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1971/Reviewer_4aQC"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduced a way that improves novel view synthesis model, e.g. zero123 by considering a multiview input in the diffusion model. While consider a shared self-attention machanism that all views  query the same key and value from the input view, which provides detailed spatial layout information for novel view synthesis.  In the method, It supports Arbitrary-length Sampling and adopted Progressive Classifier-free Guidance, yielding a further improvement of the synthesis. \n\nThe resulting novel views looks more consistent than baseline. And from the supplimentary material."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The multiview input to the diffusion is good in achieving geometric consistency, comparing against zero123 base model. \n\n2. The design of progressive scheduler is interesting by jointly considering the benefit from large cfg vs small cfg, which leverage between texture and geometry. \n\n3. The paper demonstrates through qualitative and quantitative experiments that Consistent123 significantly outperforms baselines, zero123 in particular, in view consistency, showcasing substantial improvement in various downstream tasks."
                },
                "weaknesses": {
                    "value": "Novelty is clear, while there are several publications available with open-sourced papers. Such as magic123,  zero123 xl, sync-dreamer.   for synthesizing new views and do the 3D reconstruction using SDS or direct pixel loss based on NeuS. Wonder the author may compare the results with the opensourced recon-models. \n\nFrom the experimental results after 3D reconstruction. It looks like a black biased back side are generated. Which in my perspective, they are no better than the pulished methods such as that has been implemented in threestudio [url: https://github.com/threestudio-project/threestudio] [which is available before the submission].  The autho may explain why the reconstructed results"
                },
                "questions": {
                    "value": "1. The generated views are still not fully consistent before the 3D model is reocnstructed, while the renderred image after reconstruction looks much worse.  is there any thoughts in further improve the consistency so the quality gap between diffused output and render-view can be minimized ? \n\n2. How it generalizes towards more sophisticated images ?  Please also provide some faliure cases for a better understand of the limitations."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1971/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698770911823,
            "cdate": 1698770911823,
            "tmdate": 1699636128532,
            "mdate": 1699636128532,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KegvP1NVCP",
                "forum": "wT8G45QGdV",
                "replyto": "xwmXznLOV3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1971/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1971/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer 4aQC"
                    },
                    "comment": {
                        "value": "> Can Consistent123 be compared with recent work magic123, zero123 xl, sync-dreamer?\n\n- Magic123 is an SDS-based method for image-to-3D based on Zero123, which can be regarded as a downstream application of Consistent123. Therefore, we show the experiment results of Magic123 based on Consistent123 rather than directly comparing with it. Please refer to the video supplementary (magic123.mp4) and Figure 17 for more details.\n\n- Both Zero123 XL and Syncdreamer are used for novel view synthesis, so we directly compare them in synthesized multi-views. Note that Syncdreamer is constrained at fixed poses, thus we only compare with it qualitatively. Please refer to supplementary Figure 12 for a qualitative comparison with these baselines.\n\nBesides the qualitative comparison in Figure 17 and Figure 12, we also evaluate the consistency score on GSO dataset for the quantitative comparison.\n\n|    Method     | PSNR  | SSIM | LPIPS |\n| :-----------: | :---: | :--: | :---: |\n|    Zero123    | 22.88 | 0.92 | 0.25  |\n|  Zero123 XL   | 24.13 | 0.94 | 0.21  |\n| Consistent123 | 27.98 | 0.98 | 0.11  |\n\n> Why the reconstructed results are no better than the published methods in threestudio?\n\nThe reconstruction results of dreamfusion are highly dependent on hyper-parameter tuning. For a fair comparison, all the dreamfusion experiments shown in the paper follow the same hyper-parameter settings without further per-case tuning (and note that all these experiments are also conducted under the implementation of threestudio). As for some failure textures, we could use stronger optimization-based reconstruction methods (e.g., Magic123) to improve the quality. To further demonstrate the effectiveness of Consistent123, we show better 3D reconstruction results using Magic123 in the video supplementary (magic123.mp4) and Figure 17.\n\n> Are there any thoughts on further improving the consistency so the quality gap between diffused output and render-view can be minimized?\n\n- First, introducing geometry-aware conditioning may help. For instance, spatial volume or epipolar attention can be used to further enhance the consistency. However, such architecture may reduce the quality and flexibility of synthesized views (e.g., the number and pose of view must be fixed at inference).\n- Second, a better data filtering strategy may help. There are lot of low-quality data in our training dataset Objaverse as indicated by other concurrent methods (e.g., Instant3D). Training on the full dataset of Objaverse may result in oversmoothed textures and increase the difficulty of being consistent.\n\n> How it generalizes towards more sophisticated images?\n\nMore sophisticated cases are shown in supplementary Figure 13, indicating that Consistent123 can be generalized to sophisticated images while preserving good consistency.\n\n> Please also provide some failure cases for a better understanding of the limitations.\n\nFailure cases are provided in supplementary Figure 14. The main problem falls on the latent diffusion decoder, which struggles to reconstruct some complicated pattern of input images (e.g., text, thin objects). A direct solution is to use a stronger latent decoder like the Dalle3 decoder (OpenAI). Using a cascaded diffusion model may be another solution."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1971/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700538252139,
                "cdate": 1700538252139,
                "tmdate": 1700538252139,
                "mdate": 1700538252139,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "N1B40YotHN",
            "forum": "wT8G45QGdV",
            "replyto": "wT8G45QGdV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1971/Reviewer_bDyq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1971/Reviewer_bDyq"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel method to synthesize a set of images of any objects from novel view given a single image as input. One of many challenges in this task is to generate consistent images in terms of geometry and appearance. To this end, the authors propose to generate multiple images simultaneously and enable cross-attention between novel images at different viewpoints. To strike a balance between geometry and texture of generated images, they also propose a progressive Classifier Free Guidance (CFG) after observing that a larger CFG often leads to better geometry but poor texture and a smaller CFG causes an opposite result. Experiments demonstrate that the proposed method outperform a popular baseline,  Zero123, on image synthesize."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Consistency in novel view synthesis is at the core of many image-to-3D task. The proposed method is able to outperform Zero123 by a large margin qualitatively and quantitatively.\n- The observation of more generated views improving consistency Is useful to other image- or text-conditioned novel view synthesis works.\n- Paper is generally easy to follow"
                },
                "weaknesses": {
                    "value": "- V_c should be after softmax in Eq. 5.\n- The name \"shared self-attention\" is confusing to me. It in fact is a cross-attention from the novel views to the input view. Why is it called self-attention?\n- Only qualitative results were presented for image-to-3D tasks."
                },
                "questions": {
                    "value": "- How many views were used in the NeuS experiment?\n\n- No texture on the spray bottle in Figure 7?\n\n- Is the Super Mario a failure case since the object has flattened? Could it be related to the progressive CFG? Figure 3 seems to suggest that a large CFG could lead to flat objects."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1971/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698779701183,
            "cdate": 1698779701183,
            "tmdate": 1699636128446,
            "mdate": 1699636128446,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iS1HyHNEtu",
                "forum": "wT8G45QGdV",
                "replyto": "N1B40YotHN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1971/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1971/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer bDyq"
                    },
                    "comment": {
                        "value": "> V_c should be after softmax in Eq. 5\n\nThanks for your careful reading and pointing out the typo. We will fix it in the revised paper.\n\n> Why is the name \"shared self-attention\"? It seems a cross-attention from the novel views to the input view.\n\nThe shared self-attention is similar to the cross-attention between the novel view and input view, but they have several key differences:\n\n- It reuses the pre-trained weight of vanilla self-attention layers in Zero123. \n- It is non-trainable but only changes the calculation of self-attention. \n\nTherefore, we name it \u201cshared self-attention\u201d to distinguish these differences.\n\n> Only qualitative results were presented for image-to-3D tasks.\n\nFollowing the settings of Zero123, we calculate the Chamfer Distance and Volume IoU of image-to-3D reconstruction based on SDS loss. Note that these metrics only evaluate the geometry quality, ignoring the texture improvement of Consistent123 (which is more significant compared with geometry improvement). Please refer to the updated video supplementary (dreamfusion.mp4) for more qualitative comparisons. \n\n|    Method     | Chamfer Distance \u2193 | Volume IoU \u2191 |\n| :-----------: | :----------------: | :----------: |\n|    Zero123    |       0.0880       |    0.4719    |\n| Consistent123 |     **0.0843**     |  **0.4818**  |\n\n> How many views were used in the NeuS experiment?\n\n3D reconstruction methods like Nerf and Neus typically require a large number of views. Therefore, we use 64 views for all Neus experiments.\n\n> No texture on the spray bottle in Figure 7?\n\nIn the spray bottle case, we deliberately remove the texture from the object to show the synthesized geometry of Neus, which more concentrates on geometry reconstruction. We also add the textured version of this case into the supplementary Figure 15.\n\n> Is the Super Mario a failure case since the object has flattened?\n\nSuper Mario is not a failure case. As a Lego toy, it is \u201cflat\u201d by design (feel free to google for more Lego Mario images). This case also shows that the object geometry can be constructed more reasonably with improved consistency."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1971/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700538200927,
                "cdate": 1700538200927,
                "tmdate": 1700538200927,
                "mdate": 1700538200927,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]