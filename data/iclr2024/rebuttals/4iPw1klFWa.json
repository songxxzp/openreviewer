[
    {
        "title": "Scalable Neural Network Kernels"
    },
    {
        "review": {
            "id": "fcFsLxVQJc",
            "forum": "4iPw1klFWa",
            "replyto": "4iPw1klFWa",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3828/Reviewer_hP8B"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3828/Reviewer_hP8B"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to approximate feedforward layers with kernels, thereby achieving better computational efficiency and sometimes better accuracy. This paper empirically verifies their claim on many vision and language datasets over various architectures."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper is well-written, easy to follow, and empirical results seem strong."
                },
                "weaknesses": {
                    "value": "The main weakness of the paper is that I am not convinced by whether SNNK can practically replace feed forward layers in practice. See Questions for more details"
                },
                "questions": {
                    "value": "1. This paper talks a lot about achieving computational efficiency through dimensionality reduction (if m << d). Could I achieve the same effect by using a feed forward layer but simply reducing the latent dimension from d to m?\n2. Could the authors share empirical evidence that replacing feedforward layers with SNNK indeed results in faster training?\n3. In my understanding if SNNK were to replace feed forward layers then there should be an experiment whether the authors replace every feed forward network in Transformers with SNNK and report the results?\n4. Is there any particular reason as to why the SNNK adapted architectures in Figure 2 look the way they are? In other words, how might one understand the interplay between SNNK and feed forward layers in the same architecture?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3828/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698437478245,
            "cdate": 1698437478245,
            "tmdate": 1699636340571,
            "mdate": 1699636340571,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HgyQlJ19LG",
                "forum": "4iPw1klFWa",
                "replyto": "fcFsLxVQJc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3828/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3828/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their insightful reviews. \n\n**Can SNNK practically replace feed forward layers in practice ?**\n\nWe thank the Reviewer for this excellent question. We have shown settings where we can reliably replace feedforward layers. In fact, in Figure 12, 13, we show the performance, as we replace feedforward layers in Transformers via SNNKs. This matter, at the end of the day, boils down to a trade-off between efficiency/compression vs accuracy (table 6).  \n\n\n**SNNKs vs methods that decrease the dim of the hidden layers:**\n\nWe thank the reviewer for this question. We have the experiment for UCI where we compared SNNK with a three layer MLP and we changed the middle layer size to vary the number of parameters of MLP. We find that at the same parameter size (figure 6),  SNNKs perform better than MLPs with reduced dimensions. \n\nCommonly known methods that can reduce the dimension of the hidden layers (by minimizing the quality loss, as opposed to brute-force layer-size reduction techniques that affect accuracy) are various pruning techniques. In pruning methods, one removes neurons which have small values creating a dense sub-network. However, our method is completely different from pruning as we never remove neurons, rather create a dense representation of the weights via Random Features. Another main difference from pruning methods, which mostly focus on the weights of the network, is that we create a dense projection of the inputs as well. \n\nWe added a line in the introduction contrasting our method with various techniques like distillation, pruning and quantization. \n\n\n**Empirical evidence that replacing feedforward layers with SNNKs results in faster training:**\n\n\nWe conducted experiments demonstrating that during inference, SNNK-based transformer models exhibit lower memory usage and faster inference compared to the original model. Additionally, during training, we observed a reduced memory footprint. These findings are detailed in Section J.4 of the appendix, where we included results showcasing the replacement of FFN with SNNKs. \n\nTo summarize our findings : We show that with 6 layers bundled, the BERT size is 226.71 MB down from 440, and for ViT, it is 176.42 Mb down from 346 Mb (almost a 50% reduction). We also see that on a free Google Colab, running a batch size of 64 with a sequence length of 128, it takes our SNNK-BERT .32 secs compared to .44 seconds to the original BERT model. For SNNK-ViT, it takes .26 seconds to run inference on a batch of 32 images whereas ViT takes .33 seconds. Table 6 provides details of how much increase each additional layer replacement gives in efficiency.\n\n**Replacing every feedforward network in Transformers with SNNKs:**\n\nThank you very much for the comment. We want to emphasize that in order to achieve substantial computational gains, **one does not necessarily need to remove all the feedforward layers**. It is important to emphasize this since, each SNNK approximating a regular feedforward layer introduces an error and that error accumulates over the deep network from layer to layer (see our theoretical discussion above and also Appendix sec B). \n\nHaving said that, as requested, we have conducted additional detailed ablation studies over SNNKs replacing varying numbers of the feedforward layers and present the results below (Please see section J.4 for more details).  If $d$ is the hidden size of the transformer, the FFN block consists of 2 linear layers, one taking the input representation of dimension $d$ to $4d$ with GELU non-linearity and the other taking the intermediate representation down to dim $d$. Our aim here is to replace the first expansion linear layer with non-linearity by small SNNK layers employing only 8 random features. We see from Table 6 about ~40-50% gain in efficiency while from Figure 12 and 13 we see that the tradeoff with accuracy is minimal.\n\n\n**Is there any particular reason as to why the SNNK adapted architectures in Figure 2 look the way they are? In other words, how might one understand the interplay between SNNK and feed forward layers in the same architecture?**\n\nThank you for the excellent question. Figure 2a aligns with our formulation, while the rationale behind the design choice for Figure 2b is detailed in Appendix section D. Regarding MLP (Figures 2c and 2d), we tested several configurations and presented results for the most effective ones. In the case of adapter experiments (Figures 2d and 2e), we adhered to the design outlined in [1].\n\n[1] Parameter-Efficient Transfer Learning for NLP Neil Houlsby, et al. ICML 2019"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3828/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700228307875,
                "cdate": 1700228307875,
                "tmdate": 1700228307875,
                "mdate": 1700228307875,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bYxAqdExTD",
                "forum": "4iPw1klFWa",
                "replyto": "fcFsLxVQJc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3828/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3828/Authors"
                ],
                "content": {
                    "title": {
                        "value": "updated version of the paper and addressed comments"
                    },
                    "comment": {
                        "value": "Dear Reviewer hP8B,\n\nWe would like to once more thank you for your review. We have addressed all the questions and provided corresponding edits in the manuscript (see: updated version). We have conducted an extensive set of additional experiments, in particular accurately measuring the quality of hybrid SNNK-FFL architectures (in terms of the model's accuracy) for configurations with different number of SNNKs and feedforward layers. Furthermore, we have conducted experiments with larger datasets, confirming all our previous findings (see our general comment: \"Summary of the Changes\", for the overview of the changes made in the manuscript). Thus we would sincerely appreciate if the Reviewer considers raising the score.\n\nYours sincerely,\n\nThe Authors"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3828/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700452574463,
                "cdate": 1700452574463,
                "tmdate": 1700452720261,
                "mdate": 1700452720261,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "55Ep950Yam",
                "forum": "4iPw1klFWa",
                "replyto": "bYxAqdExTD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3828/Reviewer_hP8B"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3828/Reviewer_hP8B"
                ],
                "content": {
                    "title": {
                        "value": "Response to Rebuttal"
                    },
                    "comment": {
                        "value": "I sincerely thank the authors for their hard work in producing the rebuttals. I am afraid I am not able to offer any constructive comments at this point. I will keep my score, and leave it for the area chair to decide whether this paper is an interesting contribution."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3828/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700607055672,
                "cdate": 1700607055672,
                "tmdate": 1700607055672,
                "mdate": 1700607055672,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "b9oeLQuzFa",
            "forum": "4iPw1klFWa",
            "replyto": "4iPw1klFWa",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3828/Reviewer_YEib"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3828/Reviewer_YEib"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces scalable neural network kernels (SNNKs), which disentangle the inputs and parameters of a feedforward layer before connecting them via a dot product kernel. The key ideas are:\n\n- SNNKs approximate regular feedforward layers but with reduced parameters by replacing the weight matrix with a low-dimensional embedding. This allows compression of the layer. \n\n- They introduce universal random feature maps to instantiate different SNNK variants based on the Fourier transform of the activation function.\n\n- SNNKs can express more complex relationships beyond standard feedforward layers. They demonstrate this with a ReLU-SNNK layer related to arc-cosine kernels.\n\n- SNNKs enable a neural network bundling process to compactify model architectures. In the extreme case, the entire network can be expressed as a two-tower computation.\n\n- For certain losses like MSE, the optimal parameters of a fully bundled network can be solved in closed form, bypassing backpropagation.\n\n- Experiments validate SNNKs on tasks ranging from kernel approximation to Transformer fine-tuning. SNNK adapters match baseline accuracy with 5x fewer parameters.\n\n- Bundled SNNK models maintain accuracy while reducing parameters 30x. Closed-form solutions for regression produce strong results.\n\nIn summary, the paper provides a thorough theoretical analysis of SNNKs along with empirical validation. The ideas open interesting research directions in model compression, faster training, and expressive power beyond standard neural network layers."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Here are some of the main strengths of this paper:\n\n- It makes an insightful connection between scalable kernel methods and neural network layers, introducing a novel perspective on feedforward layers.\n\n- The concept of SNNKs is very clearly presented along with detailed theoretical analysis and constructions.\n\n- The Fourier transform based universal random feature mechanism to instantiate SNNKs is interesting and useful.\n\n- SNNKs provably increase expressive power over standard layers, as shown through the analysis of the ReLU-SNNK layer. \n\n- The neural network bundling process enabled by SNNKs is an impactful idea for model compression and acceleration.\n\n- The paper provides extensive empirical validation ranging from synthetic data to large Transformer models across vision and language.\n\n- Both model compression and training acceleration are demonstrated convincingly through the experiments.\n\n- The writing is clear, incremental, and easy to follow. Theoretical concepts are explained intuitively.\n\nOverall, the solid theoretical foundation, novel perspectives introduced, and thorough experimentation are major strengths. The paper makes well-motivated connections between areas leading to useful techniques for efficient deep learning."
                },
                "weaknesses": {
                    "value": "Some potential weaknesses or limitations of this paper:\n\n- The focus is on feedforward fully-connected layers, not convolutional or recurrent layers commonly used in modern networks.\n\n- Experiments are limited to standard datasets and models; more complex domains like bioinformatics are not evaluated. \n\n- There is no investigation into how SNNKs affect representation learning or generalization. The emphasis is on compression.\n\n- Optimization and learning dynamics with SNNKs are not analyzed, apart from the fully bundled case.\n\n- The work does not connect to broader topics like kernel methods or metric learning. \n\n- Ablation studies teasing apart the contributions of different components could be more detailed.\n\n- The writing in parts of the theory and experiments lacks clarity or intuitive explanations.\n\n- Practical guidance on hyperparameter selection and tuning SNNKs is limited.\n\n- Applications beyond efficiency, like using SNNKs for privacy or interpretability, are not explored.\n\n- Potential negative results or limitations of SNNKs compared to standard layers are not discussed.\n\nIn summary, the lack of experiments on more complex data and models, limited analysis of learning dynamics, and minimal connections to related areas are notable weaknesses. However, the paper makes excellent contributions within its defined scope."
                },
                "questions": {
                    "value": "Please comment on the issues raised in the weaknesses part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3828/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698443712068,
            "cdate": 1698443712068,
            "tmdate": 1699636340487,
            "mdate": 1699636340487,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oYBW4XnD9q",
                "forum": "4iPw1klFWa",
                "replyto": "b9oeLQuzFa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3828/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3828/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer : Part-I"
                    },
                    "comment": {
                        "value": "We are happy the Reviewer admits that the paper provides a thorough theoretical analysis along with extensive empirical verification and that the ideas open interesting research direction in model compression, faster training and expressive power beyond standard NNs.\n\n**The focus is on feedforward fully-connected layers, not convolutional or recurrent layers commonly used in modern networks.**\n\nWe would like to emphasize that feedforward layers are some of the building blocks of virtually all existing neural networks, including RNNs, CNNs and Transformers. In RNNs, feedforward layers are used in particular to define the gating mechanism. In CNNs, they often constitute the last layers of the models (containing most of the parameters of the model). Thus SNNKs have an impact also on those architectures. In this paper, we have focused particularly on Transformers, since they became new SOTA also in domains, where RNNs or CNNs were traditionally applied (e.g. NLP and vision). \n\n**Experiments are limited to standard datasets and models; more complex domains like bioinformatics are not evaluated.**\n\nThank you for the question. Following Reviewer\u2019s comment, we have conducted additional experiments, **confirming all our previous findings**. In particular, we have added experimental results on a benchmark dataset like ImageNet (figure 4 right) as well as additional uptraining experiments (where we remove a certain number of MLP layers) in transformers (section 4.4 and Appendix section J.4). \n\n\n**There is no investigation into how SNNKs affect representation learning or generalization. The emphasis is on compression.**\n\nThank you very much for this interesting question. We would like to emphasize that the considered setting, where SNNKs are applied in the fine-tuning stage, providing computational benefits, is critical for the majority of the applications of Transformers leveraging already pre-trained models. Thus we do think that this scenario provides the most direct positive impact to the Machine Learning community. Needless to say, testing SNNKs for pre-training foundational models and testing  generalization properties of those models is definitely a relevant direction for future work.\nWe do believe that SNNKs might indeed have a positive impact on the generalization properties of neural networks since they lead to more compact neural network architectures (while preserving high quality). The compactification of the number of parameters might alleviate the overfitting problem and indeed, this direction was recently explored in the context of adapters (providing by definition compact parameterization) and Robotic controllers (see: for instance RoboAdapter project https://sites.google.com/view/robo-adaptersv). \n\nIt is also worth emphasizing that the randomness coming from the random feature mechanism of SNNKs might also have a positive impact on generalization, as leading to training more robust models (if for example different phases or iterations of training apply freshly sampled sets of projections). The phenomenon of robustifying the models via various randomization strategies in training is a large area of research and SNNKs naturally provide such a mechanism. As mentioned above, we leave detailed analysis of the generalization properties of SNNKs to future work.\n\n**Optimization and learning dynamics with SNNKs are not analyzed, apart from the fully bundled case.**\n\nThank you very much for the question. We would like to sincerely ask the Reviewer to clarify what they mean by the \u201coptimization and learning dynamics with SNNKs\u201d. Our extensive empirical verification clearly shows that SNNK-based models can be efficiently trained with standard optimization methods across a wide range of tasks (involving both: Finetuning Transformers, Uptraining Transformers and attention-free neural networks) and thus do not require any customized optimization techniques."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3828/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700228605699,
                "cdate": 1700228605699,
                "tmdate": 1700228605699,
                "mdate": 1700228605699,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8s1AVb0JtD",
                "forum": "4iPw1klFWa",
                "replyto": "b9oeLQuzFa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3828/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3828/Authors"
                ],
                "content": {
                    "title": {
                        "value": "updated version of the paper and addressed comments"
                    },
                    "comment": {
                        "value": "Dear Reviewer YEib,\n\nWe would like to once more thank you for your review. We have addressed all the questions and provided corresponding edits in the manuscript (see: updated version). In particular, we have conducted an extensive set of additional experiments and clarified the limitations of the method (see our general comment: \"Summary of the Changes\", for the overview of the changes made in the manuscript). Thus we would sincerely appreciate if the Reviewer considers raising the score.\n\nYours sincerely,\n\nThe Authors"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3828/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700451723486,
                "cdate": 1700451723486,
                "tmdate": 1700452685214,
                "mdate": 1700452685214,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NK3YTih5Z1",
                "forum": "4iPw1klFWa",
                "replyto": "b9oeLQuzFa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3828/Reviewer_YEib"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3828/Reviewer_YEib"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal"
                    },
                    "comment": {
                        "value": "I'd like to thank the authors for their detailed responses. However, some of concerns still stays the same and therefore I'd like to keep my original score."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3828/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700588223350,
                "cdate": 1700588223350,
                "tmdate": 1700588223350,
                "mdate": 1700588223350,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AXG63FS5yl",
                "forum": "4iPw1klFWa",
                "replyto": "b9oeLQuzFa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3828/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3828/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Unanswered questions/concerns"
                    },
                    "comment": {
                        "value": "Dear Reviewer, \n\n Thank you for your response. Would it be possible for you to let us know which of your concerns/questions are not satisfactorily answered so that we can try our best to answer them? \n\n Yours sincerely, \n\n The Authors."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3828/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700616523094,
                "cdate": 1700616523094,
                "tmdate": 1700616579558,
                "mdate": 1700616579558,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "j7zMZk8Rau",
            "forum": "4iPw1klFWa",
            "replyto": "4iPw1klFWa",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3828/Reviewer_uu7e"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3828/Reviewer_uu7e"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces Structurally Neural Network Kernels (SNNK), a novel approach to modeling interactions in neural networks. By exploiting the low-rank nature of neural networks, SNNKs offer a significant reduction in parameter count. When used in multilayer perceptrons and transformer models, SNNKs consistently outperformed traditional baselines across multiple datasets, including synthetic data, toy experiments, UCI datasets, GLUE, and CIFAR. A primary advantage is the reduction in storage requirements without compromising accuracy. The work suggests SNNK layers can be integrated seamlessly with other popular techniques, providing both efficiency and enhanced performance, making them a promising tool for neural network architectures."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The paper introduces a new computational model, the scalable neural network kernels (SNNK), providing a novel approach to efficient neural network design, particularly for replacing feedforward layers in MLPs.\n\nThe design of SNNKs ensures that inputs and parameters are disentangled, leading to efficient final computations via a dot-product kernel, which can greatly reduce computational overhead.\n\nThe bundling process highlighted in the paper leads to the compactification of the neural network stack, suggesting potential storage savings and efficiency improvements.\n\nThe paper does not rely solely on theoretical claims but provides empirical analysis, spanning from pointwise kernel estimation to practical application scenarios like training Transformers with adapters, strengthening the validity of the proposed methodology."
                },
                "weaknesses": {
                    "value": "The authors should provide some explanation or intuition why their model doesn\u2019t work well in the some of the experiments they have performed.\n\nThe analysis of how deep of a feed forward network can be approximated using the proposed method should be analyzed in further details. \n\nCan scalable neural network kernel be applied in any scenario or there are some specific scenarios when SKNN won\u2019t work well. Authors should discuss about such datasets/models. If there is none, then authors should also discuss that. They can include some of the experiments involving ImageNet datasets and other models to further support the claim.  \n\nIt would be great to analyse how well the SKNN generalizes to the unseen datasets or in general generalizability as compared to normal network network,"
                },
                "questions": {
                    "value": "I have mentioned my concerns I the weakness. Authors can go over the points mentioned in the weakness and clarify them."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3828/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698796972328,
            "cdate": 1698796972328,
            "tmdate": 1699636340387,
            "mdate": 1699636340387,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sG5iH8ie13",
                "forum": "4iPw1klFWa",
                "replyto": "j7zMZk8Rau",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3828/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3828/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer"
                    },
                    "comment": {
                        "value": "We would like to thank the Reviewer for kind words. We are very happy that the Reviewer appreciated the novelty of our approach.\n\n**Providing intuition why \u201ctheir model doesn\u2019t work in some of the experiments they have performed\u201d:**\n\nWe thank the reviewer for this insightful question. On some GLUE datasets, our pooler-SNNK models were underperforming the baseline. Those datasets are some of the largest ones in the benchmark. We believe that reducing the number of training parameters are causing the models to underfit and we observed similar behavior in our adapter experiments as well. We did ablations by increasing the number of random features and we are able to match the baselines (figure 5 & 8) . \n\n**Limitations of the SNNKs:**\n\nAs we can see from the sec 4.4 & J.4, replacing multiple layers of FFL with SNNK results in a tradeoff between accuracy and efficiency. This tradeoff is a limitation of our current work and an open question for future research. \n\nWe have added a section discussing the limitations of our method (Appendix Sec N). To summarize : \n\nTo summarize : \n\nThe computation of Universal Random Features (URFs) relies heavily on Fourier Transforms (FTs) of activation functions. But some functions used in practice, like ReLU, do not have well-behaved FTs, leading to errors when smoothed or truncated. Approximating feedforward layers with random features can also introduce errors propagating through subsequent layers, impacting accuracy and efficiency. Replacing FFL layers with SNNK involves a trade-off between accuracy and efficiency, posing a limitation and a future research question. Despite relying on the smoothness of activation functions for kernel approximation, using proxy functions and well-conditioned weights generally works in practice, as long as they're not excessively spiky or sparse.\n\n**ImageNet experiments:**\n\nThank you very much for the comment. As requested, we have conducted additional experiments, in particular on the ImageNet data. They confirm all our previous findings. In fact on ImageNet, *our SNNK-adapter-ViT achieves a top 1%-accuracy of 83.44, beating the baseline adapter-ViT by .23 (Figure 4 (right)).*\n\n**Generalization of SNNKs to the unseen datasets or as compared to normal NNs:**\n\nThank you very much for the question. We do believe that SNNKs might indeed have a positive impact on the generalization properties of neural networks since they lead to more compact neural network architectures (while preserving high quality). The compactification of the number of parameters might alleviate the overfitting problem and indeed, this direction was recently explored in the context of adapters (providing by definition compact parameterization) and Robotic controllers (see: for instance RoboAdapter project https://sites.google.com/view/robo-adaptersv). \n\nIt is also worth emphasizing that the randomness coming from the random feature mechanism of SNNKs might also have a positive impact on generalization, as leading to training more robust models (if for example different phases or iterations of training apply freshly sampled sets of projections). The phenomenon of robustifying the models via various randomization strategies in training is a large area of research and SNNKs naturally provide such a mechanism. We leave detailed analysis of the generalization properties of SNNKs to future work."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3828/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700228129498,
                "cdate": 1700228129498,
                "tmdate": 1700228129498,
                "mdate": 1700228129498,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Ioz9uV39qo",
            "forum": "4iPw1klFWa",
            "replyto": "4iPw1klFWa",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3828/Reviewer_zChr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3828/Reviewer_zChr"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces Scalable Neural Network Kernels (SNNKs), a novel alternative to regular feedforward layers (FFLs) in neural network architectures. These SNNKs, while approximating the behavior of FFLs, bring in computational advantages by separating the inputs from the neural network's parameters and then connecting them through a dot-product kernel.\n\nThe primary contribution is the conceptualization of SNNKs that can mimic FFLs but have better computational attributes. Unlike traditional FFLs, these kernels can capture complex relationships beyond just the functions of the dot-products of parameter-input vectors.\n\nThe authors propose a bundling process utilizing SNNKs to condense the architecture of deep neural networks. This leads to compression benefits, and when fully implemented, it results in a bundled network. Interestingly, for specific loss functions like mean squared error, optimal parameters for this bundled network can be explicitly derived, potentially bypassing the need for backpropagation.\n\nAn auxiliary outcome of the research is the introduction of a mechanism called \"universal random features,\" which is instrumental in formulating various SNNK variants. This mechanism also holds significance in scalable kernel methods.\n\nThe paper goes on to offer a thorough empirical evaluation of the proposed ideas, ranging from point-wise kernel estimation to the fine-tuning of Transformers using new adapter layers inspired by SNNKs. Remarkably, their method achieves up to a 5x reduction in trainable parameters while retaining competitive accuracy."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The paper introduces the concept of Scalable Neural Network Kernels (SNNKs), a fresh take on neural network architecture. This novel approach to approximating regular feedforward layers (FFLs) with computational benefits showcases a high degree of originality. The \"neural network bundling process\" and the notion of a fully bundled network present innovative methods for condensing deep neural network architectures. The \"universal random features\" mechanism, which aids in the formulation of various SNNK variants, is another original contribution.\n\nThe research maintains a high standard of quality, underpinned by a combination of rigorous theoretical foundations and empirical evaluations. Extensive experiments have been conducted across various architectures and datasets, ensuring that the proposed methods are tested in diverse scenarios. The results, especially the reduction in trainable parameters without significant performance losses, stand testament to the quality of the work.\n\nThe paper is structured well, with a clear delineation between theoretical concepts, methodologies, and experimental results. While the document is dense with technical details, the authors have made efforts to explain concepts clearly, aided by visual representations where necessary. The inclusion of a comprehensive list of references and contextualization relative to prior work adds to the clarity, helping readers understand the evolution and significance of the presented ideas.\n\nThe versatility of SNNKs, as demonstrated by their applicability in various architectures (from PINNs to Transformers), signifies their broad utility. By addressing the computational challenges associated with traditional kernel methods and FFLs, the paper offers solutions that could pave the way for more efficient and scalable neural network models in the future."
                },
                "weaknesses": {
                    "value": "The paper could benefit from a more direct comparison of SNNKs with other existing solutions or methods aimed at network compression or efficiency. Highlighting the unique advantages of SNNKs over these methods would further solidify its significance.\n\nThe paper could delve deeper into the robustness of the SNNK approach. Are there scenarios where the approximation might break down? Understanding the edge cases and potential pitfalls would be crucial for practitioners looking to adopt this method.\n\nProviding more explicit details about the implementation, hyperparameters used, or potential challenges faced during the experiments would be beneficial for researchers aiming to replicate or build upon the work."
                },
                "questions": {
                    "value": "The paper mentions that SNNKs can approximate FFLs. Could you provide more insight into the approximation error? In what scenarios might the approximation be suboptimal, and how does the error scale with the depth or complexity of the network?\n\nit would be helpful to understand more about the bundling process's efficiency. How does the network's performance vary as more layers are bundled, especially in deeper architectures?\n\nIn the experiments where SNNKs achieved up to a 5x reduction in trainable parameters, were there any notable trade-offs in terms of latency, inference time, or other metrics?\n\nCould you elaborate on the key differences between the \"universal random features\" mechanism and traditional random feature approaches? What are the primary advantages of this new mechanism?\n\nWhen applying SNNKs to Transformers, especially in the pooler layer linearization, were there any specific challenges or nuances encountered, given the attention mechanisms and positional encodings in such models?\n\nGiven the focus on computational efficiency, were there any hardware-specific optimizations or considerations when implementing SNNKs? How do SNNKs perform across different hardware platforms, like CPUs, GPUs, and TPUs?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3828/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698804605990,
            "cdate": 1698804605990,
            "tmdate": 1699636340306,
            "mdate": 1699636340306,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Qgn89jUhPU",
                "forum": "4iPw1klFWa",
                "replyto": "Ioz9uV39qo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3828/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3828/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Reviewer: Part-I"
                    },
                    "comment": {
                        "value": "We would like to thank the Reviewer for kind words. We are very happy that the Reviewer appreciated the novelty of our approach.\n\n**Comparing SNNKs with other existing solutions or methods aimed at network compression or efficiency:**\n\nWe thank the Reviewer for the interesting question. Some of the most prominent methods that are used for compression or efficiency on a regular basis are various quantization techniques [1,2,3] and distillation methods [4]. Our work is orthogonal to these techniques and can be straightforwardly combined with such methods to provide further gains in compression and model efficiency. We leave this direction to future work. \n\nWe have added a couple of lines in the introduction contrasting our work with various compression techniques.\n\nMoreover, we compare our SNNK untrained BERT with DistillBERT [5]. In this setting, we replaced the top k MLPs in BERT with SNNK. Additional experiments along with accuracy/efficiency tradeoffs are detailed in Appendix sec J.4. \n\n| Model                  | Parameters | CoLA  | MNLI  | MRPC  | QNLI  | QQP   | RTE   | SST-2 | STS-B |\n|------------------------|:----------:|-------|-------|-------|-------|-------|-------|-------|-------|\n| DistilBERT             |     66M    | 51.3  | 82.2  | 87.5  | 89.2  | 88.5  | 59.9  | 91.3  | 86.9  |\n| SNNK-BERT top-6 layers |     69M    | 49.86 | 81.23 | 87.92 | 87.41 | 90.56 | 58.12 | 90.48 | 87.19 |\n| SNNK-BERT top-5 layers |     76M    | 56.58 | 81.94 | 88.67 | 87.81 | 90.68 | 59.93 | 91.4  | 87.43 |\n\n\n1. A Survey of Quantization Methods for Efficient Neural Network Inference; Amir Gholami , Sehoon Kim , Zhen Dong , Zhewei Yao , Michael W. Mahoney, Kurt Keutzer arxiv 2021\n2. Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference; Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, Dmitry Kalenichenko arxiv 2017\n3. Understanding INT4 Quantization for Language Models: Latency Speedup, Composability, and Failure Cases; Xiaoxia Wu, Cheng Li, Reza Yazdani Aminabadi,  Zhewei Yao, Yuxiong He ICML 2023\n4. Knowledge Distillation: A Survey; Jianping Gou, Baosheng Yu, Stephen J. Maybank, Dacheng Tao International Journal of Computer Vision 2021\n5. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter Victor Sanh, Lysandre Debut Julien Chaumond, Thomas Wolf 2019"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3828/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700227259431,
                "cdate": 1700227259431,
                "tmdate": 1700227843911,
                "mdate": 1700227843911,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KggAo2gbWd",
                "forum": "4iPw1klFWa",
                "replyto": "Ioz9uV39qo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3828/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3828/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer : Part-II"
                    },
                    "comment": {
                        "value": "**Delving deeper into the robustness of the SNNK approach. Are there scenarios where the approximation might break down? Understanding the edge cases and potential pitfalls would be crucial for practitioners looking to adopt this method.**\n\nWe thank the Reviewer for an insightful question. The key ingredient in the computation of URFs is the computation of the Fourier Transform (FT) of the activation function. However, the FT of some of the activation functions used in practice is not well-behaved, e.g. ReLU, see [1] for the derivation of the FT of a \u201csmooth\u201d truncated ReLU. Smoothening and truncating the function incurs an error. Furthermore, approximating regular feedforward layers with random feature techniques incurs other errors that might propagate from one SNNK layer to the next SNNK layer. Even though the approximation of the kernel may depend on the $L^1$ integrability/smoothness of the activation function, we noticed that in practice taking a proxy function to simulate the kernel and learning the weights work well as long as they are not ill-conditioned (i.e. not too spiky or sparse). \n\nWe have added the corresponding Limitations section to the Appendix (Sec. N) and another section detailing the error propagation in a bundled deep neural network (Appendix Sec B).\n\n\n**More details regarding implementation, hyperparameters, potential challenges during experiments**\n\nWe sincerely thank the Reviewer for this question. Details regarding hyperparameter selection are given in the Appendix H.  For different experimental settings, we started with the hyperparameters that other authors have used for that dataset and the given base model for that setting. The only hyperparameter that we really tuned is the learning rate. The key observation here is the following one : if the training is too slow, increasing the learning rate significantly boosts performance.  In some cases, it may take longer to converge so we trained the models till the validation loss started increasing.\n\nOur code is written in PyTorch. For experiments with pooler and up-training runs, we have built a code on the top of Huggingface Transformers repos. For up-training experiments, we replace the intermediate layer with our SNNK and for the pooler, we replace the pooler by the corresponding SNNK. For adapter experiments, we followed the implementation of Transformer-adapters.  \n\nOur entire motivation is to do trade-off between accuracy and efficiency (training and inference). Thus, it was reassuring to see that we can improve performance and batch baselines as we increase the number of random features (figures 5 and 8). \n\nWe will make all our implementations publicly available once our manuscript is published. \n\n\n**Clarification regarding experiments with 5x reduction in trainable parameters:**\n\nFor these experiments, we showed that we can maintain the same level of accuracy with a certain amount of reduction of trainable parameters. There is a trade-off involved as further reduction of parameters may hurt performance. Figure 12 and 13 shows those trade-offs, i.e. lower inference/storage costs vs accuracy. We have also added more experimental details as well discussion about these tradeoffs in section J.4. \n\n\n**Key differences between URFs and traditional RFs:** \n\nThank you for your question. Most traditional RF-approaches rely on the Fourier analysis, but via Bochner Theorem and trigonometric functions. In contrast, URFs apply exponential functions. This approach was found to much more efficiently linearize the softmax-kernel, see: Rethinking Attention with Performers, than regular trigonometric random features (and effectively led to trainable linear-attention Transformers, as opposed to the trigonometric variants). Thus one way of thinking about URFs is as an extension of the method presented in that paper, but for a much larger class of kernels.  \n\nWe have also provided more intuition behind URFs in section A.  \n\n\n**Clarification regarding SNNKs for Transformers (pooler layers\u2019 linearization):**\n\nThere were no specific challenges involved. We mostly followed the hyperparameters that other authors have applied for pooler tuning. The key insight that we realized is that the SNNKs can be trained with a higher learning rate and need a little bit more time to converge. \n\n\n**Hardware-specific optimization or consideration ? Performance across different platforms:**\n\nNo hardware-specific optimization was conducted. We run the experiments on both: GPUs and TPUs. SNNKs are \u201caccelerators-friendly\u201d since their key computational bottleneck is matrix-matrix multiplication. \n\n\n[1] A Corrective View of Neural Networks: Representation, Memorization and Learning Guy Bressler and Dheeraj Nagaraj Conference on Learning Theory 2020."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3828/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700227424670,
                "cdate": 1700227424670,
                "tmdate": 1700227671711,
                "mdate": 1700227671711,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]