[
    {
        "title": "Unsupervised ASR via Cross-Lingual Pseudo-Labeling"
    },
    {
        "review": {
            "id": "ElWSwgcqiH",
            "forum": "4lOWCkhr4g",
            "replyto": "4lOWCkhr4g",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5862/Reviewer_GYQT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5862/Reviewer_GYQT"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces an innovative approach for training unsupervised Automatic Speech Recognition (ASR) systems in low-resource languages, using only unmatched speech and text data. The method takes advantage of the similarity in pronunciation between characters in a paired language to create pseudo-labels for the target language, which are further refined by a language model. This approach demonstrates promising results in both cross-language and cross-family scenarios."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper addresses a practical and underexplored problem. Implementing the proposed method in the pseudo-labeling framework is more straightforward than using GAN-based approaches.\n2. This approach demonstrates promising results in both cross-language and cross-family scenarios.\n3. The performance is further enhanced with the use of multi-lingual source AMs.\n4. The paper is well-written and straightforward to understand."
                },
                "weaknesses": {
                    "value": "1. The results are not particularly encouraging:\n    * It would be more valuable to demonstrate that a single source AM can be effectively applied to multiple target languages (e.g., en -> {sw, ha, X, Y, Z}) rather than showcasing the application of multiple source AMs to the same target language (e.g., {en, es, ge, fr} -> sw):\n    * It appears that this method is effective primarily for the language pairs { *-> sw} and {be -> cs}, which raises concerns about its generalization to other low-resource languages.\n\n2. In contrast to wav2vec-u, the concept of \"similarity\" might restrict the applicability of this method, potentially preventing it from identifying an appropriate source language for a specific target language."
                },
                "questions": {
                    "value": "1. In the paper, it is claimed that \"for African languages, we use Kinyarwanda only as a source language (as text data are not available in the Common Crawl dataset).\" How is the Kinyarwanda AM trained without text data?\n2. Table 3 indicates that the performance of the de degrades when language model decoding is applied. Is there any analysis provided in the paper to explain this"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5862/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5862/Reviewer_GYQT",
                        "ICLR.cc/2024/Conference/Submission5862/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5862/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698530141650,
            "cdate": 1698530141650,
            "tmdate": 1699640046635,
            "mdate": 1699640046635,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rhWvwG9vCA",
                "forum": "4lOWCkhr4g",
                "replyto": "ElWSwgcqiH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5862/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5862/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authours Rebuttal"
                    },
                    "comment": {
                        "value": "We would like to thank Reviewer GYQT for their time. Please find below our comments:\n\n> It would be more valuable to demonstrate that a single source AM can be effectively applied to multiple target languages (e.g., en -> {sw, ha, X, Y, Z}) rather than showcasing the application of multiple source AMs to the same target language (e.g., {en, es, ge, fr} \u2192 sw):\n\nThanks for the interesting suggestion! We will investigate this in the future.\n\n> It appears that this method is effective primarily for the language pairs { *-> sw} and {be -> cs}, which raises concerns about its generalization to other low-resource languages.\n\nWe wish we could have included more low-resource languages settings. However, the main obstacle is the availability of the unlabeled audio data and unpaired text for the same language. Apart Common Voice, we are not aware of many freely available datasets with 30h or more unlabeled audio per language, and large available text corpora.\n\n> In contrast to wav2vec-u, the concept of \"similarity\" might restrict the applicability of this method, potentially preventing it from identifying an appropriate source language for a specific target language.\n\nWe agree. wav2vec-U 2.0 and our approach have different tradeoffs. We trade (phoneme lexicon, GAN training, and large amount of unlabeled target audio) for (simple pseudo-labeling, limited amount of labeled source audio). In practice, as labeled source audio is always available, our approach is most likely applicable. In contrast, wav2vec-U 2.0 requires a phoneme lexicon and large amount of target audio, which may not always be available. As for which source language to choose, zero-shot performance on target language seems to be a good indicator of the cross-lingual transfer difficulty (see Figure 3).\n\n> In the paper, it is claimed that \"for African languages, we use Kinyarwanda only as a source language (as text data are not available in the Common Crawl dataset).\" How is the Kinyarwanda AM trained without text data?\n\nKinyarwanda has labeled audio data (with transcriptions) in Common Voice, which is used to train the source acoustic model for Kinyarwanda. However, no available unpaired text data to train a language model is available. If we had used the audio transcriptions for that matter, then audio and language model would have the same training text data, in which case the problem could be viewed as a simple alignment problem rather than unsupervised ASR.\n\n> Table 3 indicates that the performance of the de degrades when language model decoding is applied. Is there any analysis provided in the paper to explain this\n\nThanks for the question! As German uses compound words, the language model dictionary has limited word coverage. This shows at beam-search decoding time, when unknown words (not in the dictionary) are then decoded into multiple words allowed by the dictionary. We found that if we allow the beam-search decoding to produce unknown words, then it mitigates the issue, at the expense of an additional hyper-parameter. For simplicity, and consistency with other languages which did not require it, we sticked to beam-search decoding restricted to the original dictionary."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5862/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699983899197,
                "cdate": 1699983899197,
                "tmdate": 1699983899197,
                "mdate": 1699983899197,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "W8rXFpEVRz",
                "forum": "4lOWCkhr4g",
                "replyto": "rhWvwG9vCA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5862/Reviewer_GYQT"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5862/Reviewer_GYQT"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the author's response. The only thing I would like to confirm from the author is that for all target languages, there's no overlap between audio transcripts and LM training data (Common Crawl).  If this condition is met, I would prefer to retain the current score."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5862/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700701390972,
                "cdate": 1700701390972,
                "tmdate": 1700701390972,
                "mdate": 1700701390972,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PEg8NwfmW4",
            "forum": "4lOWCkhr4g",
            "replyto": "4lOWCkhr4g",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5862/Reviewer_mAuZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5862/Reviewer_mAuZ"
            ],
            "content": {
                "summary": {
                    "value": "This paper describes an approach to training an ASR model without transcribed speech in a language \u2013 \u201cUnsupervised ASR\u201d.  The central idea is to use an ASR model for some high resource source language to generate hypotheses for the target language, and integrate target language information through an LM. Then iterative pseudolabeling can be used to refine performance.  The authors show competitive performance.  The authors also demonstrate performance if the source and target language do not share a writing script."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The clearest strength of this approach is its simplicity.  The technical approach uses mostly off-the-shelf, well understood techniques to solve a challenging task.\n\nEvaluations are largely well done across a variety of language families (though more limited than other work on unsupervised ASR)"
                },
                "weaknesses": {
                    "value": "It would be good to compare performance on more languages, prior work has investigated FLEURS.  Comparing performance on this data set would enable clearer comparisons.\n\nThere is a reliance on the unidecode to \u201cromanize\u201d the character sets of various languages to Latin script.  The recognition performance in a language should be in its own script, not a romanized version.  It does not seem as though unidecode inverted prior to CER calculation. (Though I suppose this could also be a \u201cquestion\u201d rather than a \u201cweakness\u201d"
                },
                "questions": {
                    "value": "Is the example in Figure 1 a real example or made up to demonstrate idealized behavior? This wasn\u2019t clear from the context.\n\nIn Section 2.0 it is claimed that \u2018unsupervised ASR is viable, as long as source and target language share enough \u201dsimilarities\u201d\u2019.  Which similarities are critical for performance here? Acoustic, lexical, other?\n\nJust a note for Section 5.6 \u2013 the title describes transfer across \u201cAlphabets\u201d, however, not all writing systems are alphabets. Would this approach extend to abugida, abjad or logographic writing systems?\n\nNote: it would be nice if Figure 4 used the same Y axis in both tables.  (It\u2019s more understandable that the X axis varies)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5862/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698771210724,
            "cdate": 1698771210724,
            "tmdate": 1699636620669,
            "mdate": 1699636620669,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TaV6dUuu0F",
                "forum": "4lOWCkhr4g",
                "replyto": "PEg8NwfmW4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5862/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5862/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authours Rebuttal"
                    },
                    "comment": {
                        "value": "We would like to thank Reviewer mAuZ for their time. Please find below our comments:\n\n> It would be good to compare performance on more languages, prior work has investigated FLEURS. Comparing performance on this data set would enable clearer comparisons.\n\nAs prior work on unsupervised ASR did not report numbers on FLEURS, we did not consider it, as we would not be able to compare with them. Also, as far as we know FLEURS is used to compare multilingual systems trained on data of internet-scale, which is not the type of system trained in this paper.\n\n> There is a reliance on the unidecode to \u201cromanize\u201d the character sets of various languages to Latin script. The recognition performance in a language should be in its own script, not a romanized version. It does not seem as though unidecode inverted prior to CER calculation. (Though I suppose this could also be a \u201cquestion\u201d rather than a \u201cweakness\u201d\n\nWe used unidecode only to \u201cromanize\u201d characters which are not in the native character set of each of the language considered. In practice, we found this occurs rarely (< 0.01%). Note that Common Voice datasets come unnormalized (for example, they contain punctuation), and using unidecode is a way to normalize the Common Voice datasets in a generalizable manner (across different languages), with little effect on the WER (given those events occur rarely). We will release the normalization procedure for reproducibility. \n\n> Is the example in Figure 1 a real example or made up to demonstrate idealized behavior? This wasn\u2019t clear from the context.\n\nIt is a real example from the es \u2192 en model. We will clarify it.\n\n> In Section 2.0 it is claimed that \u2018unsupervised ASR is viable, as long as source and target language share enough \u201dsimilarities\u201d\u2019. Which similarities are critical for performance here? Acoustic, lexical, other?\n\nBased on our empirical results in Figure 3, languages which are known to have acoustic similarity transfer better. In addition, if the target language has simple pronunciation rules (like Spanish), transfer works better in practice. In contrast, French (known for its highly irregular orthography (Adda-Decker et al., 2005)) is a difficult target language, as inferring those rules is hard.\n\nZero-shot performance on target language seems to be a good indicator of the cross-lingual transfer difficulty (see Figure 3).\n\n> Just a note for Section 5.6 \u2013 the title describes transfer across \u201cAlphabets\u201d, however, not all writing systems are alphabets. Would this approach extend to abugida, abjad or logographic writing systems?\n\nUsing an adequate transliteration system, one could convert logographic writing systems to other alphabets, and still use our approach. For example, converting Chinese hanzi \u2192 pinyin (e.g. \u201d\u4f60\u597d\u201c \u2192 \u201dni hao\u201c) or Japanese kanji \u2192 romaji (e.g. \u201d\u65e5\u672c\u201c \u2192 \u201dNihon\u201c). As the point of the paper is to show how and when end-to-end unsupervised ASR via cross-lingual PL can work, we did not investigate cases where complex transliteration systems are required though. We will change \u201dacross alphabets\u201c in the section title to \u201dacross writing systems\u201c.\n\n> Note: it would be nice if Figure 4 used the same Y axis in both tables. (It\u2019s more understandable that the X axis varies)\n\nThanks for the suggestion! We will fix it."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5862/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699983704406,
                "cdate": 1699983704406,
                "tmdate": 1699983704406,
                "mdate": 1699983704406,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OBFE0evlsK",
            "forum": "4lOWCkhr4g",
            "replyto": "4lOWCkhr4g",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5862/Reviewer_26fF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5862/Reviewer_26fF"
            ],
            "content": {
                "summary": {
                    "value": "This paper tackles multilingual ASR, especially for dealing with unsupervised training in target languages given source languages' ASR systems trained on supervised source data. The idea is simple but powerful. The method uses PL techniques but extends it for a transfer learning scenario from source to target languages. The paper investigates this transfer learning capability with various dimensions (e.g., across the language, across the language family, variants of PL methods, amount of labeled source data and unlabelled target data, the use of LMs, etc.). With these investigations, the method finally achieved sufficient performance in some language pairs. Although most languages are based on the standard Latin scripts, the paper also shows the potential of applying this method to target languages with unseen scripts with the help of a transliteration technique."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Multilingual ASR is an important research topic to bridge the digital divide in underrepresented regions.\n- The proposed method does not require labeled data for the target languages\n- Intensive analyses of the proposed methods with various dimensions"
                },
                "weaknesses": {
                    "value": "- Although the topic is very important, the technique itself does not have sufficient novelty as an ML conference paper. The topic is specific to ASR, and the technique is based on one particular ASR method (i.e., CTC). The connection to general ML problems is not clear.\n  - for example, some experimental results (e.g., the use of n-gram LMs, etc.) are specific to CTC, and it does not seem to be generalized to the other architectures.\n- Most experimental findings are expected, and there are not so many new findings (e.g., it's a bit trivial that large data help the performance, etc.).\n- The methodology is not very new. Although there are several differences, some prior studies try to transcribe unseen languages with seen language ASR systems (e.g., Hasegawa-Johnson, Mark A., et al. \"ASR for under-resourced languages from probabilistic transcription.\" IEEE/ACM Transactions on Audio, Speech, and Language Processing 25.1 (2016): 50-63.)."
                },
                "questions": {
                    "value": "- Did you use SSLs? Since SSLs are obtained by unsupervised training, combining this method and SSL would be very powerful.\n- Can you explain why this method uses CTC? Is there a particular reason, or is this method applicable to the other methods (e.g., HMM, attention-based encoder-decoder, RNN-T)? I think that this part makes the paper's scope narrow.\n- Section 4.1 \"(iii) converting characters into the Latin token set via unidecode3 package; characters failing the conversion were discarded from the text\": Can you describe the examples? Also, I'm concerned that if we discard some characters in the reference, we cannot evaluate the performance validly with the other reports. Can you clarify this part?\n- Section 5.7: It is difficult to conclude since they are very different to compare (e.g., I'm not sure which one is more difficult using 800h of labeled German data and 60k hours of unlabeled English data). Can you explain the benefits of your method more clearly?\n\nOther suggestions\n- For me, the method is a little bit over-claimed since this method is primarily applicable to languages with the same scripts or with transliteration systems. It would not be easy to apply this method to the ideogram languages (e.g., Chinese and Japanese, although they are rich-resource languages, and we can build ASR systems easily). I think the paper requires some discussion about it (e.g., adding it to the DISCUSSION AND LIMITATIONS section?).\n- Section 2.2: $(\\alpha > 0)$ suddenly appears. This should be shown around Eq. (1), where $\\alpha$ first appears.\n- Section 4, first paragraph: These experimental setups are difficult to follow, as they are very diverse. I recommend you describe the design of the experiment (or the intention of what you want to show) when you describe each experimental setup.\n- Figure 3 is too small... Please improve it."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5862/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699030389436,
            "cdate": 1699030389436,
            "tmdate": 1699636620557,
            "mdate": 1699636620557,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pGlixb5HCu",
                "forum": "4lOWCkhr4g",
                "replyto": "OBFE0evlsK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5862/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5862/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authours Rebuttal"
                    },
                    "comment": {
                        "value": "We would like to thank Reviewer 26fF for their time. Please find below our comments:\n\n> Although the topic is very important, the technique itself does not have sufficient novelty as an ML conference paper. The topic is specific to ASR, and the technique is based on one particular ASR method (i.e., CTC). The connection to general ML problems is not clear. for example, some experimental results (e.g., the use of n-gram LMs, etc.) are specific to CTC, and it does not seem to be generalized to the other architectures.\n\nUnsupervised **end-to-end** ASR that practically works is a significant novelty brought by this paper. End-to-end ASR alleviates the need for any phoneme lexicon required in other successful unsupervised ASR approaches (like wav2vec-U 2.0). Another notable difference with wav2vec-U 2.0 is that we do not rely on adversarial training at all. We will highlight better these two points in the paper.\n\nWe truly think that speech-specific papers have their place at ICLR. Prominent work in ASR such as wav2vec 2.0 or wav2vec-U were published at NeurIPS and Whisper was published at ICML.\n\nOur method is generalizable to any acoustic or language models which can output posterior probabilities (see Eq. 1). There is nothing specific in our approach which limits ourselves to CTC (or Transformers acoustic models). \n\n> Most experimental findings are expected, and there are not so many new findings (e.g., it's a bit trivial that large data help the performance, etc.).\n\nAs stated above, unsupervised end-to-end ASR that practically works is not trivial, and has not been fully addressed in previous literature.\n\n> The methodology is not very new. Although there are several differences, some prior studies try to transcribe unseen languages with seen language ASR systems (e.g., Hasegawa-Johnson, Mark A., et al. \"ASR for under-resourced languages from probabilistic transcription.\" IEEE/ACM Transactions on Audio, Speech, and Language Processing 25.1 (2016): 50-63.).\n\nThanks for the reference. As for most previous work in unsupervised ASR, pointed paper uses ASR with pronunciation dictionaries. In contrast, as stated above, we introduce a practical approach for training end-to-end unsupervised ASR models. In addition, pointed paper i) uses now outdated GMM ASR ii) states that \u201cself-training (ST) [...] costs very little, and benefits little\u201d, while self-training (a.k.a. pseudo-labeling) is a core component of our approach (critical for low WER performance). We will add the reference.\n\n> Did you use SSLs? Since SSLs are obtained by unsupervised training, combining this method and SSL would be very powerful.\n\nIndeed, it has been shown that SSL and pseudo-labeling are complimentary for monolingual semi-supervised ASR settings, see *Xu, Q., et.al. Self-training and pre-training are complementary for speech recognition. ICASSP 2021* and *Zhang, Y., et.al Bigssl: Exploring the frontier of large-scale semi-supervised learning for automatic speech recognition. Journal of Selected Topics in Signal Processing 2022*. We plan to study this in a follow up paper.\n\n> Can you explain why this method uses CTC? Is there a particular reason, or is this method applicable to the other methods (e.g., HMM, attention-based encoder-decoder, RNN-T)? I think that this part makes the paper's scope narrow.\n\nWe picked CTC for simplicity. As mentioned above, our method is generalizable to any acoustic (seq2seq, RNN-T...) or language models which can output posterior probabilities (see Eq. 1)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5862/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699983452566,
                "cdate": 1699983452566,
                "tmdate": 1699983452566,
                "mdate": 1699983452566,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YoXAJJdKck",
            "forum": "4lOWCkhr4g",
            "replyto": "4lOWCkhr4g",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5862/Reviewer_UAMi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5862/Reviewer_UAMi"
            ],
            "content": {
                "summary": {
                    "value": "In this work, the authors propose a cross-lingual unsupervised ASR training framework built on top of the iterative pseudo labeling (IPL) method. They assume a practical situation where unpaired audio and text is available for some low-resource languages and the proposed method is designed to leverage existing source AM (obtained from supervised training on a source language) to generate pseudo labels for the target audio under the regulation of the target LM. The resulting target AM is then iteratively trained on the pseudo labels. Experiments are designed to explore different combination of target & source languages, impact of data size etc. The results demonstrate the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The design of the experiment is comprehensive. The source & target language set covers not just common European languages but also the less common Arican languages such that the cross-family source & target language situation can also be studied.\n2. Comparison with baseline systems such as supervised training and wav2vec-U 2.0 shows that the proposed method is effective in training an ASR model in unsupervised way."
                },
                "weaknesses": {
                    "value": "1. The contribution of this work is limited in cross-lingual scenarios.\n2. Baseline system setting is relatively limited. In the core validation experiments (section 5.2), the baseline is only zero-shot evaluation w/ and w/o target LM plus the fully supervised training. It would be more informative if other unsupervised training methods can be compared side-by-side.\n3. The paper does not demonstrate sufficient novelty. The author sets an assumption: the target language has no labelled speech accessible but has a fair amount of text data to train an LM. The paper reports how an existing IPL method works under this assumption."
                },
                "questions": {
                    "value": "Question:\n1. The LMs seem to be trained and fixed for experiments. Have you tested the correlation between the LM's PPL VS. final WERs?\n\nSuggestion:\n1. Please check and fix typos (e.g. section 5.2: \"any Indo-European language, any Indo-European language\")\n2. Please give pointers to the tables/figures in line. E.g. in section 5.5 \"the results below\" doesn't correctly point to the corresponding table."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5862/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5862/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5862/Reviewer_UAMi"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5862/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699657741283,
            "cdate": 1699657741283,
            "tmdate": 1699657741283,
            "mdate": 1699657741283,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UWcZTFnDeo",
                "forum": "4lOWCkhr4g",
                "replyto": "YoXAJJdKck",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5862/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5862/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authours Rebuttal"
                    },
                    "comment": {
                        "value": "We would like to thank Reviewer UAMi for their time. Please find below our comments:\n\n> The contribution of this work is limited in cross-lingual scenarios.\n\n> Baseline system setting is relatively limited. In the core validation experiments (section 5.2), the baseline is only zero-shot evaluation w/ and w/o target LM plus the fully supervised training. It would be more informative if other unsupervised training methods can be compared side-by-side.\n\nOne of the main contribution of our paper is to show that end-to-end unsupervised ASR is practical (in the sense that it may almost match fully supervised ASR). In contrast, successful previous unsupervised ASR approaches were phoneme-based. We compare with the best available end-to-end unsupervised ASR system, which is wav2vec-U 2.0 on LJSpeech (see Sec. 5.7). \n\n> The paper does not demonstrate sufficient novelty. The author sets an assumption: the target language has no labelled speech accessible but has a fair amount of text data to train an LM. The paper reports how an existing IPL method works under this assumption.\n\nUnsupervised **end-to-end** ASR that practically works is a significant novelty brought by this paper. End-to-end ASR alleviates the need for any phoneme lexicon required in other successful unsupervised ASR approaches (like wav2vec-U 2.0). Another notable difference with wav2vec-U 2.0 is that we do not rely on adversarial training at all. We will highlight better these two points in the paper.\n\nNote also that wav2vec-U 2.0 concludes that \u201cFuture work includes simplifying the self-training pipeline and removing the need for a phonemizer.\u201d We demonstrate in this paper a way to do this.\n\n> The LMs seem to be trained and fixed for experiments. Have you tested the correlation between the LM's PPL VS. final WERs?\n\nCorrect. We did observe a correlation between LM PPL and WER, in the context of n-gram-based word LMs. We also observed that increasing the dictionary LM size (number of words) improved WER. We will report these results.\n\n> Please check and fix typos (e.g. section 5.2: \"any Indo-European language, any Indo-European language\")\n\n> Please give pointers to the tables/figures in line. E.g. in section 5.5 \"the results below\" doesn't correctly point to the corresponding table.\n\nThanks for spotting this, we will fix it!"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5862/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699982940052,
                "cdate": 1699982940052,
                "tmdate": 1699982940052,
                "mdate": 1699982940052,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]