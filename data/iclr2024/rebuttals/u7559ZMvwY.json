[
    {
        "title": "Adversarial Training on Purification (AToP): Advancing Both Robustness and Generalization"
    },
    {
        "review": {
            "id": "r1rWKa5gtV",
            "forum": "u7559ZMvwY",
            "replyto": "u7559ZMvwY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4921/Reviewer_onZy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4921/Reviewer_onZy"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces an adversarial defense technique called Adversarial Training on Purification (AToP). AToP conceptualizes AP into two components: perturbation destruction by random transforms and fine-tu . ning the purifier model using supervised AT. The random transforms aim to enhance the robustness to unseen attacks, while the purifier model reconstructs clean data from corrupted data, ensuring correct classification output regardless of the corruption. The purifier model is fine-tuned through an adversarial loss calculated from the classifier output. The paper presents experiments on CIFAR and ImageNette datasets, comparing AToP with other AT and AP methods. The results demonstrate that AToP achieves good generalization ability against unseen attacks, and maintains standard accuracy on clean examples."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper addresses a significant problem in the field of deep learning, namely the vulnerability of neural networks to adversarial attacks.\n- The proposed AToP technique offers a promising solution that combines the advantages of AT and AP, achieving good robustness, standard accuracy, and robust generalization to unseen attacks.\n- The proposed AToP framework is well-defined, with a clear explanation of its components and their roles in achieving optimal robustness, standard accuracy, and robust generalization to unseen attacks.\u00a0\u00a0\n- The experiments are conducted on CIFAR and ImageNette dataset\u00a0\n- I appreciate the evaluation with BDPA\u00a0\u00a0\u00a0\n- The paper is well-structured, with a clear and concise presentation of ideas."
                },
                "weaknesses": {
                    "value": "- Overall the technical novelty of this work is limited. In essence, AToP combines the orthogonal techniques of AT and AP. \n- The authors mainly evaluate ResNet and Wide-ResNet architecture. Hence it is not clear if this work generalizes to other model architectures. \n- These days, transformer architectures are gaining popularity. How does the proposed AToP perform on such architectures, such as ViT?\n- While I appreciate the evaluation on BPDA, the authors should include a separate section on \u201cobfuscated gradients\u201d, where they follow the guidelines of BPDA and [A] to test for obfuscated gradients.  \n\n[B] On Evaluating Adversarial Robustness; arXiv 2019"
                },
                "questions": {
                    "value": "Please address the points in my weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4921/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4921/Reviewer_onZy",
                        "ICLR.cc/2024/Conference/Submission4921/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4921/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698136299783,
            "cdate": 1698136299783,
            "tmdate": 1700630636074,
            "mdate": 1700630636074,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "djFdLc9AKq",
                "forum": "u7559ZMvwY",
                "replyto": "r1rWKa5gtV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4921/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4921/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We greatly appreciate the comments of the Reviewer onZy. Below are our responses to the questions raised.\n\nA1: In the related work section, we conduct a comprehensive comparison to highlight the distinctions between our method and other existing methods. Considering the limitation of the existing pre-trained generator-based purifier model, where all parameters are fixed and cannot be further improved for known attacks. Regarding this challenge, We propose a novel defense pipeline as shown in following equations.\n\nPre-training:\n\n$L_{\\theta_{g}} =L_{df}(x, \\theta_{g})$\n\n$=\\mathbb{E}[D(x)]-\\mathbb{E}[D(g(t(x),\\theta_g))]+\\mathbb{E}[{\\|x-g(t(x), \\theta_g)\\|}_{\\ell_1}],$\n\nFine-tuning:\n\n$L_{\\theta_{g}} =L_{df}(x', \\theta_{g})+\\lambda \\cdot L_{c l s}(x', y, \\theta_{g}, \\theta_{f})$\n\n$ =\\mathbb{E}[D(x')]-\\mathbb{E}[D(g(t(x'),\\theta_g))]+\\mathbb{E}[{\\|x'-g(t(x'), \\theta_g)\\|}_{\\ell_1}]$\n\n$ +\\lambda \\cdot \\max_{\\delta} CE(y, f(g(t(x'),\\theta_g))), \\quad where \\ x' = x+\\delta.$\n\n\n\nA2: One of the advantages of AP is that once the generator model is fully trained, it will be a plug-and-play module that can be applied to other model architectures.\n\nA3: In the experiments comparing with the state-of-the-art methods, all classifiers used are from the model zoo in RobustBench, and we select two of the most common model architectures for the experiments. To address your question, we recheck all available classifiers. However, we do not find a vanilla transformer-based classifier. There is only one transformer-based classifier with adversarial training (XCiT [1]). Therefore, we conduct experiments on XCiT, which is equivalent to AToP+AT. Following the settings of Table 2 and Table 4, here are the results:\n\n|  | CIFAR-10 | | CIFAR-100 |  |\n|:---:|:---:|:---:|:---:|:---:|\n|  | Standard Acc. | Robust Acc. | Standard Acc. | Robust Acc. |\n| XCiT-S12 [1] | 88.48 | 80.27 | 73.44 | 52.73 |\n\n[1] A Light Recipe to Train Robust Vision Transformers. 2022.\n\nA4: In Section 4.4, we conduct experiments using BPDA based on the guidance of Athalye et al. In the ablation study, we use whether to apply AToP as the only variable. This means that in a set of comparative experiments, only the training parameters of the purifier model differ, while other details remain consistent, including the parts that cause obfuscated gradients. It is reasonable that we use the paradigm provided by Yang et al. to verify whether our method can improve the robustness of the pre-trained purifier model. More importantly, this part is only discussed as the ablation study, and we put the robustness comparison experiment with other SOTA methods in Section 4.2."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4921/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699902285760,
                "cdate": 1699902285760,
                "tmdate": 1699902787530,
                "mdate": 1699902787530,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vSnH902nKl",
                "forum": "u7559ZMvwY",
                "replyto": "c2kyRlnUnr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4921/Reviewer_onZy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4921/Reviewer_onZy"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the rebuttal"
                    },
                    "comment": {
                        "value": "Hello authors, thank you for the detailed answers. My concerns were mainly addressed. From my side, I am increasing my score for now. I am looking forward to read your responses to the other reviewers."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4921/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700630719354,
                "cdate": 1700630719354,
                "tmdate": 1700630719354,
                "mdate": 1700630719354,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yXMRad9hnv",
            "forum": "u7559ZMvwY",
            "replyto": "u7559ZMvwY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4921/Reviewer_n69U"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4921/Reviewer_n69U"
            ],
            "content": {
                "summary": {
                    "value": "1. This paper aims to enhance the adversarial robustness of deep models.\n\n2. Combining adversarial training and purification techniques, a new pipeline is proposed, i.e., adversarial training on purification.\n\n3. Different from purification methods that purify adversarial examples with a pre-trained generative model before classification, \nthe generative model can be finetuned by adversarial training in the proposed pipeline. Moreover, the introduce random transformations play an important role in defensing adversarial attacks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is written clearly and easy to follow.    \n2. Models trained by the proposed method show promising robustness and generalization."
                },
                "weaknesses": {
                    "value": "1. What if there are no random transformations in the proposed pipeline?\n    Does the robustness come from the gradient vanishing from random operations?\n\n2. For adversarial training methods, auto-attack is one of the most effective attack methods to evaluate model robustness. This is because models with adversarial training usually show much better results under FGSM, PGD, and other attacks than under the auto-attack. \n\n    However, as indicated by Table 7 and Table 8 in this paper, the trained model even achieves much better robustness under auto-attack than under PGD20, FGSM attacks.\n    From this perspective, the comparisons in Tables 2,3,4,5 can be unfair because models with adversarial training show worst-case robustness under auto-attack while it is not the case for the model trained by the proposed method.\n\n3. Considering the issue in 2, to demonstrate their robustness, the models trained by the proposed methods should be fully tested under different attacks. For example, the CW attack with a large number of steps, and the PGD attack with 1000 steps."
                },
                "questions": {
                    "value": "See above weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics concerns."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4921/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698652611119,
            "cdate": 1698652611119,
            "tmdate": 1699636477715,
            "mdate": 1699636477715,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4ScsoKwCDZ",
                "forum": "u7559ZMvwY",
                "replyto": "yXMRad9hnv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4921/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4921/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We greatly appreciate the comments of the Reviewer n69U. Below are our responses to the questions raised.\n\nA1.1: If there are no random transformations, the robustness will decrease. $RT_2$ adds more randomness based on $RT_1$, and its robust accuracy has been improved. At the same time, Cohen and Wu et al. theoretically provide robustness guarantees about randomized smoothing, which is described in Section 3.1.\n\nA1.2: Random operations contribute to robustness but not fully due to gradient vanishing. As mentioned in A1.1, there are some theories of robustness guarantees about random operations. Indeed, gradient vanishing does provide 'illusory' robustness for some attacks. Regarding this issue, we also validate our method with AutoAttack and BPDA.\n\nA2: We conduct AutoAttack verification under two different settings: In Table 2,3,4,5, we attack the entire (purifier+classifier) model, and in Table 7 and 8, we attack the classifier model. The result in Table 7 and 8 is reasonable, but we also realize that the current tables can easily lead to misunderstandings. Therefore, we plan to redesign the Table 7 and 8 according to your comments, replacing AutoAttack with CW and PDG-1000.\n\nHere is the explanation of the results: Considering the robustness misestimation caused by obfuscated gradients of the purifier model, we use BPDA (Athalye et al., 2018a) and follow the setting by Yang et al. (2019), approximating the gradient of the purifier model as 1 during the backward pass, which is equivalent to attacking the classifier model. To maintain the same settings, AutoAttack only attacks the classifier. The advantage of AutoAttack lies in using EOT and adaptive mode with white-box and black-box attacks to attack more complex architectures, such as black-box and random factors in the architectures. If there are some random factors in the classifier, although the classifier model can effectively defend against FGSM and PGD, it can still be vulnerable to AutoAttack. However, in classic classifiers (such as Resnet-18), these factors do not exist, and simple white-box attacks cause more damage. Under standard cases, the entire model should be input into AutoAttack directly, and the experimental results are shown in Table 2,3,4,5. We will redesign Table 7 and 8 to avoid confusion.\n\n| Standard Case ($RT_2, \\epsilon=8/255$) |  | | |\n|:---:|:---:|:---:|:---:|\n| Generator models | AToP | CIFAR-10 | CIFAR-100 |\n| AE (Wu et al., 2023) | $\\times$ | 44.73 | 38.48 | \n| AE (Wu et al., 2023) | $\\checkmark$ | **59.18** | **40.23** | \n| GAN (Ughini et al., 2022) | $\\times$ | 59.57 | 38.89 | \n| GAN (Ughini et al., 2022) | $\\checkmark$ | **72.85** | **43.55** | \n\n\nA3: Thanks for the comments. The following are the additional experiments on Table 7 and 8:\n\n| | | CIFAR-10 | | ImageNette | |\n|:---:|:---:|:---:|:---:|:---:|:---:|\n| Transforms | AToP | CW-100 | PGD-1000 | CW-100 | PGD-1000 |\n| RT1 | $\\times$ | 21.09 | 0.00 | 64.26 | 0.00 |\n| RT1 | $\\checkmark$ | **87.89** | **39.45** | **78.12** | **47.07** |\n| RT2 | $\\times$ | 87.70 | 67.97 | 86.52 | 77.34 |\n| RT2 | $\\checkmark$ | **88.87** | **84.38** | **88.48** | **82.03** |\n| RT3 | $\\times$ | 75.20 | 70.31 | 67.38 | 66.21 |\n| RT3 | $\\checkmark$ | **76.56** | **73.44** | **78.32** | **72.85** |"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4921/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699901748472,
                "cdate": 1699901748472,
                "tmdate": 1700020696614,
                "mdate": 1700020696614,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PLj8FPUdDX",
                "forum": "u7559ZMvwY",
                "replyto": "yXMRad9hnv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4921/Reviewer_n69U"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4921/Reviewer_n69U"
                ],
                "content": {
                    "title": {
                        "value": "Random transformations for robustness"
                    },
                    "comment": {
                        "value": "Thanks for your kind response.\n\nI still have some concerns.\nI agree that adding Gaussian noise is helpful to robustness, which has been identified by Li et al. with theoretical proof.\nHowever, why does the random masking scheme lead to significant robustness improvements?\nIf the improvements don't come from obfuscated gradients, could the authors give some explanations for it?"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4921/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700559568616,
                "cdate": 1700559568616,
                "tmdate": 1700559644549,
                "mdate": 1700559644549,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Zq4nNa1aeT",
            "forum": "u7559ZMvwY",
            "replyto": "u7559ZMvwY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4921/Reviewer_jVyX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4921/Reviewer_jVyX"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes Adversarial Training on Purification (AToP) for adversarial defense, which is expected to improve the robustness while achieving good generalization ability. The proposed method consists of two components: perturbation destruction by random transformations and a purifier model that aims to recover the clean image which is adversarially finetuned. The experiments suggest that AToP can improve the robustness against both seen and unseen attacks without significantly sacrificing the accuracy on clean images, and achieve state-of-the-art results on several benchmarks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper makes an early attempt to finetune an adversarial purification model with adversarial training, and suggests that this can be beneficial.\n- The experiments are complete and the results seem promising.\n- The figures are clear and meaningful."
                },
                "weaknesses": {
                    "value": "- Clarity of Section 3.2. \n  - Eq. (8) seems to be a mix of the loss functions for the generator (purifier) $g$ and the discriminator $D$, which are supposed to be different in GAN training. Besides, since AToP is not limited to GAN-based purifier models, it is better to present a general form of the loss function.\n  - It is stated that \"the discriminator $D$ is responsible for distinguishing between real examples (*including clean or adversarial examples*) and the purified examples\", but this is not reflected by Eq. (8) since only the clean sample $x$ is involved in $L_{df}$.\n  - Line 5 of Algorithm 1 is confusing. The notation $g_{\\theta_g}$ is not explained.\n- Some experimental details are not clearly stated.\n  - For evaluation, it is claimed that \"we randomly select 512 images from the test set for robust evaluation.\" It is not clear whether this applies to all results or all compared methods. The comparison may be unfair if different models are evaluated on different test sets.\n  - The \"Ours\" in Table 2,3,4 are not specified: Which RT and purifier model are used? What are the hyper-parameters for the training attack?\n- The results for AToP in Table 8 may be unreasonable. Specifically, considering the accuracy of the same model against different attacks, it is shown that FGSM < PGD < AutoAttack, which is exactly the reverse of common expectations since AutoAttack is believed to be a much stronger attack while FGSM should be the weakest attack. These results may significantly challenge the reliability of the robustness evaluation and should be seriously discussed."
                },
                "questions": {
                    "value": "- Why FGSM is used for adversarial finetuning instead of PGD or other attacks?\n\n- Can the reconstruction loss in Eq. (8) be applied to the reconstruction of adversarial images? Specifically, is it likely that adding $\\mathbb{E}[\\Vert x-g(t(x+\\delta),\\theta_g) \\Vert]$ to the loss will increase the performance?\n\n- Why does reconstruction loss take $\\ell_1$-distance?\n\n- What is the expected scope of purification models that AToP can generalize to?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4921/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4921/Reviewer_jVyX",
                        "ICLR.cc/2024/Conference/Submission4921/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4921/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698833182677,
            "cdate": 1698833182677,
            "tmdate": 1700729516596,
            "mdate": 1700729516596,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Tns2bs8705",
                "forum": "u7559ZMvwY",
                "replyto": "Zq4nNa1aeT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4921/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4921/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We greatly appreciate the comments of the Reviewer jVyX. Below are our responses to the questions raised. Due to exceeding the character limit for a single reply, we divide the response into two parts (1 / 2).\n\nThe first part is about weaknesses:\n\nA1.1: We also realize this might be confusing, so we have revised Eq.(8) in A1.2. Regarding the second point, we agree with your perspective. We have only used the GAN-based model as an illustrative example. We will add a general form: $L_{\\theta_{g}} = L_{org}\\left(x, \\theta_{g}\\right)+\\lambda \\cdot L_{c l s}\\left(x, y, \\theta_{g}, \\theta_{f}\\right)$. For instance, the original loss function is denoted as $L_{df}$ in the GAN-based model (Ughini et al., 2022), and as $L_{mae}$ in the MAE-based model (Wu et al., 2023).\n\nA1.2: Thanks for the comments. Eq.(8) should be divided into two stages, including pre-training and fine-tuning:\n\nPre-training:\n\n$L_{\\theta_{g}} =L_{df}(x, \\theta_{g})$\n\n$=\\mathbb{E}[D(x)]-\\mathbb{E}[D(g(t(x),\\theta_g))]+\\mathbb{E}[{\\|x-g(t(x), \\theta_g)\\|}_{\\ell_1}],$\n\nFine-tuning:\n\n$L_{\\theta_{g}} =L_{df}(x', \\theta_{g})+\\lambda \\cdot L_{c l s}(x', y, \\theta_{g}, \\theta_{f})$\n\n$ =\\mathbb{E}[D(x')]-\\mathbb{E}[D(g(t(x'),\\theta_g))]+\\mathbb{E}[{\\|x'-g(t(x'), \\theta_g)\\|}_{\\ell_1}]$\n\n$ +\\lambda \\cdot \\max_{\\delta} CE(y, f(g(t(x'),\\theta_g))), \\quad where \\ x' = x+\\delta.$\n\nA1.3: $g_{\\theta_g}$ represents the computed gradient (i.e., grad\\_$\\theta_g$). We will change the $g_{\\theta_g}$ in the Algorithm 1 to $\\nabla \\theta_g$ to avoid confusion.\n\nA2.1: Due to the high computational cost of testing models with multiple attacks, we conducted experiments based on the guidance provided by Nie et al. In fact, not all experiments tested 512 images. However, Nie et al. have demonstrated through experiments (Appendix C.2 in [1]) that the robust accuracies of most baselines do not change much on the sampled subset, compared to the whole test set.\n\n[1] Diffusion Models for Adversarial Purification. 2022.\n\nA2.2: We will add this part of the information. We utilize $RT_2$ and GAN-based models, which yielded the best results in the experiment. The adversarial examples are generated by the FGSM during training.\n\nA3: The experimental results in this section are correct, and similar results (FGSM < PGD, Table 1 in [2]) are observed in the work of Ughini et al. And we conduct AutoAttack verification under two different settings: In Table 2,3,4,5, we attack the entire (purifier+classifier) model, and in Table 7 and 8, we attack the classifier model. We also realize that the current tables can easily lead to misunderstandings. Therefore, we plan to redesign the Table 7 and 8 according to Reviewer n69U's comments, replacing AutoAttack with CW and PDG-1000.\n\nHere is the explanation of the results: Considering the robustness misestimation caused by obfuscated gradients of the purifier model, we use BPDA (Athalye et al., 2018a) and follow the setting by Yang et al. (2019), approximating the gradient of the purifier model as 1 during the backward pass, which is equivalent to attacking the classifier model. To maintain the same settings, AutoAttack only attacks the classifier. The advantage of AutoAttack lies in using EOT and adaptive mode with white-box and black-box attacks to attack more complex architectures, such as black-box and random factors in the architectures. If there are some random factors in the classifier, although the classifier model can effectively defend against FGSM and PGD, it can still be vulnerable to AutoAttack. However, in classic classifiers (such as Resnet-18), these factors do not exist, and simple white-box attacks cause more damage. Under standard cases, the entire model should be input into AutoAttack directly, and the experimental results are shown in Table 2,3,4,5. We will redesign Table 7 and 8 to avoid confusion.\n\n[2] Trust-No-Pixel: A Remarkably Simple Defense against Adversarial Attacks Based on Massive Inpainting. 2022."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4921/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699899427994,
                "cdate": 1699899427994,
                "tmdate": 1699902582339,
                "mdate": 1699902582339,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DY178qSdg1",
                "forum": "u7559ZMvwY",
                "replyto": "Zq4nNa1aeT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4921/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4921/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We greatly appreciate the comments of the Reviewer jVyX. Below are our responses to the questions raised. Due to exceeding the character limit for a single reply, we divide the response into two parts (2 / 2).\n\nThe second part is about questions:\n\nQA1: The training cost of using FGSM for adversarial fine-tuning is acceptable. At the same time, as we propose a new paradigm that considers both AT and AP, we use FGSM to verify our ideas preliminarily. Here is a simple experiment regarding time, where we have measured the time required to process 10,000 images:\n\n| Time costs |  |  |\n|-------------------|:-----------:|:-----------:|\n|  | FGSM   | PGD-10 |\n| GAN with AToP     | ~186 sec  | ~1469 sec |\n\nQA2: The impact of optimization using consistency loss of adversarial examples is limited. We can employ a variety of attacks and obtain a variety of different adversarial examples. When using consistency loss to learn information from adversarial examples, which potentially leads to overfitting and weakens the generalization ability against unseen attacks. We also discuss this point in the related work section. Additionally, as the perturbations are minimal, in many strong attacks, the consistency loss from adversarial examples is less than the consistency loss from generated examples ($\\mathbb{E}[x-g(x_{adversarial})] < \\mathbb{E}[x-g(x_{generated})]$). This also results in the consistency loss not playing a significant role during training.\n\nQA3: In the pipeline based on the image completion, the repair results of the boundary of the missing area are better than the middle area, and pixels with different distances from the filled edge should be assigned different weights. A common way is to use the distance between pixels (i.e., $l_1$ distance) for weight calculation. When repairing large missing areas, the $l_1$ loss will be more effective in improving repair quality.\n\nQA4: We hope AToP can be applied to any pre-trained generator-based purifier model. To this end, we conduct experiments on two common generator-based purifier models, including GAN-based and AE-based. At the same time, we also attempted to use diffusion-based purifier models. However, as mentioned in the limitations section, our method cannot easily fine-tune all models yet, such as some models with high computational costs."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4921/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699900340714,
                "cdate": 1699900340714,
                "tmdate": 1699902570266,
                "mdate": 1699902570266,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8hesRRU4lQ",
                "forum": "u7559ZMvwY",
                "replyto": "Tns2bs8705",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4921/Reviewer_jVyX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4921/Reviewer_jVyX"
                ],
                "content": {
                    "title": {
                        "value": "Doubts About the Evaluation Protocol for Ablation Studies"
                    },
                    "comment": {
                        "value": "Thanks for your valuable response. Most of my concerns have been addressed, but I still have some doubts about the evaluation protocol for Section 4.4 (Table 7,8). Given that only the classifier is involved in the attack, the results suggesting that FGSM is more effective are indeed reasonable. Nonetheless, **why is the evaluation protocol in Section 4.4 different from that in Section 4.1**? Considering that attacking the classifier only is not an effective approach, the evaluation protocol of Section 4.1 should be applied to all experiments in my opinion."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4921/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700017155609,
                "cdate": 1700017155609,
                "tmdate": 1700017155609,
                "mdate": 1700017155609,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7aDxuOxmQY",
                "forum": "u7559ZMvwY",
                "replyto": "Zq4nNa1aeT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4921/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4921/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the further comments. We conduct some additional experiments and provide corresponding explanations, hoping that these can address your concerns.\n\nDue to the existence of obfuscated gradients and random factors in adversarial purification, attacks such as FGSM and PGD cannot calculate the correct perturbation. At this point, the model might result in 'illusory' robustness. The results of robust accuracy against FGSM on CIFAR-10 are shown in the table below. **The robust accuracy may be unreliable when testing on FGSM and PGD with the evaluation protocol from Section 4.1.** Therefore, regarding these attacks, Athalye et al. proposed a more reliable evaluation protocol, which has been widely used in adversarial purification tasks (Yang et al. and Ughini et al.). We also follow the same setting to validate the robustness against FGSM and PGD.\n\n| Robust Acc. on FGSM |  | | |\n|:---:|:---:|:---:|:---:|\n| Evaluation protocol | $RT_1$ | $RT_2$ | $RT_3$ |\n| From Section 4.4 | 16.60 | 55.08 | 67.97 |\n| From Section 4.1 | 39.84 | 72.66 | 67.19 | \n\nAs you correctly mentioned, the evaluation protocol in Section 4.4 should not be applied to AutoAttack. Here are the results under the standard cases (Section 4.1) of AutoAttack.\n\n| Standard Case ($RT_2, \\epsilon=8/255$) |  | | |\n|:---:|:---:|:---:|:---:|\n| Generator models | AToP | CIFAR-10 | CIFAR-100 |\n| AE (Wu et al., 2023) | $\\times$ | 44.73 | 38.48 | \n| AE (Wu et al., 2023) | $\\checkmark$ | **59.18** | **40.23** | \n| GAN (Ughini et al., 2022) | $\\times$ | 59.57 | 38.89 | \n| GAN (Ughini et al., 2022) | $\\checkmark$ | **72.85** | **43.55** |"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4921/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700029797094,
                "cdate": 1700029797094,
                "tmdate": 1700454540115,
                "mdate": 1700454540115,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "515J3vuy0e",
                "forum": "u7559ZMvwY",
                "replyto": "7aDxuOxmQY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4921/Reviewer_jVyX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4921/Reviewer_jVyX"
                ],
                "content": {
                    "title": {
                        "value": "Further Concerns about the Evaluation Protocol"
                    },
                    "comment": {
                        "value": "Thanks for your further replies. Regarding the evaluation protocol in Section 4.4, I think that **using the best protocol for each attack** should be the principle for reliable robustness evaluation, and the revised results for AutoAttack seem reasonable.\n\nNonetheless, as suggested by [a], BPDA may not be the most effective way to apply PGD attacks. Instead, PGD with Expectation over Transformation (EOT) is shown to be much more effective than PGD+BPDA for DiffPure and other purification-based defenses, and it even results in lower robust accuracy than AutoAttack as reported in [a]. Therefore, it would be better to test the models with **PGD+EOT instead of PGD+BPDA** in the experiments.\n\n\n[a] Lee, Minjong, and Dongwoo Kim. \"Robust evaluation of diffusion-based adversarial purification.\" ICCV 2023."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4921/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700557199070,
                "cdate": 1700557199070,
                "tmdate": 1700557199070,
                "mdate": 1700557199070,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "r4dwsZF9gm",
                "forum": "u7559ZMvwY",
                "replyto": "djFdLc9AKq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4921/Reviewer_jVyX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4921/Reviewer_jVyX"
                ],
                "content": {
                    "title": {
                        "value": "Transferability of the Purification Models"
                    },
                    "comment": {
                        "value": "> A2: One of the advantages of AP is that once the generator model is fully trained, it will be a plug-and-play module that can be applied to other model architectures.\n\nSince the adversarial training of the purification model $g$ relies on a specific classification model $f$, it might be the case that after AToP, $g$ is tailored for $f$, and applying $g$ to another classification model $f'$ may not yield satisfactory results, e.g., using the pre-trained weights may be better than using the fine-tuned weights regarding $f$. Hence, there should be some experimental results to support this claim."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4921/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700558086787,
                "cdate": 1700558086787,
                "tmdate": 1700558086787,
                "mdate": 1700558086787,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EOZ2uTDar5",
                "forum": "u7559ZMvwY",
                "replyto": "zqoDofURcJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4921/Reviewer_jVyX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4921/Reviewer_jVyX"
                ],
                "content": {
                    "title": {
                        "value": "Further Discussion about the EOT Attack"
                    },
                    "comment": {
                        "value": "Thanks for the updated results of PGD+EOT, and I acknowledge that reporting both BPDA and EOT results is valuable.\n\nIt seems that the robust accuracy of ResNet-18 with AToP (RT2) under PGD+EOT is surprisingly high, and I think the implementation of EOT should be clarified, since certain implementation designs may lead to unreliable evaluation of the robustness according to [a]. Specifically:\n- How many EOT samples do you use for EOT attacks?\n- Do you use any numerical approximation for the computation of gradients?\n- Do you skip any components of the model in the gradient computation?\n\nBesides, do you plan to release the code and the models? This can be crucial for other researchers to validate the robustness of these models, which possibly reaches the state-of-the-art.\n\n[a] Lee, Minjong, and Dongwoo Kim. \"Robust evaluation of diffusion-based adversarial purification.\" ICCV 2023."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4921/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700624858041,
                "cdate": 1700624858041,
                "tmdate": 1700624858041,
                "mdate": 1700624858041,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9HP3k6mE3Z",
                "forum": "u7559ZMvwY",
                "replyto": "DkU9oEVGnO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4921/Reviewer_jVyX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4921/Reviewer_jVyX"
                ],
                "content": {
                    "title": {
                        "value": "Discussion on the Contribution of Random Masking to Robustness"
                    },
                    "comment": {
                        "value": "The comparison between RT1 and RT2 does suggest that random masking alone does not produce robustness. However, it is possible that the randomness of the masking partially contributes to the robustness of the models with RT2.\n\nI have a suggestion for investigating the contribution of the randomness of masking to the robustness: to consider a variant of RT2 that fixes the random mask $m$ to be a constant $m_0$, i.e., $m=m_0$ for each forward of the model. The comparison between RT2 and this variant should provide extra insights into how different components of the model contribute to the robustness."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4921/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700626036714,
                "cdate": 1700626036714,
                "tmdate": 1700626036714,
                "mdate": 1700626036714,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9KWazv27jl",
                "forum": "u7559ZMvwY",
                "replyto": "ag8lKgI0HD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4921/Reviewer_jVyX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4921/Reviewer_jVyX"
                ],
                "content": {
                    "title": {
                        "value": "Discussion on the Contribution of Random Masking to Robustness (cont.)"
                    },
                    "comment": {
                        "value": "Thanks for your comment. I think fixing the mask $m$ **solely at test time** and evaluating the existing trained models suffices to prove that the randomness of the mask plays a role in improving the robustness (if that is the case).\n\nI understand that random masking is not the contribution of this paper, and I'm just curious whether this randomness matters."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4921/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700646588247,
                "cdate": 1700646588247,
                "tmdate": 1700646588247,
                "mdate": 1700646588247,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "N0mnJzGydk",
                "forum": "u7559ZMvwY",
                "replyto": "QNnzZkSJNx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4921/Reviewer_jVyX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4921/Reviewer_jVyX"
                ],
                "content": {
                    "title": {
                        "value": "Further Discussion about the EOT Attack (cont.)"
                    },
                    "comment": {
                        "value": "Thanks for your clarifications. Skipping the purifier model in the gradient computation may lead to significantly higher robust accuracy, and I hope that more reliable evaluations following the settings in [a] can be conducted. I understand that this can be difficult due to time constraints, so I suggest that you report the results on a smaller subset of the test samples (e.g., 64 samples) if appropriate."
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4921/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700651283481,
                "cdate": 1700651283481,
                "tmdate": 1700651283481,
                "mdate": 1700651283481,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Zg3Hb8i37p",
                "forum": "u7559ZMvwY",
                "replyto": "9A83XV1R8d",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4921/Reviewer_jVyX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4921/Reviewer_jVyX"
                ],
                "content": {
                    "title": {
                        "value": "Summary of the Discussion"
                    },
                    "comment": {
                        "value": "I sincerely thank the authors for their responses and I appreciate their active participation and considerable efforts during the discussion period.\n\nMy major concern with this paper is the robustness evaluation of the proposed models. The latest preliminary results provided by the authors with PGD+EOT applied to the whole model seem reasonable and suggest that the proposed method is indeed effective, possibly reaching state-of-the-art, although further comprehensive experiments with this evaluation protocol are necessary for drawing reliable conclusions.\n\nIn my opinion, this paper makes a valuable contribution to the field of purification-based adversarial defense, but the following points are required for this paper to be above the acceptance level:\n- Improve the clarification of the text, especially the Method section.\n- Evaluate the models using PGD+EOT as discussed, apart from AutoAttack and BPDA. To compare with existing models using this same evaluation protocol is preferable.\n- Release the code and the trained models so that other researchers can validate the robustness.\n\nOverall, considering the existing state of the paper after discussion and the promises of the authors, I decided to increase my rating from 5 to 6 for now. I hope that the authors can improve the paper accordingly."
                    }
                },
                "number": 29,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4921/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700729407151,
                "cdate": 1700729407151,
                "tmdate": 1700729407151,
                "mdate": 1700729407151,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]