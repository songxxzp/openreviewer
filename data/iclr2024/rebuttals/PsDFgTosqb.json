[
    {
        "title": "Learning to Solve Bilevel Programs with Binary Tender"
    },
    {
        "review": {
            "id": "x107YoWXbJ",
            "forum": "PsDFgTosqb",
            "replyto": "PsDFgTosqb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2807/Reviewer_YfM6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2807/Reviewer_YfM6"
            ],
            "content": {
                "summary": {
                    "value": "The authors explore the use of machine learning techniques to solve complex bilevel programs with binary variables. They introduce a sampling algorithm for obtaining high-quality sample data for train neural networks. Two neural networks ( general neural networks (GNN) and input supermodular neural network  (ISNN) are proposed to estimate the lower-level value function based on upper-level decisions. These estimates are used to enhance the optimization process by adding optimality cuts to a single-level problem."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The authors propose a machine learning-based algorithm for solving bilevel programs with binary variables in the outer level problem. \n\n- The authors are able to provide some theoretical analysis for proposed neural networks with binary inputs. \n\n- The paper is easy to follow."
                },
                "weaknesses": {
                    "value": "1) The numerical experiments lack sufficient evidence to demonstrate the advantages of the proposed algorithm. It appears that the algorithm can only achieve optimality for test instances with n = 10. The authors solely conduct comparisons with the baseline method MiBS based on computational times, without assessing solution quality, such as the optimality gap.\n\n2) The test instances are only from the authors\u2019 synthetic generated ones. Some public datasets like MIBLP-XU and IBLP-FIS in Tahernejad et al., 2020 should be used.   \n\n3) This method should only be suitable when functions $f$ and $g$ are linear or MILP-representable because one needs to solve Eq (5) and (12) to optimality.\n\n4) The cost to solve a single instance of Problem (5) is very expensive because it is at least as hard as a mixed-integer quadratic program (MIQP)."
                },
                "questions": {
                    "value": "1) What is the stopping criterion for Algorithm 2 so that the generated point $(x,y)$ is guaranteed to be feasible to Problem 1? That is, $x,y$ is satisfied $x \\in X(y), y \\in Y(x), g(y,x) \\ge \\max_{z \\in Y(x)}  g(z,x)$ \n\n2) What is the definition of ``relative error\u201d in Figures 4 and 7? How do you define an error when an optimal solution cannot be found, for example, for large-scale problems with n = 60?\n\n3) Why relative errors can be negative (as in Fig. 7)?\n\n4) Why we need to care of supermodularity of an ISNN? I notice that ISNN requires more neurons compared to GNN (i.e., the lower bound value for $N_{nr}$ in Eq. (8) vs Eq. (7))\n\n5) Do we have to solve $\\max_{y \\in Y(\\hat{x})} g(y, \\hat{x})$ to optimality in Algorithm 1?\n\n6) For feasibility purpose, why do we need the second term $h^T x$ in Eq. (5)?\n\n7) I don\u2019t fully understand the notation ``GNN representability\u201d in Prop 1 (ii). This statement \u201cwe measure the GNN representability by the number of parameters it trains\u201d seems to be not mathematically sound. \n\n8) The architecture details of GNN of ISNN should be reported.\n\n9) The name of section 3.3 seems to be incomplete. \n\n10) I don\u2019t see any part of Figures to support the claims \u201cachieving relative error less than 5%,\u201d, \u201cthe relative error of instances with n = 30, 50, 60 is larger than 15%.\u201d"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2807/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698707128703,
            "cdate": 1698707128703,
            "tmdate": 1699636223201,
            "mdate": 1699636223201,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "j0WSRmhhLv",
                "forum": "PsDFgTosqb",
                "replyto": "x107YoWXbJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2807/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2807/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer YfM6"
                    },
                    "comment": {
                        "value": "**General Response**: \nWe sincerely appreciate the time you dedicated to reviewing our paper, and we are grateful for the valuable insights you provided. \nBefore we get into the answers to detailed questions, we would like to emphasize the following three points.\n\n1. Bilevel programs are challenging to solve both theoretically and numerically. \nIn particular, for bilevel programs with binary tender as considered in this paper, the computational burden increases exponentially with respect to the number of linking variables. \nTherefore, for large instances, it is intractable to obtain the optimal solution. \nIn this paper, we use the state-of-the-art solver, MiBS, as the benchmark to assess the solution quality of our method. Due to the intractability, even MiBS may fail to provide optimal solutions to some bilevel test instances. \nFor this reason, we have replaced the term ''relative error'' with ''objective difference'' to avoid confusion.\n\n2. In our method, sampling is required to obtain sufficient $(\\hat{x}$, $\\phi(\\hat{x}))$ pairs for training. \nThe proposed enhanced sampling consists of two steps, to first find a feasible $\\hat{x}$ by solving (5) and then to compute $\\phi(\\hat{x})$ by solving the lower-level problem. \nWe note that because we only need a feasible $\\hat{x}$ in each iteration before attaining the final optimum, it is unnecessary to solve (5) to optimality, which significantly reduces the computational burden during sampling. \nThe following sentence is added in Section 3.1 to state this point.\n\n    *''Here we note that though (5) is a mixed-integer quadratic program, we only need a feasible solution to (5) for sampling, which does not incur too much computational burden and saves the overall computational time.''*\n\n3. This paper focuses on bilevel programs. \nOnce we have reformulated bilevel programs into single-level ones, such as (12), we can utilize existing algorithms and highly effective commercial solvers for solution. \nFor this reason, how to solve (12) to optimality is out of the scope of this paper. \nThe following sentence is added in Section 3.1 to state this point.\n\n    *''Here we note that existing algorithms and highly-efficient off-the-shelf solvers can be adopted to handle the single-level program (12), which is yet out of the scope of this paper.''*\n\nWith the above three points in mind, we respond to your comments and questions one by one in the following.\n\n> **Weakness 1**: \nThe numerical experiments lack sufficient evidence to demonstrate the advantages of the proposed algorithm. \nIt appears that the algorithm can only achieve optimality for test instances with $n=10$. \nThe authors solely conduct comparisons with the baseline method MiBS based on computational times, without assessing solution quality, such as the optimality gap.\n\n**Response**: \nThanks for your comments. \nWe agree that we do not calculate the optimality gap in our numerical experiments, yet as we have emphasized in the general response, it is currently impossible to obtain the optimal solution to large-size bilevel programs. \nIn this paper, we use the objective difference to assess the solution quality, and the results are reported in Section 4.\n\n> **Weakness 2**: \nThe test instances are only from the authors' synthetic generated ones. \nSome public datasets like MIBLP-XU and IBLP-FIS in Tahernejad et al., 2020 should be used.\n\n**Response**: \nThanks for your suggestion. \nActually, the instance generation rule provided in Appendix C of this paper follows closely the rules of generating MIBLP-XU in Tahernejad et al., 2020. \nBecause we focus on bilevel programs with binary tender, the existing instances in MIBLP-XU are not applicable. \nSo we have used the instance generation rule of MIBLP-XU as the basis to generate new instances with binary tender. \nThe following sentence is added in Appendix C to state this point.  \n\n*''Following the instance generation rules of (Tahernejad et al., 2020), all instances are generated in the following forms.''*\n\n\n> **Weakness 3**: \nThis method should only be suitable when functions $f$ and $g$ are linear or MILP-representable because one needs to solve Eq (5) and (12) to optimality.\n\n**Response**: \nThanks for your comments. \nAs we have emphasized in the general response, we do not need to solve (5) to optimality, and how to solve (12) to optimality is out of the scope of this paper, because (12) can be solved effectively by various off-the-shelf solvers. \nTherefore, we do not require $f$ or $g$ to be linear or MILP-representable when using our method.\n\n> **Weakness 4**: \nThe cost to solve a single instance of Problem (5) is very expensive because it is at least as hard as a mixed-integer quadratic program (MIQP).\n\n**Response**: \nThanks for your comments. \nAs we have emphasized in our general response (ii) before, we do not need to solve (5) to optimality, which significantly reduces the computational burden during sampling."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2807/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700437749301,
                "cdate": 1700437749301,
                "tmdate": 1700437749301,
                "mdate": 1700437749301,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vTveCGgQ05",
                "forum": "PsDFgTosqb",
                "replyto": "x107YoWXbJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2807/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2807/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer YfM6 (continued)"
                    },
                    "comment": {
                        "value": "> **Question 1**: \nWhat is the stopping criterion for Algorithm 2 so that the generated point $(x,y)$ is guaranteed to be feasible to Problem 1? \nThat is, $x$, $y$ is satisfied $x\\in X(y)$, $y\\in Y(x)$, $g(y,x)\\geq \\max_{z\\in Y(x)}g(z,x)$\n\n**Response**: \nDue to the assumption that the leader's feasible region is independent of $y^{\\*}$, i.e., $X(y^{\\*})\\equiv X$, any generated point $(x,y)$ from Algorithm 2 is feasible, for which there is no extra stopping criterion. \nThe following gives a detailed explanation.  \n\nAccording to Step 8 of Algorithm 2, the obtained $x^{\\*}$ must satisfy the constraints of (12), which means $x^{\\*}\\in X$. \nAccording to Step 9 of Algorithm 2, we fix $x=x^{\\*}$ and solve the lower-level problem to obtain $y^{\\*}$, which means $y^{\\*}\\in Y(x^{\\*})$ and $g(y^{\\*},x^{\\*})=\\max_{z\\in Y(x^{\\*})}g(z,x^{\\*})$. \nTherefore, any generated point $(x,y)$ from Algorithm 2 must satisfy $x\\in X$, $y\\in Y(x)$, and $g(y,x)\\geq\\max_{z\\in Y(x)}g(z,x)$. \nUnder the assumption, the generated point $(x,y)$ is naturally feasible to Problem (1). \nThe following sentence is highlighted to state the point. \n\n*''In addition, we assume the leader's feasible region is independent of $y^{\\*}$, i.e., $X(y^{\\*})\\equiv X$, which is a special case of (1). In this case, we can obtain an upper bound for the BP (1) from any feasible solution $x$ and the corresponding follower's optimal solution $y^{\\*}$ (e.g., by solving the follower's problem using $x$).''*\n\n\n> **Question 2**: \nWhat is the definition of ''relative error'' in Figures 4 and 7? \nHow do you define an error when an optimal solution cannot be found, for example, for large-scale problems with $n = 60$?\n\n**Response**: \nThanks for your question. \nAs we have emphasized in our general response (i) before, we have replaced the term ''relative error'' with ''objective difference'' to avoid confusion. \nThe objective difference is assessed by using the solution of MiBS as the benchmark. \nWe also provide the definition of ''objective difference'' in the revised paper as  \n\n*''We compare our method with the state-of-the-art solver for bilevel problems, MiBS (Tahernejad et al., 2020) and use its solution under a time limit of 1 hour as the benchmark to calculate the objective differences (i.e., the gap between the objectives from our method and MiBS).''*\n\n> **Question 3**: \nWhy relative errors can be negative (as in Fig. 7)?\n\n**Response**: \nThanks for your question. According to the definition provided in the response to Question 2, the objective difference (previously relative error) would be negative if our method provides a better solution than the benchmark. \nThe following sentence is added in the revised paper for explanation.  \n\n*''The negative objective difference means that our method provides a better solution than the benchmark.''*\n\n> **Question 4**: \nWhy we need to care of supermodularity of an ISNN? \nI notice that ISNN requires more neurons compared to GNN (i.e., the lower bound value for $N_{nr}$ in Eq. (8) vs Eq. (7))\n\n**Response**: \nThanks for your question. \nFirst, the number of neurons in GNN and ISNN depend on the values of $N_{s}$ and $n$ and so there is no guarantee that one needs more neurons than the other. \nAlso, according to Proposition 3, many weight parameters of ISNN end up being zero after training. Hence, it may not be completely fair to compare the complexity of GNN and ISNN only through the number of neurons.  \n\nSecond, the supermodularity of an ISNN helps to improve the solution quality of our method. \nFor bilevel programs with binary tender, many lower-level problems generate a supermodular $\\phi(x)$ [R3, R4]. If we relax the binary limit $x\\in${$\\{0,1\\}$} of $\\hat{\\phi}(x)$ to $x\\in[0,1]$, $\\hat{\\phi}(x)$ becomes a lower envelope of the discrete $\\phi(x)$. \nConsequently, if we adopt an ISNN, the trained $\\tilde{\\phi}(x)$ tends to be a lower bound of $\\phi(x)$ (see Figure 3) so that with high probability, the optimal solution to the original problem would not be cut off by (12b). \nConversely, if we adopt a GNN, there is no guarantee that $\\tilde{\\phi}(x)\\geq\\phi(x)$ or vice versa, for which the optimal solution to the original problem may be cut off by (12b). \nTherefore, the supermodularity of an ISNN helps to improve the solution quality of our method.  \n\n[R3] X. Chen, D.Z. Long, J. Qi. Preservation of supermodularity in parametric optimization: necessary and sufficient conditions on constraint structures. Operations Research. 2021, 69(1): 1-12.  \n[R4] D.Z. Long, J. Qi, A. Zhang. Supermodularity in two-stage distributionally robust optimization. Management Science, ahead of print.\n\n> **Question 5**: \nDo we have to solve $\\max_{y\\in Y(y,\\hat{x})}$ to optimality in Algorithm 1?\n\n**Response**: \nThanks for your question. \nYes, in step 7 of Algorithm 1, we need to solve $\\max_{y\\in Y(y,\\hat{x})}$ to optimality to obtain correct $(\\hat{x}$, $\\phi(\\hat{x}))$ pairs for training."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2807/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700437894161,
                "cdate": 1700437894161,
                "tmdate": 1700504979504,
                "mdate": 1700504979504,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pgk2bzqwy1",
                "forum": "PsDFgTosqb",
                "replyto": "x107YoWXbJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2807/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2807/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer YfM6 (continued)"
                    },
                    "comment": {
                        "value": "> **Question 6**: \nFor feasibility purpose, why do we need the second term $h^{\\textrm{T}}x$ in Eq. (5)?\n\n**Response**: \nThanks for your question. \nThe objective (5a) is a general formula for quadratic programming, which does not affect feasibility. \nIn other words, we can set $h = 0$ in (5) without affecting the feasibility.\n\n> **Question 7**: \nI don't fully understand the notation ''GNN representability'' in Prop 1 (ii). \nThis statement ''we measure the GNN representability by the number of parameters it trains'' seems to be not mathematically sound.\n\n**Response**: \nThanks for your comments. \nIn the studies about the representability of neural networks, the number of trainable parameters is a very straightforward metric, which is accepted but is also criticized by [R5] due to its conservativeness when $x$ lies in a continuous domain. \nInstead, [R5] recommends using the number of linear regions of the graph $(x, \\tilde{\\phi}(x))$. \nHowever, as we study bilevel programs with binary tender in this paper, the domain of $x$ is discrete (in particular, {$\\{0,1\\}$}$^n$). \nIn this case, the graph $(x, \\tilde{\\phi}(x))$ becomes a collection of points and it is no longer well-defined to measure its representability using the number of linear regions. \nInstead, the number of trainable parameters is more applicable and is thus used to measure the representability of neural networks in this paper.  \n\n[R5] Hu, X., Chu, L., Pei, J. et al. Model complexity of deep learning: a survey. Knowl Inf Syst 63, 2585--2619 (2021).\n\n> **Question 8**: \nThe architecture details of GNN of ISNN should be reported.\n\n**Response**: \nThanks for your comments. \nIn Section 3.2, we have reported the architecture of GNN of ISNN (see below also).  \n\n''According to Proposition 1, when we adopt a GNN for training, we incorporate two hidden layers in the GNN architecture, each with $N_{nr}/2$ neurons, where $N_{nr}$ is the smallest even number that satisfies (7).''  \n\n''Since the ISNN\u2019s architecture does not affect its representability, to better compare with GNN, we incorporate two hidden layers in ISNN, each with $N_{nr}/2$ neurons, where $N_{nr}$ is the smallest integer that satisfies (8).''\n\n> **Question 9**: \nThe name of section 3.3 seems to be incomplete.\n\n**Response**: \nThanks for your comments. \nThe main steps of our method consist of sampling, training, and solving, which are used as the titles of sections 3.1-3.3, respectively.\n\n> **Question 10**: \nI don't see any part of Figures to support the claims ''achieving relative error less than 5%,'', ''the relative error of instances with n = 30, 50, 60 is larger than 15%.''\n\n**Response**: \nThanks for your comments. \nFor the claim ''the relative error of instances with n = 30, 50, 60 is larger than 15\\%,'' please see Figure 4. \nFor the claim ''achieving relative error less than 5\\%,'' please see Figure 7."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2807/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700437916521,
                "cdate": 1700437916521,
                "tmdate": 1700505003191,
                "mdate": 1700505003191,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bxqqp4VOB5",
                "forum": "PsDFgTosqb",
                "replyto": "x107YoWXbJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2807/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2807/Authors"
                ],
                "content": {
                    "title": {
                        "value": "looking forward to post-rebuttal feedback!"
                    },
                    "comment": {
                        "value": "Dear Reviewer YfM6\n\nThank you for reviewing our paper. We have carefully answered your concerns about optimality, datasets, and all questions. \n\nPlease let us know if our answers accurately address your concerns. If our response resolves your concerns, we kindly ask you to consider raising the rating of our work. Thank you very much for your time and efforts! We would like to discuss any additional questions you may have.\n\nBest, Authors"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2807/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700686688513,
                "cdate": 1700686688513,
                "tmdate": 1700686688513,
                "mdate": 1700686688513,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "tE3PTzvlVB",
            "forum": "PsDFgTosqb",
            "replyto": "PsDFgTosqb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2807/Reviewer_nqbY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2807/Reviewer_nqbY"
            ],
            "content": {
                "summary": {
                    "value": "This work studies the Bilevel programs (BPs)  with discrete decision variables. A neural network is trained to approximate the optimal value of the lower-level problem, as a function of the binary tender. Then a single-level reformulation of the BP through a mixed-integer representation of the value function can be obtained. Moreover, an enhanced sampling method is proposed for high-dimensional BPs."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This work proposes an approximation-based method for Bilevel programs (BPs)  with discrete decision variables, which is interesting.\n\n2. an input supermodular neural network (ISNN) is proposed, which ensures a supermodular mapping from input to output.\n\n3. an enhanced sampling method is proposed for solving high-dimensional BPs."
                },
                "weaknesses": {
                    "value": "1. The author should conduct some complexity analysis, such as time complexity [1], to show the effectiveness of the proposed method.\n\n2. This work employs neural networks to learn and approximate the value function $\\phi(x)$. However, training the neural networks is more computationally complex than directly approximating the lower-level optimization problem [2]. What is the advantage of the proposed method over the polyhedral approximation in [2]?\n\n3. Since one of the key contributions in this work is to employ neural networks to learn and approximate the value function $\\phi(x)$, I suggest the authors clearly discuss the existing approximation-based methods (i.e., which approximate the lower-level optimization problems and reformulate the bilevel optimization problems as single-level optimization problems, for instance, polyhedral approximation) in bilevel optimization since I can't find any discussion about the approximation-based methods in bilevel optimization.\n\n4. Training the neural networks to approximate the value function $\\phi(x)$ may introduce some variance which will lead to the solution of the resulting single-level problem far away from the original bilevel optimization problems. Can you provide a more theoretical guarantee for the proposed method?\n\n[1] A Gradient Method for Multilevel Optimization. NeurIPS 2021.\n\n[2] Asynchronous Distributed Bilevel Optimization, ICLR 2023."
                },
                "questions": {
                    "value": "I have some questions about the complexity, the comparison with the existing approximation-based methods in bilevel optimization, and the theoretical guarantee of the proposed method. Please see the Weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2807/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2807/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2807/Reviewer_nqbY"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2807/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698717082567,
            "cdate": 1698717082567,
            "tmdate": 1700613876810,
            "mdate": 1700613876810,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "l4tfTR90I9",
                "forum": "PsDFgTosqb",
                "replyto": "tE3PTzvlVB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2807/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2807/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer nqbY"
                    },
                    "comment": {
                        "value": "**General Response**: \nWe sincerely appreciate the time you dedicated to reviewing our paper, and we are grateful for the valuable insights you provided.\nBefore answering your questions, we would like to emphasize the following two points.\n\n1. This paper focuses on bilevel programs with binary tender.\nIt means that the input $x$ of the value function $\\phi(x)$ is binary-valued, to which existing gradient methods are no longer applicable.\n\n2. Although gradient methods are not applicable, there exist works that seek to approximate $\\phi(x)$ using cutting planes.\nTo our best knowledge, MiBS [R1] is the state-of-the-art among these works and is thus adopted as the benchmark in the numerical experiments of this paper.  \n\n    [R1] Sahar Tahernejad, Ted K Ralphs, and Scott T DeNegre. A branch-and-cut algorithm for mixed integer bilevel linear optimization problems and its implementation. Mathematical Programming Computation, 12(4): 529--568, 2020.\n\nWith the above two points in mind, we respond to your comments and questions one by one in the following.\n\n\n> **Weakness 1**: \nThe author should conduct some complexity analysis, such as time complexity [1], to show the effectiveness of the proposed method.  \n[1] A Gradient Method for Multilevel Optimization. NeurIPS 2021.\n\n**Response**: \nThanks for your suggestion.\nWe have carefully read the recommended paper [1], in which a gradient method is proposed to solve multilevel optimization and the time and space complexities of the gradient method are analyzed.\nHowever, as we have emphasized in the general response, gradient methods as well as the time and space complexity analyses are not applicable to the class of bilevel integer programs in this paper, for which the time complexity analysis for gradient methods in [1] is inapplicable.\n\n\n> **Weakness 2**: \nThis work employs neural networks to learn and approximate the value function $\\phi(x)$.\nHowever, training the neural networks is more computationally complex than directly approximating the lower-level optimization problem [2].\nWhat is the advantage of the proposed method over the polyhedral approximation in [2]?  \n[2] Asynchronous Distributed Bilevel Optimization, ICLR 2023.\n\n**Response**: \nThanks for your comments.\nFirst, we have carefully read the recommended paper [2], in which cutting planes based on [R2] are utilized to form polyhedral approximations.\nSpecifically, [2] construct these cutting planes using gradient information from the lower-level problem.\nHowever, as we have emphasized in the general response, the class of bilevel programs we study in this paper have binary tenders and the lower-level problem therein contain binary decision variables.\nFor these reasons, gradient methods become inapplicable, and so do the polyhedral approximation in [2].\n\nSecond, since the lower-level problem in this paper involves binary decision variables, it is in general inapproximable (see, e.gl, [R3]), that is, there do not exist polynomial-time algorithms that produce solutions to such lower-level problems with a constant competitive ratio.\n\nThird, we clarify that this paper does not intend to prove that neural networks outperform any approximation methods in solving general bilevel programs.\nInstead, in view of the inapplicability of the existing approximation methods (see above) and the power of neural networks in other machine learning problems, we seek to demonstrate that NN also help solve this challenging class of bilevel programs. \n\n[R2] Stephen Boyd and Lieven Vandenberghe. Localization and cutting-plane methods. From Stanford EE 364b lecture notes, 2007.  \n[R3] Trevisan, Luca. Inapproximability of combinatorial optimization problems. Paradigms of Combinatorial Optimization: Problems and New Approaches (2014): 381-434."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2807/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700585751621,
                "cdate": 1700585751621,
                "tmdate": 1700585751621,
                "mdate": 1700585751621,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QfWWnCKnHn",
                "forum": "PsDFgTosqb",
                "replyto": "wfGohgkglo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2807/Reviewer_nqbY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2807/Reviewer_nqbY"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for responses, my concerns have been addressed and I'm happy to change the score to 6."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2807/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700613852029,
                "cdate": 1700613852029,
                "tmdate": 1700613852029,
                "mdate": 1700613852029,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gctoahEluE",
            "forum": "PsDFgTosqb",
            "replyto": "PsDFgTosqb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2807/Reviewer_skLM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2807/Reviewer_skLM"
            ],
            "content": {
                "summary": {
                    "value": "This paper employs neural networks to help solve the bilevel programs with binary tenders. Specifically, they adopt neural networks to approximate the optimal value of the lower-level problem as a function of the binary tenders. In order to train the neural networks, the authors also introduce an enhanced sampling method to generate high-quality samples. Lastly, numerical experiments were conducted to demonstrate the performance of these approaches."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper gives a good attempt of incorporating ML (especially neural networks) methods to facilitate the solution of traditional mathematical optimization problems, which in my opinion is an area that deserves more attention from the community. Overall, this paper is clearly written, easy to understand, and the theoretical results in Section 3 are very neat. I also find it to be very impressive that I cannot find any typo throughout this paper and the propositions also appear to be of their own independent interest."
                },
                "weaknesses": {
                    "value": "Main concern:\n1. The lack of ablation study, especially on the enhanced sampling part. For instance, why do you want to solve the quadratic programming problem (5) to get the samples? I understand that matrix Q is selected to be PSD is for the polynomial-solvability, but what is the main reason of solving the quadratic program in the first place? If we replaced this enhanced sampling with some other more naive sampling methods, how would it affect the experiment results?\n2. Limitation of the experiment setup and analysis: \n   (a) Instance dimension up to 60 is too small.\n   (b) The selection of the lower level problem is LP and MILP, which both have linear lower level objective function. At least you could have tried some simple nonlinear functions like quadratic function.\n   (c) The experimental results do not support the conclusion well. I list some of the points in the next Questions section."
                },
                "questions": {
                    "value": "Major questions:\n1. In Figure 5 and Figure 6, the computational time of MiBS increases very fast with the increase of n. My question is, even though the MiBS solver might take a long time to reach optimality, have you considered to set a time limit, and compare the relative error of the best found solution given by the solver at time limit with your approaches?\n2. In your Conclusion section you mentioned: \"we demonstrated that the enhanced sampling helps reduce average relative error\". However, the whole point of your sampling method is simply to get enough data points for training the neural network. In order to claim that your proposed enhanced sampling can bring some extra benefits, you need to at least compare with other non-trivial sampling methods.\n3. In Conclusion section: \"The computational time of using ...... is significantly shorter than that of MiBS ......\" I admit that this is true, but your methods also do not reach optimality, for fair comparison you either need to have enough samples for exactly learning the value function (so that your approach can also produce a true optimal solution), or you need to compare the best feasible solution found by MiBS within a given time limit. \n\nMinor question:\n1. I suggest to give an exact definition for \"binary tender\". As far as I know, I do not think this is a well-known term in the community.\n2. Still about the definition of \"binary tender\". On Page 2 \"we assume that the entries of x appearing in the lower-level formulation are binary-valued\". However, in Appendix C, about the instance generation for the experiments, I notice that the binary constraint on x is enforced in the upper-level instead of the lower-level formulation."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2807/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2807/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2807/Reviewer_skLM"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2807/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698727726081,
            "cdate": 1698727726081,
            "tmdate": 1700721533403,
            "mdate": 1700721533403,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yYRBucbvmh",
                "forum": "PsDFgTosqb",
                "replyto": "gctoahEluE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2807/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2807/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer skLM"
                    },
                    "comment": {
                        "value": "**General Response**: \nWe sincerely appreciate the time you dedicated to reviewing our paper, and we are grateful for the valuable insights you provided.\nBefore responding to your questions, we would like to emphasize the following two points.\n\n1. Sampling sufficiently many $(\\hat{x}, \\phi(\\hat{x}))$ pairs is fundamental to our approach, which replies on the approximation of the value function $\\phi(x)$.\nHowever, as $n$ gets larger, due to the exponentially increasing input space, feasible $x$ solutions become more scarce and it gets harder to obtain samples with respect to feasible, let alone near-optimal, $x$ solutions. \nFor this reason, naive sampling approaches quickly becomes inapplicable as $n$ increases.\nInstead, through the ''sampling via optimization'' approach developed in this paper, we guarantee finding a feasible $x$ in each sampling.\nBy solving a quadratic program (not necessarily to global optimium), we can avoid repeated samples and enhance sampling efficiency.\nFurthermore, by imposing constraints like (5b), we can enhance the quality of the obtained samples which improves the quality of the approximate value function $\\tilde{\\phi}(x)$.\n\n2. In all numerical experiments, we set the time limit of MiBS to be 1 hour in this paper.\n\nWith the above two points in mind, we respond to your comments and questions one by one in the following.\n\n> **Weakness 1**: \nThe lack of ablation study, especially on the enhanced sampling part.\nFor instance, why do you want to solve the quadratic programming problem (5) to get the samples?\nI understand that matrix $Q$ is selected to be PSD is for the polynomial-solvability, but what is the main reason of solving the quadratic program in the first place?\nIf we replaced this enhanced sampling with some other more naive sampling methods, how would it affect the experiment results?\n\n**Response**: \nThanks for your comments.\nWe answer the questions one by one as follows.\n    \n1. > If we replaced this enhanced sampling with some other more naive sampling methods, how would it affect the experiment results?\n\n    If we replace enhanced sampling with naive sampling, as we have emphasized in the general response point 1, we cannot obtain sufficient feasible $x$ within a reasonable time limit, when the dimension of $x$ gets large.\n    As a result, the quality of the approximate value function $\\tilde{\\phi}(x)$ will be poor and producing low-quality solutions to the bilevel program.\n\n2. > why do you want to solve the quadratic programming problem (5) to get the samples?\n\n    The reason for solving a quadratic program (5), as opposed to for example a linear program, is to avoid repeated samples and enhance sampling efficiency.\n    If we solve a linear program to get samples, i.e., removing the term $x^{\\textrm{T}}Qx$ in (5a), all $x$ thus obtained are guaranteed to lie on the boundary of the feasible region (or the convex hull of the feasible region). \n    As a result, these $x$-solutions become increasingly repetitive as the dimension $n$ increases, decreasing the sampling efficiency.\n    Instead, when (5) has a quadratic (and convex) objective function, the optimal solution may lie both on the boundary or in the interior of the feasible region (or the convex hull of the feasible region).\n    This drastically enhances the sampling efficiency.\n    The following sentence is added in Section 3.1 to state this point.\n\n    *''We note that the reason for solving a quadratic program here is to avoid repeated samples and enhance sampling efficiency.''*\n\n3. > What is the main reason of solving the quadratic program in the first place?\n\n    There are two main reasons here.\n    The first reason is to enhance sampling efficiency, as discussed above.\n    The second reason is that we only need a feasible solution to (5), instead of an optimal solution. \n    Since most commercial solvers employ advanced heuristics to find feasible solutions to mixed-integer quadratic programs, formulating (5) as a MIQP, as opposed to other mixed-integer convex programs, further enhances the sampling efficiency.\n    The following sentence is added in Section 3.1 to state this point.\n\n    *''Here we note that though (5) is a mixed-integer quadratic program, we only need a feasible solution to (5) for sampling, which does not incur too much computational burden and saves the overall computational time.''*"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2807/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700504361950,
                "cdate": 1700504361950,
                "tmdate": 1700504361950,
                "mdate": 1700504361950,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vTJcsZkjOR",
                "forum": "PsDFgTosqb",
                "replyto": "gctoahEluE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2807/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2807/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer skLM (continued)"
                    },
                    "comment": {
                        "value": "> **Major questions 2**: \nIn your Conclusion section you mentioned: ``we demonstrated that the enhanced sampling helps reduce average relative error\".\nHowever, the whole point of your sampling method is simply to get enough data points for training the neural network.\nIn order to claim that your proposed enhanced sampling can bring some extra benefits, you need to at least compare with other non-trivial sampling methods.\n\n**Response**: \nThanks for your comments.\nThe proposed enhanced sampling brings us two aspects of benefits: sampling efficiency and sample quality.\n\nOn the one hand, as we have emphasized in the general response, as compared to naive sampling which draws samples of $x$ from {$\\{0,1\\}$}$^n$ based on a certain probability distribution, the adopted ''sampling via optimization'' approach guarantees finding a feasible $x$ in each sampling, which enhances the sampling efficiency.\nIn this revision, we compare the sampling efficiency of our method with a naive sampling method (random sampling) and a non-trivial sampling method (Latin hypercube sampling) and provide the results in Appendix D.3.\nIt is obvious that our sampling method is more stable and applicable than both benchmarks.\n\nOn the other hand, by adding and updating the constraint (5b), we can sample more often in the vicinity of the optimal solution, which enhances the quality of the found samples.\nActually, we have validated this point in our numerical experiments (see Figure 4 and Figure 6).\nAccording to Algorithm 2, $N_{iteration}=1$ means no constraint (5b), $N_{iteration}=2$ means adding a constraint (5b), and $N_{iteration}=3$ means updating the constraint (5b).\nWe observe that a larger $N_{iteration}$ helps to reduce the objective difference, which corresponds to the mentioned sentence ''we demonstrated that the enhanced sampling helps reduce average relative error\".\n\n> **Major questions 3**: \nIn Conclusion section: ''The computational time of using ...... is significantly shorter than that of MiBS ......\"\nI admit that this is true, but your methods also do not reach optimality, for fair comparison you either need to have enough samples for exactly learning the value function (so that your approach can also produce a true optimal solution), or you need to compare the best feasible solution found by MiBS within a given time limit.\n\n**Response**: \nThanks for your comments.\nWe agree with the comment and in this paper, we have chosen the latter suggestion, which compares the best feasible solution found by MiBS within a given time limit.\nPlease see our response to Major Question 1 for more details.\n\n\n> **Minor questions 1**: \nI suggest to give an exact definition for ''binary tender\".\nAs far as I know, I do not think this is a well-known term in the community.\n\n**Response**: \nThanks for your suggestion.\nIn the revised paper, a more exact definition for ''binary tender\" is provided as\n\n*''Here, ''tender'' is defined as the linking variables between the upper- and lower-level problems and ''binary tender'' means that all tender variables are binary.''*\n\n> **Minor questions 2**: \nStill about the definition of ''binary tender\".\nOn Page 2 \"we assume that the entries of x appearing in the lower-level formulation are binary-valued\".\nHowever, in Appendix C, about the instance generation for the experiments, I notice that the binary constraint on x is enforced in the upper-level instead of the lower-level formulation.\n\n**Response**: \nThanks for your comments.\nWe would like to remark that the lower-level problem can be seen as a parametric optimization problem, where the upper-level variables $x$ are treated as given parameters.\nTherefore, the binary constraint on $x$ should be enforced in the upper level, but not in the lower level where $x$ becomes parameters."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2807/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700504423542,
                "cdate": 1700504423542,
                "tmdate": 1700504911014,
                "mdate": 1700504911014,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4hqjRxETXk",
                "forum": "PsDFgTosqb",
                "replyto": "gctoahEluE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2807/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2807/Authors"
                ],
                "content": {
                    "title": {
                        "value": "looking forward to post-rebuttal feedback!"
                    },
                    "comment": {
                        "value": "Dear Reviewer skLM\n\nThank you for reviewing our paper. We have carefully answered your concerns about sampling, time limits, and experimental setup.\n\nPlease let us know if our answers accurately address your concerns. If our response resolves your concerns, we kindly ask you to consider raising the rating of our work. Thank you very much for your time and efforts! We would like to discuss any additional questions you may have.\n\nBest, Authors"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2807/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700686793019,
                "cdate": 1700686793019,
                "tmdate": 1700686793019,
                "mdate": 1700686793019,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "agCXdhfVts",
                "forum": "PsDFgTosqb",
                "replyto": "gctoahEluE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2807/Reviewer_skLM"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2807/Reviewer_skLM"
                ],
                "content": {
                    "comment": {
                        "value": "Regarding my \"Weakness 1\", I'm still not fully convinced by your explanation. I cannot see the necessity of solving a separating quadratic programming simply to get some feasible point in the interior of the feasible region. You mentioned \"as n gets larger, due to the exponentially increasing input space, feasible solutions become more scarce and it gets harder to obtain samples with respect to feasible\", but since the feasible region does not only have finitely many element, you can totally design some much easier sampling method based on the formulation of the feasible region. That being said, I appreciate your detailed response to my questions and it has addressed most of my other concerns. Therefore, I decide to raise my point to 6."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2807/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700720779945,
                "cdate": 1700720779945,
                "tmdate": 1700721498780,
                "mdate": 1700721498780,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]