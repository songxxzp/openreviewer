[
    {
        "title": "Conformal Prediction via Regression-as-Classification"
    },
    {
        "review": {
            "id": "AYMj9GGJFt",
            "forum": "rulxyXjf46",
            "replyto": "rulxyXjf46",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1120/Reviewer_ZE11"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1120/Reviewer_ZE11"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a new method to model the conditional density in heteroscedastic regression problems. The idea is to convert metric regression to an ordinal regression problem, in which the conditional density is approximated by a number of fixed bins. This is in essence a histogram approach. Unlike most ordinal regression methods, the authors do not consider an underlying latent continuous variable, but they model every bin via a separate neuron propagating from the embedding layer. A softmax operation is used to guarantee that a valid probability density function is returned, but, unlike classification methods, a loss function that incorporates the order in the bins is used during training. \n\nIn a conformal prediction type of experiments, the authors show that this new model results in better prediction sets compared to some baselines for modelling the conditional density, such as quantile regression and kernel density estimation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is well written and easy to follow\n- The paper has some novelty, although I would argue that the core idea is just an application of reference Stewart et al. \n- I enjoyed reading this paper, and I am convinced that the method can work relatively well for regression problems with multimodal conditional density distributions\n- I liked that some limitations of the method are discussed at the end. However, I do see other limitations as well."
                },
                "weaknesses": {
                    "value": "I do see several important weaknesses. \n\n1. Contributions:\nAfter reading the introduction it was not clear to me what the contributions are. This only became clear to me after reading the experiments. Contributions should be clear from the beginning. To my opinion, the main contribution is proposing a new nonparametric method for modelling the conditional density. However, I don't see any contribution w.r.t. conformal prediction. In the experiments, the authors only consider the standard split conformal prediction, which is arguably not the most useful type of evaluation for heteroscedastic regression problems. I would argue that conditional coverage guarantees need to be analyzed in such cases. This can be established via Mondrian conformal prediction or normalized nonconformity scores, such as normalized absolute residuals. Estimates of the variance could be obtained with any method that models conditional densities, or with methods that directly estimate the variance, such as mean-variance neural networks. So, in that regard, I find the scope and the experimental setup quite limited. \n\n2. Baselines:\nThe authors consider a few other methods for estimating the conditional density, such as kernel density estimation and quantile regression. However, many other methods exist for estimating the conditional density. Since this is the key contribution of the paper, I would like to see a more thorough experimental and theoretical comparison with such methods. Examples of such methods are mean-variance neural networks, conditional transformation models and methods based on normalizing flows. The proposed method estimates the conditional density via a specific histogram. Methods based on nearest neighbors and regression trees also produce histogram-style conditional densities. Another approach to model multimodal distributions is via Gaussian mixtures. So, there is a lot out there already... Where should we situate the new method is this diverse landscape? I would classify the new method as yet another way of modelling the conditional density. Not better or worse than others, perhaps useful in specific situations, such as multimodal distributions, but more discussion is needed.  \n\n3. Limitations of the method:\n- By modelling the conditional density as a histogram, one obtains a very non-smooth approximation of the density. So, that's why interpolation is needed to obtain good nonconformity scores. Some of the other methods that are described above don't have this limitation. \n\n- The number of bins looks like a very tricky hyperparameter to tune. The performance probably highly depends on that hyperparameter. What would be a good way to tune this hyperparameter? Another tricky hyperparameter is the level of entropy regularization. I would assume that the results are also very sensitive to that hyperparameter.\n\n- I believe that the proposed method might work reasonably well for multimodal distributions that are not too complex, but I would not recommend this method for unimodal regression problems. For such problems, there are too many degrees of freedom.\n\n4. Unclear aspects of the experiments\n- In Table 3 a lot of standard deviations are zero, or close to zero. How is that possible? What is randomized? With randomizations of the testset these numbers are unrealistically small I would say. \n- Table 1 versus Table 3: normally one would see that the length of the intervals increases when the coverage increases. So, the two criteria highly depend on each other. Any comparison of two methods should always make the trade-off between those two criteria."
                },
                "questions": {
                    "value": "See above.\nIn addition, what's novel in this paper compared to reference Stewart et al.?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1120/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1120/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1120/Reviewer_ZE11"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1120/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698219792479,
            "cdate": 1698219792479,
            "tmdate": 1699636038233,
            "mdate": 1699636038233,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AdEnc00HYb",
                "forum": "rulxyXjf46",
                "replyto": "AYMj9GGJFt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1120/Authors"
                ],
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1120/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1120/Authors",
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission1120/Reviewers"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1120/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal to Reviewer ZE11 (part 1)"
                    },
                    "comment": {
                        "value": "We thank this reviewer for their thoughtful review. \nWe believe that  the reviewer has several confusions regarding some of the concepts regarding conformal prediction methodologies relevant to our work. We hope this response clears up all of the confusions and we will greatly appreciate an increase in score!\n\n> *the main contribution is proposing a new nonparametric method for modelling the conditional density. However, I don't see any contribution w.r.t. conformal prediction*\n\nMost papers on CP  focus on the choice of score function. Depending if one want to consider heteroscedasticity, bimodality, skewness etc. Our contribution single handedly covers these case contrarily to DCP, CQR, CHR.\n\n> *experimental and theoretical comparison with other methods for estimating the conditional density*\n\nThe goal of our work is to improve conformal prediction, rather than proposing a better method for conditional density estimation (which is a challenging problem in itself). This is why we use best performing CP methods as baselines. Showing that better conditional density leads to better CP method would be an interesting direction, but would lead to a different paper.  \n\n> *Why not add additional baselines such as mean-variance networks?*\n\nThese methods of Conditional density estimation have not been used in CP literature before. While they are reasonable ways to estimate conditional density, our method is very simple. There are many ways to do conditional density estimation like you mentioned (see Rothfuss et al.). However, these are not any simpler and are more involved than our technique. Moreover, the existing CP baselines do not compare with these techniques either. We do compare with methods like KDE or Histogram Regression which are both conditional density estimators and the latter uses neural networks. Relative to relevant papers such as CHR, we compare to the same amount or more baselines, so we believe our experimental setup is sufficient.\n\n> *The standard split conformal prediction, which is arguably not the most useful type of evaluation for heteroscedastic regression problems.*\n\nWe are afraid that the reviewer is confusing split conformal methods (which is an alternative to the highly expensive Full conformal methods) and the choice of the score function that account for heteroscedasticity. For example, CQR or DCP explicitly account for heteroscedasticity and comes with both Split and Full version, where the former is preferred for computational efficiency.\n\n> *Conditional coverage guarantees need to be analyzed in such cases. This can be established via Mondrian conformal prediction*\n\nThere are several types of conditional guarantees Mondrian CP are designed to be conditional on the class of the input object. In this case, the p-values function itself is modified by including a specific taxonomy (e.g. provided by a classifier or clustering algorithm). These are available in both the Split CP version and the Full CP version (or even other cross-conformal alternatives). Being split or not is about computational efficiency. Heteroscedasticity or other properties are handled by the choice of the conformity score function. \n\nWe would like to add that CQR provides a benchmark against variance-based normalization and achieved superior performance.\n\nBy conformalizing our density estimation leveraging a (Mondrian or Venn) taxonomy, our method trivially accounts for class conditionality since this is independent of the choice of score function (it is just about the choice of p-value function). For a review of these concepts, please refer to Chapter 4.6.1 of the book 'Algorithmic Learning in a Random World', second edition, by Vovk, Gammerman, and Shafer.\n\nMoreover, most of the baselines in regression CP focus on split conformal prediction. It is a standard technique in the CP literature. We believe it is unfair to reject a paper for using a standard evaluation setup. \n## References\n\nJonas Rothfuss, Fabio Ferreira, Simon Walther, and Maxim Ulrich. Conditional density estimation\nwith neural networks: Best practices and benchmarks. arXiv preprint arXiv:1903.00954, 2019."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1120/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700265723178,
                "cdate": 1700265723178,
                "tmdate": 1700275697207,
                "mdate": 1700275697207,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5ssjPyol6U",
                "forum": "rulxyXjf46",
                "replyto": "AYMj9GGJFt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1120/Authors"
                ],
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1120/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1120/Authors",
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission1120/Reviewers"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1120/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal to Reviewer ZE11 (part 2)"
                    },
                    "comment": {
                        "value": "> *We learn nonsmooth distribution but other papers don\u2019t.* \n\nIf you look at Figure 4 in the appendix, we learn pretty smooth distributions out of the box. Interpolation is *not* used for this. It is only used to make a discrete density function into a continuous one. Our interpolation is a simple linear interpolation, which does no extra smoothing on top. In fact, the smoothness is a strength of our method. If you look at CHR, they learn very spiky distributions.\n\n> *You are sensitive to hyperparameter tuning.*\n\nNo tuning was done at all. We hold $K=50$ bins and $\\tau=0.2$ across all experiments which span 16 different datasets with widely varying distribution shapes. If tuning was done, we would expect even better results. We will add ablations in the final version of the paper to investigate how our results are dependent on these hyperparameters. \n\n> *This won\u2019t work on unimodal distributions.*\n\nThis is not correct. Many of the data such as MEPS are unimodal and we get the best results out of all methods. \n\n>  *There are unclear aspects of the experiments*\n\nWe repeat the experiments over different seeds and report the average mean over all seeds. The value you are referring to in Table 3 is the standard error (we have clarified this in the uploaded version). Since we use a large proportion for the test set, this suggests that our method performs well across numerous seeds, and small standard errors are reasonable. \n\n> *There is a tradeoff between coverage and length*\n\nWe have fixed all experiments at coverage level of $90\\%$. We have specified this in newly uploaded versions. This is standard in the CP literature.\n\n> *How does this differ from Stewart et al.?*\n\nStewart et. al focuses on learning conditional expectation while we focus on conditional density. Using their loss function naively performs worse than our smoothened version of the loss function according to the ablations. We will clarify better in the final version that Stewart et al\u2019s loss is included in the ablations and did not work. Our loss function is tailored to our specific problem, for example with smoothing."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1120/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700265752938,
                "cdate": 1700265752938,
                "tmdate": 1700275704317,
                "mdate": 1700275704317,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "txy0pPEMWY",
                "forum": "rulxyXjf46",
                "replyto": "AYMj9GGJFt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1120/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1120/Authors"
                ],
                "content": {
                    "title": {
                        "value": "End of discussions period"
                    },
                    "comment": {
                        "value": "Dear Reviewer ZE11,\n\nThe discussion period is about to conclude, and despite your did not engage with us yet, we are happy that you enjoyed reading our paper and find it convincing for regression problems with multimodal distributions. \n\nUnfortunately, you have the lowest score among the reviewers, and the highest confidence. \n\nWe have diligently replied to all your concerns and several confusions, in detail in our rebuttal, and we would be delighted to provide any further clarifications. We believe that not all the limitations you have expressed are well-founded, and this certainly explains your rating. If our response is not satisfactory, please let us know asap, and we will be happy to add further details in the short time available.\n\n\nBest regards,\n\nAuthors"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1120/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732683689,
                "cdate": 1700732683689,
                "tmdate": 1700732683689,
                "mdate": 1700732683689,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4Hxjb5og7i",
            "forum": "rulxyXjf46",
            "replyto": "rulxyXjf46",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1120/Reviewer_iHe5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1120/Reviewer_iHe5"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes to convert 1D regression problems into K-class classification problems by partitioning the label space into K bins.  This\nallows leveraging known techniques for classification CP. Like Lei et al. 2004, the authors use the conditional density produced by the classification model as a conformity score. On top of it, a regularization model is trained to enforce smoothness when the discrete distribution is converted back to a continuous one."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I like the idea of learning the conformity score is interesting.  Works like [1] also use density estimation to adjust the conformity score. But the authors' approach is intrinsically different.  The proposed objective function seems a good alternative to optimizing the efficiency of the prediction intervals."
                },
                "weaknesses": {
                    "value": "The role of the discretization step may be explained better. The authors could emphasize the difference with other techniques for estimating conditional densities or explain why the proposed approach does not suffer from the usual instability of standard estimators."
                },
                "questions": {
                    "value": "- Is the smoothness-enforcing penalty new? \n- How were K and Tau chosen? Is part of the data used for training the conditional distribution?\n- Why should the entropy regularization be expected to be good at learning bi-modal distributions?\n- The size of the prediction intervals is a good measure of efficiency. Another one is the correlation between the test errors and the corresponding intervals. It would be interesting to see a scatter plot of the absolute residual versus the prediction intervals in a couple of data sets where R2CCP is or is not the best algorithm.\n- Is the proposed approach equivalent to training a parameterized conformity function (see for example in [2] or [3])?\n\n[1] Guan, Localized conformal prediction: A generalized inference framework for conformal prediction (2023)\n[2] Einbinder et al., Training Uncertainty-Aware Classifiers with\nConformalized Deep Learning (2022)\n[3] Colombo, On training locally adaptive CP (2023)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1120/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698579110240,
            "cdate": 1698579110240,
            "tmdate": 1699636038160,
            "mdate": 1699636038160,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DwuYEYraoF",
                "forum": "rulxyXjf46",
                "replyto": "4Hxjb5og7i",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1120/Authors"
                ],
                "readers": [
                    "ICLR.cc/2024/Conference/Submission1120/Authors",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1120/Senior_Area_Chairs",
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission1120/Reviewers"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1120/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer Reviewer iHe5"
                    },
                    "comment": {
                        "value": "We thank this reviewer for their thoughtful review. We clarify your weaknesses and questions here. If you find this sufficient, we would greatly appreciate an increase in score!\n\n## Weaknesses\n\n> *What is the role of discretization in this work?*\n\nSeveral works in the literature, including Stewart et al. mention that the classification problem tends to lead to stabler training than regression.  Stewart et al. empirically demonstrates that R2C estimates the conditional expectation better than regression alone. The reason that this is the case is an open theoretical problem. However, intuitively, the learning problem under R2C is an unconstrained learning problem with only $K$ outputs whereas methods like CHR have to have 1000 output nodes and average among them to achieve smoothness. \n\n## Questions\n\n> *Is the smoothness-enforcing penalty new?*\n\nSmoothing-enforcing penalty has existed in other contexts such as Variational Learning. However, in this context of learning conditional densities using Neural Networks, it is new to the best of our knowledge.\n\n> *How were K and Tau chosen? Is part of the data used for training the conditional distribution?*\n \n$K $, $p$, and $\\tau$ were fixed at $50$, $.5$, and $.2$ for all experiments. We didn't change that or tune that as that would be unfair to other methods. Tuning these would likely improve the results of our method. \n\n> *Why should the entropy regularization be expected to be good at learning bi-modal distributions?*\n\nIt\u2019s true. It is possible that reducing entropy can sometimes help, but as a general tool, entropy regularization helps overall. Empirically, our structure still works well for bimodal distributions.\n\n> *Is part of the data used for training the conditional distribution?* \n\nWe are not sure we understand the question. As usual, the dataset was split in two (training and calibration), and indeed, the training set was used to learn the conditional distribution.\n\n> *What is the correlation between the test errors and the corresponding intervals?*\n\nThanks for your suggestion. We will try to illustrate this plot for a couple of benchmarks for the camera ready version.\n\n> *Is the proposed approach equivalent to training a parameterized conformity function (see for example in [2] or [3])?*\n\nWe are very different from the Einbinder paper since their loss function tries to ensure its scores form a smooth CDF during training and is nondifferentiable. We do no such thing and use much simpler differentiable methods. The second Colombo paper proposes how to transform conformity scores to make it more suitable for APS.\n\nThanks for the additional reference, we will add [1] in our related work section.\n\n## References\n\nLawrence Stewart, Francis Bach, Quentin Berthet, and Jean-Philippe Vert. Regression as classifi-\ncation: Influence of task formulation on neural network features. In International Conference on\nArtificial Intelligence and Statistics (AISTATS), 2023."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1120/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700263873458,
                "cdate": 1700263873458,
                "tmdate": 1700275687308,
                "mdate": 1700275687308,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cyfEIT05RF",
            "forum": "rulxyXjf46",
            "replyto": "rulxyXjf46",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1120/Reviewer_HZzS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1120/Reviewer_HZzS"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes to see a regression problem as a classification one, and then to apply ideas issued from conformal prediction in order to derive predictions regions that may not be compact sets (here, union of intervals)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+: an interesting view of the problem, especially as this allows the conformal predictions to be easily something else than intervals, something that may indeed be of important interest for regression. No other conformal regression methods that I know of achieve this kind of things, with maybe the exception of \"Conformal prediction in manifold learning\" (not cited by the authors, but the paper is only weakly related to the present work), since intervals on the manifold may well turn out to be non-compact in the original space. \n\n+: a quite well written paper, and a method simple enough to be applicable in a wide range of settings. \n\n+: experiments that are convincing enough to show the interest of the method."
                },
                "weaknesses": {
                    "value": "-: the way authors frame the regression problem as a classification is very, very close to the standard ordinal regression problem, and I really missed some positioning with respect to this literature. There is a huge literature on ordinal regression (maybe look at a general paper, e.g., \"Tutz, G. (2022). Ordinal regression: A review and a taxonomy of models. Wiley Interdisciplinary Reviews: Computational Statistics, 14(2), e1545.\"), but also a couple papers using ordinal conformal classification (see questions)."
                },
                "questions": {
                    "value": "- Could you position the current paper with respect to papers dealing with ordinal conformal regression? Two I know of (there could be others, but not much more) are \"Xu, Y., Guo, W., & Wei, Z. (2023, July). Conformal Risk Control for Ordinal Classification. In Uncertainty in Artificial Intelligence (pp. 2346-2355). PMLR.\" and \"Lu, C., Angelopoulos, A. N., & Pomerantz, S. (2022, September). Improving trustworthiness of ai disease severity rating in medical imaging with ordinal conformal prediction sets. In International Conference on Medical Image Computing and Computer-Assisted Intervention (pp. 545-554). Cham: Springer Nature Switzerland.\". \n\n- It seems to me that the conformal scores used in Algorithm 1 mostly rely on modal values. In ordinal regression, it is much more common to use the average rank (under $L_2$ loss) or the median (under $L_1$ loss) as predictions. Could you comment on such options? \n\n- It would be nice to have an idea of how often the output predictions regions are not interval, to have an idea of how often we depart from classical methods. Would it be possible ot have an idea (in the appendix or main paper).\n\n- Could you display full coverage curve (say, from 90% to 99%) rather than table for a fixed value? (side remark: there is a double ?? in appendix B). \n\n- Bonus question for myself: do you have an idea of how the present method could be adapted to multi-variate output settings?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "not needed"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1120/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698592717386,
            "cdate": 1698592717386,
            "tmdate": 1699636038071,
            "mdate": 1699636038071,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ghtRxdnlEM",
                "forum": "rulxyXjf46",
                "replyto": "cyfEIT05RF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1120/Authors"
                ],
                "readers": [
                    "ICLR.cc/2024/Conference/Submission1120/Authors",
                    "ICLR.cc/2024/Conference/Submission1120/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission1120/Reviewers"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1120/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer to Reviewer HZzS"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their thoughtful review. We clarify the weaknesses and questions here. If you find this sufficient, we would greatly appreciate an increase in score!\n\n## Weaknesses\n\n> *Can you connect more to ordinal regression?*\n\nThanks for the suggestion. We completely agree with you that there are strong connections with ordinal regression. We noted on this connection with citations to Weigend & Srivastava (1995) and Diaz & Marathe (2019). More recently, in computer vision, also some investigation is done to understand the effectiveness of such methods (https://arxiv.org/pdf/2301.08915.pdf). We have also added several relevant citations to the ordinal regression literature in the related works in the newly uploaded paper. \n\n## Questions\n\n> *How does this paper compare to conformal ordinal classification papers?*\n\nWe have added these two new works to the related works in the newly uploaded document. \u201cConformal Risk Control for Ordinal Classification\u201d talks about different risk types (analogous to coverage) for ordinal classification while we discuss different score functions when coverage is your loss function. \u201cImproving Trustworthiness of AI Disease Severity Rating in Medical Imaging with Ordinal Conformal Prediction Sets\u201d talks about how you can adapt APS style conformal sets to work with ordinal structure, which was later used in the CHR paper.\n\n> *Why do we focus on modal values?*\n\nA key contribution of our paper is its versatility across different distributions via building a probability density function. Consider a bimodal label distribution; the median and mean would be very unlikely answers since they lie in the middle of two valleys. To account for such distributions, we found modal values to be a better fit.\n\n> *How many predictions are not intervals?*\n\nWe can do this in the final version of the paper.\n\n>  *Can you do a full coverage curve?*\n\nWe have fixed the broken reference in the appendix in the newly uploaded version. Moreover, we can work on the full coverage curve for the camera-ready version.\n\n> *Can this be extended to multi-variate output?*\n\nOur method can be adapted to multivariate outputs, and this is an exciting direction for future work. There is a direct naive approach, which scales like $K^d$, where $K$ is the number of bins and $d$ the dimension of the output. Developing more efficient ways which do not scale exponentially with the dimension of the output (but perhaps like $O(K*d)$) is not straightforward, but can be an interesting direction for future work. Thanks for the question!\n\nRaul Diaz and Amit Marathe. Soft labels for ordinal regression. In IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2019.\n\nAndreas S Weigend and Ashok N Srivastava. Predicting conditional probability distributions: A\nconnectionist approach. International Journal of Neural Systems, 6(02):109\u2013118, 1995."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1120/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700263643191,
                "cdate": 1700263643191,
                "tmdate": 1700275677426,
                "mdate": 1700275677426,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AVy9rFrgWD",
                "forum": "rulxyXjf46",
                "replyto": "ghtRxdnlEM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1120/Reviewer_HZzS"
                ],
                "readers": [
                    "ICLR.cc/2024/Conference/Submission1120/Authors",
                    "ICLR.cc/2024/Conference/Submission1120/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission1120/Reviewers"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1120/Reviewer_HZzS"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the feedback + sorry for the lack of time to interact"
                    },
                    "comment": {
                        "value": "Dear authors,\n\nThanks for the detailed feedback and answers, as well as for the minor modifications in the main body of the paper. Unfortunately I am currently involved in lab evaluation committees which take up to ten hours of work daily, and does not have the time to really engage in time-constrained, online discussions (such things are better suited to journal publications that are less time-pressured). I apologize for this, but will take a closer look at the answers and changes to make a final recommendation. \n\nHowever, as my questions were mostly clarification questions or small updates, and as I was already quite positive about the paper, I would not consider that the answers and change have signifincalty increased the quality of the paper. Also, while I appreciate the authors detailed answer and politeness, I would like to note that their request to reviewers to change scores may actually be counter-productive.\n\nBest regards"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1120/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700735099901,
                "cdate": 1700735099901,
                "tmdate": 1700735099901,
                "mdate": 1700735099901,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9goRzFpPsP",
            "forum": "rulxyXjf46",
            "replyto": "rulxyXjf46",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1120/Reviewer_RG4V"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1120/Reviewer_RG4V"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new way to do conformal prediction for regression based on conformalizing a regression-to-classification method. It is claimed that the new method offers higher flexibility compared to using a traditional regressor, making it more suitable for heteroscedastic or multimodal data."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper tackles a traditional conformal problem, but still seems to offer improvements to the state-of-the-art. \n- The method proposed is interesting; I had not previously seen this regression-as-classification setup in literature and appreciate the authors providing a number of references to the broader framework. \n- The paper is easy to follow, although there are some issues which I will point out in weaknesses. I also noticed the choice to not state and/or highlight the conformal coverage guarantee, which is often oversold nowadays. (I still think you should state the guarantee at least in the Appendix so that readers unfamiliar with conformal prediction are aware of it.)"
                },
                "weaknesses": {
                    "value": "## Writing \nSome parts of the paper were difficult to follow: \n- The proposed method was not easy to understand before seeing Algorithm 1. I felt Section 3 had too many comments on \"motivation\" that do not focus on the actual method and are a bit counterproductive to a quick read. E.g., Sec 3.1, \"We  aim to compute... classification context\" does not add much. In the end, you have a traditional neural network with softmax output, which everyone understands. Similarly the comment \"This approach is both straightforward and efficient ... information or structure.\" does not add anything substantial. Sec 3.2 \"It would be desirable to be able to use similar methods for both classification and regression conformal prediction\". This is a somewhat subjective (and distracting) claim that I believe is best avoided. The paragraph starting with \"These values y\u02c6 \u2208 Y\u02c6 f...\" is unnecessarily verbose. Sec 3.3 introduces a loss function without giving an example of what it could be. \n- Table 1 is very hard to understand since (i) the methods are not connected to the acronyms, (ii) it is not clear what is inside the brackets, (iii) length and coverage is not defined in the paper. \n\n## Technical concerns\n- Most results are not reported with error bars making them less reliable. For instance, in Table 1, CHR does worse than CQR but it should not? (both methods are proposed by similar authors and CHR comes later). \n- Have you considered a baseline where you learn a regressor directly using the same neural network architecture, treat that regressor as \\bar{q}, and apply the same conformalization? It is similar to the \"optimal prediction sets\" in Appendix F here: https://arxiv.org/pdf/1910.10562.pdf . \n\nI am not familiar with the regression-as-classification (R2C) literature. The main concern I have is that it is unclear which aspects of the R2C method are novel and which ones directly derive from previous work. Have you made significant changes to previous R2C methods to adapt them to the conformal problem? The key relevant technique seems to be smoothing; has these been proposed before?"
                },
                "questions": {
                    "value": "- Could you elaborate on what linear interpolation is used to go from q_\\theta to \\bar{q}_\\theta? It is not clear to me at all. \n- You have pointed out a number of papers which advocate for regression-as-classification. However, it is still not convincing as to why we should first destroy the ranking structure of reals, and then reinstate it using the entropic regularization. Why not just do usual regression?\n- Which aspects of the R2C method are novel and which ones directly derive from previous work? Have you made significant changes to previous R2C methods to adapt them to the conformal problem? The key relevant technique seems to be smoothing; has these been proposed before?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1120/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1120/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1120/Reviewer_RG4V"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1120/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698876639579,
            "cdate": 1698876639579,
            "tmdate": 1700838584734,
            "mdate": 1700838584734,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DiNurIn6ah",
                "forum": "rulxyXjf46",
                "replyto": "9goRzFpPsP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1120/Authors"
                ],
                "readers": [
                    "ICLR.cc/2024/Conference/Submission1120/Authors",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1120/Senior_Area_Chairs",
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission1120/Reviewers"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1120/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer to Reviewer RG4V"
                    },
                    "comment": {
                        "value": "We thank this reviewer for their thoughtful review. We clarify your weaknesses and questions here. If you find this sufficient, we would greatly appreciate an increase in score.\n\n## Weaknesses\n\n> *The proposed method was not easy to understand before seeing Algorithm 1*\n\nAlthough we understand your concern, we do not want to assume that everyone is familiar with conformal prediction and its connection to our estimated model. Thus, we first expand on what motivates our proposal and connect it to the existing literature before diving into more technical details. We will consider some of your suggestions to clarify the final version of the paper. \n\n> *Table 1 is very hard to understand*\n\nThanks, we agree with your points. We have fixed these in the newly uploaded version.  We have specified that the brackets include the \u201cStandard Error\u201d over the experiments. We also provide definitions for length and coverage. We believed the graphs and terms were standard enough in the literature. To avoid confusion, we will make sure to be more specific in the final submission.\n\n> *Most results are not reported with error bars making them less reliable. For instance, in Table 1, CHR does worse than CQR but it should not?*\n\nWe run all of our experiments by averaging over the test set and over 5 different seeds. With every different seed, we retrain the model. The standard error of the results over the different seeds is shown in the brackets. Given that we are running experiments across significantly more datasets than the ones used in the CHR and CQR papers, it is reasonable that upon seeing new datasets, CHR does worse than CQR in some cases, and in some cases, CHR outperforms CQR. For both baselines, we use code written by the authors themselves and do not edit them at all. **We report the numbers given by their open-sourced code explicitly.** \n\n\n>  *Have you considered a baseline where you learn a regressor directly using the same neural network architecture, treat that regressor as $\\bar{q}$, and apply the same conformalization?*\n\nFrom what we are aware of, learning conditional density for regression problem using neural network require more involved methods with specific prior such as Gaussian mixture parametrization or Bayesian network (see Rothfuss et al.). Here, we are interested in computing a confidence set instead of the harder problem of getting accurate density estimation. As such, we propose to bypass those difficulties by converting the problem into a multiclassification problem where the parametrization is simpler. As you stated above, we rely on  \u201ca traditional neural network with softmax output, which everyone understands\u201d. Also, we note that CHR uses a histogram powered by a neural network to estimate the conditional density in a regressor setting, as you suggest. On top of that, we use 6 baseline effective conformal methods in our benchmark. Of course, we can always add more, but we believe that our numerical experiments are revealing enough. \n\n> *Which aspects of the R2C method are novel?*\n\nAs is presented, this method of R2C for conformal prediction has not been done before. Smoothing in different contexts has been proposed, such as in ordinal regression, but not in R2C literature. However, Stewart et al. is the standard R2C loss function, and using that directly does not work (see our ablation studies). We will make it clearer that the loss function from Stewart et al. is the one studied in the ablations. They focus on conditional expectation, whereas our loss focuses on conditional density. Our smoothened loss function works better according to the ablations. This use of R2C to learn conditional density alongside smoothing is novel to this paper. \n\n## Questions \n\n> *What is the exact formulation of linear interpolation*\n\nWe have added this in the newly uploaded version. We clarify it as below (note here the $\\hat{y_k}$ and $\\hat{y_{k+1}}$ are chosen such that  $\\hat{y_k} < y < \\hat{y_{k+1}}$: \n\n\n$\\bar{q}(y \\mid x) = $ $\\frac{(\\hat{y_{k + 1}} - y) q(\\hat{y_k} \\mid x)}{\\hat{y_{k + 1}} - \\hat{y_k}}  $ $+ \\frac{(y - \\hat{y_{k}}) q(\\hat{y_{k+1}} \\mid x)}{\\hat{y_{k + 1}} - \\hat{y_k}} $\n\n> *Why use R2C versus standard regression?*\n\nSeveral techniques exist to use regression techniques to learn conditional densities, as shown in our baselines. However, these methods can suffer from poor stability during training. For example, see Figure 4, and you will see that CHR\u2019s technique using regressors is not smooth. Our loss function results in smoother and more stable densities. In fact, the Stewart et al. paper demonstrates empirically where R2C outperforms regression techniques for estimating the conditional expectation. Why this is the case theoretically is still an open research problem, as far as we know."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1120/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700263430351,
                "cdate": 1700263430351,
                "tmdate": 1700275659407,
                "mdate": 1700275659407,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "e3oPdytmiz",
                "forum": "rulxyXjf46",
                "replyto": "9goRzFpPsP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1120/Authors"
                ],
                "readers": [
                    "ICLR.cc/2024/Conference/Submission1120/Authors",
                    "ICLR.cc/2024/Conference/Submission1120/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission1120/Reviewers"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1120/Authors"
                ],
                "content": {
                    "title": {
                        "value": "References"
                    },
                    "comment": {
                        "value": "Lawrence Stewart, Francis Bach, Quentin Berthet, and Jean-Philippe Vert. Regression as classifi-\ncation: Influence of task formulation on neural network features. In International Conference on\nArtificial Intelligence and Statistics (AISTATS), 2023\n\nJonas Rothfuss, Fabio Ferreira, Simon Walther, and Maxim Ulrich. Conditional density estimation\nwith neural networks: Best practices and benchmarks. arXiv preprint arXiv:1903.00954, 2019."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1120/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700267325557,
                "cdate": 1700267325557,
                "tmdate": 1700275669552,
                "mdate": 1700275669552,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GXSw7PqHaF",
                "forum": "rulxyXjf46",
                "replyto": "9goRzFpPsP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1120/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1120/Authors"
                ],
                "content": {
                    "title": {
                        "value": "End of discussions"
                    },
                    "comment": {
                        "value": "Dear Reviewer RG4V,\n\nWe appreciate your review and would be grateful if you could let us know if our response resolved the issues you raised.\nWe regret that the review period is ending and we have not had the opportunity to discuss your questions further with you. Consequently, we are concerned that your rating is still below the acceptance threshold.\n\nIf our response was satisfactory, please leave a comment and update your score accordingly. We would be pleased to provide further explanations if desired.\n\nBest,\n\nAuthors"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1120/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733258384,
                "cdate": 1700733258384,
                "tmdate": 1700733258384,
                "mdate": 1700733258384,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]