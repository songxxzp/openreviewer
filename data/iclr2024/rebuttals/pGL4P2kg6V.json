[
    {
        "title": "Towards Interpretable Continual Learning Through Controlling Concepts"
    },
    {
        "review": {
            "id": "668l6DI49J",
            "forum": "pGL4P2kg6V",
            "replyto": "pGL4P2kg6V",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4845/Reviewer_6BcB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4845/Reviewer_6BcB"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a class incremental learning algorithm based on incrementally growing concept bottlenecks. It uses CLIP-dissect to identify neurons responsible for certain concepts and uses GPT to generate relevant concepts for a given class label. It freezes the part of the network that is responsible for previously learned concepts and adds new concepts for new classes. Lastly, the network maps from concepts to classes like in Concept Bottleneck Models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Using interpretable concepts as middle points to guide through incremental class learning is an interesting idea.\n- Using pretrained models (backbone, CLIP, GPT) to assist continual learning is a novelty.\n- Experimental results show that the proposed method is superior to other continual learning algorithms."
                },
                "weaknesses": {
                    "value": "- Since the paper utilize a pretrained backbone, there is not much difference between the proposed method and the baselines. Moreover, it is unclear whether the gain comes from its continual learning ability or just the concept bottleneck. It would be good to see whether the proposed GPT+Concept Bottleneck procedure works well for a non-incremental learning setting.\n- The paper is most related to DEN, but there is no comparison to the method. The paper could be compared to DEN by having the same pretrained backbone network with additional two layers learned by DEN instead of incremental concept bottlenecks.\n- The paper lacks clarity on the GPT concept generation and filtering procedure. It would be helpful to give examples on what the concepts are (move some figures from Appendix to main text). It is also important to share the text prompts used to generate concepts."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4845/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699127146378,
            "cdate": 1699127146378,
            "tmdate": 1699636468030,
            "mdate": 1699636468030,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2Ss2Jrg380",
                "forum": "pGL4P2kg6V",
                "replyto": "668l6DI49J",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4845/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4845/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "Thank you for your positive feedback! We are glad to learn that you think our work is interesting and novel. Below we provide a few more details in response to your comments and requests. \n\n**#1 Details of CC-CBM**\n\nWe describe the CC-CBM in Section 4 (p.5 -p.7). Even though it uses a pretrained backbone, we think it is very different from other baselines [2,3,4]. We combine CC with LF-CBM [1] to build CC-CBM. CC is for its continual learning ability. To know where performance gain comes from, we need to investigate CC-CBM in two steps:\n1. *Finetune standard model v.s. Finetune CBM* : To understand the impact of the concept bottleneck.\n2. *Finetune CBM v.s. CC-CBM* : To understand the impact of CC.\n\nBoth points have been done in our paper, please see the main results (Section 5.1 from p.7- p.9) and Appendix (p.23 - p.26):\n* Overall, Finetune CBM outperforms Finetune standard model by up to 2.3% in $\\bar{A}_T$ and up to 16.4% in $\\bar{F}_T$. \n* CC-CBM makes CBM\u2019s continual learning ability even better, as it outperforms Finetune CBM by up to 1.8% in $\\bar{A}_T$ and up to 12.4% in $\\bar{F}_T$.  \n\nThe non-incremental learning setting is studied in LF-CBM, which shows comparable performance to the standard model. In contrast, in this paper we have studied the GPT + concept bottleneck in the continual learning setting, which shows that this direction is promising and is a novel solution to address the catastrophic forgetting as shown in Section 5.1 of the main paper (p.7- p.9).\n\n**#2 Comparison with DEN**\n\nFollowing your instructions, we compare CC-CBM (ours) and Finetune-CBM (ours) with DEN in below Table R1. Our methods outperform DEN by 15.07% and 15.81% in $\\bar{A}_T$ respectively. \n\nTable R1: Experiment results on CIFAR-100 in a 5-task scenario.\n\n|              | $\\bar{A}_T$ |\n| ------------ | ------------ |\n| DEN          | 9.18         |\n| Finetune-CBM (Ours) | **24.99**        |\n| CC-CBM (Ours)      | 24.25        |\n\n\n\n**#3 GPT-3 usage and example**\n\nCC-CBM is builded upon LF-CBM [1]. It follows the procedure described in LF-CBM to use GPT-3. Example prompts:\n```\n\u2022 List the most important features for recognizing something as a {class}:\n\u2022 List the things most commonly seen around a {class}\n\u2022 Give superclasses for the word {class}:\n\n```\n\n\n**#4 Summary**\n\nTo summarize, we have:\n1. Discussed in **#1** on the details of CC-CBM.\n2. Provided in **#2** for the comparison with DEN.\n3. Discusses in **#3** on the GPT-3 usage.\n\nWe believe we have addressed all your concerns. Please let us know if you still have any reservations and we would be happy to address them. \n\nReference: \n\n[1] Label-free Concept Bottleneck Models. ICLR 2023\n\n[2] Self-Sustaining Representation Expansion for Non-Exemplar Class-Incremental Learning. CVPR 2022\n\n[3] Training networks in null space of feature covariance for continual learning. CVPR 2021\n\n[4] Gradient episodic memory for continual learning. NeurIPS 2017"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4845/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700383823023,
                "cdate": 1700383823023,
                "tmdate": 1700385032598,
                "mdate": 1700385032598,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Zp5pW6l2Y9",
            "forum": "pGL4P2kg6V",
            "replyto": "pGL4P2kg6V",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4845/Reviewer_zdWA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4845/Reviewer_zdWA"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a framework using CLIP Dissect to alleviate catastrophic forgetting in continual learning by controlling concepts. A neuron is denoted as a \u201cconcept unit\u201d if it activates highly, and is hence highly correlated with, a human-understandable concept. These concepts are architecturally added, frozen and reused in subsequent tasks. A continual extension to concept bottleneck models is also presented, which builds on top of Label Free-CBMs."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Neuron-level interpretability used for continual learning is an underexplored direction. The paper builds upon several existing works like CLIP Dissect and LF CBMs, using them in a continual setting. The notion of concept evolution was quite interesting. Additionally, the background and related work sections are covered well. I also appreciate the detailed experimental studies presented in the Appendix."
                },
                "weaknesses": {
                    "value": "**Scalability** \nThere is no analysis provided on the order of the number concepts added to Ct for new tasks. This would affect how scalable the method is, especially as it is mentioned that repeated concepts in subsequent tasks are added to the concept set as well.\n\n**Motivation**\nThe method does not seem to necessitate having interpretable concepts to alleviate catastrophic forgetting. The same thing could have been carried out on the classification layer itself using a vision-language aligned model and backbone. To be more specific, the entire dissection and subnetwork search process could have been performed directly on the classes. How are concepts or rather interpretability in general helping here? The paper seems to be attempting to address two different albeit related things, although the motivation for doing so is not very clear.\n\n**Formulation**\n* By design, it appears that the proposed method can only be used for CNN models and not transformer architectures. It would be nice to see how the proposed work can be more contemporary in its application.\n* In the freeze-all variant, it is possible that classes in newer tasks may be based on concepts that were available earlier. How would the model learn these associations if the weights for old tasks are not allowed to change?\n\n**Experiments**\n* The paper shows experiments on relatively small datasets. Related to my point on scalability, I would like to see some results on larger scale datasets.\n* The baselines for CL are not contemporary \u2013 there have been several state-of-the-art baselines for CL in the last 2 years, which are not considered. Additionally, no existing continual interpretable baselines have been included like ICICLE (ICCV 2023). (While I understand that the ICCV conference happened after the ICLR deadline, this work was available on arXiv since March 2023, https://arxiv.org/abs/2303.07811)\n* I would also like to see some analysis on other vision-language aligned models like FLAVA.\n* It would have been nice to see some discussion on subnetworks beyond Sec 3. How big were the learned subnetworks? How many weights were actually frozen on the different datasets?\n\n**Presentation**\nThe writing is unclear in a few places. For example: \n* \u201cit\u2019s not considering classification accuracy of CBM in continual learning setting, which is different than our goal.\u201d (pg 5) and \u201cthe Concept Controller strategy follows the similar idea as CC\u2019s in step 4\u201d (pg 6). It is difficult to understand what is trying to be conveyed in these sentences. \n* In Fig 3, it is not clear whether the network is from top to bottom or the other way, since there are no arrows. This makes it hard to understand the two schemes.\n* In Sec 3, the paper states that the subnetwork is frozen. In Sec 4, it states that the concepts are frozen. Is a concept a neuron or a sub-network? This is unclear. \n* Since the main premise of this work is on concepts and their subsequent use of interpretability, it would have been nice to see results such as Figures 6 and 7 in the main paper. The primary results in the main paper are all standard CL metrics. Note that Tables 3 (and 8 in the Appendix) only studies the concept consistency \u2013 it does not study interpretability."
                },
                "questions": {
                    "value": "1. On expanding the concept set in successive tasks, it is stated that existing concepts are also added to the current concept set as they could capture a different context. Please clarify how this context is captured.\n2. How does the freezing strategy of concept controller account for old concepts occurring in new classes?\n3. How does the framework scale to large datasets?\n4. Other than the fact that a neuron-level interpretable model is being used, is such a model even necessary to the problem the paper attempts to address? As the same purpose could be served by using any VL-aligned model."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4845/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699247761708,
            "cdate": 1699247761708,
            "tmdate": 1699636467947,
            "mdate": 1699636467947,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "taO9mXM9eX",
                "forum": "pGL4P2kg6V",
                "replyto": "Zp5pW6l2Y9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4845/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4845/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your valuable feedback, please see our reply below.\n\n**#1 Scalability**\n1. Baseline papers [1, 2, 3] mostly focus on the CIFAR dataset. TinyImagenet is the largest dataset in all baseline papers we compared except SSRE [1] which does experiments in ImageNet-100. Due to the rebuttal constraint, we have tried to add additional experiments on large scale dataset (e.g. ImageNet-10), and the results are in Table R1 and R2. The experiment results are still under the same trend: CC outperforms other exemplar-free methods by up to 0.87% in $\\bar{A}_T$ and up to 5.77% in $\\bar{F}_T$. Meanwhile, can improve exemplar-based methods by combining them by up to 1.83% in $\\bar{A}_T$ and up to 2.87% in $\\bar{F}_T$.\n2. In Table R3 and Table R4, we provide the number of concepts added in the concept bottleneck layer (CBL) for CC-CBM. \n3. For large-scale datasets in common continual learning benchmarks, our methods perform well, as shown Table 1 and Table 2 (p.8 -p.9) in the draft and the new ImageNet-10 experiments. Due to the expansion ability of CC-CBM, we believe that we can handle other large-scale datasets as well.\n \nTable R1: Experiment results on ImageNet-10 in a 5-task scenario.\n|                    | $\\bar{A}_T$ | $\\bar{F}_T$ |\n| ------------------ | ------------ | ------------ |\n| Finetune           | 23.33        | 81.20        |\n| EWC                | 24.14        | 78.46        |\n| CC_sol0 (Ours)     | **25.01**        | **77.20**        |\n| GEM                | 31.34        | 61.71        |\n| CC_sol0_GEM (Ours) | **33.17**        | **59.65**        |\n\nTable R2: Experiment results on ImageNet-10 in a 5-task scenario, the backbones are pretrained on Place365 dataset.\n|                   | $\\bar{A}_T$ | $\\bar{F}_T$ |\n| ----------------- | ------------ | ------------ |\n| Finetune          | 23.83        | 81.92        |\n| EWC               | 25.40        | 78.98        |\n| CC_CBM (Ours)     | **26.12**        | **73.21**        |\n| GEM               | 32.66        | 60.27        |\n| CC_CBM_GEM (Ours) | **34.37**        | **57.40**        |\n\nTable R3: Average concepts added per task in a 5-task scenario.\n|                   | CIFAR-10 | CIFAR-100 | CUB200 | ImageNet-10 | TinyImageNet |\n| ----------------- | -------- | --------- | ------ | ----------- | ------------ |\n| Number of Concept | 37.4     | 133.1     | 276.6  | 35.6        | 301.8        |\n\nTable R4: Average concepts added per task in a 10-task scenario.\n|                   | CIFAR-100 | TinyImageNet |\n| ----------------- | --------- | ------------ |\n| Number of Concept | 154.6     | 284.3        |\n\n**#2 Motivation for the role of concepts and interpretability**\n\nWe agree that the subnetwork search process can be performed on the classification layer directly, just like DEN [2]. However, this gives us no idea about what knowledge and information is learned and preserved in the model, which lacks interpretability. This is the main motivation of our proposed method. In the detailed motivation (please see **#5** in the **Author Response (2/2)** to **Reviewer Hm1u**), we saw the catastrophic forgetting of concepts in other baselines, which motivated us to design the proposed Concept Controller.  As experiment results in the main results (Section 5.1 from p.7- p.9) and Appendix (p.23 - p.26) show, controlling concepts improves the model\u2019s performance in continual learning. Moreover, our methods provide interpretability, which is different from other continual learning algorithms. Finally, We added a comparison between our method and DEN, please see **#2** in the **Author Response** to **Reviewer 6BcB**.\n\n**#3 Questions regarded to vision-language aligned (VL-aligned) model**\n1. We build CC-CBM based on LF-CBM [4]. Following LF-CBM\u2019s procedure, we use CLIP to calculate the activation matrix $P$ as described in Section 2.2 (p.3). We can replace CLIP with other VL-aligned models as long as they have a text encoder and an image encoder to calculate matrix $P$. We would like to point out that the choice of the VL-aligned model is related to LF-CBM\u2019s design instead of the continual learning problem. We will add this discussion in the revision and leave it as a future work.\n2. As we discussed in **#2**, controlling concepts provides interpretability and improves performance. Just using a VL-aligned model is not sufficient to understand the concepts inside itself."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4845/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700383071585,
                "cdate": 1700383071585,
                "tmdate": 1700384187407,
                "mdate": 1700384187407,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "84e8QR2cRO",
                "forum": "pGL4P2kg6V",
                "replyto": "Zp5pW6l2Y9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4845/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4845/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response (2/2)"
                    },
                    "comment": {
                        "value": "**#4 Recent baselines in experiments**\n1. We have compared the two baselines within two years: Adam-NSCL[3] and SSRE [1]. As you requested, we compare our methods with ICICLE, and the experiment results are in Table R5. Our method outperforms ICICLE by 2.02% in $\\bar{A}_T$ and 31.68% in $\\bar{F}_T$. \n2. ICICLE focuses on part-based prototype concepts. Our work focuses on text-based concepts instead, which allows more general interpretability. Meanwhile, it is only suitable for particular model architectures whereas our methods are suitable for any CNN-based models.\n\nTable R5: Experiment results on CUB200 [5] in a 5-task scenario. The backbones are pretrained on Place365 dataset.\n|               | $\\bar{A}_T$ | $\\bar{F}_T$ |\n| ------------- | ------------ | ------------ |\n| Finetune      | 10.16        | 60.73        |\n| EWC           | 11.02        | 62.43        |\n| LwF           | 12.50        | 64.50        |\n| Adam-NSCL     | 13.27        | 40.73        |\n| ICICLE        | 11.40        | 53.66        |\n| CC-CBM (Ours) | **13.42**        | **21.98**        |\n\n**#5 Details for freeze concepts and subnetwork**\n1. In CC, classes in new tasks may reuse old concepts learned in previous classes. CC freezes the subnetwork from the concept units to the bottom, but does not freeze the associations between the classification layer and concept units. Therefore, the new classes can reuse old concepts. We added new experiments on CUB200 [5], which is a bird species classification dataset and has many concepts overlapped between classes. We measured the forward transfer metric [9] (FWT) to understand whether old concepts help new classes. The experiment results are in Table R6, which shows our method can reuse concepts efficiently and improve a model\u2019s performance.\n2. We added the subnetwork size of different datasets in appendix A.14 (p.29). Overall, a more complicated dataset has a bigger subnetwork.\n\nTable R6: FWT on comparing our methods with baselines.\n| CUB200, 5 tasks            | FWT     |\n| -------------------------- | ------- |\n| Without memory buffer:     |\n| Finetune                   | 0.76    |\n| EWC                        | 3.83    |\n| SI                         | 13.50   |\n| LwF                        | 10.20   |\n| CC-freeze-all (Ours)       | 28.79   |\n| CC-freeze-part  (Ours)     | **30.07**   |\n| With memory buffer:        |\n| GEM                        | 1.35    |\n| CC-freeze-all-GEM  (Ours)  | **26.76**   |\n| CC-freeze-part-GEM  (Ours) | 26.68   |\n| MIR                        | \\-12.44 |\n| CC-freeze-all-MIR  (Ours)  | 6.89    |\n| CC-freeze-part-MIR  (Ours) | 8.22    |\n\n**#6 Generalization to transformers**\n\nWe would like to point out that whether concepts can be disentangled in transformers is still under research [6, 7, 8]. We believe this is an interesting future work after transformers\u2019 interpretability has made solid progress.\n\n**#7 Presentation**\n1. Thank you for pointing out the unclear part of our paper. We have revised them in the revision. \n2. In Section 3, the subnetwork of a concept unit is frozen. In Section 4, we freeze the concept unit itself. Concept unit is a neuron as we stated in Section 1 (p.1).\n3. Due to the page limit, we are not able to put Figures 6 or 7 in the main paper. The discussion for CC\u2019s interpretability is in Section 3 (p.5).\n\n**#8 Summary**\nTo summarize, we have:\n1. Discussed in **#1** on our methods\u2019 scalability, and add a new experiment on a large-scale dataset.\n2. Discussed in **#2** on the role of concepts and interpretability.\n3. Discussed in **#3** on questions related to vision-language aligned models.\n4. Discussed in **#4** on the latest baselines we have compared, and add a comparison with ICICLE.\n5. Described in **#5** on the mechanism and details of subnetwork.\n6. Described in **#6** on the generalization to transformers.\n7. Discussed in **#7** on the presentation of our paper.\n\nWe believe we have addressed all your concerns. Please let us know if you still have any reservations and we would be happy to address them. \n\nReference: \n\n[1] Self-Sustaining Representation Expansion for Non-Exemplar Class-Incremental Learning. CVPR 2022\n\n[2] Lifelong Learning with Dynamically Expandable Networks. ICLR 2018\n\n[3] Training networks in null space of feature covariance for continual learning. CVPR 2021\n\n[4] Label-free Concept Bottleneck Models. ICLR 2023\n\n[5] The caltech-ucsd birds-200-2011 dataset. 2011.\n\n[6] An interpretability illusion for bert. arXiv:2104.07143 2021\n\n[7] \"Toy Models of Superposition\", Transformer Circuits Thread, 2022.\n\n[8] \"Towards Monosemanticity: Decomposing Language Models With Dictionary Learning\", Transformer Circuits Thread, 2023.\n\n[9] Gradient episodic memory for continual learning. NeurIPS 2017"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4845/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700383430238,
                "cdate": 1700383430238,
                "tmdate": 1700384219468,
                "mdate": 1700384219468,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yKgESiPn0v",
                "forum": "pGL4P2kg6V",
                "replyto": "Zp5pW6l2Y9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4845/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4845/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Polite reminder on requesting for final rebuttal feedback from Reviewer zdWA"
                    },
                    "comment": {
                        "value": "Dear Reviewer zdWA,\n\nAs the rebuttal period is ending within 7 hrs, we would like to ask for the feedback to our rebuttal response.\n\nAs we did not hear from you, we are not sure if you have additional questions or comments. We believe that we have addressed all your concerns and with additional new experiments and discussions (see **General response** and Appendix A. 13, A.14). Please let us know if you still have any reservations and we would be happy to address them.\n\nThank you!"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4845/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700716388645,
                "cdate": 1700716388645,
                "tmdate": 1700716415259,
                "mdate": 1700716415259,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lmTsxiruwi",
            "forum": "pGL4P2kg6V",
            "replyto": "pGL4P2kg6V",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4845/Reviewer_Hm1u"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4845/Reviewer_Hm1u"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a continual learning pipeline with quite a few modules (such as dissection-based continual training and concept bottleneck). The training also contains multiple steps. The final empirical results showcase its benefits compared to buffer-free continual learning methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is clearlly written and generally easy to follow.\n\n- The idea of introducing concept bottlenecks to continual learning is interesting and worth exploring.\n\n- The experimental results look good on the benchmark datasets."
                },
                "weaknesses": {
                    "value": "- I find the proposed method quite complex and ad-hoc in general. The dissection-based continual training is interesting, but it is eseentially to incorporate the dissection into [1].\n\n- The introduction of label-free concept bottleneck to the proposed framework makes it even more complex and also difficult to find which part actually contributes to the performance gain. Therefore, an ablation study has to be performed. What if we combine label-free concept bottleneck to DER directly. How does it perform? The paper needs to study each added module carefully and show its advantages.\n\n- The motivation to design such a complex system is weak. The usage of label-free concept bottleneck will introduce additional information from GPT-3, which is questionable. One can easily achieve good performance if you use store the text label and perform zero-shot classification on continual learning dataset (which can easilly outperform your results). Even if you use label-free concept bottlenecks, the addtional text information is still leaked to your model. I am not sure whether this is still a fair comparison.\n\n\n\n[1] Der: Dynamically expandable representation for class incremental learning, CVPR 2021"
                },
                "questions": {
                    "value": "See the weakness section"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4845/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699248705053,
            "cdate": 1699248705053,
            "tmdate": 1699636467881,
            "mdate": 1699636467881,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6Rz6a0S9uP",
                "forum": "pGL4P2kg6V",
                "replyto": "lmTsxiruwi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4845/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4845/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your valuable feedback, please see our reply below.\n\n**#1 Difference between our methods and DER (CVPR 2021)**\n\nWe would like to highlight our contributions when comparing with DER (CVPR 2021):\n1. We agree that architecture-based methods like DER are explored in other works, as we discussed in Section 2.1 (p.3). However, we are the first one to introduce neuron-level interpretability to continual learning, which is non-trivial. We are happy to hear your comment that our work in this direction is interesting and worth-exploring. \n2. As we mentioned in the Introduction session (p.2), interpretability allows us to improve the \"method design\" in an interpretable manner. This is unexplored in DER and other architectured-based methods. \n3. DER requires a different feature extractor for each task to retain learned knowledge. On the other hand, our methods use the same feature extractor to learn all tasks, which is more efficient.\n\n**#2 Ablation Study for CC-CBM**\n\nWe have done ablation studies for CC-CBM in the main results and appendix:\n1. When introducing label-free CBM (abbreviated as LF-CBM), we compare the Finetune strategy vs CC strategy in Step 2 (p.6), the experiment results are in the main results (Section 5.1 from p.7- p.9) and Appendix (p.23 - p.26).\n2. Ablation studies for freezing $W_C$ and regularizing $W_F$ are in Appendix A.5 (p.17).\n\nThese ablation studies have shown the contribution of each part in CC-CBM. Meanwhile, we believe that combining LF-CBM with DER (CVPR 2021) may not be a fair ablation study for our CC-CBM for the following reasons:\n1. DER requires a new backbone for each task, while CC-CBM only needs one.\n2. DER official code is not complete, since the code to learn a new backbone is missing. The issues have been complained about in their official codebase (https://github.com/Rhyssiyan/DER-ClassIL.pytorch/issues/18). \n3. DER is an exemplar-based method, while CC-CBM is an exemplar-free method. We have compared with other exemplar-free methods (EWC, SI, LwF, Adam-NSCL, SSRE) and exemplar-based methods (GEM, MIR, DER-NeurIPS-2020 [11]). Please see Section 5.1 in p.7 for references. Note that the DER-NeurIPS-2020 is a different method published in [11] but happens to have the same name \u201cDER\u201d as the method you suggested (CVPR 2021) and hence we denote [11] as DER-NeurIPS-2020. Overall, our methods outperform other exemplar-free baselines and improve exemplar-based methods when combining them.\n4. We have compared with baseline methods that are more recent than DER, e.g. SSRE (CVPR 2022) [10], and show that our results outperforms it in $L_T$ by up to 57.1% in Table 18 (p. 27, appendix A.12). Unfortunately, SSRE did not compare with DER either.  \n \nWe will add the above discussion into the revision. Thank you for your suggestions! \n\n**#3 Additional Information from GPT-3**\n\nWe would like to highlight several points for the potential issue:\n1. Using GPT-3 to general concept sets is firstly proposed from an earlier work LF-CBM [7], which has many concurrent and follow-up works that use language models to create concept bottleneck layers, e.g. see [8, 9]. In fact, introducing additional information is widely exploited when designing CBM [1, 2, 3], and is reasonable task-relevant information that we can leverage. \n2. We agree that the potential leaking of CBM is an interesting problem, but we would like to point out that this is not the focus of our work as this is an universal problem in CBM. We will leave it as a future work and add discussion in the revised draft.\n3. Using additional information to enhance a model's performance is common in other fields as well [4, 5].\n4. Our method is still under the setting of continual learning because during the training phase, the information for each class only appears in one task.\n\nThus, we believe that our methods are fair in the continual learning setting."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4845/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700114750464,
                "cdate": 1700114750464,
                "tmdate": 1700121684730,
                "mdate": 1700121684730,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vPp16Pn9BE",
                "forum": "pGL4P2kg6V",
                "replyto": "lmTsxiruwi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4845/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4845/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response (2/2)"
                    },
                    "comment": {
                        "value": "**#4 Further information for additional experiments**\n\nFollowing your suggestion, we performed the zero-shot classification experiment. We found that zero-shot classification on continual learning will lead to bad performance. In the 5-task scenario, we pretrained the backbone only on the first task, and did zero-shot classification for the following tasks. The experiment results of zero-shot classification are much worse than our proposed CC-CBM as below Table R1 shows.\n\nTable R1: Comparison between zero-shot classification and our methods, in the 5-task scenario of CIFAR-100. The experiment results shows that our methods (CC-CBM) outperforms the zero-shot verison in average accuracy $A_T$ and learning accuracy $L_T$ (ability to learn new tasks). \n|                  | $A_T$         | $L_T$         |\n| ---------------- | ------------- | ------------- |\n| Zero-shot CC-CBM | 4.34          | 15.36         |\n| CC-CBM           | **16.37** | **62.33** |\n\n**#5 Motivation**\n\nWe have briefly discussed our motivation in the Introduction section (p.1). Overall, our goal is to design continual learning algorithms which are interpretable. Previous work [6] points out the benefits of human-interpretable concepts for transfer learning.\n  \nMeanwhile, we measured the evolution of human-understandable concepts in the models when learning sequences of tasks. The experiment results in Table 3 in main draft (p.9) and Table 9 and Table 10 in Appendix p.19) show that existing continual learning algorithms don\u2019t really preserve human-understand concepts. To prevent this behavior, we think it is reasonable to design an algorithm that aims to mitigate catastrophic forgetting of concepts, which is the main motivation for us to design the proposed Concept Controller. We will add this discussion to the introduction in revision to make the motivation more clear.\n\n**#6 Summary**\n\nTo summarize, we have:\n* Clarified in **#1** on our contributions when comparing with DER (CVPR 2021).\n* Described  in **#2** on the ablation studies we have done, and why comparing with DER is not a fair ablation study.\n* Discussed in **#3** about the additional information from GPT-3.\n* Provided in **#4** for the additional experiments as requested.\n* Clarified in **#5** on our motivation. \n\nWe believe we have addressed all your concerns. Please let us know if you still have any reservations and we would be happy to address them. \n\n\nReference: \n\n[1] Post-hoc Concept Bottleneck Models. ICLR 2023\n\n[2] Interpretable Basis Decomposition for Visual Explanation. ECCV 2018\n\n[3] Interpretability beyond classification output: Semantic bottleneck networks. arXiv:1907.10882, 2019\n\n[4] Compare to The Knowledge: Graph Neural Fake News Detection with External Knowledge. ACL 2021\n\n[5] Entity-Aware Dual Co-Attention Network for Fake News Detection. EACL 2023 Findings \n\n[6] Network dissection: Quantifying interpretability of deep visual representations. CVPR 2017.\n\n[7] Label-free Concept Bottleneck Models. ICLR 2023\n\n[8] Language in a Bottle: Language Model Guided Concept Bottlenecks for Interpretable Image Classification. CVPR 2023\n\n[9] Learning Concise and Descriptive Attributes for Visual Recognition. ICCV 2023\n\n[10] Self-Sustaining Representation Expansion for Non-Exemplar Class-Incremental Learning. CVPR 2022\n\n[11]  Dark experience for general continual learning: a strong, simple baseline. NeurIPS 2020"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4845/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700114837869,
                "cdate": 1700114837869,
                "tmdate": 1700121705118,
                "mdate": 1700121705118,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SFl0ZPtV65",
                "forum": "pGL4P2kg6V",
                "replyto": "lmTsxiruwi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4845/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4845/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer Hm1u, \n\nWe believe that we have addressed all your concerns and with additional new experiments (see General response and Appendix A. 13, A.14). Please let us know if you still have any reservations and we would be happy to address them. Thank you!"
                    },
                    "title": {
                        "value": "Request Rebuttal feedback from Reviewer Hm1u"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4845/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700469353539,
                "cdate": 1700469353539,
                "tmdate": 1700541097076,
                "mdate": 1700541097076,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zFlpJi7lem",
                "forum": "pGL4P2kg6V",
                "replyto": "lmTsxiruwi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4845/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4845/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Polite reminder on requesting for final rebuttal feedback from Reviewer Hm1u"
                    },
                    "comment": {
                        "value": "Dear Reviewer Hm1u,\n\nAs the rebuttal period is ending within 7 hrs, we would like to ask for the feedback to our rebuttal response.\n\nAs we did not hear from you, we are not sure if you have additional questions or comments. We believe that we have addressed all your concerns and with additional new experiments and discussions (see **General response** and Appendix A. 13, A.14). Please let us know if you still have any reservations and we would be happy to address them.\n\nThank you!"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4845/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700716350179,
                "cdate": 1700716350179,
                "tmdate": 1700716350179,
                "mdate": 1700716350179,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]