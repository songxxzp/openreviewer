[
    {
        "title": "Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis"
    },
    {
        "review": {
            "id": "QvdKsoeHN0",
            "forum": "aA33A70IO6",
            "replyto": "aA33A70IO6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission156/Reviewer_xbbn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission156/Reviewer_xbbn"
            ],
            "content": {
                "summary": {
                    "value": "For the problem of achieving better alignment by reducing harmness, the paper proposes to leverage bad examples that are harmful, thus going beyond only leverage good examples that contain no harm. Technically, the authors propose to encourage the model to produce harmful content and perform analysis on it. The whole is added as part of training data to perform supervised fine tuning upon. \n\nThe authors test their approach empirically on two datasets (PKU-SafeRLHF and instruction attacks), in both cases, the propose method not only obtains better results than SFT baselines but also shown to outperform some current popular approaches including COT, Critique-Revise etc. The authors have performed careful ablation studies showing the contribution of different prompts as well as pitfalls (guided or not guided, etc.)"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The paper correctly points out that most current approaches either SFT based or RLHF based don't fully leverage bad examples. The paper has introduced a rather simple approach to tackle this problem: let the model generate the harmful data but perform the alignment by training on harmful data and the analysis. Through two benchmark datasets, the authors have demonstrated very good results: not only it outperforms the plain SFT baseline, it also outperforms some popular alignment approaches including Critique-Revise and CoH. As a reference, SFT achieves 63% harmless and CoH is about 64.7 while the best approach in the paper achieves 74.1 harmless rate.\n\nA big advantage in the proposed approach is its simplicity, the approach consists simply of generating \"bad\" examples and append analysis then train on SFT. Such simplicity probably hints for bigger impact and larger application. \n\nThe paper's analysis is well performed including interesting ablation studies."
                },
                "weaknesses": {
                    "value": "I think the paper's presentation can still be largely improved:\n\nMajor:\nSection 4 is hard to read but it shouldn't be. The first subsection is the background on SFT, the next three sections are about the specific novel training approach proposed in the paper. The fifth subsection talks about inference. Finally there is a discussion section about the mathematical intuition on the exact mechanisms. I strongly recommend the authors to give this section a clear structure to inform readers the background, exact contribution and discussions.\n \nMinor:\nI suggest avoiding using \"it still remains oblivious\" in introduction, we are not sure what \"oblivous\" exactly means in this context.\nSection 4, \"As demonstrated in Table 4\", table 4 is particularly hard to read and it hasn't been introduced before. \nI think the mathematical discussion can be expanded and made clearer. \n5.1 35.2% improvement, please add \"relative\" to clarify.\ninduction success rate: \"augments mistake induction induction\" seems a typo"
                },
                "questions": {
                    "value": "Do authors perform experiments to ground the mathematical explanations? More precisely, after applying the proposed alignment training method, have the authors observed that harmless tag prediction is improved?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission156/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698010549710,
            "cdate": 1698010549710,
            "tmdate": 1699635941156,
            "mdate": 1699635941156,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XSQqqZt92a",
                "forum": "aA33A70IO6",
                "replyto": "QvdKsoeHN0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission156/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission156/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xbbn"
                    },
                    "comment": {
                        "value": "Thank you for your acknowledgement and valuable suggestions. Here we address your questions one by one.\n\n**W1: Clearer structure of Section 4.**\nThanks for your advice. Following your suggestions, we have updated the structure of Section 4 by separating the background notations, the proposed method and the discussions. See details in the revision.\n\n**W2: Inappropriate wording and typos.**\n\nThank you for pointing that out. We have correct all the mentioned inappropriate wording and typos in the revision.\n\n**Q1: Verifying Equation (2) via harmless tag prediction.**\n\nFollowing your suggestions, we conduct an additional experiment evaluating the LLM's ability to discern the harmfulness of responses. Specifically, we utilize the test set of PKU-SafeRLHF dataset, each instruction paired with a response and the corresponding binary harmful or harmless tag, and then instruct LLMs to distinguish whether the provided response is harmful or not. The accuracy is reported in Table R5. The LLM's accuracy in identifying the harmfulness of responses improves from 54.5% for vanilla Aplaca-7B to 72.6% after applying our method, revealing the significant improvement of discrimination ability.\n\n\u25be Table R5. **Comparison between discrimination ability with respect to the binary classification accuracy on the test set of the PKU-SafeRLHF dataset.**\n\n|              | Vanilla Alpaca | SFT         | Ours             |\n| :------------: | :--------------: | :-----------: | :----------------: |\n| Accuracy (%) | 54.5           | 54.9 (+0.4) | **72.6 (+18.1)** |"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission156/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700056079084,
                "cdate": 1700056079084,
                "tmdate": 1700056079084,
                "mdate": 1700056079084,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dMv6gBZPSR",
                "forum": "aA33A70IO6",
                "replyto": "XSQqqZt92a",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission156/Reviewer_xbbn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission156/Reviewer_xbbn"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the authors' detailed feedback. It clarifies my questions and address some of my concerns that I think the paper now stands at a better shape which justifies well the score I give to this paper. I thus won't change it. \n\nTo defend my score, I would say that the main strength is the method's efficiency and simplicity. While I agree with the other reviewers that the theory part might not be satisfactory at this stage, I would argue that the content is not self conflicting and we should leave the theoretical justifications for future work for the community."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission156/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700075139210,
                "cdate": 1700075139210,
                "tmdate": 1700075139210,
                "mdate": 1700075139210,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VfJamA3mdQ",
            "forum": "aA33A70IO6",
            "replyto": "aA33A70IO6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission156/Reviewer_5o83"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission156/Reviewer_5o83"
            ],
            "content": {
                "summary": {
                    "value": "The paper describes a method of model self-critique for purposes of SFT to improve model helpfulness and harmlessness. New samples providing harmful responses along with explanations of the harmfulness, and the model is then fine-tuned to generate these explanations."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The technique presented is a novel addition to the pantheon of LLM fine-tuning techniques such as RAFT and CoH. The idea of training on intentionally bad examples during SFT is an interesting one, and can help make models more helpful and harmless as demonstrated in Tables 1 & 2."
                },
                "weaknesses": {
                    "value": "While the core idea of the paper is sound, the explanations were confusing and scattered, and the experimental results were not convincing.\n\nThe lengthy discussion of generation vs discrimination is largely tangential to the main thrust of the paper. The relative ease of discrimination compared to generation is already a core principle underlying the use of auxiliary reward models in RLHF. Moreover Figure 2 which discusses this work has a very low-information caption, that is largely redundant with the figure itself: \"Each histogram is composed of three segments with distinct colors, labeled with three score numbers...\"\n\nThe results in Tables 1 & 2 purporting to show improved performance are lacking some important rows to put the results into context:\n* In Table 1, both rows showing improvement over baselines use Alpaca as the Mistake Source. For a fair comparison, the baselines should also make use of mistakes generated by Alpaca.\n* In Table 2, the row showing improvement over baselines uses ChatGLM as the Mistake Source. For a fair comparison, the baselines should also make use of mistakes generated by ChatGLM.\n\nThere are a number of typos throughout the paper:\n* Bottom of page 2: \"into precious instruction tuning corpus\"\n* Bottom of page 5: \"Dissusion\"\n* Bottom of page 5: \"Bayesian\u2019s Theorem\""
                },
                "questions": {
                    "value": "Due to the apples-to-oranges comparisons in the two tables, it is hard to tell how efficacious this method is compared to other existing alignment methods, and therefore difficult to judge the usefulness of the method. Listing out all relevant experiments (see Weaknesses feedback) would help to clarify this, as would cleaning up some of the superfluous exposition throughout the paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission156/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698783577178,
            "cdate": 1698783577178,
            "tmdate": 1699635941058,
            "mdate": 1699635941058,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "40tEifh43m",
                "forum": "aA33A70IO6",
                "replyto": "VfJamA3mdQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission156/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission156/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 5o83"
                    },
                    "comment": {
                        "value": "Thank you for your valuable and constructive reviews. Here we address your questions one by one.\n\n**W1: The reason for discussing the relative ease of discrimination compared to generation, which is already a core principle of RLHF.**\n\nThank you for your valuable feedback. The discussion on generation versus discrimination actually sets the foundation for our methodology, which utilizes the inherent discriminative power of LLMs. Our method, leveraging the LLM's self-critique for improved generation via mistake analysis, stands out from RLHF's reliance on external human annotations and reward models. This difference emphasizes the importance of easier discrimination over generation by the model itself. \n\nAccording to your comments, we realize that the success of RLHF may also benefit from increasing the discrimination ability of models with external reward models. On the other hand, our method boosts this ability through mistake analysis, which provides more informative feedback in natural language compared to the relative quality ranking in RLHF (see Figure 1). This explains why our model achieves superior performance over RLHF even without external reward models (refer to Table 1).\n\n**W2: Redundant description in the caption of Figure 2.**\n\nThank you for highlighting the need for a more concise caption for Figure 2. We have revised the caption as below and updated the paper accordingly.\n\n```\n(a) Comparison between generation and discrimination abilities for Alpaca, GPT-3 and GPT-3.5. (b) Comparison between guided and unguided analyses. Refer to Section 3 for more details.\n```\n\n**W3 and Q1: Experimental results with the same mistake source for baseline methods.**\n\nAs demonstrated in the 4th-6th rows in Table 1 and the 3rd-5th rows in Table 2, we have conducted experiments for all baseline methods with the **original mistakes** provided by the datasets for a fair comparison, revealing the superiority of our mistake analysis. Following your suggestions, we further provide the experimental results of all baselines with **induced mistakes** below and also in the revision, which is consistent with our previous observation.\n\n\u25be Table R3. **Comparative results of LLM alignment across various methods with induced mistakes.**\n\n| Method          | Mistake Source | Analysis Source | Helpful Score | Harmless Score | Harmless Rate (%) | Harmless Helpful |\n| :---------------: | :--------------: | :---------------: | :-------------: | :--------------: | :-----------------: | :----------------: |\n| Critique-Revise | Alpaca         | -               | 6.11          | 6.17           | 61.3              | 4.56             |\n| CoH             | Alpaca         | -               | 6.28          | 6.87           | 65.7              | 5.29             |\n| Ours            | Alpaca         | Alpaca          | **6.38**      | **7.41**       | **72.4**          | **5.39**         |\n\n\u25be Table R4. **Comparative results of defense against attacks across various methods with induced mistakes.**\n\n| Method          | Mistake Source | Analysis Source | Goal Hijacking Score | Goal Hijacking Rate (%) |\n| :---------------: | :--------------: | :---------------: | :--------------------: | :-----------------------: |\n| Critique-Revise | ChatGLM        | -               | 7.42                 | 76.5                    |\n| CoH             | ChatGLM        | -               | 7.86                 | 79.4                    |\n| Ours            | ChatGLM        | ChatGLM         | **8.14**             | **85.3**                |"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission156/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700056011950,
                "cdate": 1700056011950,
                "tmdate": 1700056572310,
                "mdate": 1700056572310,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SrZLOiZsQe",
                "forum": "aA33A70IO6",
                "replyto": "40tEifh43m",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission156/Reviewer_5o83"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission156/Reviewer_5o83"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the updates. Unfortunately the revised paper is still rife with grammatical errors and awkward phrasings that make it extremely difficult to read, due to the presence of a linguistic stumbling-block every few sentences. From just the first two pages:\n\n* \"transformed into instruction tuning corpus\"\n* \"disapproving of inadequate responses\"\n* \"avoiding them exposed to bad cases\"\n* \"fully usage\"\n* \"to toxic corpus\"\n* \"harmful data pattern\"\n* \"make LLMs exposed to and actively analyse\"\n* \"analysis performs as a\"\n* \"only a few number\"\n* \"precious instruction tuning corpus\"\n* \"with significant efficiency \"\n\nI do not think it is good use of reviewers' time to annotate your paper in such detail, and would strongly advise revising the paper for clarity and resubmitting."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission156/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700069203505,
                "cdate": 1700069203505,
                "tmdate": 1700069203505,
                "mdate": 1700069203505,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KcZwX6lvRb",
                "forum": "aA33A70IO6",
                "replyto": "VfJamA3mdQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission156/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission156/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 5o83"
                    },
                    "comment": {
                        "value": "Thank you for your thorough review of the manuscript. We have carefully checked the entire paper and made the following changes:\n\n1. Corrected grammatical errors and awkward phrasings;\n\n2.  Reorganized the paper, particularly the introduction and preliminary sections, to enhance the overall coherence.\n\nThe revised version has been updated. We hope that it can address your concerns adequately."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission156/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700190469057,
                "cdate": 1700190469057,
                "tmdate": 1700190680992,
                "mdate": 1700190680992,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wf6hBr6zA2",
                "forum": "aA33A70IO6",
                "replyto": "VfJamA3mdQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission156/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission156/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 5o83"
                    },
                    "comment": {
                        "value": "Dear Reviewer 5o83,\n\nWe are deeply appreciative of the time and expertise you have devoted to reviewing our manuscript. In acknowledgment of your insightful feedback, we have conducted thorough revisions to our work, which predominantly include:\n\n1. Justifying the relative ease of discrimination versus generation in LLMs as a motivation for our method;\n\n2. Conducting additional experiments that cover various sources of mistakes in baseline methods.\n\nFurthermore, we have meticulously reviewed and refined the manuscript to correct grammatical errors and improve clarity. We hope these efforts adequately address your initial concerns. With the deadline of discussion period approaching, we keenly await your further feedback and are eager to continue this constructive exchange.\n\nBest regards,\n\nAuthors of Paper 156"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission156/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700382423731,
                "cdate": 1700382423731,
                "tmdate": 1700382423731,
                "mdate": 1700382423731,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xT71zHeFKU",
            "forum": "aA33A70IO6",
            "replyto": "aA33A70IO6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission156/Reviewer_FapY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission156/Reviewer_FapY"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel method for aligning large language models (LLMs) with human values by purposefully exposing them to flawed responses and having the models analyze their own mistakes. The authors demonstrate that discrimination (identifying harmful responses) is easier for LLMs compared to directly generating harmless responses. Experiments show the method improves alignment over supervised training and reinforcement learning, especially against new attacks."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Novel idea of using mistake analysis to align LLMs, transforming mistakes into valuable tuning data.\n2. Achieves strong performance, improving harmlessness substantially while maintaining helpfulness.\n3. Efficient defense against new attacks by learning from a few representative mistakes."
                },
                "weaknesses": {
                    "value": "1. May not generalize well to diverse contexts beyond the training distributions.\n2. Hard to control mistake induction to cover all failure modes.\n3. Still possible for aligned models to make mistakes during inference. \n4. Limited analysis on how mistake analysis exactly improves alignment."
                },
                "questions": {
                    "value": "1. How resilient is the method to adversarial instructions aimed at bypassing the alignment?\n2. Is the quality or diversity of mistake analysis critical for alignment improvements? \n3. Could you conduct an ablation study focused on mistake analysis components?\n4. Is there a theoretical understanding of why mistake analysis provides strong alignment signal?\n5. How can the safety and ethics of induced mistakes be ensured during training?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission156/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698820643235,
            "cdate": 1698820643235,
            "tmdate": 1699635940965,
            "mdate": 1699635940965,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2gfOnWEg4p",
                "forum": "aA33A70IO6",
                "replyto": "xT71zHeFKU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission156/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission156/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer FapY (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your acknowledgement and valuable suggestions. Here we address your questions one by one.\n\n**W1-3: Justification about the generalization ability of LLMs aligned with mistake analysis.**\n\n**(1) Generalization beyond training distributions.** We acknowledge that no alignment method, including ours, can fully address all out-of-distribution scenarios. Nonetheless, our model exhibits enhanced generalization capabilities by learning from a wide array of mistakes and their analyses, rather than solely relying on human-labeled instruction-response pairs. As depicted in Figure 5, our model effectively rejects unsafe user instructions in \"Goal Hijacking\" scenarios for topics not in the training data, a feat not achieved by methods like SFT. This demonstrates our model\u2019s advanced understanding of such complicated mistakes and its improved generalization ability.\n\n**(2) Addressing diverse failure modes and mistakes of alignment models.** Admittedly, even meticulously aligned LLMs face challenges in completely avoiding unsafe responses in novel or unanticipated situations. This necessitates an adaptable \"debugging\" methodology for addressing numerous corner cases. Section 5.2 details our approach\u2019s efficacy in defending against new-coming instructional attacks. After applying our method, the safety-aligned ChatGLM model demonstrates significant post-alignment improvement, with its Harmless Rate in resisting \"Goal Hijacking\" attacks increasing from 68.4% to 85.3%. This positions it as the second rank only under the OpenAI's GPT-3.5, as per the SAFETYPROMPTS benchmark [1, 2], while maintaining its helpfulness performance.\n\n\n\n**W4 & Q4: Theoretical analysis on how mistake analysis improves alignment.**\n\nIn Section 4, Equation (2), we explore the role of mistake analysis in improving LLM alignment from a Bayesian perspective, considering that LLMs operate as conditional generators. Mistake analysis involves examining the model's harmful responses to identify patterns leading to such errors. To train the model in this way, we construct triplets of the instruction $X$, the response $Y$, and the mistake analysis $T$ indicating whether the response is harmful or harmless. Such data refines the model's understanding of the conditional probabilities $p(T|Y, X)$, *i.e.*, the likelihood of a response being harmful or harmless given a certain instruction. Subsequently, we enhance its ability to generate appropriate and harmless responses $p(Y|X, T)$ through guided inference, where the model is required to produce harmless responses during inference (see Figure 1). By focusing on optimizing conditional probabilities through the analysis of mistakes, we enhance the model's ability to generate safe, coherent, and contextually appropriate responses. This is why mistake analysis benefits model alignment. See Section 4.2 and Appendix D for more details.\n\n\n**Q1: Resilience against adversarial instruction attacks.**\n\nRecent studies [3, 4] demonstrate that even well-aligned models, such as OpenAI's GPT-4 and Anthropic's Claude2, are vulnerable to adversarial instruction attacks, presenting a challenge distinct from aligning with human values. Our research primarily focuses on aligning LLM with human values. We have observed the effectiveness of our method in defending against delicately designed instruction attacks, such as \"Goal Hijacking\" with superior generalization ability (see Section 5.2). This suggests the potential effectiveness of our method in enhancing resilience against adversarial attacks. Further investigation will be considered in our future work.\n\n**Q2-3: The quality or diversity of mistake analysis for alignment improvements and more ablation study.**\n\n**(1) Quality of mistake analysis.** Yes, the quality of mistake analysis significantly impacts alignment performance. As Table 1 shows, using GPT-3.5-turbo for mistake analysis results in better alignment than using Alpaca-7B itself (see rows 7 and 8). Additionally, guided analysis outperforms unguided analysis, as shown in Table 3 (rows 3 and 4). \n\n**(2) More ablation study on mistake analysis.** In Section 5.3, we present a comprehensive ablation study on the essential components of mistake analysis, including (i) quality of mistake analysis (ii) quantity of mistake analysis and (iii) the source of responses for constructing analysis data. We find that high-quality analysis, whether guided or from advanced models like GPT, leads to better alignment. On the other hand, increasing the volume of analysis data does not always improve results. We observe a decrease in performance when multiple analyses are applied to the same instructions, possibly due to conflicting interpretations in the analyses of the same instruction. Furthermore, we observe that the alignment effectiveness is maximized when the bad cases are generated by the model itself. See Section 5.3 for detailed experimental results."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission156/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700055892128,
                "cdate": 1700055892128,
                "tmdate": 1700055892128,
                "mdate": 1700055892128,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HkvXEsvFXG",
                "forum": "aA33A70IO6",
                "replyto": "xT71zHeFKU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission156/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission156/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer FapY"
                    },
                    "comment": {
                        "value": "Dear Reviewer FapY,\n\nWe are grateful for the time and expertise you have contributed to the review of our manuscript. Considering your valuable suggestions, we have implemented extensive revisions to our paper. These revisions primarily include:\n\n1. A detailed justification of the generalization capabilities of LLMs aligned with mistake analysis;\n\n2. An in-depth theoretical discussion of how mistake analysis contributes to better alignment;\n\n3. A discussion on the quality and diversity of mistake analysis in enhancing alignment, supplemented with additional ablation studies.\n\nWe trust these changes effectively address the issues you raised. As the deadline for the discussion period draws near, we eagerly anticipate your further insights and remain dedicated to an ongoing constructive dialogue.\n\nBest regards,\n\nAuthors of Paper 156"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission156/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700382169957,
                "cdate": 1700382169957,
                "tmdate": 1700382169957,
                "mdate": 1700382169957,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HEdoFzwlgZ",
            "forum": "aA33A70IO6",
            "replyto": "aA33A70IO6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission156/Reviewer_sdVt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission156/Reviewer_sdVt"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a supervised fine-tuning (SFT) method for alignment, which uses the mistake generated by LLMs themselves to help self-criticism. The authors first try to empirically demonstrate the relationship between generation and discrimination. Then, they present a method consisting of three stages including guided response generation, guided analysis generation, and unguided analysis fine-tuning. Experimental results show that the proposed method can outperform conventional alignment techniques for safety instruction following."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The idea which uses self-generated mistake analysis to help alignment is intuitive and interesting.\n\n2. Experimental results show the effectiveness of the proposed method for alignment in the safety scenario."
                },
                "weaknesses": {
                    "value": "1. The claim about generation and discrimination in Section 3.1 is confusing. The authors use GPT-4 scores (plus human verification) on generated responses / analysis to demonstrate that discrimination is easier than generation. But in my view, discrimination requires to output a definite classification result (such as safe or unsafe), whose performance can be easily assessed via objective metrics like accuracy. The analysis may help the model to conduct discrimination. But its generation quality is not equal to discrimination results, especially the quality is measured via GPT-4 in a \u201ccontinuous\u201d scale of 1-10.\n\n2. The derivation in Equation (2) of Section 4 is not rigorous. The authors get a conclusion that $p(T|Y,X)$ is proportional to $p(Y|X,T)$. This indicates that the gap between $\\log p(T|Y,X)$ and $\\log p(Y|X,T)$ is a scalar. Following this, if we train a safety discrimination model with the loss of $\\log p(T|Y,X)$, we can directly obtain a safe response generation model. This is counter-intuitive and needs more explanations. I\u2019m afraid that the authors miss some important assumptions in their derivation.\n\n3. In Section 5.1, the authors only use Alpaca as their base model in the experiment. Since Alpaca is a weak base model, improving its performance may be relatively easy. I wonder whether the proposed method can still work well on stronger base models such as Vicuna."
                },
                "questions": {
                    "value": "I have included my questions in the weaknesses part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission156/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699468358160,
            "cdate": 1699468358160,
            "tmdate": 1699635940862,
            "mdate": 1699635940862,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TRbsCDEpLx",
                "forum": "aA33A70IO6",
                "replyto": "HEdoFzwlgZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission156/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission156/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer sdVt (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your valuable and constructive reviews. Here we address your questions one by one.\n\n**W1: Justification for comparing LLM's \"discrimination\" ability with generation\" ability.**\n\n**(1) Definitions of generalization and discrimination.** Thank you for your valuable feedback. To clarify, in our paper, \"generation\" is defined as the LLM's ability to produce helpful and harmless responses to user instructions, while \"discrimination\" refers to its capacity to identify potential errors in responses, which requires not only a binary classification between \"harmful\" and \"harmless\" but also to encompass a more nuanced analysis of the response quality, as illustrated in Figure 7 of the Appendix. Therefore, the continuous score is considered as a better metric than the simple binary classification accuracy here. The preliminary aims to show the observation that LLMs are generally more adept at identifying potential mistakes than directly generating high-quality responses. This distinction is vital for the subsequent method design, where we use self-critique data to enhance the model's generation capabilities.\n\n**(2) Experiments of binary harmful/harmless classification.** However, we acknowledge your concern about regarding the potential oversight of the binary classification accuracy. Following your suggestions, we conduct an additional experiment evaluating the LLM's ability to discern the harmfulness of responses. Specifically, we utilize the test set of PKU-SafeRLHF dataset, each instruction paired with a response and the corresponding binary harmful or harmless tag, and then instruct LLMs to distinguish whether the provided response is harmful or not. The accuracy is reported in Table R1. The LLM's accuracy in identifying the harmfulness of responses improves from 54.5% for vanilla Alpaca-7B to 72.6% after applying our method, revealing the significant improvement of discrimination ability.\n\n\u25be Table R1. **Comparison between discrimination ability with respect to the binary classification accuracy on the test set of the PKU-SafeRLHF dataset.**\n\n|              | Vanilla Alpaca |           SFT | Ours |\n| :--------: | :-----------------: | :-------------: | :-----: |\n| Accuracy (%) | 54.5           | 54.9 (+0.4) | **72.6 (+18.1)** |\n\n**W2: More explanation about Equation (2) which implies a direct link between training a safety discrimination model and obtaining a safe response generation model.**\n\nThanks for your valuable comment. We have revised the deviation of Equation (2) in Section 4 and Appendix D. In summary, your observation about the counter-intuitive nature of directly obtaining a safe response generation model from a safety discrimination model is astute when only mistake analysis data is utilized in fine-tuning. However, this scenario is different from the discussion of Equation (2), where the mistake analysis data is incorporated together with the normal SFT data, as discussed in the **\"Step 2: Guided analysis generation\"** paragraph of Section 4.1.\n\nLet us start with the Bayes' Theorem $p(T|Y, X) = [p(Y|X, T)p(X|T)p(T)]/[p(Y|X)p(X)]$, which simplifies to $p(T|Y, X) \\propto p(Y|X, T)$ under the assumption that $p(Y|X)$ remains relatively stable during the fine-tuning process. To maintain the model's capability to produce a response given an instruction (*i.e.*, preserving $p(Y|X)$\uff09, we combine original SFT data, both helpful ($D_{\\text{helpful}}$) and harmless ($D_{\\text{harmless}}$), with mistake analysis data during fine-tuning, rather than solely relying on the mistake analysis data. Therefore, Equation (2) actually aims to explore how the additional mistake analysis data, along with the original SFT instruction-response pairs, can enhance alignment performance. \n\nMoreover, our ablation study in Section 5 demonstrates that an excess of mistake analysis data, particularly when it doubles the SFT data, actually leads to poorer performance (3rd vs. 5th rows), which indicates a nuanced relationship between different types of data in fine-tuning and the model's ability to balance between generation and discrimination tasks effectively. We hope this can clarify the assumption and methodology underlying our approach and address your concerns."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission156/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700055764964,
                "cdate": 1700055764964,
                "tmdate": 1700057922139,
                "mdate": 1700057922139,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kUcEIY5104",
                "forum": "aA33A70IO6",
                "replyto": "HEdoFzwlgZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission156/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission156/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer sdVt (2/2)"
                    },
                    "comment": {
                        "value": "**W3: Justification about whether the method effective on the weak Alpaca model in Section 5.1 is also applicable to stronger models like Vicuna.**\n\n**(1) Reason for choosing Alpaca.** We select Alpaca-7B as the baseline in Section 5.1 due to its absence of prior safety alignment. This characteristic renders it an ideal candidate for evaluating the effectiveness of alignment methods for an unaligned LLM from scratch. Our results, as elaborated in Table 1 and Figures 12-13, demonstrate that even unaligned LLMs like Alpaca-7B can effectively leverage  mistake analysis to enhance their safety instruction-following capabilities.\n\n**(2) Effectiveness on SOTA models.** Moreover, to address concerns about the method's effectiveness on stronger, safety-aligned models, we have expanded our experiments to include ChatGLM-6B, the highest-ranked open-source model on the C-Eval leaderboard [1]. As shown in Table 2 and Section 5.2, after incorporating the mistake analysis to defend against a novel type of instruction attack known as \"Goal Hijacking\", ChatGLM ranks the second on the SAFETYPROMPTS benchmark [2, 3]. Notably, it is only surpassed by OpenAI's GPT-3.5 in defending against the \"Goal Hijacking\" attack, while concurrently preserving its helpfulness performance.\n\n**(3) Additional experiments on Vicuna.** In line with your suggestion, we further conduct experiments on Vicuna-7B (v1.1) following the exact setting as in Section 5.1. As reported in Table R2, our mistake analysis achieves consistent improvement regradless of the baseline models.\n\n\u25be Table R2. **Comparative results of LLM alignment across various methods on Vicuna-7B.**\n\n| Method | Helpful Score | Harmless Score | Harmless Rate (%) | Harmless Helpful |\n| :------: | :-------: | :----: | :--------: | :----: |\n| Vicuna (vanilla) | 7.17          | 9.62           | 95.9              | 8.85 |\n| SFT | **7.33**      | 9.63           | 96.1              | 8.84 |\n| Ours | 7.30          | **9.74**          | **97.1**              | **8.90** |\n\n[1] C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models. https://cevalbenchmark.com/.\n\n[2] SAFETYPROMPTS Benchmark. http://115.182.62.166:18000/.\n\n[3] Sun H, Zhang Z, Deng J, et al. Safety assessment of Chinese large language models. arXiv preprint arXiv:2304.10436, 2023."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission156/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700055796595,
                "cdate": 1700055796595,
                "tmdate": 1700056412775,
                "mdate": 1700056412775,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VORF29dRrH",
                "forum": "aA33A70IO6",
                "replyto": "HEdoFzwlgZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission156/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission156/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer sdVt"
                    },
                    "comment": {
                        "value": "Dear Reviewer sdVt,\n\nWe sincerely appreciate the time and expertise you have invested in reviewing our manuscript. Following your insightful comments, we have undertaken substantial revisions, including:\n\n1. Providing a thorough justification for comparing the \"discrimination\" and \"generation\" abilities of LLMs;\n\n2. Expanding explanations regarding Equation (2) in the main text;\n\n2. Conducting additional experiments with more baselines such as Vicuna.\n\nWe believe these modifications comprehensively address your previous concerns. With the discussion period deadline nearing, we eagerly await your further feedback and remain fully engaged in ongoing dialogue.\n\nBest regards,\n\nAuthors of Paper 156"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission156/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700381944195,
                "cdate": 1700381944195,
                "tmdate": 1700381944195,
                "mdate": 1700381944195,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]