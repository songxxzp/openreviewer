[
    {
        "title": "Vision-by-Language for Training-Free Compositional Image Retrieval"
    },
    {
        "review": {
            "id": "1jh3IV7b9O",
            "forum": "EDPxCjXzSb",
            "replyto": "EDPxCjXzSb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5738/Reviewer_Tg9t"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5738/Reviewer_Tg9t"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a training-free framework for zero-shot image retrieval, by leveraging the pretrained vision-language model and large language model. VLM is used to obtain image caption for query image, and the caption together with the text modifier is fed into LLM to get the target caption; then the target caption is used to do cross-modal retrieval with the help of VLM in database images. This framework also allows scalability and human intervention to improve the retrieval performance due to its modularity and human-understandability."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed framework is training-free and achieves promising retrieval results.\n2. The retrieval accuracy could be improved by adopting better off-the-shelf VLM and LLM, due to the modularity of the framework.\n3. Since the query is mainly processed in language domain, the framework is human-interpretable and could also improve the retrieval accuracy by involving human intervention."
                },
                "weaknesses": {
                    "value": "1.\tThe method itself is very intuitive and not quite novel, which is more like an engineering extension of existing models like VLM and LLM. Additionally, the proposed method does not achieve better results on all evaluation datasets. In Table 1, CIReVL underperforms SEARLE on CIRR dataset in terms of CIRR, which shows the unstability. \n2.\tThis work only adopts datasets of everyday life and natural scenes (CIRR, CIRCO, GeneCIS) as evaluation. It is necessary to include datasets of various domains, such as fashion domain datasets like fashioniq and fashion200k to evaluate the generalization ability. Furthermore, for GeneCIS dataset, the comparison against other ZSCIR methods such as Pic2word and SEARLE should be included for complete comparison. \n3.\tThe proposed framework is not very time-efficient since for each query, there will be two auto-regressive processes: query image captioning and target caption generation, which are very time-consuming. Both processes involve large models like VLM and LLM, which may further lead to low efficiency for image search. Therefore, it is necessary to compare the retrieval efficiency (time) with the previous methods."
                },
                "questions": {
                    "value": "The same as weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5738/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698738490187,
            "cdate": 1698738490187,
            "tmdate": 1699636601270,
            "mdate": 1699636601270,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dENcI86ECm",
                "forum": "EDPxCjXzSb",
                "replyto": "1jh3IV7b9O",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5738/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5738/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Review"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their thoughtful comments. We appreciate that they find our results promising, and our method interpretable. Below, we address the concerns raised:\n\n---\n\n__1. Novelty and results on CIRR:__    \nIndeed, our method is an application of off-the-shelf VLMs and LLMs. However, their particular recombination and reuse to transfer it to the difficult task of compositional image retrieval is novel. This is particularly interesting, as unlike previous approaches that leverage off-the-shelf VLMs as well and have to train on large amounts of data, our approach requires no training at all, while the modular structure and operation in the language domain makes our approach interpretable and intervenable as well. \n\nThis strongly contrasts our approach from existing ones! In addition, as our scaling laws study shows, our setup can be easily scaled up to account for modular improvements over time, without having to retrain. As such, we strongly believe that our work is an important contribution to the field of compositional image retrieval.\nIn addition, we outperform existing, trained methods on CIRCO and Fashion-IQ significantly, while on CIRR (with noisy supervision, lack of multiple positive annotation, etc. as shown in previous works and noted in the paper), we are able to match previously obtained results\n\n---\n\n__2. Results on Fashion benchmarks:__    \n\nWe thank the reviewer for the suggestion. We have now updated our paper with the results on the Fashion-IQ benchmark. For the detailed results, we refer to our shared comment. In summary, we find that CIReVL significantly outperforms existing methods on all three subtasks, e.g. improving average Recall@10 by nearly 6pp (28.29% versus 22.89% for SEARLE) with a ViT B/32 backbone. In addition, the simple scalability allows us to hot-swap the retrieval model to achieve additional 4pp (increasing from 28.29% to 32.19%).\n\n---\n\n__3. Inference time:__    \nIndeed, while existing methods also utilize large VLMs, the use of distinct captioning and reasoning modules elicits additional runtime. On our hardware setup (V100 / A100 GPUs) when compared to trained methods, we find inference times to increase by ~1s. However, the modularity also allows our setup to directly benefit from efficiency changes within each module (as e.g. seen in the cost & inference time reductions for LLMs over the past year), allowing one system to remain operational for a long period of time. This stands in particular contrasts to other ZS-CIR methods that require costly training to incorporate any pipeline changes or shifts in expected datasets (e.g. Pic2Word trains the mapping network on 8 GPUs on the complete CC3M dataset)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5738/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699997726753,
                "cdate": 1699997726753,
                "tmdate": 1699997758685,
                "mdate": 1699997758685,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "33NRFt3Z4Q",
                "forum": "EDPxCjXzSb",
                "replyto": "dENcI86ECm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5738/Reviewer_Tg9t"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5738/Reviewer_Tg9t"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the authors' response. However, the novelty of this paper is not significant enough. Besides, the performance and inference time of the proposed method still limit its real application. Therefore, I will keep my rating."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5738/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700645233404,
                "cdate": 1700645233404,
                "tmdate": 1700645233404,
                "mdate": 1700645233404,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PKwWAiBdi5",
                "forum": "EDPxCjXzSb",
                "replyto": "1jh3IV7b9O",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5738/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5738/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their response, and would like to address two mentioned points:\n\n---\n\n> (...) the novelty of this paper is not significant enough\"\n\nIt would be great if the reviewer could provide additional context for this statement, as in our reply, we do highlight that our method and framework is novel. Even if the single components themselves are not novel on their own, their particular recombination and reuse is. Other approaches also leverage large-scale, off-the-shelf pretrained models and existing network architectures.\nUnlike these approaches however, our setup does not require large amounts of additional training data on top.\n\nIn addition to that, our method is first and foremost motivated by important practical constraint listed above, which significantly differentiates it from existing approaches, namely it being\n* interpretable,\n* intervenable,\n* easily extendable,\n* and providing better performance.\n\nIn addition to that, it also provides an important sanity check for future method that do incorporate additional training data.\n\n---\n\n> (...) the performance and inference time of the proposed method still limits its real application\n\nLarge-scale models find ubiquitous usage in real world applications, ranging from LLM services to recent large-scale multimodal models. In addition, our performance significantly outperforms existing methods on all benchmarks but one (and comparable scores in that case), while not requiring any training. Furthermore, the ability to freely interchange models is a property that especially allows for more realistic deployment based on cost-performance trade-offs."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5738/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668394600,
                "cdate": 1700668394600,
                "tmdate": 1700668394600,
                "mdate": 1700668394600,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "r3DhoH5bBg",
            "forum": "EDPxCjXzSb",
            "replyto": "EDPxCjXzSb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5738/Reviewer_JtMi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5738/Reviewer_JtMi"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces CIReVL, a training-free pipeline for zero-shot compositional image retrieval by combining existing off-the-shelf foundation models, e.g., BLIP-2 for image captioning, GPT for text editing and CLIP for image retrieval. In addition, the specific and explicit description with captions, instead of text embeddings, facilitates human understanding over the retrieval process. Extensive experiments are conducted to validate the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThe paper effectively integrates the VLM and LLM for ZS-CIR, offering a flexible and intervenable CIR system.\n2.\tThe authors have conducted thorough experiments to validate the effectiveness of the proposed method, making the evaluation rather reliable.\n3.\tThe figures in this paper are clear, effectively conveying the processing pipeline."
                },
                "weaknesses": {
                    "value": "1.\tLimited contribution: The proposed method appears to be a combination of several foundation models and simply employs the basic modelling capacity of these models (e.g., BLIP-2 for image captioning, GPT for text editing and CLIP for image retrieval), presenting a na\u00efve and straightforward solution for CIR task. As a result, it is challenging to identify insightful and significant contributions to this field. The authors should conduct further in-depth research to enhance their contributions.\n2.\tIncorrect experimental results: The Recall@10 of CIReVL (ViT-G/14\u2217) on CIRR dataset in Table 1 is obviously incorrect. \n3.\tInconsistent formatting of table data: In some cases, the data is presented with two decimal places, while in others, only one decimal place is used. Furthermore, there is a case with a mix of formatting in Table 3. The authors may revise the paper more carefully.\n4.\tThe paper effectively combines the existing powerful VLM and LLM models. However, it would be better to provide a more insightful analysis of the caption and reason processes. For example, from Table 3, compared to utilizing different LLM models, the use of various state-of-the-art (SOTA) captioning models has a relatively minor impact on retrieval. Are there any potential explanations for this phenomenon? Does this mean the choice of caption model is not strict, as long as the model can catch the main object or attribute of images? I think it would be better to provide more analysis. \n5.\tThe description about other works in Section 3.1 is a bit obscure for me who is not so familiar with the task but proficient other related tasks.\n6.\tMissing references, such as in \u2018Similar to existing ZS-CIR methods\u2019 in the first paragraph of Section 3.2.\n7.\tExperiments: (a) In Table 1, the author only presents the experimental results of \u2018image only\u2019 and \u2018image+text\u2019 methods for reference while missing that of the \u2018text only\u2019 method under the ViT-B/32 setting. However, the authors mention that the results on CIRR benchmark are primarily dependent on the modifying instruction while the actual reference image has much less relation to the target image, which indicates that the \u2018text-only\u2019 method can be an important reference for measuring the performance of the proposed method. Better to present the result of \u2018text-only\u2019 method and make a fair comparison and discussion. (b) In Table 1, the authors miss the results of \u2018image only\u2019, \u2018text only\u2019 and \u2018image+text\u2019 method for ViT-L/14 and ViT-G/14 settings. Better to include them for reference. (c). When evaluating on the GeneCIS benchmark, the authors do not specify the architecture of the vision backbone adopted in the experiment. Better to specify it clearly for reference.\n8.\tDiscussions and evaluation on the potential limitations: The paper shows that the proposed method has several merits including free of training, good flexibility and scalability. However, it may also have some potential limitations. For example, (a) Inference costs and efficiency: The proposed method utilizes large VLMs and LLMs to conduct the image captioning, language reasoning and cross-modal retrieval during inference. Will it take more computational costs and have longer inference time compared with the previous methods? (b) Limitations of each module: Since the proposed method is composed of three different modules, the effect of each module plays an important role on the final retrieval results. For example, if the image caption module generates partial or false descriptions for the given images, the reasoning and retrieval process will be misled. Thus, it is necessary to analyze the potential negative impact brought by each module in an in-depth manner and quantify them if possible (which factor contributes more to the failure cases). (c) Compatibility of different modules: Since the proposed method conducts cross-modal retrieval by cascading three separate modules, the compatibility of different modules seems to be an important factor. For example, if the captions generated by LLMs have different styles with pretraining data of the VLMs, the VLMs used for cross-modal retrieval may produce some bias, which may hinder the final performance. Overall, it will be appreciated if the authors can present more in-depth discussions and evaluation on the potential limitations to fully demonstrate the properties of the proposed method.\n9.\tWriting: Some parts of the paper don\u2019t flow well. The overall writing requires further improvement."
                },
                "questions": {
                    "value": "1.\tThis paper seems to be a technical report with the application of current state-of-the-art foundation models. Although directly using existing models is convenient, it inevitably introduces errors in each processing step. How to address the cumulative errors resulting from these multiple models?\n2.\tCould the authors provide experimental results of CIR works with a supervised training pattern? Based on the current results in this paper, the overall zero-shot retrieval performance appears to be poor. Do those supervised methods also yield unsatisfactory performance? If so, it seems that more significant efforts are needed to address the fundamental issues in CIR task; If not, does the zero-shot approach still have its value?\n3.\tHow you get the 6 curated samples for the Rs @K metric in Table 1?\n4.\tFor reasoning, the experiments show LLM models obviously affect the performance. I wonder whether the same is true for the prompt template. Except for the prompt mentioned in Appendix A, were there any alternative prompts explored during the experimentation?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5738/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5738/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5738/Reviewer_JtMi"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5738/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698748042808,
            "cdate": 1698748042808,
            "tmdate": 1699636601162,
            "mdate": 1699636601162,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qrM71XrSem",
                "forum": "EDPxCjXzSb",
                "replyto": "r3DhoH5bBg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5738/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5738/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Review"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their detailed comments, and the appreciation of our flexible and interpretable CIR system, thorough experimentation and clear visualization. We address their issues and feedback below.\n\n---\n\n__1. Limited Contribution:__  \nWe agree with the reviewer that our approach is a combination of existing VLMs and LLMs. However, their particular recombination to transfer it to compositional image retrieval, in which previous works have always leveraged (large-scale) additional training data to make the system work, are to the best of our knowledge, novel insights. In addition to that, the recombination and particularly chosen modular structure of our proposal offers an approach to CIR which is i)  inherently interpretable unlike existing methods (which also leverage existing pretrained VLMs & LLMs), ii) can be easily scaled up and iii) allows for human intervention into the retrieval process - all properties, which are not existent in previous works which require additional training data! As such, we do believe that our paper provides a novel, practical contribution on compositional image retrieval.\n\n---\n\n__2. & 3. Typo in the result and errors in table formatting:__   \nWe thank the reviewer for pointing out these errors, and have rectified these in the updated draft.\n\n---\n\n__4. On the differing impact of captioning and LLM modules:__    \nThe reviewer rightly points out that particular choices in the image captioning module have less impact on the overall compositional image retrieval process as opposed to changes in the reasoning module (the choice of LLM). We do believe this to be primarily due to two reasons. First, the performance gaps between the tested captioning models is much less evident compared to the gap between available foundational LLMs (Llama versus GPT-4 have vastly different reasoning capabilities). This transitions to the second aspect, in that - as correctly noted - for all benchmarks, only a subset of crucial image properties have to be captured. This can be sufficiently done by all captioning models. Indeed, when looking at failure cases such as in Fig. 3, we find that a lack of captioning quality to be one of the main error sources - something which a notably better captioning model could fix in the future.\n\nGiven the modularity of our setting, something that can be simple hot-swapped, as opposed to having to retrain a full pipeline from scratch!\nFinally, we believe that a more detailed investigation should involve the development of benchmarks which cannot be easily described through single, in parts simplistic captions. This is something we leave for future work to tackle.\n\n---\n\n__5 & 6 & 7. Writing Suggestions, missing references in 3.2 and missing baseline references:__   \nWe thank the reviewer for their valuable feedback. We have updated our paper accordingly, by adding a more detailed breakdown of the compositional image retrieval task, as well as referencing related ZS-CIR methods in the beginning of Section 3.2 (particularly Pic2Word & SEARLE). In addition, we have included the \u201ctext-only\u201d, \u201cimage-only\u201d and \u201cimage+text\u201d baseline across our experimental results. We do note that the interpretation of our results is not impacted by this additional inclusion, though agree that it belongs there for completeness. Finally, for our GeneCIS experiments, we have included information about our utilized backbone network, which follows the one utilized in the original GeneCIS paper.\n\n---\n\n__8. Limitations:__ \nThe reviewer correctly points out that while our approach offers several crucial benefits such as being high-performant, training-free, flexible and scalable, interpretable and intervenable, there is no free lunch, and a certain trade-off is to be expected. We will provide additional information for each limitation noted by the reviewer individually. \n\n* _On inference cost & efficiency compared to previous works:_ Indeed, while existing methods also utilize large VLMs, the use of distinct captioning and reasoning modules elicits additional runtime. On our hardware setup (V100 / A100 GPUs) when compared to trained methods, we find inference times to increase by ~1s. However, the modularity also allows our setup to directly benefit from efficiency changes within each module (as e.g. seen in the cost & inference time reductions for LLMs over the past year), allowing one system to remain operational for a long period of time. This stands in particular contrasts to other ZS-CIR methods that require costly training to incorporate any pipeline changes or shifts in expected datasets (e.g. Pic2Word trains the mapping network on 8 GPUs on the complete CC3M dataset)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5738/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699997463488,
                "cdate": 1699997463488,
                "tmdate": 1699997463488,
                "mdate": 1699997463488,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5JiJDT8ELi",
                "forum": "EDPxCjXzSb",
                "replyto": "r3DhoH5bBg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5738/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5738/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "With the discussion period coming to an end, we would like to thank the reviewer again for their helpful feedback, and hope that our replies, alongside strong additional experimental results and an updated draft have addressed all questions. Of course, we are happy to continue the discussion in case of any further concerns."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5738/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668339141,
                "cdate": 1700668339141,
                "tmdate": 1700668339141,
                "mdate": 1700668339141,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lgT7ok9Sbs",
                "forum": "EDPxCjXzSb",
                "replyto": "5JiJDT8ELi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5738/Reviewer_JtMi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5738/Reviewer_JtMi"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Reviewer JtMi"
                    },
                    "comment": {
                        "value": "Thanks a lot for the authors' detailed responses. This work has its own sparkle (detailed in the comments from reviewers), while as said in the initial statement, many things remain to be done, including but not limited to polishing writing and providing more in-depth discussion (as the authors response). Therefore, I keep my rating."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5738/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700705437885,
                "cdate": 1700705437885,
                "tmdate": 1700705437885,
                "mdate": 1700705437885,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "v6ah6nq00c",
                "forum": "EDPxCjXzSb",
                "replyto": "r3DhoH5bBg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5738/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5738/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate the response by the reviewer, and thank them for appreciating our work.\n\nWe do have extensively discussed issues raised by the reviewer, and have uploaded an updated draft, which has polished the writing based on the reviewers suggestions, while also including comprehensive additional, very positive results on Fashion-IQ. \n\nWe also would like to note that a discussion on limitations is also available (see response), where we showcase possible failure cases (alongside the option to address them through interventions) or have experiments that showcase compatibility of different modules.\n\nIf there is any other discussion the reviewer would like to see included and they believe has not yet been addressed, we would gladly do so."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5738/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700735636541,
                "cdate": 1700735636541,
                "tmdate": 1700735636541,
                "mdate": 1700735636541,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "kKiMeQX6wj",
            "forum": "EDPxCjXzSb",
            "replyto": "EDPxCjXzSb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5738/Reviewer_QaMw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5738/Reviewer_QaMw"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to achieve training-free zero-shot compositional image retrieval by using off-the-shelf models pretrained on large-scale datasets. The authors use language as an abstraction layer for reasoning about visual content. Particularly, it uses off-the-shelf vision-language models like BLIP-2 or CoCa to generate a detailed description of the query image. Next, an LLM or GPT-like model combines the input image description (from BLIP-2) and an input textual query (e.g., \"shows only one of them, which is bigger and is next to a trash can\") to generate a caption for the desired target image. Finally, a vision language model like CLIP performs text-to-image retrieval using the generated caption from LLM and images from an image database. This modular approach ushers a few benefits like: (i) the resulting approach is training-free and being modular, we can flexibly plug-and-play individual components like replace the LLM with either GPT or Llama, replace the VLM with BLIP-2 or CoCa; (ii) the composition happens in the human interpretable language domain offering human interpretability; and (iii) it is possible for humans to intervene on the retrieval process to fix or post-hoc improvements in retrieval results."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "While there are a lot of works on compositional image retrieval, previous works typically require training several components like textual inversion and lack human interpretability. The method proposed in this paper avoids all these problems by simply composing image and textual query in the language domain.\n\nOn the surface this is ingenious because you do it in a modular way and each module is a highly-generalisable large-scale pre-trained model. For example, when training a textual inversion, we do not really guarantee it scales to open-set setups (given we train them in limited data and compute). We just use a small-scale network and hope the rest of powerful models take care of it. The proposed method works zero-shot and you know it works for open-set setups.\n\nAdditionally, unlike prior trainable compositional image retrievals, there is a lot of effort that goes into interpretability -- \"how was the image and query text was composed\". Since the proposed method does this composition entirely in the language space, you know exactly what information was extracted from the image and you can see the generated caption from LLM for the desired target image. This not only makes the retrieval process highly transparent, but also allows post-hoc edits -- you can literally make changes to adjust your retrieval results.\n\nI encourage more works that reuses as much as large-scale models and combines them as modules with minimal or training-free way."
                },
                "weaknesses": {
                    "value": "Despite its appeal, there are a few important drawbacks. This entire process sticks on the underlying assumption that our image captioning module (e.g., BLIP-2 or CoCa) can provide a \"detailed caption\" that captures all information.\n\nThis, in my opinion, is a strong assumption. A lot depends on your captioning module. While the captioning module may be super accurate, it can miss some \"less important\" details that I want to change. For example, given a photo, the sky was orange and my textual query is \"make the colour of the sky darker\". If the image captioning module omits the colour of sky (i.e., orange) and focuses on the foreground (e.g., a person holding a flower, sitting in a bench near a park where kids are playing) -- the rest of the modules have no way of combining my textual query \"make the colour of sky darker\".\n\nThis is an example, where the entire pipeline fails -- due to no fault of the image captioning module. It provided a detailed image description, but it is not possible to describe every little \"unimportant things\".\n\nLooking at the same problem from a different direction, is the information bandwidth of an image the same as language? (an image can be worth a thousand words)"
                },
                "questions": {
                    "value": "I am confused, how is the proposed method different from VisProg (https://arxiv.org/pdf/2211.11559.pdf)?\nIt feels I can get the same benefits of the proposed method by using the VisProg paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5738/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5738/Reviewer_QaMw",
                        "ICLR.cc/2024/Conference/Submission5738/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5738/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698924173268,
            "cdate": 1698924173268,
            "tmdate": 1700736453209,
            "mdate": 1700736453209,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "12TfXG42zv",
                "forum": "EDPxCjXzSb",
                "replyto": "kKiMeQX6wj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5738/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5738/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Review"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their thoughtful comments. We appreciate that they found our paper ingenious, while highlighting the particular benefits with respect to interpretability and the capacity to allow for interventions. We address the concerns below:\n\n---\n\n__1. Is a caption sufficiently descriptive?__   \nWe agree that a simple textual caption is limited in its ability to fully reflect the content of an image. This is something we also specifically highlight throughout the paper, particularly when addressing failure cases (Fig. 3) and their addressal through interventions in Fig. 4.\n\nGenerally however, when compared to current methods for Zero-shot Compositional Image Retrieval (ZS-CIR) which focus on mapping an input image to a single token in the textual embedding space, CIReVL is far less restrictive - generating a 10-20 word caption which can leverage e.g. 50+ tokens to represent the information in the image.\n\nIn addition, operating in the language space allows our approach to be inherently interpretable, and actually intervenable.\nGiven these benefits, our work particularly tries to emphasize how far one may push a language-centric compositional image retrieval approach, and successfully shows that compared to existing methods, current benchmarks can be effectively tackled by purely representing each image with a caption/description. \n\nFuture work should focus on developing benchmarks that have information that cannot be easily described with a single description. In the current benchmarks, these cases are not the majority, while we do illustrate a potential example in Fig. 3. \n\n---\n\n__2. Differences to Visual Programming:__  \nWhile related, there are several significant differences between our proposed approach and Visual Programming. \nVisual Programming aims to solve a variety of vision and language tasks at the same time using an LLM to control the APIs to various modules. \n\nHowever, all the tasks tackled by Visual Programming typically relate to working with a single image at a time, and don\u2019t account for large-scale applications in which e.g. retrieval over an expansive gallery set has to be performed. To possibly adapt the Visual Programming method to this task, one would need to precompute the image features for all the images in the gallery set, provide sufficient in-context examples and a task description to enable the LLM to reason over the description, and provide particular instructions on how to find the correct final image. All of these modifications involve significant specialization to the CIR task and move the end result far away from the Visual Programming generalist. \n\nIn general, VisProg is a general framework which leverages different specialist modules and figures out ways to recombine them. Consequently, CIReVL should be regarded more as an expert, interpretable compositional image retrieval module that can then be leveraged by generalist agents via e.g. VisProg."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5738/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699997309802,
                "cdate": 1699997309802,
                "tmdate": 1699997309802,
                "mdate": 1699997309802,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ALadZxXcza",
                "forum": "EDPxCjXzSb",
                "replyto": "kKiMeQX6wj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5738/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5738/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "With the discussion period coming to an end, we would like to thank the reviewer again for their helpful feedback, and hope that our replies, alongside strong additional experimental results and an updated draft have addressed all questions. Of course, we are happy to continue the discussion in case of any further concerns."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5738/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668330102,
                "cdate": 1700668330102,
                "tmdate": 1700668330102,
                "mdate": 1700668330102,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4bGMdPvBiB",
                "forum": "EDPxCjXzSb",
                "replyto": "12TfXG42zv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5738/Reviewer_QaMw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5738/Reviewer_QaMw"
                ],
                "content": {
                    "comment": {
                        "value": "> Generally however, when compared to current methods for Zero-shot Compositional Image Retrieval (ZS-CIR) which focus on mapping an input image to a single token in the textual embedding space, CIReVL is far less restrictive - generating a 10-20 word caption which can leverage e.g. 50+ tokens to represent the information in the image.\n\nI do not agree with this statement. (i) Textual inversion is an optimisation-based approach that (to some extent) guarantees the text token is \"faithful\" to the image with respect to the image retrieval task. The same cannot be said about the image captioning model that \"looks\" at an image and generates a 50-word caption. (ii) Also, comparing the textual feature (from inversion) with 50 words may not be a correct evaluation (\"Interpreting the Learned Prompts\" in https://arxiv.org/pdf/2109.01134.pdf)\n\nRegarding my comment:\n```While the captioning module may be super accurate, it can miss some \"less important\" details that I want to change.```\nFig-4 seems like a patchwork to me -- if a human has to modify the generated image caption, he/she might as well write it in the first place. \n\nAlthough not a definite solution, the authors can try:\n1. Use the input text to \"prompt\" BLIP2 ($\\Psi_{C}$) to generate a \"relevant\" image caption. This prompt can be hand-engineered or learned. \n2. BLIP2 now generates a caption which is mindful of the input text -- hence, it does not miss the \"less important details that I want to change\".\n3. The pipeline stays interpretable -- as the image+text combination happens in the textual domain.\n\nI am still keeping my rating as weak reject (score 5), but the ACs may feel to accept the paper nonetheless."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5738/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700700465747,
                "cdate": 1700700465747,
                "tmdate": 1700700465747,
                "mdate": 1700700465747,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "13sdVs6IB4",
                "forum": "EDPxCjXzSb",
                "replyto": "kKiMeQX6wj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5738/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5738/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their detailed reply and reference, and would like to note two things:\n\n---\n\n>  Textual inversion is an optimisation-based approach that (to some extent) guarantees the text token is \"faithful\" to the image with respect to the image retrieval task. The same cannot be said about the image captioning model that \"looks\" at an image and generates a 50-word caption.\n\nIf e.g. looking at SEARLE, the textual inversion network is trained to generate a token that simply mimics classnames/concepts - so information available through the generated token will still be very lackluster. \nIn addition to that, the computed text token is then incorporated into a full input prompt which is then fed into a text embedding system - and there is only so much information that can be packed into a single text token to account for the entire CIR task. \n\nTaken together, a lot of relevant context will still go missing - and without a clear mechanism for interpretability, it is unclear what exactly. \n\nOn the other hand, there is no limit to the captions we utilize, and as the reviewer rightly highlighted, the captioning system can be extended arbitrarily (if needed), without impacting the overall pipeline.\nThe generated captions generally provide much more context (as shown also qualitatively in the paper) beyond a single concept, and the resulting high performance gains we achieve - even when using the same retrieval system - support this.\n\n---\n\n> While the captioning module may be super accurate, it can miss some \"less important\" details that I want to change. Fig-4 seems like a patchwork to me -- if a human has to modify the generated image caption, he/she might as well write it in the first place.\n\nThis is of course true - the captioning system is not perfect, but given the modular nature, can simply we replaced in a plug-and-play fashion if needed. However, we show that even with very general open-source captioning models, that very strong CIR performance can be achieved.\n\nRegarding the human just writing the caption themselves, this is exactly the point of the intervenability, and a byproduct of our inherent interpretability. \nIf the human believes that insufficient context is extracted from an image, and wants to guarantee a more faithful retrieval, they can also write the caption themselves.\nSuch an intervention into the retrieval process is not possible for other trained systems, and is simply a bonus point on top of the modular and interpretable CIR framework."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5738/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700735555845,
                "cdate": 1700735555845,
                "tmdate": 1700735555845,
                "mdate": 1700735555845,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xHnx7C2jfO",
            "forum": "EDPxCjXzSb",
            "replyto": "EDPxCjXzSb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5738/Reviewer_W9gL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5738/Reviewer_W9gL"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a training-free solution for addressing the compositional image retrieval problem. The idea is quite simple: i) take the pre-trained VLM model to describe what the input image contains, ii) use an LLM to combine the description/caption of the input image with the desired modifications as described by the user query, finally, iii) use the resulting text to generate VLM (i.e., CLIP) representations and search the target image database via knn search. The solution is evaluated against multiple baselines (PALAVRA, Pic2Word, SEARLE) and datasets (CIRCO, CIRR, GeneCIS) and provides better results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is technically sound and widely applicable not only for text-image use case, but also text-video and probably other problem domains. \n- The paper investigates three different datasets against multiple baselines and various additional ablations make the paper and results more appealing. Limitations of the proposed method also gets discussed with examples. \n- The paper is easy to follow / well-written."
                },
                "weaknesses": {
                    "value": "- [Novelty & Literature review] There are a wider number of papers in the field since last year and some of those papers requires mention (at least the ones before the ICLR submission deadline). The most similar paper ((https://arxiv.org/pdf/2310.05473.pdf) proposes the same approach but with an additional training method for merging the automatically generated image caption and the desired modification's text description. This paper contains additional papers/baselines to cite/include. It would be great if there is a way to understand whether the proposed training-free method would perform better or worse compared to the paper mentioned above.\n\n- [Experiments] Baseline PALAVRA attack different set of problems, uses different datasets and metrics. It does not mention CIR task in the text. Not clear if PALAVRA serves as a baseline."
                },
                "questions": {
                    "value": "- A short discussion about extensibility / applicability of the proposed approach could be beneficial for the research community. \n- FashionIQ is yet another dataset which other papers use for CIR evaluations, it might be worth considering it in the future. \n- Is there a way to detect when the LLM fails to provide a meaningful / valid re-composed text ? How much room left to improve this step? What is the future research direction?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5738/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5738/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5738/Reviewer_W9gL"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5738/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699048763716,
            "cdate": 1699048763716,
            "tmdate": 1700695941883,
            "mdate": 1700695941883,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Tgq58V5HGB",
                "forum": "EDPxCjXzSb",
                "replyto": "xHnx7C2jfO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5738/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5738/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Review"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their thoughtful comments. We appreciate that they found our paper technically sound, widely applicable, easy to follow and with rigorous experimentation. \nWe address the concerns below:\n\n---\n\n__1. Novelty of the proposed approach:__   \nWe thank the reviewer for pointing out the additional reference. However, the paper is contemporary to our submission (i.e. it was published on arxiv after the ICLR deadline), thus we could not have provided a comparison with it/known about its existence. We acknowledge several related works in Section 2 of our main paper, and highlight crucial differences between these works and our approach. \n\nMost works on compositional image retrieval aim to train/fine-tune models specific for each dataset (i.e., training separate models for CIRR, Fashion-IQ etc.). Zero-shot Compositional Image Retrieval (ZS-CIR) instead aims to train a single model that generalizes across multiple benchmarks without the use of paired (source image, text, target image) data. The only works that explicitly focus on this task (and that we compare with) are Pic2Word [Saito et al 2023] and SEARLE [Baldrati et al. 2023], which are based on a different principle, i.e. trained textual inversion. Both methods already leverage off-the-shelf, large-scale pre-trained models as part of their pipeline.\n\nIn our work, we go one step further and propose the first training-free approach for this task, that only relies on off-the-shelf pre-trained models, while allowing for easier scalability, interpretability and the possibility of interventions. \n\n---\n\n__2. PALAVRA as a baseline:__   \nWe agree that PALAVRA has not been proposed for the CIR task. However, since it is conceptually very similar to Pic2Word and SEARLE, it was originally introduced as a comparison in these works. To retain direct, comprehensive comparability, we also utilize PALAVRA as a corresponding baseline.\n\n---\n\n__3. Results on Fashion-IQ:__   \nWe thank the reviewer for the suggestion. We have now updated our paper with the results on the Fashion-IQ benchmark. For the detailed results, we refer to our shared comment. In summary, we find that CIReVL significantly outperforms existing methods on all three subtasks, e.g. improving average Recall@10 by nearly 6 percentage points (pp) (28.29% versus 22.89% for SEARLE) with a ViT B/32 backbone. In addition, the simple scalability allows us to hot-swap the retrieval model to achieve additional 4pp (increasing from 28.29% to 32.19%).\n\n---\n\n__4. Short Discussion on extensions and applications of our approach:__   \nThe reviewer correctly points out that our method would in-principle be applicable to videos as well as other forms of retrieval contexts, such as with interleaved images and texts (e.g. visual storytelling).\n\nWhile our exact setup is structured to operate and scale especially well to the compositional image retrieval task, as long as the particularities of the problem at hand can be reasoned over in the language domain while retaining a downstream compositional retrieval or alignment target, CIReVL can certainly be adapted to account for this. \n\nWe believe this to be of definite interest for future research, and will expand our discussion to incorporate the arguments above.\n\n---\n\n__5. Detect LLM Failures and future steps:__   \nIndeed, it is not easy to automatically detect failures in the captioning model or the LLM reasoning, even though our intervention examples highlight a clear need to account for these shortcomings, and the benefits of addressing them.\n\nFor instance, one strategy may be in using another LLM to verify if the generated description is plausible or not, given the initial caption and instruction. However, doing so can only tackle issues in the LLM reasoning part, and is not able to address failures in the image captioning.\n\nAs can be seen, finding an automated mechanism to detect all manners of intervention cases is a difficult problem on its own, albeit a crucial component to improve the performance of compositional image retrieval systems. It is something we would love to see tackled in future work."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5738/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699997201140,
                "cdate": 1699997201140,
                "tmdate": 1699997201140,
                "mdate": 1699997201140,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "D1E7ZnG5to",
                "forum": "EDPxCjXzSb",
                "replyto": "xHnx7C2jfO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5738/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5738/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "With the discussion period coming to an end, we would like to thank the reviewer again for their helpful feedback, and hope that our replies, alongside strong additional experimental results and an updated draft have addressed all questions. Of course, we are happy to continue the discussion in case of any further concerns."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5738/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668319882,
                "cdate": 1700668319882,
                "tmdate": 1700668319882,
                "mdate": 1700668319882,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dxcenQITrT",
                "forum": "EDPxCjXzSb",
                "replyto": "D1E7ZnG5to",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5738/Reviewer_W9gL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5738/Reviewer_W9gL"
                ],
                "content": {
                    "comment": {
                        "value": "Although other reviewers expressed concerns over novelty, the paper proposes a simple (easy to re-implement) baseline for future work in this research area. The solution also provides outstanding results compared to other ZS-CIR baselines. I am in favor of increasing the initial rating. The authors also largely addressed my previous questions. Additional experiments on Fashion-IQ, discussions on novelty and future work are also convincing. This also contributed to the final decision."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5738/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700695892400,
                "cdate": 1700695892400,
                "tmdate": 1700695892400,
                "mdate": 1700695892400,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]