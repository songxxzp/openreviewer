[
    {
        "title": "Skip-Attention: Improving Vision Transformers by Paying Less Attention"
    },
    {
        "review": {
            "id": "iKpQku4Cq7",
            "forum": "vI95kcLAoU",
            "replyto": "vI95kcLAoU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4929/Reviewer_f6zn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4929/Reviewer_f6zn"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a new attention mechanism, named skip attention, aiming to reduce the computional cost of vision transformers. It is based on a simple observation that the attention maps of  adjacent  transformer blocks share similar patterns. Authors propose to reuse the attention maps of the current block in the next several ones by introducing a series lightweight operations, like linear transformations and efficient channel attention."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This paper is well written. In the introduction section, the authors clearly explain the motivation of this paper, which is originally from the visualization of the attention maps of ViTs. The presentation is also clearly. It is easy for readers to follow the work.\n\n- The results are good. When applied different versions of ViTs, the proposed method receives clear improvement over the baselines."
                },
                "weaknesses": {
                    "value": "- It seems that the motivation of this paper has been mentioned in Zhou et al. (Refiner: Refining self-attention for vision transformers). They observe that reusing the attention maps in the next transformer block does not brings performance drop. The authors should more clearly explain the differences between this paper and the work mentioned above.\n\n- The baselines used in this paper are not recently proposed. The results are already not state-of-the-art compared to recent works, like CMT (CVPR'2022). I would like to see how would the performance go when the proposed approach is applied to recent state-of-the-art ViT models as they mostly did not change the self-attention part.\n\n- Many ViT models are based on window self-attention, which is original proposed in Swin Transformer (ICCV'2021). The authors have shown that the proposed method works well for the original self-attention. So, how would the performance go when the proposed method is applied to ViTs with window self-attention.\n\n- In my view, one of the important functionalities of this paper is to compress vision transformers. Maybe the authors can show more comparisons with methods for compressing ViTs, like DynamicViT and EViT. As I found the proposed approach can improve the baselines' performance with even less computations. This may better highlight the strength of this paper."
                },
                "questions": {
                    "value": "I care more about the novelty of this paper as the originality of this paper has been mentioned in a previous work. If the authors make this clear, I would like to raise the ranking score."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4929/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4929/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4929/Reviewer_f6zn"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4929/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698499818735,
            "cdate": 1698499818735,
            "tmdate": 1700481587579,
            "mdate": 1700481587579,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bUQwKTF08K",
                "forum": "vI95kcLAoU",
                "replyto": "iKpQku4Cq7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4929/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4929/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer f6zn"
                    },
                    "comment": {
                        "value": "We appreciate the Reviewer f6zn's valuable feedback. We address the concerns as follows:\n\n**1. Comparison with Refiner**\n\nThank you for the pointer. We discuss Refiner (Zhou et al.) in Section 2 under ``Efficient attention\" (6th line from the end of the paragraph). \n\nWe acknowledge that both SkipAT and Refiner leverage the high correlation across layers of a transformer, but for two different purpose: increasing the classification accuracy (by Refiner) and increasing the computational efficiency (by SkipAT). Refiner introduces additional convolutions on top of ViT to de-correlate the attention maps to increase the expressive power of the model. This obviously comes at an additional computational cost both in terms of parameters and latency compared to the vanilla ViT. In contrast, SkipAt removes the correlated MSA blocks and approximate them using a cheap convolutional alternative leading to a reduced computation compared to the vanilla ViT.\n\nOn ImageNet-1K, Refiner achieves a top-1 accuracy of 80.3\\% using 2M more parameters than SkipAT, which achieves a similar accuracy of 80.2\\%. We shall clarify and further emphasize the novelty of SkipAT with respect to Refiner in the paper.\n\n**2. Comparison with CMT**\n\nWe thank the reviewer for providing the reference. In Table 2(a), we compare SkipAT with methods that focus on improving the efficiency of transformers without changing the backbone. We also provide a comprehensive list of SoTA methods in Table 7 in the Appendix. We shall add CMT to Table 7. \n\nIn Table 2(a), we apply SkipAT to SoTA transformer backbones such as PvT and LIT, and show that SkipAT is agnostic to transformer architecture and improves both top-1 accuracy and throughput of these backbones.\n\n**3. Use of SkipAT with window self-attention**\n\nIn Section 4.1 -- ``Image Denoising\" and Table 4, we describe the application of SkipAT to Uformer, which is a U-shaped hierarchical network with Swin transformer blocks as the encoder and decoder. We skip the whole window self-attention (WSA) block in each decoder block by reusing attention of the corresponding encoder block via the SkipAT parametric function. A detailed description is provided in Section 6.2 in Appendix.\n\nWe observe from Table 4, that  SkipAT outperforms the baseline Uformer variants with the 25\\% higher\nthroughput on average. \n\n**4. Comparison with DynamicViT and EViT**\n\nWe compare SkipAT with DynamicViT in Table 7 in the Appendix. When using ViT-T/16 as the backbone, SkipAT outperforms DynamicViT by 2\\% in terms of top-1 accuracy and also achieves higher throughout (6900 vs 6100 im/sec). \n\nComparing EViT [1*] with SkipAT using ViT-S/16 as backbone, SkipAT has comparable top-1 accuracy (80.2\\% of SkipAT vs. 79.8\\% of EViT) and comparable throughput (3800 vs/ 3900 im/sec). We shall add these results in Table 7.\n\n[1*] Liang *et al.*, Not All Patches are What You Need: Expediting Vision Transformers via Token Reorganizations, ICLR 2022"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4929/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700333761643,
                "cdate": 1700333761643,
                "tmdate": 1700333761643,
                "mdate": 1700333761643,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VccVb3gA9N",
                "forum": "vI95kcLAoU",
                "replyto": "bUQwKTF08K",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4929/Reviewer_f6zn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4929/Reviewer_f6zn"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the feedback. The authors have solved most of my concerns. I'd like to lift the score to 6."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4929/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700481565400,
                "cdate": 1700481565400,
                "tmdate": 1700481565400,
                "mdate": 1700481565400,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "aYk8uXAV2x",
            "forum": "vI95kcLAoU",
            "replyto": "vI95kcLAoU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4929/Reviewer_iYVP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4929/Reviewer_iYVP"
            ],
            "content": {
                "summary": {
                    "value": "This paper improves the efficiency of Vision Transformer by replacing some attention layers with a compute-efficient parametric function, ie, convolutional feed-forward layer. The idea is motivated by a clear observation and analysis that attention patterns tend to be redundant between different layers, indicating a strong correlation. With the novel design, the authors validated the framework on various architecture and datasets. Comprehensive experiments have shown the advantage of their method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The motivation of this paper is very clear, accompanied by strong analysis in the attention patterns. \n2. The figures and visualizations can clearly demonstrate their method. The overall presentation is good to me.\n3. Experiments are comprehensive, including different architectures, datasets, tasks, which strongly demonstrate that the proposed method is general.\n4. The performance gain is also consistent across different settings."
                },
                "weaknesses": {
                    "value": "1. Based on the analysis in Section 3.2, it makes sense for the authors to apply their method from layer 2 to 8. However, it is not convincing for different pretrained ViTs to skip layer 2 to 8 as well if considering different training objectives or pretrained datasets. Thus, it would be better for the authors to study if other pretrained ViTs (MAE [A], DINOv2 [B], SAM [C]), have the same phenomenon.\n\n2. Introducing convolution into ViTs has shown to be effective in related works [D], which is intuitive to me to achieve performance gain for SKIPAT. In this paper, SKIPAT adopts convFFN as a parametric function to replace MSAs, which still needs to be trained from scratch in order to achieve efficiency gain. It would be promising if this parametric function can be used as a drop-in replacement for existing large ViTs.\n\n\n[A] He, Kaiming, et al. \"Masked autoencoders are scalable vision learners.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022.\n\n[B] Oquab, Maxime, et al. \"Dinov2: Learning robust visual features without supervision.\" arXiv preprint arXiv:2304.07193 (2023).\n\n[C] Kirillov, Alexander, et al. \"Segment anything.\" ICCV (2023).\n\n[D] Wang, Wenhai, et al. \"Pvt v2: Improved baselines with pyramid vision transformer.\" Computational Visual Media 8.3 (2022): 415-424."
                },
                "questions": {
                    "value": "Can the authors specify more on the experimental setting of applying SKIPAT into hierarchical ViTs? I can understand that SKIPAT works for layer 2 to 8 in plain ViTs. But it is not intuitive to me how to select the layers to skip in PVT, LIT, etc."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4929/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4929/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4929/Reviewer_iYVP"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4929/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698551737540,
            "cdate": 1698551737540,
            "tmdate": 1699636478572,
            "mdate": 1699636478572,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Yrc7SGBTnJ",
                "forum": "vI95kcLAoU",
                "replyto": "aYk8uXAV2x",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4929/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4929/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer iYVP"
                    },
                    "comment": {
                        "value": "We appreciate the Reviewer iYVP's valuable feedback. We address the concerns as follows:\n\n**1. Different pretrained ViTs**\n\nThanks for raising this interesting question: To what extent the correlation analysis from a supervised image classification model, as in Section 3.2, generalize to other tasks and datasets?\nWe empirically observe that the optimal configurations of parametric function is determined mostly by the transformer architecture rather than the task.\n\n When considering different training objectives, we applied SkipAT to DINO in Section 7 of the Appendix under ``Self-Supervised Learning with DINO\". We observe that SkipAT achieves almost the same performance as fully trained DINO with around 26\\% less training time (73.3\\% in 96 GPUhours vs. 73.6\\% in 131 GPU-hours). \nMoreover, we also use SkipAT on semantic segmentation on ADE20K (Table 3) and Image Denoising on SIDD (Table 4), showing SkipAT is generalizable to different training objectives.\n\nFor other architectures, a principled way could be to compute the correlation matrix by computing the CKA between different layers and using a threshold to identify the layers on which SkipAT can be applied, as we have done for PvT and LIT on Table 2(a).\n\n**2. Using SkipAT in pretrained models**\n\nIndeed finding a parametric function, which can be plugged into any pretrained model is practically coveted but is very challenging given the complex interplay between the layers of a deep neural network.\nSurprisingly, our experiments demonstrate that using an identity function on a pretrained ViT-T/16 in a plug-and-play fashion could still perform reasonably well: only 9.3\\% drop in performance, when copying self-attention matrix $A$ in layers 3-8 (Table 9 of the Appendix). \n\nRegularizing transformer blocks to be more correlated during training could be an step toward more robust plug-and-play adaptors.\n\n**3. Experimental setting in hierarchical ViTs**\n\n*Using SkipAT in PvT*\n\nAs reported in Table 2, we applied SkipAT on a hierarchical architecture PvT-S, which consist of 4 stages and each stage consists of 3, 3, 6 and 3 transformer blocks respectively. We observe high correlation of $Z^{MSA}$ in the third stage and apply SkipAT in the intermediate blocks i.e. between 2 through 5 blocks. In this case, we still have applied SkipAT to adapt the features of the same dimensionality. However, by increasing the stride of the depth-wise separable convolutions in the parametric function, SkipAT can adapt features across different resolutions. \n\n*Using SkipAT in LIT*\n\nAs reported in Table 2, we applied SkipAT in LIT-T and LIT-S, which consist of 4 stages. The first two stages consists of convolution layers and the last two stages consists of transformer blocks. We apply SkipAT in the intermediate transformer blocks in the third stage of LIT i.e. we skip layers 2 to 5 in the third stage.\n\nWe will clarify this in the text."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4929/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700333537249,
                "cdate": 1700333537249,
                "tmdate": 1700333537249,
                "mdate": 1700333537249,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Z0FYIM3Vv5",
                "forum": "vI95kcLAoU",
                "replyto": "Yrc7SGBTnJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4929/Reviewer_iYVP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4929/Reviewer_iYVP"
                ],
                "content": {
                    "title": {
                        "value": "Additional Comments"
                    },
                    "comment": {
                        "value": "Thanks for the authors's rebuttal. I'm satisfied with the response. The additional experiments with other pretrained ViTs are interesting. It would be great to include these results in the appendix."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4929/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700474485889,
                "cdate": 1700474485889,
                "tmdate": 1700474485889,
                "mdate": 1700474485889,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NSxyaCwH4L",
            "forum": "vI95kcLAoU",
            "replyto": "vI95kcLAoU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4929/Reviewer_VJcB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4929/Reviewer_VJcB"
            ],
            "content": {
                "summary": {
                    "value": "A core component of the vision transformer is the self-attention layer, which is quadratic in the number of tokens. Following similar insights in (Raghu et al. 2022), the authors observe that self-attention operation is redundant at least in the intermediate layers, i.e. there is high correlation between:\n* (CLS -> token) Attention maps between  at layer $L$ and layer $L - 1$.\n* MSA representations between layer $L$ and layer $L - 1$.\n\nLeveraging this insight, the authors propose to replace the more computationally intensive attention operation with a lightweight refinement module termed SkipAt. More specifically, MSA at depth $L$ is replaced with inverted bottleneck layers (depthwise convolutions sandwiched between two dense layers). SkipAt layer $L$ takes the output of SkipAt layer $L - 1$ as input, as opposed to Multi-Head Self-Attention (MSA) at layer $L$ which takes the output of the MLP layer $L - 1$ as input.\n\nThe authors show experiments on classification, segmentation, unsupervised object discovery and image (+video) denoising.  They plugin their SkipAt technique and attain improved throughput and in most cases, improved accuracy."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The approach is simple and effective.\n* The authors have tested their SkipAt approach on a number of tasks. Their approach improves over similar Vision Transformer backbones and leads to improved throughputs.\n* The writing is crisp and clear."
                },
                "weaknesses": {
                    "value": "Some experiments can be added which decouple the improvements obtained with convolutions vs the SkipAt formulation. I initially rate this  above bordeline. If the authors can convincingly answer my questions, I am happy to increase the score."
                },
                "questions": {
                    "value": "## Major Requests:\n-----------\n\n* The main motivation of SkipAt is that the output representations of MSA in vision transformers are redundant. So, the paper claims that just refining the outputs of the previous MSA layers is sufficient. However, SkipAt consists of depthwise convolutions which are also quite powerful modules themselves, so it is unclear if the throughput gains come just by the convolutions rather than the SkipAt formulation. I suggest that the authors run the couple of ablations below, If these ablations reach lower accuracy, it would be convincing that the SkipAt formulation is responsible for the accuracy gains.\n  * Replace all layers with SkipAt instead of just layers from 3 through 8. According to the authors hypothesis, since layers 9 though 12 have lesser correlation, using SkipAt at these layers should hurt accuracy.\n  * To show the importance of skipping the attention blocks, in Eq 7) the authors can replace $\\phi(Z_{l-1}^{MSA})$ with just $\\phi(Z_{l-i})$. This will give more evidence that skipping the attention block is necessary.\n\n* The authors test their module on Ti, B and S which all have 12 layers so they recommend to use SkipAt layers from 3 through 8. How do they recommend tuning these for larger depths?\n* In page 6, authors say that $n >> d$ and so $O(n^d)$ term dominates. I would suggest the authors add that this is specific to dense prediction tasks, since for image classification even for a S model (d=384, and n=196), so the claim that n >> d is not general.\n* Are the throughput increase in Fig a) significant? What do the error bars look like?\n\n## Minor Comments:\n-----------\nThese are just nice to have and are not likely to influence my final rating.\n\n* The authors use the efficient channel module. But, the ablation is missing from Table 5.\n* The figures from Raghu et al, indicate that there might be redundancy across the MLP layers as well. Does it make sense to have a Skip-MLP module?\n\n## Minor suggestions:\n------\n\nSome suggestions to improve presentations:\n\n* The authors can consider making the numbers in Figure 3 and Figure 5 bigger.\n* It is clear by comparing Fig 3 b) and Fig 5 b), that Fig 5 b) has lower correlation. The authors can also have a line plot where the x-axis is the layer id and y-axis is the average correlation of layers before it. If we plot the baseline ViT and the ViT with SkipAt on the same graph, it will make the comaprison even clearer. \n* In Fig 1), are the circles to indicate #params necessary since they are roughly the same? It gives the impression that the improvements are not significant even if they are, since the circles overlap"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4929/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698676939643,
            "cdate": 1698676939643,
            "tmdate": 1699636478481,
            "mdate": 1699636478481,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YAxmebDmIF",
                "forum": "vI95kcLAoU",
                "replyto": "NSxyaCwH4L",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4929/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4929/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer VJcB"
                    },
                    "comment": {
                        "value": "We appreciate the Reviewer VJcB's valuable feedback. We address the concerns as follows:\n\n**1. Additional ablations** \n\nThank you for this interesting suggestion. Based on the reviewer's suggestion, we implemented the SkipAT parametric function across all layers of VIT-T/16 and train for 100 epochs on ImageNet-1K. The default configuration, which employs SkipAT at layers 3 to 8, yields a top-1 accuracy of 67.7\\% (Table 5 in the main paper). When SkipAT is applied to layers 1 through 12, the top-1 accuracy decreases to 54.1\\%, indicating a significant drop of 13.6\\%. This suggests that leveraging the parametric function is only beneficial for approximating MSA blocks with high correlation. \n\nMoreover, we observe that using an Identity function, where we simply copying $Z^{MSA}$ features across 3 to 8 layers, achieves decently high top-1 accuracy of 61.1\\% (Table 5), witnessing high correlation in MSA blocks. Thus, we hypothesize that the performance gains is due to the SkipAT formulation. Additionally, using more powerful parametric function such as convolution layer marginally increases performance (top-1 accuracy: 65.4\\%) but increases throughput by 39\\% (8500 vs. 5200 im/sec) (Table 5).\n\nWe understand the reviewer asks us to replace SkipAT parametric function $\\phi(Z^{MSA}_{l-1})$ with an Identity function (i.e. using features from prev. blocks). If so, we mentioned the effect of using an Identity function above. \n\nWe are happy to follow-up with the reviewer for any clarifications or additional ablations.  \n\n**2. Using larger models**\n\nThe reviewer raises an interesting point. \n\nFor large models, a principled approach would be to compute the correlation matrix using the CKA analysis of $Z^{MSA}$ for every layer. Using a threshold on this correlation matrix would indicate the layers with high correlation, on which the SkipAT parametric function can be applied. \nWe use this approach to apply SKipAT on pyramid (PvT) and hybrid (LIT) transformer architectures (Table 2(a)) where arrangement of transformer blocks are different than ViT. For example, PvT-S consists of 4 stages with 3, 3, 6 and 3 transformer blocks at each stage. We observe high correlation of $Z^{MSA}$ in the third stage and apply SkipAT in the intermediate blocks i.e. between 2 through 5 blocks.\n\nAnother approach could be using more advanced techniques such as Neural Architectural Search (NAS). While NAS might propose valid solutions, it is computationally more expensive.\n\n**3. Complexity for dense prediction tasks**\n\nThank you for the precise point: Indeed the claim holds for dense prediction tasks. We shall make this change.\n\n**4. Error bars of throughput**\n\nWe report the mean and standard deviation of the throughput of SkipAT for 10 runs on ImageNet-1K using ViT-T/16, ViT-S/16 and ViT-B/16. For a single run, we measure throughput (image/sec $\\times 10^3$) with a batch size of 1024 on a single NVIDIA A100 GPU, averaged over the validation set of ImageNet-1K. The results are in the table below:\n\n| Arch               | Throughput (im/sec $\\times 10^3$) |\n| ------------------ | ---------------------------------- |\n| ViT-T/16           | 5.9 $\\pm$ 0.3                     |\n| SkipAT + ViT-T/16  | 6.8 $\\pm$ 0.2                     |\n| ViT-S/16           | 3.1 $\\pm$ 0.3                     |\n| SkipAT + ViT-S/16  | 3.7 $\\pm$ 0.3                     |\n| ViT-B/16           | 1.2 $\\pm$ 0.2                     |\n| SkipAT + ViT-B/16  | 1.6 $\\pm$ 0.1                     |\n\n**5. Minor comments and suggestions**\n\nThank you very much for these insightful comments and suggestions. We shall incorporate all of them in the camera-ready version."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4929/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700333165731,
                "cdate": 1700333165731,
                "tmdate": 1700333165731,
                "mdate": 1700333165731,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "a3RQhW9cp4",
                "forum": "vI95kcLAoU",
                "replyto": "YAxmebDmIF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4929/Reviewer_VJcB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4929/Reviewer_VJcB"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal Response"
                    },
                    "comment": {
                        "value": "Thanks for the additional experiments:\n\nAll my comments are addressed except this:\n\n\"To show the importance of skipping the attention blocks, in Eq 7) the authors can replace $\\phi(Z_{l-1}^{MSA})$ with just  $\\phi(Z_{l-i})$.\" The authors show what happens if $\\phi$ is replaced by an identity function..\nHowever the requested ablation was to simply replace the self-attention blocks in the original ViT formulation with depthwise convolutions. According to the mathematical notation, precisely, the request was for:\n\n* $\\phi$ to be the same (i.e, depthwise convolution)\n* the ablated input to $\\phi$ to be $Z_{l-i}$ (See: Eq 4 and Eq 5) instead of $Z_{l-1}^{MSA}$.\n\nIn particular, the authors claim still holds that they are able to outperform baseline transformers with the skip attention modules. But it is still unclear to me if the gains observed by the authors can come with just replacing self-attention in the baseline VisionTransformer with depthwise convolutions."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4929/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700644447824,
                "cdate": 1700644447824,
                "tmdate": 1700644447824,
                "mdate": 1700644447824,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]