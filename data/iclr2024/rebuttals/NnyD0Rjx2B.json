[
    {
        "title": "fairret: a Framework for Differentiable Fairness Regularization Terms"
    },
    {
        "review": {
            "id": "DQNHySyicJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1706/Reviewer_BABL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1706/Reviewer_BABL"
            ],
            "forum": "NnyD0Rjx2B",
            "replyto": "NnyD0Rjx2B",
            "content": {
                "summary": {
                    "value": "Paper claims that current tools for machine learning fairness only admit a limited range of fairness definitions and have seen little integration with automatic differentiation libraries.\nFor this reason they introduce a framework of fairness regularization terms which quantify bias as modular objectives that are easily integrated in automatic differentiation pipelines."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The overview, FAIRRET, and results are interesting and valuable."
                },
                "weaknesses": {
                    "value": "Authors claim is an overstatement in fact there are plenty of work that proposes differentiable (and in some case even convex) regularisers.\nState of the art is largely incomplete."
                },
                "questions": {
                    "value": "There are plenty of work that proposes differentiable (and in some case even convex) regularisers (e.g. [1] but in ICML, NeurIPS, etc. you can find plenty of work on this). Paper should elaborate on that.\n\n[1] Exploiting MMD and Sinkhorn Divergences for Fair and Transferable Representation Learning, NeurIPS 2020.\n[2] Deep Fair Models for Complex Data: Graphs Labeling and Explainable Face Recognition, Neurocomputing 2021."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "none"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1706/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697129353795,
            "cdate": 1697129353795,
            "tmdate": 1699636099153,
            "mdate": 1699636099153,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9GA8ZF8UQ3",
                "forum": "NnyD0Rjx2B",
                "replyto": "DQNHySyicJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1706/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1706/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the Reviewer for sharing their thoughts.\n\nThe area of *fair representation learning* is indeed a line of work that was not acknowledged in our Related Work section. Note that we *do* compare against this class of methods by having an adversarial method (Adel et al. 2019) as a baseline, which also tries to find fair representations in a hidden layer. However, a broader overview of fair representation learning was indeed warranted, and so we added additional references in our Related Work. \n\nYet, this line of research does not refute our claim that \u201ccurrent tools for machine learning fairness (1) only admit a limited range of fairness definitions and (2) have seen little integration with automatic differentiation libraries\u201d. Though methods for fair representation learning typically use automatic differentiation (2), they cannot directly pursue a wide range of fairness definitions (1). For example, both your provided references (Oneto et al. 2020; Franco et al. 2022) only address the fairness notion of demographic parity. The reason is clear: fair representation learning focuses on the distribution of dense, internal representations. However, fairness definitions are typically model-agnostic and only consider the output of the model. It is these latter definitions that we generally pursue with differentiable regularizers in our framework, which is also model-agnostic (requiring only that the model is a differentiable, probabilistic classifier). \n\nMoreover, the claim \u201ccurrent tools for machine learning fairness have seen little integration with automatic differentiation libraries\u201d clearly holds for most fairness toolkits, such as Fairlearn and AIF360. These require models to follow the interface of `scikit-learn` Estimators. Instead, fairrets are modular objectives that can be plugged into a training step. We show an example of this in the revised Appendix E.\n\nBesides the questioning of this single claim regarding the state-of-the-art, the Reviewer did not provide further arguments to support their low rating.\n\n**References**\n\nAdel, T., Valera, I., Ghahramani, Z., & Weller, A. (2019, July). One-network adversarial fairness. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 33, No. 01, pp. 2412-2420).\n\nOneto, L., Donini, M., Luise, G., Ciliberto, C., Maurer, A., & Pontil, M. (2020). Exploiting mmd and sinkhorn divergences for fair and transferable representation learning. Advances in Neural Information Processing Systems, 33, 15360-15370.\n\nFranco, D., Navarin, N., Donini, M., Anguita, D., & Oneto, L. (2022). Deep fair models for complex data: Graphs labeling and explainable face recognition. Neurocomputing, 470, 318-334."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1706/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700067145188,
                "cdate": 1700067145188,
                "tmdate": 1700067145188,
                "mdate": 1700067145188,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JEHnSbvHwk",
                "forum": "NnyD0Rjx2B",
                "replyto": "9GA8ZF8UQ3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1706/Reviewer_BABL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1706/Reviewer_BABL"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors, thanks for the reply.\nRegarding comparison with baselines i think that  (Adel et al. 2019) is not enough nether a state-of-the-art baseline given the large amount of more recent works.\nRegarding the comment on the fact that just demographic parity is tested, i think is not accurate. Most group fairness definitions (e.g., equal odds, equal opportunity, uncorellation) can be defined under the same hat bus simply constraining the distribution of the representation to be close for a subset of the data distribution (e.g., for equal opportunity the distribution of the representation of the male labeled with +1 and the distribution of the representation of the female labeled with +1).\nRegarding the claim of differentiability, there is a number of works which propose both differentiable or even convex relaxation of the fairness definitions with theoretical properties so the statement for me is still too much.\nAll these comments led me to my low rank: paper novelty, in my opinion, is limited since large amount of related works are not properly compared to the proposal neither theoretically nor empirically."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1706/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700116543070,
                "cdate": 1700116543070,
                "tmdate": 1700116543070,
                "mdate": 1700116543070,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AOynSDvpnx",
            "forum": "NnyD0Rjx2B",
            "replyto": "NnyD0Rjx2B",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1706/Reviewer_hHUL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1706/Reviewer_hHUL"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors study the problem of fairness in ML. To be specific, they introduce a formal framework which facilitates defining regularization terms for fairness which can be minimized using auto-differentiation tools. The authors report significant improvements over baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ The paper provides a good formal coverage of the fundamental concepts."
                },
                "weaknesses": {
                    "value": "1. I have concerns about the novelty. It appears that the paper's novelty is a formal fairness regularization framework. The paper criticizes existing frameworks for not being formal and limited in terms of fairness definitions. However, the paper does not showcase what the benefit of this formal framework is. Moreover, it is not clear why FFB or FairTorch cannot be extended to include more fairness definitions.\n \n2. I find \"fairness tools\" misleading. This is sometimes used to refer to fairness measures,  sometimes to their approximations as regularization terms and sometimes to their implementations.\n\n\nMinor comments:\n- I would recommend following the following guide while writing equations: \nhttp://www.ai.mit.edu/courses/6.899/papers/mermin.pdf"
                },
                "questions": {
                    "value": "Please see Weaknesses.\n\n**After Rebuttal**\n\nI've read the comments provided by other reviewers and the responses by the authors. I find that the authors have sufficiently addressed my concerns. Looking at the sample code in Appendix E and the implementations in FFB, I see the contribution of the paper better. I think it will be beneficial for the community. Therefore, I've changed my recommendation."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1706/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1706/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1706/Reviewer_hHUL"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1706/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698082538900,
            "cdate": 1698082538900,
            "tmdate": 1700135774443,
            "mdate": 1700135774443,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RFD5Yr2MQv",
                "forum": "NnyD0Rjx2B",
                "replyto": "AOynSDvpnx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1706/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1706/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank the Reviewer for their review. Though critical, we appreciate the Reviewer directly engages with the intended goal and significance of our paper.\n\n**Weaknesses**\n\n1. *On the novelty/significance*\n\nWe admit that the benefit of our formal framework was not explicitly stated in the paper; please allow us to do so now - in this rebuttal as well as in the revised version of the paper. Prior work such as by Padh et al. (2021) or Wick and Tristan (2019) on fairness regularizers only admits a few select fairness definitions and for a single, categorical sensitive feature. By formally considering widely applicable fairness definitions and focussing on regularizers that can obtain those general definitions, we can both consolidate existing work and serve as a foundation for future results that can benefit from this generality. Future regularizers need only show they strictly quantify the discrepancy between our statistics to enjoy the same wide applicability. \n\nThis is already evident in our empirical results: our fairret implementations significantly outperform existing implementations due to their modularity in how fairness is defined. The mathematical simplicity of our interface also translates to easily extensible code, as shown in the newly added Appendix E that contains a code snippet of how fairrets can easily be used in automatic differentiation and how novel fairness (linear-fractional) statistics can be defined. In addition to Appendix E, we now also more clearly state the intended benefit of our formal framework in Section 1 of the revised main paper.\n\n*On the differences with FFB and FairTorch*\n\nThe structure of both FFB and FairTorch does not lend itself to the general applicability of the fairret framework. Both these packages have their fairness regularization terms hardcoded with a select few fairness measures. In our fairret implementation, the regularization term is completely modular with respect to the fairness measure. A user can thus easily create their own fairness measure and immediately use them in the framework. Again, an example of this use can now be found in Appendix E. Hence, our framework is a generalization of the methods implemented in FFB and FairTorch. Please note we reached out to (but failed to receive a response from) one of the authors of FFB and will continue to seek collaboration, as their extensive experiments are complementary (rather than overlapping) with our work. \n\n\n2. *The use of fairness tools*\n\nWe appreciate the Reviewer\u2019s comments and streamlined the use of the term \u2018fairness tools\u2019 in order to mitigate any confusion.\n\n*Minor comments*\n\nWe thank the Reviewer for this valuable resource. We\u2019ve adjusted the use of equations in the paper according to the article\u2019s suggestions. \n\n**References**\n\nPadh, K., Antognini, D., Lejal-Glaude, E., Faltings, B., & Musat, C. (2021, December). Addressing fairness in classification with a model-agnostic multi-objective algorithm. In Uncertainty in artificial intelligence (pp. 600-609). PMLR.\n\nWick, M., & Tristan, J. B. (2019). Unlocking fairness: a trade-off revisited. Advances in neural information processing systems, 32."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1706/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700067019535,
                "cdate": 1700067019535,
                "tmdate": 1700067019535,
                "mdate": 1700067019535,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FJw1nOrGN8",
                "forum": "NnyD0Rjx2B",
                "replyto": "RFD5Yr2MQv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1706/Reviewer_hHUL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1706/Reviewer_hHUL"
                ],
                "content": {
                    "title": {
                        "value": "Re: Official Comment by Authors"
                    },
                    "comment": {
                        "value": "Dear authors,\n\nThank you for the detailed and rich response. Based on your response, I looked at your example code in Appendix E and the implementations in FFB. I concur that your framework makes it easier for other researchers to integrate new measures of fairness and regularize their networks accordingly. I find this approach positive and useful for the community. \n\nI will increase my original rating.\n\nGood luck"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1706/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700135647877,
                "cdate": 1700135647877,
                "tmdate": 1700135647877,
                "mdate": 1700135647877,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TT8A7gNDKB",
            "forum": "NnyD0Rjx2B",
            "replyto": "NnyD0Rjx2B",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1706/Reviewer_eBDd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1706/Reviewer_eBDd"
            ],
            "content": {
                "summary": {
                    "value": "The authors present a general framework for formulating fairness regularization terms which are differentiable. Their framework encompasses a wide range of group fairness notions. The authors then propose a series of regularizers for enforcing the fairness definitions. Finally, an experimental evaluation is given."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "I think the authors propose a valuable contribution to the fairness community. Specifically, I appreciate the effort the authors make on combining multiple fairness notions into a general framework. I also think that providing a python package can be valuable for research and adaptation of fairness in ML."
                },
                "weaknesses": {
                    "value": "The theoretical contribution of this work is in my opinion quite limited. While combining different fairness notions is valuable, I do not think that this is in itself a theoretical contribution. For instance, if in (3) you fix $\\overline{\\gamma}(h)=c$, then a norm based regularizer would be convex in $f(X)$. Thus if the model is linear, you would get a convex regularizer. Combine this with a convex $\\mathcal{L}_Y$ and your problem is convex in the model parameters. This would allow you to get fast convergence rates."
                },
                "questions": {
                    "value": "Some minor questions:\n- Why do you call your method \"Partition Fairness\" and not \"Group Fairness\" as is common in the literature?\n- Regarding continuous sensitive variables. It seems that this approach only captures linear correlation between the sensitive variables and the score. Is this correct?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1706/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1706/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1706/Reviewer_eBDd"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1706/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698656400663,
            "cdate": 1698656400663,
            "tmdate": 1699636098912,
            "mdate": 1699636098912,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "B8Z7hSkXQ2",
                "forum": "NnyD0Rjx2B",
                "replyto": "TT8A7gNDKB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1706/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1706/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the Reviewer for the time they invested into reviewing the paper and for providing insightful comments.\n\n**Weaknesses**\n\n*Novelty of theoretical contribution*\n\nWe agree that the significance of our paper is not found in theoretical results. Rather, we provide an algorithmic contribution by combining recent advances into a general, formal framework, for which we provide a thorough empirical analysis. The flexibility of this framework can have it serve as a foundation for future work on fairness regularization. This is clarified in the revised Section 1 of the paper. In the newly added Appendix E, we demonstrate that fairrets are easy to use in typical PyTorch settings and allow an easy extension to new linear-fractional fairness statistics. \n\n*Linear model leads to convex optimization*\n\nWe agree this would be interesting to try. A similar approach was proposed by Zafar et al. (2019), but they explicitly leave this optimization with respect to fairness definitions with non-linear statistics (like Predictive Parity) for future work. By using our $c$-fixed constraints, which construct linear constraints for linear-fractional statistics, our method directly solves this stated limitation in their work. We opted not to go this route, as the goal of our framework is to help mitigate bias in non-linear models optimized through automatic differentiation.\n\n**Questions**\n\n*The use of Partition Fairness*\n\nOur notation for sensitive values is different from most papers, as we allow for an arbitrary vector of sensitive features. Hence, we use the term \u2018partition fairness\u2019 to emphasize that the fairness definitions in our notation are only equivalent to the popular statistical fairness definitions when there is only a *single categorical* sensitive variable, i.e. one that poses a partition over the data. In our experience, the use of the term \u2018group fairness\u2019 in prior work is often vague and could also refer to equality along *multiple categorical sensitive variables*, e.g. all gender groups are treated equally and all ethnicity groups are treated equally. However, as discussed in Remark 2 (and Appendix B.2), enforcing equality for every axis of discrimination separately, neglects biases towards intersections of groups (e.g. \u2018black women\u2019 and \u2018white women\u2019) (Buolamwini et al., 2018). By formally stating the assumptions that come with partition fairness in our notation, we can clearly remark on this limitation (which is very common in the state-of-the-art).\n\n*Continuous sensitive variables*\n\nIndeed, linear-fractional statistics only measure linear effects of the sensitive variables on the numerator and denominator. For demographic parity (with the positive rate as its statistic), this is equivalent to requiring zero covariance/correlation between the sensitive variables and the output score. We show that this notation is exactly equivalent to traditional fairness metrics for a single, categorical attribute (partition fairness), but fails to capture non-linear effects for continuous sensitive variables (see also Appendix B.1 that goes into more detail). A possible extension could be to use the work of Mary et al. (2019) to estimate the joint density of the sensitive variables and the numerator and denominator of the linear-fractional statistic. By mapping these variables into a kernel space, that density becomes tractable to compute. \n\n\n**References**\n\nBuolamwini J., Gebru T., (2018). Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification. Proceedings of the 1st Conference on Fairness, Accountability and Transparency, PMLR 81:77-91\n\nMary, J., Calauzenes, C., & El Karoui, N. (2019, May). Fairness-aware learning for continuous attributes and treatments. In International Conference on Machine Learning (pp. 4382-4391). PMLR.\n\nZafar, M. B., Valera, I., Gomez-Rodriguez, M., & Gummadi, K. P. (2019). Fairness constraints: A flexible approach for fair classification. The Journal of Machine Learning Research, 20(1), 2737-2778."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1706/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700066808567,
                "cdate": 1700066808567,
                "tmdate": 1700066808567,
                "mdate": 1700066808567,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9OT5AsLe3V",
                "forum": "NnyD0Rjx2B",
                "replyto": "B8Z7hSkXQ2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1706/Reviewer_eBDd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1706/Reviewer_eBDd"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the reviewers for their comments. I would like to retain my score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1706/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700475016631,
                "cdate": 1700475016631,
                "tmdate": 1700475016631,
                "mdate": 1700475016631,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CSeaIq9KZ7",
            "forum": "NnyD0Rjx2B",
            "replyto": "NnyD0Rjx2B",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1706/Reviewer_ALkR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1706/Reviewer_ALkR"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a tool that implemented generalized group fairness metrics that can be used in automatic differentiation libraries. The fairness metrics are expressed as a linear-fractional statistic, which can be further represented as a smoothed regularization term. The authors also present an alternative projection method that penalize the divergence between models. Through extensive experiments, the authors shows the framework is lucrative for the optimization of fairness constraints."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The presented SmoothMax regularization terms are elegant and provide expressive representations for widely applied group fairness metrics.\n- The methods can be combined with automatic differentiation tools, such as PyTorch.\n- The methods can be naturally applied with multiple axes of sensitive attributes, allowing wider applications."
                },
                "weaknesses": {
                    "value": "- The method still applies relaxed fairness metrics, rather than the exact metrics as the regularization terms.\n- A superior learning objective should be a minimax game with the optimization of the $\\lambda$-player. As far as my understanding, the authors use fixed $\\lambda$ values as a hyper-parameter and run grid search to get the optimal results."
                },
                "questions": {
                    "value": "1. If the model $h$ is not a probabilistic classifier, then Equation (4) is no longer differentiable. Can the framework still be useful? \n2. How is the projection $f^\\star$ initialized in the constrained optimization problem of Equation (5)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1706/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698720560532,
            "cdate": 1698720560532,
            "tmdate": 1699636098807,
            "mdate": 1699636098807,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hcsrmPA9XZ",
                "forum": "NnyD0Rjx2B",
                "replyto": "CSeaIq9KZ7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1706/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1706/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the Reviewer for the time they invested into reviewing the paper and for providing insightful comments. Of course, we highly appreciate their positive evaluation of our paper. \n\n**Weaknesses**\n\n*On relaxed fairness notions*\n\nRemark 3 in our paper fully agrees that, because we assume classifiers to be probabilistic in Section 3, the fairrets only use relaxed versions of the discrepancy between statistics. Yet, as we argue in our response to your Question 1, this weakness is not necessarily a problem in practice, where probabilistic classifiers are common. Moreover, as discussed in Appendix B.3, the hardness of the classification (and thus the fidelity of the fairness metric) can be traded-off with the quality of the fairret\u2019s gradient by scaling the logits (Padh et al. 2021).\n\n*On finding $\\lambda$*\n\nThis is a great idea that would fit well in the envisioned framework. We share the hypothesis that a smarter $\\lambda$ that evolves during training would indeed lead to better results. In fact, we noticed that setting $\\lambda = 0$ for the first 20 epochs led to significantly more stable convergence (see Appendix D.2). We would only add that this illustrates the value of our modular approach to regularizers: the impact of the fairness adjustment is easily controlled.\n\n**Questions**\n\n1. To clarify, we refer to probabilistic classifiers as classifiers that provide a distribution of outputs to sample decisions from, in response to an input feature vector. Such classifiers have become standard in modern machine learning setups, which typically use automatic differentiation libraries. Hard classifiers, with their discontinuity along the decision boundary, are unfit for this paradigm. Hence, they fall outside the target scope of our framework. Please see Appendix B.3 for a detailed discussion on the topic. However, do note that all results in Section 2 *are* directly applicable to these non-probabilistic classifiers. In particular, future work in non-probabilistic classifiers could use Proposition 1 to obtain general, linear fairness constraints. \n\n2. We did not extensively evaluate the use of different initializations for the projection $f^*$. To make the implementation as simple as possible, we maximally benefit from the `cvxpy` library to handle the optimization. There, we saw some benefit from using a warm start (https://www.cvxpy.org/tutorial/advanced/index.html#warm-start), i.e. the previous optimization parameters are used to initialize the next optimization\u2019s parameters. We hypothesize this is because the dual variables in the convex optimization, i.e. those related to the fairness constraints, are expected to be similar from one batch to the next, since these dual variables represent the imbalance in statistics between groups.  \n\n**References**\n\nPadh, K., Antognini, D., Lejal-Glaude, E., Faltings, B., & Musat, C. (2021, December). Addressing fairness in classification with a model-agnostic multi-objective algorithm. In Uncertainty in artificial intelligence (pp. 600-609). PMLR."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1706/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700066723382,
                "cdate": 1700066723382,
                "tmdate": 1700066723382,
                "mdate": 1700066723382,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bLSnxDbwer",
                "forum": "NnyD0Rjx2B",
                "replyto": "hcsrmPA9XZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1706/Reviewer_ALkR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1706/Reviewer_ALkR"
                ],
                "content": {
                    "comment": {
                        "value": "I acknowledge that I have read the authors' response as well as the comments with other reviewers."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1706/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700091994595,
                "cdate": 1700091994595,
                "tmdate": 1700091994595,
                "mdate": 1700091994595,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]