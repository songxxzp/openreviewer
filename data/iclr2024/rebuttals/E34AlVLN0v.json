[
    {
        "title": "Parallelizing non-linear sequential models over the sequence length"
    },
    {
        "review": {
            "id": "VheTUFyeNe",
            "forum": "E34AlVLN0v",
            "replyto": "E34AlVLN0v",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1788/Reviewer_gw76"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1788/Reviewer_gw76"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes parallelizing evaluation and training of nonlinear sequential models using fixed point iteration methods. The paper proposes a method that restates nonlinear differential equations as fixed-point iterations with quadratic convergence, as in Newton's root finding method. The result is the DEER method. Speed and performance results are presented for a NeuralODE and GRU that are parallelized using DEER. Theoretical equivalences and convergence rates are established in the appendix."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The proposed DEER method is well-motivated and presented clearly\n\n- The theoretical results seem sound, though I only skimmed the proofs to follow the arguments, rather than check every detail line-by-line\n\n- The method is general and can be applicable to a host of nonlinear differential equation methods such as neural ODEs and any nonlinear RNN (e.g. LSTM, GRU, etc) of broad interest to the sequence modeling community.\n\n- The method has the advantage of theoretically having quadratic convergence. Experimental results support the claim that this method should lead to nearly equivalent results (up to numerical precision) as evaluating the nonlinear ODE sequentially\n\n- For small hidden sizes, the empirical speed ups are substantial"
                },
                "weaknesses": {
                    "value": "- The biggest weakness of the paper is the empirical results, particularly related to performance. Only two tasks are considered, a synthetic physics system where HNNs are trained and the EigenWorms task where a GRU is trained. \n  - For the EigenWorms task, the GRU does not significantly perform that much better than several of the baselines and is outperformed by others.  It is claimed that the DEER method enables faster training and thus experimentation to identify optimal GRU architectures. But is this the best the GRU could do? \n  -  It would have been interesting to see the DEER method applied to some of the other methods such as LEM or UnICORNN since they could presumably be trained with the DEER method also. Would they maintain the same performance? Could better performance be achieved with the DEER method since they could potentially be trained faster and thus be hyperparameter tuned more? An exploration of this would strengthen the paper and broaden the impact.\n  - A greater variety of tasks would increase the impact of the paper and potentially broaden the audience. E.g. there are many other common sequential \"long sequence\" tasks ranging from sequential MNIST, sequential CIFAR, to the LRA benchmarks. It would have been interesting to see how a nonlinear RNN such as GRU performed on LRA. \n\n- There are many interesting theoretical results pushed to the Appendix. The authors might potentially consider packaging some of the formal statements as propositions in the main paper, to signpost the results and guide the reader to understand the rigor behind the method as well as point to the location of the proofs more explicitly in the Appendix.\n\n- A weakness of the proposed method is the cubic complexity in the hidden size that the DEER method incurs. This limits scalability and the applicability of the method to larger scale systems. This is mentioned, but perhaps a further discussion of potential paths to address this could be interesting and helpful."
                },
                "questions": {
                    "value": "1. Can you somehow quantify the claim that the DEER method can improve the performance of the GRU since it allows more hyperparameter tuning? Perhaps report the results of a single sequential GRU run, and then show the improvement that can be achieved by hyperparameter tuning the GRU for multiple runs (allowed by the parallelization of the DEER method).\n\n2. Can you apply the DEER method to LEM and/or UniCORNN and report the results and compare the runtimes? \n\n3. Can we add more tasks? Is it possible to run a GRU or other nonlinear RNN on Long Range Arena? Or does the cubic complexity of DEER prevent running large enough models for some of these tasks? Can we at least add other common RNN benchmarks such as sequential CIFAR?  \n\n4. Can you add a PDE example (discussed in the appendix)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1788/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1788/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1788/Reviewer_gw76"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1788/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698038533698,
            "cdate": 1698038533698,
            "tmdate": 1700590433269,
            "mdate": 1700590433269,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AqMvVuFnbG",
                "forum": "E34AlVLN0v",
                "replyto": "VheTUFyeNe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1788/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1788/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer gw76"
                    },
                    "comment": {
                        "value": "Thank you for your reviews and comments. Below is our responses to your questions.\n\n> Can you somehow quantify the claim that the DEER method can improve the performance of the GRU since it allows more hyperparameter tuning? Perhaps report the results of a single sequential GRU run, and then show the improvement that can be achieved by hyperparameter tuning the GRU for multiple runs (allowed by the parallelization of the DEER method).\n\nWe ran hyperparameters tuning during the discussion period as the reviewer suggested and we can achieve the improvement for GRU from 82.1% to 88.0%, beating a more modern technique Neural RDE. We will update the results in the paper.\n\n[comment]: <> (<p style=\"color:blue\">[TODO: put in the paper ???]</p>)\n\n> Can you apply the DEER method to LEM and/or UniCORNN and report the results and compare the runtimes?\n\nWe have ran experiments using DEER on LEM. It is more challenging to reproduce their results than we thought as running the code from the official repository does not really reproduce the results on the paper. So we used DEER method to tune the hyperparameters efficiently and we can obtain similar results to their paper. With DEER, one experiment can run in less than 30 minutes, while with the code from the official repository, it took us 6 hours to run an experiment.  We will put the results in the paper.\n\n[comment]: <> (We have added the results in our paper in [???])\n\n[comment]: <> (<p style=\"color:blue\">[TODO: write the results for LEM in our paper ???]</p>)\n\n> Can we add more tasks? Is it possible to run a GRU or other nonlinear RNN on Long Range Arena? Or does the cubic complexity of DEER prevent running large enough models for some of these tasks? Can we at least add other common RNN benchmarks such as sequential CIFAR?\n\nYes, we have added a new experiment result on sequential CIFAR-10 (sCIFAR-10) using multiple head GRU with different strides. We can get around the cubic complexity of DEER by splitting the GRU into multiple heads. For example, instead of using correlated 256 states, we split them into 16 heads with 16 states each. We ran an experiment with this idea on sequential CIFAR-10 where we elaborate in the \"Response to all reviewers\" section 1 above. We will put the results in the paper.\n\n> Can you add a PDE example (discussed in the appendix)?\n\nAlthough DEER can be used for PDE, the focus of our paper is for one dimensional differential equation (i.e. ODE or discrete difference equation). Applying it for a PDE would require writing the solver of the linear equation which would take a significant amount of effort. Exploring this direction is a possible future work. For now, we put an example on how to apply DEER framework to a simple PDE case in the Appendix."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1788/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700494249096,
                "cdate": 1700494249096,
                "tmdate": 1700494249096,
                "mdate": 1700494249096,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "neDCvqFlZT",
                "forum": "E34AlVLN0v",
                "replyto": "AqMvVuFnbG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1788/Reviewer_gw76"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1788/Reviewer_gw76"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response and additional experiments. I have increased my score."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1788/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700590458734,
                "cdate": 1700590458734,
                "tmdate": 1700590458734,
                "mdate": 1700590458734,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZxyJtAOLys",
            "forum": "E34AlVLN0v",
            "replyto": "E34AlVLN0v",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1788/Reviewer_EMpw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1788/Reviewer_EMpw"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel parallelization scheme for sequential models. The method works by recasting the model as a fixed point problem and using Newton's method to solve it in parallel. Involves computing the Jacobian and Hessian of the model explicitly to enable quadratic convergence. An adjoint method is developed to compute the gradient in parallel. Experimental results are shown in speeding up RNNs and neural ODEs, showing significant speedups especially for long sequence lengths. Extensive proofs are given."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ The method clearly has large speedups in certain training regimes, particularly for long sequences and small batch sizes\n+ The theory of the method is clearly described, with proofs provided in the appendix.\n+ The proofs in the appendix are clearly described. \n+ The method is generally applicable to any sequential method, unlike previous methods which require specific architectures or structural assumptions.\n+ Convergence is quicker than previous methods which did not incorporate Jacobians\n+ Potentially, the method could also be applied at inference time to speed up sequence generation, not just at training time."
                },
                "weaknesses": {
                    "value": "+ The practical importance of this method is somewhat unclear.\n  From a practical point of view, the DEER method is a mechanism to use more memory in order to speed up the forward and backwards pass.\n  Therefore, many of the experiments, particularly figure 2, are not really a fair comparison, as it's very common to increase the batch size until the memory is fully utilized.\n  In other words, the fairer comparison would be to fix the throughput (i.e. FLOP/s or memory usage) of DEER and the sequential method the same and to compare the two methods. I think it's quite plausible that for a given throughput, using DEER to do more minibatch steps\n  with smaller batch size can result in faster convergence. However, I don't believe this argument is made in the paper. Figure 4 gets close, but I don't think this is normalized in terms of\n  throughput. \n+ The increased memory usage of DEER at higher hidden dimensions seems a big stumbling block. As far as I am aware, a hidden dimension of 8 would be considered very small and impose a limit on the expressiveness\n  of a recurrent model. This issue is not really addressed in the work."
                },
                "questions": {
                    "value": "+ Can you provide comparisons to the sequential method when the throughput of the GPU is held constant (e.g. the memory is approximately fully utilized)?\n+ How do RNNs with smaller hidden sizes compare to those with larger hidden sizes? Is it the case that the increased speed of DEER is able to offset for a reduction in hidden size that might be needed to use DEER?\n+ Would you be able to use an approximation for the Jacobian (low-rank, etc) to reduce the memory usage?\n+ How many iterations are typically required to converge? How does this compare against zero-order methods that don't use a Jacobian?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1788/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1788/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1788/Reviewer_EMpw"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1788/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698819038610,
            "cdate": 1698819038610,
            "tmdate": 1700693449681,
            "mdate": 1700693449681,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xrumUBLFhK",
                "forum": "E34AlVLN0v",
                "replyto": "ZxyJtAOLys",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1788/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1788/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer EMpw"
                    },
                    "comment": {
                        "value": "Thank you for your reviews and comments. Below is our responses to your comments and questions.\n\n> Can you provide comparisons to the sequential method when the throughput of the GPU is held constant (e.g. the memory is approximately fully utilized)?\n\nWe ran an experiment using LEM on EigenWorms to compare DEER and sequential methods. To make the memory consumption similar, we increase the batch size when running with the sequential method. As the result, although DEER method took more steps to achieve the similar results as sequential method, it was 3 times faster in terms of wall-clock time. This confirms Reviewer EMpw's hypothesis that *\"for a given throughput, using DEER to do more minibatch steps with smaller batch size can result in faster convergence\"*. We will put the results in the paper.\n\n[comment]: <> (<p style=\"color:blue\">[??? TODO: put in the paper]</p>)\n\n> How do RNNs with smaller hidden sizes compare to those with larger hidden sizes? Is it the case that the increased speed of DEER is able to offset for a reduction in hidden size that might be needed to use DEER?\n\nThe unfavourable scaling of DEER with respect to the hidden size can be avoided by having multiple \u201cheads\u201d where each head has a smaller hidden size. We ran a new experiment with this idea on sequential CIFAR-10 where we elaborate in the \"Response to all reviewers\" section 1 above.\n\n> Would you be able to use an approximation for the Jacobian (low-rank, etc) to reduce the memory usage?\n\nYes! In fact, this is the area that we would like to explore as a continuation of this work.\n\n> How many iterations are typically required to converge? How does this compare against zero-order methods that don't use a Jacobian?\n\nWith a typical untrained GRU with 16 hidden sizes, 10000 sequence length, float32, it usually takes about 4 iterations to converge (i.e. max absolute error < 1e-4). With zero-order methods, it typically takes about 23-26 for an untrained GRU. Despite it not looking very different for untrained GRU, the number of zero-order iteration could significantly increase with different settings. For example, multiplying all the parameters of untrained GRU by 5 would get the numbers to about 7 for our method and 330 - 360 for the zero order method."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1788/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700494178509,
                "cdate": 1700494178509,
                "tmdate": 1700494178509,
                "mdate": 1700494178509,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "l73oqEr4I6",
                "forum": "E34AlVLN0v",
                "replyto": "ZxyJtAOLys",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1788/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1788/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We kindly request your attention to our recent comments and updates, and would greatly appreciate your response before the end of the discussion period on Thursday, 23 November 2023 at 11:59 GMT. Thank you for your time and consideration."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1788/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700688710820,
                "cdate": 1700688710820,
                "tmdate": 1700688710820,
                "mdate": 1700688710820,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tD8Somij9q",
                "forum": "E34AlVLN0v",
                "replyto": "xrumUBLFhK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1788/Reviewer_EMpw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1788/Reviewer_EMpw"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for your response. I'm glad to see that the fairer comparisons still resulted in favourable results for DEER. \nThe idea of using multiple heads for the RNNs is an interesting one, and a nice approach to sidestep the unfavourable scaling of the DEER method with hidden size. \n\nBased on the rebuttal and responses, I'm increasing my score."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1788/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700693438055,
                "cdate": 1700693438055,
                "tmdate": 1700693438055,
                "mdate": 1700693438055,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IscNAb6uzz",
            "forum": "E34AlVLN0v",
            "replyto": "E34AlVLN0v",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1788/Reviewer_QySF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1788/Reviewer_QySF"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a method for parallel evaluation of non-linear sequence models (neural differential equations, GRUs) as a limit case of differentiable multiple shooting. The key idea is to linearize the multiple shooting update equation and use exact parallel methods (e.g., parallel scans) to evaluate each step of the root-finding procedure. The method is evaluated on time series classifiction, hamiltonian neural ODE training, and benchmarked for efficiency."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The method is clearly presented. The authors do a good job contextualizing the approach with respect to direct multiple shooting, including recent work on adapting it to Neural ODEs (Appendix A.1, A.2)\n* The efficiency evaluation is quite thorough, investigating the effect of batch size, state dimension and sequence length."
                },
                "weaknesses": {
                    "value": "* It is not clear whether this approach improves over direct multiple shooting (which is also applicable to GRUs). Is there some drawback to the linearization you introduce? Do you require more steps to converge?\n* The benchmarking is quite limited, both tasks showcased are small scale. The tasks do a good job at showing relative performance (end-to-end time) improvements, but they do not provide any insight on the method itself. Are there important hyperparameters, methods that could impact the final results and convergence? You mention mid-point in passing in Sec 3.3, could you elaborate on the choice of other interpolation methods that you have tried?"
                },
                "questions": {
                    "value": "* How is the recurrent step baseline implemented here? Since you use JAX, have you tried to otimize the recurrence (jit-compile, or fuse)?\n* Although the scaling of the proposed method in with state dimension is quite unfavourable, many existing state-of-the-art sequence models (S4, H3, Hyena, RWKV) take a specific \"multi-SISO\" (single-input, single-output) form, where each channel of the model (in width) is assigned its own state. Although these models typically use linear recurrences that can simply apply the convolution theorem or a parallel-scan, the same structure can be used in the nonlinear case. Since these models are usually trained with small state dimensions (8 - 64), this would correspond to integrating many more systems in parallel (model width times batch size, rather than batch size as in the GRU case). Can the authors commend on whether DEER would be applicable in this case (and ideally show some preliminary result on a simple task?)  \n* How many steps do you need to converge with DEER? How much higher is the latency of one DEER evaluation vs one step of a corresponding GRU?\n* Can you comment on the different rates of approximation of this linearized version of multiple shooting, and regular multiple shooting?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1788/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699044415270,
            "cdate": 1699044415270,
            "tmdate": 1699636108119,
            "mdate": 1699636108119,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SsydPKlU9Z",
                "forum": "E34AlVLN0v",
                "replyto": "IscNAb6uzz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1788/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1788/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer QySF"
                    },
                    "comment": {
                        "value": "Thank you for your review on our paper. Here are our response to your comments and questions.\n\n> It is not clear whether this approach improves over direct multiple shooting (which is also applicable to GRUs). Is there some drawback to the linearization you introduce? Do you require more steps to converge?\n\nIn regards to the linearization we refer to in the appendix, we realise that we made a mistake: the direct multiple shooting layer (MSL) equation (eq. 17 in our paper) is actually already a linear equation ($b_{i+1}^{(k+1)}$ is linear w.r.t. $b_i^{(k+1)}$), so we didn\u2019t actually do linearization over MSL because it\u2019s already linear. We apologise for the mistake. As they are both linear and can use Newton\u2019s method, the number of iterations to converge should be the same.\n\nTo show the advantage of our method over the multiple shooting layer (MSL), we ran the runtime comparisons between DEER, multiple shooting layers from torchdyn (maintained by the authors of the MSL paper: S. Massaroli, *et al.*, 2021), and our JAX implementation of MSL from torchdyn. In the comparison, we used a simple ODE case with 10,000 time points.\n\n| Method | Device | Runtime | Notes |\n|-|-|-|-|\n| **DEER** | **GPU** | **1.62 ms** | |\n| torchdyn's MSL | GPU | 12.47 s | 20 sections |\n| torchdyn's MSL | GPU | 11.80 s | 200 sections |\n| JAX MSL | GPU | 202 ms | 20 sections |\n| JAX MSL | GPU | 117 ms | 200 sections |\n| DEER | CPU | 105 ms | |\n| torchdyn's MSL | CPU | 4.59 s | 20 sections |\n| torchdyn's MSL | CPU | 3.60 s | 200 sections |\n| JAX MSL | CPU | 8.39 ms | 20 sections |\n| **JAX MSL** | **CPU** | **7.32 ms** | **200 sections** |\n\nAlthough our method is slower in CPU, DEER can take more parallelization advantage with GPU. We will put this result in the paper.\n\n> Are there important hyperparameters, methods that could impact the final results and convergence?\n\nDEER has only one hyperparameter to tune: the tolerance for convergence. However, as the method has quadratic convergence rate, the effect of the tolerance value is not significant to the number of iterations to reach the convergence.\n\n> You mention mid-point in passing in Sec 3.3, could you elaborate on the choice of other interpolation methods that you have tried?\n\nYes, we explored other kinds of interpolation methods, including: (1) left value, (2) right value, (3) linear interpolation, and (4) quadratic interpolation with Simpson\u2019s rule. Interpolation (1) and (2) gives a quadratic error ($O(\\Delta t^2)$). Interpolation (3) gives cubic error ($O(\\Delta t^3)$), but the analytical expression of equation (8) becomes complicated. Interpolation (4) gives a fifth-power error ($O(\\Delta t^5)$), but it gives a very complicated analytical expression of equation (8). We end up with midpoint interpolation because it gives reasonably low error (cubic error) with a very simple expression of equation (8). We will put the details on the interpolations in the paper.\n\n> How is the recurrent step baseline implemented here?\n\nYes, we used jax.lax.scan and jit-compile (which I believe includes operator fusion) for the sequential method.\n\n> Although the scaling of the proposed method in with state dimension is quite unfavourable, many existing state-of-the-art sequence models (S4, H3, Hyena, RWKV) take a specific \"multi-SISO\" (single-input, single-output) form, where each channel of the model (in width) is assigned its own state. ... Can the authors commend on whether DEER would be applicable in this case (and ideally show some preliminary result on a simple task?)\n\nWe agree with the reviewer\u2019s suggestion on having multiple independent states. We can group the channel dimension into several \u201cheads\u201d, thus avoiding the cubic scaling for DEER. We ran an experiment with this idea on sequential CIFAR-10 where we elaborate in the \"Response to all reviewers\" section 1 above.\n\n> How many steps do you need to converge with DEER? How much higher is the latency of one DEER evaluation vs one step of a corresponding GRU?\n\nWith a typical untrained GRU, it usually takes about 4 iterations, but during the training it can take more steps to converge. For a GRU of input and hidden sizes of 16, batch size 16, sequence length 10K, a DEER iteration typically takes 5.4 ms, while one step of GRU (from one sequence point to another sequence point) takes 22 \u03bcs. Although one DEER iteration takes much longer than one sequential step, the number of required iterations is usually much less than the number of sequence steps.\n\n> Can you comment on the different rates of approximation of this linearized version of multiple shooting, and regular multiple shooting?\n\nThis is related to our earlier comment regarding linearization. We made a mistake by saying that we do linearization over direct multiple shooting Newton\u2019s method update step, while the direct multiple shooting update step is actually already linear."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1788/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700494099547,
                "cdate": 1700494099547,
                "tmdate": 1700494099547,
                "mdate": 1700494099547,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nAzZDXI97Y",
                "forum": "E34AlVLN0v",
                "replyto": "IscNAb6uzz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1788/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1788/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We kindly request your attention to our recent comments and updates, and would greatly appreciate your response before the end of the discussion period on Thursday, 23 November 2023 at 11:59 GMT. Thank you for your time and consideration."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1788/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700688694191,
                "cdate": 1700688694191,
                "tmdate": 1700688694191,
                "mdate": 1700688694191,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]