[
    {
        "title": "Debias your VLM with Counterfactuals: A Unified Approach"
    },
    {
        "review": {
            "id": "4c54n3CTvN",
            "forum": "xx05gm7oQw",
            "replyto": "xx05gm7oQw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6651/Reviewer_nQv5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6651/Reviewer_nQv5"
            ],
            "content": {
                "summary": {
                    "value": "Vision-language models (VLMs) have achieved impressive performance on various tasks but have been shown to exhibit biases due to biased training data. In this study, the authors propose a simple debiasing framework, counterfactual vision-language debiasing\n(CVLD), that aims to quantify and mitigate biases in vision-language models. CVLD introduces a causal intervention module to generate counterfactual image-text pairs and use causal fairness metrics to measure the difference in model predictions between original and counterfactual distributions. The authors also propose bias-free adaptation techniques to minimize bias in pre-trained models, achieving promising results in image classification, retrieval, and captioning tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper provides a robust framework that scales to different visual-language downstream tasks like image classification, image retrieval, and image captioning tasks.\n\n2. The proficiency of CVLD is demonstrated in a set of fine-tuning experiments across different tasks using well-established fairness measures.\n\n3. The paper is well-written and details the objectives and results for each of the downstream tasks separately."
                },
                "weaknesses": {
                    "value": "1. One of the primary weaknesses of the paper is its novelty in terms of the main framework. In order to infuse fairness, Agarwal et al. [1] introduced a triplet-based objective that maximizes the agreement between the original graph and its counterfactual views. Given that CVLD follows suit and incorporates a similar framework, the novelty is limited.\n\n2. In most cases, the counterfactual image seems noisy and is non-reflective of the counterfactual protected attribute (e.g., in Fig. 3, we don't observe women riding the rowboat). In such cases, are the counterfactual just some noisy version of the original image? How do we attribute the debiasing to a protected attribute if the quality of the counterfactuals is not good?\n\n3. The framework is a data-extensive approach, i.e., for debiasing, it needs a counterfactual version of each image-text pair and expensive fine-tuning of VLMs for debiasing.\n\n\n**References**\n\n1. Agarwal, C., Lakkaraju, H. and Zitnik, M. Towards a unified framework for fair and stable graph representation learning. In UAI, 2021."
                },
                "questions": {
                    "value": "Please see the weaknesses for more details."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6651/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698517702363,
            "cdate": 1698517702363,
            "tmdate": 1699636760389,
            "mdate": 1699636760389,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xvh4lrLvqP",
                "forum": "xx05gm7oQw",
                "replyto": "4c54n3CTvN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6651/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6651/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your insightful review and suggestions. Please refer to the general response for issues not discussed below.\n\n---\n\n## Novelty in terms of the main framework.\nPlease see point \u201c**Novelty**\u201d in general comments. We appreciate the additional reference of NIFTY (Agarwal et al., 2021) which is conceptually similar and will acknowledge the work in the updated version, although we argue that the scope of the two works differ substantially: while NIFTY aims to fair and robust GNNs with a *known* underlying causal graph, our work involves vision-language transformer models on raw images and text as inputs, with the underlying graph *unknown* and simulated by image-text editing models. The primary contribution of CVLD is that it demonstrates the effectiveness of image-text editing models in debiasing VLMs across multiple downstream tasks with no architectural changes, which has not been demonstrated in prior work to our knowledge.\n\n## Counterfactual image seems noisy.\nRefer to \u201c**Quality of counterfactual editing**\u201d in general response. More examples of intervened images are provided in Fig. 7 and 8, highlighting the capabilities of editing models. Quantitative measures of edit quality are also included. We agree that the noise and imperfections in the counterfactual edits are unavoidable, but would like to emphasize that CVLD has shown its effectiveness for improving model fairness on standard metrics across classification, retrieval and captioning. Such improvements cannot be achieved simply by adding random noise to training images.\n\n## Data-extensive approach.\nSee point \u201c**Computational cost/overhead of CVLD**\u201d in general response. It should also be noted that the experiments are deliberately conducted in a low-resource regime where we considered few-shot classification and fine-tuned retrieval/captioning on as low as 1% of COCO, which greatly reduces the cost from adapting to the full training set. CVLD is found to be effective across different scales of training set size."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6651/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700588072879,
                "cdate": 1700588072879,
                "tmdate": 1700588072879,
                "mdate": 1700588072879,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xVzteju5Ux",
            "forum": "xx05gm7oQw",
            "replyto": "xx05gm7oQw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6651/Reviewer_KRWy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6651/Reviewer_KRWy"
            ],
            "content": {
                "summary": {
                    "value": "This work considers the problem of bias mitigation on vision-language models (VLMs). The authors introduced Counterfactual vision-language debiasing (CVLD), a technique that can be summarized in two main contributions: 1- a data generation pipeline based on off-the-shelf generative models to create counterfactual augmentations from real data; 2- a bias mitigation strategy based on fine-tuning a VLM using the generate counterfactuals. The authors evaluate the proposed approach empirically on 3 tasks: image classification, image-text retrieval, and image captioning. Experiments are carried out using a model from the BLIP family and considering both fairness and task performance metrics. Overall, results show that the proposed yields the best trade-off between improving fairness (i.e. mitigating biases) and attaining good performance in the task."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper tackles a critical problem and very relevant open research question: how to mitigate bias on foundation models;\n\n- The manuscript is overall well-written and most sections are easy to follow; \n\n- The counterfactual augmentation approach is grounded in formal definitions from the counterfactual fairness literature;\n\n- The experimental evaluation is extensive in the number of considered tasks."
                },
                "weaknesses": {
                    "value": "- One of the central claims of the work is that the proposed approach is a unified way to mitigate bias in VLMs across multiple tasks, as per the following evidence: \n  - In the title (Debias your VLM with Counterfactuals: A **Unified** Approach, bold text by myself). \n  - Also throughout text: e.g. In Section 2 \"[...] we focus on a task-agnostic fairness framework for VLMs, unifying the study of bias across different tasks and domains.\" ). \n\n   Claiming that the proposed approach is unified and task-agnostic seemed reasonable until Section 4. However, after reading through the details of how fine-tuning with synthetic data should be carried out for the three considered tasks, it seems to me that CVLD practical instantiation takes a very different format from task to task, rendering it a specific and not-unified framework for debiasing.\n\n\n\n- One of the key parts of the introduced approach is the counterfactual data generation. However, the authors did not mention at any part of the manuscript details of the evaluation of the data generation pipeline. Moreover, it is not clear how the quality of generated counterfactuals could affect the performance of CVLD. Moreover, other fine-grained aspects such as *how much synthetic data is needed* and how the number of synthetic samples used at training time affects performance were not addressed in the manuscript, making it difficult to judge to what extent this framework would generalize to other scenarios where it might be difficult to generate high quality counterfactuals.  \n\n- Some parts of the text do not seem to reflect the actual insights that can be extracted from the results. For example, in the introduction, the authors mentioned that CVLD \"demonstrates striking effectiveness for the most studied problems in the bias literature\" while it is not clear from the results that the CVLD demonstrates **striking** effectiveness, neither there are references to support the statement that the considered problems in this work are the most studied ones in the bias literature.\n\n- The experimental results are a bit confusing and hard to parse. The employed metrics in the evaluation are not standard in the literature and it is not clear whether an improvement is observed when a metric increases or decreases. Moreover, it is not clear why some methods were grouped in different parts of the tables (e.g. in Table 1 it is not clear why both CVLDs are in different sections from BLIP-PT and the ResNet-50, aren't they directly comparable?). On a similar note, it is not clear how the different bolded numbers in the tables represent and how they should be compared against each other."
                },
                "questions": {
                    "value": "- How can all the three different approaches to fine-tune VLMs with counterfactual data be seen as a unified framework? Also, how would this generalize to other tasks such as, for example, counting and object detection?\n\n- How is CVLD performance affected by counterfactual data generation quality? How did the authors assess the quality of generated data in order to know whether it was \"good enough\" to be employed for bias mitigation?\n\n- How computationally expensive is the data generation approach? How does it compare to techniques that do not rely on data generation for bias mitigation?\n\n- How is the performance of CVLD affected by the choice of the lambda hyperparameter?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6651/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6651/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6651/Reviewer_KRWy"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6651/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698758223263,
            "cdate": 1698758223263,
            "tmdate": 1699636760260,
            "mdate": 1699636760260,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7Yfmv3tcui",
                "forum": "xx05gm7oQw",
                "replyto": "xVzteju5Ux",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6651/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6651/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your insightful review and suggestions. Please refer to the general response for issues not discussed below.\n\n---\n\n## Unified approach.\nWe acknowledge that the framing of CVLD as \u201cuniversal\u201d and \u201ctask-agnostic\u201d may have been misleading, since they refer to the data debiasing process, while the subsequent model adaptation still involves \u201ca simple regularized training objective and minimal modifications to the adaptation pipeline\u201d for each task, as mentioned in the Intro section. However, in defense of the title of the paper, we argue that CVLD, as a data-centric approach, has the distinctive advantage over model-centric mitigation methods in that it greatly reduces the effort to tailor the training pipeline to downstream tasks. The adaptation procedures in Sec. 4 can be distilled into two modes: For classification and captioning, the training is merely augmented by counterfactual examples with a weight parameter ($\\lambda$), with no change to model computation at all. For retrieval, the contrastive loss itself is modified to account for counterfactual candidates in each batch, which reduces to simply modifying the optimization targets during training. The internals of the VLM is again unchanged, as illustrated in Fig. 3, meaning no overhead in deployment over a vanilla VLM (more discussion in \u201cComputational cost/overhead of CVLD\u201d in general comments). In contrast, model debiasing approaches tend to utilize additional modules, multiple training stages, and/or sophisticated loss functions, making them less trivial to adapt to a wide range of tasks. While there was not enough time to conduct the experiments, we believe that edited datasets produced by CVLD may also benefit tasks like detection/segmentation on COCO, as we find the edits to preserve the scene layout and background objects with high probability (Fig. 7 in appendix). In general, we believe that while not truly universal, CVLD is a significant step forward in minimizing the efforts needed to debias foundation VLMs on different downstream tasks.\n\n## Evaluation of the data generation pipeline & relation to debiasing performance.\nRefer \u201c**Quality of counterfactual editing**\u201d in general comments. We appreciate the suggestion to include additional evaluation metrics for the counterfactual intervention. It is difficult to conclude how the edit quality metrics relate to the eventual debiasing performance, as the metrics rely on pre-trained CLIP which itself may be biased.\nHowever, the current version of CVLD performs sample selection based on visual similarity to the input image, and we observed in preliminary experiments that this strategy produces slightly better debiasing results than uniformly sampling edits at random, suggesting that poor quality images deviating from the input data may be detrimental.\n\nIn general, we believe that the ability of CVLD is bounded by the capabilities of the editing models, and that future study may be needed to establish a definitive link between edit quality and debiasing performance. However, as demonstrated in \u201cEvaluation beyond gender bias\u201d in general comments, we observe positive sign that the existing editing models are powerful enough for many commonly studied forms of bias in vision-language.\n\n## Experimental results.\nWe would like to clarify that all evaluation datasets and metrics in this work are well-established standards in the literature. These include DEO for classification on CelebA (Ramaswamy et al., 2021), Bias and MaxSkew for retrieval on FairFace (Berg et al., 2022), BiasAmp and LIC for captioning on COCO (Hirota et al., 2022). For all fairness metrics lower is better, as is now indicated in the main tables of the paper. The fact that CVLD not only reduces model bias by large margins without sacrificing in-distribution performance, but also achieves so through simple image-text editing and no task-specific model debiasing, is particularly striking to us. \n\nIn Tab. 1, we showed results of CVLD in both 16- and 256-shot settings, and compared them to naive BLIP using the same number of shots. In the first section of the table, BLIP-PT is not adapted to CelebA dataset while BLIP and RN-50 are fine-tuned on the full dataset, making them not directly comparable to the few-shot results below. Best results of each section of the table are highlighted in bold.\n\n## Computational cost of data generation.\nSee \u201c**Computational cost/overhead of CVLD**\u201d in general response."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6651/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700587970853,
                "cdate": 1700587970853,
                "tmdate": 1700587970853,
                "mdate": 1700587970853,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "r34K75rPbf",
            "forum": "xx05gm7oQw",
            "replyto": "xx05gm7oQw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6651/Reviewer_Kjn2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6651/Reviewer_Kjn2"
            ],
            "content": {
                "summary": {
                    "value": "This paper attempts to unify the study of biases across vision-language problems. It attempts to create counterfactuals to swap the gender in an image-text pair using readily available tools like LLMs and image-editing methodologies. With the help of the generated counterfactuals, it is shown that the model performances improve for multiple tasks like image retrieval, image classification and image captioning."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The use of LLMs and other models like Instruct Pix2Pix is smart to generate counterfactuals.\n2. The authors adopt different ways to incorporate these counterfactuals into multiple downstream tasks as it is not always possible to alter the pretraining itself.\n2. Using existing VLMs on top of the original datasets along with the counterfactuals seem to help reducing the bias while also maintaining model performance."
                },
                "weaknesses": {
                    "value": "1. Lack of novelty: The paper simply uses some state-of-the-art LLM to generate counterfactual text, and null text inversion/InstructPix2Pix to generate the counterfactual images.\n2. The paper only covers gender biases - no experiments on other biases like racial/age. Biases may exist even in non-social cases (like the water-land bias in the popular Waterbirds dataset). This has not been explored.\n3. No comparison with other debiasing VLM methods ([1], [2]).\n4. The paper advocates generating counterfactuals for bias mitigation. However, not many sample examples are shown even in the supplementary.\n5. Not all biases (for example, models are seen to learn various spurious correlations like camels can only be present in deserts, airplanes can only be in the sky, etc) are quantifiable like gender. Is generating counterfactuals the solutions for those kinds of biases too?\n\n[1] Zhu et al., Debiased Fine-Tuning for Vision-Language Models by Prompt Regularization, AAAI 2023\n[2] Chuang et al., Debiasing Vision-Language Models via Biased Prompts, arxiv 2023"
                },
                "questions": {
                    "value": "1. The method of generating counterfactuals does not generate diverse images, but only modifies the existing images. Generating diverse images can help the models further. Can this be addressed?\n2. What if multiple biases are present at once? Like gender and race together. Can this method of generating counterfactuals handle such scenarios?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6651/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6651/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6651/Reviewer_Kjn2"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6651/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698760426093,
            "cdate": 1698760426093,
            "tmdate": 1699636760139,
            "mdate": 1699636760139,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0jr4mDelGT",
                "forum": "xx05gm7oQw",
                "replyto": "r34K75rPbf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6651/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6651/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your insightful review and suggestions. Please refer to the general response for issues not discussed below.\n\n---\n\n## Only covers gender biases; unquantifiable biases & multiple biases.\nPlease see \u201c**Evaluation beyond gender bias**\u201d in general response. Additional results on *background* bias on Waterbirds and *racial* bias on COCO are included, demonstrating that CVLD is generally applicable to various forms of biases. Our understanding is that CVLD is potentially useful for all settings where 1) bias can be described in natural language, and 2) corresponding causal interventions (e.g., change gender/race of subjects, or replace the background) can be reliably performed by the image-text editing models.\n\nSimultaneous mitigation of multiple biases is a great suggestion. One simple way is to extend CVLD with multiple edits, one for each protected attribute, which presents a challenge to the compositionality of editing models. However, as we intend for CVLD to become a baseline in data-centric VLM debiasing, we believe that the study of multiple bias attributes lies beyond the scope of this work and should be left for follow-up research.\n\n## Lack of novelty; comparison to other methods.\nPlease see point \u201c**Novelty**\u201d in general comments. CVLD showcases the effectiveness of image-text editing models in debiasing VLMs across multiple downstream tasks with no architectural changes, which has not been demonstrated in prior work. We consider the simplicity of the proposed framework a strength, as it does not require tweaking the internal model architecture or sophisticated training procedures, making the method more practical for tuning and deploying models at scale. It also makes the method complementary to model debiasing approaches, which may further benefit from the availability of debiased data.\n\n## Qualitative results.\nAs mentioned in \u201c**Quality of counterfactual editing**\u201d in general response, we have added qualitative results of generated images to the appendix of the paper, as well as a set of quantitative evaluation for edit quality.\n\n## Generating diverse examples.\nThis is a great point. While the editing models are not deterministic in their output, text-to-image generation certainly may improve the diversity of the images, albeit at a cost of lower accuracy. Using the CelebA dataset we studied the potential of generating examples of target attributes (e.g., \u201ca person with blond hair\u201d) with Stable Diffusion, instead of editing existing images. This has the benefit of scaling easily to larger numbers of examples in theory, yet in our experiments, the trained models on generated samples (BLIP-SD) underperform those trained with real/edited data substantially, as shown in the table. The gap only widens after increasing the number of shots from 16 to 256. This can be explained by a larger domain gap between generated images and real images, compared to a much smaller gap with editing models. \n\n| Model            | DEO \u2193 | \u0394Acc \u2193 | AP \u2191 | mAcc \u2191 |\n| ---------------- | ----- | ------ | ---- | ------ |\n| BLIP 256-shot    | 16.0  | 14.2   | 88.7 | 82.8   |\n| &nbsp;&nbsp; \\+ CVLD          | 10.2  | 11.2   | 89.4 | 83.9   |\n| BLIP-SD 256-shot | 23.9  | 39.7   | 78.3 | 65.5   |\n| &nbsp;&nbsp; \\+ Unbiased      | 15.1  | 25.8   | 78.3 | 68.5   |\n| BLIP 16-shot     | 20.1  | 20.7   | 82.3 | 76.5   |\n| &nbsp;&nbsp; \\+ CVLD          | 15.6  | 18.2   | 82.2 | 76.5   |\n| BLIP-SD 16-shot  | 16.8  | 27.8   | 77.8 | 65.1   |\n| &nbsp;&nbsp; \\+ Unbiased      | 11.5  | 31.0   | 77.6 | 67.4   |\n\nAnecdotally, we also found the diffusion models to struggle on certain attribute combinations of CelebA, such as men wearing earrings. Editing models on the other hand produce more accurate results by visual inspection. Further study may be needed to understand the trade-off between accuracy and diversity of synthetic images."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6651/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700587879409,
                "cdate": 1700587879409,
                "tmdate": 1700587879409,
                "mdate": 1700587879409,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iaRy7VmYee",
            "forum": "xx05gm7oQw",
            "replyto": "xx05gm7oQw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6651/Reviewer_sFrN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6651/Reviewer_sFrN"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a simple framework to debias vision-language models. First, one generates a text prompt for the target image that can guide the image editing procedure. Second, one generates a counterfactual image from the text prompt that has been edited by flipping the bias-related word (e.g., boy -> girl). Third, one fine-tunes the target VLM with the generated counterfactual images. The empirical results show that this method can be used to mitigate the gender bias of many VLMs."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- **Soundness.** The proposed framework is very reasonably designed; it makes perfect sense that such a method will work, given access to well-performing LLMs and text-based image editors.\n\n- **Novelty.** As far as I know, the method CVLD is novel.\n\n- **Significance of the topic.** VLMs are now one of the core backbones of most machine learning applications, and thus having a safety guarantee on such foundation models is a very important yet understudied topic.\n\n- **Writing.** The paper is clearly written and easy to read, despite having many typos."
                },
                "weaknesses": {
                    "value": "- **Limited Empirical Evaluation.** The proposed method has been evaluated almost exclusively on a specific type of bias---the gender bias. This is a very severe limitation for a paper which frames itself as targeting general bias in VLMs; the paper exemplifies racial bias multiple times in the text. If the authors are exclusively targeting the gender bias, a significant portion of this paper should be re-written to clarify this point.\n\n- **Relies on external models, which may be prone to other types of bias.** The debiasing procedure of this paper relies on the generative/editing capabilities of existing models (e.g., prompt-to-prompt editing). This is a vulnerability in terms of a bias, because such edited images may be prone to other types of biases that may be difficult to detect (see, e.g., Bias-to-Text by Kim et al. (2023)). I wonder if authors could demonstrate any \"robustness\" of the proposed paradigm to the potential biases hidden in the LLMs or prompts.\n\n- **(minor) Clarity.** Figure 1 is not very informative and difficult to parse what the figure is trying to say. What the sketch part is trying to say is unclear (perhaps more details in the caption will be better). Also, it took me some time to notice that \"M -> F\" means male -> female. The \"lock\" figures are somewhat difficult to tell whether they are locked or unlocked (maybe use frozen <-> fire analogy, like many other papers, or use additional color cues?)."
                },
                "questions": {
                    "value": "Please see the \"weaknesses\" section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6651/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699119066005,
            "cdate": 1699119066005,
            "tmdate": 1699636760018,
            "mdate": 1699636760018,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "29y6Xvwndh",
                "forum": "xx05gm7oQw",
                "replyto": "iaRy7VmYee",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6651/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6651/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your insightful review and suggestions. Please refer to the general response for issues not discussed below.\n\n---\n\n## Limited empirical evaluation.\nPlease see \u201c**Evaluation beyond gender bias**\u201d in general response. In short, while we experimented primarily with gender bias, CVLD is by no means exclusive to gender editing, and is as capable as the ability of the image-text editing models to alter protected attributes under text prompt. We show additional results on 1) background bias on Waterbirds, and 2) racial bias on COCO, to demonstrate the potential of the method.\n\n## Bias from editing models.\nThanks for the insightful suggestion. As discussed in \u201c**Quality of counterfactual editing**\u201d in general comments, it is true that the external editing models may be biased themselves, although very few studies have attempted to study this bias in a quantitative manner. Therefore, we resort to standard fairness benchmarks in classification, retrieval and captioning in the main paper for evaluating the debiasing quality. The fact that CVLD reduces model bias across dataset and tasks can be seen as strong evidence that the potential bias of external models is not enough to offset their effectiveness to remove spurious bias in the training data.\n\nAlthough it is difficult to directly study the robustness of CVLD to the bias of LLMs/diffusion models, there are steps taken to reduce the potential impact. For example, by sampling multiple edits per image and keeping those most similar to the original, we minimize the chance of inadvertently altering image regions not causally related to the protected attribute (e.g., if some edits cause the color of the clothes or background objects to be changed while others preserve the original colors, the latter would be accepted).\n\n## Improve clarity of figures.\nThanks for the suggestion. We have revised Fig. 1 and 3 to improve the clarity of modules and editing operations. We will continue to improve the readability of the paper."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6651/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700587708112,
                "cdate": 1700587708112,
                "tmdate": 1700587708112,
                "mdate": 1700587708112,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]