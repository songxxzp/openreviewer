[
    {
        "title": "TabR: Tabular Deep Learning Meets Nearest Neighbors in 2023"
    },
    {
        "review": {
            "id": "yVdQ7kKCcl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9502/Reviewer_EnVq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9502/Reviewer_EnVq"
            ],
            "forum": "rhgIgTSSxW",
            "replyto": "rhgIgTSSxW",
            "content": {
                "summary": {
                    "value": "The authors meticulously designed a supervised deep learning model for tabular data prediction, which operates in a retrieval-like manner. It outperformed tree-based models on middle-scale datasets, as well as other retrieval-based deep learning tabular learning models. To achieve this, they introduced a k-Nearest-Neighbors-like idea in model design."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- As emphasized by the authors, their method has managed to outperform tree based models like xgboost on middle-scale datasets.\n\n- Overall, the presentation is clear, and the experiments are comprehensive. The details are clear and the model is highly reproducible.\n\n- This model is the best-performing retrieval based model."
                },
                "weaknesses": {
                    "value": "- The motivations behind the module designs are not entirely clear. It appears that the authors made meticulous module (equation) optimization based on its performance on some datasets empirically. Then: \n\n(1) Why does employing the L2 distance, instand of the dot product, lead to improved performance (as shown in Eq. 3)? \n\n(2) Why is the T function required to use LinearWithoutBias? \n\n(3) We are uncertain about the robustness of the designed modules. If the dataset characteristics are changed, is it likely that the performance rankings will change significantly? The performances only on middle-sized datasets cannot show the robustness.\n\n...\n\nI suggest that providing a theoretical analysis or intuitive motivation would enhance the reader's understanding of those details.\n\n- Some sota DL approaches are not compared, such as T2G-Former (an improved version of FTT)[1], TabPFN [2], and TANGOS [3]. Especially, TabFPN is relatively similar to TabR. These papers are current SOTA, and may outperforms tree based models.\n\n[1] T2G-Former: Organizing tabular features into relation graphs promotes heterogeneous feature interaction\n\n[2] TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second\n\n[3] TANGOS: Regularizing Tabular Neural Networks through Gradient Orthogonalization and Specialization\n\n- The major comparison lies among middle-scale datasets, accompanied with some results on few other datasets shown in Table 3. In scenarios involving sparse, medium, and dense data-distributed datasets (which typically occur in small, medium-sized, and large-sized datasets, respectively), I suppose that there exists a variance in the nearest neighbor retrieval pattern. Hence, conducting tests solely on medium-sized datasets may not suffice. Furthermore, the issue of inefficiency when dealing with large-scaled datasets appears to have hindered the authors from proving the method's effectiveness in large-scaled datasets.\n\n- The method proposed by the authors appears to have achieved slight performance advantages on certain datasets (although some SOTA are not compared). However, due to the lacks of explanation for the model details that are designed empirically, it seems unnecessary and risky to apply this method in real-world scenarios (for example, it's unclear whether L2 distance may fail when uninformative features are present; or, for instance, when a table has a feature with values [f_1, f_2, f_3, ..., f_n], and we take the logarithm of these values [log f_1, log f_2, log f_3, ..., log f_n] or their reciprocals, the method may perform poorly in such cases)."
                },
                "questions": {
                    "value": "- In Section 3.1, you mentioned \"continuous (i.e., continuous) features.\" Could this be a typographical error?\n\n- I am curious if the L2 design is sensitive to uninformative features? You can offer some analysis or conduct experiments by adding some gaussian noise columns (uninformative features are commonly seen in tabular datasets) and observe the change of performances. Some transformation like logarithm may impact the results.\n\n- Some questions in weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9502/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697437452011,
            "cdate": 1697437452011,
            "tmdate": 1699637193978,
            "mdate": 1699637193978,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1OMXgtFdYB",
                "forum": "rhgIgTSSxW",
                "replyto": "yVdQ7kKCcl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9502/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9502/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal (part 1/N)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the detailed review. Below, we address all the raised concerns and questions.\n\n> Some sota DL approaches are not compared ... T2G-Former ... TabPFN ... TANGOS ... These papers are current SOTA, and may outperforms tree based models.\n\nOur reply to this comment consists of three paragraphs.\n\n**First**, using the benchmark from `[1]` (43 classification and regression tasks; used in Figure 1 and Table 5 in our submission) we show that:\n1. **Our model TabR significantly outperforms all the suggested baselines**.\n2. **The baselines used in our paper are representative**.\n\nThe results:\n\n| Model `[citation]`     | Average rank (+- std) | Wins/Ties/Losses vs TabR | Not applicable |\n| ---------------------- | --------------------- | ------------------------ | -------------- |\n| **Used in our paper:** |                       |                          |               |\n| TabR `[ours]`          | 1.36 (\u00b1 0.65)        | 0/43/0                   | 0              |\n| XGBoost `[11]`         | 1.93 (\u00b1 0.94)        | 7/13/23                     | 0              |\n| MLP-PLR `[2]`          | 1.95 (\u00b1 0.92)        | 5/19/19                  | 0              |\n| FT-Transformer `[4]`   | 2.45 (\u00b1 1.20)        | 2/14/27                  | 0              |\n| MLP                    | 3.07 (\u00b1 1.55)        | 2/12/29                  | 0              |\n| **Additional:**        |                       |                          |               |\n| T2G-Former `[10]`      | 2.76 (\u00b1 1.56)        |  3/14/23                | 3           |\n| MLP + TANGOS `[9]`     | 3.29 (\u00b1 1.69)        | 2/13/28                | 0           |\n| TabPFN `[8]`           | 4.38 (\u00b1 1.98)        | 0/4/5                | 34           |\n\nTechnical details:\n- \"Not applicable\" (the righmost column) means that the method is fundamentally not applicable to a given task or it does not fit into A100-80GB or (the case of T2G-Former) the hyperparameter tuning did not finish on time.\n- If a method is not applicable to a task, it is assigned the largest rank.\n\n**Second**, a closer look at the suggested baselines reveals things that are not consistent with being SOTA *in the scope and niche of our paper*:\n- TANGOS\n    - **Different niche:** TANGOS is not a model, it is a regularization method, which is completely orthogonal to our niche (model).\n    - **Different scope**: the original work mostly focuses on small data with only two datasets having more than 1000 objects.\n    - **Significant limitations:** the method seems to make training order(s) of magnitude slower (40x times slower on average on our datasets).\n- TabPFN\n    - **Different scope:** the work fully focuses on small (<1000 objects) classification tasks.\n    - **Significant limitations:** not applicable to regression tasks, not applicable to more than 10 classes and more than 100 features, high memory consumption.\n- T2G-Former\n    - **Already tested** in its paper on many of the datasets used in our paper and in `[2]` (where embeddings for continuous features and, in particular, MLP-PLR were introduced). The results from `[10]` and `[2]` indicate that models from `[2]` perform at least on par (and often better) while being more efficient. And, the above table confirms that MLP-PLR is indeed a good baseline, while T2G-Former did not generalize well to the new benchmark.\n\n**Third**, in the new PDF, in Section 2 Paragraph \"Tabular deep learning\", we improved the communication of the niche of our work and, in particular, cited TANGOS and TabPFN."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9502/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700326209091,
                "cdate": 1700326209091,
                "tmdate": 1700326209091,
                "mdate": 1700326209091,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HI8YZsTla0",
                "forum": "rhgIgTSSxW",
                "replyto": "yVdQ7kKCcl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9502/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9502/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal (part 2/N)"
                    },
                    "comment": {
                        "value": "> **[REGARDING THE BENCHMARK AND DATASET SIZES]**\n>\n> If the dataset characteristics are changed, is it likely that the performance rankings will change significantly?  \n> ...  \n> The performances only on middle-sized datasets cannot show the robustness  \n> ...  \n> conducting tests solely on medium-sized datasets may not suffice  \n> ...  \n> **FEW** other datasets\n\nOur understanding is that the above quoted comments express uncertainty about our benchmark and whether it is comprehensive enough to support our story.\n\nWe did our best to build a **comprehensive, challenging and representative benchmark** to support our claims. Our reply consists of several points.\n\n**First**, let us show that **our experimental setup is competitive** by comparing it to other works on tabular DL:\n\n| Model `[citation]`   | The number of datasets used in the paper                                | Max. train set size               |\n| -------------------- | ----------------------------------------------------------------------- |:------------------------------- |\n| **TabR** `[ours]`    | **~50 datasets (roughly, `[1]` + some datasets from `[2]` and `[3]`)**  | **2.9M** (+ 10M in the rebuttal) |\n| NODE `[7]`           | 6 datasets                                                              | 10M                            |\n| TabNet  `[5]`        | <10 datasets                                                            | 10M                            |\n| FT-Transformer `[4]` | 11 datasets                                                             | 723K                            |\n| SAINT `[6]`          | 30 datasets                                                             | 321K                            |\n| \\<survey\\> `[12]`    | 5 datasets                                                              | 10M                            |\n| MLP-PLR `[2]`        | 11 datasets                                                             | 723K                            |\n| TabPFN `[8]`         | 67 small datasets (different scope)                                     | 1K                            |\n| \\<benchmark\\> `[1]`  | <50 *unique* datasets (>50 with multiple versions of the same datasets) | 50K                            |\n| TANGOS `[9]`         | 20 datasets                                                             | <100K                            |\n| T2G-FORMER `[10]`    | 12 datasets                                                             | 320K                            |\n\n**Second,** to illustrate that **our study is based on many diverse datasets**, we provide the structure of our benchmark:\n- The \"default\" benchmark based on prior work (mainly `[2]`):\n    - 11 datasets (binary classification, multiclass classification and regression)\n    - from 6.4K to 723K training objects\n    - from 8 to 136 features\n- The widely known and cited benchmark from `[1]`:\n    - 43 datasets (binary classification and regression)\n    - from 1.7K to 50K training objects\n    - from 3 to 613 features\n- One weather prediction regression task from `[3]`: 2.9M training objects and 119 features.\n\n**Third, crucially**, to make the benchmark challenging for TabR, we ensured that **there are many challenging tasks that favour GBDT over prior neural networks.** In particular:\n- On the benchmark from `[1]`, GBDT was shown to be superior to neural networks.\n- `[2]` also contains multiple GBDT-friendly tasks.\n- The big weather prediction dataset `[3]` favors GBDT over simple neural networks.\n\n**To sum up**, we truly hope that the above points demonstrate why we consider our benchmark to be of reasonable size, diversity and complexity to support the claims that we make in the abstract and in the list of contribution in Section 1."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9502/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700326254426,
                "cdate": 1700326254426,
                "tmdate": 1700326254426,
                "mdate": 1700326254426,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8U9OS7M74l",
                "forum": "rhgIgTSSxW",
                "replyto": "yVdQ7kKCcl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9502/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9502/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal (part 3/N)"
                    },
                    "comment": {
                        "value": "> Furthermore, the issue of inefficiency when dealing with large-scaled datasets appears to have hindered the authors from proving the method's effectiveness in large-scaled datasets.\n\nHowever, **in the paper, we used datasets with up to three million training objects.**\nTo further illustrate the applicability of TabR to large datasets, below, we report the performance of TabR on the Higgs dataset with **ten million training objects:**\n\n| Model   | Hyperparameter tuning time | Accuracy   |\n| ------- |:-------------------------- |:---------- |\n| XGBoost | 1.5 days                   | 0.783      |\n| TabR    | **0**                      | **0.797** |\n\nTechnical details:\n- We used TabR with the default hyperparameters with the simplest `Linear-ReLU` embeddings for continuous features from `[2]` without any hyperparameter tuning (`d_embedding = 32`).\n- We applied the \"context freeze\" technique described in Section 5.1 of the paper for faster training.\n\n> The motivations behind the module designs are not entirely clear ... I suggest that providing a theoretical analysis or intuitive motivation would enhance the reader's understanding of those details.\n\nProviding clear motivation is of high importance to us. Our reply consists of two parts.\n\n**First**, *based on the content from the submission*, let us show how **the design steps behind TabR (Section 3.2) are motivated formally, informally and by prior work:**\n\n- **Step-0 (baseline) is fully motivated by related work.**\n    - (Section 3.2, Paragraph \"Step-0\") *\"the self-attention operation was often used in prior work ... Then, instantiating retrieval module R as the vanilla self-attention ... is a reasonable baseline\"*\n- **Step-1 (using labels) is highly intuitive:** labels is an extremely valuable signal, so we try using them to improve the model.\n    - (Section 3.2, Paragraph \"Step-1\") *\"A natural attempt to improve the Step-0 configuration is to utilize labels of the context objects\"*\n- **Step-2 (modifying the similarity module $\\mathcal{S})$ has a whole range of motivations** provided in the main text (\"Step-1\" and \"Step-2\" paragraphs of Section 3.2) and appendix (Section A.1 referenced from Section 5.3, Section A.3 referenced from Section 3.2):\n    - **Intuitively,** it is motivated by the outcome of Step-1:  (Section 3.2, Paragraph \"Step-1\") *\"Perhaps, the similarity module S taken from the vanilla attention does not allow benefiting from such a valuable signal as labels.\"*\n    - Section A.1.1 provides **formal motivation** for removing the query representations.\n    - Section A.1.1 provides **informal motivation** for using L2 instead of the dot product.\n    - **Empirically**, it is motivated by the exhaustive (literally) ablation study in Section A.3, which is references from the main text.\n- **Step-3 (modifying the value module $\\mathcal{V})$**\n    - **Formal motivation:** using $\\tilde{x}$ in the value module makes it strictly more expressive: (Section 3.2 Paragraph \"Step-3\"): *\"we make the value module V more expressive by taking the target object\u2019s representation $\\tilde{x}$ into account\"*\n    - **Related work:** (Section 3.2 Paragraph \"Step-3\") *\"we take inspiration from DNNR -- the recently proposed generalization of the kNN algorithm\"*\n    - **Intuitive interpretation:** (Section 3.2 Paragraph \"Step-3\"): *\"\\<the decomposition into the \"raw\" and \"correction\" terms\\>\"*\n    - **Quantifying the intuitive interpretation:** Section A.2 referenced from Section 5.3\n- **Step-4** is the only purely technical step. However, it is not uncommon for a new architecture to require different technical defaults.\n\n**Second,** we summarize how we further improve the communication and make the available content more visible in the new PDF:\n- In Section 3.2 Paragraph \"Step-2\", we added a reference to Section A.1.\n- In Section 3.2 Paragraph \"Step-3\", we added a reference to Section A.2.\n- In Section A.2.1, we added extended comments on Section 3.2 Paragraph \"Step-3\".\n- In Section A.8, we added a minor technical note on Section 3.2 Paragraph \"Step-3\"."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9502/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700326344963,
                "cdate": 1700326344963,
                "tmdate": 1700326344963,
                "mdate": 1700326344963,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uz8VykP2gQ",
                "forum": "rhgIgTSSxW",
                "replyto": "yVdQ7kKCcl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9502/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9502/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal (part 4/N)"
                    },
                    "comment": {
                        "value": "> It appears that the authors made meticulous module (equation) optimization based on its performance on some datasets empirically.\n\nWe consider the design-step-based storytelling to be a purely stylistic choice resembling an ablation study, and this is not uncommon in literature, for example, see Section 2 in (ConvNeXt) \"A ConvNet for the 2020s\", CVPR 2022, Liu et al. In our specific case, we find it to be a productive way to highlight the important insights behind TabR.\n\n> Why does employing the L2 distance, instand of the dot product, lead to improved performance (as shown in Eq. 3)?\n\n**The changes introduced in Equation 3 are discussed in the paper** in Section 3.2 Paragraph \"Step-2\" and Section A.1 referenced from Section 5.3 (and, in the new PDF, from Section 3.2):\n\n- (Section 3.2 Paragraph \"Step-2\") Using L2 alone is not enough: *\"removing any of the three ingredients (context labels, key-only representation, L2 distance) results in a performance drop back to the level of MLP\"*.\n- (Section A.1.1) Because of specific details of TabR's encoder and the nature of tabular data, L2 is a more reasonable default choice for TabR.\n- (Section A.1.2) Using L2 leads to more diverse neighbor patterns, which, without any additional assumptions, may be a better default behavior for retrieval-based models.\n- (Section A.1.3) For several datasets, we empirically observed that TabR with L2 was capable of uncovering much better neighbors than with the dot product.\n\nAlso, in Section 3.2 Paragraph \"Step-2\", we add that L2 is just a reasonable default choice: *\"While the L2 distance is unlikely to be the universally best choice for problems (even within the tabular domain), it seems to be a reasonable default choice for tabular data problems.\"*\n\n> Why is the T function required to use LinearWithoutBias?\n\nIn the new PDF, we clarified this in Section A.8: using Linear instead of LinearWithoutBias would be redundant because of the term $W_Y$ that already implicitly contains the bias:\n\n$\\mathcal{V}(\\tilde{x}, \\tilde{x}_i, y_i) = W_Y(y_i) + T(W_K(\\tilde{x}) - W_K(\\tilde{x}_i))$\n\nAnd, just in case, we avoid this redundancy."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9502/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700326379855,
                "cdate": 1700326379855,
                "tmdate": 1700326379855,
                "mdate": 1700326379855,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LuY3RJH9K6",
                "forum": "rhgIgTSSxW",
                "replyto": "yVdQ7kKCcl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9502/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9502/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal (part 5/5, N=5)"
                    },
                    "comment": {
                        "value": "> **[REGARDING ROBUSTNESS OF L2 TO UNINFORMATIVE AND IRREGULARLY DISTRIBUTED FEATURES]**\n>\n> it's unclear whether L2 distance may fail when uninformative features are present ... You can offer some analysis or conduct experiments by adding some gaussian noise columns  \n> ...  \n> or, for instance, when a table has a feature ... and we take the logarithm of these values ...\n> Some transformation like logarithm may impact the results\n\nBelow, we provide results indicating that **the presense of L2 does not make TabR any worse than alternatives**, (dot-product-based TabR or simple MLP) in the presence of uninformative features or after the logarithmic transformation.\nMoreover, the alternatives continue lagging significantly behind TabR.\n\nTechnical setup:\n- Datasets (see Table 1 in the paper): CA, HO, AD, DI, HI.\n- We use TabR with the default hyperparameters.\n- The uniformative (noisy) features are added as new features.\n- For the logarithmic transformation, original features are replaced with the transformed ones.\n\nThe table below reports the average (over datasets) difference in `%` compared to the metric of the corresponding model trained on the original unmodified datasets.\n \n| Ratio of new uninformative features: | 0.1    | 0.25   | 0.5    |\n|:------------------------------------ |:------ |:------ |:------ |\n| TabR                                 | -0.6%  | -1.9%  | -2.87% |\n| TabR (dot product)                   | -1.27% | -2.26% | -3.77% |\n| MLP                                  | -1.32% | -2.63% | -4.37% |\n\n| Ratio of log-transformed features | 0.1    | 0.25   | 0.5    |\n|:--------------------------------- |:------ |:------ |:------ |\n| TabR                              | -0.15% | -0.77% | -1.07% |\n| TabR (dot product)                | -0.23% | -0.18% | -0.39% |\n| MLP                               | -0.05% | -0.55% | -0.45% |\n\n**Importantly**, in both of the above regimes, TabR continues to *significantly* outperform alternatives.\nFor example, this is how the second of the above tables would look like if we reported the differences relative to the *same* baseline, namely, to TabR trained on unmodified original data:\n\n| Ratio of log-transformed features | 0.1    | 0.25   | 0.5    |\n|:--------------------------------- |:------ |:------ |:------ |\n| TabR                              | -0.15% | -0.77% | -1.07% |\n| TabR (dot product)                | -6.95% | -6.95% | -7.17% |\n| MLP                               | -6.65% | -7.23% | -7.15% |\n\nNote that, currently, TabR uses a simple MLP-like encoder, and naturally inherits all properties related to robustness to various data challenges  such as uniformative features, irregularly distributed features, etc. `[1]`, `[13]`, `[14]`. In particular:\n- These properties explain why, in the above table, there is a performance loss for all DL models regardless of the usage of L2.\n- It is also known how to avoid/alleviate these problems: by using transformer-like encoders `[1]`, `[13]` or by using embeddings for continuous features `[2]`.\n\n**References**\n- `[1]` \"Why do tree-based models still outperform deep learning on tabular data?\", NeurIPS 2022 Datasets and Benchmarks, Grinsztajn et al.\n- `[2]` \"On Embeddings for Numerical Features in Tabular Deep Learning\", NeurIPS 2022, Gorishniy et al.\n- `[3]` \"Shifts: A Dataset of Real Distributional Shift Across Multiple Large-Scale Tasks\", NeurIPS 2021 Datasets and Benchmarks , Malinin et al.\n- `[4]` \"Revisiting Deep Learning Models for Tabular Data\", NeurIPS 2021, Gorishniy et al.\n- `[5]` \"TabNet: Attentive Interpretable Tabular Learning\", AAAI 2021, Arik et al.\n- `[6]` \"SAINT: Improved Neural Networks for Tabular Data via Row Attention and Contrastive Pre-Training\", ICLR 2022 submission, Somepalli et al.\n- `[7]` \"Neural Oblivious Decision Ensembles for Deep Learning on Tabular Data\", ICLR 2020, Popov et al.\n- `[8]` \"TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second\", ICLR 2023, Hollmann et al.\n- `[9]` \"TANGOS: Regularizing Tabular Neural Networks through Gradient Orthogonalization and Specialization\", ICLR 2023, Jeffares et al.\n- `[10]` \"T2G-FORMER: Organizing Tabular Features into Relation Graphs Promotes Heterogeneous Feature Interaction\", AAAI 2023, Yan et al.\n- `[11]` \"XGBoost: A Scalable Tree Boosting System\", KDD 2016, Chen et al.\n- `[12]` \"Deep Neural Networks and Tabular Data: A Survey\" \n- `[13]` \"A Performance-Driven Benchmark for Feature Selection in Tabular Deep Learning\", NeurIPS 2023 Datasets and Benchmarks, Cherepanova et al.\n- `[14]` \"When Do Neural Nets Outperform Boosted Trees on Tabular Data?\", NeurIPS 2023 Datasets and Benchmarks, McElfresh et al."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9502/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700326421975,
                "cdate": 1700326421975,
                "tmdate": 1700326421975,
                "mdate": 1700326421975,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CEBB2izG6I",
            "forum": "rhgIgTSSxW",
            "replyto": "rhgIgTSSxW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9502/Reviewer_Ly5o"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9502/Reviewer_Ly5o"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces TabR, a retrieval-augmented tabular deep learning model that outperforms gradient-boosted decision trees (GBDT) on various datasets. TabR incorporates a novel retrieval module that is similar to the attention mechanism, which helps the model achieve the best average performance among tabular deep learning models and is more efficient compared to prior retrieval-based models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. TabR demonstrates superior performance compared to GBDT and other retrieval-based tabular deep learning models on multiple datasets.\n2. The new similarity module in TabR has a reasonable intuitive motivation, allowing it to find and exploit natural hints in the data for better predictions."
                },
                "weaknesses": {
                    "value": "1. Some aspects are not clear, see the questions section."
                },
                "questions": {
                    "value": "1.  What's the reason for choosing m to be 96? How does m affect the performance of TabR?\n2.  What's the inference efficiency of TabR and how does it compare with other baselines (e.g., GBDT)?\n3.  Is TabR applicable to categorical features? It seems like the paper only considers continuous features."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9502/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698007867322,
            "cdate": 1698007867322,
            "tmdate": 1699637193838,
            "mdate": 1699637193838,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HgsxdjJ4gJ",
                "forum": "rhgIgTSSxW",
                "replyto": "CEBB2izG6I",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9502/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9502/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal (part 1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the feedback!\n\n> What's the reason for choosing m to be 96? How does m affect the performance of TabR?\n\n**In the new PDF, this is addressed in Section A.6.** Below, we provide a summary for convenience and an explanation for the obtained results.\n\n**First,** to evaluate how performance depends on $m$, we consider TabR-S with default hyperparameters and, for each value of $m$, we report the average rank of TabR-S on the default benchmark (Table 1 of the main text). For each `(dataset, m)` pair, the performance is computed as the average over five random seeds.\n\nThe choice of $m$  must be made based on the *validation* metrics (not on the *test* metrics), so we provide the ranks on the *validation* sets first:\n\n|     |  m=1 | m=2  | m=4  | m=8  | m=16 | m=32 | m=64 | m=96  | m=128 | m=256 |\n|-----|------|------|------|------|------|------|------|-------|-------| ------|\n| avg | 5    | 4.5  | 4    | 3.25 | 2.75 | 2.12 | 2.12 | 1.88  | 1.88  | 1.88 |\n| std | 2.45 | 1.87 | 1.73 | 1.71 | 1.56 | 1.45 | 1.45 | 0.93  | 1.05  | 1.36 |\n\nIt seems that the m=96 turns out to be a reasonable default choice. \n\nAdditionally, here are the ranks on the *test* sets:\n\n|     | m=1 | m=2  | m=4  | m=8 | m=16 | m=32 | m=64 | m=96 | m=128 | m=256 |\n|-----|------|------|-----|------|------|------|------|-------|-------|-----|\n| avg | 4.12 | 3.62 | 3.5 | 3.12 | 2.38 | 2.12 | 2    | 1.62  | 1.75  | 1.75 |\n| std | 2.2  | 1.8  | 1.5 | 1.05 | 1.41 | 1.27 | 1.41 | 0.99  | 0.97  | 1.09 |\n\n**Second**, we suggest an explanation for the obtained results. The presence of the softmax function in the retrieval module of TabR gives a hope that the only requirement for $m$ is to be large enough, and softmax will \"automatically\" choose the optimal value for each sample. That said, in this work, we don't analyse extreme cases like $m = |I_{train}|$ and recommend values like 96 as a starting point.\n\n> What's the inference efficiency of TabR and how does it compare with other baselines (e.g., GBDT)?\n\n**In the new PDF, this is addressed in Section A.4.2.** For convenience, here, we provide a summary.\n\nBelow, we report the inference throughput of TabR and XGBoost.\n\n(The technical setup:\n- XGBoost and TabR-S with tuned hyperparameters as in Table 4 of the main text\n- Computation is performed on NVIDIA 2080 Ti.\n- For both models, objects are passed by batches of 4096 objects.)\n\n**The key observations:**\n- On the considered datasets, the throughputs of TabR and XGBoost are mostly comparable.\n- **Important**: our implementation of TabR is naive and lacks even basic optimizations.\n\n|                                | CH    | CA   | HO   | AD   | DI   | OT   | HI  | BL   | WE   | CO   | MI   |\n|--------------------------------|------|------|------|------|------|-----|------|------|------|------|------|\n| `#trainingObjects`                | 6400 | 13209 | 14581 | 26048 | 34521 | 39601 | 62751 | 106764 | 296554 | 371847 | 723412 |\n| `#features`                      | 11    | 8    | 16   | 14   | 9    | 93  | 28   | 9    |  119 | 54   | 136 |\n| `n_estimators` in XGBoost         | 121   | 3997 | 1328 | 988  | 802  | 524 | 1040 | 1751 | 3999 | 1258 | 3814 |\n| `max_depth` in XGBoost            | 5     | 9    | 7    | 10   | 13   | 13  | 11   | 8    | 13   | 12   | 12   |\n| XGBoost throughput (obj./sec.)  | 2197k | 33k  | 179k | 131k | 417k | 19k | 72k  | 84k  | 15k  | 10k  | 14k  |\n| TabR-S throughput (obj./sec.)  | 35k   | 35k  | 55k  | 33k  | 43k  | 40k | 37k  | 27k  | 34k  | 23k  | 11k  |\n| Overhead                        | 62.3  | 0.9  | 3.3  | 3.9  | 9.6  | 0.5 | 1.9  | 3.1  | 0.5  | 0.4  | 1.2  |\n\n**In more detail:**\n- On \"non-simple\" tasks (CA, OT, WE, CO), TabR is faster (\"non-simple\" = XGBoost needs many trees AND/OR XGBoost needs high depth AND/OR a dataset has more features).\n- On \"simple\" tasks, XGBoost is faster (\"simple\" = XGBoost is shallow AND/OR dataset has few features).\n- With the growth of training size (MI), TabR may become slower because of the retrieval, however, there is *a lot* of room for optimizations (caching candidate representations instead of recomputing them on each forward pass; using float16 instead of the current float32; doing approximate search instead of the current brute force; using only a subset of the training data as candidates, etc.)\n\n(P.S. in the paper, we also analyze *training* efficiency in Table 10)"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9502/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700325954537,
                "cdate": 1700325954537,
                "tmdate": 1700325954537,
                "mdate": 1700325954537,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TAws16eMTE",
                "forum": "rhgIgTSSxW",
                "replyto": "CEBB2izG6I",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9502/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9502/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal (part 2/2)"
                    },
                    "comment": {
                        "value": "> It seems like the paper only considers continuous features\n\n**In fact, the used datasets include both continuous and categorical features**:\n- For the default benchmark, Table 1 in the main text provides information on how many features of each type is presented in each dataset.\n- The benchmark from `[1]` also includes many datasets with categorical features, please, see the original paper for details.\n\n> Is TabR applicable to categorical features?\n\n**Yes, TabR is applicable to all kinds of features:** its input encoder consists of conventional common modules, as explained in the caption of Figure 3. In particular, categorical features are encoded with the one-hot encoding, and, if needed, one can use other encoding/embedding schemes.\n\n**References**\n\n- `[1]` \"Why do tree-based models still outperform deep learning on tabular data?\", NeurIPS 2022 Datasets and Benchmarks, Grinsztajn et al."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9502/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700325986750,
                "cdate": 1700325986750,
                "tmdate": 1700325986750,
                "mdate": 1700325986750,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9XjvrbNKYr",
                "forum": "rhgIgTSSxW",
                "replyto": "CEBB2izG6I",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9502/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9502/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Inference efficiency: extended analysis"
                    },
                    "comment": {
                        "value": "Dear reviewer,\n\nIn our \"Rebuttal (part 1/2)\" reply, we provided some analysis on the inference efficiency.\n\nNow, we are excited to share an **extended study on the inference throughput covering SEVEN models in TWO modes.** We did not update the PDF document with the new content, but we are committed to do that based on feedback.\n\n**The main observations**:\n- TabR is faster than prior retrieval-based tabular DL (SAINT).\n- TabR is slower than simple retrieval-free models.\n- Notably, TabR is only moderately slower than FT-Transformer (a non-retrieval model).\n- *(not reflected in the tables below)* On large dataset, where the retrieval becomes the bottleneck, TabR can be made *significantly* faster (e.g. by order of magnitude) by using approximate nearest neighbor search instead of the current brute force.\n\nTechnical details:\n- We did *not* use DL-specific optimizations like `torch.compile`, mixed precision computation, pruning and other techniques.\n- Models with tuned hyperparameters are used (that is, from Table 3 and Table 4). For FT-Transformer, the tuned hyperparameters were taken from `[1]` and `[2]` for most datasets, and the default hyperparemeters were used on two datasets.\n\n**The first mode: \"Online predictions\"**\n- Device: CPU Intel Core i7-7800X 3.50GHz\n- Thread count: 1\n- Batch size: 1\n\n*Notation*\n- *Values are aggregated over the 11 datasets from Table 1 (the \"default\" benchmark)*\n- *(A) ~ Absolute throughput (objects per second)*\n- *(R) ~ Relative throughput w.r.t. TabR-S*\n\n| [CPU & Batch size = 1]   | (A) Min   | (A) Median   | (A) Max   | (R) Min   |   (R) Median |   (R) Max |\n|------------------------|:-----------|:--------------|:-----------|:-----------|:--------------|:-----------|\n| TabR-S                 | 31        | 470          | 2.0K      | 1       |         1    |       1   |\n| TabR                   | 26        | 236          | 1.2K      | 0.2       |         0.5  |       1.3 |\n| SAINT                  | < 1       | 15           | 59        | < 0.01    |         0.02 |       0.1 |\n| XGBoost                | 270       | 2.2K         | 10K       | 0.6       |         5.5  |      17.5 |\n| MLP                    | 2.6K      | 4.8K         | 15K       | 4.0       |        13.9  |     387.1 |\n| MLP-PLR                | 532       | 1.6K         | 5.1K      | 0.9       |         5.2  |      47.2 |\n| FT-Transformer         | 219       | 694          | 1.7K      | 0.4       |         1.6  |      11.6 |\n\n**The second mode: \"Offline batch processing\"**\n- Device: GPU NVIDIA 2080Ti 12GB\n- Batch size: the largest possible batch size for a given model (but no larger then `2 ** 17 ~= 128_000`)\n\nIt turns out that XGBoost can achieve better numbers in this mode than with the fixed batch size 4096.\n\n*Notation*\n- *Values are aggregated over the 11 datasets from Table 1 (the \"default\" benchmark)*\n- *(A) ~ Absolute throughput (objects per second)*\n- *(R) ~ Relative throughput w.r.t. TabR-S*\n\n| [CUDA & Batch size = max]   | (A) Min   | (A) Median   | (A) Max   |   (R) Min |   (R) Median |   (R) Max |\n|---------------------------|:-----------|:--------------|:-----------|:-----------|:--------------|:-----------|\n| TabR-S                    | 24K       | 123K         | 319K      |      1    |          1   |       1   |\n| TabR                      | 28K       | 91K          | 308K      |      0.4  |          0.8 |       2.5 |\n| SAINT                     | 2.1K      | 32K          | 133K      |      0.05 |          0.3 |       0.8 |\n| XGBoost                   | 63K       | 1.4M         | 16.6M     |      0.9  |         11.4 |     126.8 |\n| MLP                       | 3.2M      | 7.7M         | 47.8M     |     23.9  |         67.4 |     512   |\n| MLP-PLR                   | 63K       | 1.1M         | 8.5M      |      0.5  |          6.7 |     101.7 |\n| FT-Transformer            | 21K       | 198K         | 752K      |      0.2  |          2   |       4.5 |\n\n**References**\n\n- `[1]` \"Revisiting Deep Learning Models for Tabular Data\", Gorishniy et al.\n- `[2]` \"On Embeddings for Numerical Features in Tabular Deep Learning\", Gorishniy et al."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9502/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700699320610,
                "cdate": 1700699320610,
                "tmdate": 1700699320610,
                "mdate": 1700699320610,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LGzAT6gNeL",
            "forum": "rhgIgTSSxW",
            "replyto": "rhgIgTSSxW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9502/Reviewer_fJsx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9502/Reviewer_fJsx"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a retrieval-augmented deep learning architecture for tabular regression/classification. The model passes $x$, the row to be classified/predicted, as well as additional retrieval context rows, through a learned encoder. TabR then retrieves the rows most similar to the encoded form of $x$, where similarity is defined as the Euclidean distance between the encoded versions of two rows, mapped through a linear layer. The top retrieval candidates and their respective labels are then sent through some more learned transformations before being aggregated and combined with the encoded form of the row to be classified/regressed. This combined embedding goes through more MLP layers to result in the output.\n\nThe paper goes through variants of the architecture and how each respective change impacts performance. It then compares against other deep learning-based models as well as gradient boosted decision trees. In both default-hyperparameter and tuned-hyperparameter settings, TabR performs well."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The extensive amount of open-sourcing and experiment reproducibility is greatly appreciated.\n1. Strong results relative to both deep learning and boosted tree methods, and TabR-S's relatively strong performance relative to out-of-the-box boosted tree libraries suggests this isn't just excessive parameter tweaking and overfitting via architecture search.\n1. Easy to read, with key pieces of information generally emphasized appropriately."
                },
                "weaknesses": {
                    "value": "1. Paper doesn't go into detail describing differences with prior deep learning-based tabular methods. What might explain the performance differences? Ex. \"prior work, where several layers with multi-head attention between objects and features are often used\" but was this what led to retrieval's low benefit in the past?\n1. Insufficient discussion of categorical variables. Is accuracy or training time particularly affected by their relative abundance relative to numerical features?\n1. The steps of Section 3.2 seem rather arbitrary. Some of the detail could be compressed to make room for more intuition why the final architecture makes more sense (content from A.1.1). Description of architectural changes that didn't work would also be very insightful.\n1. Paper describes training times in A.4, but I believe a summary of this is important enough to warrant inclusion in the main paper. Something like a mention of the geometric mean (over the datasets) of the ratio between TabR's training time to a gradient boosted methods, described in the conclusion, would be sufficient. While the ratio is likely >1, it is better to acknowledge this weakness than to hide it."
                },
                "questions": {
                    "value": "See weaknesses. Also, what is $I_{cand}$? Is it all rows of the table that labels have been provided for? It's mentioned in page 3 that \"we use the same set of candidates for all input objects\" but what it the set of candidates exactly?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9502/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9502/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9502/Reviewer_fJsx"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9502/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698808400807,
            "cdate": 1698808400807,
            "tmdate": 1699637193697,
            "mdate": 1699637193697,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TgRoLEmYHG",
                "forum": "rhgIgTSSxW",
                "replyto": "LGzAT6gNeL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9502/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9502/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal (part 1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the comments and questions!\n\n> What might explain the performance differences? ... but was this what led to retrieval's low benefit in the past?\n\n(1) **TL;DR: yes, the main reason behind the difference in the performance lies in the retrieval module designed in Section 3.2.** A quick summary of the main differences in the retrieval-related functionality:\n- Prior work: *multiple multi-head vanilla* attention modules, where the attention is performed *between objects and features*.\n- TabR: *one single-head custom* attention-like module, where the attention is performed *only between objects*.\n\nIn the new PDF, in Section 2 on Page 3, we improved the wording and the formatting to better contrast TabR against prior solutions.\n\n(2) **The relevant parts from the original PDF** (here, we uppercased the key pieces for convenience):\n\n- (Paragraph \"Step 2\" Page 5, where we improved the similarity module commonly used in prior work) *This change is a turning point in our story, WHICH WAS OVERLOOKED IN PRIOR WORK.*\n- (abstract) *\"NOVEL FINDINGS ... LIE IN THE ATTENTION-LIKE MECHANISM that is responsible for retrieving the nearest neighbors and extracting valuable signal from them.\"*\n- (Section 2 Page 3) *\"Compared to prior work, where SEVERAL LAYERS WITH MULTI-HEAD ATTENTION ... TabR implements its retrieval component with just ONE SINGLE-HEAD ATTENTION-LIKE MODULE. Importantly ... module of TabR is CUSTOMIZED in a way that makes it BETTER SUITED FOR TABULAR DATA PROBLEMS\"*\n- etc.\n\n> Insufficient discussion of categorical variables.\n\nHowever, as indicated by Figure 3 and its caption, **TabR is equally capable of handling all feature types, there is no conceptual difference between them.** In particular:\n- Categorical features are encoded with the one-hot encoding (as mentioned in the caption of Figure 3 and in Section D6). This is a simple and efficient operation, and one can switch to any other encoding scheme if needed.\n- Continuous features are normalized and, optionally, transformed with embeddings from `[2]` (which, in fact, makes continuous features more demanding than categorical features in terms of compute).\n\nJust in case, we will mention that **the used datasets include both continuous and categorical features**:\n- The default benchmark: Table 1 provides information on how many features of each type is presented in each dataset.\n- The benchmark from `[1]` also includes many datasets with categorical features.\n\n> Is accuracy or training time particularly affected by their relative abundance relative to numerical features?\n\nWe did not notice any different behaviour caused by the presence of categorical features, which is consistent with the explanation provided above (*\"... for TabR, there is no conceptual difference between feature types ...\"*).\n\n> The steps of Section 3.2 seem rather arbitrary.\n\n**First**, just in case, we provide a quick informal recap of the story behind the design steps in Section 3.2:\n- \"Step-0. The vanilla-attention-like baseline\" is motivated by prior work.\n- \"Step-1. Adding context labels\" is a natural attempt to use the available labels of the neighbors. The important outcome: *\"the similarity module $\\mathcal{S}$ taken from the vanilla attention does not allow benefiting from such a valuable signal as labels\"*.\n- \"Step-2. Improving the similarity module $\\mathcal{S}$\" is fully motivated by the quoted outcome of Step-1.\n- \"Step-3. Improving the value module $\\mathcal{V}$\" is inspired by a simple observation that $\\mathcal{V}$ can be made more expressive by adding the dependency on the target object's representation $\\tilde{x}$.\n- Step-4 is the only purely technical step where we identified better technical defaults for the new architecture.\n\n**Second**, how we improved the communication and made the available content more visible in the new PDF:\n- In Section A.2.1, we added extended motivation behind the design of the value module of TabR.\n- In the \"Step-2\" and \"Step-3\" paragraphs, we added references to the extended motivation and analysis available in the appendix.\n- In the \"Step-3\" paragraph, we improved the wording to highlight the transition of our focus from the similarity module to the value module."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9502/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700325744410,
                "cdate": 1700325744410,
                "tmdate": 1700325841503,
                "mdate": 1700325841503,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "p1UOBdaIBT",
                "forum": "rhgIgTSSxW",
                "replyto": "LGzAT6gNeL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9502/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9502/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal (part 2/2)"
                    },
                    "comment": {
                        "value": "> Paper describes training times in A.4, but I believe a summary of this is important enough to warrant inclusion in the main paper. Something like a mention of the geometric mean (over the datasets) of the ratio between TabR's training time to a gradient boosted methods, described in the conclusion, would be sufficient.\n\nWe are excited to discuss the aspect of efficiency in more detail!\n\nRegarding the suggestion to announce Table 10 (training times), currently, we achieve that and prepare readers for Table 10 by:\n\n- repeatedly mentioning two things throughout the paper:\n    - TabR is more efficient than prior retrieval-based tabular DL.\n    - TabR is less efficient than simple models and may require special considerations on larger datasets.\n- directly mentioning Section A.4 and Section A.4.1 multiple times.\n\n**Importantly**, we believe that Table 10 (training times) needs a full-fledged presentation and nuanced discussion, and it may suffer from simplifications. In particular, announcing Table 10 with a single number can be misleading: for example, the suggested geometric mean of the ratios will completely hide absolute training times, which is what actually matters in practice. To avoid such effects, we report specific numbers for both \"positive\" (e.g. order(s) of magnitude improvements over prior work) and \"negative\" efficiency-related stories only in Table 10.\n\n>  it is better to acknowledge this weakness than to hide it.\n\nBased on the content from the original submission, let us illustrate that, in fact, **we care deeply about efficiency, invest heavily in this aspect of our work, and strive for complete transparency in this matter.** We truly consider the storyline about efficiency an important part of our paper. In particular, throughout the work, we repeatedly mention two things:\n- TabR is more efficient than prior retrieval-based tabular DL.\n- TabR is less efficient than simple models and may require special considerations on larger datasets.\n    - (The end of Introduction) *\"Tree-based models, in turn, remain a cheaper solution\"*\n    - **The whole Section 5.1 is dedicated to improving training times of TabR**.\n    - Even the seemingly unrelated Section 5.2 mentions that the continual updates can be used to make TabR train faster: *\"Additionally, this approach can be used to scale TabR to large datasets by training the model on a subset of data and retrieving from the full data.\"*\n    - (Conclusion) *\"An important direction for future work is improving the efficiency of retrieval-augmented models to make them faster in general and in particular applicable to tens and hundreds of millions of data points\"*\n    - (Limitations on Page 6 refer to Section B, where we write:) *\"the retrieval module R still causes overhead compared to purely parametric models, so TabR may not scale to truly large datasets as-is\"*\n\n> Also, what is $I_{cand}$? Is it all rows of the table that labels have been provided for?\n\n1. **In most of the experiments, yes**: the neighbors are retrieved from all training objects, which formally means $I_{cand} = I_{train}$.\n2. Strictly speaking, we implicitly rely on the fact that  $I_{cand}$ can be different from $I_{train}$ in the following places:\n    - In Section 5.1, for *training* objects,  $I_{cand} = I_{train}$ is true only until the context freeze, after which the retrieval for *training* objects is not performed (instead, the same neighbors are reused until the end of training).\n    - In Section 5.2, for *test* objects, $I_{cand} = I_{train}$ is true only until new labeled objects are added to the set of candidates. \n    - The *\"Second, depending on an application ...\"* paragraph of Section B.\n\n> It's mentioned in page 3 that \"we use the same set of candidates for all input objects\"\n\nIn the new PDF, we improved the wording, now it says: *\"In this work, unless otherwise noted, we use the same set of candidates for all input objects and set $I_{cand} = I_{train}$ (which means retrieving from all training objects)\"*.\n\n**References**\n- `[1]` \"Why do tree-based models still outperform deep learning on tabular data?\", NeurIPS 2022 Datasets and Benchmarks, Grinsztajn et al.\n- `[2]` \"On Embeddings for Numerical Features in Tabular Deep Learning\", NeurIPS 2022, Gorishniy et al."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9502/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700325765766,
                "cdate": 1700325765766,
                "tmdate": 1700325765766,
                "mdate": 1700325765766,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7UJsyfPyd4",
            "forum": "rhgIgTSSxW",
            "replyto": "rhgIgTSSxW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9502/Reviewer_aaV3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9502/Reviewer_aaV3"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers the problem of making predictions on tabular data. The authors propose a retrieval-augmented approach where a predictor takes the representation not of the table being predicted but also the representation of the nearest neighbors from a training dataset. The encoding representations and the predictors are training together and use straightforward architecture architectures. The main result is that a combination of the carefully crafted techniques outperforms GBDT on an ensemble of tasks. The training time is higher than GBDT but not unreasonable, and better compared to prior deep learning methods. The prediction times are better"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The results seem to be a significant advance over prior work in tabular data predictions. In particular, the first deep learning model to outperform GBDT on an ensemble of datasets.\n2. The experiments and analysis are quite extensive. Multiple datasets of different kinds of data, analysis of training and prediction times.\n3. Clear articulation of which techniques helped. the techniques are overall not too complex."
                },
                "weaknesses": {
                    "value": "A comparison of the inference and query complexity between the methods is lacking."
                },
                "questions": {
                    "value": "1. Inference time and compexity -- are the studies based on normalized inference time between models? If not, could you comment more? How does the inference complexity depend on the size of the table data?\n\n2. Could a different selection of datasets prove that the tabR is not superior to GBDT? In other words, are these datasets highly representative?\n\n3. Is it not surprising that Step-1 (adding context labels) did not help that much? One would guess that this is a big component of signal in retrieval augmentation.\n\n4. Not a question, but the methodology here reminds one of extreme classification and specifically this paper. https://arxiv.org/abs/2207.04452"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9502/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9502/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9502/Reviewer_aaV3"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9502/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698809806217,
            "cdate": 1698809806217,
            "tmdate": 1699637193544,
            "mdate": 1699637193544,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pfSPP72xj0",
                "forum": "rhgIgTSSxW",
                "replyto": "7UJsyfPyd4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9502/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9502/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal (part 1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the positive feedback!\n\n> A comparison of the inference and query complexity between the methods is lacking.\n\n**In the new PDF, this is addressed in Section A.4.2.** For convenience, here, we provide a summary.\n\nBelow, we report the inference throughput of TabR and XGBoost.\n\n(The technical setup:\n- XGBoost and TabR-S with tuned hyperparameters as in Table 4 of the main text\n- Computation is performed on NVIDIA 2080 Ti.\n- For both models, objects are passed by batches of 4096 objects.)\n\n**The key observations:**\n- On the considered datasets, the throughputs of TabR and XGBoost are mostly comparable.\n- **Important**: our implementation of TabR is naive and lacks even basic optimizations.\n\n|                                | CH    | CA   | HO   | AD   | DI   | OT   | HI  | BL   | WE   | CO   | MI   |\n|--------------------------------|------|------|------|------|------|-----|------|------|------|------|------|\n| `#trainingObjects`                | 6400 | 13209 | 14581 | 26048 | 34521 | 39601 | 62751 | 106764 | 296554 | 371847 | 723412 |\n| `#features`                      | 11    | 8    | 16   | 14   | 9    | 93  | 28   | 9    |  119 | 54   | 136 |\n| `n_estimators` in XGBoost         | 121   | 3997 | 1328 | 988  | 802  | 524 | 1040 | 1751 | 3999 | 1258 | 3814 |\n| `max_depth` in XGBoost            | 5     | 9    | 7    | 10   | 13   | 13  | 11   | 8    | 13   | 12   | 12   |\n| XGBoost throughput (obj./sec.)  | 2197k | 33k  | 179k | 131k | 417k | 19k | 72k  | 84k  | 15k  | 10k  | 14k  |\n| TabR-S throughput (obj./sec.)  | 35k   | 35k  | 55k  | 33k  | 43k  | 40k | 37k  | 27k  | 34k  | 23k  | 11k  |\n| Overhead                        | 62.3  | 0.9  | 3.3  | 3.9  | 9.6  | 0.5 | 1.9  | 3.1  | 0.5  | 0.4  | 1.2  |\n\n**In more detail:**\n- On \"non-simple\" tasks (CA, OT, WE, CO), TabR is faster (\"non-simple\" = XGBoost needs many trees AND/OR XGBoost needs high depth AND/OR a dataset has more features).\n- On \"simple\" tasks, XGBoost is faster (\"simple\" = XGBoost is shallow AND/OR dataset has few features).\n- With the growth of training size (MI), TabR may become slower because of the retrieval, however, there is *a lot* of room for optimizations (caching candidate representations instead of recomputing them on each forward pass; using float16 instead of the current float32; doing approximate search instead of the current brute force; using only a subset of the training data as candidates, etc.)\n\n> Inference time and compexity -- are the studies based on normalized inference time between models? If not, could you comment more?\n\nStrictly speaking, the results reported in all main tables are obtained without fixing the inference time budget: all methods can spend any time they need to make predictions. *However,* generally, the efficiency of TabR vs. prior work is an important storyline of our paper with significant wins over prior work, and, in the considered scope of datasets, the inference time of TabR is always reasonable (as indicated by the above table).\n\n> How does the inference complexity depend on the size of the table data?\n\nAs indicated by the above table, for datasets of sizes up to (roughly) 500K objects, for TabR-S, the dataset size is not a major factor defining the inference throughput. For larger datasets, the dataset size becomes a more important factor (formally, for the current brute force search, the search complexity grows linearly with the dataset size). Luckily, the current implementation of TabR has a lot of room for simple optimizations:\n- caching candidate key representations instead of recomputing them on each forward pass.\n- performing the search in float16 instead of the current float32.\n- performing approximate similarity search instead of the current brute force.\n- using only a subset of the training data as candidates.\n- etc."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9502/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700325638178,
                "cdate": 1700325638178,
                "tmdate": 1700325638178,
                "mdate": 1700325638178,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "43qPvKk0Yi",
                "forum": "rhgIgTSSxW",
                "replyto": "7UJsyfPyd4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9502/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9502/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal (part 2/2)"
                    },
                    "comment": {
                        "value": "> are these datasets highly representative?\n\nWe did our best to build a representative benchmark using **~50 datasets from prior work**.\nIn particular, we ensured that there are **many challenging tasks that favour GBDT over traditional neural networks.** Examples:\n- We use the widely cited `[1]` where GBDT was shown to be superior to neural networks.\n- We use some datasets from `[2]` including multiple GBDT-friendly tasks.\n- We also use the real world large scale dataset for weather prediction from `[3]`.\n\nTo put our work in the context of prior work, here are the main properties of the benchmarks used in some prior work on tabular deep learning:\n\n| Model `[citation]`   | The number of datasets used in the paper                                |\n| -------------------- | ----------------------------------------------------------------------- |\n| **TabR** `[ours]`    | **~50 datasets (roughly, `[1]` + some datasets from `[2]` and `[3]`)**  |\n| NODE `[7]`           | 6 datasets                                                              |\n| TabNet  `[5]`        | <10 datasets                                                            |\n| FT-Transformer `[4]` | 11 datasets                                                             |\n| SAINT `[6]`          | 30 datasets                                                             |\n| \\<survey\\> `[11]`    | 5 datasets                                                              |\n| MLP-PLR `[2]`        | 11 datasets                                                             |\n| TabPFN `[8]`         | 67 small datasets (different scope)                                     |\n| \\<benchmark\\> `[1]`  | <50 *unique* datasets (>50 with multiple versions of the same datasets) |\n| TANGOS `[9]`         | 20 datasets                                                             |\n| T2G-FORMER `[10]`    | 12 datasets                                                             |\n\n\n> Is it not surprising that Step-1 (adding context labels) did not help that much?\n\nThis is discussed in the \"Step-1\" paragraph on Page 5:\n\n*> \"Table 2 shows no improvements from using labels, which is counter-intuitive. Perhaps, the similarity module taken from the vanilla attention does not allow benefiting from such a valuable signal as labels.\"*\n\nIn other words, yes, this is indeed surprising! However, after the subsequent Step-2 in Section 3.2 (and, in particular, after the ablation study in Section A.3), we can confidently say that *to benefit from a valuable signal (such as labels), good similarity module is required.*\n\n**References**\n- `[1]` \"Why do tree-based models still outperform deep learning on tabular data?\", NeurIPS 2022 Datasets and Benchmarks, Grinsztajn et al.\n- `[2]` \"On Embeddings for Numerical Features in Tabular Deep Learning\", NeurIPS 2022, Gorishniy et al.\n- `[3]` \"Shifts: A Dataset of Real Distributional Shift Across Multiple Large-Scale Tasks\", NeurIPS 2021 Datasets and Benchmarks , Malinin et al.\n- `[4]` \"Revisiting Deep Learning Models for Tabular Data\", NeurIPS 2021, Gorishniy et al.\n- `[5]` \"TabNet: Attentive Interpretable Tabular Learning\", AAAI 2021, Arik et al.\n- `[6]` \"SAINT: Improved Neural Networks for Tabular Data via Row Attention and Contrastive Pre-Training\", ICLR 2022 submission, Somepalli et al.\n- `[7]` \"Neural Oblivious Decision Ensembles for Deep Learning on Tabular Data\", ICLR 2020, Popov et al.\n- `[8]` \"TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second\", ICLR 2023, Hollmann et al.\n- `[9]` \"TANGOS: Regularizing Tabular Neural Networks through Gradient Orthogonalization and Specialization\", ICLR 2023, Jeffares et al.\n- `[10]` \"T2G-FORMER: Organizing Tabular Features into Relation Graphs Promotes Heterogeneous Feature Interaction\", AAAI 2023, Yan et al.\n- `[11]` \"Deep Neural Networks and Tabular Data: A Survey\""
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9502/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700325671155,
                "cdate": 1700325671155,
                "tmdate": 1700325671155,
                "mdate": 1700325671155,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6bGJyzEwju",
                "forum": "rhgIgTSSxW",
                "replyto": "pfSPP72xj0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9502/Reviewer_aaV3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9502/Reviewer_aaV3"
                ],
                "content": {
                    "title": {
                        "value": "inference time comparison"
                    },
                    "comment": {
                        "value": "Is it reasonable to compare XGBoost with deep learning with 4096 batch size on GPU. How do the inference times compare on CPU or with small batching as is the case with inference in reality?"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9502/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700517720978,
                "cdate": 1700517720978,
                "tmdate": 1700517720978,
                "mdate": 1700517720978,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]