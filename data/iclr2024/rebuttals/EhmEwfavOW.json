[
    {
        "title": "HoloNets: Spectral Convolutions do extend to Directed Graphs"
    },
    {
        "review": {
            "id": "6i0HEtUmHx",
            "forum": "EhmEwfavOW",
            "replyto": "EhmEwfavOW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3367/Reviewer_XGUM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3367/Reviewer_XGUM"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces spectral convolutions to directed graphs without relying on the graph Fourier transform. The paper provides a frequency-response interpretation of the proposed filters with a theoretical analysis. Experimental results seem promising."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1) The usage of holomorphic functional calculus to introduce directed graph filters is novel.\n2) Directed graphs are important to consider.\n3) Theoretical analysis seems sound.\n4) The proposed method seems effective and expressive."
                },
                "weaknesses": {
                    "value": "1) The formulation seems to be restricted to one-dimensional node weights for the nodes, but it is not clear what modifications should be made if we are given multiple-dimensional node features.\n2) The underlying reason for using holomorphic functional calculus has not been clearly demonstrated.\n3) Some typos exist. For example, in the \"Contributions\" part at the end of Sec. 1, there is one redundant \"the\" in line 6; in the second paragraph of Sec. 3.3.1, there is one more \"near\" in line 5; there should be an 'as' after 'referred to' right after equation (5) on page 17; the first 'eigenvalue' should indeed be 'eigenvector' in Sec. C on page 17; 'interpreting' should be 'interpreted' right after the mention of Appendix B on page 18."
                },
                "questions": {
                    "value": "1) The formulation seems to be restricted to one-dimensional node weights for the nodes. What modifications should be made if we are given multiple-dimensional node features?\n2) What is the computational complexity of HoloNets compared to DiGCN, MagNet, and Dir-GNN?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3367/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698416024008,
            "cdate": 1698416024008,
            "tmdate": 1699636287068,
            "mdate": 1699636287068,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0905aQkY40",
                "forum": "EhmEwfavOW",
                "replyto": "6i0HEtUmHx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3367/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3367/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Provided additional Details on the Necessity of the Holomorphic Functional Calculus and added a Discussion of Computational Complexity"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer for the careful consideration of our paper! We were especially happy to read  the sentiments that \n> the usage of holomorphic functional calculus to introduce directed graph filters is novel,\n\nthat our\n>  theoretical analysis seems sound\n\n and our \n> proposed method seems effective and expressive.\n\nLet us adress the raised points individually below:\n\n* The formulation seems to be restricted to one-dimensional node weights for the nodes, but it is not clear what modifications should be made if we are given multiple-dimensional node features.\n\nChannel mixing in spectral graph networks is implemented in analogy to the Euclidean case: A Euclidean convolutional filter applied to an image acts only on a single (r, g or b)-channel. Channel mixing is then facilitated by connecting all channels in two consecutive layers via convolutional filters. All incoming information at a given channel is then summed up. A convenient visual representation may e.g. be found in Figure 2 of (Koke (2023)) . \n\nOur method follows this paradigm. This might for example be explicitly seen in the layer update rule in Section 4: As described there, the  weight matrices $W_k^{\\text{fwd/bwd},\\ell+1}$ are of dimension $F_{\\ell}\\times F_{\\ell+1}$, with $F_{\\ell}, F_{\\ell+1}$ the feature dimensions of the respective layers. \nThese matrices implement the contact between different channels.\n\n* \"The underlying reason for using holomorphic functional calculus has not been clearly demonstrated.\"\n\nThe guiding principle behind modern spectral convolutional methods is that of designing filters of the form $g(T)$, with $T$ a characteristic operator and $g$ a learnable (scalar) function (Defferrard et. al (2016)). In the undirected setting, we had $T = T^*$, which \u2013 as we detail in Sections 1 through 3 \u2013 allows to unambiguously evaluate the scalar function $g$ on the matrix $T$ by essentially evaluating $g$ on each eigenvalue of $T$ independently.\n\nAs we detail in Sections 2 and 3, this is no longer possible in the directed setting; instead the only way to consistently define the matrix $g(T)$ is to make use of the holomorphic functional calculus. \n\nFollowing this concern raised by the reviewer, we have modified the corresponding discussion of this matter in Section 3.1,  to reflect even more strongly that $g(T)$ may only be consistently defined if $g$ is assumed to be holomorphic.\n\nIf the reviewer would however like us to incorporate additional specific comments on this matter, we would be more than happy to do so.\n\n\n\n* \"Some typos exist. \"\n\nWe are very grateful to the reviewer spotting these typos! We have dilligenly fixed them all in the updated version of our manuscript.\n\n* \"What is the computational complexity of HoloNets compared to DiGCN, MagNet, and Dir-GNN?\"\n\nFor definiteness, we will assume the DirGCN configuration of the DirGNN baseline (other configurations would yield higher complexity). For a given graph $G$, let $N$ denote its number of nodes and let $E$ denote its number of edges. Assume  we have input features of dimension $C$ and  a $F$ hidden dimensions in our architectures. \n\nDiGCN: In its full form, DiGCN has a $\\mathcal{O}(N^2)$ complexity, which is reduced to a  $\\mathcal{O}(EFC)$ complexity in its approximate propagation scheme, which is used in practice.\n\nDirGNN: Similarly, DirGNN possesses a $\\mathcal{O}(EFC)$. In both cases this stems from the sparse-dense matrix multiplications that facilitate the forward function.\n\nMagNet: MagNet is similarly implemented via sparse-dense matrix multiplications; resulting in an $\\mathcal{O}(EFC)$-complexity.\n\nFaberNet: Since FaberNet too implements its forward using sparse-dense matrix multiplications, its filtering operation also has $\\mathcal{O}(EFC)$ complexity.\n\nDir-ResolvNet: The filtering operation in Dir-ResolvNet is implemented as dense-dense matrix multiplication, and as such has complexity $\\mathcal{O}(N^2)$ like the full (not approximated) forward operation of DiGCN would have.\n\n\nFollowing this question, we have also included this discussion in a new section (Section I.3: Efficiency Analysis) in our updated manuscript."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3367/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700318577127,
                "cdate": 1700318577127,
                "tmdate": 1700318577127,
                "mdate": 1700318577127,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BaTxupkEcX",
                "forum": "EhmEwfavOW",
                "replyto": "0905aQkY40",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3367/Reviewer_XGUM"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3367/Reviewer_XGUM"
                ],
                "content": {
                    "title": {
                        "value": "Thank you"
                    },
                    "comment": {
                        "value": "Thank you for your response. I am happy to keep my current score to support the paper's acceptance."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3367/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700481152121,
                "cdate": 1700481152121,
                "tmdate": 1700481152121,
                "mdate": 1700481152121,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "SKh4Yw7XHq",
            "forum": "EhmEwfavOW",
            "replyto": "EhmEwfavOW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3367/Reviewer_T8Vv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3367/Reviewer_T8Vv"
            ],
            "content": {
                "summary": {
                    "value": "This paper targets the limitation of conventional Graph Neural Networks (GNNs) that struggle with handling directed graphs effectively. The objective is to enhance spectral convolution networks to cater to directed graphs, thereby eliminating reliance on the Graph Fourier Transform. The proposed method's effectiveness is confirmed through experiments conducted on real-world datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. The paper is novel and has theoretical depth.\n2. The proposed method is technically sound.\n3. The paper is well-written and easy to follow.\n4. The proposed method achieves SOTA performance in several real-world directed heterophilic datasets."
                },
                "weaknesses": {
                    "value": "I am not an expert in spectral GNNs thus I could only point out a limited number of issues in the experiments:\n1. It would be good to examine the efficiency of the proposed method, especially when compared with DiGCN and DirGNN. Moreover, a discussion on the number of learnable parameters would be beneficial.\n2. The impact of the number of layers on the model's performance is worth exploring. Do we have the same problem of over-smoothing in directed graphs when the GNN goes deeper?\n3. Did HoloNet outperform the baselines in graph classification tasks? More evaluation tasks could be investigated.\n\n\nOverall, I think it's a good paper ready for publication at NeurIPS."
                },
                "questions": {
                    "value": "Please reply to questions in weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3367/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698591028711,
            "cdate": 1698591028711,
            "tmdate": 1699636286962,
            "mdate": 1699636286962,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZapojIrU8P",
                "forum": "EhmEwfavOW",
                "replyto": "SKh4Yw7XHq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3367/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3367/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Added a discussion of efficiency as well as number of learnable parameters"
                    },
                    "comment": {
                        "value": "We would like to sincerely thank the reviewer for the careful evaluation  of our paper and appreciation of our results. We were especially  happy to read that\n> the paper is well written and easy to follow,\n\n>\u201cnovel and has theoretical depth\n and is \n\n> ready for publication at NeurIPS.\n\nLet us address the raised points individually:\n\n* It would be good to examine the efficiency of the proposed method, especially when compared with DiGCN and DirGNN.\n\nWe are very happy to do so:\n\nFor definiteness, we will assume the DirGCN configuration of the DirGNN baseline (other configurations would yield higher baseline-complexity). For a given graph $G$, let $N$ denote its number of nodes and let $E$ denote its number of edges. Assume  we have input features of dimension $C$ and   $F$ hidden dimensions in each layer of our architectures. \n\nDiGCN: In its full form, DiGCN has a $\\mathcal{O}(N^2)$ complexity, which is reduced to a  $\\mathcal{O}(EFC)$ complexity in its approximate propagation scheme, which is used in practice.\n\nDirGNN: Similarly, DirGNN possesses a $\\mathcal{O}(EFC)$ complexity. In both cases this stems from the sparse-dense matrix multiplications that facilitate the forward process.\n\nFaberNet: Since FaberNet too implements its forward using sparse-dense matrix multiplications, its filtering operation also has $\\mathcal{O}(EFC)$ complexity.\n\nDir-ResolvNet: The filtering operation in Dir-ResolvNet is implemented as dense-dense matrix multiplication, and as such has complexity $\\mathcal{O}(N^2)$ like the full (not approximated) forward operation of DiGCN would have.\n\nFollowing this point raised by the reviewer, we have now included this discussion into a new subsection titled \u201cEfficiency Analysis\u201d in \"Appendix I: Additional Details  on Experiments\u201d in our revised manuscript.\n* \"Moreover, a discussion on the number of learnable parameters would be beneficial.\"\n\nCompared to DirGNN (in its DirGCN Setting) and assuming the same network width and depth, our FaberNet has $K$ times the number of learnable parameters in the real setting. In the complex setting, our method has $2\\cdot K$ as many _real_ parameters as DirGNN.  Here $K$ refers to the highest utilized order of Faber polynomials $\\Psi_k$. In our experiments, the maximal attained value of $K$ was $K = 5$ on  the \u201cSquirrel\u201d dataset. On this dataset, complex weights and biases performed best, so that our method has 6.3 M  trainable parameters in the corresponding utilized hyperparameter-configuration for Squirrel (detailed in Appendix I).\n\nWe have included this discussion above into the newly created subsection titled \u201cEfficiency Analysis\u201d in \"Appendix I: Additional Details  on Experiments\u201d in our revised manuscript."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3367/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700317195470,
                "cdate": 1700317195470,
                "tmdate": 1700317195470,
                "mdate": 1700317195470,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SEkWwvsypw",
                "forum": "EhmEwfavOW",
                "replyto": "SKh4Yw7XHq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3367/Reviewer_T8Vv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3367/Reviewer_T8Vv"
                ],
                "content": {
                    "title": {
                        "value": "Response to the authors"
                    },
                    "comment": {
                        "value": "Thank you for addressing my questions. I believe it's a good paper and vote for acceptance."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3367/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700456434228,
                "cdate": 1700456434228,
                "tmdate": 1700456434228,
                "mdate": 1700456434228,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "fbXpdTnXDK",
            "forum": "EhmEwfavOW",
            "replyto": "EhmEwfavOW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3367/Reviewer_RxaJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3367/Reviewer_RxaJ"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel approach to extending spectral graph neural networks (GNNs) from undirected graphs to directed graphs. \nInstead of leveraging the traditional Graph Fourier Transform, this paper employs the holomorphic function to avoid the self-adjoint requirement. This paper further discusses the significance of this extension, highlighting its frequency-response interpretation and the basis used to express filters. Finally, the proposed method, namely FaberNet, is validated through experiments on diverse datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper provides a new direction for designing spectral GNNs in directed graphs, i.e., holomorphic functions, which can preserve the algebraic relations to construct the polynomial filters.\n\n2. The authors give sufficient justifications to verify the advantages of holomorphic functions, convincing me about their effectiveness.\n\n3. The proposed FaberNet consistently outperforms other directed GNNs in both heterophilic node classification and regression tasks."
                },
                "weaknesses": {
                    "value": "I'm concerned about the datasets used in the node classification task. Results in Table 1 show that the performance of MagNet is far way from FaberNet. I guess that this is because MagNet operates as a low-pass filter and therefore cannot perform well in the heterophilic datasets.\n\nSince extending spectral GNNs to direct graphs mainly relies on the modification of eigenvectors, I think the choice of filters should not be the major reason to affect the model performance.\n\nIt would be better if the authors could provide more experimental results in the homophilic graphs to validate the effectiveness of the proposed methods."
                },
                "questions": {
                    "value": "Replacing $\\lambda$ with $T$ yields equation 2. I wonder what $d$ indicates in $T - z \\cdot I d$? This symbol seems to conflict with the differential symbol."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3367/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698681928784,
            "cdate": 1698681928784,
            "tmdate": 1699636286896,
            "mdate": 1699636286896,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jT0OFVwt0k",
                "forum": "EhmEwfavOW",
                "replyto": "fbXpdTnXDK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3367/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3367/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Provdided an extended Discussion of the Interplay between Filter-Choice, Operator Selection and Dataset-specific-Performance + Are running additional Experiments on homophilic Graphs"
                    },
                    "comment": {
                        "value": "We would like to sincerely thank the reviewer for the careful consideration of our paper! We were especially happy to read that we \n> give sufficient justifications to verify the advantages of holomorphic functions, convincing [the reviewer] about their effectiveness\u201d\n\nand it was appreciated by the reviewer that our approach\n> consistently outperforms other directed GNNs in both heterophilic node classification and regression tasks\u201d.\n\nLet us address the raised points individually below:\n\n\n\n* \"Results in Table 1 show that the performance of MagNet is far way from FaberNet. I guess that this is because MagNet operates as a low-pass filter and therefore cannot perform well in the heterophilic datasets.\"\n\nWe personally do not believe that the observed performance gap necessarily arises because MagNet operates as a low pass filter:\n\nIn standard graph convolutional architectures, the effect of (low-pass) Laplacian smoothing indeed makes node representations more and more similar as a signal is propagated through the layers (c.f. e.g. [1]). This is then especially bad in the heterophilic setting, where node-labels of a given node and those of its neighbourhood differ.\n\nMathematically, Laplacian smoothing can be understood as mapping an original feature matrix $X$ to its smoothed version $X \\mapsto e^{-\\Delta \\cdot t}\\cdot X$. Here $\\Delta$ denotes a suitable graph Laplacian and the continuous time $t$ is replaced by the layer-depth $\\ell$ when discussing the low-pass behaviour of GNNs. Smoothing now occurs, bevause the function $\\lambda \\mapsto e^{-\\lambda\\cdot t}$ suppresses all non-zero eigenvalues $\\lambda$. \n\nFor large times ($t \\gg 1$) or respectively deep architectures, the only information that survives is that corresponding to the $\\lambda = 0$ eigenvalue. Since the corresponding eigenvector is (up to normalisation) simply given as $v_0^T = (1,\u2026,1)^T$, this means that only a graph-average  $v_0^T\\cdot  X$ of all the available information $X$ survives. \n\nThe story is however different when the magnetic Laplacian $\\Delta_q$ as in the MagNet Paper is considered: Generically, this Laplacian does not have $0$ as an eigenvalue if $q \\neq 0$. \n((To be exactly precise, $0$ is an element of the spectrum $\\sigma(\\Delta_q)$ if and only if the magnetic potential that gives rise to $\\Delta_q$ is gauge-equivalent to the trivial potential; c.f. e.g. [2].))\nSince $0$ is (generically) not an eigenvalue of $\\Delta_q$, we have that $e^{-\\Delta_q\\cdot t}$ does not (generically) converge to a graph average, when  $t$ (respectively the layer-depth) is increased.\nThis makes us question, whether the MagNet architecture really acts as a low-pass filter.\n\nOur interpretation of the performance gap between MagNet and FaberNet is a different one:  \n\nThe operator on which MagNet is based, is the magnetic Laplacian $\\Delta_q$. Up to magnetic modifications and normalization, this operator is of the form $D \u2013 A$, with $D$ the degree matrix and $A$ the adjacency matrix. Applying this operator to the feature matrix $X$, thus essentially compares the local feature of a given node (retained via $D$) with the features of all surrounding nodes (aggregated via $A$).\n\nAs was remarked in \"Improving Graph Neural Networks with Simple Architecture Design\u201d (i.e. \u201cMaurya et al., 2021\u201d in our Bibliography) the following holds:\n> Under homophily, nodes are assumed to have neighbors with similar features and labels. Thus, the cumulative aggregation of node\u2019s self-features with that of neighbors reinforce the signal corresponding to the label and help to improve accuracy of the predictions. While in the case of heterophily, nodes are assumed to have dissimilar features and labels. In this case, the cumulative aggregation will reduce the signal and add more noise causing neural network to learn poorly and causing drop in performance. Thus it is essential to have node\u2019s self-features separate from the neighbor\u2019s features.\n\nTo avoid comparing the features of surrounding nodes with the feature vector of a given node our architecture is based on (a modified version of) only the adjacency matrix $A$ (instead of a Laplacian $\\sim D - A $).  \nThe adjacency matrix has only the entry $0$ on the diagonal. Thus, when updating the feature vector of a given node, the previous feature vector of this node is discarded, and the new feature vector is solely made up of information about surrounding nodes (i.e. information about the neighbourhood(structure) of the original node). This avoids  mixing and comparing  self-features with neighbourhood features. This is beneficial in the heterophilic setting, as (Maurya et al., 2021) pointed out theoretically, and we also observed empirically."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3367/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700313908341,
                "cdate": 1700313908341,
                "tmdate": 1700313908341,
                "mdate": 1700313908341,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6sY90lu0N8",
            "forum": "EhmEwfavOW",
            "replyto": "EhmEwfavOW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3367/Reviewer_tzrV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3367/Reviewer_tzrV"
            ],
            "content": {
                "summary": {
                    "value": "This paper studied the problem of designing graph filters for directed graphs. Instead of following the same approach for undirected cases (first define the spectrum based on Graph Fourier Transform, then define the filters based on spectral responses), the authors proposed to analyze the relationship between spectral response $g(\\lambda)$ and graph filter $g(T)$ directly when function $g$ comes from some constrained set. By observing an interesting connection between this problem and results from functional calculus, this paper presented a principal design for directed graph filters and two feasible approximations: faber polynomials and resolvent. Furthermore, the stabilities of these graph filters are established, and the simulations show that the new method outperforms other directed GNN baselines over several benchmark datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "I believe the most important contribution of this paper is that the authors point out the fact that we do not really need Graph Fourier Transform to define graph filters, which has always been an obstacle in graph learning and signal processing community due to the non-symmetric nature of directed Laplacian. The literature review and observations presented in this paper can be viewed as a general guidance to design convolutional kernels for directed graphs, and lay the theoretical foundations for other future works on this subject. Following the general design, this paper also introduced two computationally feasible filter banks to implement the directed graph kernels in practice, which achieves good results on standard benchmark datasets."
                },
                "weaknesses": {
                    "value": "Although the technical insights shown in this paper are great and very interesting, the presentation of this paper, especially the main text, can be improved. I understand that the authors want to convey as much information as possible within the main text, but the main goal of this paper, which is the implementation of the practical directed convolutional kernels (layers), should still be explained clearly in detail. For example, after reading the entire paper, I still do not know if we need to perform the eigendecomposition of the graph operator or not. In other words, even if we set the function to be faber polynomials $\\Psi_k(\\lambda)=\\lambda^k/2^k$, what will $\\Psi_k(T)$ in this case, where $T$ is the operator? Do we still have to consult Eq (3) to compute $\\Psi_k(T)$, which can be computationally expensive? This type of question should be clearly explained in the main text to help readers better understand the big picture. A pseudocode of the algorithm/training procedure can help solve this issue.\n\nBesides the computation of graph kernels, some of the heuristic choices adopted in Holonets lack theoretical insights as well. For example, why do we need to use forward and backward filters separately? Why the operator is chosen to be $T=(D^{in})^{-1/4}W(D^{out})^{-1/4}$? Why use absolute value as the nonlinear activation, which is unusual in GNN literature? All of these choices seem to just come out of trial and error in simulations. It would be good if the authors could provide some further explanations for these choices.\n\nOne minor concern that I have is about the simulation of directed graph regression problems. I am not very convinced why we should treat this as a directed graph problem, and how it helps when we consider the edges to be directed and the nodes to be weighted. Maybe it has some physical/chemical meanings, but since I am not an expert in physics, I will leave this to other reviewers to decide.\n\nTwo other small comments: I would not describe the difference between the performance of FaberNet and that of Dir-GNN to be significant, as most of the differences are within one standard deviation. Also from the implementation side, Dir-GNN is much simpler than FaberNet; on page 5, \"Faber polynomials provide near near mini-max polynomial approximation\" there are two \"near\"."
                },
                "questions": {
                    "value": "Besides the ones listed above, I have the following questions as well:\n\n1. What is the choice of $y$ for Dir-ResolvNet?\n\n2. Why is the normalization for $T$ $-1/4$ not $-1/2$? If in undirected case, I believe the practice is to use $D^{-1/2}$. Why is there a discrepancy? Also, I am not sure how this is related to the motivation stated in the paper \"We thus use as characteristic operator a matrix that avoids direct comparison of feature vectors of a node with those of immediate neighbours\".\n\n3. Where do the imaginary weights in FaberNet come from? Is it because the eigenvalues can be complex?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3367/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698816980560,
            "cdate": 1698816980560,
            "tmdate": 1699636286806,
            "mdate": 1699636286806,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tcvVYOBwJr",
                "forum": "EhmEwfavOW",
                "replyto": "6sY90lu0N8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3367/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3367/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Followed the received advice dilligently, answered raised questions and added more detail to discussions of architectures/design choices"
                    },
                    "comment": {
                        "value": "We would like to sincerely thank the reviewer for the careful review of our paper! We were especially happy to read that \n> the technical insights shown in this paper are great and very interesting\n\n and that \n>[our paper] lay[s] the theoretical foundations for other future works on [convolutional kernels for directed graphs].\n\nLet us address raised points individually:\n\n* \"Although the technical insights shown in this paper are great and very interesting,  [...] the implementation of the practical directed convolutional kernels (layers), should still be explained clearly in detail.\"\n\nWe agree that our paper is written to serve a dual purpose: To --on the one hand-- develop the theory of spectral convolutions on directed graphs in detail so that it may be  taken up by the community for further research and on the other hand to establish our own specific architectures conforming to the developed framework.\n\nFollowing this advice by the reviewer, we have now put additional emphasis on introducing and describing our specific architectures in more detail, as we explain below.\n\n\n* \"For example, after reading the entire paper, I still do not know if we need to perform the eigendecomposition of the graph operator or not.\"\n\nWe thank the reviewer for this important and indeed central question.  The answer is that an explicit eigendecomposition (or in fact a Jordan Chevalley decomposition in the directed setting) NEVER needs to be computed in our approach. \n\nFollowing this question by the reviewer, we have now modified the final paragraph of the corresponding Section 3.2 in our updated manuscript. It now reads:\n> \u201cFor us, the spectral response (4) provides guidance when considering scale-insensitive convolutional filters on directed graphs in Sections 3.3 and 4 below. The spectral response (4) is however never used to _implement_ filters: As discussed above, this is achieved much more economically via (3).\u201d\n\n* \"[...] if we set the function to be faber polynomials $\\Psi_k(\\lambda) = \\lambda^k/2^k$, what will $\\Psi_k(T)$ in this case, where $T$ is the operator? \"\n\nIf we have $\\Psi_k(\\lambda) = \\lambda^k/2^k$ then we exactly have \n\n$$ \\Psi_k(T) = \\frac{1}{2^k}T^k.$$\n\nWe had initially discussed this effect of applying the holomorphic functional calculus to monomials and polynomials at the end of Section 3.1. Following this valid point raised by the reviewer, we have -- for additional clarity -- promoted said discussion at the end of Section 3.1 into a Theorem (namely Theorem 3.1 in our updated manuscript) into its own right for heightened visibility. \n\nAdditionally, we now also explicitly provide the above result of applying the Faber polynomial $\\Psi_k(\\lambda)$ to the operator $T$ already in Section 3.3.1 (i.e. precisely when Faber Polynomials are first introduced).\n\n\n\n* \"Do we still have to consult Eq (3) to compute $\\Psi_k(T)$, which can be computationally expensive? \"\n\nThe answer to this question is a resounding NO:\n\nEquation (3) never needs to be used to implement a filter in practice. The purpose of this equation is instead to serve as a theoretical tool to spectrally interpret the action of given filters. Additionally it provides guidance on the choice of suitable filters adapted to a given task.\n\nTo avoid a costly explicit Jordan-Chevalley decomposition, our model instead uses parametrized spectral convolutional filters, which were introduced and discussed in the first paragraph of Section 3.4.\n\nThis is indeed an important point brought up by the reviewer, and \u2013 as already mentioned above \u2013 we now explicitly state the following in our revised manuscript \n[Note that equation (4) in the revised document refers to equation (3) in the original submission]:\n>\u201cFor us, the spectral response (4) provides guidance when considering scale-insensitive convolutional filters on directed graphs in Sections 3.3 and 4 below. The spectral response (4) is however never used to _implement_ filters: As discussed above, this is achieved much more economically via (3).\u201d.\n\n* \"This type of question [as above] should be clearly explained in the main text to help readers better understand the big picture.\"\n\nWe hope that the provided modifications in our updated manuscript have clarified these points. Should this not yet be the case, we will of course be more that happy to act on any additional feedback.\n\n* \"pseudocode of the algorithm/training procedure can help solve this issue.\"\n\nApart from the aforementioned added and modified discussions, we have also included a pseudocode of the forward of our model  in our revised manuscript (c.f. Appendix J) which we reference in the main text of our revised manuscript."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3367/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700311362140,
                "cdate": 1700311362140,
                "tmdate": 1700311362140,
                "mdate": 1700311362140,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fiFPx5sTdD",
                "forum": "EhmEwfavOW",
                "replyto": "6sY90lu0N8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3367/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3367/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Discussing Design Choices II and Providing additional Details on Experiments"
                    },
                    "comment": {
                        "value": "2) Why is the normalization chosen as $D^{-frac14}$ as opposed to $D^{-frac12}$ ?\n\nIt is indeed true that a symmetric normalization by $D^{-frac12}$ is traditional in the undirected setting. Historically, this can be traced back to the utilization of the symmetrically normalized Laplacian as providing the Fourier-atoms (i.e. the Laplacian eigenvectors) of the graph Fourier Transform that underlies e.g. the original undirected spectral models such as (Bruna et al., 2014), (Defferrard et al., 2016) (Kipf & Welling, 2017) [using the nomenclature of the Bibliography of our submission]. \n\nA main point of the present paper however, is transcending this graph Fourier transform, as the traditional approach is limited to undirected graphs. Our approach instead does not need to be based on any underlying Laplacian on the graph: Indeed, the holomorphic functional calculus may be applied to arbitrary operators. \n\nAs such, a traditional holding-on-to a normalization stemming from a traditional use of a Laplacian-based Graph-Fourier transforms is no longer necessary and we want to point this out to the community. As we observed experimentally, such a traditional normalization is also (slightly) disadvantageous, as far as performance is concerned.\n\n\n* \"Why use absolute value as the nonlinear activation, which is unusual in GNN literature?\"\n\nIt is indeed true that a large percentage of the machine learning literature in general uses some form of ReLU activations. In the graph setting, an exception is constituted by graph-scattering-networks (c.f. e.g. [1,2]): Corresponding scattering architectures are based on the  absolute-value-nonlinearity.\n\nFor us, the choice to use either ReLu or the absolute value arises as our networks are potentially complex. In the complex setting there is no clear consensus which activation function performs best (c.f. e.g. [3]). Hence we consider two possible choices in our architecture, whenever we enable complex parameters (c.f.also Table 7 in Appendix I)\n\n\n* \"All of these choices seem to just come out of trial and error in simulations. It would be good if the authors could provide some further explanations for these choices.\"\n\nWe hope that we were able to clarify these points. As mentioned above, we have added corresponding discussions of the points raised by the reviewer in our revised manuscript.\n\n\n* \"One minor concern that I have is about the simulation of directed graph regression problems. I am not very convinced why we should treat this as a directed graph problem, and how it helps when we consider the edges to be directed and the nodes to be weighted. Maybe it has some physical/chemical meanings, but since I am not an expert in physics, I will leave this to other reviewers to decide.\"\n\nOur view of this matter is as follows: \n\nOur aim in this present paper is to establish within the graph learning community that the use of spectral methods on directed graphs is not limited to achieving  state of the art performance on node-classification: HoloNets can e.g. also be used at the graph level, to construct networks that are transferable between directed graphs describing the same underlying object at different resolution scales.\n\nWhile transferability is certainly an established research topic for graph neural networks, the approach via of such multiscale-consistency has only recently started to be investigated. As such standard benchmarks have not yet been established; especially not in the directed setting. We thus modify a standard dataset (i.e. QM7) that was recently used to test for multiscale-consistency in the undirected setting (c.f. Koke et al. (2023)) in order to test for such consistency in the directed setting. \n\nWhether a molecule is represented as a directed or undirected graph is irrelevant from the perspective of retained physical information: Both descriptions may be translated into each other.\nIn either case, the node-weights arise naturally as a way to represent additive (under combining nodes) node-wise information, such as charges (like in the case at hand), particle numbers (e.g. Neutron- or Proton numbers) or masses corresponding to individual nodes/atoms.\n\n* \"I would not describe the difference between the performance of FaberNet and that of Dir-GNN to be significant, as most of the differences are within one standard deviation. Also from the implementation side, Dir-GNN is much simpler than FaberNet; \"\n\nThat is a valid point. We have amended the corresponding statement about the comparison between Dir-GNN and Faber-Net in Section 5.1.\n\n* \"on page 5, \"Faber polynomials provide near near mini-max polynomial approximation\" there are two \"near\".\"\n\nWe thank the reviewer for spotting this typo, which we have now correced."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3367/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700312368095,
                "cdate": 1700312368095,
                "tmdate": 1700312390643,
                "mdate": 1700312390643,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "o29r4yeFwq",
                "forum": "EhmEwfavOW",
                "replyto": "6sY90lu0N8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3367/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3367/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Discussing Final Questions and References"
                    },
                    "comment": {
                        "value": "* \"What is the choice of $y$  for Dir-ResolvNet?\n\nWe have chosen $y = -1$ for simplicity. We had already detailed this in \u201cAppendix I: Additional Details on Experiments\u201d, but have now also included this in the main text of our paper in our revised manuscript.\n\n* \"Where do the imaginary weights in FaberNet come from? Is it because the eigenvalues can be complex?\"\n\nYes, that can indeed be thought of as the underlying reason. \n\nIn more detail: If the underlying graph is directed, the associated characteristic operators $T$ are generically not self-adjoint. Hence eigenvalues of such operators are generically complex. If one intends to apply a function $g$ to such a matrix $T$, this necessitates $g$ to be defined at least in a neighbourhood of each eigenvalue of $T$, as can e.g. be seen from the spectral response discussed in the paper. Thus $g(z)$ needs to be defined for complex $z$. If one represents such a function via simpler functions (e.g. a polynomial $g(z)$ represented via a sum of monomials $z^k$ as $g(z) = \\sum_k a_k z^k$ ), the corresponding coefficients $a_k$ are generically complex. These coefficients precisely constitute the learnable parameters in our method.\n\nReferences:\n\n[1]: Michael Perlmutter, Feng Gao, Guy Wolf, and Matthew Hirn. Understanding graph neural networks with asymmetric geometric scattering transforms, 2019. URL https://arxiv.org/abs/1911.06253.\n\n[2]: Christian Koke and Gitta Kutyniok. Graph scattering beyond wavelet shackles. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2020, November 28 - December 9, 2022, New Orleans. OpenReview.net, 2023. URL https://openreview.net/forum?id=ptUZl8xDMMN.\n\n[3]: ChiYan Lee, Hideyuki Hasegawa, and Shangce Gao. Complex-valued neural networks: A comprehensive survey. IEEE/CAA Journal of Automatica Sinica, 9(8):1406\u20131426, 2022. doi:10.1109/JAS.2022.105743."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3367/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700312592736,
                "cdate": 1700312592736,
                "tmdate": 1700312686762,
                "mdate": 1700312686762,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Rrir97CsHq",
                "forum": "EhmEwfavOW",
                "replyto": "o29r4yeFwq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3367/Reviewer_tzrV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3367/Reviewer_tzrV"
                ],
                "content": {
                    "comment": {
                        "value": "I really appreciate the authors' effort to prepare this detailed response. It took me a bit long to read through all the content. I would suggest next time the authors can use a different color in the revision to mark the modified text. I have no further questions for now."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3367/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700632876305,
                "cdate": 1700632876305,
                "tmdate": 1700632876305,
                "mdate": 1700632876305,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]