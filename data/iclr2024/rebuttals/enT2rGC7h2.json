[
    {
        "title": "Impact of Agent Behavior in Distributed SGD and Federated Learning"
    },
    {
        "review": {
            "id": "KNBRif1flJ",
            "forum": "enT2rGC7h2",
            "replyto": "enT2rGC7h2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission412/Reviewer_qXnB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission412/Reviewer_qXnB"
            ],
            "content": {
                "summary": {
                    "value": "The manuscript studies the asymptotic convergence of a generalized distributed SGD method (GD-SGD) for distributed leaning problem. The authors consider various communication patterns and different sampling strategies, including iid sampling and Markovian sampling, for GD-SGD. They show the influence of sampling strategies on the limiting covariance matrix according to the definition of Loewner ordering, which is also examined in a regularized logistic regression task."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The authors analyze the asymptotic convergence of the D-SGD algorithm under more general communication topologies and different sampling strategies including iid sampling and Markovian sampling. The theoretical analysis seems solid.\n\n2. The paper is well-written and easy to follow."
                },
                "weaknesses": {
                    "value": "1. There have been many studies on communication topology in existing work, e.g., [Koloskova et al. (2020), Wang et al. (2021)]. Generally speaking, as long as assumption 2.5 is made, the consistency of the distributed learning algorithm can be guaranteed, so the GD-SGD algorithm designed in this paper is not novel.\n\n2. Technically, the main proof techniques used in the paper can be found in [Li et al. (2022)] and [Hu et al. (2022)], except for the expansion of the communication patterns. Therefore, combined with the first weakness, the technical contribution of the paper is insufficient.\n\n3. The analysis and comparison of different sampling strategies in Cor. 3.4 are trivial. The authors only give a qualitative comparison of different sampling strategies based on existing work [Hu et al. (2022)]. In fact, this simple relationship can be easily generalized in existing works with both asymptotical and non- asymptotical results. From this point of view, the contribution of this article seems to be over-claimed.\n\n4. Logistic regression is a toy model, it is better to further consider other real-world models.\n\nReference:\n\nAnastasia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, and Sebastian Stich. A unified theory of decentralized sgd with changing topology and local updates. In International Conference on Machine Learning, pp. 5381\u20135393, 2020.\n\nWang, Jianyu, and Gauri Joshi. \"Cooperative SGD: A unified framework for the design and analysis of local-update SGD algorithms.\" The Journal of Machine Learning Research 22.1 (2021): 9709-9758.\n\nXiang Li, Jiadong Liang, Xiangyu Chang, and Zhihua Zhang. Statistical estimation and online inference via local sgd. In Proceedings of Thirty Fifth Conference on Learning Theory, volume 178 of Proceedings of Machine Learning Research, pp. 1613\u20131661, 02\u201305 Jul 2022.\n\nJie Hu, Vishwaraj Doshi, and Do Young Eun. Efficiency ordering of stochastic gradient descent. In Advances in Neural Information Processing Systems, 2022."
                },
                "questions": {
                    "value": "One of the key concern of the reviewer is on the fundamental difference in proof techniques compared to [Li et al. (2022)] and [Hu et al. (2022)]; the authors should properly address this. \n\nAnother concern of the reviewer is that the results established in this paper are in an asymptotic sense; can these results be extended to non-asymptotic ones?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission412/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698827370105,
            "cdate": 1698827370105,
            "tmdate": 1699635967731,
            "mdate": 1699635967731,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zVOBO1dNRp",
                "forum": "enT2rGC7h2",
                "replyto": "KNBRif1flJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission412/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission412/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer qXnB (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their detailed comments. Following are our detailed responses to the questions posed.\n\n>### (Q1). There have been many studies on communication topology in existing work, e.g., [Koloskova et al. (2020), Wang et al. (2021)]. Generally speaking, as long as assumption 2.5 is made, the consistency of the distributed learning algorithm can be guaranteed, so the GD-SGD algorithm designed in this paper is not novel.\n\nAnswer: We acknowledge that Assumption 2.5 is indeed a common condition to guarantee the consistency of various distributed learning algorithms. However, the primary aim of our paper is not to introduce a novel distributed learning algorithm per se. Rather, our focus is on providing a detailed asymptotic analysis within the unified framework of *generalized* decentralized SGD (GD-SGD), by showing the consistency (almost sure convergence) as well as asymptotic normality via CLT, with which we can single out the impact of each agent on the overall performance. The essence of our contribution lies in elucidating the effects of different sampling strategies employed by individual agents in a distributed learning setting. This aspect of our research is particularly significant as it addresses nuances that are not captured by the current non-asymptotic analysis frameworks, as highlighted in Table 1 of our paper. \n\n>### (Q2). Technically, the main proof techniques used in the paper can be found in [Li et al. (2022)] and [Hu et al. (2022)], except for the expansion of the communication patterns. Therefore, combined with the first weakness, the technical contribution of the paper is insufficient.\n\nAnswer: Please refer to the response to (Q1) in the \u2018response to all reviewers\u2019 part.\n\n>### (Q3). The analysis and comparison of different sampling strategies in Cor. 3.4 are trivial. The authors only give a qualitative comparison of different sampling strategies based on existing work [Hu et al. (2022)]. In fact, this simple relationship can be easily generalized in existing works with both asymptotical and non- asymptotical results. From this point of view, the contribution of this article seems to be over-claimed.\n\nAnswer: Our study, indeed, uses efficient sampling strategies like non-backtracking random walks and shuffling methods as an application of our main CLT result in Theorem 3.3. However, these examples serve merely as a demonstration of the broader applicability of our theoretical findings rather than being the focal point of our study. \nThe core of our contribution lies in the generality of our model and the unified results we offer within the broad framework of decentralized learning, where we uncover that the sampling strategy of each individual agent affects the overall performance while the effect of communication pattern contributes only via its leading eigenvector (stationary distribution) under the most general setup. This approach is fundamentally different from the specific settings explored in [Hu et al. (2022)]. While [Hu et al. (2022)] focus on certain types of sampling in single-agent settings, our work represents not just a straightforward extension of these concepts into a much more complex multi-agent environment with versatile communication patterns, but a substantial expansion in technical analysis in handling the consensus error among multiple agents under Markovian sampling with increasing communication interval. \nAs we have highlighted in the response to (Q1) in the \u2018response to all reviewers\u2019 part, finite-time analyses, while valuable, do not differentiate the impact of each individual agent\u2019s sampling strategy to the same extent as our asymptotic approach. They also fail to isolate long-lasting factors that significantly influence performance over extended periods. While certain communication matrices might offer short-term benefits, our numerical results in Figure 2 demonstrate that they have minimal long-term effects on the MSE with the same sampling strategy setup, which is supported by findings in [Pu et al. 2020, Olshevsky 2022]. In contrast, our new results under most general settings affirm that the sampling strategies of agents have a lasting and more substantial influence on the system's performance.\n\n>Pu, S., Olshevsky, A., & Paschalidis, I. C. Asymptotic network independence in distributed\nstochastic optimization for machine learning: Examining distributed and centralized stochastic\ngradient descent. IEEE signal processing magazine, 37(3):114\u2013122, 2020.\n>\n>Olshevsky, A. Asymptotic network independence and step-size for a distributed subgradient\nmethod. Journal of Machine Learning Research, 23(69):1\u201332, 2022."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission412/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700362631354,
                "cdate": 1700362631354,
                "tmdate": 1700362631354,
                "mdate": 1700362631354,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "26miErjdPc",
                "forum": "enT2rGC7h2",
                "replyto": "KNBRif1flJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission412/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission412/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer qXnB (2/2)"
                    },
                    "comment": {
                        "value": ">### (Q4). Logistic regression is a toy model, it is better to further consider other real-world models.\n\nAnswer: We understand the importance of demonstrating the applicability of our GD-SGD algorithm to more complex, real-world scenarios. In response to this, we would like to clarify that we have also performed the simulation with a non-convex objective function in Appendix G in our original submission. In this additional simulation, the GD-SGD algorithm has not yet reached the asymptotic regime, and the communication patterns (e.g., size of partial client sampling set, communication interval, and communication matrix) still significantly influence the MSE. Notably, the core finding from our study remained consistent: enhanced sampling strategies employed by a subset of agents resulted in faster convergence rates across almost all time periods. This result is particularly insightful as it suggests that employing effective sampling strategies at the level of individual agents can mitigate performance losses that might arise from less frequent aggregation. Such a strategy is valuable in reducing communication costs, whether among agents or between agents and a central server.\n\n>### (Q5). The results established in this paper are in an asymptotic sense; can these results be extended to non-asymptotic ones?\n\nAnswer: Please see the response to (Q2) in the \u2018response to all reviewers\u2019 part."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission412/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700362703478,
                "cdate": 1700362703478,
                "tmdate": 1700362703478,
                "mdate": 1700362703478,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "t4QFq9DTzn",
                "forum": "enT2rGC7h2",
                "replyto": "26miErjdPc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission412/Reviewer_qXnB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission412/Reviewer_qXnB"
                ],
                "content": {
                    "title": {
                        "value": "Response to the rebuttal"
                    },
                    "comment": {
                        "value": "The reviewer thank the authors's effort in the reply which have partially addressed the reviewer's concerns. The reviewer would thus maintain the score."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission412/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700728459826,
                "cdate": 1700728459826,
                "tmdate": 1700728459826,
                "mdate": 1700728459826,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "aWGiTaFNZE",
            "forum": "enT2rGC7h2",
            "replyto": "enT2rGC7h2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission412/Reviewer_iur4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission412/Reviewer_iur4"
            ],
            "content": {
                "summary": {
                    "value": "This work revolves around distributed learning and specifically studies  the asymptotic behavior of Generalized Distributed Gradient SGD under various communication patterns and sampling strategies. The authors provide theoretical results showing asymptotic consensus convergence across clients and analyze the impact of different sampling strategies on the limiting covariance matrix. Those results provide useful insights and the generalized framework under consideration incorporates numerous results as special cases such as SGD and Distributed SGD. Experimental results on CIFAR10 further support the theoretical findings."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "-This paper studies an interesting framework in distributed learning. Analyzing the Generalized Distributed SGD provides useful insights and the derived theoretical results are aligned with the results from numerous prior works (observed as special cases).\n\n-The importance of sampling strategies for the convergence rate is being explored as well as different communication patterns in Generalized Distributed SGD."
                },
                "weaknesses": {
                    "value": "-The theoretical results of this paper appear to be straightforward extensions of existing works (Morral et al., 2017; Koloskova et al., 2020; Hu et al., 2022). As a result the theoretical contribution, novelty and impact of this work appears to be marginal.\n\n-The analysis although insightful is asymptotic in nature which somewhat diminishes the impact of the results.\n\n-Although, there is extensive description on how the current findings are aligned with known results, the authors do not emphasize enough on the new challenges they had to overcome in order to derive their theoretical results or discuss how their work is more challenging from related works. \n\n-The structure of the introduction could be improved curving out a related work section.\n\n-The experimental results provided are limited to the CIFAR10 dataset."
                },
                "questions": {
                    "value": "See weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission412/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699320180733,
            "cdate": 1699320180733,
            "tmdate": 1699635967650,
            "mdate": 1699635967650,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Oa7000NHr0",
                "forum": "enT2rGC7h2",
                "replyto": "aWGiTaFNZE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission412/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission412/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer iur4"
                    },
                    "comment": {
                        "value": "Our sincere thanks for the in-depth review of our paper. We now answer the question posed by the reviewer.\n\n>### (Q1). The theoretical results of this paper appear to be straightforward extensions of existing works (Morral et al., 2017; Koloskova et al., 2020; Hu et al., 2022). As a result the theoretical contribution, novelty and impact of this work appears to be marginal.\n>### (Q2). Although, there is extensive description on how the current findings are aligned with known results, the authors do not emphasize enough on the new challenges they had to overcome in order to derive their theoretical results or discuss how their work is more challenging from related works.\n\nAnswer: We believe that both questions are relevant to the technical novelty of this paper and to technical comparisons with existing works. We refer the reviewer to our response to (Q1) in the \u2018response to all reviewers\u2019 part.\n\n>### (Q3). The analysis although insightful is asymptotic in nature which somewhat diminishes the impact of the results.\n\nAnswer: Please see our response to (Q2) in the \u2018response to all reviewers\u2019 part.\n\n>### (Q4). The structure of the introduction could be improved curving out a related work section.\n\nAnswer: We thank the reviewer for the comment regarding the structure of our paper. We acknowledge that in the current structure of our introduction, the discussion of related works is closely intertwined with our motivations for undertaking this research. This integration was intentional, as it helps to contextualize our study within the existing body of literature while simultaneously highlighting the unique aspects and motivations of our work. As such, extracting these discussions into a separate related work section would present some challenges, potentially disrupting the flow and coherence of our narrative. \n\nHowever, in light of your feedback and the valuable comments from other reviewers, we have taken steps to enhance the 'Influence of Agent\u2019s Sampling Strategy' paragraph in the introduction. This revised part now includes a more detailed technical comparison with existing works. By doing so, we aim to provide a clearer understanding of how our study is positioned relative to the current state of research, while also emphasizing the technical contributions and motivations behind our work. We believe that these revisions will address your concerns about the structure, ensuring that the introduction effectively sets the stage for our research."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission412/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700362465148,
                "cdate": 1700362465148,
                "tmdate": 1700362465148,
                "mdate": 1700362465148,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PGsxAdGBAR",
                "forum": "enT2rGC7h2",
                "replyto": "Oa7000NHr0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission412/Reviewer_iur4"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission412/Reviewer_iur4"
                ],
                "content": {
                    "title": {
                        "value": "Post Rebuttal"
                    },
                    "comment": {
                        "value": "After thoroughly reading the rebuttal and the comments from the other reviewers I would like to first thank the authors for their efforts to address my concerns. However despite their additional clarifications regarding the theoretical contributions of the paper (although beneficial and helpful in better understanding the new challenges) I still believe that the novelty and impact of these results are limited. On top of that the experimental results presented are insufficient (as noted before) and therefore I am not currently inclined to change my score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission412/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700721939508,
                "cdate": 1700721939508,
                "tmdate": 1700721939508,
                "mdate": 1700721939508,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3fhXY3pB0v",
            "forum": "enT2rGC7h2",
            "replyto": "enT2rGC7h2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission412/Reviewer_bYV1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission412/Reviewer_bYV1"
            ],
            "content": {
                "summary": {
                    "value": "This paper provides an asymptotic convergence analysis of generalized distributed SGD (i.e., with a time-varying communication graph, c.f., Kolosokova et al.). It underlines the dependence of the limiting covariance matrix on each client's data-sampling strategy. The paper's main contribution is identifying that while non-asymptotic analyses of GD-SGD using Markovian sampling rely on the mixing time of the worst agent, the asymptotic analysis can benefit from every agent (not just the slowest one), improving their sampling strategies (c.f., Corollary 3.4). Simulations are provided to judge how quickly optimization enters the asymptotic phase and whether client sampling strategies affect the convergence rate."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is well-written, and the results are rigorously discussed. The paper highlights an essential difference between asymptotic and finite time bounds and how the latter might sometimes be misleading while looking at client sampling strategies. While the idea of looking at asymptotic regimes and Markovian sampling is not new (as can be seen in Table 1), the paper offers an interesting insight."
                },
                "weaknesses": {
                    "value": "I feel that technical comparison to existing work is lacking. While the table summarizes the existing results and what settings they operate in, it does not discuss what are precisely the bounds obtained by papers such as Doan et al. (2017). As a result, it is unclear whether these bounds actually fail to capture the effect of sampling on all the clients. All the results in this paper require Assumption 2.3. Was that required by the previous papers as well? The experiments at least seem to suggest that an increasing number of local steps is not needed (I am assuming a constant step size was used in the experiments)."
                },
                "questions": {
                    "value": "- Can the authors comment on technical comparison to related works, as I mentioned above? \n- What were the technical challenges of going to the distributed setting from the known serial analyses? Are any novel techniques needed? Theorem 3.2 seems like a corollary for an existing result.\n- What is the step-size schedule in the experiments?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission412/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699457605116,
            "cdate": 1699457605116,
            "tmdate": 1699635967589,
            "mdate": 1699635967589,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7VVaeD5fyk",
                "forum": "enT2rGC7h2",
                "replyto": "3fhXY3pB0v",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission412/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission412/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer bYV1"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer for the detailed comments. Please find our replies to your questions below.\n\n>### (Q1). I feel that technical comparison to existing work is lacking. While the table summarizes the existing results and what settings they operate in, it does not discuss what are precisely the bounds obtained by papers such as Doan et al. (2017). As a result, it is unclear whether these bounds actually fail to capture the effect of sampling on all the clients. All the results in this paper require Assumption 2.3. Was that required by the previous papers as well? Can the authors comment on technical comparison to related works, as I mentioned above?\n\nAnswer: When the reviewer mentions [Doan et al. (2017)], we guess it is [Doan et al. (2019)] in our reference. Regarding the technical comparison to existing works, we refer the reviewer to bullet point #2 of the response to (Q1) in the \u2018response to all reviewers\u2019 part.\n\nRegarding assumption 2.3, since most of the previous papers focus on the constant communication interval only, we only need assumption 2.3(i), where the usual polynomial step size $\u03b3_n=1/n^a$  for $a\u2208(0.5,1]$ is assumed. Moreover, the only paper we are aware of talking about the increasing communication interval scheme is [Li et al. (2022)] and our assumption 2.3(ii) is slightly stringent than theirs in order to cover more decentralized settings and Markovian sampling, and we have already included the discussion of this assumption in Remark 1 in our original submission.\n\n>### (Q2). What were the technical challenges of going to the distributed setting from the known serial analyses? Are any novel techniques needed? Theorem 3.2 seems like a corollary for an existing result.\n\nAnswer: Please see bullet point #1 of the response to (Q1) in the \u2018response to all reviewers\u2019 part.\n\n>### (Q3). What is the step-size schedule in the experiments? The experiments at least seem to suggest that an increasing number of local steps is not needed.\n\nAnswer: Thank you for highlighting the omission regarding the step-size schedule in our experiments. Your observation has helped us improve the clarity of our simulation setup. In our experiments, we employ a decreasing step size $\u03b3_n=1/n^{0.9}$. This choice is made to ensure compliance with Assumption 2.3 and thus consistency with the theoretical framework we have established. We have now incorporated this detail into the revised version of our paper.\n\nRegarding the increasing communication interval, our findings depicted in Figure 2(b) reveal that the convergence rate is not adversely affected by this scheme. Contrary to the suggestion that increasing the number of local steps may be unnecessary, our results actually indicate a different conclusion. The implementation of an increasing communication interval scheme effectively reduces the frequency of aggregations, either with neighboring agents or a central server. This strategy leads to a reduction in communication costs, which is a significant consideration in distributed learning scenarios. Importantly, we found that this approach has minimal impact on the convergence speed in the asymptotic regime. This insight is particularly valuable as it suggests that we can achieve cost-effective communication without compromising the efficiency of convergence in distributed learning algorithms."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission412/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700362292970,
                "cdate": 1700362292970,
                "tmdate": 1700362292970,
                "mdate": 1700362292970,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rXmxRbZPs1",
                "forum": "enT2rGC7h2",
                "replyto": "7VVaeD5fyk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission412/Reviewer_bYV1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission412/Reviewer_bYV1"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their detailed response. I have gone through the responses and the other reviews. I have decided to retain my score. I believe there are additional technical challenges over previous works, such as dealing with consensus error and non-iid sampling. But again, any extension of serial results to the distributed setting must deal with that analysis. So overall, my impression is that the work closely builds on existing tools in the literature. I appreciate the asymptotic viewpoint and concede that the algorithms presented hit the asymptotic regime in the experiments, thus making it worthwhile to study the regime. \n\nRegarding the experiments, the current experiments do validate the theoretical results, when the local steps are not growing, using $a=0.9$. However, why do the authors use this step size when logarithmically growing the local steps? Overlooking this issue, I would have liked to see how other step-size schemes pan out. In particular, if the step size were tuned optimally (i.e., tuning $a$) for each instance for a fixed number of time steps, I would imagine shifting the asymptotic regime for different instances. This could, in turn, change the relative performance of different sampling schemes. The non-convex experiments offered in the appendix are very \"convex-like\", and it would be good to have more comprehensive experiments using even a simple neural network. Finally, the simulation doesn't have data heterogeneity. I believe this makes it harder to comprehend the differences between the agents, which the paper claims is a benefit of the asymptotic analysis over the non-asymptotic one.\n\nThe authors write in response to the reviewer qxNb: \n\n> The core of our contribution lies in the generality of our model and the unified results we offer within the broad framework of decentralized learning, where we uncover that the sampling strategy of each individual agent affects the overall performance while the effect of communication pattern contributes only via its leading eigenvector (stationary distribution) under the most general setup.    \n\nIn the current write-up, this takeaway is obfuscated by the technical results. The discussion below corollary 3.4 can be revised with more examples. This relates to the limitation that the authors do not discuss the practical relevance of their results. Which federated learning applications can benefit from non-iid sampling, or where can the Markovian sampling suggested in this paper be implemented efficiently? Is there a natural decentralized setting where this is possible? What is the additional computational cost of doing this? How can this be implemented in online settings where the data is not stored on the device? Some of these questions might have simple answers, but providing this context is important, otherwise, the work comes off as a mechanical composition of two existing techniques: asymptotic analyses and consensus error-based analyses---something most reviewers have complained about. While there is not much time left in the discussion period, hopefully the authors can address these issues in their revision."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission412/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700708822142,
                "cdate": 1700708822142,
                "tmdate": 1700708822142,
                "mdate": 1700708822142,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]