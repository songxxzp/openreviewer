[
    {
        "title": "Graph Generation with Destination-Predicting Diffusion Mixture"
    },
    {
        "review": {
            "id": "cWcdAmv3FQ",
            "forum": "UQVhOVhUi4",
            "replyto": "UQVhOVhUi4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3082/Reviewer_HfR4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3082/Reviewer_HfR4"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a novel diffusion-based graph generation framework. The framework leverages the process conditioning on the generation destination, which aims to more accurately capture the graph topology. The training objective in this framework can be approximately represented as to predict a weighted mixture/average of the data.  The empirical evaluation of 2D/3D graph generation shows the effectiveness of the proposed methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ This work introduces a new diffusion-based graph generation framework where the generation procedure is conditioned on the destination graph. The framework at a high level combines the benefits of the previous discrete diffusion process for graphs (DiGress) that directly predict the destination, and the continuous diffusion process to handle the potential 3D continuous features that 3D graphs may have. \n \n+ The derivation seems reasonable at a high level. (I have not checked the proof). \n\n+ The empirical results show the effectiveness of the framework."
                },
                "weaknesses": {
                    "value": "- From the technical side, the framework seems to directly leverage the tool from [1][2]. So, the fundamental technique is not novel, though the application to graph generation may be novel. Moreover, the derivation seems to be nothing specified to graph topology but quite generic. So, it is not persuasive that the adopted framework may capture graph topology better than other frameworks. The benefits of practical performance seem to entirely inherit from the previously developed framework [1][2].\n \n- Eq.9 seems to just estimate the exact endpoint $G_T$ instead of the destination mixture $D(G_t, t)$. However, Eq.8 is to estimate $D(G_t, t)$, isn't it? This seems a conflict with the statement just below Eq. 9.\n\n[1] Diffusion-based molecule generation with informative prior bridges, NeurIPS 2022\n\n[2] Learning Diffusion Bridges on Constrained Domains, ICLR 2023"
                },
                "questions": {
                    "value": "Please address the two weaknesses: 1) why is the framework related to graph topology? The framework seems to be a direct application of previous frameworks to graph generation; 2) why Eq.9 is to estimate a mixture of data points.\n\nI may re-evaluate this work based on the authors' response."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3082/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3082/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3082/Reviewer_HfR4"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3082/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698814836778,
            "cdate": 1698814836778,
            "tmdate": 1700622899321,
            "mdate": 1700622899321,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WzspqzGNYa",
                "forum": "UQVhOVhUi4",
                "replyto": "cWcdAmv3FQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3082/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3082/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Initial Response to Reviewer HfR4 (1/2)"
                    },
                    "comment": {
                        "value": "We sincerely thank you for your constructive and helpful comments. We appreciate your positive comments that \n- Our work introduces a new diffusion-based graph generation framework.\n- Our method has benefits that can directly predict the destination of the diffusion process while being able to handle potential continuous features.\n- Experimental results demonstrate the effectiveness of the framework.\n\nWe provide an updated revision of the paper highlighted in orange, and we address your concerns in our initial response below.\n\n---\n\n**Comment 1-1:**\nThe framework seems to be a direct application of previous frameworks [1, 2] to graph generation. \n\n[1] Wu et al., Diffusion-based molecule generation with informative prior bridges, NeurIPS 2022.\n\n[2] Liu et al., Learning Diffusion Bridges on Constrained Domains, ICLR 2023.\n\n**Response 1-1:**\n**Our framework is not a direct application of [1, 2]** since **the superior ability of DruM to generate valid graphs comes from our contribution in the parameterization and the training objective**. \n\nIn our work, we aim to model the graph topology by directly modeling the graph structure, and thereby our framework learns to **predict the weighted mean of the data**, i.e. destination mixture which is achieved from our **novel parameterization of Eq. (5) and our training objective in Eq. (9)**. \n\nOn the other hand, previous works [1, 2] learn to predict the drift function focusing on improving the generative process by adding prior information to the diffusion process [1] or using domain-specific (e.g., bounded, ordinal) bridges [2], which are not related to graphs. Implicitly modeling the topology through estimating drift is sub-optimal for modeling the discreteness of the graph structure as well as the structural properties. As a result, the **direct application of previous works for graph generation fails to generate valid graphs**. We experimentally validate this in Section 4.4: \n- In Figure 4, learning the drift (denoted as Drift) fails to generate planar graphs resulting in low validity compared to our approach.\n- In Figure 3 (Left), previous work [1] (denoted as Bridge) achieves lower molecule stability compared to our DruM on 3D molecule generation.\n- Especially, DruM outperforms Bridge+Force [1] even without the need of adding domain-specific knowledge (i.e., AMBER Inspired Physical Energy), and achieving $\\\\times$17.5 faster training compared to [1] due to simulation-free training.\n\nMoreover, the difference in the objectives results in a significant difference in training models. **Learning the destination mixture (ours)** is considerably easier compared to learning the drift function ([1,2]) as the drift function diverges approaching the terminal time whereas the destination mixture is supported inside the bounded data space (as explained at the end of Section 3.2). Figure 7 (Appendix D.2) of the updated revision shows that the complexity of the model estimating the destination mixture (ours) is significantly lower than that of estimating the drift function (previous works), especially for the later stage of the diffusion process where the drift function diverges. \n\nNotably, our work provides novel technical contributions that are not covered by the related works as follows:\n- We propose to construct the generative process as a **mixture of the OU bridge process (Eq. (2))**, whereas previous works use the Brownian bridge process which is a special case of our OU bridge process (when $\\\\alpha\\\\rightarrow 0$). \n- We **derive the OU bridge mixture** from the OU bridge processes and present novel parameterization using the destination mixture (Eq. (5)).\n- We propose a **novel training objective (Eq. (9))** that can effectively estimate the destination mixture and further guarantees maximizing the likelihood.\n\nAdditionally, we demonstrate that predicting the destination mixture enables faster convergence of the generative process achieved in the early stage as visualized in Figures 16~19 resulting in valid graphs and stable molecules, which was not studied in previous works."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3082/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700043153786,
                "cdate": 1700043153786,
                "tmdate": 1700199257251,
                "mdate": 1700199257251,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mVsuzczaju",
                "forum": "UQVhOVhUi4",
                "replyto": "cWcdAmv3FQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3082/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3082/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Initial Response to Reviewer HfR4 (2/2)"
                    },
                    "comment": {
                        "value": "**Comment 1-2:**\nWhy is the framework related to graph topology? \n\n**Response 1-2:**\nOur framework is specialized for graph generation in threefold: (1) the motivation, (2) the parameterization and the training objective, and (3) the exploitation of graph structure. \n\nFirst, the motivation for our work is that the key to generating valid graphs is **modeling the discrete structures and the properties determined by the structure**, which previous objectives of graph diffusion models are ill-suited. Due to the discreteness and the combination of node features and adjacency, graph data is distinguished from other data such as images, and it is crucial to model the graph topology for the generation.\n\nFrom this motivation, our goal is to **directly learn the graph structure by estimating the weighted mean of data** (destination mixture), and we propose a novel parameterization of the OU bridge mixture using the destination mixture (Eq. (7)) and further derive novel training objective (Eq. (9)) for estimating the destination mixture.\n\nMoreover, to effectively estimate the destination mixture of graph data, **we exploit the discrete structure of graphs and generate both the node and adjacency simultaneously**. For modeling the discreteness, we utilize an additional function (e.g., sigmoid or softmax) at the end of the model, as explained at the end of Section 3.2 and Section 4.4. Further, we use the framework for attributed graphs (Section A.4) which describes the generative process of both the node features $\\\\textbf{X}$ and the adjacency matrices $\\\\textbf{A}$. These also contribute to the superior performance of DruM.\n\n---\n\n**Comment 2:**\nHow can Eq. (9) be used to estimate the destination mixture?\n\n**Response 2:**\nThis is because **minimizing Eq. (9) is equivalent to minimizing Eq. (8)**, where Eq. (8) is designed so that the model $s\\_{\\\\theta}$ to estimate $\\\\textbf{D}(\\\\textbf{G}\\_t, t)$. As derived in Section A.6, the equivalence is due to the fact that \n- The expectation of Eq. (8) is computed over all the bridge processes in the mixture process (Eq. (47)).\n- The destination mixture is a weighted mean of data (Eq. (49)). \nTherefore, estimating the destination mixture can be achieved by minimizing Eq. (9).\n\nA similar case of equivalence in training objectives can be found in previous works, for example, Song et al. [3]: The denoising score matching, which aims to estimate the score function, is equivalent to the continuous-time score matching objective in Eq. (7) of [3] which minimizes the loss between the model output and the log gradient of the transition distribution. Although Eq. (7) of [3] seems to train the model for estimating the log gradient of the transition distribution, the equivalence guarantees that it actually trains the model to estimate the score function.\n\n[3] Song et al., Score-Based Generative Modeling through Stochastic Differential Equations, ICLR 2021\n\n---\n---\n\nWe thank the reviewer again for their time and feedback, and we hope that our responses have addressed any remaining questions. We hope the reviewer would kindly consider a fresh evaluation of our work given our responses above and the revised paper."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3082/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700043179380,
                "cdate": 1700043179380,
                "tmdate": 1700203974529,
                "mdate": 1700203974529,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tMmTFcLCce",
                "forum": "UQVhOVhUi4",
                "replyto": "cWcdAmv3FQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3082/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3082/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Gentle Reminder for Reviewer HfR4"
                    },
                    "comment": {
                        "value": "Dear Reviewer HfR4,\n\n\nThank you for reviewing our paper. As the interactive discussion phase will end this Wednesday (22nd AOE), we politely ask you to check our responses. We summarize our responses as follows:\n\n\n- We made clear that the superior performance of our framework comes from our novel contribution in the parameterization and the training objective.\n- We clarified that our framework is specialized for generating graphs, from the motivation for modeling the topology to the parameterization and the training objective, as well as the exploitation of the graph structures.\n- We explained that our training objective of Eq. (9) is equivalent to Eq. (8) which is designed for estimating the destination mixture.\n\n\nWe hope that our responses have addressed your concerns, and we hope you kindly consider updating the rating accordingly. Please let us know if there are any other things that we need to clarify or provide. We sincerely appreciate your valuable suggestions.\n\nBest regards, \n\nAuthors"
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3082/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700547416592,
                "cdate": 1700547416592,
                "tmdate": 1700547416592,
                "mdate": 1700547416592,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LDk3gI1egS",
                "forum": "UQVhOVhUi4",
                "replyto": "tMmTFcLCce",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3082/Reviewer_HfR4"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3082/Reviewer_HfR4"
                ],
                "content": {
                    "title": {
                        "value": "Re-evaluation"
                    },
                    "comment": {
                        "value": "Many thanks for the detailed response. I accept the claimed novelty as composed to the references [1][2]. Now, I understand the equivalence between (8) and (9). However, the connection to graph generation is not clear to me. The relation to graphs mentioned in the response sounds related to some implementation specifics, which is irrelevant to a mixture of the OU bridge process, the technical key ingredient of this work. Also, why the adopted method is good for graph generation is also unclear, though as claimed by the authors, the experiments have shown better empirical performance. \n\nOverall, I evaluate think work as a borderline work. I am okay with scores 5 or 6. Given the clear claimed novelty beyond [1,2], I increased my rating to 6.  \n\n[1] Diffusion-based molecule generation with informative prior bridges, NeurIPS 2022\n[2] Learning Diffusion Bridges on Constrained Domains, ICLR 2023"
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3082/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700622870353,
                "cdate": 1700622870353,
                "tmdate": 1700622870353,
                "mdate": 1700622870353,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rrEwa39Qp6",
            "forum": "UQVhOVhUi4",
            "replyto": "UQVhOVhUi4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3082/Reviewer_w5ob"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3082/Reviewer_w5ob"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes Destination-Predicting Diffusion Mixture:\n - While DDPMs derive the generative process by reversing the forward noising process, DruM constructs the generative process from the mixture of OU bridge processes, which does not rely on the time-reversal approach. The mixture process of DruM defines an exact transport from an arbitrary prior distribution to the data distribution by construction."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "Being a graph diffusion paper, the paper makes a very large contribution to the general diffusion literature, as well as the growing literature on diffusion bridge process. The paper, if read with the appendix, is well written. The experiment compares the proposed method to the most comparable recent works and demonstrates great performance."
                },
                "weaknesses": {
                    "value": "- The paper mentions in numerous places that the proposed method captures the topology of the graph distribution because it predicts the destination of the diffusion process. I don't understand the arguments there, or if they are attempting to make one. x0-parameterization of DDPM (e.g. Vignac 2023) by definition predicts the destination. Does it capture topology? In fact, every generative model predicts destination in one way or another (VAE, GAN, and $\\epsilon$-parameterized diffusion models are equivalent to x0-parameterization). Do they capture topology? \"Loss-Guided Diffusion Models for Plug-and-Play Controllable Generation\", for example, attempts to generate a destination \"distribution\" by doing MC on top of an x0-parameterization.\n - The paper assumes that the audience has prior knowledge on bridge processes. The only explanation offered is that they are \"processes conditioned to an endpoint,\" which I find unhelpful. The writing can be improved in this regard.\n - The paper is without a discussion about the graph data structure in use; the omission is made explicit at the end of Sec 3. For this I find the writing difficult to follow at times. How does one take the weighted mean of discrete graphs and molecules? They are finally discussed in appendix B.3, but only in reference to other papers. \n - The main paper can really use an \\Algorithm block from appendix B.2 (although not sure what to remove), to help the readers follow the sampling procedure."
                },
                "questions": {
                    "value": "- See Weaknesses\n - Why graphs? It appears that the methodology proposed isn't at all particular to discrete/combinatorial data structure."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3082/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698829459387,
            "cdate": 1698829459387,
            "tmdate": 1699636253711,
            "mdate": 1699636253711,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SZuSEFLKkO",
                "forum": "UQVhOVhUi4",
                "replyto": "rrEwa39Qp6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3082/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3082/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Initial Response to Reviewer w5ob (1/2)"
                    },
                    "comment": {
                        "value": "We sincerely thank you for your constructive and helpful comments. We appreciate your positive comments that \n- Our work makes a very large contribution to the general diffusion literature as well as the literature on the diffusion bridge process.\n- Our method outperforms the most comparable recent works.\n\nWe provide an updated revision of the paper reflecting your comments highlighted in orange, and initially address your concerns below. \n\n---\n\n**Comment 1-1:**\nDo previous diffusion models ($\\\\epsilon$-parameterized, $x\\_0$-parameterized) capture the graph topology? Every generative model predicts a destination in one way or another.\n\n**Response 1-1:**\nPrevious objectives of graph diffusion models, learning the noise ($\\\\epsilon$) or $x\\_0$, are **ill-suited for capturing the discrete structure of graphs.** \n\nAlthough every generative model predicts a destination in one way or another, $\\\\epsilon$-parameterized diffusion models fail to capture graph topology since they do not directly learn the graph structure from the dataset. Since these models learn to denoise each step to **implicitly** recover the topology, the predicted destination derived from the estimated noise (score) is sub-optimal for modeling the discreteness of the graph structure as well as the structural properties defined by the combination of node features and adjacency. We experimentally validate this by analyzing noise prediction models (ConGress and EDM) and score prediction model (GDSS):\n\n- Please check Figure 2 (Left), which shows that GDSS and ConGress fail to model the spectral topology, and, in Table 1, the high MMD results indicate that these models are unable to model the local graph topology.\n\nOn the other hand, $x\\_0$-parameterized (continuous) diffusion models, that predict the exact destination, are problematic for generating valid structure of graphs, since the **predictions are highly inaccurate in earlier steps**. Making an inaccurate prediction of the structure is critical since due to the discreteness of the graph structure, a small error in the prediction causes a very different topology. We experimentally validate this as follows:\n \n- Figure 8 (Right) shows that predicting the exact destination (EDM-Dest) results in lower molecule stability (35.95%) compared to ours (87.34%), and further shows lower results compared to the $\\\\epsilon$-parameterized model (EDM).\n\nAlthough $x\\_0$ and $\\\\epsilon$-parameterization are mathematically equivalent, **what the model learns is completely different and crucially affects modeling the discreteness of the graph structure**. This is shown by the different performances of EDM and EDM-Dest. \nWe can conclude that the objectives of previous graph diffusion models are sub-optimal for modeling the graph structure.\n\n---\n\n**Comment 1-2:**\nHow does the proposed framework capture the graph topology?\n\n**Response 1-2:**\nInstead of learning the noise ($\\\\epsilon$) or the exact destination ($x\\_0$), our framework learns to **predict the weighted mean of the data** derived from the OU bridge mixture process (Eq. (4) and (5)), i.e., the destination mixture. In our work, we construct the generative process as the OU bridge mixture (Eq. (4)) where the process is driven toward the predicted destination mixture. By the definition of the destination mixture, it converges to the final graph to be generated, and therefore our generative process ends up in the correct structures within the data distribution. Based on the mixture process, we train our model to estimate the destination mixture which can directly learn the graph structure without the concern of making highly inaccurate predictions in the early steps. Therefore, our method can generate graphs with accurate topology, and we experimentally validate this as follows:\n\n- In Figure 2 (Left), DruM perfectly models the spectral topology, resulting in high validity (V.U.N.) compared to previous methods.\n- Figure 3 (Right) shows that DruM can capture the local topological properties of the graphs.\n- Visualization of the generation processes in Figures 16~19 indicates that our method can predict the topology at an early stage."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3082/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700043016587,
                "cdate": 1700043016587,
                "tmdate": 1700043411736,
                "mdate": 1700043411736,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "g0FCDqMR8k",
                "forum": "UQVhOVhUi4",
                "replyto": "rrEwa39Qp6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3082/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3082/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Initial Response to Reviewer w5ob (2/2)"
                    },
                    "comment": {
                        "value": "**Comment 2:**\nIt appears that the methodology proposed isn't at all particular to discrete/combinatorial data structure.\n\n**Response 2:**\nOur framework is designed and specialized for graph generation in threefold: (1) the motivation, (2) the parameterization and the training objective, and (3) the exploitation of graph structure. \n\nFirst, the motivation for our work is that the key to generating valid graphs is **modeling the discrete structures and the properties determined by the structure**, which previous objectives of graph diffusion models are ill-suited. Due to the discreteness and the combination of node features and adjacency, graph data is distinguished from other data such as images, and it is crucial to model the graph topology for the generation.\n\nFrom this motivation, our goal is to **directly learn the graph structure by estimating the weighted mean of data** (destination mixture), and we propose a novel parameterization of the OU bridge mixture using the destination mixture (Eq. (7)) and further derive novel training objective (Eq. (9)) for estimating the destination mixture.\n\nMoreover, to effectively estimate the destination mixture of graph data, **we exploit the discrete structure of graphs and generate both the node and adjacency simultaneously**. For modeling the discreteness, we utilize an additional function (e.g., sigmoid or softmax) at the end of the model, as explained at the end of Section 3.2 and Section 4.4. Further, we use the framework for attributed graphs (Section A.4) which describes the generative process of both the node features $\\\\textbf{X}$ and the adjacency matrices $\\\\textbf{A}$. These also contribute to the superior performance of DruM.\n\nHowever, as you mentioned, the extension of our work can be beneficial for generating other data structures where understanding the topology of the data is crucial.\n\n---\n\n**Comment 3:**\nDetailed explanation of the bridge processes.\n\n**Response 3:** \nDue to the page limit, we provided the **detailed explanations in Appendix A.1**. Yet we agree that adding a detailed explanation would help the readers, and have updated the paper (Section 3.1) explaining that a bridge process can be obtained by conditioning a diffusion process, e.g., Brownian motion or OU process, to an endpoint using Doob\u2019s h-transform.\n\n---\n\n**Comment 4:**\nExplanation of the graph data structure.\n\n**Response 4:**\nA graph $\\\\textbf{G}$ with $N$ nodes is defined by a pair $(\\\\textbf{X}, \\\\textbf{A})$ where $\\\\textbf{X}\\\\in\\\\mathbb{R}^{N\\\\times F}$ is the node features with feature dimension $F$ and $\\\\textbf{A}\\\\in\\\\mathbb{R}^{N\\\\times N}$ is the weighted adjacency matrix. The weighted mean of the graph data can be represented by the pair of the weighted mean of node features and the weighted mean of the adjacency matrices. We have added an explanation of the graph data structure in the updated revision (beginning of Section 3).\n\n---\n\n**Comment 5:**\nThe main paper can use the pseudo-code in Appendix B.2 to help the readers follow the sampling procedure.\n\n**Response 5:**\nAlthough we have referenced Appendix B.2 in the main paper, we agree that moving the pseudo-code to the main paper would help the readers. Due to the page limit, we will move the pseudo-code in the final revision if we have additional pages. We have added a brief explanation of the sampling procedure in the revised paper (Section 3.2).\n\n---\n---\n\nWe thank the reviewer again for their time and feedback, and we hope that our responses have addressed any remaining questions. We hope the reviewer would kindly consider a fresh evaluation of our work given our responses above and the revised paper."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3082/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700043059656,
                "cdate": 1700043059656,
                "tmdate": 1700397814396,
                "mdate": 1700397814396,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "f5YEoNFSE8",
                "forum": "UQVhOVhUi4",
                "replyto": "rrEwa39Qp6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3082/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3082/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Gentle Reminder for Reviewer w5ob"
                    },
                    "comment": {
                        "value": "Dear Reviewer w5ob,\n\n\nThank you for reviewing our paper. As the interactive discussion phase will end this Wednesday (22nd AOE), we politely ask you to check our responses. We summarize our responses as follows:\n\n\n- We clarified that previous diffusion models fail to capture the graph topology as their objectives are sub-optimal for accurately modeling the discrete graph structure and the properties determined by the structure.\n- We made clear that our framework is designed and specialized for the generation of graphs, including the motivation for modeling the topology, our novel parameterization and the training objective, and the exploitation of the graph structures.\n- We revised our paper following your suggestions by adding a detailed explanation of the bridge processes, the graph data structure, and the sampling procedure of our method.\n\n\nWe hope that our responses have addressed your concerns, and we hope you kindly consider updating the rating accordingly. Please let us know if there are any other things that we need to clarify or provide. We sincerely appreciate your valuable comments.\n\nBest regards, \n\nAuthors"
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3082/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700547384073,
                "cdate": 1700547384073,
                "tmdate": 1700547384073,
                "mdate": 1700547384073,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SFYCS5ka3i",
                "forum": "UQVhOVhUi4",
                "replyto": "f5YEoNFSE8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3082/Reviewer_w5ob"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3082/Reviewer_w5ob"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response"
                    },
                    "comment": {
                        "value": "Thank you for the response and the revised manuscript. All my questions were answered. Although I still think \"capturing graph topology\" is a very vague claim, I have a better understanding of what you are trying to convey, and will take it into account in my reevaluation."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3082/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631333190,
                "cdate": 1700631333190,
                "tmdate": 1700631333190,
                "mdate": 1700631333190,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wwK4SqnJyD",
                "forum": "UQVhOVhUi4",
                "replyto": "rrEwa39Qp6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3082/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3082/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for reviewing our paper"
                    },
                    "comment": {
                        "value": "We sincerely appreciate the reviewer\u2019s time and effort in reviewing our paper. We are happy to hear that all your concerns were addressed by our responses. Here, we would like to **clarify more about \u201ccapturing the graph topology\u201d and why it is crucial for graph generation**.\n\nA **major difference between graph data and image data** is that while a slight modification of the edges may significantly change the structure of the graphs and hence its properties, for example, connecting a pair of nodes can break the planarity or a change in the bond type of a molecule can transform the property from beneficial to toxic (e.g., Figure 1 of [1]), it is not the case for images where a slight change in the pixel values does not notably affect its semantic contents or its property (e.g., image class). \n\nTherefore, unlike image data, the key to generating valid graphs is **accurately modeling the discrete structures of graphs and the properties determined by the structure** which corresponds to \u201ccapturing the graph topology\u201d. As explained in *Response 1-1*, previous diffusion models fail to model the discreteness of the graph structures and hence the topological properties such as planarity or clusteredness. In contrast, our work can capture the graph topology by directly learning the graph structure via predicting the weighted mean of data (i.e., the destination mixture), as explained in *Response 1-2*. \n\n\nThank you once again for reviewing our paper and providing valuable comments.\n\n[1] Jo et al., Edge Representation Learning with Hypergraphs, NeurIPS 2021\n\nBest,  \nAuthors"
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3082/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700664060006,
                "cdate": 1700664060006,
                "tmdate": 1700711312943,
                "mdate": 1700711312943,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Gv0E1BqY2n",
            "forum": "UQVhOVhUi4",
            "replyto": "UQVhOVhUi4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3082/Reviewer_txBt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3082/Reviewer_txBt"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to use score function to predict the final graph. The resulting stochastic process is named \"Destination-Predicting Diffusion Mixture (DruM)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The method is theoretically sound. The underlying stochastic process is well defined.\n2. The motivation to forecast the final state at the early de-noising stage is interesting."
                },
                "weaknesses": {
                    "value": "1. The idea of \"prediction results in graphs with correct topology\" is not very convincing.\n2. Empirical results are limited to small graphs.\n3. The proposed method sacrifices novelty in generated graph."
                },
                "questions": {
                    "value": "1. Does Table 4 illustrate a tradeoff between novelty and other capabilities for DruM? If this outcome is a direct consequence of your objective in Eq. (65), would it be advisable to introduce a control mechanism in your algorithm to balance these capabilities?\n2. Is it your view that DruM could be better suited for generating large graphs, prioritizing validity and other capabilities over novelty? Do you believe DruM has the potential to alleviate the scalability bottleneck for larger graphs?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3082/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699242127279,
            "cdate": 1699242127279,
            "tmdate": 1699636253615,
            "mdate": 1699636253615,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kbqIi5qNyU",
                "forum": "UQVhOVhUi4",
                "replyto": "Gv0E1BqY2n",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3082/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3082/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Initial Response to Reviewer txBt (1/2)"
                    },
                    "comment": {
                        "value": "We sincerely thank you for your constructive comments. We appreciate your positive comments that \n- Our method is theoretically sound with a well-defined diffusion framework.\n- Motivation to predict the final state at the early stage is interesting.\n\nWe provide an updated revision of the paper highlighted in orange, addressing all your concerns below.\n\n---\n\nBefore addressing your concern, we would like to **recap the main contributions of our work**:\n- We propose a new graph generative framework that models the graph topology by directly predicting the destination of the generative process as a weighted mean of data. \n- We derive theoretical groundwork for modeling the generative process using the OU bridge mixture and introduce a novel training objective for estimating the destination mixture.\n- Our framework has benefits that can directly predict the destination of the diffusion process while being able to handle potential continuous features.\n- Our method significantly outperforms previous graph generative models on diverse generation tasks.\n\nNow we initially address your concerns below.\n\n---\n\n**Comment 1:**\nThe idea of \"prediction results in graphs with correct topology\" is not very convincing.\n\n**Response 1:**\nOur method is superior in capturing the correct topology because **we directly learn the discrete structure of graphs by learning to predict the destination mixture** which is a weighted mean of data, in contrast to predicting noise or score function as done in previous works. In order to capture the topology, it is best for the model to directly learn the structure since the topology of a graph is its discrete structure and the corresponding properties of the structure.\n\nTo this end, we aim to predict the graph structure as a weighted mean of graph data, namely the destination mixture $\\\\textbf{D}(\\\\textbf{G}\\_t, t)$ (Eq. (1)), which converges to the final graph to be generated by its definition. In our work, we construct the generative process as the OU bridge mixture (Eq. (4)) where the process is driven toward the destination mixture, ending up in the correct structures within the data distribution. Thus we train our model to estimate $\\\\textbf{D}(\\\\textbf{G}\\_t, t)$, and in this way, the prediction of $\\\\textbf{D}(\\\\textbf{G}\\_t, t)$ from our framework results in a graph with accurate topology. \n\nWe empirically validate this through extensive experiments:\n\n- Figure 2 (Left) shows that DruM perfectly models the spectral topology of the dataset through the diffusion process.\n- Figure 2 (Right) shows that DruM can capture the local topological properties of the graph.\n- Visualization of the generation processes in Figures 16~19 indicates that our method can generate accurate graph topology at an early stage.\n\n---\n\n**Comment 2-1:**\nThe proposed method sacrifices the novelty of the generated graphs.\n\n**Response 2-1:**\nThis is a misunderstanding as our method does not sacrifice the novelty. Our **DruM is able to generate valid graphs that are different from that of the training set (novel)**, and the generated graphs are different from each other and are not limited to specific structures, which we experimentally validate as follows:\n\n- For general graph generation, we report the V.U.N. (valid, unique, and novel) metric in Table 1 where ours achieves the highest V.U.N. with 100% novelty. In other words, our method is able to generate valid graphs that are novel. \n- For molecule generation, we report the novelty as well as the uniqueness in Table 5 where ours achieves 99.98% novelty and 99.97% uniqueness for the ZINC250k dataset. Note that the low novelty in the QM9 dataset, similar to that of DiGress, is due to the fact that the molecules in the QM9 dataset only consist of a small number of atoms (up to 9 atoms) of four atom types (C, N, O, F).\n\n| DruM (ours)    | Planar | SBM   | Proteins | QM9   | ZINC250k |\n|----------------|--------|-------|----------|-------|----------|\n| Uniqueness (%) | 100.0  | 100.0 | 100.0    | 96.90 | 99.97    |\n| Novelty (%)    | 100.0  | 100.0 | 100.0    | 24.15 | 99.98    |\n\n---\n\n**Comment 2-2:**\nDoes Table 4 illustrate a tradeoff between novelty and other capabilities for DruM?\n\n**Response 2-2:**\nNo, our method **does not have a tradeoff between generation quality (e.g., low MMD, high validity, low FCD) and novelty**. DruM can generate valid graphs (high validity, high V.U.N.) where the structural properties follow the data distribution (low MMD, low FCD, low NSPDK) and further not seen in the training dataset (high novelty), as explained in *Response 2-1*. \n\nIn Table 4, the low novelty in QM9, similar to that of DiGress, is due to the fact that the molecules in the dataset consist of a small number of atoms (up to 9 atoms) of four atom types (C, N, O, F). Experiments on larger molecules (ZINC250k) verify that our method is able to generate novel molecules."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3082/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700042896334,
                "cdate": 1700042896334,
                "tmdate": 1700042896334,
                "mdate": 1700042896334,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZCpXGoKC3N",
                "forum": "UQVhOVhUi4",
                "replyto": "Gv0E1BqY2n",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3082/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3082/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Initial Response to Reviewer txBt (2/2)"
                    },
                    "comment": {
                        "value": "**Comment 3:**\nEmpirical results are limited to small graphs\n\n**Response 3:**\nWe respectfully disagree as we have experimentally evaluated on large real-world graph benchmark datasets, the Protein dataset (up to 500 nodes) and the GEOM-DRUGS dataset (up to 181 atoms with 16 atom types), that have been widely used in previous works. Our DruM significantly outperforms existing works on these large datasets achieving high validity. \n\n---\n\n**Comment 4:**\nDo you believe DruM has the potential to alleviate the scalability bottleneck for larger graphs?\n\n**Response 4:**\nAlthough the goal of our framework does not lie in alleviating the scalability bottleneck, we empirically demonstrate that our method shows superior performance for generating large graphs (e.g., Proteins and GEOM-DRUGS) providing promising direction for the generation of very large graphs.\n\n---\n---\n\nWe thank the reviewer again for their time and feedback, and we hope that our responses have addressed any remaining questions."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3082/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700042933255,
                "cdate": 1700042933255,
                "tmdate": 1700203898809,
                "mdate": 1700203898809,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "N11a5l3QX1",
                "forum": "UQVhOVhUi4",
                "replyto": "Gv0E1BqY2n",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3082/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3082/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Gentle Reminder for Reviewer txBt"
                    },
                    "comment": {
                        "value": "Dear Reviewer txBt,\n\n\nThank you for reviewing our paper. As the interactive discussion phase will end this Wednesday (22nd AOE), we politely ask you to check our responses. Please let us know if there are any other things that we need to clarify or provide. We sincerely appreciate your constructive suggestions.\n\nBest regards, \n\nAuthors"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3082/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700547277564,
                "cdate": 1700547277564,
                "tmdate": 1700547277564,
                "mdate": 1700547277564,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FAyvCHHgwu",
            "forum": "UQVhOVhUi4",
            "replyto": "UQVhOVhUi4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3082/Reviewer_cip6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3082/Reviewer_cip6"
            ],
            "content": {
                "summary": {
                    "value": "This work outlines the destination-predicting diffusion mixture, which is a novel framework using a mixture of learned OU bridge processes to generate graph data. The purpose of this method, as opposed to traditional diffusion approaches which invert a learned a mapping from noise to data, is to explicitly predict the (distribution of the) destination of a diffusion process sending noisy samples to a data distribution. The main claim is that this destination prediction facilitates more effective modeling of graph-structured data, as opposed to typical diffusion models like DDPM. This claim is substantiated by extensive experiments on diverse datasets of small to large graphs."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The explanation of the modeling approach is fairly clear, though some of the notation is a bit confusing. Regardless, the authors do well to explain how the mixture of bridge processes is constructed, finally leading to a straightforward objective in Eq (9).\n2. The experiments are very thorough; the GEOM-DRUGS result is particularly demonstrative of the improved performance of the destination prediction of DruM. The chemical/physical metrics of Table 2 and Figure 3 do well to highlight that this method is well-suited to real-data scenarios.\n3. The simulation-free training and rapid convergence to the destination distribution is a significant advantage when compared to expensive SDE simulation in typical diffusion."
                },
                "weaknesses": {
                    "value": "1. The main weakness in the paper is the lack of clarity about the argument behind the central claim that destination prediction facilitates more effective modeling of graph topological data. While the experimental results clearly show that this is the case, I failed to understand why this would be the case as I read the paper. The methodology described in Section 3 is completely agnostic to the fact that we are considering graph data, and while I can understand why it might be in general advantageous to predict the destination of diffusion, I'm not sure why this is particularly the case for graphs. The authors claim that destination prediction explicitly learns graph structure, but it is not clear to me why this is the case. Maybe this can be explained more clearly early in the text, towards the beginning of section 3.\n2. The sampling procedure does not seem to be explained in the main text. Figure 1 (b) demonstrates the diffusion process at sampling time, but since this is not explained in the text, it is a bit confusing to see how to connect the learned ${\\bf s}_\\theta$ to the sampling procedure. It would be better to include Algorithms 1 and 2 in the main body of the text, including a brief explanation of Algorithm 2.\n\nDespite my concerns above, I think this paper outlines a novel, effective procedure for graph generation. I believe some clarity in the text regarding the main claim and the sampling procedure would greatly enhance the clarity of the contribution."
                },
                "questions": {
                    "value": "1. As above, how exactly does destination prediction relate to better graph generation? It is mentioned that it takes the graph topology into account more explicitly, but it is not clear to me how this is the case.\n2. How does this approach compare to denoising-diffusion-type models in non-graph settings? While I understand this is not the focus of the paper, I think conducting a small experiment or at least commenting on this would make it clear why destination prediction is well-suited to graph data. As in the previous question, it is not clear to me why destination prediction is connected to the topology of the data."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3082/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3082/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3082/Reviewer_cip6"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3082/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1700187432439,
            "cdate": 1700187432439,
            "tmdate": 1700404541426,
            "mdate": 1700404541426,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "G4el3AM8qZ",
                "forum": "UQVhOVhUi4",
                "replyto": "FAyvCHHgwu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3082/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3082/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Initial Response to Reviewer cip6 (1/2)"
                    },
                    "comment": {
                        "value": "We sincerely thank you for your constructive and helpful comments. We appreciate your positive comments that \n- The paper outlines a novel, effective procedure for graph generation\n- The authors explain well how the bridge processes are constructed, finally leading to a straightforward objective in Eq (9).\n- The experiments are very thorough and chemical/physical metrics highlight that it is well-suited to real-data scenarios.\n- The simulation-free training and rapid convergence to the destination distribution is a significant advantage compared to previous diffusion models.\n\nWe provide an updated revision of the paper highlighted in orange, and we initially address your concerns below:\n\n---\n\n**Comment 1:** \nWhile the experimental results clearly show destination prediction facilitates more effective modeling of graph topological data, and while I can understand why it might be in general advantageous to predict the destination of diffusion, I'm not sure why this is particularly the case for graphs.\n\n**Response 1:** \nA major difference between graph data and image data is that while a slight modification of the edges may significantly change the structure of the graphs and hence its properties, for example, connecting a pair of nodes can break the planarity or a change in the bond type of a molecule can transform the property from beneficial to toxic (e.g., Figure 1 of [1]), it is not the case for images where a slight change in the pixel values does notably affect its semantic contents or its property (e.g., image class).\n\nTherefore, unlike image data, the key to generating valid graphs is **accurately modeling the discrete structures of graphs and the properties determined by the structure**. Generating graphs by estimating the noise (or score) is ill-suited since what the model learns is to denoise each step, not learning the graph structure. Since the topology is implicitly recovered from denoising, they cannot fully capture the structure as shown in Figure 2 (Left) where GDSS and ConGress fail to model the spectral topology.\n\nTo address the limitation of previous works, **our model learns to predict the graph structure directly**. By learning the accurate structure, **we can also capture the properties determined by the structure**, for example, planarity or clusteredness. Specifically, by constructing the OU bridge mixture, we propose the destination mixture matching objective (Eq. (9)) where the graph structure is explicitly learned. Especially, in contrast to estimating the noise, we can **exploit the discreteness of graphs** (e.g., the entries of the adjacency matrices are 0-1) when predicting the structure by adding an additional function (e.g., sigmoid or softmax) at the end of our model. In this way, our model can easily capture the discrete structure, which we validate in Figure 2 (Right) achieving a significant decrease in the model complexity.\n\nWe experimentally validate in Figure 2 (Left) and (Middle) that **our model predicts the accurate structure of graphs** (low MMDs) at the halfway stage of the generation and similarly observed in the visualization of the generation processes in Figures 16~19. \n\nWe would like to emphasize that our framework is tailored for graph generation in the following aspects:\n\n- The motivation of our work is to capture the graph topology by predicting the destination.\n- This leads to our novel parameterization of the bridge mixture (Eq. (5)) through the destination mixture, and the training objective (Eq.(8) and (9)). \n- The parameterization and the training objective allow us to exploit the discrete graph structure for effectively learning the destination mixture (Section 3.2 paragraph: Advantages of our framework).\n- To generate attributed graphs (graphs with node features), we simultaneously generate both the node features and the adjacency matrices using the system of SDEs in Eq. (31) (Section A.4).\n\nAll the components contribute to the superior performance of our DruM validated by extensive experiments.\n\nFollowing the reviewer\u2019s suggestion, we have added more explanation in the revised paper (beginning of Section 3) regarding the importance of directly modeling the graph structure.\n\n[1] Jo et al., Edge Representation Learning with Hypergraphs, NeurIPS 2021"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3082/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700397733018,
                "cdate": 1700397733018,
                "tmdate": 1700397733018,
                "mdate": 1700397733018,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UctFzi9UWc",
                "forum": "UQVhOVhUi4",
                "replyto": "FAyvCHHgwu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3082/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3082/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Initial Response to Reviewer cip6 (2/2)"
                    },
                    "comment": {
                        "value": "**Comment 2:** \nIt would be better to include Algorithms 1 and 2 in the main body of the text, including a brief explanation of Algorithm 2.\n\n**Response 2:** \nDue to the page limit, we provided a detailed explanation of the sampling procedure in Appendix B.4 and the pseudo-code in Algorithm 2. Yet we agree that adding a brief explanation would help the reader, and we have added an overview of sampling from DruM in Section 3.2 of the revised paper explaining:\n \n- By starting from samples from the prior distribution, we simulate the parameterized bridge process of Eq. (7) from time $t=0$ to $t=T$, where the drift is computed from the trained model $s_{\\\\theta}$.\n- We can leverage any SDE solver used in previous works, for example, the Euler-Maruyama method or the Heun's 2nd-order method. \n- Especially, we simultaneously generate both the node features ($X$) and adjacency matrices ($A$) by simulating the system of SDEs as in Eq. (31) (described in Section A.4). \n\nWe would like to move the pseudo-code to the main paper in the final revision if we have additional pages.\n\n---\n\n**Comment 3:** \nWhile I understand it is not the focus of the paper, how does this approach compare to denoising-diffusion-type models in non-graph settings?\n\n**Response 3:** \nIn order to compare our approach with DDPM on image generation, we trained on the CIFAR-10 dataset following the training setup and the model architecture of [2]. We achieved an FID of 3.95 (until Nov. 19) which shows comparable results to DDPM (FID: 3.17) and slightly inferior to Score SDE (FID: 2.20).  Please note that we could not perform a proper hyperparameter search due to limited time. Yet we observe that our framework does not provide particular benefits for image generation. However, we believe our approach could be beneficial for generating data where understanding the topology of the data is crucial. We provide the samples of generated images in the following anonymized link: https://anonymous.4open.science/r/paper3082_rebuttal/sample.png. \n\n[2] Song et al., Score-Based Generative Modeling through Stochastic Differential Equations, ICLR 2021\n\n---\n---\n\nWe thank the reviewer again for their time and feedback. We hope that our responses have addressed any remaining questions."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3082/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700397784180,
                "cdate": 1700397784180,
                "tmdate": 1700397784180,
                "mdate": 1700397784180,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ualtQAWY4Z",
                "forum": "UQVhOVhUi4",
                "replyto": "UctFzi9UWc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3082/Reviewer_cip6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3082/Reviewer_cip6"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the thorough response. The additions regarding the importance of leveraging structural properties of the data and regarding sampling do well to clarify the contribution of the work. I agree that, if space permits, moving the pseudo-code to the main paper in a final revision would be worthwhile, as well. I would say that adding some comment or brief results of the experiment you conducted on CIFAR-10, noting that your approach is most beneficial when applied to data with specific topological properties rather than typical image data, could also help clarify why your model is working well in the graph generation setting. Regardless, I think this work outlines a strong, novel approach, and that the added information in the text strengthens the clarity of the contribution. I've raised my score, accordingly."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3082/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700404491485,
                "cdate": 1700404491485,
                "tmdate": 1700404491485,
                "mdate": 1700404491485,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]