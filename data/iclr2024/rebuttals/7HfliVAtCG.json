[
    {
        "title": "Detect Every Thing with Few Examples"
    },
    {
        "review": {
            "id": "I18ys0EKo3",
            "forum": "7HfliVAtCG",
            "replyto": "7HfliVAtCG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission313/Reviewer_Vm41"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission313/Reviewer_Vm41"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to use vision only backbone to represent categories as visual prototype to unify few-shot object detection and open vocabulary detection in cases of input and output. They propose DE-ViT, a combined approach that combine both DINO-v2 and RCNN together. They evaluate DE-ViT on open-vocabulary, few-shot, and one-shot object detection benchmark with COCO and LVIS, which achieves the new state-of-the-art results."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The overall writing is good and easy to follow. \n\n- The proposed method achieves significant improvements on few-shot detection benchmarks. It also achieves strong performance on COCO/LVIS open vocabulary benchmark.\n\n- The proposed refined localization architecture is interesting."
                },
                "weaknesses": {
                    "value": "- The idea of learning prototype to compare is not novel in few-shot learning (detection). In this work, authors adopt more recent strong DINO-v2 to extra visual features to enhance visual entity's comparison. \n\n- The idea is similar to OV-DETR since both uses the bypassing per-class inference procedure. \n\n- I double that the proposed approach may not be a real open vocabulary setting, since the authors assume the novel classes of COCO can be accessed during the inference. This means the vocabulary is still limited and pre-defined. For example, the proposed model is hard to extend into zero-shot setting where the novel prototypes are not available.\n\n- The visual novel objects are sampled from COCO or LVIS, which also may lead to data leakage. What about the classes sampled from ImageNet-21k to build prototypes?\n\n- The model needs extra Region Proposal Network along with backbone. What about the shared frozen DINOv2-ViT backbone design such as works [1][2]?\n\n- Missing parameters and GFLops analysis. \n\n- Presentation: What are the meanings of frozen or unfrozen? Better add the notations in the figure-2. \n\n\n[1]. Convolutions Die Hard: Open-Vocabulary Segmentation with Single Frozen Convolutional CLIP, NeurIPS-2023\n\n[2]. F-VLM: Open-Vocabulary Object Detection upon Frozen Vision and Language Models, ICLR-2023"
                },
                "questions": {
                    "value": "See the weakness part. I rate the current draft as boardline."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission313/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698656325968,
            "cdate": 1698656325968,
            "tmdate": 1699635957671,
            "mdate": 1699635957671,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4qbchp0JNu",
                "forum": "7HfliVAtCG",
                "replyto": "I18ys0EKo3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission313/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission313/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer 4 (W1-W2)"
                    },
                    "comment": {
                        "value": "We are greatly thankful to the reviewer for the comments. We hope the following reply resolve the concerns raised in the review.\n\n**(W1, novelty) \"The idea of learning prototype to compare is not novel in few-shot learning (detection). In this work, authors adopt more recent strong DINO-v2 to extra visual features to enhance visual entity's comparison.\"**\n\nWe conducted an ablation study (See Section 4.3) to demonstrate that a naive ensemble of DINOv2 and prototype-based few-shot detector leads to poor few-shot performance, and every proposed component is important for the performance of our method. Therefore, combining DINOv2 and prototype-based few-shot detectors is not trivial. Our contributions do not rely on prototype learning and are not simply combining DINOv2 with existing methods.\n\n| Conventional Prototypes Head (baseline) | Prototype Projection | Background Tokens | Class Reorder and Resample | COCO nAP50 | COCO nAP75 |\n| --- | --- | --- | --- | --- | --- |\n| \u2713 |  |  |  | 4.5 | 2.2 |\n|  | \u2713 |  |  | 26.2 | 9.7 |\n|  | \u2713 | \u2713 |  | 38.4 | 23 |\n|  | \u2713 | \u2713 | \u2713 | **39.5**| **24.1** |\n\n\n| Conventional  REG Head (baseline) | Expanded Proposal | Region Propagation | COCO nAP50 | COCO nAP75 |\n| --- | --- | --- | --- | --- |\n| \u2713 |  |  | 37.7 | 14.6 |\n| \u2713 | \u2713 |  | 35.6 | 12.5 |\n|  | \u2713 | \u2713 | **39.5** | **24.1**  |\n\n\nExisting few-shot detection approaches can be broadly classified into finetuning-based [3-6, 9-14] and meta-learning-based strategies [1-2]. Finetuning-based methods demand redundant multi-stage training procedures [7, 8]. Meta-learning methods avoid finetuning by online adaptation, but exhibit inferior accuracy [15].\n\nContrary to existing work, we are the first to build a few-shot detector on top of DINOv2. Our method transforms the multi-class classification into multiple binary classifications, so a binary classifier can be trained and used for all classes without any finetuning. Our method localizes objects with a novel propagation-based mechanism that we developed. The new mechanism is based on computing similarity maps between features and prototypes.\n\nWe revised the introduction section to highlight our technical contributions.\n\n\n[1]: Few-shot object detection via feature reweighting, ICCV 2019\n\n[2]: Meta r-cnn: Towards general solver for instance-level low-shot learning, ICCV 2019\n\n[3]: Frustratingly Simple Few-Shot Object Detection, ICML 2020\n\n[4]: Query adaptive few-shot object detection with heterogeneous graph convolutional networks, ICCV 2021\n\n[5]: Generalized few-shot object detection without forgetting, CVPR 2021\n\n[6]: Fsce: Few-shot object detection via contrastive proposal encoding, CVPR 2021\n\n[7]: Semantic-aligned fusion transformer for one-shot object detection, CVPR 2022\n\n[8]: Balanced and hierarchical relation learning for one-shot object detection, CVPR 2022\n\n[9]: Label, verify, correct: A simple few shot object detection method, CVPR 2022\n\n[10]: Few-shot object detection and viewpoint estimation for objects in the wild, TPAMI 2022\n\n[11]: Meta faster r-cnn: Towards accurate few-shot object detection with attentive feature alignment, AAAI 2022\n\n[12]: Few-shot object detection with fully cross-transformer, CVPR 2022\n\n[13]: NIFF: Alleviating Forgetting in Generalized Few-Shot Object Detection via Neural Instance Feature Forging, CVPR 2023\n\n[14]: DiGeo: Discriminative Geometry-Aware Learning for Generalized Few-Shot Object Detection, CVPR 2023\n\n[15]: Few-shot object detection: a comprehensive survey, IEEE Transaction on Neural Networks and Learning Systems, 2023\n\n\n**(W2, comparision with OV-DETR) \"The idea is similar to OV-DETR since both uses the bypassing per-class inference procedure.\"**\n\nOur idea is not just bypassing per-class inference but also training a binary classifier that can be used for all classes without any finetuning, and a novel propagation-based localization mechanism. On the contrary, OV-DETR [1] does not bypass the per-class inference procedure [2, 3].\n\n\nThe inefficiency of OV-DETR is also pointed out by CORA [2] as *\u201cConditional Matching in OV-DETR  also proposes to condition the queries on the text embedding for class-aware regression. But it suffers from repetitive per-class inference.\u201d*  and Prompt-OVD [3] as *\u201cHowever, the main limitation of OV-DETR is the need for multiple forward passes in the decoder due to its large number of object queries, which results in slow inference speeds that may not be suitable for real-world use cases.\"*, and *\"OV-DETR needs linearly increasing number of object queries with respect to the number of classes.\u201d*.\n\n\nWe updated Section 3.1 at Page 3 of the paper to clarify the procedure of bypassing per-class inference.\n\nReference: \n\n[1]: Open-vocabulary detr with conditional matching, ECCV 2022\n\n[2]: CORA: Adapting CLIP for Open-Vocabulary Detection with Region Prompting and Anchor Pre-Matching, CVPR 2023\n\n[3]: Prompt-Guided Transformers for End-to-End Open-Vocabulary Object Detection, Arxiv 2023"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission313/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700364798139,
                "cdate": 1700364798139,
                "tmdate": 1700364798139,
                "mdate": 1700364798139,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hZgm31Cyvg",
                "forum": "7HfliVAtCG",
                "replyto": "I18ys0EKo3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission313/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission313/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer 4 (W3-W5)"
                    },
                    "comment": {
                        "value": "**(W3, open vocabulary setting) \"I double that the proposed approach may not be a real open vocabulary setting, since the authors assume the novel classes of COCO can be accessed during the inference. This means the vocabulary is still limited and pre-defined. For example, the proposed model is hard to extend into zero-shot setting where the novel prototypes are not available.\"**\n\nWe agree with the reviewer that our proposed method is not an open-vocabulary detector. We revised the entire introduction and experiment section to clarify the  setting and comparison towards open-vocabulary detectors.\n\nOur original intent is to use this comparison to indicate the significant improvement we achieved on few-shot object detection. Because the detection accuracy of existing few-shot methods fall behind that of open-vocabulary detectors, especially on challenging datasets such as LVIS (the open-vocabulary SoTA on LVIS has 31.2 box APr, while the few-shot one only has 16.6 box APr). Our proposed method DE-ViT outperforms the SoTAs on both few-shot and open-vocabulary (33.6 box APr), which has not been achieved before by existing few-shot work.\n\n\n**(W4, data leakage) \"The visual novel objects are sampled from COCO or LVIS, which also may lead to data leakage. What about the classes sampled from ImageNet-21k to build prototypes?\"**\n\nSampling base and novel objects from the same datasets is the standard evaluation procedure for few-shot object detection [1].  Regarding the concern for data leakage, all annotations for novel categories are removed during training. So even if the network has seen the images with novel objects inside, the novel objects will be treated as background. There is no data leakage.  In fact, this makes it even more difficult for detecting novel objects that are present in the training set.\n\nReference: \n\n[1]: Few-shot object detection: a comprehensive survey, IEEE Transaction on Neural Networks and Learning Systems, 2023\n\n\n**(W5, detector over frozen backbone) \"The model needs extra Region Proposal Network along with backbone. What about the shared frozen DINOv2-ViT backbone design such as works [1][2]?\"**\n\n\nThough both using a frozen backbone, our proposed method DE-ViT delivers more accurate detection results than F-VLM [2] on COCO 2017 (39.5 AP50, 24.1 AP75 vs. F-VLM 28.0 AP50, AP75 not reported), with our propagation-based localization mechanism. Our region proposal network involves small computation cost.\n\n\n\n| Conventional  REG Head (baseline) | Expanded Proposal | Region Propagation | COCO nAP50 | COCO nAP75 |\n| --- | --- | --- | --- | --- |\n| \u2713 |  |  | 37.7 | 14.6 |\n| \u2713 | \u2713 |  | 35.6 | 12.5 |\n|  | \u2713 | \u2713 | **39.5** | **24.1**  |\n\n\nIn Table 8, our proposed method DE-ViT achieves 39.5 AP50 and 24.1 AP75 on COCO with our propagation-based localization. However, when replacing propagation architecture with a conventional regression head, DE-ViT (w contentional regression) only achieves 37.7(-1.8) AP50 and 14.6(-9.5) AP75. The massive drop of AP75 shows the effectiveness of our propagation localization on the frozen DINOv2 backbone. F-VLM [2] trains detectors on frozen backbone using conventional FPN detector head, and has 28.0 AP50. F-VLM does not report AP75.\n\n\n\nOur region proposal network involves a small computation cost. In our model for LVIS, RPN has only **27M** parameters, and our total parameters for ViT-L backbone is 350M. While the F-VLM contains a total parameters of 450M without additional RPN, where the backbone (RN50x64) has 420M parameters. In our model for COCO, RPN has only **8M** parameters.\n\n\n|  | Backbone | Total Params | Trained Params |  APr |\n| --- | --- | --- | --- | --- | \n| F-VLM [2] | RN50x64 | 445M  (backbone 420M) | 25M |  32.8 |\n| DE-ViT (Ours) | ViT-L | **350M** (backbone 320M, RPN 27M) | **23M** | **34.3** |\n\n\n\nWe added the mentioned papers [1, 2] into the reference of our paper and incorporated this answer into the efficiency analysis at Section A.1.3 .\n\nReference: \n\n[1]. Convolutions Die Hard: Open-Vocabulary Segmentation with Single Frozen Convolutional CLIP, NeurIPS 2023\n\n[2]. F-VLM: Open-Vocabulary Object Detection upon Frozen Vision and Language Models, ICLR 2023"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission313/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700365126515,
                "cdate": 1700365126515,
                "tmdate": 1700365126515,
                "mdate": 1700365126515,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hHy2U1t4ya",
                "forum": "7HfliVAtCG",
                "replyto": "I18ys0EKo3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission313/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission313/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer 4 (W6-W7)"
                    },
                    "comment": {
                        "value": "**(W6, parameter analysis) \"Missing parameters and GFLops analysis.\"**\n\nWe added the parameters and training epochs comparison for recent detectors on LVIS in Table 6.\n\n\nCompared to the recent detectors trained on LVIS, our method only has 23M trainable parameters, and is trained orders of magnitude faster than F-VLM and OWL-ViT. We also provide inference time comparison in Tables 5, A2, and A3.\n\n\n|  | Backbone | Total Params | Trained Params | Training Epochs | APr |\n| --- | --- | --- | --- | --- | --- |\n| OWL-ViT [3] | ViT-L | 433M | 433M | 1800 | 31.2 |\n| F-VLM [2] | RN50x64 | 445M | 25M | 118 | 32.8 |\n| DE-ViT (Ours) | ViT-L | **350M** | **23M** | **14.4** | **34.3** |\n\n\n| Few-shot Methods | Backbone | Total Params | Trained Params | Inference Time (Sec / Img) | nAP50 |\n| --- | --- | --- | --- | --- | --- |\n| Meta Faster RCNN [4]  | RN101 | 82M | 82M | 0.61 | 31.8 |\n| Cross Transformer [5] | Custom | 24M | 24M | 3 | 35.8 |\n| DE-ViT (Ours) | ViT-S | 70M | **23M** | **0.25** | **42.7** |\n\n| Method | COCO nAP50 | Secs/Img |\n| --- | --- | --- |\n| CORA [6] | 41.7 | 0.5 |\n| DE-ViT (Ours)  | **50** | **0.33** |\n\n| Method | LVIS box APr | Secs/Img |\n| --- | --- | --- |\n| OWL-ViT [3] | 31.2 | **0.42** |\n| DE-ViT (Ours)  | **32.6** | 0.5 |\n\n\n\nFor few-shot detection, our proposed method DE-ViT has the smallest inference time of 0.25s/img, compared to 0.61s/img of Meta Faster RCNN and 3s/img of CrossTransformer, while having better performance. DE-ViT also has the smallest trainable parameters. For open-vocabulary detection, DE-ViT is faster (0.33s/img vs 0.5s/img) and more accurate (50 AP50 vs 41.7 AP50) than CORA , the SoTA on COCO. DE-ViT is more accurate (32.6 box APr vs 31.2 box APr) and slightly slower (0.5 s/img vs 0.42 s/img) than OWL-ViT. While our method DE-ViT uses PyTorch, OWL-ViT uses JAX [1]. JAX is commonly regarded as faster in general than PyTorch. Inference time of all methods is measured using  the same machine.\n\nReference:\n\n[1]: \"Composable transformations of Python+NumPy programs: differentiate, vectorize, JIT to GPU/TPU, and more\" https://github.com/google/jax\n\n[2]: F-VLM: Open-Vocabulary Object Detection upon Frozen Vision and Language Models, ICLR 2023\n\n[3]: OWL-ViT: Simple open-vocabulary object detection, ECCV 2022\n\n[4]: Meta faster r-cnn: Towards accurate few-shot object detection with attentive feature alignment, AAAI 2022\n\n[5]: Few-shot object detection with fully cross-transformer, CVPR 2022\n\n[6]: CORA: Adapting CLIP for Open-Vocabulary Detection with Region Prompting and Anchor Pre-Matching, CVPR 2023\n\n\n**(W7, missing notation) \"Presentation: What are the meanings of frozen or unfrozen? Better add the notations in the figure-2.\"**\n\nWe thank the reviewer for pointing this out. We updated Figure 2 to add a  description for the symbol \u2744, which denotes frozen parameters. We removed the symbol \u2668, which was used to denote trainable parameters."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission313/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700365456265,
                "cdate": 1700365456265,
                "tmdate": 1700365456265,
                "mdate": 1700365456265,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "SCgljIm6IT",
            "forum": "7HfliVAtCG",
            "replyto": "7HfliVAtCG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission313/Reviewer_bhpM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission313/Reviewer_bhpM"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, a new method is introduced for few-shot and open-vocabulary object detection. Using DINOv2, the method takes a small set of examples to create main patterns, called prototypes. With these prototypes, the authors describe a way to train a network that can tell apart one item from others, based on the proposals generated by the RPN. They also introduce a technique to improve the initial box predictions. Through detailed tests, the paper shows that their method works better than other current methods. The paper also includes a study that looks closely at how each part of their method contributes to its overall performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The utilization of DINOv2 embeddings to construct prototype for different classes is interesting.\n2. The introduction of the propagation procedure offers a novel approach to the problem.\n3. The method exhibits exemplary performance, surpassing preceding methodologies across diverse datasets and tasks.\n4. The manuscript provides an comprehensive ablation study evaluated the contribution from different modules."
                },
                "weaknesses": {
                    "value": "The writing of Section 3.1 could be enhanced. Some portions might be presented in a more reader-friendly manner."
                },
                "questions": {
                    "value": "1. Would it be possible to incorporate notations such as `f` and `h` into Figure 3 for easier referencing?\n2. Should the notation be \"[C]\\\\c_k\" rather than \"[C]/c_k\"?\n3. The upsampling process described in Eq.3 is somewhat unclear. Could you first elucidate the underlying objective or insight you aim to convey? The rationale behind upsampling on the channel dimension is not immediately evident. If the intent is to balance the number of prototypes and base categories, might a re-weighting approach similar to the focal loss be more appropriate?\n4. During the propagation procedure, what transpires if the expanded box encompasses another object of the same category?\n5. Could you provide a more detailed explanation of Eq. 4, particularly concerning the `w` and `h` components? An illustrative figure might enhance comprehension.\n6. Clarification on terms such as \"bAP\" and \"Split-n\" would be beneficial.\n7. Are there prior methods that employ DINO/DINOv2 embeddings to craft representations for detection/re-identification? If so, what's the difference between their methods and yours?\n8. For the definition of h, it might be better to use \"h=\\\\bar{f}\\\\dot p\" directly to create better reading experience.\n9. What are the limitations and failure cases of this work?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission313/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission313/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission313/Reviewer_bhpM"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission313/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698816691113,
            "cdate": 1698816691113,
            "tmdate": 1699635957581,
            "mdate": 1699635957581,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fMIbz8eqhj",
                "forum": "7HfliVAtCG",
                "replyto": "SCgljIm6IT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission313/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission313/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer 3 (W1-4, 8)"
                    },
                    "comment": {
                        "value": "We are greatly thankful to the reviewer for the comments and acknowledgement of our work. We hope the following reply resolves the raised concerns.\n\n**(W1 W2 W8, fix notation) \"1. Would it be possible to incorporate notations such as f and h into Figure 3 for easier referencing?\n2. Should the notation be \"[C]\\c_k\" rather than \"[C]/c_k\"?\n3. For the definition of h, it might be better to use \"h=\\bar{f}\\dot p\" directly to create better reading experience.\"**\n\nWe thank the reviewer for pointing out the notation issue. We updated Figure 3 to include f and h notations as requested. We fixed all equations and figures to change $[C]/c_k$ to $[C]\\backslash c_k$. We also rewrote the definition of h to use dot product. \n\n\n**(W3, Eq3) \"The upsampling process described in Eq.3 is somewhat unclear. Could you first elucidate the underlying objective or insight you aim to convey? The rationale behind upsampling on the channel dimension is not immediately evident. If the intent is to balance the number of prototypes and base categories, might a re-weighting approach similar to the focal loss be more appropriate?\"**\n\n\nEq.3 sorts and resamples the input similarity map from shape $H\\times W\\times (C-1)$ to $H\\times W\\times T$, where $C$ is the number of classes and $T$ is a fixed hyper-parameter.\n\n\nIn details, since the number of classes $C$ is different per dataset, the shape of the similarity map is also different, which is problematic because the similarity map is used as the input to the binary classification network, which is the same for all datasets. Therefore, in order to standardize the input size, we resample them to $H\\times W\\times T$. The intent is not to balance the number of prototypes and base categories. \n\nWe updated the paper to incorporate this answer in Section 3.1.\n\n\n**(W4 - propagation procedure) \"During the propagation procedure, what transpires if the expanded box encompasses another object of the same category?\"**\n\nWe added a visual analysis of the propagation localization from expanded proposals covering same-class objects in the Section A.1.1 at Page 14. We will use the term over-expanded proposals to denote the scenario mentioned by the reviewer.\n\n\n| Score Threshold | Average IoU of refined boxes with groundtruth from over-expanded Proposals | Average IoU of refined boxes with groundtruth for other proposals |\n| --- | --- | --- |\n| 0 | 0.49 | 0.59 |\n| 0.1 | 0.56 | 0.69 |\n| 0.2 | 0.58 | 0.72 |\n| 0.3 | 0.60 | 0.74 |\n| 0.4 | 0.62 | 0.75 |\n| 0.5 | 0.63 | 0.77 |\n| 0.6 | 0.65 | 0.78 |\n| 0.7 | 0.68 | 0.80 |\n| 0.8 | 0.72 | 0.83 |\n| 0.9 | 0.76 | 0.86 |\n| 0.95 | 0.82 | 0.87 |\n\n\n\n\nAs shown in Figure A1, over-expanded proposals generally degrade localization accuracy, as their final predicted boxes have smaller IOU towards the ground truth. But the degradation is far from total, e.g., they still produce boxes whose IOU > 0.7 on average, under the score threshold of 0.85,  which we use to generate qualitative visualizations. We also observe that the over-expanded proposals occupy around 7% of all proposals in our model for COCO, and only half of them appear in the final prediction (after NMS and score filtering). This means even if an over-expanded proposal predicts an inaccurate box, its impact is softened by filtering of NMS and score thresholding.\n\nIn terms of actual behavior, we listed success and failure cases of over-expanded proposals in Figure A7 and A8. In failure cases, propagation either encompasses both objects, or produces erratic boxes. But all failure cases happen under inferior proposals, where the proposals are already poorly located and do not cover any object, or cover multiple objects before expansion. In success cases, propagation generally prefers central objects, but can locate an object accurately regardless of the proposal quality. This means our propagation localization does not fully rely on the proposal quality."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission313/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700364085779,
                "cdate": 1700364085779,
                "tmdate": 1700364085779,
                "mdate": 1700364085779,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cx3yp1WmQ6",
                "forum": "7HfliVAtCG",
                "replyto": "SCgljIm6IT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission313/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission313/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer 3 (W5, W6)"
                    },
                    "comment": {
                        "value": "**(W5 - Eq4) \"Could you provide a more detailed explanation of Eq. 4, particularly concerning the w and h components? An illustrative figure might enhance comprehension.\"**\n\nThe first line Eq.4 computes the expected position of the heatmap as the bounding box center, the second line computes a weighted average of row sum of the heatmap as the bounding box width, and column sum as the height. \n\n\nIn more details, consider a toy example of converting a binary mask into a bounding box, a reasonable approach is to compute the mask center as the bounding box center, and pick the maximum row and column sum as width and height, as illustrated in Figure **[X]**. Motivated by this, our method estimates the center by computing the expected position under the spatial distribution $softmax(\\textbf{g})$, where $\\textbf{g}$ denotes the heatmap values. To emulate the row / column sum, we compute  $\\sum_{j=1}^W  \\sigma(\\textbf{g})_{ij}$, which sums up the sigmoid activation along the i-th row. Instead of picking the maximum, we aggregate all row sums in terms of magnitude. The rationale of this aggregation is that a larger estimation is more likely to over-cover the entire object and a small estimation may produce a box that is too tight. The aggregation is done by sorting the estimation and then weighted averaging, which explains the use of order statistics notation $(i)$ and $(j)$. \n\nWe updated the paper to incorporate this answer in Section 3.2 and add an illustrative example in Figure 5.\n\n\n**(W6 -evaluation metrics) \"Clarification on terms such as \"bAP\" and \"Split-n\" would be beneficial.\"**\n\n\"bAP\" denotes mAP on base classes. For \"Split-n\", one-shot methods divide 80 classes of COCO2017 into four even partitions, and alternatively takes three partitions as base classes and one partition as novel classes. There are 4 base/novel splits in total, named as Split-1/2/3/4. \n\n\n\nWe clarify the evaluation metrics used in the papers. We updated Paragraph 2 in Section 3 for a more comprehensive description of evaluation metrics.\n\n- Few-shot \n    - bAP: mAP on base classes\n    - nAP, nAP50, nAP75: mAP, AP50, AP75 on novel classes\n- One-shot\n    - Seen -> Split-1/2/3/4: AP50 on base classes, for each split\n    - Unseen -> Split-1/2/3/4: AP50 on novel classes, for each split\n    - Avg: AP50 averaged among all splits, for base or novel classes\n- Open-vocabulary\n    - COCO\n        - Novel: AP50 on novel classes\n        - Base: AP50 on base classes\n        - All: AP50 on all classes\n    - LVIS\n        - APr: AP on rare categories, which are novel classes\n        - AP: AP on all classes\n        - APc: AP on common categories, which are base classes\n        - APf: AP on frequent categories, which are base classes"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission313/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700364197280,
                "cdate": 1700364197280,
                "tmdate": 1700364197280,
                "mdate": 1700364197280,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2NXvkuWGA8",
                "forum": "7HfliVAtCG",
                "replyto": "SCgljIm6IT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission313/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission313/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer 3 (W7, W8)"
                    },
                    "comment": {
                        "value": "**(W7-prior work that uses DINO/DINOv2) \"Are there prior methods that employ DINO/DINOv2 embeddings to craft representations for detection/re-identification? If so, what's the difference between their methods and yours?\"**\n\n\nTo the best of our knowledge, we are the first to incorporate DINO/DINOv2 into the few-shot object detection task.\n\nOW-DETR [1] proposes to use DINO features in open-world object detection tasks, while our proposed method DE-ViT is used in few-shot object detection. The difference between open-world detection and few-shot detection is that open-world detection marks all novel objects as unknown, while few-shot detection detects novel categories based on the support set. OW-DETR uses the attention map of DINO as pseudo labels for unknown objects, i.e., unlabeled areas with high attention saliency is more likely to have an unknown object. But OW-DETR only detects and does not classify unknown objects into specific categories. Our DE-ViT uses the similarity map between DINOv2 features and prototypes to detect and classify novel objects. \n\nNormCut [2] proposes a graph-cut method on top of DINO features to detect the most salient object in images. However, it can only detect a single object for each image, while our DE-ViT does not have the limitation on number of objects. \n\nReference: \n\n[1]: Ow-detr: Open-world detection transformer, CVPR 2022\n\n[2]: Self-supervised transformers for unsupervised object discovery using normalized cut, CVPR 2022\n\n\n\n**(W9, limitation) \"What are the limitations and failure cases of this work?\"**\n\n\n\nOne of the limitations is that our current architecture is a mix of ViT and RCNN, while a full transformer network can clearly be more scalable and unlock more possible abilities and integrations to other modalities. Another limitation is that our current model relies on an external region proposal network (RPN). It is possible to train an RPN on top of frozen DINOv2 with some engineering efforts. \n\n\nWe added some failure examples on COCO in Figure A4 in Appendix. Image (a) in Figure A4 detects the floor as a dining table, possibly due to visual similarity. Image (b) shows a single box over two cuddling cats, likely caused by very little visual separation between the two.  We discussed limitations of our work in Section 5. We updated the paper to include more qualitative results and failure case visualization in Figure 8, A4, A9, A10, and A11."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission313/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700364329742,
                "cdate": 1700364329742,
                "tmdate": 1700364329742,
                "mdate": 1700364329742,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YCNhN9gCp1",
            "forum": "7HfliVAtCG",
            "replyto": "7HfliVAtCG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission313/Reviewer_f8VP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission313/Reviewer_f8VP"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed an open-set object detector using vision-only DINOv2 backbone to learn new classes through images rather than language. The paper proposed a new region propagation technique for localization and evaluate the method on open-vocabulary, few-shot and one-shot benchmark. The experimental results show good improvement."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well-written and presented.\n2. The description of methods is clear."
                },
                "weaknesses": {
                    "value": "1. The method lacks some novelty and doesn't look attractive to me. This is the main problem of the paper. So many parts ensembles together only cost more computational resources.\n2. On the few-shot benchmark, the good experiments are more shown on 10 or more shot rather than fewer-shot. There are so many different measures  shown in the experimental results, which are not convincing to me about its real performance.\n3. There is no visualization of the experimental results shown in the paper."
                },
                "questions": {
                    "value": "please see the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission313/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698889325471,
            "cdate": 1698889325471,
            "tmdate": 1699635957490,
            "mdate": 1699635957490,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "75dykcw8ye",
                "forum": "7HfliVAtCG",
                "replyto": "YCNhN9gCp1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission313/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission313/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer 2 (W1, part1)"
                    },
                    "comment": {
                        "value": "We are greatly thankful to the reviewer for the comments. We hope the following reply resolve the concerns raised in the review.\n\n**(W1, part1, novelty)  \"The method lacks some novelty and doesn't look attractive to me. This is the main problem of the paper. \"**\n \n\nExisting few-shot detection approaches can be broadly classified into finetuning-based [3-6, 9-14] and meta-learning-based strategies [1-2]. Finetuning-based methods demand redundant multi-stage training procedures [7, 8]. Meta-learning methods avoid finetuning by online adaptation, but exhibit inferior accuracy [15].\n\nContrary to existing work, we are the first to build a few-shot detector on top of DINOv2. Our method transforms the multi-class classification into multiple binary classifications, so a binary classifier can be trained and used for all classes without any finetuning. Our method localizes objects with a novel propagation-based mechanism that we developed. The new mechanism is based on computing similarity maps between features and prototypes.\n\n\n\nOur method outperforms existing few-shot detection models by a significant margin on COCO (+15 nAP on 10shot, +7.2 nAP on 30shot) and LVIS (+20 box APr) while eliminating the finetuning requirement. We conducted an ablation study (See Section 4.3) to demonstrate that every proposed component is important for the performance of our proposed method.\n\n| Conventional Prototypes Head (baseline) | Prototype Projection | Background Tokens | Class Reorder and Resample | COCO nAP50 | COCO nAP75 |\n| --- | --- | --- | --- | --- | --- |\n| \u2713 |  |  |  | 4.5 | 2.2 |\n|  | \u2713 |  |  | 26.2 | 9.7 |\n|  | \u2713 | \u2713 |  | 38.4 | 23 |\n|  | \u2713 | \u2713 | \u2713 | **39.5**| **24.1** |\n\n\n| Conventional  REG Head (baseline) | Expanded Proposal | Region Propagation | COCO nAP50 | COCO nAP75 |\n| --- | --- | --- | --- | --- |\n| \u2713 |  |  | 37.7 | 14.6 |\n| \u2713 | \u2713 |  | 35.6 | 12.5 |\n|  | \u2713 | \u2713 | **39.5** | **24.1**  |\n\n\n\nWe revised the introduction section to highlight our technical contributions.\n\nReference: \n\n[1]: Few-shot object detection via feature reweighting, ICCV 2019\n\n[2]: Meta r-cnn: Towards general solver for instance-level low-shot learning, ICCV 2019\n\n[3]: Frustratingly Simple Few-Shot Object Detection, ICML 2020\n\n[4]: Query adaptive few-shot object detection with heterogeneous graph convolutional networks, ICCV 2021\n\n[5]: Generalized few-shot object detection without forgetting, CVPR 2021\n\n[6]: Fsce: Few-shot object detection via contrastive proposal encoding, CVPR 2021\n\n[7]: Semantic-aligned fusion transformer for one-shot object detection, CVPR 2022\n\n[8]: Balanced and hierarchical relation learning for one-shot object detection, CVPR 2022\n\n[9]: Label, verify, correct: A simple few shot object detection method, CVPR 2022\n\n[10]: Few-shot object detection and viewpoint estimation for objects in the wild, TPAMI 2022\n\n[11]: Meta faster r-cnn: Towards accurate few-shot object detection with attentive feature alignment, AAAI 2022\n\n[12]: Few-shot object detection with fully cross-transformer, CVPR 2022\n\n[13]: NIFF: Alleviating Forgetting in Generalized Few-Shot Object Detection via Neural Instance Feature Forging, CVPR 2023\n\n[14]: DiGeo: Discriminative Geometry-Aware Learning for Generalized Few-Shot Object Detection, CVPR 2023\n\n[15]: Few-shot object detection: a comprehensive survey, IEEE Transaction on Neural Networks and Learning Systems, 2023"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission313/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700362981850,
                "cdate": 1700362981850,
                "tmdate": 1700362981850,
                "mdate": 1700362981850,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nQCGB0xeH1",
                "forum": "7HfliVAtCG",
                "replyto": "YCNhN9gCp1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission313/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission313/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer 2 (W1, part2-3)"
                    },
                    "comment": {
                        "value": "**(W1, part2, computational cost) \"So many parts ensembles together only cost more computational resources.\"**\n\nWe compared our proposed method DE-ViT with recent few-shot work (see Table 5). Our proposed DE-ViT has shortest inference time 0.25s/img, compared to 0.61s/img for Meta Faster RCNN and 3s/img for CrossTransformer, while having significantly higher detection accuracy. DE-ViT also has the smallest number of trainable parameters (23M). The inference time of all compared methods is measured using the same machine.\n\n\nIt is worth noting that the few-shot detection SOTA LVC [1] requires a total of 18 stages for self-training and pseudo-labeling procedures [2]. A pretrained model for LVC has never been released.  Other recent few-shot works also include multiple pretraining and finetuning stages [3, 4].  Our proposed method DE-ViT can be trained in a single stage and used on novel objects directly without any finetuning.\n\n\n\n| Few-shot methods | Backbone | Total Params | Trained Params | Inference Time (Sec / Img) | nAP50 |\n| --- | --- | --- | --- | --- | --- |\n| Meta Faster RCNN [3] | RN101 | 82M | 82M | 0.61 | 31.8 |\n| Cross Transformer [4] | Custom | 24M | 24M | 3 | 35.8 |\n| DE-ViT (Ours) | ViT-S | 70M | **23M** | **0.25** | **42.7** |\n\n\n\n\nCompared against the recent  detectors trained on the large-scale dataset LVIS, our proposed method DE-ViT has the smallest number of total parameters, and is trained orders of magnitude faster than F-VLM and OWL-ViT. \n\n\n|  | Backbone | Total Params | Trained Params | Training Epochs | APr |\n| --- | --- | --- | --- | --- | --- |\n| OWL-ViT | ViT-L | 433M | 433M | 1800 | 31.2 |\n| F-VLM | RN50x64 | 445M | 25M | 118 | 32.8 |\n| DE-ViT (Ours) | ViT-L | **350M** | **23M** | **14.4** | **34.3** |\n\n\nWe updated the paper to include the inference time and parameter size comparison in Section 4.2 at Page 8.\n\nReference: \n\n[1]: Label, verify, correct: A simple few shot object detection method, CVPR 2022\n\n[2]: Official github repo for \"Label, verify, correct: A simple few shot object detection method\" https://github.com/prannaykaul/lvc\n\n[3]: Meta faster r-cnn: Towards accurate few-shot object detection with attentive feature alignment, AAAI 2022\n\n[4]: Few-shot object detection with fully cross-transformer, CVPR 2022\n\n\n\n**(W1, part3, parts ensemble) \"So many parts ensembles together only cost more computational resources.\"**\n\n\nWe did an ablation study to show that a naive ensemble of DINOv2 and prototype-based few-shot detector leads to poor few-shot performance (See Section 4.3). Therefore, each one of our proposed component is critical for our results.\n\n\n| Conventional Prototypes Head | Prototype Projection | Background Tokens | Class Reorder and Resample | nAP50 | nAP75 |\n| --- | --- | --- | --- | --- | --- |\n| \u2713 |  |  |  | 4.5 | 2.2 |\n|  | \u2713 |  |  | 26.2 | 9.7 |\n|  | \u2713 | \u2713 |  | 38.4 | 23 |\n|  | \u2713 | \u2713 | \u2713 | **39.5** | **24.1** |\n\n\nRegarding the concern related to the number of parts, we clarify that our method only has three trainable components, i.e., the Binary Classification Network, the Binary Segmentation Network, and the Spatial Integral Layer, as shown in Figures 2, 3. The number of parts is similar, if not smaller, than those of existing works [1-3].\n\nIn comparison, CrossTransformer [1] includes 4 stages of dense feature cross interactions, and each stage is trained separately. Meta Faster RCNN [2] has fours trainable components: Meta-RPN, Meta-Classifier, Base-RPN and Base-Classifier. CORA [3] has 3 components: region classifier, anchor pre-matching, and deformable DETR encoder/decoder. \n\nReference: \n\n[1]: Meta faster r-cnn: Towards accurate few-shot object detection with attentive feature alignment, AAAI 2022\n\n[2]: Few-shot object detection with fully cross-transformer, CVPR 2022\n\n[3]: CORA: Adapting CLIP for Open-Vocabulary Detection with Region Prompting and Anchor Pre-Matching, CVPR 2023"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission313/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700363169693,
                "cdate": 1700363169693,
                "tmdate": 1700363169693,
                "mdate": 1700363169693,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Vp57btqgOf",
                "forum": "7HfliVAtCG",
                "replyto": "YCNhN9gCp1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission313/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission313/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer 2 (W2, part1)"
                    },
                    "comment": {
                        "value": "**(W2, part1, performance on fewer shots) \"On the few-shot benchmark, the good experiments are more shown on 10 or more shot rather than fewer-shot. \"**\n\n\nRegarding the results on fewer shots, it is common practice for few-shot detection work to only report 10, 30 shots results on COCO, such as [1-4].  In the revised paper, we included plots of the model performance for fewer shots in Figure 7 and A2 for COCO 2014 and COCO 2017, correspondingly. In Section 4.2, we identified the inflection points for each dataset, after which adding more shots does not help. Our model outperforms existing few-shot SOTA results on all shots. \n\nWe updated the experiment section to include the performance analysis with more shots in Section 4.2 and Section A.1.2.\n\nReference:\n\n[1]: Frustratingly Simple Few-Shot Object Detection, ICML 2020\n\n[2]: Label, verify, correct: A simple few shot object detection method, CVPR 2022\n\n[3]: Few-shot object detection and viewpoint estimation for objects in the wild, TPAMI 2022\n\n[4]: DiGeo: Discriminative Geometry-Aware Learning for Generalized Few-Shot Object Detection, CVPR 2023"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission313/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700363390578,
                "cdate": 1700363390578,
                "tmdate": 1700363390578,
                "mdate": 1700363390578,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4Ec1ENmvIe",
                "forum": "7HfliVAtCG",
                "replyto": "YCNhN9gCp1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission313/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission313/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer 2 (W2, part2)"
                    },
                    "comment": {
                        "value": "**(W2, part2, evaluation measures and real performance) \"There are so many different measures shown in the experimental results, which are not convincing to me about its real performance.\"**\n\nWe clarify the measures used in the paper. In the few-shot/one-shot/open-vocabulary detection tasks, classes are split into base and novel [2-4]. Base classes are seen during training and novel classes are unseen. The performance on novel classes is more important. One-shot detection methods conventionally use the term seen / unseen classes, but it has the same meaning as base / novel classes.  One-shot methods conventionally divide 80 classes of COCO into four even partitions, and alternatively takes three partitions as base classes and one partition as novel classes.  There are 4 base/novel splits in total, named as split-1/2/3/4. Metrics on LVIS are computed separately on bounding boxes (e.g., box AP) or instance segmentation masks (e.g., mask AP) [1].\n\n\n\nList of all measures: \n\n- Few-shot \n    - bAP: mAP on base classes\n    - nAP, nAP50, nAP75: mAP, AP50, AP75 on novel classes\n- One-shot\n    - Seen -> Split-1/2/3/4: AP50 on base classes, for each split\n    - Unseen -> Split-1/2/3/4: AP50 on novel classes, for each split\n    - Avg: AP50 averaged among all splits, for base or novel classes\n- Open-vocabulary\n    - COCO\n        - Novel: AP50 on novel classes\n        - Base: AP50 on base classes\n        - All: AP50 on all classes\n    - LVIS\n        - APr: AP on rare categories, which are novel classes\n        - AP: AP on all classes\n        - APc: AP on common categories, which are base classes\n        - APf: AP on frequent categories, which are base classes\n        \n\n\n\nFor few-shot detection, our proposed method DE-ViT outperforms existing SoTA significantly on nAP at 10-shot (+15) and 30-shot (+7.2) in COCO, and +20 box APr on LVIS. For one-shot detection, DE-ViT outperforms SoTA on unseen classes by 2.8 AP50. When compared with open-vocabulary detectors, DE-ViT outperforms SoTA by 6.9 AP50 on COCO, and 1.5 mask APr on LVIS. In summary, our proposed method DE-ViT outperforms all existing solutions on detecting objects of novel categories.\n\n| Few-shot Method on COCO | Finetune on Novel | nAP (10-shot) | nAP (30-shot) |\n| --- | --- | --- | --- |\n| Meta Faster R-CNN [5] | \u2713 | 12.7 | 16.6 |\n| Cross-Transformer [7]  | \u2713 | 17.1 | 21.4 |\n| LVC [6] | \u2713 | 19 | 26.8 |\n| DE-ViT (Ours) | \u2717 | **34** | **34** |\n\n| Few-shot Method on LVIS | APr | APc | APf | AP |\n| --- | --- | --- | --- | --- |\n| DiGeo [9] | 16.6 | 22.8 | 28 | 24.4 |\n| DE-ViT (Ours) | **33.6** | **30.1** | **30.7** | **30.9** |\n\n\n| One-shot Method on COCO | bAP50 | nAP50 |\n| --- | --- | --- |\n| BHRL [3] | 53.6 | 25.6 |\n| SaFT [8] | 48.3 | 24.9 |\n| DE-ViT (Ours) | **59.6** | **28.45** |\n\n| Open-vocabulary Method | Use Extra Training Set | LVIS mask APr | LVIS box APr | COCO nAP50 |\n| --- | --- | --- | --- | --- |\n| CORA+ [10] | \u2713 | - | 28.1 | 43.1 |\n| RO-ViT [11] | \u2717 | 32.1 | - | 34 |\n| F-VLM [12] | \u2717 | 32.8 | - | 28 |\n| DE-ViT (Ours) | \u2717 | **34.3** | **33.6** | **50** |\n\n\n\n\nWe updated Section 4 Paragraph 2 to have a more comprehensive description of the measures, and unify the measure names in Tables 1, 2 and 4.\n\nReference: \n\n[1]: Lvis: A dataset for large vocabulary instance segmentation, CVPR 2019\n\n[2]: Frustratingly Simple Few-Shot Object Detection, ICML 2020\n\n[3]: Balanced and hierarchical relation learning for one-shot object detection, CVPR 2022\n\n[4]: Regionclip: Region-based language-image pretraining, CVPR 2022\n\n[5]: Meta faster r-cnn: Towards accurate few-shot object detection with attentive feature alignment, AAAI 2022\n\n[6]: Label, verify, correct: A simple few shot object detection method, CVPR 2022\n\n[7]: Few-shot object detection with fully cross-transformer, CVPR 2022\n\n[8]: Semantic-aligned fusion transformer for one-shot object detection, CVPR 2022\n\n[9]: DiGeo: Discriminative Geometry-Aware Learning for Generalized Few-Shot Object Detection, CVPR 2023\n\n[10]: CORA: Adapting CLIP for Open-Vocabulary Detection with Region Prompting and Anchor Pre-Matching, CVPR 2023\n\n[11]: Region-aware pretraining for open-vocabulary object detection with vision transformers, CVPR 2023\n\n[12]: F-vlm: Open-vocabulary object detection upon frozen vision and language models, ICLR 2023"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission313/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700363531155,
                "cdate": 1700363531155,
                "tmdate": 1700363531155,
                "mdate": 1700363531155,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9J4NFxNnjH",
                "forum": "7HfliVAtCG",
                "replyto": "YCNhN9gCp1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission313/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission313/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer 2 (W3)"
                    },
                    "comment": {
                        "value": "**(W3, visualizations) \"There is no visualization of the experimental results shown in the paper.\"**\n\nIn the original submission, we have the visualization on detecting YCB objects with our proposed method DE-ViT in Figure 1. In the revised version, we added Figures 8, A9, A10, and A11 to include qualitative results and comparisons of our method and Meta Faster RCNN [1], CORA [2] on COCO and LVIS datasets.  DE-ViT detects more novel objects while having much fewer false positives, as shown in Figures A10 and A11. Base and novel objects are colored in \"green\" and \"yellow\", respectively.\n\nReference: \n\n[1]: Meta faster r-cnn: Towards accurate few-shot object detection with attentive feature alignment, AAAI 2022\n\n[2]: CORA: Adapting CLIP for Open-Vocabulary Detection with Region Prompting and Anchor Pre-Matching, CVPR 2023"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission313/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700363595555,
                "cdate": 1700363595555,
                "tmdate": 1700363595555,
                "mdate": 1700363595555,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DaMBYOsU3q",
            "forum": "7HfliVAtCG",
            "replyto": "7HfliVAtCG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission313/Reviewer_Jjnu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission313/Reviewer_Jjnu"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a paradigm for few-shot object detection which does not rely on fine-tuning the base model or top layers for the few-shot classes. The authors compare the paradigm with few-shot detection, open-vocabulary detection, and one-shot detection. The authors propose to use a few samples from novel classes to create prototypes for objects which can be used to detect new objects in images. The proposed paradigm leads to improved performance compared to prior few-shot, one-shot, and open-vocabulary methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper proposes a different paradigm for few-shot object detection which alleviates the need for fine-tuning during the few-shot inference process. The paper also shows that this paradigm can lead to improved performance compared to prior methods."
                },
                "weaknesses": {
                    "value": "The main weaknesses of the paper are based around a lack of clear motivations for some decisions and some un-fair comparisons. In particular, the authors should respond to the following:\n\n1. I believe that the comparisons to standard open-vocabulary object detection methods are unfair because the setting proposed in this paper is quite different - open-vocabulary (or zero-shot object detection) does not assume any knowledge of visual representation of novel classes, however, the proposed approach does. So, placing this method as an open-vocabulary object detection approach is unfair, and frankly, unnecessary. The authors can focus on few-shot/one-shot approaches - this is already a good paper for these settings.\n\n2. In figure 1, which objects come from based classes and which are the few-shot/novel categories? There are different colours used in the image but no legend. This makes it difficult to ascertain the performance of the proposed approach based on this image.\n\n3. If you are selecting the top-K classes in one step, why is there a need to up-sample to T? Can't we just keep using \"K\" because that is deterministic - just up/down-sample to K?\n\n4. How does the performance change with the number of samples used for prototype creation? From Table 3, it seems that the performance for 30-shot is lower than for 10-shot in some cases. Why? Is there an inflection point where adding more samples stops helping?\n\n5. Do prior one-shot object detection methods also follow the few-shot paradigm of fine-tuning the model? If not, what exactly is the difference between the proposed method and one-shot methods?\n\n6. Suggestion: It might be better to show equation 1 (and other similar things) as dot-products to make it more straight-forward to understand."
                },
                "questions": {
                    "value": "Please see weaknesses above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission313/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699223373417,
            "cdate": 1699223373417,
            "tmdate": 1699635957390,
            "mdate": 1699635957390,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rEs2k7CYYi",
                "forum": "7HfliVAtCG",
                "replyto": "DaMBYOsU3q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission313/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission313/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer 1 (W1 - W4)"
                    },
                    "comment": {
                        "value": "We are greatly thankful to the reviewer for the comments and acknowledgement of our work. We hope the following reply resolves the raised concerns.\n\n**(W1, open-vocabulary setting) \"The comparisons to standard open-vocabulary object detection methods are unfair because the setting proposed in this paper is quite different - open-vocabulary (or zero-shot object detection) does not assume any knowledge of visual representation of novel classes, however, the proposed approach does. So, placing this method as an open-vocabulary object detection approach is unfair, and frankly, unnecessary. The authors can focus on few-shot/one-shot approaches - this is already a good paper for these settings.\"**\n\nWe agree with the reviewer that our proposed method is not an open-vocabulary detector. We revised the entire introduction and experiment sections to focus on few-shot/one-shot settings, clarify the comparison to open-vocabulary detectors. We also revised the method section to clarify the motivations for our technical designs.\n\n\nOur original intent is to use this comparison to indicate the significant improvement we achieved on few-shot object detection. Because the detection accuracy of existing few-shot methods fall behind that of open-vocabulary detectors, especially on challenging datasets such as LVIS (the open-vocabulary SoTA on LVIS has 31.2 box APr, while the few-shot one only has 16.6 box APr). \n\nOur proposed method DE-ViT outperforms the SoTAs on both few-shot and open-vocabulary (33.6 box APr), which has not been achieved before by existing few-shot work. \n\n\n\n**(W2, Figure 1) \"In figure 1, which objects come from based classes and which are the few-shot/novel categories? There are different colours used in the image but no legend. This makes it difficult to ascertain the performance of the proposed approach based on this image.\"**\n\n\n\nWe added legends and colors to differentiate the base and novel categories in Figure 1. Specifically, \"green\" denotes base classes, and \u201cyellow\u201d denotes novel classes. Our proposed method DE-ViT's performance is demonstrated by detecting novel classes such as: mustard, clamp, cheez-it, coffee jar, peg-hole, toy airplane box, comet pine, wood blocks jar, chips, spray bottle, cleaser bottle.\n\n\n**(W3, K vs T) \"If you are selecting the top-K classes in one step, why is there a need to up-sample to T? Can't we just keep using \"K\" because that is deterministic - just up/down-sample to K?\"**\n\n$K$ and $T$ serve different purposes. $K$ is used to select classes, reducing unnecessary computation. $T$ is used to standardize the shape of the similarity map, while preserving information. So they should not be set identical.\n\n\nIn details, we select top $K$ classes to estimate class score, so we want $K$ to be as small as possible. In practice, we use $K=3$ or $K=10$ in our experiments. To estimate probability for each selected class, the input is a similarity map $H\\times W\\times C$ between features and prototypes, where $C$ denotes number of classes. As $C$ is different per dataset, we standardize the input shape to $H\\times W\\times T$, where $T$ is a hyperparameter and set to 128 in our experiments. \n\n\nWe updated the paper to incorporate this answer in Section 3.1 .\n\n\n\n\n**(W4, shots performance) \"How does the performance change with the number of samples used for prototype creation? From Table 3, it seems that the performance for 30-shot is lower than for 10-shot in some cases. Why? Is there an inflection point where adding more samples stops helping?\"**\n\n\nPerformance generally increases with the number of shots. The performance difference between 30-shot (nAP50=52.9) and 10-shot (nAP50=53.0) is smaller than 0.1% and could be interpreted as statistically insignificant. There exists indeed inflection point after which more samples do not help.\n\n| shots | nAP50 (COCO 2014) |\n| --- | --- |\n| 1 | 43.3 |\n| 2 | 50.1 |\n| 3 | 52.5 |\n| 4 | 52.7 |\n| 5 | 52.8 |\n| 6 | 53 |\n| 7 | 52.8 |\n| 8 | 52.9 |\n| 9 | 53 |\n| 10 | 53 |\n| 20 | 52.8 |\n| 30 | 52.9 |\n\n| shots | nAP50 (COCO 2017) |\n| --- | --- |\n| 1 | 22.1 |\n| 2 | 29.9 |\n| 3 | 33.8 |\n| 4 | 36.3 |\n| 5 | 37.3 |\n| 6 | 38.5 |\n| 7 | 39.4 |\n| 8 | 39.8 |\n| 9 | 40.5 |\n| 10 | 40.7 |\n| 15 | 42.3 |\n| 20 | 42.4 |\n| 30 | 43.2 |\n| 40 | 43.5 |\n| 50 | 43.7 |\n| 75 | 43.9 |\n| 100 | 43.9 |\n\n\n\nWe ploted the nAP50 with different shots in Figure 7 and Figure A2 for COCO 2014 and COCO 2017, correspondingly. The inflection point is located around 50 to 75 shots for COCO 2017 and 6 shots for COCO 2014. We updated the paper to detail this analysis in Section 4.3 at Page 9, and Section A.1.2 at Page 14."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission313/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700362509551,
                "cdate": 1700362509551,
                "tmdate": 1700362509551,
                "mdate": 1700362509551,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0DAHBJXaES",
                "forum": "7HfliVAtCG",
                "replyto": "DaMBYOsU3q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission313/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission313/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer 1 (W5-W6)"
                    },
                    "comment": {
                        "value": "**(W5, one-shot vs few-shot) Do prior one-shot object detection methods also follow the few-shot paradigm of fine-tuning the model? If not, what exactly is the difference between the proposed method and one-shot methods?**\n\nOne-shot object detection does not follow the finetuning paradigm of few-shot. However, one-shot detection restricts the setting to single-class detection [1], and focuses on designing interaction mechanisms of dense spatial features between the support image and target images. One-shot detection requires separate inference for each class and does not leverage additional support images even if available, as mentioned in the related work [2]. \n\nCompared to one-shot detection work, our method focuses on the interaction between class-level prototypes with dense features of target images, instead of interaction of dense spatial features between support and target images. Our method does not require separate inference for each class and can leverage multiple support images. \n\n[1]: One-Shot Instance Segmentation, NeurIPS 2018\n\n[2]: One-Shot Object Detection with Co-Attention and Co-Excitation, NeurIPS 2019\n\n\n**(W6, use dot-product in Eqs)**\n\nWe thank the reviewer for this suggestion. We updated the corresponding equations with dot-product."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission313/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700362534602,
                "cdate": 1700362534602,
                "tmdate": 1700362534602,
                "mdate": 1700362534602,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]