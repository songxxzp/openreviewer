[
    {
        "title": "Differentiable Euler Characteristic Transforms for Shape Classification"
    },
    {
        "review": {
            "id": "IVBMUenRuP",
            "forum": "MO632iPq3I",
            "replyto": "MO632iPq3I",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6202/Reviewer_Y6Ss"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6202/Reviewer_Y6Ss"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a differentiable version of a geometry descriptor - ECT aka Euler Characteristic Transform, and apply it to the shape classification. In a nutshell, it computes a curve(s) describing a shape (which can be formalized as an exponential family model) as a alternating sum of sigmoids (a \"smooth counter\" of primitives above / below a collection of hyperplane directions).\nExperimental evaluation is provided on a set of benchmarks, and methods performs on-par with a set of GCN-like baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- (Clarity) Paper is well-written and is easy to follow, a concise description of the math background will be appreciated by the readers.\n- (Originality) Looking at shape descriptors like ECT and studying their performance on realistic applications seems like a novel direction which could benefit a lot of downstream tasks. \n- (Significance) Method produces close to state-of-the-art results, and also seems to be quite scalable. \n- It looks like the method can also be used when defining loss functions as a way to compare shapes, which is nice."
                },
                "weaknesses": {
                    "value": "- (Novelty, minor) One of the main technical contributions of this work is swapping a Dirac function to a sigmoid with a hyperparameter. It is unclear if this is a non-obvious contribution.\n- (Evaluation) Not sure if 5.1 is very meaningful - isn't the task trivial? Providing baseline results would help interpretation.\n- (Evaluation) 5.3 - point cloud classification - it is a bit of an overstatement to say that \"accuracy of 77%\" is \"surprising close to\" 87.0?"
                },
                "questions": {
                    "value": "- How important is the differentiability aspect of DECT? E.g. one could potentially take a predefined set of parameters for (6), and use the output of that as a descriptor? Do you actually estimate the parameters of the transform, and does it add to the performance? \n- In Table 2, CNN performs significantly worse than ECT + MLP. A \"vanilla\" MLP also leads to performance which is quite a bit higher than GCN, which seems strange. Some commentary of reliability of these numbers would be useful.\n- (purely out of interest) Would it be possible to combine the proposed description with methods like GCN, e.g. for dense prediction tasks (point cloud segmentation), where descriptor would be per-primitive (e.g. per point)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6202/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698788276981,
            "cdate": 1698788276981,
            "tmdate": 1699636675936,
            "mdate": 1699636675936,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UckCdndttZ",
                "forum": "MO632iPq3I",
                "replyto": "IVBMUenRuP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6202/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6202/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the time reviewing our paper. Please find our comments below. We highly appreciate that the reviewer shares our enthusiasm for the novelty of this method.  \n\n > One of the main technical contributions of this work is swapping a Dirac function to a sigmoid with a hyperparameter. It is unclear if this is a non-obvious contribution. \n\nThe computation with indicator functions is also novel. One usually does this with a loop, which is less scalable than our method. Moreover, we add differentiability with respect to the angles and with respect to the spatial coordinates, also novel and show our method is sometimes up to an order of magnitude faster than classical GNN methods. When comparing to other topological methods the speedup is even more dramatic.  \n\nThe application of the ECT to learn directions and do point cloud approximation has not been attempted before, and we believe that this is both a novel and valuable contribution to the literature. \n\n> (purely out of interest) Would it be possible to combine the proposed description with methods like GCN, e.g. for dense prediction tasks (point cloud segmentation), where descriptor would be per-primitive (e.g. per point). \n\nWe very much appreciate the curiosity! The DECT, due to its differentiability, is very well suited to be used in a GNN/GCN aggregation layer.  Together with node prediction, we would love to tackle point cloud segmentation in future work. We also believe that we can do this in a flexible and adaptive framework for many types of data.   \n\n> How important is the differentiability aspect of DECT? E.g. one could potentially take a predefined set of parameters for (6), and use the output of that as a descriptor? Do you actually estimate the parameters of the transform, and does it add to the performance? \n\nTheoretically yes. There is an upper bound with respect to dimension, the cardinality of the input data and minimum distance between points that guarantees injectivity. This theoretical bound is typically rather high and for point clouds and graphs in practice and it is typically prohibitive to require injectivity. The precise bound can be found in [1], Theorem 7.14. \n\nHence we use fewer directions and instead opt to learn which directions can discriminate between the classes.  \n\nTo show that this indeed yields increased classification we ran an additional ablation on the differentiability aspect of DECT. Please see the general remarks above or section 5.1 in the manuscript. \n\n> It looks like the method can also be used when defining loss functions as a way to compare shapes, which is nice. \n\nThis is correct and we believe that we can explore this further in future work, hoping to use it for point cloud approximation / sparse approximation and segmentation."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6202/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700335113940,
                "cdate": 1700335113940,
                "tmdate": 1700335113940,
                "mdate": 1700335113940,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "jweJ0b9d1z",
            "forum": "MO632iPq3I",
            "replyto": "MO632iPq3I",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6202/Reviewer_5rSB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6202/Reviewer_5rSB"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a novel computational layer called DECT, which enables learning the Euler Characteristic Transform (ECT) in an end-to-end fashion. The ECT is a powerful representation that combines geometrical and topological characteristics of shapes and graphs. The authors overcome the computational limitations of the ECT and demonstrate its scalability and integration into deep neural networks. They also show that DECT achieves competitive performance in graph and point cloud classification tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- DECT enables learning the ECT in an end-to-end fashion, overcoming the previous inability to learn task-specific representations.\n\n- The method is highly scalable and can be integrated into deep neural networks as a layer or loss term.\n\n- DECT exhibits advantageous performance in different shape classification tasks for various modalities especially graphs."
                },
                "weaknesses": {
                    "value": "- Although ECT is theoretically injective, it happens only when the number of directions is sufficient. For example, for point cloud classification it could be the case that the number of directions is required to be no less than the cardinality of the point set, for ECT to be injective. This restricts the expressivity, especially for the application on point clouds, and explains why the results on point cloud classification is relatively weaker than graph classification.\n\n- One key contribution of the method is the differentiation on both the coordinates and the directions \\ksi. I would like to see an ablation showing the advantage of being able to optimise the direction \\ksi, compared to uniformly sampling the direction. \n\n- Number of directions \\ksi is set as 16 and I would expect an ablation on different numbers.\n\n-  To apply the method to graph learning, it requires the graph to have spatial coordinates."
                },
                "questions": {
                    "value": "- In Eq 1, (-1)^k should be (-1)^n?\n\n- See weaknesses 2 and 3."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6202/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698824898378,
            "cdate": 1698824898378,
            "tmdate": 1699636675786,
            "mdate": 1699636675786,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gUOXLVBPhA",
                "forum": "MO632iPq3I",
                "replyto": "jweJ0b9d1z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6202/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6202/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their time reviewing our paper and for appreciating the utility of DECT for shape classification. \nPlease find our comments below.  \n\n> Although ECT is theoretically injective, it happens only when the number of directions is sufficient. For example, for point cloud classification it could be the case that the number of directions is required to be no less than the cardinality of the point set, for ECT to be injective. This restricts the expressivity, especially for the application on point clouds, and explains why the results on point cloud classification is relatively weaker than graph classification. \n\nThis is indeed true. Guaranteed injectivity depends on the cardinality of the set and dimension see [1]. For classification tasks, only the characteristics of the data distribution are needed, hence we can do with a rather coarse approximation of the ECT and still get useful results. We believe that the point cloud results can also be partially explained by the fact that we eschew an approximation in terms of additional combinatorial complexes. Nevertheless, further research into how to get more expressive representations of the ECT in the context of machine learning are still very much needed.  We would also like to stress that this is the first application of the ECT on more complex datasets, something not attempted yet (except for MNIST and some binary classification tasks) in the literature. \n\nThe literature usually applies a coarse dimensionality reduction such as UMAP to the ECT to get a representation for an SVM. Finding better suited classification representations is still an open question. Such a pipeline does not exploit the fact that similar directions share information, and hence organizing them in a way that allows this spatial information in the representation can be exploited by, for instance, a CNN architecture, leads to better performance. The closest analogy is flattening an image to a single vector, whereby one loses the spatial information that a CNN uses to get its performance.   \n\n> To apply the method to graph learning, it requires the graph to have spatial coordinates. \n\nWe believe that this can be overcome: if data features on the vertices are not present, one can construct them in terms of filtrations, such as node degree or curvature information. Our method will work for these types of metrics as well. Moreover, filtrations can be learned in a task-specific fashion, thus posing no strong restriction of the scope of our work. We will clarify this in the revision. \n\n> Ablations on differentiability and number of directions. \n\nWe agree with the reviewer that an ablation on the number of directions would be a valuable addition to the paper. \n\nWe provide an ablation study on the number of directions. We find that rather few (compared to theoretical injectivity guarantees) provide significant expressivity. \n\n \n\n[1] 1805.09782.pdf (arxiv.org)"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6202/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700335004325,
                "cdate": 1700335004325,
                "tmdate": 1700335004325,
                "mdate": 1700335004325,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8hVu8LnPxn",
            "forum": "MO632iPq3I",
            "replyto": "MO632iPq3I",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6202/Reviewer_v8eE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6202/Reviewer_v8eE"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a differentiable topological shape descriptor that is conceptually based on the Euler characteristic transform (ECT).  Given a shape in its discrete representation as a n-dimensional simplicial complex, ECT computes a descriptor that is a function of a direction and the height function for the topological filtration. The idea is to project the simplices of the shape in all directions and compute the topological property of those directional signatures. Taken together, all these signatures can be concatenated and used together as a global shape descriptor. The main contribution in this paper is to rewrite the ECT in terms of an indicator function (Equation 5), that can be relaxed into a differentiable formulation using the sigmoid, leading to DECT.\n\nThe authors have shown a series of experiments to show the benefits of their approach. Table 1 demonstrates a simple proof of concept that DECT classifies shapes of different topologies. Section 5.2 demonstrates how to use it as a loss as well as optimize the descriptor for best directions and section 5.3 for their use in classifying geometric graphs.  Generally, the experiments make a favorable proof of concept."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- I find the core idea of this paper to be interesting. Rewriting the topological formulations using more computable components like indicator functions and sigmoids is nice. \n- Overall, the paper has been compiled quite well. Despite the relative inaccessibility of the core material, the writing and structure are quite good.\n- The choice of experiments to demonstrate the benefits of the descriptor is refreshing. I particularly enjoyed the angle of investigation in sections 5.1 and 5.2, validating the main message of this paper."
                },
                "weaknesses": {
                    "value": "- I find it hard to truly appreciate a more stronger impact of the proposed descriptor for a wide range of applications. Despite the simplicity and comparable accuracies of Table 2, it would be nice to be more direct in explaining what features of data are simply not achievable using standard feature descriptors and how the proposed contributions alleviate it. \n- More significantly, I see no baseline comparison with other prior topological descriptors. For eg, how do some of the methods in: (Hajij et al., 2023; Hensel et al., 2021,  (Moor et al., 2020; Trofimov et al., 2023; Vandaele et al., 2022) compare with the proposed construction in section 5.1, 5.2 and 5.3?  \n- It would be valuable to elaborate more concretely on the multi-scale aspect of the descriptor. I suspect it comes as a result of the height h, but it's hard to easily make this observation in the paper. Please confirm and elaborate."
                },
                "questions": {
                    "value": "- How do you take inner products along given directions for higher dimensional simplices like the edge and face on a mesh? \n- The ECT and DECT are a set of descriptors lying on the unit hypersphere, and in higher dimensions working with directions sampled on the unit hypersphere becomes computationally very demanding. How can this be alleviated in the current framework? \n- Please annotate/reference the direction and height components in the image of Figure 2, to make it clear. \n\n\nOverall I am on the border for this paper. On the positive side, the paper has been compiled well and the main idea has been enumerated and experimented as a good proof of concept. However, the lack of comparison with conceptually similar baselines is a strong drawback, and more generally the wider applicability of the method is not promoted well. Taken together, I vote for a borderline accept as a pre-rebuttal rating."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6202/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699023705279,
            "cdate": 1699023705279,
            "tmdate": 1699636675676,
            "mdate": 1699636675676,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sxkKbwOeKd",
                "forum": "MO632iPq3I",
                "replyto": "8hVu8LnPxn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6202/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6202/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank reviewer v8eE for the review and very much appreciate that the reviewer also finds the core ideas of our work useful! \n\nPlease find more detailed responses to your queries below:  \n\n> Be more direct in explaining what features of data are simply not achievable using standard feature descriptors and how the proposed contributions alleviate it. \n\n- We believe that our method offers substantial advantages over existing methods along the dimensions of *speed*, *scalability*, *expressivity*, *permutation invariance*, and *flexibility*. \n\n- In terms of *speed*, we are almost an order of magnitude faster than some classical GNN methods. This advantage goes up significantly when comparing to other topological methods. In our framework, training cycles that take regular methods\u2014in particular methods based on message passing\u2014days, take minutes in our framework (and we made sure to use the most optimized implementations of our baseline comparison partners). \n\n- In terms of *scalability*, we are the first topological method that can make use of hardware acceleration and (although not explicitly done in this paper) can also be computed using distributed computing.  \n\n- In terms of *expressivity*, our method can also be used in traditional classification tasks and exhibits performance that is on a par with other methods. Notably, we achieve this using a comparatively simple architecture.  \nMoreover, our method is *permutation invariant* and can be easily used for sets of arbitrary sizes without requiring additional architectural changes. Finally, in terms of *flexibility*, to our knowledge there is no framework that exhibits this level of versatility. Our method can deal with any type of simplicial complex at the same time without changing anything. For instance, `DeepSets` still assumes a fixed cardinality of the input point clouds. GNNs cannot work directly with point clouds. Neither can `DeepSets` work directly on graphs, or simplicial complexes. Our method works on *all* data types without changing a single line of code. \n\n- Representation: Moreover, it outputs a *fixed* size representation of your data! Hence any standard method can be used afterwards. We display this by considering one of the simplest neural network architectures available and still having decent performance.  \n\n- Mathematical guarantees: Viewed as a feature layer, it comes backed up by strong mathematical guarantees. Injectivity being one. \nMoreover, our method can also be used for aggregation layer in a Graph Neural Network architecture. We have not explored this in this paper but plan on doing so in future work.\n\n> Elaborate more concretely on the multi-scale aspect of the descriptor. \n\nFor a multi-scale approach, one would have to define a different type of filtration function, in particular an alpha complex would much better capture this type of information. Our method as of now captures spatial information and directional information.  \n\n> How do you take inner products along given directions for higher dimensional simplices like the edge and face on a mesh? \n\nFunctions on simplicial complexes are defined through an extension principle. One defines a \u201cnormal\u201d function on the vertices (in our case the inner product of the vertex coordinates and the direction) and extend it to higher simplices. In our case we use the maximum extension, i.e. a higher-order simplex is assigned the maximum of its vertices. For example, to compute the value of an edge, we take the maximum of the vertex values it is spanned by. \n\n> The ECT and DECT are a set of descriptors lying on the unit hypersphere, and in higher dimensions working with directions sampled on the unit hypersphere becomes computationally very demanding. How can this be alleviated in the current framework? \n\nThis is indeed one of the core principles and *advantages* of our method as we do not need to sample uniformly\u2014we are learning directions instead. Since we take inner products with the direction vectors and the coordinates of our n-sphere, computations do not get much more involved and this property is very much in contrast to other topological methods, where the dimensions and size and dimension become prohibitive factors rather quickly. In most cases one computes an alpha or Vietoris-Rips complex and this captures the multi-scales aspect of point clouds in a natural fashion. Comparison with additional topological baselines.  \n\nWe will discuss the suggested references in more detail, but they deal with different use cases than ours, namely unsupervised representation learning (in particular the \u2018Topological Autoencoders\u2019 works and follow-ups). While we believe that the ECT has the potential to be used as a loss term in unsupervised data analysis as well, we plan on postponing such an analysis for future work and believe that additional experiments in that direction would be out of scope. Instead, we restrict ourselves to graph classification and shape analysis."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6202/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700334869781,
                "cdate": 1700334869781,
                "tmdate": 1700334869781,
                "mdate": 1700334869781,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FQvZFAZ4wO",
            "forum": "MO632iPq3I",
            "replyto": "MO632iPq3I",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6202/Reviewer_DHr6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6202/Reviewer_DHr6"
            ],
            "content": {
                "summary": {
                    "value": "The paper describes a differentiable graph descriptor based on Euler Characteristics Curves.  In practice, given a graph and a simplex order k, an ECC is constructed by computing, for each d-simplex, the cosine of the angle of the simplex feature with a predefined direction (filtering function) and counting the number of simplices above a given sequence of thresholds. The idea of the paper is to replace the counting with a sum after a soft thresholding, letting the gradient to be backpropagated to both simplex features and the predefined direction. \nThe paper shows promising results on graph classification and a proof of concept experiment for pointcloud optimization."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The proposed method to compute differentiable ECC is straightforward and consists of simply replacing the counting of the elements above a threshold with a sum after a softmax. Nevertheless, the method has significant potential and allows not only the use of ECCs as a graph descriptor but also to investigate of the most significant direction (it is differentiable w.r.t. the ECC direction) and to \u2018invert\u2019 the descriptor and optimize directly the input graph."
                },
                "weaknesses": {
                    "value": "The method description is not easy to follow, and many relevant details are not clear or missing. A detailed list is provided in the question section.\nIn particular, the architecture description is a bit confusing. In \u201cIntegration into deep NN\u201d it is written that MLP + global pooling is used to achieve rotation permutation invariance, but the architecture is then described as a CNN over a 16x16 image. Wouldn\u2019t this break permutation invariance? \n\nA discussion about limits is missing.  For instance, since performing sums over simplices, the method is probably dependent on the sampling density. This is particularly relevant for PC classification. The authors also briefly mention the rotation equivariance of the method, but this is not elaborated much. For instance, if the network is invariant w.r.t. rotation permutation invariance, wouldn\u2019t it make the model also rotation invariant (this probably depends also on the distribution of angles)? \n\nMy last concern is about the experimental part. In particular, to prove the importance of optimizing angles, I believe that the paper should compare DECT with building the ECT with fixed angles. Also, the method should be compared with more recent GNN methods, especially based on higher-order simplices. (e.g. Weisfeiler and Lehman Go Topological: Message Passing Simplicial Networks, Provably Powerful Graph Networks)"
                },
                "questions": {
                    "value": "- In the case of point clouds, how is the graph built? Do you consider all disconnected points?\n- Eq 1 is not clear: what is k in the exponent? \n- Notation in eq 3 and 4 is not straightforward, is the second row the actual function definition? What is x?\n- I find eq 5 difficult to read, especially the definition of the indicator function 1. Also, what is \\sigma_k?\n-Table 3 reports 2 times ECT-CNN."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6202/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6202/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6202/Reviewer_DHr6"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6202/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699094895240,
            "cdate": 1699094895240,
            "tmdate": 1699649337595,
            "mdate": 1699649337595,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Vu7rhWDW26",
                "forum": "MO632iPq3I",
                "replyto": "FQvZFAZ4wO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6202/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6202/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their helpful comments and positive remarks, especially for recognizing the potential of our method, which we indeed believe to permit copious follow-up work. Concerning the novelty, all computational aspects and the description of the ECT are in fact novel as well and deviate from the existing literature. The **key changes** to existing literature are (1) speed, (2) scalability, and (3) differentiability. Please also check out the resource [1] for additional details. We are more than happy to answer any additional questions you might have! \n\nPlease find below a detailed set of responses to the concerns.  \n\n> Architecture description: In \u201cIntegration into deep NN\u201d it is written that MLP + global pooling is used to achieve rotation permutation invariance, but the architecture is then described as a CNN over a 16x16 image. Break Permutation invariance? \n\nWe recognize that additional clarity is required and will rewrite the paper accordingly.  We evaluate two types of architectures in our paper. The first is indeed done by considering the ECT as an image of size {directions} x {discretization steps} and use a CNN or MLP architecture to classify the dataset. The second type of architecture embeds the Euler curve in each direction into a higher-dimensional feature space. We subsequently use the embedding for classification. This means we are not only invariant with respect to permutations of the vertices, but also equivariant with respect to permutations in the directions. We will clarify this in the revision. \n\n > What is the dependence on the sampling density for point clouds etc. \n\nAn exact upper bound for injectivity is given in [2] Theorem 7.14 and depends on the dimension and cardinality of the set. Hence, we believe that our sparse method is rather powerful. \n\n> In the case of point clouds, how is the graph built? Do you consider all disconnected points? \n\nWe count the number of points above a hyperplane in a direction theta and let this hyperplane go from $-\\infty$ to $\\infty$. We do indeed consider all disconnected points as we scale the point cloud to fit in a unit sphere. In the case of point clouds, we do not construct a simplicial complex, we only count the points above hyperplanes in each direction. \n\n> Eq 1 is not clear: what is k in the exponent? \n\nThe reviewer caught a typo, it is corrected\u2014thanks! The Euler Characteristic is computed as the alternating sum of the number of simplices in dimension n. \n\n> Notation in eq 3 and 4 is not straightforward, is the second row the actual function definition? What is $x$? \n\nWe will clarify this in the document, the $x$ is the spatial coordinate of the vertex and $\\xi$ is the direction, viewed as a point on the n-sphere. \n\n> I find eq 5 difficult to read, especially the definition of the indicator function 1. Also, what is $\\sigma_k$?  \n\nWe will add some extra clarification in the manuscript. A k-dimensional simplex is denoted by $\\sigma_k$ and $h_\\xi(\\sigma_k)$ denotes the height of the k-simplex in the direction $\\xi$. This new notation is hopefully clearer. \n\n>Table 3 reports 2 times ECT-CNN. \n\nThat is a correct observation. In the top row we use a low amount of parameters (4k, see the column next to it) for comparisons with other methods. In the row below it we use a higher number of parameters to see how well the model behaves. Therefore, we have also not considered in the highlighting. We will stress this in the manuscript. \n\nAdditional resources: \n- [1] 2307.13940.pdf (arxiv.org)   \n- [2] 1805.09782.pdf (arxiv.org)"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6202/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700334185461,
                "cdate": 1700334185461,
                "tmdate": 1700334383973,
                "mdate": 1700334383973,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "o30gIHrGLW",
                "forum": "MO632iPq3I",
                "replyto": "Vu7rhWDW26",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6202/Reviewer_DHr6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6202/Reviewer_DHr6"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their response. \nI still have a couple of questions/observations:\n- Could they please point me out where in the revised paper they clarified the architecture details and achieved invariances?\n- Also, it would be nice to insert a statement resuming the results of [2] for set cardinality in the main manuscript. If I'm not wrong, at the moment, it is only referenced for what concerns the minimum number of directions. From the answer, I did not understand if cardinality could be a problem of the current formulation or not (maybe using relative frequencies rather than absolute counts could be a solution in case it is)."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6202/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700573331133,
                "cdate": 1700573331133,
                "tmdate": 1700573331133,
                "mdate": 1700573331133,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "U4JNQ3nPGj",
                "forum": "MO632iPq3I",
                "replyto": "pHmR4kV3bN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6202/Reviewer_DHr6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6202/Reviewer_DHr6"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the further clarification, now everything is clear."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6202/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700651362995,
                "cdate": 1700651362995,
                "tmdate": 1700651362995,
                "mdate": 1700651362995,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sSYKOtdGuk",
                "forum": "MO632iPq3I",
                "replyto": "3dIHPaCtDE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6202/Reviewer_DHr6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6202/Reviewer_DHr6"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks, I don't have any further questions. I'll adjust my final score after the discussion period with the PC."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6202/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700656248570,
                "cdate": 1700656248570,
                "tmdate": 1700656248570,
                "mdate": 1700656248570,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]