[
    {
        "title": "Communication-Efficient Gradient Descent-Accent Methods for Distributed Variational Inequalities: Unified Analysis and Local Updates"
    },
    {
        "review": {
            "id": "kBMPzaCdlK",
            "forum": "hORCalGn3Z",
            "replyto": "hORCalGn3Z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1325/Reviewer_HQew"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1325/Reviewer_HQew"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies federated minimax optimization problems. \n\nThe authors proposed Proxskip-GDA and Proskip-L-SVRGDA which generalize the recent advance of Proxskip (Mishchenko et.al 2022) on the minimax (variational inequalities) problems and establish the new state-of-art in terms of communication complexity in both deterministic and stochastic setting.  \n\nHowever, the analysis and design of the proposed methods are very similar to the ones for minization problems, including the deterministic setting (Mishchenko et.al 2022) and the stochastic setting (Malinovsky et.al 2022). The author need to clarify the differences between their methods and the previous Proxskip framework (especially the hardness when generalize these methods from minimization to minimax)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The convergence rates in this paper are significant. They cover the settings of both stochastic and deterministic and establish the new SOTA."
                },
                "weaknesses": {
                    "value": "My main concern about this paper is its novelty. The generalization of proxskip framwork (Mishchenko et.al 2022) into minimax optimization seems direct. The variance reduction variant is interesting, but the technique is also similar to the work for minimization problems (Malinovsky et.al 2022).\n\nFor the experimental part, the author should also compare their methods with Sun et.al 2022 which firstly establish the linear rate under similar settings."
                },
                "questions": {
                    "value": "Please refer to the weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1325/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1325/Reviewer_HQew",
                        "ICLR.cc/2024/Conference/Submission1325/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1325/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698243101183,
            "cdate": 1698243101183,
            "tmdate": 1700722067968,
            "mdate": 1700722067968,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mh6b7ojyRl",
                "forum": "hORCalGn3Z",
                "replyto": "kBMPzaCdlK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1325/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1325/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer HQew"
                    },
                    "comment": {
                        "value": "Thank you very much for your time and insightful comments.\n\nWe thank the Reviewer HQew for appreciating the convergence guarantees provided in our work for different settings (deterministic and stochastic). Moreover, we are glad that HQew finds the variance reduction variant of our algorithm interesting. \nWe address your concerns as follows.\n\n---\n**Question 1**: My main concern about this paper is its novelty. The generalization of proxskip framwork (Mishchenko et.al 2022) into minimax optimization seems direct. The variance reduction variant is interesting, but the technique is also similar to the work for minimization problems.\n\n**Response 1**: Thank you for the comments. We respectfully express a differing viewpoint regarding the originality of our results. While we agree that our work is inspired by Mishchenko et al., 2022 and Malinovsky et.al 2022, we have significantly expanded upon their analysis in various key aspects. This divergence primarily stems from our critical Assumption 2.2, which plays a pivotal role in defining the behavior of operator estimation. This assumption necessitates the incorporation of $M\\gamma^2 \\sigma_t^2$ in our Lyapunov function, leading to a distinctly different analytical approach in our work. At the same time, we need to incorporate convergence analysis tricks appearing in the gradient descent-ascent (GDA) literature to derive the final convergence result.\n\nAn important point to we want to highlight is that the previous works in simple optimization problems involve function suboptimality (a concept that cannot be useful in VI problems) in their proof techniques. In our proofs, there are no function values. Thus, the difference between the two pieces of literature does not solely lie in different settings (minimization vs VIPs) but begins at a deeper conceptual level.\n\nFinally, let us highlight that from a significance perspective, our work provides a unified algorithm framework under a very general estimator setting in the VIP regime; also, besides improved communication complexity, we are the first to provide an algorithm that avoids the bounded heterogeneity assumption. **For the above points, we respectfully stand by our claim of novelty and believe this is not a reason for suggesting rejection.**\n\n---\n**Question 2**: For the experimental part, the author should also compare their methods with Sun et.al 2022 which firstly establish the linear rate under similar settings.\n\n**Response 2**: Thank you very much for the pointer. As suggested, we have incorporated the FedGDA-GT algorithm from Sun et al. (2022) into the comparison and attached the new result [(HERE)](https://ibb.co/PMwcfwT).\n\nLet us also highlight some critical differences in theoretical results between our work and Sun et al. (2022):\n- In terms of convergence guarantees, the algorithm the FedGDA-GT algorithm from Sun et al. (2022) converges only in the full batch setting (deterministic), while our approach and algorithms handle the more challenging stochastic scenarios as well. In addition, our algorithm avoids the compact constraints in the function (a requirement in Sun et al. (2022)). \n- As we can see in the attached plot in the deterministic strongly-monotone quadratic game (Eq.6), our algorithm still outperforms all other candidates, including FedGDA-GT, which further rationalizes the empirical benefits of our proposed ProxSkip-GDA-FL algorithm. Finally, let us note that ProxSkip-GDA-FL enjoys acceleration in terms of communication complexity for solving strongly monotone problems (see our main theorems), which is not the case in Sun et al. (2022). This can explain the reason why ProxSkip-GDA-FL performs better than FedGDA-GT in the experiments.  \n- In addition, as we mentioned in Appendix A of our paper, the gradient tracking's (the technique used by Sun et al. (2022))  provable communication complexity scales linearly in the\ncondition number even when combined with local steps. This does not happen with our methods. In the camera-ready version of our work, we will be happy to include the comparison with the FedGDA-GT algorithm from Sun et al. (2022) in our experiments.\n\n---\n**Final Comments**:\nThank you again for the review and efforts in our paper. Hopefully, we have managed to address all your concerns. **Please consider raising your score if you agree. If you believe this is not the case, please let us know so that we have a chance to respond.** Thank you."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1325/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700637517573,
                "cdate": 1700637517573,
                "tmdate": 1700637517573,
                "mdate": 1700637517573,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gKDuV62j2v",
                "forum": "hORCalGn3Z",
                "replyto": "mh6b7ojyRl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1325/Reviewer_HQew"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1325/Reviewer_HQew"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your response, it has addressed some of my concern. The authors have added numerical experiments in comparison with FedGDA-GT and highlighted their novelty in the analysis. I will be happy to raise my score to borderline accept."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1325/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700722049235,
                "cdate": 1700722049235,
                "tmdate": 1700722049235,
                "mdate": 1700722049235,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "K84eJw0R2J",
            "forum": "hORCalGn3Z",
            "replyto": "hORCalGn3Z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1325/Reviewer_5mwf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1325/Reviewer_5mwf"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies minimax optimization and variational inequality problems in a distributed setting. It proposes and analyzes several novel local training algorithms under a single framework for solving a class of structured non-monotone VIPs."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The minimax problem covers a variety of crucial tasks within the field of machine learning, and it becomes imperative to investigate minimax problems in a distributed context.\n\n2. It proposes a single framework for solving a class of structured non-monotone VIPs"
                },
                "weaknesses": {
                    "value": "1. The main concern is the motivation behind this paper is not clear. Why do we need to design communication-efficient federated learning algorithms suitable for multi-player game formulations\n\n\n2. The experimental analysis is very limited.\n \n3. The main contributions of this paper are theoretical analysis. It would be better if this paper were submitted to other conferences such as COLT, and AISTAT."
                },
                "questions": {
                    "value": "1.  minimax optimization is important. But do GAN, adversarial training, robust optimization and multi-agent reinforcement learning tasks match the study in this paper?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1325/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1325/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1325/Reviewer_5mwf"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1325/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698683318881,
            "cdate": 1698683318881,
            "tmdate": 1699636059860,
            "mdate": 1699636059860,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OQrWNATjEU",
                "forum": "hORCalGn3Z",
                "replyto": "K84eJw0R2J",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1325/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1325/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 5mwf"
                    },
                    "comment": {
                        "value": "Thank you very much for your time. We appreciate your effort in reviewing.\n\nThanks also for acknowledging that the minimax problem covers a variety of crucial tasks within the field of machine learning, and it becomes imperative to investigate minimax problems in a distributed context. We also appreciate that you highlight as a strength our proposed single framework for solving a class of structured non-monotone VIPs.\n\n---\n**Question 1**: The main concern is the motivation behind this paper is not clear. Why do we need to design communication-efficient federated learning algorithms suitable for multi-player game formulations\n\n**Response 1**: Thank you for the question. Let us further explain the motivation of our study. \n\nAs we mentioned in the introduction of our manuscript, in the last few years, more and more machine learning models have moved from the classical optimization formulation to a multi-player game perspective where the problems are formulated as the equilibrium of a game. Also, considering communication-efficient federated learning algorithms are natural with the surge of data-intensive applications and factors like privacy. On that end, several applications require solving min-max optimization problems and VIPs using federated learning algorithms. Recent studies have been conducted on the combination of federated learning approaches and multi-player games. For example, paper [1] (see below) focuses on federated generative adversarial networks (GANs), while the work [2] explains how federated learning algorithms can be used in adversarial learning. Both problems are classical applications formulated as min-max problems (a special case of VIPs) that can also be cast as special cases of our general framework. This clearly motivates the reasons behind working in this area. In addition, existing work on the convergence analysis cannot showcase the superiority of Local GDA-type algorithms, or they need some other strong assumptions like bounded heterogeneity. Regarding all the arguments above, we believe our study here is fully motivated, and our result is significant to the community.\n\n[1] Rasouli, Mohammad, Tao Sun, and Ram Rajagopal. \"Fedgan: Federated generative adversarial networks for distributed data.\" arXiv preprint arXiv:2006.07228 (2020).\n\n[2] Zhang, Jie, et al. \"Delving into the adversarial robustness of federated learning.\" arXiv preprint arXiv:2302.09479 (2023).\n\n---\n**Question 2**: The experimental analysis is very limited.\n\n**Response 2**: Our work primarily focuses on the theoretical analysis of the proposed novel algorithms. As we mentioned in our manuscript with our experiments, our goal was to corroborate our theoretical findings. That is, running experiments on very complicated problems is not the main focus of our work. The selected experiments are well-designed, and through them, we show the performance of our algorithms for solving **robust learning** problems on **real datasets**. This is a practical setting satisfying the assumptions of our setting. \n\nPer the suggestion of Reviewers bdRp and  HQew, we tested the proposed methods in different settings (not captured by our theory) and compared them with other recently proposed methods, and for all settings, our approach has favorable performance. \nFor example, in the burglar problem, our proposed method outperforms Local GDA and Local EG. In addition, for all settings tested, our method is faster than the recently proposed FedGDA-GT. Please see the figures attached in our general response at the beginning of our rebuttal and the individual responses to Reviewers bdRp and  HQew. As we show, our algorithm performs better compared to other state-of-the-art algorithms in the problems we considered.\n\n---\n**Question 3**: The main contributions of this paper are theoretical analysis. It would be better if this paper were submitted to other conferences such as COLT, and AISTAT.\n\n**Response 3**: We politely disagree with this statement. In fact, we believe that ICLR is embracing interdisciplinary works covering both theory and experimental applications, and our paper bridges the gap between theoretical analysis and practical applications. This fusion of theoretical depth and practical relevance aligns closely with the mission of ICLR. Furthermore, our work has significant ties to real-world applications. The contributions presented in our paper are not just novel; they also hold the potential to influence a variety of practical domains. Given this, we consider ICLR a fitting venue for our work as COLT and AISTATS.\n\n---\n**Final Comment**:\nThanks again for your review and efforts in our work. \n\n**If you agree that we have addressed all the issues, please consider raising your score. If you believe this is not the case, please let us know so that we have a chance to respond. Thank you.**"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1325/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700637190758,
                "cdate": 1700637190758,
                "tmdate": 1700637190758,
                "mdate": 1700637190758,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1GlXzLVtGz",
            "forum": "hORCalGn3Z",
            "replyto": "hORCalGn3Z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1325/Reviewer_bdRp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1325/Reviewer_bdRp"
            ],
            "content": {
                "summary": {
                    "value": "This paper suggests a family of algorithms for decentralized and federated learning settings of problems of solving variational inequalities. This algorithms share the same framework which is based on ProxSkip optimisation method. Concise convergence guarantees are obtained for these methods with general assumptions on problem, and obtained convergence rates expectably improve previously existed algorithms, because achieve acceleration and variance reduction."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "All the proofs are concise and easy to follow, which is a metodical merit of the paper. Though the improvements achieved are not surprising, the way they were achieved is demonstrative enough."
                },
                "weaknesses": {
                    "value": "Experiments seem not comprehensive. It would be interesting for authors to consider more complicated variational inequality problems than random quadratic and least squares, for example, particular test matrix games like policeman and burglar problem https://www2.isye.gatech.edu/~nemirovs/BrazilTransparenciesJuly4.pdf, and advanced practical problems like GAN training. Also, figures with comparison of the methods have onlu convergence curves without shadows showing standard deviation of function values from run to run, which is required due to stochasticity of the algorithms."
                },
                "questions": {
                    "value": "1) Is usage of colors in Appendix C motivated?\n2) Only strongly-monotone co-coercitive case is considered in the paper. Can authors report on the convergence guarantees in non-strongly-monotone case?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1325/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1325/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1325/Reviewer_bdRp"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1325/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698959630809,
            "cdate": 1698959630809,
            "tmdate": 1699636059800,
            "mdate": 1699636059800,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DYu2HXfbOu",
                "forum": "hORCalGn3Z",
                "replyto": "1GlXzLVtGz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1325/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1325/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer bdRp"
                    },
                    "comment": {
                        "value": "Thank you very much for your time, and we appreciate your effort in reviewing our work.\n\nWe are glad that the Reviewer bdRp finds the proofs of our results concise and easy to follow. The main concerns of bdRp are related to our paper's experiments. We followed your suggestions to update the numerical experiments. We have added the details to your concerns below: \n\n---\n**Question 1**: Experiments seem not comprehensive. It would be interesting for authors to consider more complicated variational inequality problems than random quadratic and least squares, for example, particular test matrix games like policeman and burglar problem, and advanced practical problems like GAN training. \n\n**Response 1**: The main goal of our experimental section was to corroborate our theoretical findings. As such, we selected popular experiments that satisfy all of our assumptions.\nWe thank the reviewer for the suggestion. Following Reviewer bdRp's suggestion, we used our method to solve the policemen burglar problem, even though this does not satisfy our settings perfectly (it is a bilinear problem). \n\n[(HERE)](https://ibb.co/DKY0LYw) is the result based on our fine-tuned stepsize in the deterministic setting. In the attached figure, we plot the duality gap (on the y-axis) with respect to the moving average of the iterates, i.e., $\\frac{1}{K} \\sum_{k = 1}^K x_k$ (here, $x_k$ is the output after $k$ many communications). Note that the duality gap for the problem $\\min_{x \\in \\mathcal{X}} \\max_{y \\in \\mathcal{Y}} f(x, y)$ at $(\\hat{x}, \\hat{y})$ is defined as $\\max_{y \\in \\mathcal{Y}} f(\\hat{x}, y) - \\min_{x \\in \\mathcal{X}} f(x, \\hat{y})$. You can see that our proposed algorithm clearly outperforms Local EG and Local GDA. We will happily include the extra experiment in the camera-ready version of our work.\n\n---\n**Question 2**: Also, figures with comparison of the methods have only convergence curves without shadows showing standard deviation of function values from run to run, which is required due to stochasticity of the algorithms.\n\n**Response 2**: Thank you very much for the suggestion. In our experiments, each stochastic algorithm in the stochastic setting was run for 10 trials, and in the final figures we plotted the mean of the trajectories (still fair and standard comparison in the stochastic regime). We can easily include the suggestion of the reviewer in the camera-ready version of our work and revise the figures to include shadows showing the standard deviation of the stochastic algorithms. Example for the updated plots for  Figure 1(b) and 1(d) is inlcuded [(HERE)](https://ibb.co/Rcr8S3K) and [(HERE)](https://ibb.co/RbNqLTk), respectively. We plan to include such plots in the camera-ready version of the paper.\n\n---\n**Question 3**: Is usage of colors in Appendix C motivated?\n\n**Response 3**: We simply mark the key variables in the algorithm using colors aiming to facilitate the readability of the paper.\n\n---\n**Question 4**: Only strongly-monotone co-coercitive case is considered in the paper. Can authors report on the convergence guarantees in non-strongly-monotone case?\n\n**Response 4**: Thank you for the question. Let us highlight that our theorems hold quasi-strongly monotone and star-cocoercive problems. This is a structured, non-monotone class of problems, which, as we explained in the paper, includes several non-monotone problems (non-convex, non-concave min-max problems) as special cases.\n\nExtending the convergence guarantees of our proposed methods to different settings is an interesting and challenging future research direction. Let us also note that even in the much simpler minimization problems (single-player), no convergence guarantees with an accelerated communication complexity have been provided beyond the strong convex case.\n\nFollowing the reviewer's suggestion in the first question, we have applied our algorithm in the Policemen Burglar Game, and as shown in the attached figure above, our algorithm still outperforms the popular methods of Local GDA and Local EG. This gives the motivation to dive into the theoretical understanding of the methods in different settings beyond the (quasi-)strongly-monotone case.\n\n---\n**Final Comment**: In our opinion, the issues/weaknesses raised are minor and can be easily handled in the camera-ready version of our work, as explained above.\n**If you agree that we managed to address all issues, please consider raising your score. If you believe this is not the case, please let us know so that we have a chance to respond.**"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1325/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700636795040,
                "cdate": 1700636795040,
                "tmdate": 1700636795040,
                "mdate": 1700636795040,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OljxGGs4W9",
            "forum": "hORCalGn3Z",
            "replyto": "hORCalGn3Z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1325/Reviewer_5WaQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1325/Reviewer_5WaQ"
            ],
            "content": {
                "summary": {
                    "value": "This paper provides a unified analysis of algorithms for general regularized VIPs and distributed VIPs. The proposed algorithms improve communication complexity and have strong performance compared to state-of-the-art algorithms. The paper's main contributions include the development of a new communication-efficient algorithm for solving VIPs, theoretical analysis of the algorithm's convergence properties, and experimental results demonstrating the algorithm's effectiveness on a range of problems."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Originality: The proposed algorithms are inspired by ProxSkip algorithm with a probability and a control variate. As far as I'm concerned, the originality of this paper is not strong, but the strength lies in the framework that recovers state-of-the-art algorithms and their sharp convergence guarantees. \n\nQuality: The paper's theoretical analysis of the proposed algorithm's convergence properties is rigorous and well-supported. The paper's experimental results demonstrate the algorithm's effectiveness on a range of problems.\n\nClarity: The paper is well-written and organized, with clear explanations of the key concepts and results. \n\nSignificance: This paper has the potential to advance the field of distributed/federated learning and have practical implications for solving variational inequality problems in machine learning."
                },
                "weaknesses": {
                    "value": "1. $prox_{\\gamma R}(v)$ in Equation (4) should be $prox_{\\gamma R}(x)$.\n\n2. This paper does not mention the challenges or difficulties in algorithm design or theorem proofs. More explanations may help to clarify this paper\u2019s originality."
                },
                "questions": {
                    "value": "1. Theorem 3.1 shows that ProxSkip-VIP converges to the neighborhood of the solution, while the first experiment shows that the proposed variance-reduced method converges to the exact solution. Please add some words to explain it.\n\n2. Is the choice of the probability ($p=\\sqrt{\\gamma\\mu}$) because of the purpose of analysis? What impact will the change of $p$ have on the performance of the proposed algorithms."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1325/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1325/Reviewer_5WaQ",
                        "ICLR.cc/2024/Conference/Submission1325/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1325/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699552889751,
            "cdate": 1699552889751,
            "tmdate": 1700669065684,
            "mdate": 1700669065684,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jcuGJ0ePvb",
                "forum": "hORCalGn3Z",
                "replyto": "OljxGGs4W9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1325/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1325/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 5WaQ"
                    },
                    "comment": {
                        "value": "Thank you for investing time in the review process and for the feedback on our work.\n\nWe are glad that Reviewer 5WaQ finds our paper well-organized with a clear explanation of concepts and rigorous proofs for convergence guarantees. We also thank the reviewer for acknowledging the potential of our work to advance the field of distributed/ federated learning. Moreover, Reviewer 5WaQ agrees that our experiments explain the effectiveness of the proposed algorithms on a range of problems. We address your concerns as follows:\n\n---\n**Question 1**: Typo in Equation (4)\n\n**Response 1**: Thank you for the pointer. We have fixed it in the updated version.\n\n---\n**Question 2**: This paper does not mention the challenges or difficulties in algorithm design or theorem proofs. More explanations may help to clarify this paper\u2019s originality.\n\n**Response 2**: Thank you for the suggestion. We should certainly add the following paragraph to the camera-ready version of our work (where we will have an extra page for the main paper) highlighting the challenges/difficulties in algorithm design and theorem proofs:\n\n**Challenges or difficulties in algorithm design or theorem proofs**: \nThe theoretical results of our work and the proposed algorithms are inspired by Mishchenko et al. (2022), who first proposed a ProxSkip algorithm for solving the minimization problem.  However, our work extends these ideas to the more challenging min-max and VIP settings. The detailed proof procedures of our work differ substantially compared to Mishchenko et al. (2022) as we allow the use of a more general unbiased operator via the Key Assumption 2.2. This key Assumption 2.2 on stochastic estimates allows us to study several variants of the ProxSkip update rule under a single framework, including the stochastic (Corollary 3.5), the deterministic (Corollary 3.6), and a variance-reduced method (Corollary 3.9).\n\nIn addition, our convergence analysis requires using the quantity $M\\gamma^2 \\sigma_t^2$ in the Lyapunov function appearing in our main theorems, which also made the convergence guarantees of our work much different. Finally, for our convergence results, we needed to use recent convergence tricks from the analysis of gradient descent-ascent (GDA) from Beznosikov et al. (2022b) in combination with the ProxSkip ideas, which was also more challenging.\n\nAn important point that is also good to have in mind is that the previous works in simple optimization problems (Mishchenko et al. (2022) and Malinovsky et al. (2022)) involve function suboptimality (a concept that cannot be useful in VI problems) in the assumptions and their proof techniques. In our setting and proofs, there are no function values. Thus, the difference between the two pieces of literature does not solely lie in different settings (minimization vs VIPs) but begins at a deeper conceptual level.\n\n---\n**Question 3**: Theorem 3.1 shows that ProxSkip-VIP converges to the neighborhood of the solution, while the first experiment shows that the proposed variance-reduced method converges to the exact solution. Please add some words to explain it.\n\n**Response 3**: We agree with the reviewer that the general Theorem 3.1 shows that ProxSkip-VIP (with any unbiased estimator) converges to the neighborhood of the solution.  However, we highlight in Lemma 3.8 that for the special case of a variance-reduced variant, the corresponding parameters $D_1=D_2=0$, which in turn drives the result in Theorem 3.1 to converge exactly to the solution. This is why the variance-reduced method converges to the exact solution in the first experiment. The experiment verifies precisely our theoretical findings.\n\n---\n**Question 4**: Is the choice of the probability ($p=\\sqrt{\\gamma\\mu}$) because of the purpose of analysis? What impact will the change of $p$ have on the performance of the proposed algorithms.\n\n**Response 4**: Thank you for the interesting question. The general result of Theorem 3.1 holds under any choice of probability $p$. However, to obtain the accelerated communication complexity in Corollary 3.2, the choice $p=\\sqrt{\\gamma\\mu}$ is needed. As the reviewer pointed out, this is an artifact of the proof techniques related to our theoretical analysis. \nWe also agree that different choices of $p$ will lead to different convergence guarantees for the proposed algorithms.\nThis requires a detailed check on each different choice of $p$. For example, if $p$ is set to be a small enough choice satisfying $\\tau=p^2$, via our theory, we can show that the stepsize $\\gamma$ is required to satisfy\n$\n\\frac{p^2}{\\mu}\\leq \\gamma\\leq \\sqrt{\\frac{p^2\\epsilon}{2(D_1+MD_2)}}.\n$\nThus, in some particular cases of parameter settings, there will be no feasible $\\gamma$ that leads to convergence. \nWe highlight that our proposed parameter choice is universally applicable (there are always choices of all parameters that guarantee convergence)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1325/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700636227924,
                "cdate": 1700636227924,
                "tmdate": 1700637208911,
                "mdate": 1700637208911,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OMwbIBUbDo",
                "forum": "hORCalGn3Z",
                "replyto": "jcuGJ0ePvb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1325/Reviewer_5WaQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1325/Reviewer_5WaQ"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for addressing my concerns. The explanations about the challenges in algorithm design and theorem proofs help a lot to clarify this paper\u2019s novelty. I will be raising my score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1325/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668933152,
                "cdate": 1700668933152,
                "tmdate": 1700668933152,
                "mdate": 1700668933152,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]