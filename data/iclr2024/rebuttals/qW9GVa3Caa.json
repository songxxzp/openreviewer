[
    {
        "title": "Prototype Generation: Robust Feature Visualisation for Data Independent Interpretability"
    },
    {
        "review": {
            "id": "xIE3FHT1V7",
            "forum": "qW9GVa3Caa",
            "replyto": "qW9GVa3Caa",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7267/Reviewer_5oJb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7267/Reviewer_5oJb"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a methodology called Prototype Generation for enhancing the interpretability of deep learning models. The authors demonstrate the effectiveness of this approach in identifying biases, spotting spurious correlations, and enabling rapid iteration in model development. The paper also discusses the advantages of this methodology in terms of understanding what a model has learned and facilitating targeted retraining. However, the authors acknowledge the limitations of their method and suggest future work to address these limitations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Overall, the paper presents an interesting approach to improving the interpretability of deep learning models through Prototype Generation. The methodology is well-explained, and the examples provided effectively demonstrate its potential benefits. The paper also highlights the importance of interpretability in high-stakes applications and the need for a deeper comprehension of model behavior.\n\n1. The paper addresses an important problem in the field of deep learning - interpretability. \n2. The Prototype Generation methodology is well-described and provides meaningful insights into model behavior. \n3. The examples and case studies presented in the paper effectively demonstrate the usefulness of prototypes in identifying biases and understanding model failures. \n4. The discussion on targeted retraining and the iterative feedback loop adds practical value to the proposed methodology."
                },
                "weaknesses": {
                    "value": "1. The limitations of the method are acknowledged but not thoroughly discussed. It would be helpful to provide more insights into the potential challenges and drawbacks of Prototype Generation.\n2. The paper could benefit from a more detailed comparison with existing interpretability techniques to highlight the novelty and advantages of the proposed approach."
                },
                "questions": {
                    "value": "Please help to check the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7267/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698649803616,
            "cdate": 1698649803616,
            "tmdate": 1699636866790,
            "mdate": 1699636866790,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2yUKyE0VrM",
                "forum": "qW9GVa3Caa",
                "replyto": "xIE3FHT1V7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7267/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7267/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their thoughtful comments and suggestions.\n\nWe have expanded our Future Work section to show more future direction directly targeting many limitations brought up by the reviewers."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7267/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700590788102,
                "cdate": 1700590788102,
                "tmdate": 1700590788102,
                "mdate": 1700590788102,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "kdtJHT2VnT",
            "forum": "qW9GVa3Caa",
            "replyto": "qW9GVa3Caa",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7267/Reviewer_fFHp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7267/Reviewer_fFHp"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors proposed a method called \"prototype generation\" to visualize the prototypical input (called the \"prototype\") that would maximally activate the logit of a particular class c. To find such a prototype for each class c for a given neural network, the method starts from a baseline image (generated by optimizing a probability variance loss, which encourages the generated baseline image to have an equal probability to be classified into any class by the network), and then optimizing the baseline image by minimizing the negative logit of the desired class c and the high-frequency loss that penalizes large differences between adjacent pixel values. The expected result is a prototypical input image for class c that maximizes the logit for class c and is smooth and \"natural\" looking. The authors also performed experiments using a trained ResNet-18 and a trained InceptionV1."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- The authors made an attempt to formalize the notion of \"path similarity\" between a generated prototype and a \"natural\" image, using L1 distance or spearman similarity."
                },
                "weaknesses": {
                    "value": "- The proposed method lacks novelty. It is exactly the same as activation maximization applied to the logit of each class c, with regularizations to smooth generated images. This idea has been well explored in past work (e.g., Nguyen et al., 2016).\n- There is only one prototype generated per class. In reality, each class could have multiple prototypical images.\n- While there is an attempt to make generated prototypes more \"natural,\" they do not actually look natural.\n- While the authors made an attempt to formalize the notion of \"path similarity\" between a generated prototype and a \"natural\" image, this notion is not used to generate prototypes."
                },
                "questions": {
                    "value": "N/A."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A."
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7267/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698788995408,
            "cdate": 1698788995408,
            "tmdate": 1699636866671,
            "mdate": 1699636866671,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qRiTKz0Alh",
                "forum": "qW9GVa3Caa",
                "replyto": "kdtJHT2VnT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7267/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7267/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their comments and thoughts. \n\nWe overturn previous claims by Geirhos et al. 2023 [1] about the disparity between internal activations of natural activations and feature visualizations by introducing Prototype Generation, a technique that does lead to natural internal activations. We do this by adding in carefully tested objectives and regularisations for path similarity. This is highly non-trivial.\n\nWhile the notion of path similarity is not used to generate prototypes directly, we used this to test our regularisation hyperparameters as shown in Appendix A.\n\nWe have experimented with using path similarity as a metric to generate prototypes with limited success pointing to redundancies in oversubscribing to this objective.\n\nWe acknowledge the need for a diversity objective and it\u2019s implications in the new Future Work section. \n\n[1] Robert Geirhos, Roland S. Zimmermann, Blair Bilodeau, Wieland Brendel, and Been Kim. Don\u2019t trust your eyes: on the (un)reliability of feature visualizations, July 2023."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7267/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700590785337,
                "cdate": 1700590785337,
                "tmdate": 1700590785337,
                "mdate": 1700590785337,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RJkNO2Wv7k",
            "forum": "qW9GVa3Caa",
            "replyto": "qW9GVa3Caa",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7267/Reviewer_WSWR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7267/Reviewer_WSWR"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a technique for global interpretability of image classification networks via prototype generation. The main idea is to optimize for an input image (called a prototype for a class) which maximizes the logit corresponding to that class while keeping model parameters constant. The authors then determine a \u2018good\u2019 prototype by measuring the spearman correlation between activations of the prototype across all layers ($A_P$) vs the activations of a sample of images from  the class across all layers ($A_I$). They also use L1 distance as another distance metric to measure similarities on $A_P$ and $A_I$. \n\nExperiments are performed on pertained ResNet-18 and Inception V1 on ImageNet. The authors visualize the prototypes on the \u2018academic gown\u2019 and \u2018mortarboard\u2019 (graduation cap) classes in ImageNet. The gown prototype shows a patch resembling a face with a light skin and the authors conclude that this would result on the model being biased against darker skinned people wearing gowns. This is confirmed via experiments on a hold out test set which shows 12.5% higher performance on lighter skinned individuals."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Significance of the problem: The paper introduces a technique to very important problem in the interpretability community (easy-to-understand global interpretability). \n2. The ideas in the paper are simple to understand and the intuition is well explained.\n3. Novelty: Global interpretability via visualization for image classification models has not been done before to my knowledge. I think this makes the technique interesting."
                },
                "weaknesses": {
                    "value": "1. My main concern lies with the qualitative nature of the findings presented in the paper and the resulting ambiguity in the explanations. The authors claim that \u201cthe academic gown prototype in Figure 6a shows a lighter-skinned face prominently\u201d. Here 6a refers to the image of the prototype for \u2018academic gown\u2019.  Just by looking at that image, I cannot definitively say that the light colored patch is definitely a human face. Without looking at images in the dataset, one cannot assume that there is no other object that shows up in a prototype. Since one of the main claims of the paper is that their method allows us to explain models without combing through the dataset, I\u2019m unconvinced about the practical gain the authors claim their method provides.\n2. The second experiment in the paper is about the \u2018mortarboard\u2019 (graduation cap) class where the authors look at the prototype and conclude that the model would get confused if they saw faces with caps in the image instead of just caps. Again, the prototype images only contain light colored patches so it is impossible to conclude that they are faces with certainty unless you are explicitly looking for faces in the first place. Thus, in this case, the user already had an explanation in mind and they are only looking at the model for a confirmation of their pre-existing biases rather than a novel explanation.\n3. Experiments not comprehensive enough: The authors report experiments on two classes from ImageNet (no justification was given for why these two were chosen specifically). To make sure their method works, we would need to know if this method generalizes to more classes.\n4. Details lacking: At several points while reading the paper, I felt there were some significant details lacking. For example: Equations for the loss are not stated (what is the high frequency loss?). Similarly, the authors report having adapted the visualization technique from Olah et al. It would be nice to have a brief description of technique so that the paper is more self contained. I was also confused by Fig 3 which talks about optimizing parameters. I was under the impression that we are optimizing the input image w.r.t output logits. What parameters are we talking about here? Please correct me if I am wrong.\n5. Minor: Notation in equation 1 is not clearly explained. I think I understand what each of the terms mean from the context but clearly spelling out what each term means (including the subscripts) is important for good readability.\n6. Limitations: Since the activations of the prototype is compared to the average of a sample of images from the input class distribution, the prototype is going to be biased towards covering the average of the most common features and ignore outliers."
                },
                "questions": {
                    "value": "1. Why have a single prototype for each class? I would imagine having multiple prototypes would enhance coverage over the full diversity of class images.\n2. What about outliers? When we take a mean over activations, we would effectively start to ignore outliers in our explanations.\n3. Wouldn\u2019t using the high frequency loss (no large difference in adjacent pixel values) smooth out the prototype image and, thus, cause the prototype to look more diffuse? I\u2019m not sure if this is a good or a bad thing so I\u2019m curious to know what the authors think."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7267/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699297742343,
            "cdate": 1699297742343,
            "tmdate": 1699636866574,
            "mdate": 1699636866574,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vppKpiCiRB",
                "forum": "qW9GVa3Caa",
                "replyto": "RJkNO2Wv7k",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7267/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7267/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their time and effort in this well-thought out review. \n\nWe\u2019ll start by addressing the questions:\n\n1. Although multiple prototypes would cover all possible features that would contribute towards a particular class prediction, in this paper we were interested in exploring the potential of prototypes to cover the most relevant features of a class. We are very interested in adding robust diversity functionality in the prototype generation process and we have included this in our expanded future work section.\n\n2. While it is true that taking the mean of activations would subdue outliers, this does not stop us from using the mean of activations to meaningfully compare our prototypes that aim to capture the most useful features in the average case with natural images. The notion of path similarity introduced by Geirhos et al 2023 [1] does fall into this trap of not appropriately addressing outliers.\n\n3. The high frequency loss would lead to diffuse prototypes in isolation but it is an important objective to have in conjunction with all the other objectives in play during prototype generation. In our implementation, the high frequency loss helps smooth out adversarial jaggedness and artifacts rather than promote diffusion.\n\nAddressing the weaknesses:\n\nAlthough the prototypes require some subjective reasoning, they do enable development of hypotheses that can be effectively tested. Looking through a dataset of millions of examples is prohibitively impossible and even in the case of a reasonably sized dataset, hypotheses built purely on dataset analysis are boundless. Our prototypes help bound this process and guide us to actionable insights by directly querying the model.\n\nWe do agree with the need for diversity and more natural-looking prototypes which we have added to our Future Work section.\n\nOur comprehensive experiments consist of comparing spearman similarity between prototype activations and natural image activations. Since we show high similarity we go ahead and show a case study of gathering insights from prototypes for two classes that seem interesting in our subjective analysis.\n\nActivation similarity is compared to counter the claims by Geirhos et al. 2023[1] , which demonstrated that visualisations using the established method have very unnatural activation paths. We include this measure against the mean natural image paths to demonstrate that our prototype generation method produces inputs that do have natural activations. We do not use this metric to generate our prototypes so it is possible that the prototypes can show outlier features in conjunction with the average features for a given class.\n\n[1] Robert Geirhos, Roland S. Zimmermann, Blair Bilodeau, Wieland Brendel, and Been Kim. Don\u2019t trust your eyes: on the (un)reliability of feature visualizations, July 2023."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7267/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700590782134,
                "cdate": 1700590782134,
                "tmdate": 1700590782134,
                "mdate": 1700590782134,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Uj9Unud3LV",
                "forum": "qW9GVa3Caa",
                "replyto": "vppKpiCiRB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7267/Reviewer_WSWR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7267/Reviewer_WSWR"
                ],
                "content": {
                    "title": {
                        "value": "Response to Rebuttal"
                    },
                    "comment": {
                        "value": "I thank the authors for their response.\n\nI do agree that this technique helps narrow down a space of possible explanations but it still does not provide a concrete explanation as has been stated in the paper and that is what I object to in my 'Weaknesses' section. \n\nThe authors also haven't added details about their losses etc as I mentioned the review. I will, thus, maintain my score."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7267/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700729424794,
                "cdate": 1700729424794,
                "tmdate": 1700729424794,
                "mdate": 1700729424794,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]