[
    {
        "title": "Valley: Video Assistant with Large Language model Enhanced abilitY"
    },
    {
        "review": {
            "id": "n2PPefVYG3",
            "forum": "bjyf5FyQ0a",
            "replyto": "bjyf5FyQ0a",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1069/Reviewer_Vdhg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1069/Reviewer_Vdhg"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces Valley for comprehending video, image, and language within a general framework. Valley is developed by combining a large language model, a temporal modeling module, a visual encoder, and a projection module, and is trained on a video instruction dataset using a two-stage tuning procedure. Experiments show that Valley has the potential to be a highly effective video assistant."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is well-written and easy to follow.\n2. The authors collect a video instruction-following dataset using ChatGPT with diverse types of tasks and categories, which can be helpful for instruction tuning VideoLLMs.\n3. The propsed Valley is able to comprehend video, image, and language within a general framework."
                },
                "weaknesses": {
                    "value": "1. The authors mention that they collect videos from Jukinmedia2 that provides videos with wide detailed descriptions, but do not clarify where the descriptions come from. They claim that the instrcution data constructed in this way will not bring about the illusion of object level. However, since the data is constructed by ChatGPT, which is prone to hallucination, this claim may not be entirely accurate.\n2. The authors evaluate their model on general video question answering tasks, but on domain-specific image understanding benchmarks,  which seems illogical to me. As the authors claim that Vally can comprehend video, image, and language within a general framework, common image question answering datasets such as VQAv2 and GQA should be evaluated."
                },
                "questions": {
                    "value": "Please see Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "In my opinion, no ethics review are needed."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1069/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1069/Reviewer_Vdhg",
                        "ICLR.cc/2024/Conference/Submission1069/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1069/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698059818644,
            "cdate": 1698059818644,
            "tmdate": 1700891490357,
            "mdate": 1700891490357,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ytvPKyKqIr",
                "forum": "bjyf5FyQ0a",
                "replyto": "n2PPefVYG3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1069/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1069/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal to Reviewer Vdhg"
                    },
                    "comment": {
                        "value": "We are thankful for your meticulous review and valuable recommendations. It is heartening to see that you recognize the merits of this work. Our responses to your queries are provided below:\n\n### Q1: About descriptions of collected video instruction dataset from Jukinmedia2 and less hallucination of this dataset.\n\n**A**: See details in global response\n\n### Q2: Lack of domain-specific image understanding benchmarks\n\n**A**: We acknowledge that our focus is primarily on video understanding, which is why we have fewer experiments specifically related to images. However, we want to emphasize that Valley, in terms of its structure, is capable of comprehending both images and videos. The image case analyses presented in the paper demonstrate this capability. Also, we have evaluated our valley model on LLaVA benchmark\uff0csee details in global response. Results have demonstrated its effectiveness on image understanding.\n\nWe appreciate your suggestion to include benchmarks such as VQAv2 and GQA in our future work."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1069/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733315451,
                "cdate": 1700733315451,
                "tmdate": 1700733315451,
                "mdate": 1700733315451,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "EUnEVtNpYM",
            "forum": "bjyf5FyQ0a",
            "replyto": "bjyf5FyQ0a",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1069/Reviewer_1s3m"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1069/Reviewer_1s3m"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces Valley, a multimodal foundational model designed to comprehend video, image, and language within a unified framework. The authors have constructed a video instruction dataset using ChatGPT, which aids in the creation of task-oriented conversation data covering a wide range of tasks. A two-stage tuning procedure is adopted for model training. Both qualitative and quantitative experiments reveal that Valley holds promise as an effective video assistant, capable of simplifying complex video understanding scenarios."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This paper gathers a 73k video-based instruction dataset with the help of ChatGPT. This is somewhat larger than the instruction datasets used in previous methods (e.g., VideoChat uses 11K video instruction data). Judging from the experimental results provided by the authors, the quality of this dataset appears to be quite good.\n- Experiments show that Valley excels in visual question answering and captioning, demonstrating optimal performance, strong zero-shot capability. It also generates content with fewer hallucinations compared to similar models."
                },
                "weaknesses": {
                    "value": "- In terms of instruction dataset construction, there seems to be a lack of innovation and comparative experiments. It appears that a higher-quality data source was simply used to collect data, and then common methods were employed to construct the instruction dataset. This was combined with instruction datasets from previous methods to obtain a larger instruction dataset. The decent performance achieved by this method in quantitative analysis may reflect the quality of the instruction dataset to some extent. However, there has been no comparison of the quality of the instruction dataset collected in this paper with that of previous datasets under fair conditions (such as equal data volume).\n- In terms of the model methodology, there is a lack of innovation and comparative experiments. This paper attempts three temporal modeling strategies for video input, but these are very basic methods that have been widely used in tasks such as action recognition. There are no effective improvements aimed at enhancing conversational capabilities. Furthermore, the comparison with other methods like VideoChat is not detailed enough. More comparisons are needed (e.g. performance comparison under\nthe same dataset or same backbone) to demonstrate that the temporal modeling and other modules used in this paper have superior performance and efficiency."
                },
                "questions": {
                    "value": "1. More fair experimental comparisons are needed to demonstrate that the instruction dataset collected in this paper is of higher quality than previous instruction datasets.\n2. More fair experimental comparisons are needed to demonstrate that the methodology proposed in this paper performs better in terms of performance or efficiency compared to previous methods.\n3. Are there any technical contributions that I have overlooked? In my view, the biggest difference between Valley and previous methods is the instruction dataset used."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1069/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698802009472,
            "cdate": 1698802009472,
            "tmdate": 1699636033464,
            "mdate": 1699636033464,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "a0eN6KhF4V",
                "forum": "bjyf5FyQ0a",
                "replyto": "EUnEVtNpYM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1069/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1069/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal to Reviewer 1s3m"
                    },
                    "comment": {
                        "value": "We are appreciative of your comprehensive review and beneficial suggestions. It is encouraging to know that you acknowledge the strengths of this work. Our answers to your questions are as follows:\n### Q1&Q2: Regarding the fairness of our proposed Valley and dataset\n\n**A**: We re-trained on the valley-v1 structure using only videochat data (videochat-instruct-11k), and the experimental result on three datasets (MSVD, MSRVTT, and ActivityNet) are detailed in global response. We can see that under the same amount of instruction data (11k), Valley performs better than the videochat model of the same size (13b). It shows that we use simple linear layers to align video and text modalities is effective. And we prove that splice VIT's \"[CLS]\" token for temporal representation performs well.\n\nAnd through experiments, we also prove the advantages and disadvantages of different timing modeling methods (valley-v1, valley-v2, valley-v3) under the same amount data, which has guiding significance for future research.\n\n\n\n### Q3: Regarding the novelty of our proposed Valley\n\n**A**: \n1. The prevalent works, such as VideoChat, Video LLaMA, adopt Q-former to bridge the textual and visual modality, while we turn to the simpler linear projection layer.\n2. Similar concurrent works such as Video-ChatGPT also use the linear projection layer, but its use mean pooling on both spatial and temporal tokens for video modeling, which is sub-optimal in building video assistant. We design three temporal modeling methods, and experimentally verified the superiority of valley-v3 in temporal understanding compared to other baselines.\n3. We collect and design a large video instruction-following dataset with the assistance of ChatGPT. Experiments demonstrate the quality of our proposed dataset. We have released this dataset 'Valley-702K', and there are already works beneficial from our proposed dataset such as *Video-LLaVA*[1]\n\n### Reference\n\n[1] Lin B, Zhu B, Ye Y, et al. Video-LLaVA: Learning United Visual Representation by Alignment Before Projection[J]. arXiv preprint arXiv:2311.10122, 2023."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1069/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700729986087,
                "cdate": 1700729986087,
                "tmdate": 1700734827742,
                "mdate": 1700734827742,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NDflAHib2a",
            "forum": "bjyf5FyQ0a",
            "replyto": "bjyf5FyQ0a",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1069/Reviewer_AvQ5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1069/Reviewer_AvQ5"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a new video-based model using instruction-tuning. The key is to collect a large number of videos with detailed captions. Given the collected video-instruct data as well as the publicly available image-instruct data, they conduct a two-stage training method: (1) train the projection layer and (2) train both the projection layer and LLM. Experimental results show promising improvements on multiple public benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This is a simple and effective method. The paper is well-written and easy to follow.\n- In my humble opinion, this work could be one of the first to explore instruction tuning in the video domain.\n- Strong results on multiple benchmarks.\n- The constructed dataset should be a valuable resource to the community."
                },
                "weaknesses": {
                    "value": "- While the data collection pipeline is well-formulated, this method requires very high-quality training data.  Gathering high-quality video instruction data remains very challenging when aiming for large-scale training. This prohibits very large-scale training to significantly boosting the model quality, especially when it comes to the video domain where the video data is often sparse and requires a very large number of training data.\n-  The direct integration of vision transformers and LLMs may encounter obstacles, especially when dealing with lengthy videos. While the method was tested on some benchmarks, the videos tested in this paper are often very short (only a few seconds). There is still a long distance to real-world video applications.\n- It is also difficult to ensure no hallucination in the instruction tuning data."
                },
                "questions": {
                    "value": "see weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1069/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698812475028,
            "cdate": 1698812475028,
            "tmdate": 1699636033394,
            "mdate": 1699636033394,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "N49XWL00Ld",
                "forum": "bjyf5FyQ0a",
                "replyto": "NDflAHib2a",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1069/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1069/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal to Reviewer AvQ5"
                    },
                    "comment": {
                        "value": "We are grateful for your thorough review and helpful recommendations. And we are encouraged that you appreciate the strengths of this work. Our responses to your questions are as follows:\n\n### Q1\uff1aAbout high-quality video instruction data.\n\n**A**: We completely agree with your viewpoint and acknowledge the importance of high-quality video instruction data for our method. We are also well aware that video data in the domain is often sparse and requires a substantial amount of high-quality training data.  Therefore, in our current research, we have made significant efforts to gather the available high-quality data from open websites and design instruction and conversations to train the model. Based on our knowledge, it is of the largest scale in existing video instruction dataset. And the results prove its \"high quality\". We hope the data will be helpful to the community and call for researchers to seek additional resources and collaborations to obtain more high-quality video data from the real world. \n\n### Q2:  Obstacles between vision transformers and LLMs and long distance to real-world video applications due to the length.\n\n**A**: Thank you for your insightful comments regarding the misalignment between the Vision Transformer (ViT) and the Language and Vision Model (LLM). To address this issue, ViT from CLIP which has undergone text-to-vision alignment training is selected and we have pretrained a projection layer with large amount of video-text pairs and image-text pairs to effectively align the representations. Regarding the evaluation datasets, we selected them based on wide usage in video understanding research. However, we acknowledge the importance of considering a diverse range of evaluation datasets, especially lengthy videos, and will take your suggestion into account for future research.\n\n### Q3: Difficult to ensure no hallucination in the instruction tuning data.\n\n**A**: See details in Global response, other works have used our dataset, e.g. *VideoLLaVA*[1]\n\n### Reference:\n\n[1] Lin B, Zhu B, Ye Y, et al. Video-LLaVA: Learning United Visual Representation by Alignment Before Projection[J]. arXiv preprint arXiv:2311.10122, 2023."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1069/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700729737102,
                "cdate": 1700729737102,
                "tmdate": 1700734796485,
                "mdate": 1700734796485,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HekmcCEcqj",
                "forum": "bjyf5FyQ0a",
                "replyto": "N49XWL00Ld",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1069/Reviewer_AvQ5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1069/Reviewer_AvQ5"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you for providing answers to my questions. Agree with most of the replies which are largely aligned with my initial review, and this is also my reason for initially voting for a score 6 (weak accept). \n\nRegarding Q2, my question is not about the alignment, but more about the model design to potentially process various video lengths (i.e., ranging from a few seconds to hours). I wonder if the authors have some possible remedies to address this challenge.  In fact, this question is relevant to the technical novelty concerns raised by fellow reviewers.\n\nOn the other hand, I totally agree with using the widely used benchmarks. However, in the era of LLMs/LMMs, these models have demonstrated advanced capabilities that can directly address real-world problems. In my humble opinion, the current eval benchmark may not clearly reflect the full capability of the advanced LLM/LMM-based models. In other words, evaluating with the selected benchmark (e.g., with very short videos) may limit the scope of this work."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1069/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700735014049,
                "cdate": 1700735014049,
                "tmdate": 1700735014049,
                "mdate": 1700735014049,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "G2UhZFs7MN",
            "forum": "bjyf5FyQ0a",
            "replyto": "bjyf5FyQ0a",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1069/Reviewer_rHRB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1069/Reviewer_rHRB"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes large video-language model Valley which consists of an LLM, a temporal modeling module, a visual encoder, and a cross-modality projection module. This work also constructs a video instruction dataset using the ChatGPT to obtain conversational data for multi-shot captions, long video descriptions, action recognition, and causal relationship inference. Valley is evaluated on different benchmarks including MSVD-QA, MSRVTT-QA, Meme-Cap and Science-QA."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* Valley achieves the state-of-the-art performance of multiple video QA benchmarks MSVD-QA, MSRVTT-QA and ActivityNet-QA.\n* Valley collects a dataset of 100k videos with detailed caption and plans to release the dataset which will benefit the research community."
                },
                "weaknesses": {
                    "value": "* It is not clear what is the technical novelty of the proposed method Valley. Throughout the introduction, related works, and method sections there is not statement that explains the technical difference distinct from the existing video-language models.\n* No ablation is provided other than the temporal modeling modules (v1, v2, v3), which also makes it difficult to judge what technical component mainly contributes to the performance.\n* Among the temporal modeling modules, what is the unique advantage of the version-2 with linear layers over v1 and v3? There seems no benchmark where the v2 performs the best and so it is not clear why v2 is proposed as one of the main methods. \n* There is no implementation details. It seems not easy to reproduce the result based on what is provided in the main manuscript."
                },
                "questions": {
                    "value": "* While the dataset collection is one of the main contributions, there seems no license information about the used video data (https://www.jukinmedia.com/).\n* What is the model size and runtime comparison with other state-of-the-art methods (VideoChat, Video LLaMA, Video-ChatGPT)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Responsible research practice (e.g., human subjects, data release)"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "This work collects and plans to release a dataset with 100K videos from https://www.jukinmedia.com/, and corresponding text annotations using the ChatGPT."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1069/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698951174878,
            "cdate": 1698951174878,
            "tmdate": 1699636033324,
            "mdate": 1699636033324,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "t1umvKf2JX",
                "forum": "bjyf5FyQ0a",
                "replyto": "G2UhZFs7MN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1069/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1069/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal to Reviewer rHRB"
                    },
                    "comment": {
                        "value": "Thank you for your valuable feedback. Below, we show you the point-to-point responses, which we hope can address your concerns.\n\n### Q1: Regarding the novelty of our proposed Valley\n\n**A**: We propose employing a simple linear projection layer as a bridge between the video and text modalities, while similar works mostly adopt the complex Q-former structure. Besides, we specifically design three temporal modeling modules  for improving video understanding.\n\n### Q2: Regarding the unique advantage of the version-2 with linear layers over v1 and v3\n\n**A**: For improving v1, v2 introduces  a learnable linear layer to learn the temporal importance score of a certain frame. In order to more fully model the temporal variation representation of these spatial tokens, we derived the v3 based on Transformer. In light of experiments, the performance of V2 is basically between V1 and V3, such as the MSRVTT-QA and the video-based text generation benchmark provided by Video-ChatGPT. Refer to Table 1 and Table 2. \n\n### Q3: Regarding the implementation details.\n\n**A**: We will release all the source code to facilitate the reproduction of the results in the paper.\n\n### Q4: About model size and inference time. \n\n**A**: Model size of VideoChatGPT and LLaMA-adaptor is 7B, and others are all 13B. We did not perform the inference of all these models and the evaluation metrics are claimed from the papers.\n\n**For more details of the novelty and ablations, please refer to our global response above.**"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1069/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700729258269,
                "cdate": 1700729258269,
                "tmdate": 1700729368830,
                "mdate": 1700729368830,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]