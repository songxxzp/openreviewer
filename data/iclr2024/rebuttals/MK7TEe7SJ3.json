[
    {
        "title": "USTAM: UNIFIED SPATIO-TEMPORAL ATTENTION MIXFORMER FOR VISUAL OBJECT TRACKING"
    },
    {
        "review": {
            "id": "XREIQKJ63m",
            "forum": "MK7TEe7SJ3",
            "replyto": "MK7TEe7SJ3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5087/Reviewer_yHrZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5087/Reviewer_yHrZ"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a spatial-temporal attention MixFormer framework for visual object tracking, with experimental results affirming its effectiveness."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The USTAM approach is crafted as an end-to-end VOT network, integrating spatial and temporal attentions."
                },
                "weaknesses": {
                    "value": "The proposed approach is an incremental improvement of MixFormer tracker. Many of the components in the paper such as MAM block,\nasymmetric attention, loss function have already been proposed in MixFormer."
                },
                "questions": {
                    "value": "1. It would be beneficial to allocate a dedicated section to MAM, considering it serves as the primary building block for this approach. Distinguishing between the author's specific contributions and those stemming from MAM can be challenging otherwise.\n2. Rearranging the reference section in order of the last name of the first author would enhance searchability.\n3. The dimensions of G_f^i nxn don't seem to align. G_f^i represents the attention map between the search area and mixed, which is the combination of both the search and target areas.\n4. Figure 1 appears too small to discern the letters effectively.\n5. In equation (6), given that g_i,j is the attention map in equation (5), it follows that the sum of each row of G_f_t should be 1 after applying softmax. This operation essentially subtracts a constant value. Could you elaborate on the rationale behind this step?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5087/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698674842715,
            "cdate": 1698674842715,
            "tmdate": 1699636499556,
            "mdate": 1699636499556,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Min94ZrDNV",
                "forum": "MK7TEe7SJ3",
                "replyto": "XREIQKJ63m",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5087/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5087/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Answer of question (1) Thank you for your kind comment. Contrary to what you understood, all blocks in our network are composed of MAM blocks. However, we apply temporal attention using the attention map of the search region from the previous frame, and we introduce spatial attention in the last MAM block to enhance attention for the target. This way, we leverage spatiotemporal information to improve tracking performance.\n\nAnswer of question (2) Thank you for your kind suggestion. We have crafted the reference section following the guidelines provided by ICLR to enhance searchability. I hope it proves helpful in your searches.\n\nAnswer of question (3) Thank you for your valuable comment. To explain further, during the attention operation where the Query comes from the search region and the Key and Value come from the mixed (i.e., mixed targets and search region), the resulting attention map is utilized to apply spatial attention. In this case, we specifically use the attention values corresponding to the search region, and thus, the dimension is nxn.\n\nAnswer of question (4) Thank you for your comment. In Figure 1, we have done our best to effectively portray the overall architecture of the proposed network. While it may not be efficient for character identification, we have prioritized the visual representation as the image is more crucial than the characters. We would appreciate it if you could focus more on the visual aspects rather than the characters.\n\nAnswer of question (5) We are very sorry to confuse you with the typo in equation (6). We changed  j=1 to i=1. Softmax was applied to rows and the average was obtained for columns. The score that each token has on all tokens was calculated by averaging the columns."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5087/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700456066607,
                "cdate": 1700456066607,
                "tmdate": 1700458033237,
                "mdate": 1700458033237,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DJQSW8qfUV",
            "forum": "MK7TEe7SJ3",
            "replyto": "MK7TEe7SJ3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5087/Reviewer_ciuV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5087/Reviewer_ciuV"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to track the target object using spatial and temporal attention-based Transformer networks. This paper points out that existing works fail to find the appropriate balance between effective feature extraction and the incorporation of attention modules. They also lack explicit modeling of the relationship between spatial and temporal information. The experiments are conducted based on three widely SOT datasets.\n\nthe issues of this work are that:\n\nthe idea of incorporating attention mechanisms into the Transformer networks for tracking is not new;\nthe speed of this tracker is about 30-40+ FPS, which is not fast compared with other SOTA trackers, such as OSTrack;\nConsidering the limited novelties and regular tracking efficiency, I tend to reject this paper."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "This paper proposes to track the target object using spatial and temporal attention-based Transformer networks. This paper points out that existing works fail to find the appropriate balance between effective feature extraction and the incorporation of attention modules. They also lack explicit modeling of the relationship between spatial and temporal information. The experiments are conducted based on three widely SOT datasets."
                },
                "weaknesses": {
                    "value": "the issues of this work are that:\n\nthe idea of incorporating attention mechanisms into the Transformer networks for tracking is not new;\nthe speed of this tracker is about 30-40+ FPS, which is not fast compared with other SOTA trackers, such as OSTrack;\nConsidering the limited novelties and regular tracking efficiency, I tend to reject this paper."
                },
                "questions": {
                    "value": "1. re-organization of the novelties proposed in this work; \n2. showing the real advantages of this work;"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5087/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698763537901,
            "cdate": 1698763537901,
            "tmdate": 1699636499471,
            "mdate": 1699636499471,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2IGRrjZHkh",
                "forum": "MK7TEe7SJ3",
                "replyto": "DJQSW8qfUV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5087/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5087/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Answer of question (1) Let me reiterate the novelties we propose in this paper. In the MixFormer, which we use as a baseline, additional training is employed with a score prediction module for online target update, utilizing temporal information. However, instead of using this module, we enhance performance by applying temporal attention that only a very slight increase in speed and parameters. To enhance the reliability of temporal attention, we apply spatial attention to the last block of the transformer for the previous frame. In this manner, we propose spatial and temporal attention that leverages spatiotemporal information to enhance tracking performance.\n\nAnswer of question (2) Let me explain you the real advantages of our work. As demonstrated in the qualitative analysis in Section 4.2 of the paper, even when the scale of the target changes in a frame or when there are similar objects around the target, our proposed spatial attention strengthens the attention map for the target. Subsequently, through temporal attention in the next frame, we guide the model with positional information from the previous frame, resulting in a more robust tracking performance."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5087/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700456003384,
                "cdate": 1700456003384,
                "tmdate": 1700458002937,
                "mdate": 1700458002937,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZbMLbR7ET2",
                "forum": "MK7TEe7SJ3",
                "replyto": "2IGRrjZHkh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5087/Reviewer_ciuV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5087/Reviewer_ciuV"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your response. After reviewing the comments from other reviewers I still think the current work is not ready for publication on iclr."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5087/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700701938143,
                "cdate": 1700701938143,
                "tmdate": 1700701938143,
                "mdate": 1700701938143,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gsgRwkbiwu",
            "forum": "MK7TEe7SJ3",
            "replyto": "MK7TEe7SJ3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5087/Reviewer_DDtH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5087/Reviewer_DDtH"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a unified spatio-temporal attention mixformer framework for video object tracking (VOT). Specifically, they\u2019re two main contributions stated by the authors: 1) a simple yet effective unified pipeline is proposed for feature extraction, target information integration, and localization estimation within the framework of a ViT network; 2) a spatio-temporal attention module is introduced to more effectively distinguish the target from the complicated background. Experimental results on several popular VOT benchmarks show the proposed approach performs favorably against SOTA trackers."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The idea seems to be somewhat effective, which can observe some performance improvements on the main VOT benchmarks (e.g., LaSOT and TrackingNet).\n- The paper is well organized, which is easy to follow.\n- Sufficient related works are discussed in Sec. 2."
                },
                "weaknesses": {
                    "value": "- The statement for \u2018We present a simple but effective unified VOT pipeline for feature extraction, target information integration, and localization estimation within the framework of a ViT network\u2019 is not really true. This unified framework has already been proposed in previous one-stage trackers, e.g., OSTrack, including all the feature extraction, target interaction and localization in the same ViT framework.\n- The contribution in this paper is somewhat incremental. It seems that the proposed framework is still similar to the MixFormer framework, although it uses a ViT-based architecture and considering the previous target state by using the temporal attention module.\n- The usage of the temporal attention module is a bit similar to use the Cosine Window (e.g., also used in OSTrack), which also makes the tracker object moves smoothly in consecutive frames. In this paper, the authors make it in a learnable way by using the attention map in the previous frame. But one unsolved problem is about the reliability of the previous target state. If the previous prediction is noisy, the effectiveness of the proposed approach is also questionable.\n- Missing some essential details and unfair comparison. It is not clear whether the proposed tracker use the pre-trained models. e.g., OSTrack and DropTrack. In Table 2, the authors compare with the OSTrack-384 only trained on GOT-10k training split, while the proposed approach additionally  uses more training data, which is not fair. From Table 3, it seems that the proposed USTAM-B-384 trained on GOT-10K is inferior to OSTrack-384. What\u2019s the reason? Does the compared two approaches use the same pre-trained model?"
                },
                "questions": {
                    "value": "- The statement for \u2018We present a simple but effective unified VOT pipeline for feature extraction, target information integration, and localization estimation within the framework of a ViT network\u2019 is not really true. This unified framework has already been proposed in previous one-stage trackers, e.g., OSTrack, including all the feature extraction, target interaction and localization in the same ViT framework.\n- The contribution in this paper is somewhat incremental. It seems that the proposed framework is still similar to the MixFormer framework, although it uses a ViT-based architecture and considering the previous target state by using the temporal attention module.\n- The usage of the temporal attention module is a bit similar to use the Cosine Window (e.g., also used in OSTrack), which also makes the tracker object moves smoothly in consecutive frames. In this paper, the authors make it in a learnable way by using the attention map in the previous frame. But one unsolved problem is about the reliability of the previous target state. If the previous prediction is noisy, the effectiveness of the proposed approach is also questionable.\n- Missing some essential details and unfair comparison. It is not clear whether the proposed tracker use the pre-trained models. e.g., OSTrack and DropTrack. In Table 2, the authors compare with the OSTrack-384 only trained on GOT-10k training split, while the proposed approach additionally  uses more training data, which is not fair. From Table 3, it seems that the proposed USTAM-B-384 trained on GOT-10K is inferior to OSTrack-384. What\u2019s the reason? Does the compared two approaches use the same pre-trained model?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5087/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699517405314,
            "cdate": 1699517405314,
            "tmdate": 1699636499361,
            "mdate": 1699636499361,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9Yq0qKqEaV",
                "forum": "MK7TEe7SJ3",
                "replyto": "gsgRwkbiwu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5087/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5087/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Answer of question (1) Thank you for your good point. As you said, the unified ViT framework has indeed enhanced performance in the realm of visual object tracking. However, methods for effective binding and treatment of spatio-temporal features have not been studied in the unified ViT framework. This paper distinguishes itself from existing SOTA works by not only presenting a novel method but also substantiating its efficacy through experimental validation.\n\nAnswer of question (2) Thank you for your comment. There is a problem that the networks are similar because we use MixFormer as a baseline. However, the existing MixFormer uses a score prediction module for online target update to utilize temporal information. However, this requires additional learning. However, we propose a temporal attention module that simply but effectively utilizes temporal information without the need for additional learning.\n\nAnswer of question (3) Thank you for your good comment. Since the prediction of the previous target may be noisy, there is a problem that the reliability of the prediction of the previous target state may decrease. To solve this, we apply a spatial attention module to the last layer of the transformer for the previous frame and a temporal attention module to the first layer of the transformer for the current frame to increase the reliability of predictions about the target state.\n\nAnswer of question (4) Thank you for your comment. We prepared a performance table in Table-2 as compared in other papers. However, there was no indication of the GOT-10k model in Table-2, so it was updated. The GOT-10K model is marked with *. The performance of our GOT-10k model was compared in table-3 of the ablation study section."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5087/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700455935418,
                "cdate": 1700455935418,
                "tmdate": 1700458017575,
                "mdate": 1700458017575,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]