[
    {
        "title": "ResTran: A GNN Alternative To Learn Graph With Features"
    },
    {
        "review": {
            "id": "mPEOVOWdC0",
            "forum": "PuKRVPXXpR",
            "replyto": "PuKRVPXXpR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2051/Reviewer_Fupb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2051/Reviewer_Fupb"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose an unsupervised method that first combine the node feature with graph topology into a node-wise embedding. Then apply any standard machine learning method for the downstream tasks."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "-\tThe connection to some unsupervised graph learning method is interesting."
                },
                "weaknesses": {
                    "value": "-\tInsufficient literature survey on the related work.\n-\tThe novelty is extremely limited. See comments below.\n-\tThe claim that existing GNNs can only work well on heterophilic graphs with complicated architecture is false. The authors not only ignore the complete literature of spectral GNNs but also LINKX method.\n\nMy first concern with the work is that its literature survey on prior related works is insufficient. Note that one major claim of this paper is that current GNNs can not handle heterophilic graphs if not using complex architecture. However, this is apparent wrong as the line of spectral GNNs research tackles this problem with a very simple design [1,2,3]. Also, LINKX [4] is another simple architecture that has been shown superior performance on heterophilic graphs. Notably, spectral GNNs are shown to be capable of learning ``any\u2019\u2019 graph spectral filtering that is beyond just low-pass (homophily) and high-pass (heterophily) cases. It is surprising that the authors completely ignore this literature.\n\nOn the other hand, the idea of obtaining node embedding from node features and graph topology in an unsupervised fashion has also been proposed previously. One of the early model SIGN [5] propose to compute propagated features $X, AX, A^2X,\\cdots,A^KX$ first (with $A$ being potentially normalized or use $L$ instead) and concatenate them as the node embedding for applying MLP in downstream tasks. This work has also led to a series of works focusing on scalable graph learning methods such as SAGN [6] and GAMLP [7] with similar ideas. The ResTrans method is just using the embedding $L^{-1/2}X$. Note that one potential drawback of ResTrans is its computational complexity. Indeed, as the authors mentioned, naively compute $L^{-1/2}$ is computationally infeasible. They propose to apply the Krylov subspace method, which essentially computes $X,LX,L^2X,\\cdots,L^rX$ and is very similar to SIGN design. It is a surprise to me that the authors completely miss this line of work as well. Compared to these prior works, I think the novelty of the proposed method is relatively limited. \n\nI would suggest the authors explain the difference of their method to at least SIGN and compare them carefully in the experiments. Also, I think the authors should also compare to some spectral GNN baselines such as those in [1,2,3]. Otherwise, it is hard to convince me that ResTrans is a good method for heterophilic graphs.\n\n## References\n\n[1] Adaptive Universal Generalized PageRank Graph Neural Network, Chien et al. ICLR 2021.\n\n[2] Bernnet: Learning arbitrary graph spectral filters via bernstein approximation, He et al., NeurIPS 2021.\n\n[3] How powerful are spectral graph neural networks, Wang et al. ICML 2022.\n\n[4] Large scale learning on non-homophilous graphs: New benchmarks and strong simple methods, Lim et al., NeurIPS 2021.\n\n[5] Sign: Scalable inception graph neural networks, Frasca et al. ICML GRL+ workshop 2020.\n\n[6] Scalable and adaptive graph neural networks with self-label-enhanced training, Sun et al. 2021.\n\n[7] Graph attention multi-layer perceptron, Zhang et al. KDD 2022."
                },
                "questions": {
                    "value": "1.\tDiscuss and compare with spectral GNNs [1,2,3] in both methodologies for heterophilic graphs and experiments.\n2.\tCompare with SIGN methods in both methodology and experiments."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2051/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698536416023,
            "cdate": 1698536416023,
            "tmdate": 1699636136894,
            "mdate": 1699636136894,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GEJe3IyH4k",
                "forum": "PuKRVPXXpR",
                "replyto": "mPEOVOWdC0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2051/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2051/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the review!"
                    },
                    "comment": {
                        "value": "Thank you for the review! We will address your concern on the existing methods.\n\n> Literature reviews\n\nThank you for pointing out the literature. We first apologize that we should have been careful when we use \"simple.\" We say simple since we separate the graph consumption and prediction; consumption is ResTran and we may use any vector based ML methods that include the existing ones such as LP, SVM, AVAE and so on. This may be phrased as \"versatile.\" GNNs are designed to graph problems, while we can use any vector based methods, such as AVAE or SVM. In this sense, while we appreciate the work you raised [1,2,3,4], those are sophisticated GNN models, which are different from our aims. \n\nThank you also for pointing out SIGN. The difference between SIGN and ResTran is as follows. SIGN learns the weight of $X, AX, \\ldots, A^{r}X$ through a GNN framework, while we project $L^{+1/2}$ to Krylov subspace. In this sense, SIGN is a more established model. Also, since SIGN needs to learn the weight of  $X, AX, \\ldots, A^{r}X$ and store them, SIGN needs space. In fact, $r$ for SIGN is typically smaller, like 5 or less as discussed in the SIGN paper. On the other hand, since our ResTran aims to $L^{+1/2}$, we don't have to store each weight of  $X, AX, \\ldots, A^{r}X$, instead, we only need to have the summation of these. By this we have more space. Also, since the weight of $X, AX, \\ldots, A^{r}X$ are learnt by GNNs, these have GNNs' advantages as well as ones' disadvantages, which we want to avoid."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2051/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700414000090,
                "cdate": 1700414000090,
                "tmdate": 1700414000090,
                "mdate": 1700414000090,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hP55o7F57f",
                "forum": "PuKRVPXXpR",
                "replyto": "GEJe3IyH4k",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2051/Reviewer_Fupb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2051/Reviewer_Fupb"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the comments. I keep my evaluation unchanged."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2051/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700707052511,
                "cdate": 1700707052511,
                "tmdate": 1700707052511,
                "mdate": 1700707052511,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xPg46pW20m",
            "forum": "PuKRVPXXpR",
            "replyto": "PuKRVPXXpR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2051/Reviewer_hGqh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2051/Reviewer_hGqh"
            ],
            "content": {
                "summary": {
                    "value": "The paper seeks to addresses the vertex classification problem in a manner different to standard GNN research. The idea is to utilize standard spectral methods such as clustering and sparsification methods to define new embeddings. The authors experimentally demonstrate that these embeddings out-perform standard GNNs on some datasets. Specifically, the authors propose \"Resistance Transformation\" on feature vectors X: Simply transform X by utilizing the Laplacian basis used to compute effective resistances, and feed the resulting data to standard vectorial learning algorithms."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Devising novel positional encodings, to the extent of eliminating the need for message-passing mechanisms, might be a worthwhile idea to explore. The authors also place their method within the homoplily/heterophily narrative, arguing that their embedding has lesser homophilious bias as compared to standard GNNs."
                },
                "weaknesses": {
                    "value": "I am not sure about the computational complexity of these methods: The authors should have included some experimental results on time complexity to indicate whether the usually expensive eigenvector computations can be justified instead of simple combinatorial message passing. \n\nSpectral embeddings/methods have inherent limitations in the kind of data they can capture from a graph: They fail to capture relational aspects of data (such as node/edge-colors) and so on, unlike combinatorial message-passing algorithms. If the authors propose such a radical departure from standard GNN methods, they should investigate their method on a variety of datasets, such as molecular graphs or synthetic graphs arising from relational sources."
                },
                "questions": {
                    "value": "1. The computational cost of spectral methods typically goes to O(n^3). Can the authors comment on the running time complexity of the Krylov-subspace based embedding and compare it with the run-time costs of a standard message-passing GNN? Have the authors carried out any experiments to compare the run-time costs of the two approaches? Especially, I would like to know the status on sparse graphs, where the pseudo-inversions might be way more costly than message-passing. And how does the time complexity scale with graph size? The empirical investigation considers only medium-sized graphs.\n\n2. How does the proposed method differ from the commonly used positional encodings based on spectral properties of the input graph?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2051/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698822393376,
            "cdate": 1698822393376,
            "tmdate": 1699636136781,
            "mdate": 1699636136781,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qcW2fQqifI",
                "forum": "PuKRVPXXpR",
                "replyto": "xPg46pW20m",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2051/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2051/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you very much for the review!"
                    },
                    "comment": {
                        "value": "Thank you very much for the review! We will address what you raised in weakness and question sections.\n\n> Complexity analysis \n\nWe provided the complexity analysis on page 4. Basically, ResTran takes O(rfm), and r is typically small, e.g., our experiments we used 15. O(fm) is equivalent to the established GNN models.\n\n> Edge-coloring\n\nThank you for the suggestion. As stated in future work, at this stage, we are unsure how much of the expressive power our ResTran has. We speculate that at least ResTran has expressive power as two established models, GCN and GAT. However, we leave these as future work.\n\n> Difference between ResTran and positional encoding \n\nAs written in Appendix B, our ResTran can be plugged into any vector based model, while positional encoding using spectral features aims to graph transformer architectures."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2051/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700413933831,
                "cdate": 1700413933831,
                "tmdate": 1700413946810,
                "mdate": 1700413946810,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "x0LlSU4nX8",
                "forum": "PuKRVPXXpR",
                "replyto": "qcW2fQqifI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2051/Reviewer_hGqh"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2051/Reviewer_hGqh"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your answer. I intend to keep my original rating."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2051/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700423077126,
                "cdate": 1700423077126,
                "tmdate": 1700423077126,
                "mdate": 1700423077126,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dIr8pV45og",
            "forum": "PuKRVPXXpR",
            "replyto": "PuKRVPXXpR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2051/Reviewer_MjNb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2051/Reviewer_MjNb"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors proposed a simple architecture for node classification tasks in graph formatted dataset name ResTran. It utilized the well-known spectral clustering methods to first generate vector representation of nodes in graph that incorporate both node features and graph connectivity, then apply standard vector based ML methods to them for downstream task. ResTran was claimed to be robust to homophilous bias which is commonly seem in traditional GNN settings."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "Pros: \n- Extensive summary of spectral clustering and other preliminaries. \n- Overall good written and easy to follow.\n- Simplify structure."
                },
                "weaknesses": {
                    "value": "Overall, I found the paper raise more concerns than it claimed to solve. Here are some of my major concerns: \n- Not much novelty from traditional spectral clustering methods, most part of the papers are well-known results or naive extension of existing methods.  \n- Most of the background or preliminaries can be distilled into shorter context or put in appendix such as propositions from previous papers, it\u2019s currently taking more than 2 pages of the main paper. \n- No complexity analysis to support the claim that it\u2019s less complicated than GNN. \n- Using the shifted graph Laplacian term b to control the heterphilous information in feature map seems to require a lot of fine tuning. How to choose the hyperparameters (b, r, etc) in experiment section is not clear to me, based on the appendix the authors used a fixed value, some ablation study would be nice to see. \n- There are multiple works in GNN that already support heterophilous dataset without over-smoothing. The authors\u2019 claim about the lack of GNN is not valid. I would suggest the authors to at least do some comparison with the recent ones. \n- Experiment section lack of comparison to more recent GNN works that also targeting at heterophilous datasets."
                },
                "questions": {
                    "value": "It seems like the authors didn't include most recent works in heterophilous GNNs and most of the claims against GNNs are lack of support. To name a few, JKNet [Xu et al., 2018], H2GCN [Zhu et al., 2020a], Geom-GCN [Pei et al., 2020],  GPR-GNN [Chien et al., 2020], GPNN [Yang et al., 2022] and many more are all methods that work with heterophilous graph dataset. I would recommend the authors to at least go over the literature before making the final conclusion."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2051/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698826677315,
            "cdate": 1698826677315,
            "tmdate": 1699636136707,
            "mdate": 1699636136707,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dRQSD0AqRV",
                "forum": "PuKRVPXXpR",
                "replyto": "dIr8pV45og",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2051/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2051/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the review!"
                    },
                    "comment": {
                        "value": "Thank you.for the review. We will respond to what you raised in weakness and question sections.\n\n> Lack of Novelty of the methods.\n\nAs summarized in Appendix G.2, our theorem 8 is novel for the following points. Mainly, while the previous studies (stated in Prop 2), such as (Dhillion et al., 2004), only hold for the vector data, Thm.8 holds for the \"given\" graph data. Note also that the proof of Thm.8 is NOT the off-the-shelf application of Prop 2. See more in Appendix G.2\n\n> Presentation of preliminaries\n\nThank you for the suggestion. We will endeavor to further shorten the preliminaries. We also stress that our main claims are not incremental results of preliminaries.\n\n> No complexity analysis of ResTran.\n\nWe provided the complexity analysis on page 4.\n \n> Parameters\n\nThank you for pointing out. From our preliminary experiments, the choice of b does not affect much. Also, the choice of r does not affect either if r is larger than 10. Thus, for simplicity, we used the fixed b and r. We will further clarify this point.\n\n> Experiments\n\nWe are aware that multiple GNN models address the homophily bias. However, in the experiments, as written in the manuscript, we wanted to exclude sophistication for both methods as much as possible. Thus, we did not compare with the advanced models you raised. At the same time, we do not use some complicated methods for ResTran either. See more for the reply to Fupb."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2051/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700413873139,
                "cdate": 1700413873139,
                "tmdate": 1700413873139,
                "mdate": 1700413873139,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xXtNTJbxQY",
            "forum": "PuKRVPXXpR",
            "replyto": "PuKRVPXXpR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2051/Reviewer_gotZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2051/Reviewer_gotZ"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes ResTran as an alternative to GNNs that may not suffer from homophilous bias and over-smoothing. ResTran is to first transform node features using graph spectral information so that graph structural information can be preserved in the transformed features. After that, one can directly apply vector-based learning methods on the transformed features, e.g., SVM, for node classification tasks on graph-structured data. The authors justify ResTran theoretically by drawing connections from effective resistance, k-means, and spectral clustering and justify it empirically by comparing with three traditional GNN architectures over 11 datasets. The experiments show that ResTran can perform comparably with baselines on homophilous datasets and outperform them on heterophilous datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. ResTran may not be biased towards homophilous data like traditional GCNs.\n2. The proposed feature transformation is simple and may already be effective in capturing topology information."
                },
                "weaknesses": {
                    "value": "1. It seems to me ResTran is closely related to 1-layer message-passing neural networks (MPNNs), since it utilizes $L^+$ to transform node features $X$ and it is known that $ L^+_{ij} $ represents the effective resistance between two end nodes in the graph interpreted as an electrical network. While traditional MPNNs utilize adjacent matrix $A$ to transform $X$ and have weight 0 when there is no edge between two nodes, I find the key idea is similar, which sounds like ResTran is still in some sense a GNN.\n\n2. I do not really see that ResTran is simpler than existing GNNs. Depending on the definition of complexity, I find the feature transformation is already non-trivial as it includes utilizing Krylov subspace method to approximate the transformed features. After that, from the experiments, it seems it still needs complex neural networks to get decent results, i.e., AVAE, and using simple methods such as SVM does not seem to work. \n\n3. The authors claim that ResTran may not suffer from over-smoothing and can overcome homophilous bias, and I think these need to be further discussed. It appears it is because ResTran only utilizes $L^+$ transforms features **once** (somewhat like a 1-layer MPNN) that it does not suffer from those issues, but it may come at the cost of the capability of capturing topological information in graphs. More experiments need to be done to demonstrate its capability, for example, comparing ResTran with [1], where some simple tricks were proposed to improve GCNs and, even with different splits, it seems it significantly outperforms ResTran, especially on heterophilous datasets.\n\n4. I am unsure if the comparison is fair. Different from traditional GNNs, which propagate features with $A$, use MLPs to make predictions, and train the network with a classification loss, it seems critical for ResTran to use some semi-supervised models such as VAT and AVAE to get good results. However, the node representations yielded by those GNNs are not trained in the same way, e.g., VAT involves adversarial training--it is known that adversarial training can also further improve GNNs' performance [2].\n\n\n[1] Chen, Ming, et al. \"Simple and deep graph convolutional networks.\" International conference on machine learning. PMLR, 2020.\n\n[2] Kong, Kezhi, et al. \"Robust optimization as data augmentation for large-scale graphs.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022."
                },
                "questions": {
                    "value": "1. Are there any particular reasons for using a different dataset split instead of the one that has been widely used in the previous literature?\n2. I was wondering how AVAE is used exactly for node classification tasks. Will ResTran + MLP + Cross-entropy work?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2051/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2051/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2051/Reviewer_gotZ"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2051/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699593456372,
            "cdate": 1699593456372,
            "tmdate": 1699636136618,
            "mdate": 1699636136618,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "eaMuJHm2ZH",
                "forum": "PuKRVPXXpR",
                "replyto": "xXtNTJbxQY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2051/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2051/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the review!"
                    },
                    "comment": {
                        "value": "Thank you for the review! We will respond to the weakness and questions\n\n> It seems to me ResTran is closely related to 1-layer message-passing neural networks (MPNNs).\n\nOur contribution is to propose ResTran. Using ResTran, we obtain a vector representation of the graph-with-features. We may apply the transformed vector to any vector-based ML method. From this sense, our aim can be explained as \"separating\" consumption of graph and prediction. Contrary, GNNs do these at the same time. If we use NN models, the first layer \"touches\" $L^{+1/2}$; in some sense, we may say this is a sort of GNN. However, again, the point is that we separate consumption of graphs and prediction, which is different from most the frameworks of GNNs. \n\n> I do not really see that ResTran is simpler than existing GNNs.\n\nWe apologize for the confusion. We should have been careful when we use \"simple.\" We say simple since we separate the graph consumption and prediction; consumption is ResTran, and we may use any ML methods that include the existing ones. To describe this separation, this may be phrased as, say, \"versatile.\" GNNs are designed for graph problems, and therefore we apply GNNs only to some specific problems over graphs. On the other hand, we can use any vector-based methods, such as AVAE or SVM, which is more \"versatile\" methods than GNNs.\n\n> Comparison methods and Experimental settings\n\nWe are aware that various GNNs address various problems. Also, there may be various vector-based methods that better work for ResTran. For example, there may be better NN layers for ResTran or some kernel functions that work better than Gaussian kernel together with SVMs. However, as written in the manuscript, in order to eliminate any sophistications for both GNNs and ResTran, we may want to keep our experimental setting as simple as possible. For this purpose, i) we use the most established models, and ii) We use these methods with the simplest settings. For GNNs, GCN and GAT are the two most established models. We used SGC since SGC is close to ResTran, as discussed in Appendix B. For ResTran, AVAE is a simpler version of VAE, and VAE is one of the most established models. VAT is one of the early established models for semi-supervised learning using NNs. Also, we use SVMs with the established kernel function, Gaussian. We use GNNs and ResTran with the simplest settings as written in the manuscript. Also, by this, we say our comparison is fair since we do not use any sophistication for both methods. We simply plug in three GNN methods and SVM, LP, NN based methods, all of which are already established models.\n\n> Are there any particular reasons for using a different dataset split instead of the one that has been widely used in the previous literature?\n\nAs written in the manuscript, we avoided using public split since \"overfitting\" to for some classical datasets public split is reported (Shchur et al. 2018). Therefore, we use random splitting.\n\n> I was wondering how AVAE is used exactly for node classification tasks. Will ResTran + MLP + Cross-entropy work?\n\nExactly! We consume vector representation by ResTran using MLP + Cross Entropy."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2051/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700413786257,
                "cdate": 1700413786257,
                "tmdate": 1700413786257,
                "mdate": 1700413786257,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9cID2G4tn6",
                "forum": "PuKRVPXXpR",
                "replyto": "eaMuJHm2ZH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2051/Reviewer_gotZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2051/Reviewer_gotZ"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your responses. I have also read other reviews, and I find that the concerns about method novelty and simplicity still remain. Thus, I will keep my initial score."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2051/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700697778317,
                "cdate": 1700697778317,
                "tmdate": 1700697778317,
                "mdate": 1700697778317,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]