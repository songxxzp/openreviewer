[
    {
        "title": "A unified sampling framework for solver searching of Diffusion Probabilistic Models"
    },
    {
        "review": {
            "id": "HMcW6fIXPz",
            "forum": "W2d3LZbhhI",
            "replyto": "W2d3LZbhhI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4858/Reviewer_qhfh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4858/Reviewer_qhfh"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents an AutoML-style experimental setup for searching optimal Diffusion Model sampler (solver). The authors, first noted the various \u201cmoving parts\u201d of modern reverse process solver and then applied an evolution-based search technique to explore these while keeping track of FID under an NFE budget. Authors argued and validated that a non-uniform combination of these design choices in different timesteps is the key to reach optimal \u201csolver schedule\u201d. A lot of experiments are conducted to search through this space of solver schedules. In fact, they also proposes a special evolutionary search method S3 that does it more efficiently."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "As far as I can tell, there hasn\u2019t been such an effort to use AutoML setup for searching for better solver schedule \u2014 so it is indeed novel in some sense. Also, the large amount of experiments have been conducted, which can also be quite helpful for the community in general. The presentation/writing quality is also pretty good."
                },
                "weaknesses": {
                    "value": "Here are some concerns I\u2019d like to emphasize:\n\n- The major weakness I see is that the paper can be called \u201cjust a search\u201d without any solid conclusion. While I agree that figuring out good FID numbers for existing benchmarks is pretty good, it is not usually preffered to lock the contribution to very specific models/dataset/solver. If a new dataset/model/solver comes out in future, one need to perform the entire search again.\n- The USF isn\u2019t really something new. It is just a \u201cdumb\u201d aggregation of a bunch of design options. It is only based on empirical evidence \u2014 but I wonder if it can be theoretically justified."
                },
                "questions": {
                    "value": "Here are some concrete questions I\u2019d like the authors to answer.\n\n- In high-level, it is not clear to me which part of the paper is being claimed to be the primary contribution \u2014 the search algorithm S3 itself, or the optimal solver schedules and corresponding FID numbers ? Is the paper supposed to be about the \u201csearch algorithm S3\u201d or an empirical AutoML report ?\n- If your focus is S3, there is surprisingly little written about it in the main paper. Almost all details are in supplementary. So that indicates it to be just an AutoML experimental report \u2014 is it ?\n- Outright confession: I am not at all experienced in AutoML. So, is this S3 something novel or it is a usual practice in AutoML ? I don\u2019t see any proper citation in case this has been adopted from existing literature/libraries.\n- What exactly the paper \u201cconcludes\u201d at the end the day \u2014 did you find any \u201cpattern\u201d in the optimal solver schedules that may motivate further work? I don\u2019t see the exact configuration of the optimal solver schedules (found by S3) \u2014 are they written anywhere? You can certainly report the ones with low NFE. This is important for verifying them later on by others.\n\nCouple of high-level points are unclear to me; so I am now going with a \u201cbelow threshold\u201d rating. I am open to increasing it depending on authors response, if satisfactory."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4858/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4858/Reviewer_qhfh",
                        "ICLR.cc/2024/Conference/Submission4858/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4858/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698530137129,
            "cdate": 1698530137129,
            "tmdate": 1700684417645,
            "mdate": 1700684417645,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5KM9zZbEO7",
                "forum": "W2d3LZbhhI",
                "replyto": "HMcW6fIXPz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4858/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4858/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**W1.1: While I agree that figuring out good FID numbers for existing benchmarks is pretty good, it is not usually preffered to lock the contribution to very specific models/dataset/solver. If a new dataset/model/solver comes out in future, one need to perform the entire search again.**\n\n**A**: Thanks for raising this valuable concern. For new models, our experimental results in Tab.21 show that the searched schedules of S3 have the potential to be transferred across models.\n\nAs for new tasks, we would like to highlight the efficiency of our search method, S3 (please refer to the analysis of search cost in the global response). The efficiency of S3 makes it feasible to employ S3 for new tasks. \n\nSpecifically, on low-resolution datasets like CIFAR-10, S3 is about 100 $\\times$ more efficient than training-based methods like Consistency Models [1]. And on high-resolution datasets like LSUN-Bedroom, S3 is around 700 $\\times$ more efficient. Compared with a concurrent work of efficient diffusion sampler, DPM-Solver-v3 [2], the search cost of S3 is 4 $\\times$ smaller and can achieve superior performance in the meantime on CIFAR-10.\n\nMoreover, our experiments find that the search cost of S3 can be easily decreased by evaluating fewer schedules with fewer images without significant performance dropping, according to the results in Tab.3 and Tab.16.\n\n[1] Consistency Model, Song et al., ICML 2023\n\n[2] DPM-Solver-v3: Improved Diffusion ODE Solver with Empirical Model Statistics, Zheng et al., NeurIPS 2023\n\n**W1.2: The major weakness I see is that the paper can be called \u201cjust a search\u201d without any solid conclusion.**\n\n**A**: We conduct detailed analysis based on the search results and give some knowledge that can offer guidance for designing solver schedules in the future. \n\nWe first **investigate the contributions of each component to the final results**. Specifically, we fix other components and search only for one component. We run the evolutionary search for a few steps and report the results. Below are the search results on CIFAR-10 and LSUN-Bedroom (\"LOE\" stands for \"low order estimation\" of derivatives). More details can be found in App C in the revision.\n\n|  |  | CIFAR10 |  |  | Bedroom |  |\n| --- | --- | --- | --- | --- | --- | --- |\n| NFE | 5 | 7 | 10 | 5 | 7 | 10 |\n| Baseline | 23.44 | 6.47 | 3.90 | 33.92 | 25.94 | 23.65 |\n| Timestep | **10.06** | **5.36** | **2.88** | **27.23** | **22.99** | **18.64** |\n| Order | 17.76 | 5.86 | 3.90 | 31.89 | 25.41 | 22.38 |\n| Prediction | 23.44 | 6.47 | 3.90 | 32.43 | 24.84 | 23.25 |\n| Corrector | 22.58 | 6.23 | 3.90 | 33.40 | 25.60 | 23.27 |\n| Scaling | 21.67 | 5.56 | 3.90 | 32.84 | 25.60 | 23.25 |\n| LOE | 23.44 | 5.91 | 3.84 | 33.92 | 25.62 | 23.52 |\n\nThe results show that: (1) There is room for improvement across all components; (2) The timestep schedule has the largest influence on the performance; (3) The order schedule is the second most crucial component. According to these observations, we know that (1) Future work might get higher performance gains by focusing on refining the timesteps and order schedule. (2) When applying S3, if the search budget is tight, one can conduct the search on a search space only encompassing a subset of components."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4858/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700400344021,
                "cdate": 1700400344021,
                "tmdate": 1700450747663,
                "mdate": 1700450747663,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NIxqZhCXJE",
                "forum": "W2d3LZbhhI",
                "replyto": "HMcW6fIXPz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4858/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4858/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "(Continued from the Previous Page)\n\nThen we **analyze the pattern of the searched schedules**. \n   - Timestep: Current solvers use \"logSNR uniform\" for low-resolution datasets and \"time uniform\" for high-resolution datasets. (1) We find that for low-resolution datasets, more time points should be placed when $t$ is small. (2) However, our observations reveal that the default \"logSNR uniform\" excessively emphasizes very small and large timesteps. According to our observation, we suggest putting more points at $0.2<t<0.5$ rather than putting too much at $t<0.05$. (3) For high-resolution datasets, we recommend a slightly smaller step size at $0.35<t<0.75$ on top of the uniform timestep schedule. \n  - Prediction Type: (1) We find that the data prediction model outperforms the noise prediction model by a large margin on low-resolution datasets, especially when the budget is very low (i.e., 4 or 5). (2) Current solvers commonly apply the data prediction model on all datasets.  But on large resolution datasets, we find noise prediction often outperforms data prediction, opposite to current methods. (3) When sampling with guidance in pixel space, noise prediction with dynamic thresholding can also outperform data prediction under very low NFE budgets. (4) Moreover, we find that noise prediction is not suitable when using a high Taylor order.\n  - Derivative estimation methods: Existing solvers use equal order for Taylor expansion and derivative estimation. USF proposes to decouple the Taylor order and derivative estimation order and apply a lower order for the latter. We find that a low-order derivative estimation is preferred for high-order Taylor expansion when the step size is very large. Oppositely, full-order estimation is more likely to outperform low-order estimation when the step size is not so large.\n   - Sample-wise Consistency: We find that the performance of solver strategies remains consistent among data samples. We use a very small number of images (1k in our main experiments, 500 and 250 in our ablation studies) to evaluate the solver schedule during the search, while using the standard setting for the final evaluation (i.e., 10k for ImageNet-256 and MS-COCO, 50k for other datasets). The performance ranking of solver strategies is highly consistent between evaluating with a few images and many images. Therefore, we can directly apply the solver strategies discovered with a few images to more samples.\n\nWe update Sec 5.3 of the revision to discuss these observations."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4858/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700400510389,
                "cdate": 1700400510389,
                "tmdate": 1700450934860,
                "mdate": 1700450934860,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "flDUvMyOz7",
                "forum": "W2d3LZbhhI",
                "replyto": "HMcW6fIXPz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4858/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4858/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**W2: The USF isn\u2019t really something new. It is just a \u201cdumb\u201d aggregation of a bunch of design options. It is only based on empirical evidence \u2014 but I wonder if it can be theoretically justified.**\n\n**A**: We would like to share our perspectives on the contribution of USF.  \n  - Our work indeed draws inspiration from existing diffusion sampler methods. However, these methods are presented in very different ways, even when they are working on the same solver component and proposing similar strategies. The USF framework is the first to comprehensively summarize all components of diffusion samplers. It can unify all existing exponential integral-based solvers like DEIS (in the $\\lambda$ domain), DPM-Solver, DPM-Solver++, and UniPC. In our view, a unified framework can provide guidance for future implementation and research.\n  - For instance, a unified framework can make the extension and combination of existing strategies much more straightforward. In fact, thanks to the USF framework, our work managed to propose and implement quite a few new strategies, like low-order derivative estimation, more types of scaling magnitude, and searchable time steps. In addition, USF also offers flexibility in incorporating other strategies like arbitrary prediction types (See the results in Tab.19 for a simple attempt at interpolation between data and noise prediction), free choice of corrector, and using arbitrary off-the-shelf numerical derivative estimators.\n  - In addition to the newly proposed single-step strategies mentioned above, USF also advocate the utilization of various strategies at different timesteps.\n  - To the best of our knowledge: (1) No existing work has systematically discussed the relationship between existing samplers. (2) The new strategies we proposed have not been used. (3) The idea of using different solver strategies at different timesteps is not fully explored either.\n\nFor theoretical justification, we provide a discussion of the accuracy order of solution at App I to ensure the convergence of USF. It is worth mentioning that the performance of a solver is not only decided by accuracy order but also influenced by the coefficient of the high order error term $\\mathcal{O}(h^{p})$. However, this coefficient changes with the trajectory value and function evaluation value, making it difficult to provide theoretical analysis other than the convergence order. This fact is an important motivation for us to propose searching optimal solver schedules. \n\nWe're not sure we understand the concern on \"theoretical justification\" correctly. Besides convergence order analysis, what types of theoretical justification do the reviewer think are helpful for this work? We'll appreciate further suggestions on this point.\n\n**Q1, Q2 & Q3: it is not clear to me which part of the paper is being claimed to be the primary contribution \u2014 the search algorithm S3 itself, or the optimal solver schedules and corresponding FID numbers ? Is the paper supposed to be about the \u201csearch algorithm S3\u201d or an empirical AutoML report ?**\n\n**A**: Our primary contribution lies both in the framework USF and the search method S3. We have discussed the novelty and contribution of USF in the above response. And here, we'd like to expand on the contribution of the S3 search method.\n\nAs we've discussed in our paper, the idea of multi-stage predictor-based search is not new in the AutoML literature, e.g., [1]. What we want to claim as our contribution is that we propose an actionable and relatively efficient search method. The effectiveness of S3 has been validated by our extensive experiments, showing the possibility of sampling with very few NFEs without retraining the diffusion U-Net. And the search cost of S3 is moderate as we have analyzed in the public comment and App F.5 in the revision. This means S3 can be practically used for solver optimization for new tasks. \n\nAnother on-the-side contribution is that our discovered schedules can be used directly, and empirical knowledge based on the search results can help guide future solver design.\n\nAccording to your concern, we have adjusted the summarization of contributions in the revision as follows.\n1. (Primary) Design of the USF, which unifies existing solver methods and provides flexible extension space.\n2. (Primary) Practical search method for solver schedule optimization, whose effectiveness is validated by extensive experiments.\n3. The knowledge analysis based on the search results and the searched solvers that can be directly used.\n\n[1] GATES: A Generic Graph-based Neural Architecture Encoding Scheme for Predictor-based NAS, Ning et al., ECCV 2020"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4858/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700400779848,
                "cdate": 1700400779848,
                "tmdate": 1700450689650,
                "mdate": 1700450689650,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "F5fQuFIJen",
                "forum": "W2d3LZbhhI",
                "replyto": "HMcW6fIXPz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4858/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4858/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q4: What exactly the paper \u201cconcludes\u201d at the end the day \u2014 did you find any \u201cpattern\u201d in the optimal solver schedules that may motivate further work? I don\u2019t see the exact configuration of the optimal solver schedules (found by S3) \u2014 are they written anywhere? You can certainly report the ones with low NFE. This is important for verifying them later on by others.**\n\n**A**: Thanks for your reminder! We have already demonstrate about some of our searched solver schedules in Sec G.5. More general patterns have been discussed in our response to W1.2. Additionally, we upload our code and these schedules to https://anonymous.4open.science/r/USF-anonymous-code-56E6 for reproduction. Due to the time limitation, we didn't have enough time to clean our code and systematically arrange more searched schedules. We will rearrange and open source our code together with all searched solver schedules in the future."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4858/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700400842551,
                "cdate": 1700400842551,
                "tmdate": 1700634033151,
                "mdate": 1700634033151,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Y4lw9kpEnt",
                "forum": "W2d3LZbhhI",
                "replyto": "HMcW6fIXPz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4858/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4858/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up"
                    },
                    "comment": {
                        "value": "Dear reviewer qhfh,\n\nThanks once again for your valuable time and helpful suggestions. Did we address the concerns satisfactorily? If you have any further comments, please do not hesitate to tell us. We are more than willing to provide further clarifications."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4858/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700536542778,
                "cdate": 1700536542778,
                "tmdate": 1700536542778,
                "mdate": 1700536542778,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jg2FoUFVPj",
                "forum": "W2d3LZbhhI",
                "replyto": "Y4lw9kpEnt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4858/Reviewer_qhfh"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4858/Reviewer_qhfh"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the rebuttal"
                    },
                    "comment": {
                        "value": "The authors clarified most of my doubts. They updated the paper in a meaningful way incorporating my and other's comments. They provided with a lot of insights into the method and what it found out. They also provided some of the searched schedules outright, which are important for someone to verify the claims.\n\nI say again that I am not an AutoML expert in any way. But, the response convienced me that this paper does provide some value. Hence I will update my score to \"above threshold\"."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4858/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700684402569,
                "cdate": 1700684402569,
                "tmdate": 1700684402569,
                "mdate": 1700684402569,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "oHVhkn5Ce8",
            "forum": "W2d3LZbhhI",
            "replyto": "W2d3LZbhhI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4858/Reviewer_HfC4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4858/Reviewer_HfC4"
            ],
            "content": {
                "summary": {
                    "value": "To further improve the sample quality with less than 10 NFE, this paper proposed a unified sampling framework (USF) to study the optional strategies for solver. Specifically, USF splits the solving process of one step into independent decisions of several components, which also reveal that an appropriate solver schedules will benefit the quality and efficiency of diffusion models. Moreover, for each component, $\\mathcal{S} ^{3}$ is proposed to search for optimal solver schedules in each time step automatically. Based on this framework, USF enables to achieve significant performance on various benchmark datasets when implemented to various diffusion models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.USF improves the sample quality with less than 10 NFE, which greatly benefits diffusion models in practical application. It unifies previous fast training-free samplers in every independent sampling process, which is very flexible and extensible.\n\n2.The generated images are comparable to previous SOTA fast sampling method in various time step, which demonstrates the superior of USF.\n\n3.In my humble opinion, the mathematical analysis is rigorous and can support the improvements on the experiment results."
                },
                "weaknesses": {
                    "value": "1.The caption of Table 3 should be improved, it\u2019s hard to figure our the meaning.\n\n2.The search budget should be analysed more since it is very important to implement to previous diffusion models.\n\n3.Why not demonstrated more categories of generated images when NFE is less? A fuller qualitative analysis would add strength on the USF.\n\n4.How about the reproducibility of USF? Can you share the code? The open source code will help researchers continue to improve slow sampling speed since USF has great performance."
                },
                "questions": {
                    "value": "The same as Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4858/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4858/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4858/Reviewer_HfC4"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4858/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698566379349,
            "cdate": 1698566379349,
            "tmdate": 1699636469301,
            "mdate": 1699636469301,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NJq10P2W5T",
                "forum": "W2d3LZbhhI",
                "replyto": "oHVhkn5Ce8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4858/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4858/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**W1: The caption of Table 3 should be improved, it\u2019s hard to figure our the meaning.**\n\n**A**: Tab.3 shows the FID results on the MS-COCO dataset. \"Ours\" stands for our method with default setting. \"Ours-500\" and \"Ours-250\" stand for generating only 500 and 250 images when evaluating solver schedules, which are demonstrated to show the performance of our method with a tighter search cost.\n\nThanks for your questions. We have improved this caption in our revision for better understanding.\n\n**W2\uff1aThe search budget should be analysed more since it is very important to implement to previous diffusion models.**\n\n**A**: Thank you for raising the need for more discussion on this valuable question. We discuss the search budget thoroughly in the public comment above. Please refer to it.\n\n**W3: Why not demonstrated more categories of generated images when NFE is less? A fuller qualitative analysis would add strength on the USF.**\n\n**A**: Thank you for your suggestions! We add more qualitative results in App J of our revision. Please refer to it.\n\n**W4: How about the reproducibility of USF? Can you share the code? The open source code will help researchers continue to improve slow sampling speed since USF has great performance.**\n\n**A**: Thank you for your advice! Due to the time limitation, we can only upload the unclean code and some of our searched solver schedules to https://anonymous.4open.science/r/USF-anonymous-code-56E6. We will surely open source our code and upload all searched solver schedules in the future after carefully rearranging them."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4858/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700399870374,
                "cdate": 1700399870374,
                "tmdate": 1700706779373,
                "mdate": 1700706779373,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WCZ8jtRvLj",
                "forum": "W2d3LZbhhI",
                "replyto": "oHVhkn5Ce8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4858/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4858/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up"
                    },
                    "comment": {
                        "value": "Dear Reviewer HfC4,\n\nThanks once again for your valuable time and helpful suggestions. Did we address the concerns satisfactorily? If you have any further comments, please do not hesitate to tell us. We are more than willing to provide further clarifications."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4858/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700536494636,
                "cdate": 1700536494636,
                "tmdate": 1700536494636,
                "mdate": 1700536494636,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1hM3WsoQNQ",
            "forum": "W2d3LZbhhI",
            "replyto": "W2d3LZbhhI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4858/Reviewer_7SgP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4858/Reviewer_7SgP"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors proposed a predictor-based search method that optimizes the solver schedule to get a better time-quality trade-off of sampling. It is based on a unified sampling framework (USF) to study the optimal strategies for solver. Therefore, it enables us to take different solving strategies at different time steps, and according to the authors' study, it has a considerable potential to improve the sample quality and model efficiency. Finally, the authors used extensive experiments to validate the method of a large number of datasets such as CelebA, Cifar-10, LSUN, ImageNet and so on. Also, the authors applied S^3 to Stable-Diffusion models, and achieve 2x acceleration without losing the performance on text-to-image generation task on MS-COCO 256x256 dataset."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The reference list of this paper is very complete and the authors introduced their differences and connections quite clearly.\n2. The biggest strength of this paper is that, the experiments are really solid for me."
                },
                "weaknesses": {
                    "value": "1. The only weakness for me is on the writing style:\n(a) In Equation (1), dt and dw_t should be \\mathrm{d} t and \\mathrm{d}w_t. Similarly, in other parts of this paper, the authors sometimes use dt and sometimes use \\mathrm{d} t. The notation should be unified. \n(b) In Equation (5), the \\mathrm{d} t is missing in the right hand side of the equation. \n(c) For the Table 1, I suggest you using \\bigstrut before each line to make the text shown in the middle."
                },
                "questions": {
                    "value": "1. Can you briefly tell me the differences between this paper and Lu et al's DPM-solver++, since both of them seem to use different strategies as multi-step solver? What kind of improvements have you done in your paper compared with DPM Solver++?\n2. If time is available for you, maybe you can follow my suggestions on writing in the previous section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4858/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698702262194,
            "cdate": 1698702262194,
            "tmdate": 1699636469229,
            "mdate": 1699636469229,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "O8cJTqiOUh",
                "forum": "W2d3LZbhhI",
                "replyto": "1hM3WsoQNQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4858/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4858/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**W & Q2: Writing Style**\n\n**A**: We sincerely appreciate you for pointing out our typos. We have corrected them in the revision.\n\n**Q1: Can you briefly tell me the differences between this paper and Lu et al's DPM-solver++, since both of them seem to use different strategies as multi-step solver? What kind of improvements have you done in your paper compared with DPM Solver++?**\n\n**A**: The main contribution of DPM-Solver++ is the introduction of two new strategies: 1. multi-step solver and 2. data prediction model. Except for the first and last few steps that use a low-order solver, DPM-Solver++ uses the same solver strategies along timesteps.\n\nUSF comprehensively summarizes tunable components of all diffusion solvers based on the exponential integral. The two new strategies of DPM-Solver++ are also incorporated in USF. See Tab.1 and App E.2 for details. Moreover, USF introduces different solver strategies among all timesteps, expanding the design space. With searched solver schedules, USF outperforms DPM-Solver++ by a large margin. In conclusion, DPM-Solver++ is a special case of USF, and USF is far more effective and flexible than DPM-Solver++."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4858/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700399627187,
                "cdate": 1700399627187,
                "tmdate": 1700399928224,
                "mdate": 1700399928224,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xGsni8bBpR",
                "forum": "W2d3LZbhhI",
                "replyto": "1hM3WsoQNQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4858/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4858/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up"
                    },
                    "comment": {
                        "value": "Dear Reviewer 7SgP,\n\nThanks once again for your valuable time and helpful suggestions. Did we address the concerns satisfactorily? If you have any further comments, please do not hesitate to tell us. We are more than willing to provide further clarifications."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4858/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700536470674,
                "cdate": 1700536470674,
                "tmdate": 1700536470674,
                "mdate": 1700536470674,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wJgyIHBnwD",
                "forum": "W2d3LZbhhI",
                "replyto": "xGsni8bBpR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4858/Reviewer_7SgP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4858/Reviewer_7SgP"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your explanation on the difference between you paper and DPM-Solver++. I also see your revisions on the writing style. Overall, I think I will still keep the \"6\" score."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4858/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700681872822,
                "cdate": 1700681872822,
                "tmdate": 1700681872822,
                "mdate": 1700681872822,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bnh16p5FRN",
            "forum": "W2d3LZbhhI",
            "replyto": "W2d3LZbhhI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4858/Reviewer_VK39"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4858/Reviewer_VK39"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors propose a sampling framework for diffusion models for the systematic study of solving strategies for diffusion models. Specifically, the authors explore the solver schedule in the following aspects: timestep discretization, prediction type, starting point, solver order, derivative estimation, and corrector usage. Experiment results show that the proposed method boosts the performance, especially in the regime of very few NFEs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "$\\cdot$ This paper combines the AutoML methods into the diffusion sampling procedure. The authors explore the search space over timestep discretization, prediction type, starting point, solver order, derivative estimation, and corrector usage.\n\n$\\cdot$ The experiment results are appealing, especially in a few NFEs regime. The proposed method overperforms all baseline methods.\n\n$\\cdot$ The paper is overall well-written and the structure is clear."
                },
                "weaknesses": {
                    "value": "$\\cdot$ The experiment part seems to lack exact data on the computational cost of the overall pipeline of proposed methods. The overall pipeline needs iterative sampling images under a given sampling configuration, evaluating the performance and training of the proposed predictor models.\n\n$\\cdot$ The content of section 4.2 is not well-organized and needs further explanation."
                },
                "questions": {
                    "value": "Does the predictor model need to be trained separately on different NFEs?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4858/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4858/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4858/Reviewer_VK39"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4858/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698743874742,
            "cdate": 1698743874742,
            "tmdate": 1699636469150,
            "mdate": 1699636469150,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QyLpe4093C",
                "forum": "W2d3LZbhhI",
                "replyto": "bnh16p5FRN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4858/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4858/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**W1:The experiment part seems to lack exact data on the computational cost of the overall pipeline of proposed methods. The overall pipeline needs iterative sampling images under a given sampling configuration, evaluating the performance and training of the proposed predictor models.**\n\n**A**: Thank you for pointing out our oversight of inadequate discussion of this question. We have discussed the search overhead thoroughly in the global response above. Please refer to it.\n\n**W2:The content of section 4.2 is not well-organized and needs further explanation.**\n\n**A**: Thank you for your question. Sec 4.2 describes the workflow of our predictor-based multi-stage search algorithm, S3. We first initialize the population (a set of truly evaluated solver schedules) with several baseline schedules under all NFE budgets. Then we evolutionary sample a bunch of solver schedules from the search space and truly evaluate them to expand the initial population. Then we iteratively conduct the following 3 steps: 1. Training predictor with all schedules in the population; 2. Using the predictor to sample a new bunch of promising solver schedules from the search space; 3. Evaluating these new schedules to expand the population. After all iterations end, we finally choose the schedule with the best performance in the population as our search results. This method works because of the efficiency of evolutionary search and the acceleration brought by the predictor for exploring the search space.\n\nWe have reconstructed Sec 4.2 in the updated paper for better presentation.\n\n**Q:Does the predictor model need to be trained separately on different NFEs?**\n\n**A**: The predictor doesn't need to be trained on different NFEs.  The predictor is trained on the whole solver schedule population, which contains schedules under all NFE budgets. We pad all schedules to the same length for the predictor. The predictor is used to give a predicted score of any solver schedule with any NFE in the range."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4858/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700399403690,
                "cdate": 1700399403690,
                "tmdate": 1700454277320,
                "mdate": 1700454277320,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ceijGBmQgc",
                "forum": "W2d3LZbhhI",
                "replyto": "bnh16p5FRN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4858/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4858/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up"
                    },
                    "comment": {
                        "value": "Dear Reviewer VK39,\n\nThanks once again for your valuable time and helpful suggestions. Did we address the concerns satisfactorily? If you have any further comments, please do not hesitate to tell us. We are more than willing to provide further clarifications."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4858/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700536438253,
                "cdate": 1700536438253,
                "tmdate": 1700536438253,
                "mdate": 1700536438253,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Kz2y194OQc",
                "forum": "W2d3LZbhhI",
                "replyto": "ceijGBmQgc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4858/Reviewer_VK39"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4858/Reviewer_VK39"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed response. The authors' response addressed most of my issues."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4858/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700620358883,
                "cdate": 1700620358883,
                "tmdate": 1700620358883,
                "mdate": 1700620358883,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lM5Hr1QuXX",
            "forum": "W2d3LZbhhI",
            "replyto": "W2d3LZbhhI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4858/Reviewer_HHFa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4858/Reviewer_HHFa"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a framework to unify the existing Diffusion Probalisitc Model (DPM) solvers such as DPM-Solver, DPM-Solver++, UniPC and DESI. Under the framework, these methods can be represented with different options on some strategies and hyperparameters.\nThis paper shows the phenomenon that different solver strategies at different timesteps may yield different performances. This paper is inspired to design a schedule between different solving strategies during the sampling based on the unified framework. In this paper, the predicator-based method is proposed to schedule the solvers, which outperforms the SOTA solvers on several conditional datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. USF: This paper unifies the existing DPM solvers under a framework with detailed derivations and analysis, which provides a new perspective to review the DPM solvers and improve their performance in real applications.\n2. $S^3$: This paper proposes the $S^3$ method which uses light predictors to search the solver schedule which demonstrates good performance on unconditional datasets with limited NEF.\n3. Empirical Results: This paper has plentiful empirical studies. It demonstrates the situation that there is no single best solver under different timesteps. It also shows good performance compared with baselines and ablation studies on the consumption of generated images."
                },
                "weaknesses": {
                    "value": "1. This paper illustrates the phenomenon that the suitable strategies vary among timesteps but has no further explanation on why this phenomenon happens. The proposed method paper may lack motivation.\n2. This paper may lack novelty and contributions in the sense that it does not provide a further understanding of why suitable solvers are different on different timesteps. \n3. On empirical results, this paper may lack the results when the NFE is larger than 10. The gap between the baselines and the proposed method may decrease with larger NFE."
                },
                "questions": {
                    "value": "1. Can you provide an intuitive explanation of the phenomenon observed in the paper? \n2. Can you add the experiment results on larger NFEs? Since the search method also has the time consumption, can you provide some experiment results on it?\n3. Can you give a brief explanation of the meaning and effect of multi-stage in Algorithm 2?\n4. Since the USF can include the method DEIS in the domain $t$, why do you not include this method as a baseline in experiments?\n\nHere are some typos in App. D. In analyzing the current solvers, the index of $\\tilde{x}_{i-1}$ may be incorrect.\n\nThey should be $t_{i-1}$ in DPM-Solver-2, DPM-Solver++(2S) and DPM-Solver++(2M). \n\nWhen using the Assumption in the Appendix, it may not be Thm A.1, A.2 etc."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4858/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4858/Reviewer_HHFa",
                        "ICLR.cc/2024/Conference/Submission4858/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4858/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698764724674,
            "cdate": 1698764724674,
            "tmdate": 1700682199455,
            "mdate": 1700682199455,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5DvXZ8fknL",
                "forum": "W2d3LZbhhI",
                "replyto": "lM5Hr1QuXX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4858/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4858/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**W1, W2 & Q1: This paper may lack motivation, novelty and contributions in the sense that it does not provide a further understanding of why suitable solvers are different on different timesteps. Can you provide an intuitive explanation of the phenomenon observed in the paper?**\n\n**A**: Thank you for pointing out this question! Below we discuss the intuitions of the phenomenon \"suitable strategies are different at different steps\". We first lay out the high-level analyses, and then expand on each component.\n  - High level\n    - **The curvature of the ODE trajectory might vary across timesteps.** When $t$ is small, the neural network can reconstruct $x_0$ from $x_t$ more precisely and the ODE trajectory points to the target data. Intuitively, the curvature in the small $t$ region is small. Conversely, when $t$ is not close to 0, the ODE trajectory does not necessarily orient to the final target point and could have a large curvature [1]. Therefore, the curvature of the ODE trajectory might change over time. Some other papers have discussed similar findings. [2] demonstrates the pattern of VP-SDE in Sec.3, from which we can see that $x_t$ changes slowly at large $t$ and changes rapidly when $t$ is small; [1] finds that the trajectory orients to the mean value of the distribution at large $t$ while points to the data when $t$ is small. Different curvature leads to varying truncation errors, making certain solver strategies more suitable for specific timesteps.\n      - [1] Minimizing Trajectory Curvature of ODE-based Generative Models, Lee et al., ICML 2023\n      - [2] Elucidating the Design Space of Diffusion-Based Generative Models, Karras et al., NeurIPS 2022\n    - **The fitting error of the neural network also varies among timesteps.** The distributions of input for the neural network are different among timesteps. Therefore, the fitting tasks for the diffusion U-Net have unequal difficulty at different timesteps [3], leading to different distances between the predicted score and the ground truth score. Since the discretized solving error is related to the model's fitting error, it is natural for solver strategies to change across timesteps to adapt to different fitting accuracies.\n      - [3] OMS-DPM: Optimize the Model Schedule of Diffusion Probabilistic Models, Liu et al., ICML 2023\n    - **The requirements of solving accuracy are different at different timesteps.** At larger $t$, deviant values might be corrected to the right trajectories in subsequent solving processes. Conversely, for small $t$, a high solving error could directly cause a drop in sample quality. Therefore, different timesteps might need different solving strategies.\n  - Specific to components: Here we discuss the analyses that motivate using different strategies for each component. \n    - Step size: For low-resolution datasets, the processing of details is more important than large-resolution datasets. Therefore, the step size should be smaller near $t=0$ than near $t=T$ for low-resolution datasets. This pattern has been adopted empirically for low-resolution datasets in existing solvers like DDIM and DPM-Solver, which motivates us to use different step sizes and search for better schedules than empirical patterns.\n    - Order: Existing high-order solvers (e.g., DPM-Solver, DPM-Solver++, and UniPC) empirically use low orders for the last several steps when the NFE budget is tight, indicating that the suitable orders for different timesteps may not be the same. We are inspired by this setting to use changeable orders at all timesteps for better performance.\n    - Prediction Type: The output of the data prediction model changes rapidly when $t$ is large and changes slowly when $t$ is small since the data ratio of $x_t$ changes from 0 to 1 along timesteps. For the noise prediction model, the trend is the opposite. The change speed of model output has a close relationship with the stability of high-order derivatives. Therefore, the proper prediction types at different timesteps might be different.\n    - Corrector: The ODE corrector can be viewed as a special ODE predictor with the involvement of the function evaluation $f_\\theta(x_t, t)$ on the target point $t$. Thus, the above analysis also holds for corrector. We find that different corrector patterns are used by concurrent work [1] empirically to boost its results.\n      - [1] DPM-Solver-v3: Improved Diffusion ODE Solver with Empirical Model Statistics, Zheng et al., NeurIPS 2023\n    - The choices of different components are coupled with each other. For example, low Taylor order and low derivative estimation order are intuitively suitable for a large step size; prediction type affects the choice of derivative scaling method. In addition, the choice of solver strategy at one step is impacted by the solution accuracy of previous steps. Therefore, we apply different strategies for all components at different timesteps and search for the optimal schedule."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4858/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700398747516,
                "cdate": 1700398747516,
                "tmdate": 1700453622313,
                "mdate": 1700453622313,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XTfsZSoBZu",
                "forum": "W2d3LZbhhI",
                "replyto": "lM5Hr1QuXX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4858/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4858/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**W3 & Q2: This paper may lack the results when the NFE is larger than 10. The gap between the baselines and the proposed method may decrease with larger NFE.**\n\n**A**: Thanks for pointing out the need for larger NFE results. We report the FID result of baselines and our searched schedules below, where Baseline-B represents the baseline with the best setting and Baseline-W represents for the worst (see Sec 5 and Sec G.1 for details). We can see that our method still outperforms all baselines by a large margin at larger NFEs like 12, 15, and 20. Our method completely converges at NFE=15, much faster than existing solvers.\n\n|  |  | CIFAR-10 |  |  | CelebA |  |\n| --- | --- | --- | --- | --- | --- | --- |\n| NFE | 12 | 15 | 20 | 12 | 15 | 20 |\n| Baseline-W | 5.31 | 4.52 | 3.54 | 6.12 | 4.20 | 3.56 |\n| Baseline-B | 3.67 | 3.03 | 2.80 | 3.82 | 2.72 | 2.70 |\n| Ours | **2.65** | **2.41** | **2.41** | **2.32** | **2.06** | **2.06** |\n\nWe would like to emphasize that when the NFE budget is adequate, the negative impact of sub-optimal empirical strategies diminishes. This fact can be verified by the decreasing gap between Baseline-W and Baseline-B. So, it is very reasonable that the gap between our method and baselines decreases. Considering this fact, we choose a more challenging and meaningful scenario to optimize, i.e., sampling under very few NFE budgets (4-10).\n\n**Q4: Since the USF can include the method DEIS in the domain $t$, why do you not include this method as a baseline in experiments?**\n\n**A**: For a more comprehensive comparison, we report the results of DEIS directly from their paper [3] and the results of our method below. We don't choose DEIS as a baseline in our paper because UniPC and DPM-Solver++ have already compared DEIS with their methods in their papers. DEIS fails to outperform DPM-Solver++ or UniPC in most cases. \n  - [3] FAST SAMPLING OF DIFFUSION MODELS WITH EXPONENTIAL INTEGRATOR, Zhang & Chen, ICLR 2023\n\n|  |  | CIFAR-10 |  |  |  | CelebA  |  |  |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| NFE | 5 | 7 | 10 | 15 | 5 | 7 | 10 | 20 |\n| DEIS | 15.37 | - | 4.17 | 3.37 | 25.07 | - | 6.95 | 3.41 |\n| Ours | **7.65** | **3.91** | **3.09** | **2.41** | **5.17** | **3.80** | **2.73** | **2.06** |\n\n**Q3: Can you give a brief explanation of the meaning and effect of multi-stage in Algorithm 2?**\n\n**A**: Our main idea of S3 is to train a predictor to efficiently predict the performance of a solver schedule rather than evaluate its true performance by sampling a large number of images.\n\nThe **meaning** of \"multi-stage\": we iteratively conduct the following process with each iteration called a \"stage\": 1. Training predictor with all truly evaluated solver schedules in the population. 2. Using predictor to explore within the search space. 3. Truly evaluating the promising solver schedules selected by the predictor to expand the population. In Alg.2, the $for$ loop starting from line 3 represents the index of stage. The overall workflow is demonstrated in Fig.4.\n\nThe **effect** of multi-stage search: At the early stage of the search, the predictor is only required to explore in a limited sub-space. Therefore, only a small amount of schedule-performance data pair is needed to train the predictor, and the predictor can be quickly put into use. As the search process advances, the exploration range of the predictor increases. Thus, it needed to be updated with more data to ensure the accuracy of its prediction. Compared to the single-stage search (i.e., evaluate a large number of solver schedules in one go to train the predictor and perform the predictor-based search only once), the muli-stage method makes the sampling of solver schedule for true evaluation more effective since the predictor involves in this process.\n\nThanks for your question! For a better presentation of the search method, we have reconstructed Sec 4.2 in the revision.\n\n**Q: Typos** \n\n**A**: We appreciate you very much for pointing out these typos in our paper. We have corrected these mistakes in our revision."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4858/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700399104787,
                "cdate": 1700399104787,
                "tmdate": 1700556257431,
                "mdate": 1700556257431,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LNqbrbqJwZ",
                "forum": "W2d3LZbhhI",
                "replyto": "lM5Hr1QuXX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4858/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4858/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up"
                    },
                    "comment": {
                        "value": "Dear Reviewer HHFa,\n\nThanks once again for your valuable time and helpful suggestions. Did we address the concerns satisfactorily? If you have any further comments, please do not hesitate to tell us. We are more than willing to provide further clarifications."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4858/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700536345716,
                "cdate": 1700536345716,
                "tmdate": 1700536428484,
                "mdate": 1700536428484,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VjfKFgHDQA",
                "forum": "W2d3LZbhhI",
                "replyto": "LNqbrbqJwZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4858/Reviewer_HHFa"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4858/Reviewer_HHFa"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your detailed response. This addressed most of my concerns. I raised my score."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4858/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700682223387,
                "cdate": 1700682223387,
                "tmdate": 1700682223387,
                "mdate": 1700682223387,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]