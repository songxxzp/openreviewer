[
    {
        "title": "EventRPG: Event Data Augmentation with Relevance Propagation Guidance"
    },
    {
        "review": {
            "id": "XUo3DimIPL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3287/Reviewer_rLwX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3287/Reviewer_rLwX"
            ],
            "forum": "i7LCsDMcZ4",
            "replyto": "i7LCsDMcZ4",
            "content": {
                "summary": {
                    "value": "This paper proposes a mixup-based data augmentation method for training Spiking Neural Networks (SNNs). Inspired by saliency-based augmentation in RGB image vision such as Puzzle Mix and SaliencyMix, the authors derive the Class Activation Map (CAM) and saliency map for SNNs, and then mix two samples based on them. Experimental results show that the proposed EventRPG consistently improves the classification accuracy on various datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The derivation of the CAM and saliency map of SNNs itself is a clear contribution. The results in Table 1 prove the correctness of this\n- EventRPG is able to consistently improve performance across event datasets\n- The time cost of EventRPG is comparable to similar augmentations in conventional vision"
                },
                "weaknesses": {
                    "value": "My main concern is regarding the experiments:\n- The paper motivates the need for event data augmentation with the statement that \"the lack of huge event-based datasets similar to Imagenet prevents us from improving the model performance on relatively small datasets using a Pretrain-Finetune paradigm\". However, there is an event camera version of ImageNet available [1], and its paper shows that pre-training on N-ImageNet can greatly improve the accuracy on other datasets via transfer learning. Therefore, this statement in the Introduction is wrong\n- Since N-ImageNet is available, I would like to see results on this dataset. This is similar to conventional vision research on data augmentation, where ImageNet is the best testbed. If the authors are not able to train on N-ImageNet, the mini subset can be considered, though I do not think that is a comprehensive benchmark\n- The authors compare EventRPG with conventional vision methods such as Grad-CAM in Table 1. Thus, I wonder if it is possible to compare with Puzzle Mix and SaliencyMix in Table 3?\n- NDA applies the method to unsupervised contrastive learning and shows promising results. Is it possible to conduct such experiments using EventRPG?\n\n[1] Kim, Junho, et al. \"N-ImageNet: Towards robust, fine-grained object recognition with event cameras.\" ICCV. 2021."
                },
                "questions": {
                    "value": "See Weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3287/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3287/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3287/Reviewer_rLwX"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3287/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697257881036,
            "cdate": 1697257881036,
            "tmdate": 1700586969974,
            "mdate": 1700586969974,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Dbpf1XqfKF",
                "forum": "i7LCsDMcZ4",
                "replyto": "XUo3DimIPL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3287/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3287/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> **Weakness 1**\nThe paper motivates the need for event data augmentation with the statement that \"the lack of huge event-based datasets similar to Imagenet prevents us from improving the model performance on relatively small datasets using a Pretrain-Finetune paradigm\". However, there is an event camera version of ImageNet available [1], and its paper shows that pre-training on N-ImageNet can greatly improve the accuracy on other datasets via transfer learning. Therefore, this statement in the Introduction is wrong\n\nThanks for pointing out this mistake. We've removed the misleading sentences accordingly in the revised version. Here's the revised content:\n\n\"In terms of classification tasks, a number of event-based datasets, such as N-MNIST, N-Caltech101 [1], and CIFAR10-DVS [2], have been used to evaluate the performance of artificial neural networks (ANNs) and SNNs. **However, the issue of overfitting still poses a significant challenge for event-based datasets.** Data augmentation is an efficient method for improving the generalization and performance of a model.\"\n\n[1] Orchard, Garrick, et al. \"Converting static image datasets to spiking neuromorphic datasets using saccades.\" Frontiers in neuroscience 9 (2015): 437.\n\n[2] Li, Hongmin, et al. \"Cifar10-dvs: an event-stream dataset for object classification.\" Frontiers in neuroscience 11 (2017): 309."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3287/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700478115142,
                "cdate": 1700478115142,
                "tmdate": 1700499137490,
                "mdate": 1700499137490,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3sny26N6fO",
                "forum": "i7LCsDMcZ4",
                "replyto": "XUo3DimIPL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3287/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3287/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> **Weakness 2**\nSince N-ImageNet is available, I would like to see results on this dataset. This is similar to conventional vision research on data augmentation, where ImageNet is the best testbed. If the authors are not able to train on N-ImageNet, the mini subset can be considered, though I do not think that is a comprehensive benchmark\n\nThanks for your advice. Mini N-ImageNet is a comprehensive datasets suitable for evaluating our method. We conduct experiments on this dataset and present them in the Appendix A.1. On this relatively difficult datasets, our method still achieves best results and is also shown to effectively alleviate the overfitting problem. As shown by the notable positive effects, augmentations significantly enhance the performance of the model. In this comparison, our approach outperforms the nearest competitor NDA, achieving a higher Top-1 accuracy by $5.06\\\\%$ and Top-5 accuracy by $4.1\\\\%$. Also the loss curves in Fig. 6 highlight the efficiency of our approach in boosting the model's performance and mitigating overfitting.\n\n| Data Augmentation | Identity | EventDrop | NDA    | EventRPG |\n| :------------------ | :------- | :-------- | :----- | :------- |\n| Top-1 Accuracy      | 28\\.16   | 34\\.18    | 35\\.84 | **40\\.90**   |\n| Top-5 Accuracy      | 52\\.14   | 60\\.94    | 63\\.64 | **67\\.74**   |"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3287/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700478161546,
                "cdate": 1700478161546,
                "tmdate": 1700582155264,
                "mdate": 1700582155264,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MDp82gdWNp",
                "forum": "i7LCsDMcZ4",
                "replyto": "XUo3DimIPL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3287/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3287/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> **Weakness 3**\nThe authors compare EventRPG with conventional vision methods such as Grad-CAM in Table 1. Thus, I wonder if it is possible to compare with Puzzle Mix and SaliencyMix in Table 3?\n\nThanks for your comments. Comparing our RPGMix method with Puzzle Mix and SaliencyMix is crucial for a thorough analysis. We compared various saliency-based mixing methods on N-Caltech101 and SL-Animals datasets, with results detailed in Appendix A.2. \n\n| Dataset          | Model        | Identity | Saliency Mix | PuzzleMix | PuzzleMix (mask only) | RPGMix |\n| :--------------- | :----------- | -------: | -----------: | ---------------------------: | ---------------------------: | -----: |\n| N-Caltech101     | Spiking-VGG11       | 75\\.7    | 72\\.63       | 79\\.38                       | 78\\.13                       | **81\\.75** |\n| SL-Animals-4Sets | SEW-Resnet18 | 85\\.42   | 84           | 85\\.34                       | 85\\.68                       | **88\\.67** |\n| SL-Animals-3Sets | SEW-Resnet18 | 89\\.09   | 89\\.29       | 88\\.39                       | 90\\.18                       | **90\\.45** |\n\nContrary to Saliency Mix and Puzzle Mix, which can sometimes reduce performance, our RPGMix consistently has a positive impact on both datasets. This proves the advantage of RPGMix compared with Puzzle Mix and Saliency Mix on event-based data."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3287/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700478329068,
                "cdate": 1700478329068,
                "tmdate": 1700499210699,
                "mdate": 1700499210699,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GWVjw0S0fz",
                "forum": "i7LCsDMcZ4",
                "replyto": "XUo3DimIPL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3287/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3287/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> **Weakness 4**\nNDA applies the method to unsupervised contrastive learning and shows promising results. Is it possible to conduct such experiments using EventRPG?\n\nIn line with NDA, we apply SimSiam [1] for unsupervised pretraining on a Resnet-18 model, distinguishing between non-mixing and mixing augmentations. SimSiam employs non-mixing augmentations to create varied samples from a single one, which we adopt for EventDrop's unsupervised pretraining, as it exclusively uses non-mixing augmentations. For EventRPG and NDA, which incorporate both types of augmentations, we follow the Mixsiam [2] approach for unsupervised contrastive pretraining.\n\nWithin the SimSiam training framework, the predictor's output layer does not reflect classification results. Therefore, implementing CLRP at the output layer is not feasible. To address this, we initialize the relevance scores of the output layer with the neural network's output values.\n\nDuring the finetuning phase, to ensure a fair comparison, we do not apply any augmentations to the data. As a result, our outcomes are expected to be lower than those presented in the original paper by NDA [3].\n\n| Radom Initialized | ImageNet Pretrain | EventDrop | NDA    | EventRPG |\n| :---------------- | :---------------- | :-------- | :----- | :------- |\n| 47\\.56            | 74\\.32            | 74\\.56    | 74\\.56 | **76\\.81**   |\n\nThe table shows that pretraining markedly improves model performance. Models pretrained using EventDrop and NDA perform comparably to those pretrained on ImageNet. However, models pretrained with our approach show a notable $2.49\\\\%$ improvement. This highlights the promise of applying EventRPG to unsupervised contrastive learning, an area we intend to investigate more in future work. Therefore, these results are not included in the current manuscript.\n\n[1] Chen, Xinlei, and Kaiming He. \"Exploring simple siamese representation learning.\" In CVPR 2021.\n\n[2] Guo, Xiaoyang, et al. \"Mixsiam: a mixture-based approach to self-supervised representation learning.\" arXiv preprint arXiv:2111.02679 (2021).\n\n[3] Li, Yuhang, et al. \"Neuromorphic data augmentation for training spiking neural networks.\" In ECCV 2022."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3287/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700478409659,
                "cdate": 1700478409659,
                "tmdate": 1700499250366,
                "mdate": 1700499250366,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3YoD1Kzrsn",
                "forum": "i7LCsDMcZ4",
                "replyto": "GWVjw0S0fz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3287/Reviewer_rLwX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3287/Reviewer_rLwX"
                ],
                "content": {
                    "title": {
                        "value": "Re: Rebuttal"
                    },
                    "comment": {
                        "value": "I thank the authors for the detailed replies and additional experiments. Can you explain more about the last point, especially the statement:\n\n> During the finetuning phase, to ensure a fair comparison, we do not apply any augmentations to the data. As a result, our outcomes are expected to be lower than those presented in the original paper by NDA [3].\n\nWill you achieve better results and outperform NDA if you augment the training data? I do not see any issues doing this (I assume this is also what NDA does). Maybe one issue is about the training speed/computation overhead brought by the data augmentation.\nThus one more question I have is how is the speed of your method compared to NDA?"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3287/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700501820154,
                "cdate": 1700501820154,
                "tmdate": 1700501820154,
                "mdate": 1700501820154,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "y5YkqDnHJL",
                "forum": "i7LCsDMcZ4",
                "replyto": "XUo3DimIPL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3287/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3287/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Our expression \"During the finetuning phase, to ensure a fair comparison, we do not apply any augmentations to the data. As a result, our outcomes are expected to be lower than those presented in the original paper by NDA [3].\" may causes misunderstanding. Here we put more details and hope it will be clearer:\n\n\nIn our experiments on contrastive learning, we pretrain the model using NDA, EventDrop, and EventRPG, respectively, while no augmentation is used in the finetuning stage. If we augment the data in the finetuning stage, we would not know how much improvement the pretraining brings, since the data augmentation in the pretraining stage and the finetuning stage both bring improvements to the performance. Excluding the data augmentation in the finetuning stage is helpful for a fair comparison. Under this setting, our method is better than NDA.\n\n\n\nIf we leverage data augmentation in both pretraining and finetuning stage as what NDA does, EventRPG still achieves better results with an improvement of $0.78\\\\%$ compared to NDA, as shown in the table below (The result of NDA under this setting is from its original paper [1]). We hope you are satisfied with the experiment.\n\n\n| Pretrain Augmentation | Finetune Augmentation | Pretrain Epoch | Finetune Epoch | Accuracy   |\n| :---------------------: | :-------------------: | :------------: | :------------: | :--------: |\n| NDA                     | Identity              | 600            | 100            | 74\\.56     |\n| EventRPG                     | Identity              | 600            | 100            | **76\\.81** |\n| NDA                     | NDA                   | 600            | 100            | 80\\.80      |\n| EventRPG                     | EventRPG                   | 600            | 100            | **81\\.58** |\n\nWe believe that EventRPG is the superior choice, both during the pretraining and finetuning stages, as demonstrated by the experimental results.\n\nWe also record the time cost durint the finetuning, and the results are shown below. The time cost of EventRPG is very close to that of NDA, indicating that our method is time-efficient for ANNs.\n\n\n|                                     | EventRPG | NDA   |\n| :----------------------------------: | :--------: | :----: |\n| Augmentation Time (ms/event stream) | 6\\.17     | 5\\.43 |\n\n\nAdditionally, We carried out experiments comparing the time costs (ms per event stream) of different augmentation methods on SNN at CIFAR10DVS dataset, as shown in the table below. \n\n| Batch Size per GPU | EventDrop | NDA      | EventRPG       |\n| :----------------: | :-------: | :------: | :-------: |\n| 1                  | 5\\.00743  | 5\\.29057 | 11\\.05086 |\n| 2                  | 5\\.2048   | 5\\.63054 | 7\\.83654  |\n| 4                  | 5\\.24821  | 5\\.95036 | 6\\.46512  |\n| 8                  | 4\\.88     | 5\\.9433  | 6\\.13769  |\n| 16                 | 4\\.93906  | 6\\.14297 | 5\\.55679  |\n| 32                 | 4\\.65249  | 6\\.03272 | 5\\.34178  |\n\nIt can be seen that when the batch size of each GPU surpasses 4, the time cost of EventRPG is close to those of EventDrop and NDA. More details can be found in the Section 5.2.3 of our revised paper.\n\nWe thank you again for your suggestions, which have been very helpful in improving our paper. Please let us know if you have further questions.\n\n[1] Li, Yuhang, et al. \"Neuromorphic data augmentation for training spiking neural networks.\" In ECCV 2022."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3287/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700581172960,
                "cdate": 1700581172960,
                "tmdate": 1700581355106,
                "mdate": 1700581355106,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OsDawQHKqg",
                "forum": "i7LCsDMcZ4",
                "replyto": "y5YkqDnHJL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3287/Reviewer_rLwX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3287/Reviewer_rLwX"
                ],
                "content": {
                    "title": {
                        "value": "Re: Rebuttal"
                    },
                    "comment": {
                        "value": "Thanks. I am satisfied with the results and have updated my score."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3287/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700586947407,
                "cdate": 1700586947407,
                "tmdate": 1700586947407,
                "mdate": 1700586947407,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vBuBemJ6fq",
            "forum": "i7LCsDMcZ4",
            "replyto": "i7LCsDMcZ4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3287/Reviewer_DHWr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3287/Reviewer_DHWr"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes two efficient and practical methods, SLTRP and SLRP, for generating CAMs and saliency maps for SNNs for the first time. Based on these, the authors propose EventRPG to achieve data augmentation, which drops events and mixes events with Relevance Propagation Guidance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper proposes Spiking Layer-wise Relevance Propagation(SLRP) rule and Spiking Layer-Time-wise Relevance Propagation(SLTRP) rule, the layer-wise relevance propagation method of SNNs for the first time, which can obtain the feature contribution at each pixel. RGBDrop and RGBMix are established to achieve data augmentation based on the generated CAMs. The results of experiments prove the usefulness of SLRP and SLTRP both for accuracy and efficiency. The EventRPG shows good performance in object recognition and action recognition tasks."
                },
                "weaknesses": {
                    "value": "1. The description of RPGMix is quite simple and unclear. Section 4.3 fails to clearly illustrate the algorithm flow of RPGMix. Many operations in Fig 3(b) are not carefully analyzed, such as sample position in the Nonoverlapping Region, which makes it hard to understand. I think Fig3 is the main figure of this paper and Fig 3(b) accounts for the most part of Fig3, hence the authors need to spend more space to describe it. Otherwise, the readers may feel confused about RPGMix.\n2. Equation 15 lacks physical meaning and theoretical basis. It needs more explanations for researchers to make further progress.\n3. The objective faithfulness of SLRP and SLTRP is not outstanding enough. In N-Cars,  DVSGesture, and SL-Animals datasets, the proposed methods have similar or even inferior performance compared to other algorithms."
                },
                "questions": {
                    "value": "The author should carefully illustrate RPGMix and the performances of SLRP and SLTRP are challenged in the metric of the objective faithfulness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics concerns."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3287/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698740768472,
            "cdate": 1698740768472,
            "tmdate": 1699636277294,
            "mdate": 1699636277294,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Pd4KLj3WSK",
                "forum": "i7LCsDMcZ4",
                "replyto": "vBuBemJ6fq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3287/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3287/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> **Weakness 1**\nThe description of RPGMix is quite simple and unclear. Section 4.3 fails to clearly illustrate the algorithm flow of RPGMix. Many operations in Fig 3(b) are not carefully analyzed, such as sample position in the Nonoverlapping Region, which makes it hard to understand. I think Fig3 is the main figure of this paper and Fig 3(b) accounts for the most part of Fig3, hence the authors need to spend more space to describe it. Otherwise, the readers may feel confused about RPGMix.\n\n> **Weakness 2**\nEquation 15 lacks physical meaning and theoretical basis. It needs more explanations for researchers to make further progress.\n\nWe apologize for the earlier lack of clarity. We have added the motivation and more details of the implementation in the Section 4.3. Here's the content:\n\n\"**Event-based data, in contrast to image-based data, does not include color details, with the most crucial aspect being the texture information it contains. The overlapping of label-related objects will impair the texture details of these objects, which in turn further degrades the quality of features extracted in SNNs. Building upon this motivation, we propose Relevance Propagation Guided Event Mix (RPGMix).** The whole mixing strategy is illustrated in Fig. 2b. For two event-based data candidates, we utilize relevance propagation to localize the label-related regions and obtain two bounding boxes. **To mix two objects with clear texture features, we randomly select two positions ensuring minimal overlap of their bounding boxes. This involves initially positioning one box at a corner to maximize the nonoverlapping area for the other box's placement, then selecting positions for both boxes in order, maintaining minimal overlap and maximizing sampling options.** Finally, the two event streams are moved to the sampled positions. Although this linear translation part prevents the overlapping of label-related objects, the background of one object would still overlap with the other object. Moreover, in one single time step, the representation ability of the spiking neurons (which only output binary information) is much worse than that of the activation layer (usually ReLU) of ANNs, making them less capable of spatial resolution and more likely to fall into local optima. Therefore, to promise the presence of only events from a single event stream candidate per pixel, avoiding regions with mixed information from interfering with the SNN, we adopt a CutMix strategy to mask the two event streams based on the bounding box of the second event stream, as demonstrated in the left part in Fig. 2b. **[1] takes the sum of each sample's mask as the ratio of their corresponding labels. This ensures that the proportion of labels in the mixed label matches the proportion of pixels belonging to each sample. In our approach, we further aim to align the proportion of labels with the proportion of label-related pixels, which can be estimated using the bounding boxes. As a result, the labels of the two event streams are mixed as** \n$$L_{mix} = \\frac{L_{1}(w_{1}h_{1} - S_{overlap}) + L_{2}w_{2}h_{2}}{w_{1}h_{1} + w_{2}h_{2} - S_{overlap}},$$\nwhere $w_{i}$ and $h_{i}$ denote the width and height of the object in the event stream $i$. $L_1$ and $L_2$ are the one-hot labels of the two event streams and $S_{overlap}$ is the area of the overlapping region of the two bounding boxes.\"\n\n\n\n[1] Kim, Jang-Hyun, Wonho Choo, and Hyun Oh Song. \"Puzzle mix: Exploiting saliency and local statistics for optimal mixup.\" In ICML 2020."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3287/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700477896222,
                "cdate": 1700477896222,
                "tmdate": 1700499052236,
                "mdate": 1700499052236,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OerrMGETKz",
                "forum": "i7LCsDMcZ4",
                "replyto": "vBuBemJ6fq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3287/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3287/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> **Weakness 3**\nThe objective faithfulness of SLRP and SLTRP is not outstanding enough. In N-Cars, DVSGesture, and SL-Animals datasets, the proposed methods have similar or even inferior performance compared to other algorithms.\n\nIn this comparison, although traditional methods may perform well on specific datasets, our methods stand out for their overall superior performance. To provide a comprehensive comparison, we calculated the average improvement percentage for each method.\n\nThe A.I. or A.D. score of method $ i $ on dataset $j $ is denoted as $ v_{i, j} $. We compute each dataset's mean value as $ v_{mean,j}=\\\\frac{1}{N_{a}}\\\\sum_i v_{i,j} $, where $ N_a $ represents the count of augmentations. The augmentation improvement percentage is $ v'\\_{i,j} =\\\\frac{(v_{i, j} - v_{mean, j})}{v_{mean, j}}$. Finally, we average the improvement for each method across datasets for A.I. or A.D. as $ I_{i}=\\\\frac{1}{N_{d}}\\\\sum_j v'_{i,j} $, with $ N_d $ being the number of datasets. Here's the averaged improvement of A.I. and A.D. In this comprehensive statistical analysis, SLTRP-Saliency Map and SLRP-Saliency Map surpass the performance of other methods, proving their efficiency.\n\n| Method             | Average A.I. Improvement\u00a0\u2191 | Average A.D. Improvement\u00a0\u2193 |\n| :----------------- | -------------------------: | -------------------------: |\n| SAM                | -0\\.52                     | -0\\.04                     |\n| Grad-CAM           | -0\\.43                     | 1\\.29                      |\n| Grad-CAM++         | -0\\.07                     | -0\\.30                     |\n| SLRP-RelCAM        | 0\\.13                      | -0\\.25                     |\n| SLTRP-RelCAM       | 0\\.13                      | -0\\.25                     |\n| SLRP-CAM           | -0\\.05                     | 0\\.18                      |\n| SLTRP-CAM          | -0\\.05                     | 0\\.18                      |\n| SLRP-Saliency Map  | **0\\.44**                      | **-0\\.41**                     |\n| SLTRP-Saliency Map | 0\\.42                      | -0\\.40                     |                 |\n\nAdditionally, CAMs produced by previous methods highlight the model's focused regions at the feature map level, while saliency maps obtained from SLRP and SLTRP disclose the model's attention at a more specific pixel level."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3287/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700478042975,
                "cdate": 1700478042975,
                "tmdate": 1700499095718,
                "mdate": 1700499095718,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LTjojp5gdU",
            "forum": "i7LCsDMcZ4",
            "replyto": "i7LCsDMcZ4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3287/Reviewer_dKSN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3287/Reviewer_dKSN"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses the problem of event-based data augmentation for Spiking Neural Networks by computing saliency. Two methods are presented with different levels of overhead on training; these methods are incorporated in two augmoentation schemes that either drop events or mix two event streams. A number of SNNs was evaluated on both object and action recognition datasets, and improvement in accuracy was demonstrated - especially on action recognition datasets."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1) The authors mention that the code will be released; to me this is important as it enables other researchers to easily build on top of this paper.\n\n2) I particularly enjoyed the theoretical introduction into the SNNs, this makes the rest of the paper much easier to read."
                },
                "weaknesses": {
                    "value": "1) If anything, I would like to notice here that the improvements compared to competing methods are generally small. It would be interesting to see an apples-to-apples comparison in therms of compute overhead (which is mentioned, but I do not see numberical benchmark results); or, another compelling reason to use the presented methods vs e.g. second best.\n\n2) Also see 'questions': it would be good to show that saliency on motion-related datasets is more than just event density. This could be done e.g. by correlating saliency to raw optical flow magnitude, or evaluating on actions that are slower."
                },
                "questions": {
                    "value": "1) In abstract, expand SNN as 'Spiking Neural Networks', for the benefit of readers unfamiliar with the abbreviation.\n\n2) Fig. 1 highlights a failure case of one of the augmentation methods; it would be great to see a citation that explains why this limitation is difficult to alleveiate (e.g. by considering correct label classes). The image also seems excessive, and a single-sentence explanation should be enough.\n\n3) I am curious if the better performance / saliency on action-related datasets is an artifact of the event camera itself - since the density of the events correlates with motion and SNN may get more cues on regions with faster motion.\n\n4) It would be beneficial, in tables 3 and 4 to mention that all of the datasets are 'native' event-based datasets (unless I am mistaken). What would be the comparison if a classic camera dataset was converted to events, especially on action classification tasks?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3287/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699136082403,
            "cdate": 1699136082403,
            "tmdate": 1699636277198,
            "mdate": 1699636277198,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "egC4gf5DtE",
                "forum": "i7LCsDMcZ4",
                "replyto": "LTjojp5gdU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3287/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3287/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> **Weakness 1** If anything, I would like to notice here that the improvements compared to competing methods are generally small. It would be interesting to see an apples-to-apples comparison in therms of compute overhead (which is mentioned, but I do not see numberical benchmark results); or, another compelling reason to use the presented methods vs e.g. second best.\n\nThanks for pointing out this missing experiment. In the updated version of our paper, we've included time consumption experiments (refer to section 5.2.3). It can be seen that as batch size increases, the time cost of our method decreases in a near-linear manner. Particularly when the batch size per GPU exceeds 4, the augmentation time for EventRPG aligns closely with that of EventDrop and NDA. Additionally, we've conducted new experiments comparing various augmentation methods on the mini N-ImageNet dataset (see Appendix A.1) and in the context of unsupervised contrastive learning (refer to our responses to Reviewer rLwX's W4 query). Under these scenarios, our method surpasses both EventDrop and NDA without incurring extra computational costs, as all batch sizes per GPU are set above 4. Additionally, to the best of our knowledge, our results on N-Caltech101, CIFAR10-DVS, and SL-Animals represent the state-of-the-art (SOTA) in the direct training track for Spiking Neural Networks (SNNs)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3287/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700477450880,
                "cdate": 1700477450880,
                "tmdate": 1700498755765,
                "mdate": 1700498755765,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rTn8rSCt13",
                "forum": "i7LCsDMcZ4",
                "replyto": "LTjojp5gdU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3287/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3287/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> **Weakness 2**  Also see 'questions': it would be good to show that saliency on motion-related datasets is more than just event density. This could be done e.g. by correlating saliency to raw optical flow magnitude, or evaluating on actions that are slower.\n\n> **Question 3** I am curious if the better performance / saliency on action-related datasets is an artifact of the event camera itself - since the density of the events correlates with motion and SNN may get more cues on regions with faster motion.\n\nYour suggestion to validate our method's selectivity via qualitative comparison is excellent. We attempted to replicate leading optical flow estimation works like TMA [1] and E-RAFT [2], but their primary training on autonomous driving datasets somewhat restricts their effectiveness in object recognition tasks. As an alternative, we have visualized additional saliency map results using SLTRP in Appendix A.3. We found that saliency maps don't concentrate on high event density areas, but rather on objects or actions related to the label. For instance, in SL-Animals results (second row), the saliency maps focus on the person's hand raised to their head, assigning minimal attention to the lower area of the person, despite its high event density. Similarly, in the fourth row, the saliency maps show little interest in the person's right hand (from our perspective), focusing instead on the left hand, even though both hands have similar event densities. We believe that it's the label-related actions, rather than event density, that truly draw the focus of the saliency maps. This observation further confirms the selectivity of the SLTRP-generated saliency maps in both action recognition and object recognition datasets (see Fig. 8).\n\n[1] Liu, Haotian, et al. \"TMA: Temporal Motion Aggregation for Event-based Optical Flow.\" In ICCV 2023.\n\n[2] Gehrig, Mathias, et al. \"E-raft: Dense optical flow from event cameras.\" In 3DV 2021."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3287/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700477542003,
                "cdate": 1700477542003,
                "tmdate": 1700499322956,
                "mdate": 1700499322956,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Y8jYZQ3KNu",
                "forum": "i7LCsDMcZ4",
                "replyto": "LTjojp5gdU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3287/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3287/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> **Question 1** In abstract, expand SNN as 'Spiking Neural Networks', for the benefit of readers unfamiliar with the abbreviation.\n\n> **Question 2** Fig. 1 highlights a failure case of one of the augmentation methods; it would be great to see a citation that explains why this limitation is difficult to alleveiate (e.g. by considering correct label classes). The image also seems excessive, and a single-sentence explanation should be enough.\n\nThanks for the clarification. We expand \"SNN\" to \"Spiking Neural Networks\" in the Abstract and instead of Fig. 1, we include a brief citation to introduce the failure case, maintaining brevity in the text. This also helps to save more space for other valuable content.\n\nHere's the revised version of the introduction to failure cases:\n\n\"Nevertheless, current mixing augmentation strategies in event-based field do not consider the size and location information of label-related objects, **and thus may produce events with incorrect labels and disrupt the training process [1, 2]. To address this problem in image processing field, They mix the label-related objects together based on the saliency information obtained from neural networks.** This paradigm achieves better results compared with conventional non-saliency augmentations.\"\n\n[1] Uddin, A. F. M., et al. \"Saliencymix: A saliency guided data augmentation strategy for better regularization.\" In ICLR 2020.\n\n[2] Kim, Jang-Hyun, Wonho Choo, and Hyun Oh Song. \"Puzzle mix: Exploiting saliency and local statistics for optimal mixup.\" In ICML 2020."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3287/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700477625600,
                "cdate": 1700477625600,
                "tmdate": 1700498952531,
                "mdate": 1700498952531,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9x6kjy6m4a",
                "forum": "i7LCsDMcZ4",
                "replyto": "LTjojp5gdU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3287/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3287/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> **Question 4** It would be beneficial, in tables 3 and 4 to mention that all of the datasets are 'native' event-based datasets (unless I am mistaken). What would be the comparison if a classic camera dataset was converted to events, especially on action classification tasks?\n\nIn our experiments, we used three object recognition datasets. Two of them, N-Caltech101 and CIFAR10-DVS, are both derived by moving an event camera in front of static images from the Caltech-101 [1] and CIFAR-10 [2] image datasets, respectively. We've included explanations regarding this in the captions of Tables 3 and 4. Converting video frames into event data is indeed a promising area, as it can significantly diversify event-based datasets. However, the quality of events generated from videos through simulation largely depends on the video frame rate. A lower frame rate can lead to discontinuity in the time dimension of the converted events, thereby increasing the disparity between simulated and real event datasets. We recognize that pretraining on these simulated datasets and then fine-tuning on real event-based datasets is an intriguing approach. We plan to delve into this research area in the near future.\n\n[1] Fei-Fei, Li, Rob Fergus, and Pietro Perona. \"Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories.\" In CVPRW 2004.\n\n[2] Krizhevsky, Alex, and Geoffrey Hinton. \"Learning multiple layers of features from tiny images.\" (2009): 7."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3287/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700477727597,
                "cdate": 1700477727597,
                "tmdate": 1700498993199,
                "mdate": 1700498993199,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]