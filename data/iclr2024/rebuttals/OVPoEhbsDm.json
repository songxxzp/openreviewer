[
    {
        "title": "Thermodynamics-inspired Structure Hallucination for Protein-protein Interaction Modeling"
    },
    {
        "review": {
            "id": "HRfa8PvLls",
            "forum": "OVPoEhbsDm",
            "replyto": "OVPoEhbsDm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7154/Reviewer_Criu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7154/Reviewer_Criu"
            ],
            "content": {
                "summary": {
                    "value": "The paper deals with the problem of mutation stability prediction\u2014i.e., given a mutation to a bound complex with known structure, predicting the change in binding free energy. The authors make two contributions (1) they propose _masked mutation modeling_ (MMM) a auxiliary training task where the model must generate a structure for the mutant complex, and this generated (\u201challucinated\u201d) structure is used as additional input to the free energy predictor; (2) they introduce a _probability density cloud_ (PDC) modification to EGNN which is meant to capture the uncertainty in atomic positions. The empirical results slightly improve over RDE, the previous state-of-the-art."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The introduction of the MMM task is quite sensible and well-suited for data-poor tasks such as mutation stability prediction.\n* The \u201cprobability density cloud\u201d modification to EGNN is quite interesting and represents a commendable attempt to introduce physical inductive biases into point cloud networks.\n* The clarity of the exposition is above average for ICLR submissions. Each contribution is well-motivated and contextualized."
                },
                "weaknesses": {
                    "value": "* The PDC module, while a very interesting idea, is on shaky ground mathematically.\n    * The authors assume all atom positions to be independent, which is a very questionable assumption as the thermal fluctuations and epistemic uncertainty of neighboring atoms certainly should be dependent.\n    * It is not possible to write down the self-covariance of the difference of random variables without knowing the cross-covariances. However, even assuming the positions to be independently distributed, then if their mean is updated as in Eq 6, then Eq 7 should read $${\\sigma_{x_i}^{(l+1)}}^2 = \\left[1 + \\frac{1}{|N(i)|}\\sum_{j \\in N(i)} \\phi_\\mu(m_{j \\rightarrow i})\\right]^2{\\sigma_{x_i}^{(l)}}^2 + \\frac{1}{|N(i)|}\\sum_{j \\in N(i)}{\\sigma_{x_j}^{(l)}}^2\\phi_\\mu(m_{j \\rightarrow i})$$ i.e., with $\\phi_\\mu$ instead of a different $\\phi_\\sigma$, and distributing and squaring the $x_i$ terms because $x_i - x_j$ is not independent of $x_i$.\n    * With that said, I don\u2019t think it\u2019s a serious issue in itself if the network is not updating the co-variances properly. But my general concern is that the authors have not given sufficient treatment to these subtleties, and hence, the PDC module is not actually constrained to model the atomic uncertainties as the authors claim; rather, to the network ${\\sigma_{x_i}^{(l+1)}}^2$ just looks like some other latent variable which it can use to help model the positional updates. It would be better to call the module \u201cloosely inspired\u201d by the modeling of uncertainty. If the authors nevertheless claim that this is a generally helpful modification to EGNN, this is a claim that requires significantly more thorough evaluation (on many different tasks ideally) than is given here.\n\n* There are several critical missing details in the methodology and experiments (see Questions below). \n\n* Some over- or mis-claiming throughout the paper\n    * The authors claim (Eq 1) to recover the structural distribution of the masked residues, but the training objective is risk minimization (Eq 3), not any kind of distributional modeling objective. \n    * The authors state that MMM \u201cencourages graph manifold learning with the denoising objective\u201d, but there is no further discussion or elaboration on how \u201cgraph manifold learning\u201d is accomplished.\n    * Repeatedly misleading use of the term \u201cthermodynamics\u201d when the authors mean \u201cuncertainty.\u201d The former term should be reserved when explicitly referring to physically meaningful quantities like energy, entropy, and free energy.\n    * \u201cThe pictures show that particles in the interface have smaller variation compared to those in the edges of proteins.\u201d This claim is not backed quantitatively, and, as discussed, there is no reason to believe that the learned $\\sigma^2$ actually corresponds to uncertainty.\n    * \u201cIt can be found that generally, a small error of wide-type structure reconstruction leads to a more accurate $\\Delta\\Delta G$ prediction.\u201d I see no such correlation in Figure 4B.\n\n\n\nJustification for score: there is the potential for interesting technical contribution in the PDC, but the current presentation is not thorough enough for a conference paper. The MMM objective by itself is less novel, as auxiliary training or pretraining for mutational stability prediction has been done before, and the results are only a bit better than those prior approaches."
                },
                "questions": {
                    "value": "* Methodology details and design are unclear\n    * Where is the PDC module actually used? How are the $\\sigma^2$s initialized?\n    * \u201cMoreover, it is readily apparent that PDC-Net maintains the equivariance property.\u201d The authors should provide a proof here.\n    * Because there are no gradients from the $\\Delta\\Delta G$ task to the structure refinement module $f_\\theta$, the MMM task is really a pretraining task and not an auxiliary training task. Is there any reason to not train MMM across the entire PDB?\n    * Is there any reason to use the same encoding module $h_\\rho$ for the $\\Delta\\Delta G$ predictor and for the structure refinement module $f_\\theta$. Why can\u2019t $f_\\theta$ carry its own encoding module?\n* Experimental details\n    * It is not clear how the baselines are run in order to obtain \\Delta\\Delta G predictions, especially. ESM-1v, B-Factor, etc. While some reasonable guesses exist, the authors should spell it out and not leave it up to guessing.\n    * How many different complexes are in SKEMPI and is the average per-complex improvement statically significant?\n    * In the ablation studies, why does Model 1 use the RDE-Net backbone instead of the Refine-PPI modules $h_\\rho$?\n* Minor issues\n    * Broken link to figure 3.2 where a visualization of the PDCs is promised. \n    * The term \u201cprobability density cloud\u201d suggests a more expressive parameterization than Gaussian uncertainty. I suggest the authors rename the module.\n    * In Table 1, what is the meaning of \u201cpretraining\u201d? How is it possible that Refine-PPI and ESM are classified as no-pretraining, yet B-factors are classified as pretraining?\n    * Inconsistent use of $\\Sigma$ vs $\\sigma$.\n    * Typos: \u201cWide type\u201d, \"paradiagm\", \"disucssion\", \"intergrate\", \"envision\" instead of \"visualize\"\n    * The clarity could be improved with a figure illustrating the coordinate initialization."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7154/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7154/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7154/Reviewer_Criu"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7154/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698156287428,
            "cdate": 1698156287428,
            "tmdate": 1699636847260,
            "mdate": 1699636847260,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iEZz6hWxDV",
                "forum": "OVPoEhbsDm",
                "replyto": "HRfa8PvLls",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7154/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7154/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Criu"
                    },
                    "comment": {
                        "value": "Reviewer Criu, we appreciate your thoughtful and detailed review of our paper on mutation stability prediction. Your constructive feedback has been instrumental in identifying areas for improvement. We acknowledge your positive feedback on the introduction of the MMM task and the PDC modification to EGNN and also appreciate your recognition of the clarity in the exposition of our contributions. Below, please allow us to address each of your comments and concerns in turn.\n\n**Questions**\n\n(1) Methodology Details \n\n-- To elucidate, the backbone module $h_\\rho$ can take the form of any conventional geometric neural network (e.g., GVP-GNN, EGNN, SE(3)-Transformer, Graph-Transformer). Here, we adopt a one-layer Graph Transformer [A, B] to extract general representations of proteins. The refinement model $f_\\theta$ needs to output both updated features and coordinates, and, therefore, **we use the PDC module as $f_\\theta$** in our experiments. Lastly, the head predictor $g_\\tau$ is a simple linear layer that accepts the concatenation of both wide-type and mutation-type representations and forecasts the free energy change. The total model size of Refine-PPI is approximately 6M. \n\n[A] Shan, Sisi, et al. \"Deep learning guided optimization of human antibody against SARS-CoV-2 variants with broad neutralization.\" Proceedings of the National Academy of Sciences 119.11 (2022): e2122954119.\n\n[B] Luo, Shitong, et al. \"Antigen-specific antibody design and optimization with diffusion-based generative models for protein structures.\" Advances in Neural Information Processing Systems 35 (2022): 9754-9767.\n\nSpecifically, we investigate three sorts of initialization mechanisms for $\\sigma$. First and naively, we turn all $\\sigma$ to be equivalent to one (a unit length). Second, we depend on physical principles and utilize molecular dynamic (MD) simulations to attain the short motion trajectories (10 nano-seconds) of these complexes in the 3D space. Then we calculate the root-mean-square fluctuation (RMSF) of each amino acid and take this value as the initial input of $\\sigma$. Third, we adopt a learnable strategy to initialize $\\sigma$. To be explicit, an embedding layer is created for each category of 20 residue types to a 3-dimensional continuous vector. This routine learns the variance of different components completely from the data. \n\nThe performance of different initialization approaches is listed below, and it can be found that constant initialization is the worst. Besides, the MD-based methodology outperforms slightly better results than the embedding-based one. However, since MD simulations are time-consuming and costly, it is prohibited to implement MD during the inference stage each time. As a consequence, we use the third sort of initialization method in our paper. In the revised manuscript, we provide a more detailed explanation of the initialization process in the Appendix. \n|       Method       | Per-Structure | Per-Structure |\n|:------------------:|:-------------:|:-------------:|\n|                    |    Pearson    |    Spearman   |\n|     Unit Length    |     0.4422    |     0.4043    |\n|   MD Simulations   |    **0.4522**   |   **0.4287**  |\n| Learnable Variance |     0.4475    |     0.4102    |\n\n-- Thanks for your advice. We have provided proof in the Appendix. It would be helpful for readers who are interested in the equivariance property of our model. \n\n-- Thanks for your question about the gradient backpropagation. Notably, since the MMM task and the $\\Delta\\Delta G$ task share the same encoder $h_\\rho$, the MMM task here is an auxiliary training task instead of a pretraining task. We have tried to allow gradient backpropagation from the $\\Delta\\Delta G$ task to the structure refinement module $f_\\theta$ but found the training process unstable with slightly worse performance. This phenomenon can be attributed to the small number of available experimental data in the downstream Skempi v2 dataset. It can be expected that if given more ground truth complex structures, the training procedure would be stabilized and the allowance of gradient backpropagation can lead to enhanced results."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7154/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699841612416,
                "cdate": 1699841612416,
                "tmdate": 1699847690810,
                "mdate": 1699847690810,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mLo3693Je0",
                "forum": "OVPoEhbsDm",
                "replyto": "HRfa8PvLls",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7154/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7154/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Criu (Part II)"
                    },
                    "comment": {
                        "value": "Yes! you are absolutely correct that MMM can be trained across the entire PDB rather than the limited 345 wide-type structures. Our preliminary aim is to propose a new framework for **solving the absence of mutant structures with no additional database such as PDB and Alphafold-DB**. Moreover, we have already observed that Refine-PPI outperforms existing algorithms without any pretraining technique. Therefore, we only report the non-pretrained performance in our paper. However, it is undoubtedly expected to have a strong promotion on the capability of our Refine-PPI once it is pretrained on PDB, and please see the updated results below. It can be found that **the per-structure Spearman correlation will significantly increase from 0.41 to 0.44**. Notably, We emphasize the importance of per-structure metrics over overall metrics for practical applications.\n\n|   Method   | Pretrain | Per-Structure | Per-Structure |   Overall  |   Overall  |   Overall  |   Overall  |   Overall  |\n|:----------:|:--------:|:-------------:|:-------------:|:----------:|:----------:|:----------:|:----------:|:----------:|\n|            |          |    Pearson    |    Spearman   |   Pearson  |  Spearman  |    RMSE    |     MAE    |    AUROC   |\n|   RDE-Net  |    Yes   |     0.4448    |     0.4010    |   0.6447   |   0.5584   |   0.5799   |   1.1123   |   0.7454   |\n| Refine-PPI |    No    |     0.4475    |     0.4102    |   0.6584   |   0.5394   | **1.5556** | **1.0946** |   0.7517   |\n| Refine-PPI |    Yes   |   **0.4561**  |   **0.4374**  | **0.6592** | **0.5608** |   1.5643   |   1.1093   | **0.7542** |\n\n-- Thanks for your question regarding the design of our Refine-PPI. To be specific, we adopt the same encoder $h_\\rho$ for both the $\\Delta\\Delta G$ predictor and structure refinement module $f_\\theta$, so that the knowledge learned by $f_\\theta$ in structure prediction can be jointly used by the $\\Delta\\Delta G$ predictor. We hold the view that the capacity to be aware of geometries in mutant regions is of great significance for deep learning models to predict mutant effects, especially when mutant structures are absent. If we make $f_\\theta$ to carry its own encoding module, then the MMM and $\\Delta\\Delta G$ prediction are two completely independent tasks and will not enjoy the benefit of multi-task learning. \n\n(2) Experimental Details \n\n-- Thanks for your suggestions about the baseline implementations. It is absolutely very necessary to spell them out rather than leave it up to guessing. We have formally stated the details in the Appendix about how these baselines are run to obtain $\\Delta\\Delta G$. \n\n-- There are 345 different complexes in Skempi V2, and the average per-complex improvement is statistically significant. Please see Section B in the Appendix, where we envision some case studies in SKMEMPI and select 4 examples for each different number of mutations.\n\n-- We apologize for the confusion in Model 1. In our experiments, the backbone module $h_\\rho$ is exactly the same as the RDE-Net backbone. Thus, we adopt the RDE-Net backbone for Model 1 in the ablation studies for fair comparison.  \n\n(3) Minor Issues\n\n-- We have rectified the broken link to Figure 3.2 by using '\\hyperref'. Thanks for pointing out this issue!\n\n-- We agree that the \u201cprobability density cloud\u201d suggests a more expressive parameterization. Do you think it would be better and properer if we adopt 'Gaussian Uncertainty Network' or 'Uncertainty-aware Neural Network' to name our module? \n\n-- We acknowledge your concern about the meaning of \"pretraining\". We classify baselines and our approach as pretrained or non-pretrained by the criteria of whether they have been trained on or use any additional database such as PDB and Alphafold-DB. Traditionally, \"pretraining\" is usually applied to deep learning models. Since Rosetta and FoldX are energy-based methods, we leave them alone. After more careful consideration, we separate ESM, PSSM, MAS-Transformer, Tranception, B-factor, MIF, and RDE as pretrained methods, while regarding DDGPred and End-to-End as non-pretrained. Notably, B-factor is first pretrained to predict per-atom b-factors for proteins in PDB and then is transferred to this mutant effect prediction task. We have updated the latest classification of those baselines in the revised version. Great thanks for helping and encouraging us to clarify this issue!  \n\n-- We appreciate your clarification on the usage of $\\Sigma$ and $\\sigma$. The main reason is that $\\boldsymbol{\\Sigma}_i\\in \\mathbb{R}^{3\\times 3}$ is a diagonal covariance matrix indicating that different axes are independent of each other. Consequently, we utilize $\\boldsymbol{\\sigma}$ to denote the diagonal elements of the variance matrix $\\mathbf{\\Sigma}$ for simplicity. we will incorporate this explanation into our revised manuscript."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7154/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699841742358,
                "cdate": 1699841742358,
                "tmdate": 1699841806829,
                "mdate": 1699841806829,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zd12so11uS",
                "forum": "OVPoEhbsDm",
                "replyto": "HRfa8PvLls",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7154/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7154/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Criu (Part III)"
                    },
                    "comment": {
                        "value": "-- Thanks for your advice for improving the clarity. We have added a figure in the Appendix to illustrate the coordinate initialization. We believe with this new figure, the audience can understand the MMM task more easily. \n\n**The design of PDC-Net**\n\n-- We appreciate your diligence in scrutinizing the mathematical foundation of our PDC module and also acknowledge that the thermal fluctuations and epistemic uncertainty of neighboring atoms certainly should be dependent. However, if we assume that the positions of particles are highly correlated to their neighbors, the mathematical analysis of their uncertainty becomes intricate. Just as you mentioned, we first need to hypothesize the cross-covariance of each pair of entities in the molecular system so that the self-covariance of each atom can be calculated. A major challenge lies in computing geometric variables like distances and angles. For instance, once $\\mathbf{x}_i$ and $\\mathbf{x}_j$ \n\nfollow dependent Gaussian distributions, the squared norm of their difference $d_{ij}$  will deviate from a generalized chi-square distribution $\\chi^2(.)$. Due to our mathematical limitations, explicitly defining a set of natural parameters to model the exact distribution of $d_{ij}$ is not feasible. Additionally, portraying higher-order geometries, such as the angle between vectors $\\mathbf{x}_i - \\mathbf{x}_j$ and $\\mathbf{x}_i - \\mathbf{x}_k$, becomes more challenging. We really understand and align with your perspective that close atoms have a significant impact on each other in the 3D space, and plan to address this in future work by introducing a more nuanced architecture. It is undeniable that separating individual particles into isolated elements provides great convenience for us to model complex molecular systems. More importantly, the interdependency between close entities will be captured via the following message-passing-based mechanism when training geometric networks. \n\n-- Thanks for your advice on the update of coordinate variance! It is rational to use the same operation $\\phi_\\mu$ for renewing both mean and variance rather than employing separate operations (i.e., $\\phi_\\mu$ and $\\phi_\\sigma$). In addition, we concur with your suggestion to square the $\\mathbf{x}_i$ terms since the vector $\\mathbf{x}_i - \\mathbf{x}_j$ is dependent on $\\mathbf{x}_i$. Experiments show that this form of position variance computation performs slightly better in the mutant effect prediction task (without pretraining on PDB). We have included this new Equation in the Appendix of our revised manuscript and would like to express our gratitude for your insight on PDC-EGNN. We believe the introduction of positional uncertainty is a promising direction, worthy of further exploration and the proposal of advanced architectures.  \n|  Method  | Per-Structure | Per-Structure |\n|:--------:|:-------------:|:-------------:|\n|          |    Pearson    |    Spearman   |\n|  Equ. 7  |     0.4475    |     0.4102    |\n| New Equ. |     **0.4490**    |    **0.4153**    |\n\n-- We agree with your assertion that modeling atomic uncertainty is crucial, prompting us to consider a more fitting name for our PDC module. As suggested, we are contemplating renaming it as the **\"Uncertainty-aware Neural Network\"** or **\"Gaussian Uncertainty Network.\"** Do you think this modification is better? We are open to your thoughts on whether this modification is preferable or if an \"Uncertainty-inspired Neural Network,\" as mentioned in your comment, would be a more suitable choice.\n\n**Over- or Mis-claiming throughout the Paper**\n\n-- Thanks for your question regarding the structural distribution recovery. You are right that our aim is to recover the structural distribution of the masked residues, namely, $p(\\{\\mathbf{x}^{\\textrm{WT}}\\}^{m+r}_{i=m-l} \\big| \\tilde{\\mathcal{G}}^{\\textrm{WT}}, a_m, \\theta, \\rho )$. \n\nIn order to supervise the MMM task, the exact training object would be measuring and minimizing the gap between the predicted structural distribution $Q$ and the ground truth structural distribution $P$. A widely used measure is the KL-divergence, writtern as $D_{KL}(P||Q) = \\mathbb{E}_{x\\sim P}\\big[log \\frac{P(X)}{Q(X)}\\big]$. Then if our goal is to learn a model $f:\\mathcal{X} \\rightarrow \\mathcal{Y}$, our objective becomes \n\n${\\textrm{argmin}}_ \\theta  \\mathbb{E}_ {x,y\\sim\\mathcal{D}} [-logQ_ \\theta (y|x)].$\n\nFor classification problems, the cross-entropy loss is exactly what KL divergence minimizes. On the contrary, for regression task, minimizing the negative log likelihood (NLL) of this normal distribution is clearly equivalent to the mean-squared-error loss, namely, Equ.3 in our paper. For more information, please refer to this wonderful blog: https://dibyaghosh.com/blog/probability/kldivergence.html."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7154/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699847670194,
                "cdate": 1699847670194,
                "tmdate": 1699858109159,
                "mdate": 1699858109159,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZMdy1W5H95",
                "forum": "OVPoEhbsDm",
                "replyto": "HRfa8PvLls",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7154/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7154/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Criu (Part IV)"
                    },
                    "comment": {
                        "value": "-- We appreciate your clarification on the use of terminology. Please allow us to address the point you raised about the claims related to the graph manifold learning. To be specific,  By using an implicit mapping from corrupted data to clean data, the MMM objective encourages the model to learn the manifold on which the clean data lies \u2014 we speculate that the deep learning model learns to go from low probability graphs to high probability graphs. In the denoising-based initialization case, Refine-PPI learns the manifold of the input data. When node targets are provided, the model learns the manifold of the target data (e.g. the manifold of atoms at equilibrium). We speculate that such a manifold may include commonly repeated substructures that are useful for downstream prediction tasks. A similar motivation can be found in prior denoising-based molecular pretraining methods [A, B, C].\n\n[A] Godwin, Jonathan, et al. \"Simple gnn regularisation for 3d molecular property prediction & beyond.\" ICLR 2022.\n\n[B] Vincent, Pascal, et al. \"Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion.\" Journal of machine learning research 11.12 (2010).\n\n[C] Song, Yang, and Stefano Ermon. \"Generative modeling by estimating gradients of the data distribution.\" Advances in neural information processing systems 32 (2019).\n\n--  We appreciate your guidance on the distinction between \"thermodynamics\" and \"uncertainty.\" We will rectify the repeated misuse of the term \"thermodynamics\" in situations where \"uncertainty\" is more appropriate. This correction will ensure a more accurate and precise representation of our concepts.\n\n-- -- We acknowledge your concern about the need for quantitative support for certain claims, particularly in relation to the variation of particles at the protein interface. To address this, we conducted short trajectories of all structures in Skempi V2, calculating the Root Mean Square Fluctuation (RMSF) of each residue. Additionally, we determined the magnitude of $||\\boldsymbol{\\sigma}_{\\mathbf{x}_i}||^2$, which is the learned variance of our uncertainty module. A detailed comparison of these values, categorized by residues at and not at the interface, is presented in the table below. Notably, the ground truth RMSF at the interface is significantly smaller than that observed elsewhere. \n\nConcurrently, the learned  $\\boldsymbol{\\sigma}$ exhibits a parallel pattern, where $||\\boldsymbol{\\sigma}_{\\mathbf{x}_i}||^2$ at the interface is much smaller. This quantitative analysis serves to substantiate the claim that the learned variance indeed corresponds to uncertainty. We appreciate your suggestion to quantify the uncertainty, as it has enhanced the rigor of our analysis!\n\n|     | Interface | Non-Interface |\n|:-----------------------------:|:---------:|:-------------:|\n|     RMSF in MD Simulations    |   0.4945  |     0.9735    |\n| Magtitude of Learned Variance |   0.6072  |     0.8940    |\n\n-- We acknowledge the discrepancy you pointed out regarding the statement on a correlation between a small error in wide-type structure reconstruction and a more accurate prediction in Figure 4B. Upon further examination, we realize the oversight in the interpretation. We will detele the corresponding text for clarity and accuracy.\n\n-------------------------\n\nAt last, we sincerely appreciate your commitment to maintaining the scientific rigor of our work, and your feedback will undoubtedly contribute to enhancing the quality and clarity of our manuscript. If you have any further comments or suggestions, please feel free to share them. Thank you for your time and consideration!"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7154/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699859209044,
                "cdate": 1699859209044,
                "tmdate": 1699859209044,
                "mdate": 1699859209044,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ojjx3FFsaE",
                "forum": "OVPoEhbsDm",
                "replyto": "HRfa8PvLls",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7154/Reviewer_Criu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7154/Reviewer_Criu"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response. I appreciate the diligent resolution of many miscellaneous concerns. I also appreciate that the authors have tried out the suggested change to Eq 7. However:\n\n(1) I think the paper still lacks a clean and satisfactory conceptual presentation of the PDC module. I emphasize that I really like this direction of thinking, but it seems that the design and conceptual framework of such a module has not been carefully thought through and the current presentation raises more questions / confusion than it inspires answers.\n\n(2) I disagree with the claim that RMSE is a distributional modeling objective, because this is true only if the distributional model is Gaussian. It's actually not clear to me why it's necessary to make this claim, because it doesn't seem to affect the rest of the paper.\n\n(3) While there are cherry-picked examples to illustrate that the learned \"variance\" semantically corresponds to positional uncertainty, there is still, at the end of the day, no conceptual justification for believing that this is true systematically. Do the authors claim that any latent variable which is additive according to Eq 7 must be some kind of uncertainty? Note that while a loss term is applied directly on the learned positions, there is no loss term to enforce any kind of semantic meaning to the learned variances.\n\nFor these reasons, I intend to keep the current score."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7154/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700057988975,
                "cdate": 1700057988975,
                "tmdate": 1700058009793,
                "mdate": 1700058009793,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SwP8GwUOZh",
                "forum": "OVPoEhbsDm",
                "replyto": "HRfa8PvLls",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7154/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7154/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Official Comment of Reviewer Criu (Part II)"
                    },
                    "comment": {
                        "value": "Second, since we have the simulated uncertainty, it is feasible for us to compute its similarity to learned variance. Specifically, we compare the magnitude of $||\\boldsymbol{\\sigma}_{\\mathbf{x}_i}||^2$ with this simulated uncertainty, categorized by residues at and not at the interface. It can be found that both the simulated uncertainty and learned variance at the interface are significantly smaller than that observed elsewhere (presented in the Table below). This provides further evidence that our learned variance accords with a similar pattern of simulated uncertainty.\n\n|     | Interface | Non-Interface |\n|:-----------------------------:|:---------:|:-------------:|\n|     Simulated Uncertainty (RMSF)   |   0.4945  |     0.9735    |\n| Magtitude of $\\boldsymbol{\\sigma} _{\\mathbf{x} _i} $ |   0.6072  |     0.8940    |\n\nThird, we implement additional experiments to directly predict the simulated uncertainty and examine its effectiveness. On the one hand, we adopt an EGNN with the PDC module and directly enforce the learnable variance to accord with the simulated uncertainty. The loss term is therefore set as $MSE(||\\boldsymbol{\\sigma}_{\\mathbf{x}_i}||^2, RMSF)$.  On the other hand, we leverage a naive EGNN without the PDC module and require it to output RMSF based on the residue feature of the final layer. The loss is written as $MSE(MLP(\\mathbf{h}^{(L)}), RMSF)$, where MLP is the abbreviation of the multi-layer perceptron. The experiments show that the PDC module significantly improves the capability of EGNN to forecast the simulated uncertainty. This phenomenon illustrates that our design of $\\Sigma$ can be a good choice to represent and encode atomic uncertainty in the 3D space. \n\n|  Model   | MSE | \n|:-----------------------------:|:---------:|\n|     EGNN   |   0.2609  |   \n| PDC-EGNN |   0.0381  |   \n\n[A] RMSF Analysis. https://ctlee.github.io/BioChemCoRe-2018/rmsd-rmsf/#:~:text=RMSF%20stands%20for%20root%20mean,(fluctuates)%20during%20a%20simulation.\n\n[B] Wiki. https://en.wikipedia.org/wiki/Root-mean-square_deviation_of_atomic_positions \n\nTo summarize, based on these three facts, we have the confidence to believe that $\\Sigma$ does have some semantic meaning. While our efforts are not perfect, we hope our proposal for this PDC module marks a crucial step in parameterizing the uncertainty of particles' spatial states. We welcome suggestions to further conceptualize the connection between learned variance and atomic uncertainty.\n\n------------------------------\n\nWe genuinely thank your quick feedback and value the opportunity to discuss this with you. If you have any further suggestions or specific areas you would like us to focus on during revision, please feel free to let us know."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7154/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700145762111,
                "cdate": 1700145762111,
                "tmdate": 1700145825540,
                "mdate": 1700145825540,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "eoZnqtuTc8",
            "forum": "OVPoEhbsDm",
            "replyto": "OVPoEhbsDm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7154/Reviewer_xfqG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7154/Reviewer_xfqG"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel deep learning architecture, Refine-PPI, for protein-protein binding mutation effect (DDG) prediction. Refine-PPI consists of two modules. The first module learns to predict the mutated structure through a masked mutation modeling task on wild-type structures. The second module learns to predict DDG of a protein-protein complex based on wild-type and mutated structures. The second module represents a protein-protein complex as a probabilistic density cloud (PDC) and encodes it using a novel PDC-GNN, where the messages are represented by its mean and variance. Refine-PPI achieves state-of-the-art performance on the standard SKEMPI benchmark."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* This paper models a protein as a dynamic structure, using a probabilistic density cloud representation.\n* This paper develops a new message passing network architecture for probabilistic density clouds. The messages between each node consists of both mean and variance.\n* The evaluation setup is comprehensive, with all the relevant baselines"
                },
                "weaknesses": {
                    "value": "* The model only slightly outperforms previous state-of-the-art RDE-Net on a subset of metrics. It seems that overall performance of Refine-PPI and RDE-Net is similar.\n* The evaluation of Refine-PPI on the first mutation structure prediction task is missing."
                },
                "questions": {
                    "value": "* The description of probabilistic density cloud representation is a bit unclear. In particular, how are the $sigma$'s initialized? If they are initialized as zero, then they will stay zero all the time. Are they initialized by some physical calculations?\n* For the first task of mutation structure prediction task, can you report the RMSD between predicted mutated structure and ground truth?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7154/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698782291519,
            "cdate": 1698782291519,
            "tmdate": 1699636847115,
            "mdate": 1699636847115,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CbVjYSW3PD",
                "forum": "OVPoEhbsDm",
                "replyto": "eoZnqtuTc8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7154/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7154/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reponse to Reviewer xfqG"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your time and effort in reviewing our paper, and we are delighted that you identified the strengths of our approach, particularly the dynamic modeling of proteins and the development of a novel message-passing network architecture for probabilistic density clouds. Your insightful feedback has been invaluable, and we would like to address the points raised in your review as follows.\n\n(1) We appreciate your honesty in highlighting the weaknesses of our work and understand your concern regarding the marginal improvement over the previous state-of-the-art model, RDE-Net. However, it is worth noting that RDE is pretrained on the additional protein structure dataset, PDB-REDO, which contains more than 130K refined X-ray structures in Protein Data Bank. Meanwhile, our Refine-PPI adopts no pretraining strategy and is directly trained on Skempi v2 with only 345 wide-type structures. Refine-PPI enjoys no benefits of unsupervised pretraining but achieves competitive or even better performance than existing algorithms, underscoring the superiority of our architecture design. As mentioned in Appendix C, several prior studies [A, B, C] have demonstrated that structural pretraining is beneficial to dramatically expand the representation space of deep learning models, and it is promising to pretrain Refine-PPI with more experimental protein structures and transfer the knowledge to this mutation effect prediction task. If we leverage the same PDB-REDO for pretraining, the per-structure Spearman correlation will significantly increase from 0.41 to 0.44 (see Table below for a clear comparison). \n\nMoreover, as declared in both our paper and RDE-Net, the correlation for one specific protein complex is often of greater interest and importance in practical applications. It is preferred to attach more attention to the average per-structure Pearson and Spearman correlation coefficients rather than the overall metrics. \n\n|   Method   | Pretrain | Per-Structure | Per-Structure |   Overall  |   Overall  |   Overall  |   Overall  |   Overall  |\n|:----------:|:--------:|:-------------:|:-------------:|:----------:|:----------:|:----------:|:----------:|:----------:|\n|            |          |    Pearson    |    Spearman   |   Pearson  |  Spearman  |    RMSE    |     MAE    |    AUROC   |\n|   RDE-Net  |    Yes   |     0.4448    |     0.4010    |   0.6447   |   0.5584   |   0.5799   |   1.1123   |   0.7454   |\n| Refine-PPI |    No    |     0.4475    |     0.4102    |   0.6584   |   0.5394   | **1.5556** | **1.0946** |   0.7517   |\n| Refine-PPI |    Yes   |   **0.4561**  |   **0.4374**  | **0.6592** | **0.5608** |   1.5643   |   1.1093   | **0.7542** |\n\n\n[A] Zhang, Zuobai, et al. \"Protein representation learning by geometric structure pretraining.\" arXiv preprint arXiv:2203.06125 (2022).\n\n[B] Chen, Can, et al. \"Structure-aware protein self-supervised learning.\" Bioinformatics 39.4 (2023): btad189.\n\n[C] Wu, Fang, et al. \"Pre\u2010Training of Equivariant Graph Matching Networks with Conformation Flexibility for Drug Binding.\" Advanced Science 9.33 (2022): 2203796.\n\n(2) For the first task of mutation structure prediction, we apologize for not reporting the RMSD between the predicted mutated structure and the ground truth. This information is, indeed, very crucial. However, it is worth mentioning that obtaining the structures of mutant proteins is a persistent challenge, as they are often elusive to acquire. All mutant structures in Skempi V2 are not available and we only have 345 wide-type structures. Therefore, we are afraid that we are unable to report the exact structure prediction error between the predicted mutated structure and ground truth. Notably, the inaccessibility of mutant structures is the core motivation for our structure refinement module, which is first trained by a mask mutation modeling (MMM) task on available wide-type structures and then transferred to hallucinate the inaccessible mutant protein structures. During the MMM training phase, the RMSD between predicted wide-type structures and ground truth is 1.437, which will increase if we enlarge the length of the masked region. We hope this statistic can provide insight into the modeling accuracy and partially answer your question."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7154/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699672130759,
                "cdate": 1699672130759,
                "tmdate": 1699672130759,
                "mdate": 1699672130759,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BQJ8AvQ5uc",
                "forum": "OVPoEhbsDm",
                "replyto": "eoZnqtuTc8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7154/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7154/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reponse to Reviewer xfqG (Part II)"
                    },
                    "comment": {
                        "value": "(3) Regarding the initialization of probabilistic density cloud representation, we agree that our description needs clarification. Specifically, we investigate three sorts of initialization mechanisms for $\\sigma$. First and naively, we turn all $\\sigma$ to be equivalent to one (a unit length). Second, we depend on physical principles and utilize molecular dynamic (MD) simulations to attain the short motion trajectories (10 nano-seconds) of these complexes in the 3D space. Then we calculate the root-mean-square fluctuation (RMSF) of each amino acid and take this value as the initial input of $\\sigma$. Third, we adopt a learnable strategy to initialize $\\sigma$. To be explicit, an embedding layer is created for each category of 20 residue types to a 3-dimensional continuous vector. This routine learns the variance of different components completely from the data. \n\nThe performance of different initialization approaches is listed below, and it can be found that constant initialization is the worst. Besides, the MD-based methodology outperforms slightly better results than the embedding-based one. However, since MD simulations are time-consuming and costly, it is prohibited to implement MD during the inference stage each time. As a consequence, we use the third sort in our paper. In the revised manuscript, we will provide a more detailed explanation of the initialization process (e.g., in the Appendix). \n\n|       Method       | Per-Structure | Per-Structure |\n|:------------------:|:-------------:|:-------------:|\n|                    |    Pearson    |    Spearman   |\n|     Unit Length    |     0.4422    |     0.4043    |\n|   MD Simulations   |    **0.4522**    |   **0.4287**    |\n| Learnable Variance |     0.4475    |     0.4102    |\n--------------------\nOnce again, we appreciate your thorough review and constructive feedback. Your insights will undoubtedly contribute to the refinement and improvement of our work. We will submit a revised version that addresses the mentioned concerns and provides a clearer understanding of our contributions."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7154/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699672164751,
                "cdate": 1699672164751,
                "tmdate": 1699672243331,
                "mdate": 1699672243331,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BPr05phZzP",
                "forum": "OVPoEhbsDm",
                "replyto": "eoZnqtuTc8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7154/Reviewer_xfqG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7154/Reviewer_xfqG"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response"
                    },
                    "comment": {
                        "value": "Based on other reviewer's comments, I think it's important to report the standard deviation of your method (e.g. 5-fold cross validation) and see how significant your improvement is. Also, it would be good to compare your side-chain reconstruction (MMM task) with existing methods (e.g., FoldX or RDENet side-chain packing) on the wildtypes. I will keep my original score nevertheless."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7154/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700597348066,
                "cdate": 1700597348066,
                "tmdate": 1700597453223,
                "mdate": 1700597453223,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZD7J0i1uOV",
            "forum": "OVPoEhbsDm",
            "replyto": "OVPoEhbsDm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7154/Reviewer_cuXz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7154/Reviewer_cuXz"
            ],
            "content": {
                "summary": {
                    "value": "A framework, Refine-PPI, of predicting $\\Delta\\Delta G$ is proposed in this paper, which include 3 components: a structure encoder $h_\\rho$, a structure refiner $f_\\theta$, and a readout (pooling) function $g_\\tau$.\n\nNew backbone coined as PDC-Net is proposed to model structures.\n\nExperimental results show marginal improvements on $\\Delta\\Delta G$ prediction."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The general framework of Refine-PPI is interesting.\n\nThe $\\Delta\\Delta G$ prediction quality is seemingly equivalent / marginally improved."
                },
                "weaknesses": {
                    "value": "1. A lot of missing details hinder the reproducibility of the paper. See Questions.\n\n2. Overall the paper introduce a new framework and a new architecture. Although a general ablation is done, the paper very much lacks deep analysis to each components.\n\n3. Weak benchmark performance: the elevation of performance is too marginal (especially those in the Appendix), and the time complexity is not studied. No variance is reported.\n\n4. The usage of term \"thermodynamics\" and \"hallucination\" is hardly relevant to the proposed method and thus confusing. I would suggest the authors to use plainer descriptions."
                },
                "questions": {
                    "value": "Q1 How are $h_\\rho, f_\\theta, g_\\tau$ built precisely? What are the $\\phi$ functions in Eq 5-7?\n\nQ2 How are $\\sigma_{x_i}$s modeled? And how are those initialized? In Eq 4 they are matrices while in Eq 7 they seem to be vectors. In my opinion 3d variance should either be a scalar or a learnable SPD matrix. Using vectors does not satisfy SE(3) invariance because an ellipsoid with standard axis is presumed, and the results are thus varied when rotations are applied to the input structure. Thus the method is not \"readily apparent\" to be equivalent.\n\nQ3 There lacks a visualization of the \"hallucinated\" structures.\n\nQ4 On what data, precisely, is Refine-PPI trained? The description seems to point to a 3-fold cross validation, but the concrete splits should be specified. And, since all benchmark performances are \"directly copied\" from a preprint, the authors must justify that their evaluation scheme is exactly the same to all benchmarks."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7154/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699001713554,
            "cdate": 1699001713554,
            "tmdate": 1699636846995,
            "mdate": 1699636846995,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yCIXcV0H04",
                "forum": "OVPoEhbsDm",
                "replyto": "ZD7J0i1uOV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7154/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7154/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer cuXz"
                    },
                    "comment": {
                        "value": "Dear Reviewer cuXz, we sincerely appreciate your comprehensive comment on our paper. Your thoughtful comments are invaluable, and we are committed to addressing the concerns raised in your review. Below, we provide detailed responses to each point:\n\n(1) First and foremost, we apologize for any confusion. To elucidate, the backbone module $h_\\rho$ can take the form of any conventional geometric neural networks (e.g., GVP-GNN, EGNN, SE(3)-Transformer, Graph-Transformer). Here, we adopt a one-layer Graph Transformer [A,B] to extract general representations of proteins. The refinement model $f_\\theta$ needs to output both updated features and coordinates, and, therefore, we use PDC-EGNN as $f_\\theta$ in our experiments. Lastly, the head predictor $g_\\tau$ is a simple linear layer that accepts the concatenation of both wide-type and mutation-type representations and forecasts the free energy change. The total model size of Refine-PPI is approximately 6M. In addition, $\\phi_e, \\phi_h, \\phi_{\\mu}, \\phi_\\sigma$ are the edge, node, mean, and variance operations respectively that are commonly approximated by Multilayer Perceptrons (MLPs). We will provide explicit details on the construction of these notations to enhance understanding in our revised paper. Thanks for your question to improve the clarity. \n\n[A] Shan, Sisi, et al. \"Deep learning guided optimization of human antibody against SARS-CoV-2 variants with broad neutralization.\" Proceedings of the National Academy of Sciences 119.11 (2022): e2122954119.\n\n[B] Luo, Shitong, et al. \"Antigen-specific antibody design and optimization with diffusion-based generative models for protein structures.\" Advances in Neural Information Processing Systems 35 (2022): 9754-9767.\n\n(2) We recognize your concern about the usage of terms like \"thermodynamics\" and \"hallucination.\" Please allow us to explain our motivations for using these words. \n\ni) Thermodynamics [A] is a branch of physics that deals with heat, work, and temperature, and their relation to energy, entropy, and the physical properties of matter and radiation. It applies to a wide variety of topics in science and engineering, especially physical chemistry, and biochemistry. In our scenario, biologically relevant macromolecules, such as proteins, do not operate as static, isolated entities. On the contrary, they are involved in numerous interactions with other species, such as proteins, nucleic acid, membranes, small molecule ligands, and also, critically, solvent molecules. Like any other spontaneous process, binding occurs only when it is associated with a negative Gibbs' free energy of binding ($\\Delta G$), which may have differing thermodynamic signatures, varying from enthalpy- to entropy-driven. Thus, the understanding of the forces driving the recognition and interaction require a detailed description of the binding thermodynamics, and a correlation of the thermodynamic parameters with the structures of interacting partners. Such an understanding of the nature of the recognition phenomena is of a great importance for medicinal chemistry research, since it enables truly rational structure-based molecular design. A great number of studies have explored the contribution of protein dynamics to the binding thermodynamics and kenetics of drug-like compounds [B, C, D, E, F]. Here, we adopt this term to express the need for accurately modeling biomoecules' inherent dynamics.  \n\n[A] Wikipedia. https://en.wikipedia.org/wiki/Thermodynamics\n\n[B] K., A. (2011). Thermodynamics of Ligand-Protein Interactions: Implications for Molecular Design. InTech. doi: 10.5772/19447 \n\n[C] Khatri, K.S., Modi, P., Sharma, S., Deep, S. (2020). Thermodynamics of Protein-Ligand Binding. In: Singh, D., Tripathi, T. (eds) Frontiers in Protein Structure, Function, and Dynamics. Springer, Singapore. https://doi.org/10.1007/978-981-15-5530-5_7\n\n[D] Olsson, Tjelvar SG, et al. \"The thermodynamics of protein\u2013ligand interaction and solvation: insights for ligand design.\" Journal of molecular biology 384.4 (2008): 1002-1017.\n\n[E] Amaral, Marta, et al. \"Protein conformational flexibility modulates kinetics and thermodynamics of drug binding.\" Nature communications 8.1 (2017): 2276.\n\n[F] Zheng, Li-E., et al. \"Machine Learning Generation of Dynamic Protein Conformational Ensembles.\" Molecules 28.10 (2023): 4047."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7154/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699767200552,
                "cdate": 1699767200552,
                "tmdate": 1699788385289,
                "mdate": 1699788385289,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6MMUFnk28a",
                "forum": "OVPoEhbsDm",
                "replyto": "ZD7J0i1uOV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7154/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7154/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer cuXz (Part II)"
                    },
                    "comment": {
                        "value": "ii) Previous studies exemplified by Google\u2019s DeepDream train networks to recognize faces and other patterns in images, and invert and adjust arbitrary input images to draw more strongly resemble faces or other patterns as perceived by the network. The generated images are often referred to as **hallucinations** because they may not faithfully represent any actual face, but what DL models view as an ideal face. Remarkably, this mechanism has also demonstrated success in the context of macromolecules. It has been shown that the information stored in the many parameters of the trained networks can be harnessed to design new protein structures featuring new sequences. A lot of recent works [A, B, C, D] published in top journals have used deep network hallucination to generate a wide range of functional proteins. In our algorithm, we take a similar methodology and explore whether networks trained on existing wide-type structures could be inverted to generate brand new \u201cideal\u201d protein structures according to the mutant information.\n\n[A] Anishchenko, ... & Baker, D. (2021). De novo protein design by deep network hallucination. Nature, 600(7889), 547-552.\n\n[B] Wicky, ... & Baker, D. (2022). Hallucinating symmetric protein assemblies. Science, 378(6615), 56-61.\n\n[C] An, Linna, et al. \"Hallucination of closed repeat proteins containing central pockets.\" Nature Structural & Molecular Biology (2023): 1-6.\n\n[D] Costello, Zak, and Hector Garcia Martin. \"How to hallucinate functional proteins.\" arXiv preprint arXiv:1903.00458 (2019).\n\n-------------------------------\nHowever, while we find these terms conceptually appropriate, we fully understand that these words can be confusing for some readers. We would like to replace them with more precise and relevant descriptors for improved clarity. Do you have any suggestions or recommendations for the title? We are very pleased to modify it with your opinion. \n\n\n(3) We appreciate your honesty in highlighting the weaknesses of our work and understand your concern regarding the marginal improvement over the previous state-of-the-art model, which has also been mentioned by Reviewer xfqG. However, it is worth noting that RDE is pretrained on the additional protein structure dataset, PDB-REDO, which contains more than 130K refined X-ray structures in Protein Data Bank. Meanwhile, our Refine-PPI adopts no pretraining strategy and is directly trained on Skempi v2 with only 345 wide-type structures. Refine-PPI enjoys no benefits of unsupervised pretraining but achieves competitive or even better performance than existing algorithms, underscoring the superiority of our architecture design. As mentioned in Appendix C, several prior studies [A, B, C] have demonstrated that structural pretraining is beneficial to dramatically expand the representation space of deep learning models, and it is promising to pretrain Refine-PPI with more experimental protein structures and transfer the knowledge to this mutation effect prediction task. If we leverage the same PDB-REDO for pretraining, **the per-structure Spearman correlation will significantly increase from 0.41 to 0.44** (see Table below for clear comparison). Notably, We emphasize the importance of per-structure metrics over overall metrics for practical applications.\n\n|   Method   | Pretrain | Per-Structure | Per-Structure |   Overall  |   Overall  |   Overall  |   Overall  |   Overall  |\n|:----------:|:--------:|:-------------:|:-------------:|:----------:|:----------:|:----------:|:----------:|:----------:|\n|            |          |    Pearson    |    Spearman   |   Pearson  |  Spearman  |    RMSE    |     MAE    |    AUROC   |\n|   RDE-Net  |    Yes   |     0.4448    |     0.4010    |   0.6447   |   0.5584   |   0.5799   |   1.1123   |   0.7454   |\n| Refine-PPI |    No    |     0.4475    |     0.4102    |   0.6584   |   0.5394   | **1.5556** | **1.0946** |   0.7517   |\n| Refine-PPI |    Yes   |   **0.4561**  |   **0.4374**  | **0.6592** | **0.5608** |   1.5643   |   1.1093   | **0.7542** |\n\nBesides, as for the time complexity of Refine-PPI, we empirically verify that it has little difference from traditional deep learning-based methodologies. To be explicit, it spends RDE and Refine-PPI about 60 and 66 seconds, separately, to perform 3-fold inference predictions of all 6.7K entries in Skempi V2. The generation process of mutant structures is very fast, making it feasible for real-world applications\n\n|   Method   | Inference Time | \n|:----------:|:--------:|\n|   RDE-Net  | 60 s | \n| Refine-PPI | 66 s | \n\n[A] Zhang, Zuobai, et al. \"Protein representation learning by geometric structure pretraining.\" ICLR 2023.\n\n[B] Chen, Can, et al. \"Structure-aware protein self-supervised learning.\" Bioinformatics 39.4 (2023): btad189.\n\n[C] Wu, Fang, et al. \"Pre\u2010Training of Equivariant Graph Matching Networks with Conformation Flexibility for Drug Binding.\" Advanced Science 9.33 (2022): 2203796."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7154/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699767314754,
                "cdate": 1699767314754,
                "tmdate": 1699867305500,
                "mdate": 1699867305500,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MILwhbp5wI",
                "forum": "OVPoEhbsDm",
                "replyto": "ZD7J0i1uOV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7154/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7154/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer cuXz (Part III)"
                    },
                    "comment": {
                        "value": "(4)  We appreciate your clarification on the use of $\\sigma$ and how to initialize them in our probability density cloud representation. \ni) You are correct that $\\sigma$ are matrices in Equ. 4 while are vectors in Equ. 7. The main reason is that $\\boldsymbol{\\Sigma}_i\\in \\mathbb{R}^{3\\times 3}$ is a diagonal covariance matrix indicating that different axes are independent of each other. Consequently, we utilize $\\boldsymbol{\\sigma}$ to denote the diagonal elements of the variance matrix $\\mathbf{\\Sigma}$ for simplicity.  we will incorporate this into our revised manuscript.\n\nii) Specifically, we investigate three sorts of initialization mechanisms for $\\sigma$. First and naively, we turn all $\\sigma$ to be equivalent to one (a unit length). Second, we depend on physical principles and utilize molecular dynamic (MD) simulations to attain the short motion trajectories (10 nano-seconds) of these complexes in the 3D space. Then we calculate the root-mean-square fluctuation (RMSF) of each amino acid and take this value as the initial input of $\\sigma$. Third, we adopt a learnable strategy to initialize $\\sigma$. To be explicit, an embedding layer is created for each category of 20 residue types to a 3-dimensional continuous vector. This routine learns the variance of different components completely from the data. \n\nThe performance of different initialization approaches is listed below, and it can be found that constant initialization is the worst. Besides, the MD-based methodology outperforms slightly better results than the embedding-based one. However, since MD simulations are time-consuming and costly, it is prohibited to implement MD during the inference stage each time. As a consequence, we use the third sort in our paper. In the revised manuscript, we will provide a more detailed explanation of the initialization process (e.g., in the Appendix). \n\n--------------------\n|       Method       | Per-Structure | Per-Structure |\n|:------------------:|:-------------:|:-------------:|\n|                    |    Pearson    |    Spearman   |\n|     Unit Length    |     0.4422    |     0.4043    |\n|   MD Simulations   |     0.4522    |     0.4287    |\n| Learnable Variance |     0.4475    |     0.4102    |\n\n(5) We acknowledge the absence of visualization for \"hallucinated\" structures. In the revision, we will include visualizations (see Appendix) to provide a clearer understanding of the proposed method.\n\n(6) We apologize for any confusion regarding the training data and evaluation scheme. In our evaluation, we adopt the same 3-fold cross-validation split as RDE-Net. To be specific, the numbers of training and validation samples for fold-0, fold-1, and fold-2 are 4777/1929, 4290/2416, and 4345/2361. We confirm that our splitting strategy completely accords with all benchmarks reported in the RDE paper. Besides that, we would like to clarify that RDE was officially accepted by last year's ICLR [A] and it is our fault to use its preprint version as the reference. \n\n[A] Rotamer Density Estimator is an Unsupervised Learner of the Effect of Mutations on Protein-Protein Interaction. ICLR 2023. https://openreview.net/forum?id=_X9Yl1K2mD \n\n---------------------------------------------------\nWe would like to extend our sincere appreciation for the meticulous review you have conducted and the valuable insights you have shared. Your constructive feedback is highly valuable to us, and we are fully committed to preparing a revised version that meticulously addresses all the raised concerns. We aim to enhance the clarity of our contributions and ensure that the revised manuscript aligns seamlessly with your feedback."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7154/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699767411530,
                "cdate": 1699767411530,
                "tmdate": 1699959630077,
                "mdate": 1699959630077,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ShkTpTk86k",
                "forum": "OVPoEhbsDm",
                "replyto": "MILwhbp5wI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7154/Reviewer_cuXz"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7154/Reviewer_cuXz"
                ],
                "content": {
                    "title": {
                        "value": "reply to rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for the detailed rebuttal contents. It seems that implementation details are enlarged (I think since we are in ICLR the authors are free to revise their paper, besides adding the rebuttal contents), and the authors did some experiments, but my concerns are not addressed.\n\n1) the shown improvements, even with pretrain, are still marginal. (seems that the RMSE of baseline is buggy)\n\n2) in iii) the authors argues that as long as the covariance matrix is SPD the model is equivariant, but this is wrong. Using diagonal matrices presumes that the variance are independent along standard axis. This is not SO(3) equivariant.\n\n3) Visualization is discussed but I can't find any."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7154/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699933400933,
                "cdate": 1699933400933,
                "tmdate": 1699933400933,
                "mdate": 1699933400933,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BDdIBQdlGa",
                "forum": "OVPoEhbsDm",
                "replyto": "ZD7J0i1uOV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7154/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7154/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reply"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful engagement with our work. We appreciate the time and effort you invested in examining our response to your concerns so quickly. We are glad to know that you recognize the significant enlargement of implementation details. Please see our latest response to your reply below:\n\n(1) We understand your concern that the improvement of Refine-PPI in RMSE is marginal over baselines. However, as pointed out by RDE [A] and DiffAffinity [B], **these per-structure correlation scores are more relevant to practical applications and should better reflect how good the models are for real-world challenges**. Biologists and protein designers typically prioritize whether binding affinity magnitude improves after mutations, rather than the absolute value of $\\Delta\\Delta G$. More importantly, **the scalar values of experimental $\\Delta \\Delta G$ in Skempi v2 are not standardized** [B], and the robustness of $\\Delta \\Delta G$ is affected by factors such as batch effects and environmental fluctuations. Consequently, ranking coefficients such as Spearman and Pearson's correlations are always regarded as much more vital metrics in evaluating the effectiveness of different algorithms in predicting binding affinity or protein stability [B, C, D]. Our model selection during training is based on Spearman's correlation on the validation set, prioritizing it over RMSD or MAE.\n \n[A] Rotamer Density Estimator is an Unsupervised Learner of the Effect of Mutations on Protein-Protein Interaction. ICLR 2023. \n\n[B] Liu, Shiwei, et al. \"Predicting mutational effects on protein-protein binding via a side-chain diffusion probabilistic model.\" NIPS 2023.\n\n[C] \u00d6zt\u00fcrk, Hakime, Arzucan \u00d6zg\u00fcr, and Elif Ozkirimli. \"DeepDTA: deep drug\u2013target binding affinity prediction.\" Bioinformatics 34.17 (2018): i821-i829.\n\n[D] Kim, Ryangguk, and Jeffrey Skolnick. \"Assessment of programs for ligand binding affinity prediction.\" Journal of computational chemistry 29.8 (2008): 1316-1331. \n\n(2) We are grateful for your clarification regarding the equivariance property. Upon reevaluation, we have determined that our initial assumption of the covariance matrix being a diagonal matrix is insufficient to guarantee equivariance.  As a solution, we take a further step and posit that **the covariance matrix is an isotropic matrix**, meaning that all elements on the diagonal are equal.  Under this condition, our Refine-PPI ensures that for any rotation $Q\\in \\mathbb{R}^{3\\times 3}$ and any translation $g \\in \\mathbb{R}^{3}$, we have \n\n$\\mathbf{h}^{(l+1)}, \\{Q \\boldsymbol{\\mu} ^{(l+1)} + g, Q^\\top \\boldsymbol{\\Sigma}^{(l+1)}Q \\}= \\textrm{PDC-L}[\\mathbf{h}^{(l)}, \\{Q \\boldsymbol{\\mu} ^{(l)} + g, Q^\\top\\boldsymbol{\\Sigma} ^{(l)}Q \\}, \\mathcal{E}].$\n\nThere, $\\boldsymbol{\\Sigma}$ can be initialized by a pre-defined value (e.g., an identity matrix or RMSF in MD simulations) or a learnable embedding. We further assessed the performance difference between this new covariance matrix setting (i.e., isotropic matrix) and the previous one (diagonal matrix), with the results summarized below. Notably, there is no significant difference observed using the spherical matrix as the covariance. To elucidate this, we examined the previously learned diagonal matrix and discovered that all elements in the diagonal are very close to each other. In essence, even without imposing the isotropic restriction, the model inherently learned to assign similar values to diagonal elements. This perspective sheds light on the necessity of isotropy in our covariance matrix.\n\n|       Method       | Per-Structure | Per-Structure |\n|:------------------:|:-------------:|:-------------:|\n|                    |    Pearson    |    Spearman   |\n|     Isotropic Matrix   |     0.4461    |     **0.4106**    |\n|    Diagonal Matrix  |    **0.4475**    |   0.4102    |\n\nWe have provided a detailed proof of the equivariance of Refine-PPI in Appendix D and would value your review to enhance our theoretical foundations.\n\n(3) We apologize for any confusion regarding the visualization aspect. We have already revised our paper once on 13th November, but perhaps the updated version is delayed from being attained by the reviewers because of some unknown systematic reasons in Openreview. Just confirm that we have uploaded the new manuscript again, and please see Appendix C for the visualization of hallucinated structure examples. If you encounter difficulties accessing our updated manuscript, kindly inform us. Your keen eye for such details is instrumental in enhancing the overall quality of our work.\n\n------------------------------------------\n\nOnce again, we appreciate your dedication to the peer-review process, and we are committed to addressing each of your concerns to improve the clarity, correctness, and overall contribution of our paper. Thank you for your constructive feedback."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7154/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699960792577,
                "cdate": 1699960792577,
                "tmdate": 1700026174189,
                "mdate": 1700026174189,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]