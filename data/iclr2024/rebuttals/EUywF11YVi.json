[
    {
        "title": "SimPLR: A Simple and Plain Transformer for Object Detection and Segmentation"
    },
    {
        "review": {
            "id": "j5U2dunZyd",
            "forum": "EUywF11YVi",
            "replyto": "EUywF11YVi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2415/Reviewer_SZaB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2415/Reviewer_SZaB"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents SimPLR. It shows that a plain vision transformer without feature pyramids but with scale-aware attention is able to achieve detection and segmentation performance comparable with pyramid designs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ The presented SimPLR detector maintain the design philosophy of simplicity in ViTs, with a plain architecture and single-scale features.\n+ SimPLR indicates a clear advantage over the ViTDet baseline.\n+ SimPLR reports good performance on three dense prediction tasks on COCO."
                },
                "weaknesses": {
                    "value": "- The novelty of the work seems marginal. A very similar observation has been made recently. See (DETR Does Not Need Multi-Scale or Locality Design, ICCV\u201923). Learning an object detector with single-scale features is also not new. See (You Only Look One-Level Feature, CVPR\u201921)\n\n- The proposed multi-head scale-aware attention is an incremental extension of the Box Attention by (Nguyen et al., 2022). The extension from the Box Attention to the Fixed-Scale Attention and Adaptive-Scale Attention is straightforward. This leads to rather limited technical contribution.\n\n- The improvement over the BoxeR is marginal. In Table 1, SimPLR achieves the same performance as BoxeR (55.4 box AP), with also comparable FLOPs. While SimPLR is slightly efficient, the improvement is not significant.\n\n- The claim of SimPLR outperforms the multi-scale Mask2Former is questionable. In Table 4, the comparison is unfair. SimPLR and Mask2Former use different backbones and pretraining. Such a comparison seems meaningless."
                },
                "questions": {
                    "value": "I don't have additional questions for the authors. The paper is easy to understand, and the problem addressed is clear. \n\nWhile I appreciate the simplicity of the SimPLR and the substantial experiments performed, the results and the claim are not surprising enough or can find similar work in open literature. The technical contribution is also somewhat incremental."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2415/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2415/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2415/Reviewer_SZaB"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2415/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698590435668,
            "cdate": 1698590435668,
            "tmdate": 1699636176669,
            "mdate": 1699636176669,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NntbwH8PBx",
                "forum": "EUywF11YVi",
                "replyto": "j5U2dunZyd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2415/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2415/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the detailed and constructive comments to help our work, acknowledging the ```simplicity``` of our approach.\n\nOur response to each concern:\n1. ***The novelty of the work seems marginal. A very similar observation has been made recently. See (DETR Does Not Need Multi-Scale or Locality Design, ICCV\u201923). Learning an object detector with single-scale features is also not new. See (You Only Look One-Level Feature, CVPR\u201921)***\n\n- Our goal is to pursue the plain detector for both detection and segmentation. In YOLOF [a], while the authors use single-scale input in the detection head, the backbone is still hierarchical. In PlainDETR [b], the multi-scale features are still necessary to generate object proposals. Instead, our SimPLR completely removes the hierarchical and multi-scale constraints in both backbone and encoder of the network.\n- Furthermore, it can be seen that our work is a natural step towards the plain detector. While YOLOF simplifies the detection pipeline by using single-scale input for the detection head, the PlainDETR takes a step forward by removing the multi-scale input in backbone and encoder (even though, they still need multi-scale features for proposal generation). In our work, we push the direction of plain detector forward by completely removing these constraints. Moreover, our plain detector can perform both detection and segmentation\n\n2. ***The proposed multi-head scale-aware attention is an incremental extension of the Box Attention by (Nguyen et al., 2022)***\n\n- We choose to find a simple but effective approach rather than a more complicated one. This is proved by the performance of our detector (SimPLR outperforms PlainDETR by 1.6 AP point and can perform segmentation as well). As mentioned above, we believe that it is much harder to find a simple and effective solution.\n- Our work gives an insight about learning scale-equivariant features in attention computation and a simple setting of plain detector. We strongly believe that the contribution of our work is valuable to the community.\n\n3. ***The improvement over the BoxeR is marginal.**\n\n- Similar to YOLOF or PlainDETR, the goal of our work is to simplify the detection pipeline. Regarding the performance, our plain detector shows better performance than PlainDETR. It also shows a good scaling behaviour while being more efficient. This is different from multi-scale detector like Mask2Former when the speed is very slow with larger backbones.\n\n4. ***The claim of SimPLR outperforms the multi-scale Mask2Former is questionable. In Table 4, the comparison is unfair. SimPLR and Mask2Former use different backbones and pretraining. Such a comparison seems meaningless.***\n\n- It is clear that our detector shows 2 times faster speed than Mask2Former. Furthermore, we also acknowledge that SimPLR benefits from the self-supervised learning and scaling research progress with ViT. We think it's the advantages of using plain backbone like ViT. Similarly, in ViTDet, the authors show that ViT seems to benefit more from self-supervised learning than hierarchical backbones.\n\n[a] Chen et al. You Only Look One-level Feature. In CVPR 2021.\n\n[b] Lin et al. DETR Doesn't Need Multi-Scale or Locality Design. In ICCV 2023."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2415/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700243022831,
                "cdate": 1700243022831,
                "tmdate": 1700243022831,
                "mdate": 1700243022831,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "sM6y2JUC6o",
            "forum": "EUywF11YVi",
            "replyto": "EUywF11YVi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2415/Reviewer_ogBT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2415/Reviewer_ogBT"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents SIMPLR, a simple and straightforward transformer for object detection and segmentation. SIMPLR aims to eliminate the need for the Feature Pyramid Network structure and instead introduces scale-aware attention. This allows the backbone and detection head to effectively utilize single-scale features. Extensive experiments show that SIMPLR outperforms other detectors equipped with FPN while maintaining superior speed."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The motivation behind the proposed components, scale-aware attention, is clear and effectively addresses the issue of relying on FPN. \n\n2. By introducing the idea of cropping box features at different scales from a single-scale map, similar performance to FPN can be achieved. The idea is logical and makes sense in this context.\n\n3. The experiments conducted in this study are robust and thorough. The author systematically evaluates the impact of various hyper-parameters and settings of the proposed multi-scale box attention method. Furthermore, the author compares the performance of the proposed method with state-of-the-art approaches on different datasets and tasks."
                },
                "weaknesses": {
                    "value": "1. If I've understood correctly, the primary contribution of this article is the concept of scale-aware attention. However, I noted the absence of any graphical representation to thoroughly elucidate this complex concept. It would be immensely beneficial if a detailed, comprehensible image could be included to help readers better grasp the intricate technicalities of this key component.\n\n2. While the paper presents some novel aspects, there are areas that fall short. From my understanding, the key innovation lies in the scale-aware attention, which builds upon box-attention [1] by assembling attention from boxes at different scales. While I appreciate this advancement, I believe it may not be sufficient on its own. I fear that it may not meet the rigorous standards expected of an ICLR paper.\n\n3. While the author presents the performance of other methods utilizing single-scale feature maps in Figure 1, it might be more compelling to incorporate detailed, quantified comparisons in the Experiments Section. This would further validate the effectiveness of SIMPLR.\n\n[1] BoxeR: Box-Attention for 2D and 3D Transformers"
                },
                "questions": {
                    "value": "I have stated my questions and doubts in the weakness section. Please correct me if I am wrong."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2415/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698732939053,
            "cdate": 1698732939053,
            "tmdate": 1699636176572,
            "mdate": 1699636176572,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4EYGjNSMHL",
                "forum": "EUywF11YVi",
                "replyto": "sM6y2JUC6o",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2415/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2415/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the detailed and constructive comments to help our work, acknowledging our approach: ```maintaining superior speed```, ```idea is logical```, ```experiments conducted are robust and thorough```:\n\nOur response to each concern:\n1. ***If I've understood correctly, the primary contribution of this article is the concept of scale-aware attention.***\n\n- The primary contribution of our work is to show that a plain detector can achieve strong performance compared to multi-scale counterparts in both object detection and segmentation. This is challenging problem specially for segmentation.\n\n- We show that the plain detector is made possible by: 1) the scale-aware attention that incorporates scale information into attention computation for scale-equivariant learning (this is proven in our ablation), 2) our proposed detector is highly efficient and shows a good scaling behaviour. We believe our work is significant as our method is scalable and shows good performance\n\n2. ***While the paper presents some novel aspects, there are areas that fall short. From my understanding, the key innovation lies in the scale-aware attention, which builds upon box-attention [1] by assembling attention from boxes at different scales. While I appreciate this advancement, I believe it may not be sufficient on its own.***\n\n- While our method is simple, it is efficient and effective. We think that it's more difficult to find a simple and effective method and choose to not go for more complicated settings.\n- This applies to the NLP as well when global self-attention is expensive when dealing with long sequence input, they need to come up with sparse attention [a] to tackle the problem. This is even more problematic in detection and segmentation due to high-resolution images.\n\n[a] Qiu et al. Blockwise Self-Attention for Long Document Understanding. In EMNLP 2020."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2415/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700242228646,
                "cdate": 1700242228646,
                "tmdate": 1700242228646,
                "mdate": 1700242228646,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9uPTaQrTk8",
            "forum": "EUywF11YVi",
            "replyto": "EUywF11YVi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2415/Reviewer_VYm9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2415/Reviewer_VYm9"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes an object detector that is based on single-scale plain ViT backbone. It follows the detector of DETR and adapts scale-aware attention for learning multi-scale features from single-scale backbone features for detection. Experiments are conducted on COCO to show the performance of both detection and segmentation. SimPLR outperform ViTDet and Mask2Former with single-scale backbone (plain ViT) on detection and segmentation task."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The motivation and exploration of using single-scale feature / backbone for object detection is highly meaningful. Previous attempt like ViTDet tries to keep the pretrained backbone plain, but still needs to produce feature pyramids for classic object detectors to perform well.  SimPLR uses multi-scale attention instead to get comparable performance. It extends box-attention with multiple reference windows of different scales, similar to RPN."
                },
                "weaknesses": {
                    "value": "While SimPLR is able to get comparable/better performance with ViTDet, it is still behind sota object detector (e.g. DINO), it is not clear how SimPLR can be transferred to different advanced detectors."
                },
                "questions": {
                    "value": "The proposed method is based on a specific box-attention approach,  can such method be applied to other transformer-based detector as well?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2415/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698820366816,
            "cdate": 1698820366816,
            "tmdate": 1699636176495,
            "mdate": 1699636176495,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Zud1RAqXR1",
                "forum": "EUywF11YVi",
                "replyto": "9uPTaQrTk8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2415/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2415/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the detailed and constructive comments to help our work, acknowledging our approach ```highly meaningful```.\n\nOur response to each concern:\n1. ***While SimPLR is able to get comparable/better performance with ViTDet, it is still behind sota object detector (e.g. DINO), it is not clear how SimPLR can be transferred to different advanced detectors.***\n- As mentioned above, we preserve a simple design of decoder and focus more on fixing hierarchical and multi-scale constraints that appear in backbone and encoder. Therefore, techniques like *object query denoising* in DINO or *hybrid-matching* in other works can still be applied in our detector to further improve the performance.\n- It can also be seen that without Object365 pre-training, DINO/MaskDINO achieves comparable performance with our SimPLR+ViT-H, and SimPLR+ViT-H is still faster than DINO/MaskDINO+Swin-L\n2. ***The proposed method is based on a specific box-attention approach, can such method be applied to other transformer-based detector as well?***\n- In our work, we find that encoding scale information into the attention computation helps the model to learn scale-equivariant features. I think the same strategy can be easily applied to deformable attention as well. However, we choose box-attention because it shows better performance and it can do both detection and segmentation tasks."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2415/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700240950555,
                "cdate": 1700240950555,
                "tmdate": 1700240950555,
                "mdate": 1700240950555,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]