[
    {
        "title": "Amortized Network Intervention to Steer the Excitatory Point Processes"
    },
    {
        "review": {
            "id": "hGBhto182N",
            "forum": "8g26Yv1EOu",
            "replyto": "8g26Yv1EOu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2510/Reviewer_ya7t"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2510/Reviewer_ya7t"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposed a model-based reinforcement learning approach to model networked point processes. Considerations are given on the scalability of the proposed method. The paper describes an amortized policy approach to deal with the scalability issue. The proposed method is applied to synthetic data, real-world covid data, and real-world traffic data."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* originality: the problem formulation of networked point processes described by the author has some originality. \n* quality: the proposed method makes sense to me. The scalability issue is highlighted in particular. I think the scalability issue is an important one to make the algorithm pratical. The authors proposed the use of amortized policy to deal with this issue.\n* presentaiton is clear. I can follow the rationale of the paper.\nsignificance: the proposed method is associated with application scenarios of high significance."
                },
                "weaknesses": {
                    "value": "* I appreciate the authors applying their proposed method to two important pratical problems. However, I think the paper can benefit from comparison to alternative methods. \n\n* Using intensity cost as the evaluation metric is also somewhat obscured."
                },
                "questions": {
                    "value": "* Is it possible for the authors to describe the potential limitation due to the use of mean field approximation for reward modeling?\n* Are there experiments to demonstrate the incorporation of fairness constraints and more?\n* \"For instance, when regulating the coronavirus, government interventions must balance health concerns with economic implications and public sentiment.\" It is not clear whether the experiments described in the paper achieve such a balance."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2510/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698712254545,
            "cdate": 1698712254545,
            "tmdate": 1699636187384,
            "mdate": 1699636187384,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8tHbXwaMf6",
                "forum": "8g26Yv1EOu",
                "replyto": "hGBhto182N",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2510/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2510/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer ya7t [Q1-4]"
                    },
                    "comment": {
                        "value": "**[Summary]** We first want to appreciate reviewer ya7t a lot for your acknowledgment of the innovation and contributions of our work including 1) \u201cthe problem formulation of networked point processes is original\u201d, 2) \u201cthe scalability issue is addressed well by the proposed amortized policy\u201d, 3) \u201cthe presentation is clear\u201d, and 4) \u201cthe proposed method is associated with application scenarios of high significance\u201d. We then provide point-wise responses below to address the concern.\n\n> **[Q1]**: Alternative methods.\n\nAs the suggestion to add more comparisons, we implemented two common RL baselines: SAC and PPO on the COVID data. The empirical results have been incorporated into Appendix I.2 (Figure 13). By using NJODE as our simulator, we observe that the proposed method has successfully reduced the number of infested people (with an average reduced intensity: 0.49) in six counties among the chosen nine counties while SAC and PPO struggle to have a positive intervention effect to control the pandemic.\n\nOne interesting alternative method would be to combine causal inference with RL. In fact, in our experiment, we have introduced a pre-learned Boolean mask to determine the support of the action space, which can be interpreted as leveraging the Granger causal structure to guide policy optimization. Conducting online causal inference given the data collected by RL provides an interesting direction, yet is still open in our context. Particularly, ensuring the identifiability of causal effect estimation requires careful investigation, especially considering our dynamics are modeled as neural ODEs with jumps.  \n\nA most related reference we found is [1], which asks the counterfactual question given the observed temporal point processes by behavior policy and is related to off-policy evaluation for target policy.\n\nReferences: \n[1] Noorbakhsh, K., & Rodriguez, M. (2022). Counterfactual temporal point processes. Advances in Neural Information Processing Systems, 35, 24810-24823.\n\n> **[Q2]**: Intensity cost as the evaluation metric.\n\nHere the intensity cost refers to the derived average intensity throughout the whole intervention process for each node. Since our model is based on the Counting process, instead of simulating the process itself, this average intensity will give us a fair and efficient indicator of how fast the counting process will grow during a given period. Moreover, the economic effect caused by the growing number of infected people and the carbon emissions because of the traffic congestion are all highly related to the intensity of newly infected people and newly arrived vehicles. Thus, in this paper, we think it is fair to use intensity cost to evaluate t.\n\n> **[Q3]**: Potential limitation due to the use of mean-field approximation for reward modeling.\n\nFor the limitation, we have a detailed error-bound analysis for the presented mean-field approximation which is covered in the updated manuscript Appendix E.2. This analysis is built upon a Lipschitz assumption of the given dynamic. The theoretical analysis further provides the insight that, as long as the spectral norm of the ODE neural layer is smaller than one, the mean-field approximation error can be well bounded. \n\n> **[Q4]**: Experiments to demonstrate the incorporation of fairness constraints.\n\nWe added experiments by considering two soft fairness-related constraints and provided an empirical study in Appendix Section I.2. Specifically, two fairness augmentation terms were added to the objective function: 1) intervention budget cost; and 2)  policy smoothing cost. The first cost penalizes implementing the intervention policy that exceeds the predefined budget and the second one trades off the policy-changing frequency. In addition to the two terms under consideration, more realistic constraints relevant to specific scenarios can also be included in practice."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2510/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700673703651,
                "cdate": 1700673703651,
                "tmdate": 1700673703651,
                "mdate": 1700673703651,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DN6EmBXxun",
                "forum": "8g26Yv1EOu",
                "replyto": "hGBhto182N",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2510/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2510/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer ya7t [Q5]"
                    },
                    "comment": {
                        "value": "> **[Q5]**: Experiment to show the balance of health concerns with economic implications and public sentiment.\n\nDuring the intervention, we have three constraints related to the balance of health concerns and economic implications:\n1. We have set up a hard constraint and implemented a dynamic mask to explicitly exclude actions that fall outside the feasible space, ruling out the action that consecutively locks down one county. This maximum length of consecutive lockdown for one city can give us a trade-off between enforcing long-time continuous lockdowns to control pandemics and allowing temporary \u2018freedom\u2019 to recover the economy.\n2. We introduce a policy smoothing cost to penalize the abrupt difference between consecutive policies. This means a larger weight will heavily discourage the agent from frequently changing the policy (which will incur more financial burden) while a smaller weight will less discourage a frequent policy change.\n3. We also introduce an intervention budget cost and penalize the intervention policy that exceeds the predefined budget. Intuitively, a large weight of the intervention cost will discourage the agent from intervening more edges and thus have limited control over the pandemic while a small weight of the intervention cost allows more connections to be controlled but will generate a higher expense.\n\nTo understand this trade-off, we have performed experiments of choosing different weights of intervention budget cost and policy smoothing cost in Appendix I.2."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2510/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700673871230,
                "cdate": 1700673871230,
                "tmdate": 1700673871230,
                "mdate": 1700673871230,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tfQr9epnpP",
                "forum": "8g26Yv1EOu",
                "replyto": "hGBhto182N",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2510/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2510/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Sincerely expecting further discussion from Reviewer ya7t"
                    },
                    "comment": {
                        "value": "Dear Reviewer ya7t,\n\nThank you very much for your commitment and effort in evaluating our paper. We understand that this is a hectic time, and we are writing to gently remind you about providing your insights on our rebuttal. As the discussion phase is drawing to a close, your feedback would be invaluable to us.\n\nShould you have any further thoughts or recommendations about our work, we are eager to discuss them with you.\n\nWe eagerly await your reply.\n\nThank you once again,\n\nThe Authors"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2510/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700734065697,
                "cdate": 1700734065697,
                "tmdate": 1700734065697,
                "mdate": 1700734065697,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DrHRQhOUrN",
            "forum": "8g26Yv1EOu",
            "replyto": "8g26Yv1EOu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2510/Reviewer_hvRi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2510/Reviewer_hvRi"
            ],
            "content": {
                "summary": {
                    "value": "The paper discusses a method for addressing the challenge of large-scale network intervention in scenarios like controlling the spread of infectious diseases or managing traffic congestion. The approach uses model-based reinforcement learning with neural ODEs to predict how excitatory processes in a network will change over time as the network structure evolves. It incorporates Gradient-Descent based Model Predictive Control (GD-MPC) to provide flexibility in policy development, accommodating prior knowledge and constraints. To handle complex planning problems with high dimensionality, the authors introduce an Amortize Network Interventions (ANI) framework, which allows pooling of optimal policies from historical data and various contexts while ensuring efficient knowledge transfer. This method has broad applications, including disease control and traffic optimization."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is well-motivated and the proposed model is technically sound and can notably handle large-scale systems. The overall writing is easy to follow. The experiment section is comprehensive though missing some baselines."
                },
                "weaknesses": {
                    "value": "In the experiment section, why baseline comparison is only limited on one synthetic dataset? Also, can the author explains why the NHPI baseline almost have a constant intensity cost in Figure 1?"
                },
                "questions": {
                    "value": "For adding interventions, can we also consider causal-inference-based methods as valid comparison?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2510/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698995916457,
            "cdate": 1698995916457,
            "tmdate": 1699636187313,
            "mdate": 1699636187313,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZVqtYgO8Z9",
                "forum": "8g26Yv1EOu",
                "replyto": "DrHRQhOUrN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2510/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2510/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reponse to reviewer hvRi"
                    },
                    "comment": {
                        "value": "**[Summary]** Many thanks to reviewer hvRi for your positive comments and recognition of our contribution including 1) \u201cThe paper is well-motivated and technically sound\u201d; 2) \u201cThe proposed method can notably handle large-scale systems\u201d; 3) \u201cThe overall writing is easy to follow\u201d and 4) \u201cThe experiment section is comprehensive\u201d. We would like to address your concerns one by one.\n\n> **[Q1]**: NHPI Baselines and Additional Baselines.\n\nAs the suggestion to add more baselines, we implemented two common RL baselines: SAC and PPO on the COVID data. The empirical results have been incorporated into Appendix I.2 (Figure 13). By using NJODE as our simulator, we observe that the proposed method has successfully reduced the number of infested people (with an average reduced intensity: 0.49) in six counties among the chosen nine counties while SAC and PPO struggle to have a positive intervention effect to control the pandemic.\n\nAs for the constant intensity cost of the NHPI baseline, it was originally proposed on less dense point processes (i.e., tweet data) and used an event-driven RL to steer the point process. We conjecture the less satisfied performance is mainly caused by two aspects: \n1. NHPI used a transformer-based sequential model on historical events to learn the dynamic model. This indeed is not easy to capture the inherent connections between different types of events and also not the best model representation for event sequences generated from a dense graph. On the contrary, the proposed NJODE is built on explicit graph embeddings and has a strong potential to model the mutual interaction between different types of events.\n2. NHPI learns a stationary value function through the Bellman function on an infinite horizon semi-MDP. We think the single-step TD learning for semi-MDP presented in NHPI could cause bias to the learned value function, especially in the non-stationary environment. Instead, our policy is learned from a receding horizon window without learning additional value functions and can be adapted to a new model by iteratively updating the parameters of neural ODE.   \n\n> **[Q2]**: Causal-inference-based baseline method\n\nIt is a great idea to combine causal inference with RL. In fact, in our experiment, we have introduced a pre-learned Boolean mask to determine the support of the action space, which can be interpreted as leveraging the Granger causal structure to guide policy optimization. Conducting online causal inference given the data collected by RL provides an interesting direction, yet is still open in our context. Particularly, ensuring the identifiability of causal effect estimation requires careful investigation, especially considering our dynamics are modeled as neural ODEs with jumps.  \nA most related reference we found is [1], which asks the counterfactual question given the observed temporal point processes by behavior policy and is related to off-policy evaluation for target policy. \n\nReferences: \n[1] Noorbakhsh, K., & Rodriguez, M. (2022). Counterfactual temporal point processes. Advances in Neural Information Processing Systems, 35, 24810-24823."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2510/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700673354023,
                "cdate": 1700673354023,
                "tmdate": 1700673354023,
                "mdate": 1700673354023,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RdXN9AV7nn",
                "forum": "8g26Yv1EOu",
                "replyto": "DrHRQhOUrN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2510/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2510/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Sincerely expecting further discussion from Reviewer hvRi"
                    },
                    "comment": {
                        "value": "Dear Reviewer hvRi,\n\nThank you very much for your commitment and effort in evaluating our paper. We understand that this is a hectic time, and we are writing to gently remind you about providing your insights on our rebuttal. As the discussion phase is drawing to a close, your feedback would be invaluable to us.\n\nShould you have any further thoughts or recommendations about our work, we are eager to discuss them with you.\n\nWe eagerly await your reply.\n\nThank you once again,\n\nThe Authors"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2510/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733981553,
                "cdate": 1700733981553,
                "tmdate": 1700733981553,
                "mdate": 1700733981553,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2m32Pau3h7",
            "forum": "8g26Yv1EOu",
            "replyto": "8g26Yv1EOu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2510/Reviewer_YqcV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2510/Reviewer_YqcV"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a re-enforcement learning algorithm to adaptively change the network structure to steer the observed event counts over a potentially large network. The proposed algorithm is carefully crafted to model network point process data with complicated data structures, which is desirable for solving large-scale real-data applications. The proposed algorithm is theoretically sound, and its effectiveness is demonstrated through simulation studies and real data applications."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is well written, the presentation is clear and the idea is sound."
                },
                "weaknesses": {
                    "value": "More details are needed in some components of the proposed model, including the mean field approximation for the rewarding model and the construction of the amortized policy."
                },
                "questions": {
                    "value": "1. The review of the work on networked excitatory point processes seems to only focus on work that used ODEs. However, I would like to point out that there are also other streams of research on networked excitatory point processes, for example, [1], [2], [3]. It is better to conduct a more comprehensive review of the topic.\n\n2. In the definition of the temporal graph network, what are the definitions of edges in the two examples given in the paper? I do not find formal definitions. Could you please clarify?\n\n3. On page 3, it is claimed that the \"high-dimensional event sequences $\\{X_t\\}_{t\\ge 0}$ has a stationary dynamics\". What do you mean by \"stationary\"? If $X_t$ is the accumulated counts up to time $t$, it is unlikely to be a stationary time series. Please clarify.\n\n4. Is the influence matrix $W$ in equation~(4) a given matrix or learned from the data?\n\n5. The use of mean field approximation on page 4 seems to be rather ad-hoc, without much theoretical or even heuristic justifications. Can you give an example to show the difference between the actual reward and the one calculated based on the mean-field approximation? Are they actually close? Perhaps some simulation studies can be carried out to show the difference in actual reduction in intensity using these two reward objectives in some simple settings.\n\n6. On page 5, it is stated that \"Given a sequence of local policies $\\{\\pi_i\\}$, $1\\le i\\le M$,  addressing $M$ distinct sub-problems, our goal is to create an amortized policy $\\pi_{amo}$\". However, I did not find any detail on how this goal can be achieved. Can you clarify?\n\n7. The label of the y-axis in Figure 3 reads \"Intensity Cost\". What is the definition of the \"Intensity Cost\"?\n\n\n[1] Delattre, S., Fournier, N., & Hoffmann, M. (2016). Hawkes processes on large networks.\n\n[2] Fang, G., Xu, G., Xu, H., Zhu, X., & Guan, Y. (2023). Group network Hawkes process. Journal of the American Statistical Association, (just-accepted), 1-78.\n\n[3] Cai, B., Zhang, J., & Guan, Y. (2022). Latent Network Structure Learning From High-Dimensional Multivariate Point Processes. Journal of the American Statistical Association, 1-14."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2510/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699376970884,
            "cdate": 1699376970884,
            "tmdate": 1699636187226,
            "mdate": 1699636187226,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "V8ClGIdmeX",
                "forum": "8g26Yv1EOu",
                "replyto": "2m32Pau3h7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2510/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2510/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reponse to reviewer YqcV"
                    },
                    "comment": {
                        "value": "**[Summary]**: We appreciate that reviewer YqcV has a positive impression of the **\u201csound idea\u201d** and **\u201cwell written\u201d** of our paper. To address your questions about the details of our method, we provide point-wise responses as follows.\n\n> **[Q1]**: Related work of networked excitatory point processes.\n\nWe have carefully reviewed and incorporated the listed publications in Appendix Section A (related work) in our updated version.\n\n> **[Q2-Q4]**: Definition clarifications of the model details.\n\nHere we make the following clarifications.\n1. **Edge**: For pandemic control cases, the node is defined as county/city, and the edges are defined as the accessibility from one city to another city (see Figure 1). For traffic control cases, the node is defined as a road lane, and edges are defined as the connections between different lanes.\n2. **Stationary Dynamic**: Here $X_{t}$ refers to the number of a sequence of spike counts within a time step, so $X_{t}$ can be stationary in a long-term period. Moreover, under a careful choice of the initial parameters for the neural network (as the contraction condition derived in Appendix E), the dynamic system is also stationary. \n3. **Influence matrix $W$**: In our cases, only the support of $W$ matters, and it can be obtained from prior knowledge. The reason is if we jointly learn the influence matrix $W$ and the discrete kernel function $\\phi$ in the proposed system (Eq. (4) in the manuscript), W will become not identifiable and lose its original physical meaning. Therefore, we choose to fix the support of $W$ (i.e., $w_{ij} \\in [0,1] $) and the specific value can be inferred from the data-driven method. For instance, in the COVID case, the influence matrix is restricted to $[0,1]$ and is proportional to the inverse distance between different counties.\n\nWe have also updated the above clarifications according to the updated manuscript.\n\n> **[Q5]**: Mean-field approximation justification\n\nHere, we provide a brief justification for the mean-field approximation. For supplementary numerical and theoretical justification, please refer to our modified draft (Appendix E).\n\nTo evaluate the policy gradient at each iteration, one needs to get sufficient roll-out trajectories. Numerically, obtaining one roll-out trajectory requires solving ODEs repeatedly within each time interval, and sampling random counts at the end of each interval, which will be fed back to the system to update the state. The mean-field approximation, however, only needs to compute the roll-out once \u2013 it uses a deterministic trajectory to approximate the mean of the stochastic trajectories, by replacing the random counts sampled at the end of each interval by their means. \n\nMean-field approximation dramatically reduces the computational burden in policy optimization. We added numerical results (shown in Fig. 11, Appendix E.2) and theoretical analysis (in Appendix E.1) to demonstrate the approximation accuracy. \n\nThe theoretical analysis further provides the insight that, as long as the spectral norm of the ODE neural layer is smaller than one, the mean-field approximation error can be well bounded. \n\n> **[Q6]**: Amortized policy construction\n\nMathematically, the proposed amortized policy $\\pi_{ani}$ is learned over a distribution of random tasks from $\\mathcal{M}$, i.e., $\\pi_{ani} = E_{M_{i}}[\\pi_{i}]$ where $M_{i} \\sim \\mathcal{M}$. The meta-policy gradient over different tasks is then updated in a similar fashion with Reptile [1]. We have clarified the point in the updated version.\n\n[1] Nichol, A., Achiam, J., & Schulman, J. (2018). On first-order meta-learning algorithms. arXiv preprint arXiv:1803.02999.\n\n\n> **[Q7]**: Figure label clarification\n\nFor brevity, here intensity cost means the average intensity throughout a fixed period and over all the nodes within a local region. We use this indicator to reflect how fast the pandemic/traffic will grow within a multivariate system. We have clarified this point in the updated version."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2510/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700673096376,
                "cdate": 1700673096376,
                "tmdate": 1700673096376,
                "mdate": 1700673096376,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AnnP4xSfY0",
                "forum": "8g26Yv1EOu",
                "replyto": "2m32Pau3h7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2510/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2510/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Sincerely expecting further discussion from Reviewer YqcV"
                    },
                    "comment": {
                        "value": "Dear Reviewer YqcV,\n\nThank you very much for your commitment and effort in evaluating our paper. We understand that this is a hectic time, and we are writing to gently remind you about providing your insights on our rebuttal. As the discussion phase is drawing to a close, your feedback would be invaluable to us.\n\nShould you have any further thoughts or recommendations about our work, we are eager to discuss them with you.\n\nWe eagerly await your reply.\n\nThank you once again,\n\nThe Authors"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2510/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733890000,
                "cdate": 1700733890000,
                "tmdate": 1700733890000,
                "mdate": 1700733890000,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ca7dfOHSOR",
            "forum": "8g26Yv1EOu",
            "replyto": "8g26Yv1EOu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2510/Reviewer_9miN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2510/Reviewer_9miN"
            ],
            "content": {
                "summary": {
                    "value": "The paper looks at both the learning and optimal intervention planning problem for a large-scale network whose evolutions include both continuous state drift and discrete jumps. The authors propose to learn the transition of the states via MLE. Directly dealing with the large-scale problem is intractable. Hence, the authors also propose the Amortized Policy Learning framework to \n1) segment the network into pieces and each time pick one subnetwork to update the model estimation;\n2) update the policy parameters with data collected from the current subnetwork using common feature representation learned by minimizing the bi-contrastive loss. \n\nThe authors apply the proposed framework to tasks including synthetic data, covid data and traffic data where improvements are obtained and show that the proposed policy equivalent embeddings successfully decouple the position embeddings and value embeddings."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors' contributions include: \n1) Formulate the problem as a RL problem.\n2) Propose to decompose the problem into subproblems of smaller scale and learn a policy that can generalize to the full-scale problem via   ensuring permutation equivalence. \n3) Test and compare the methods to previous methods on several settings of practical interest. Also, the authors show concrete evidence of the out-of-distribution generalization power of the proposed method."
                },
                "weaknesses": {
                    "value": "In my point of view, the authors did not clarify their contributions. \n\nIn terms of problem formulation, modeling the discrete events by counting the occurrences in a time window should not be counted as the novelty. In terms of model learning, the authors are basically using the MLE method, which is standard in model-based RL. \nPolicy learning within the permutation equivalence class through learning embedding $p^t, m^t$ via contrastive method is an interesting idea. However, the authors do not differentiate their work from (Chen et al., 2020b).\n\nMore crucially, each part in a large networks may have both global and local patterns. How to deal with the local patterns in model learning and policy optimization is not clear. Or the authors are just trying to obtain a policy that only has \"overall\" good performance.\n\nAnother question is how to deal with the soft/hard constraints. I did not find evidence supporting the effectiveness of the constraint ensuring methods."
                },
                "questions": {
                    "value": "As I have mentioned before, is it possible for the current framework to capture the local patterns in a large network and perhaps steer the learned policy towards local patterns while maintaining the generalization power (such as regularized policy optimization)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2510/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699502043424,
            "cdate": 1699502043424,
            "tmdate": 1699636187152,
            "mdate": 1699636187152,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "X2GmWYYDTe",
                "forum": "8g26Yv1EOu",
                "replyto": "ca7dfOHSOR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2510/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2510/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer 9miN"
                    },
                    "comment": {
                        "value": "**[Summary]**: We first thank reviewer 9miN for the insightful comments, especially for the questions about our high-level contributions and experiment designs, which benefit us to further clarify our paper. We would like to address the concerns one by one.\n\n> **[Q1]**: Contributions of the paper and the difference between our proposed loss and SimCLR.\n\nWe highlight the contributions part in the Introduction of the revised manuscript. Our contributions include three aspects: 1) a technically sound method to tackle the challenge of large-scale network intervention, 2) the proposal of a permutation-equivalent intervention paradigm, and 3) comprehensive experiments on traffic congestion and COVID data. Most importantly, our key contribution lies in introducing the permutation-equivalent intervention paradigm within a dynamically evolving large network topology, facilitated by the utilization of the proposed bi-contrastive loss function.\n\nAs for the difference between the proposed Bi-contrastive loss and the previous SimCLR (Chen et al., 2020b):\n - Firstly, unlike SimCLR which directly performs augmentations on visual representations, we augment our network on its latent embedding space by utilizing graph augmentation techniques. \n- Secondly, our model does not require sampling additional data as negative examples and projecting the augmented sample to one embedding. Instead, we only use one anchor sample (here is the embedded network) project the sample to two different embeddings (p and m), and augment the anchor sample by permuting its node orders or adding noise to the node embeddings such that it can form two negative pairs naturally (as depicted in Figure 1). More details will be provided in Appendix Section F in the updated version.\n\n> **[Q2]**: Global and local pattern for network work intervention.\n\nIn our model, we have the permutation equivalent property to extract the global invariant patterns. Regarding the local pattern, we use the node embeddings $H_{t}$ learned by the Networked Jump ODE model to store the information. For instance, the elements of node embedding can have the meaning of a local region\u2019s population and economy.  When performing policy optimization, the node embeddings will be used to learn the policy and global invariant patterns. Therefore, our policy can capture both local patterns and global patterns elegantly. \n\n> **[Q3]**: Dealing with soft and hard constraints.\n\nFor hard constraints like strictly controlling the maximum number of consecutive interventions, we use a \u2018\u2019dynamic mask\u201d in our policy learning. For soft constraints, we can treat the constraints as \u2018\u2019regularization/augmentation terms\u201d and add these terms to the objective function. \n\nWe added experiments by considering two soft fairness-related constraints and provided an empirical study in Appendix Section I.2. Specifically, two fairness augmentation terms were added to the objective function: 1) intervention budget cost; and 2) policy smoothing cost. The first cost penalizes implementing the intervention policy that exceeds the predefined budget and the second one trades off the policy-changing frequency. In addition to the two terms under consideration, more realistic constraints relevant to specific scenarios can also be included in practice."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2510/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700672623670,
                "cdate": 1700672623670,
                "tmdate": 1700672623670,
                "mdate": 1700672623670,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aVaXo0gh8D",
                "forum": "8g26Yv1EOu",
                "replyto": "ca7dfOHSOR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2510/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2510/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Sincerely expecting further discussion from Reviewer 9miN"
                    },
                    "comment": {
                        "value": "Dear Reviewer 9miN,\n\nThank you very much for your commitment and effort in evaluating our paper. We understand that this is a hectic time, and we are writing to gently remind you about providing your insights on our rebuttal. As the discussion phase is drawing to a close, your feedback would be invaluable to us.\n\nShould you have any further thoughts or recommendations about our work, we are eager to discuss them with you.\n\nWe eagerly await your reply.\n\nThank you once again,\n\nThe Authors"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2510/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733799275,
                "cdate": 1700733799275,
                "tmdate": 1700733799275,
                "mdate": 1700733799275,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]