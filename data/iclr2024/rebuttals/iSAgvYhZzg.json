[
    {
        "title": "You Only Look at Screens: Multimodal Chain-of-Action Agents"
    },
    {
        "review": {
            "id": "pS9WH5HwaI",
            "forum": "iSAgvYhZzg",
            "replyto": "iSAgvYhZzg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2533/Reviewer_BNVX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2533/Reviewer_BNVX"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes Auto-UI, a multimodal solution that directly interacts with the interface, which eliminates the need for environment parsing or reliance on application-specific APIs. The authors introduce a chain-of-action technique that incorporates previous action histories and future action plans to guide the agent's decision-making process. The approach is evaluated using a device-control benchmark called AITW, which consists of 30,000 unique instructions covering tasks like application operation, web searching, and web shopping. Experimental results demonstrate that Auto-UI achieves state-of-the-art performance, with high accuracy in predicting action types (90%) and an overall success rate of 74%. The authors have made the code available for review."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed Auto-UI approach demonstrates a level of originality in addressing the challenges of autonomous user interface agents. By directly interacting with the interface instead of relying on environment parsing or application-specific APIs, it offers a novel solution that bypasses common inefficiencies and risks associated with existing approaches. The introduction of the chain-of-action technique also adds a unique element to the decision-making process of the agent.\n2. The approach is evaluated through experiments with the AITW benchmark. The inclusion of 30,000 unique instructions covering various multi-step tasks provides a comprehensive assessment of the Auto-UI system. Achieving a state-of-the-art performance demonstrates the effectiveness and reliability of the proposed solution.\n3. Overall, the paper is clear and easy to follow. The text provides a clear description of the challenges faced by existing approaches, introduces the Auto-UI solution, and explains the chain-of-action technique. The inclusion of experimental results contribute to a clear understanding of the proposed methodology and its performance.\n4. By addressing the challenges of inference inefficiency and error propagation, Auto-UI offers a more efficient and reliable approach to task automation. The multimodal solution and the elimination of environment parsing and reliance on application-specific APIs provide a significant advancement in the development of autonomous UI agents. Furthermore, the state-of-the-art performance achieved on the AITW benchmark showcases the practical applicability and potential impact of the proposed approach."
                },
                "weaknesses": {
                    "value": "1. While the authors highlight the chain-of-action technique as a contribution, it appears to primarily concatenate the output actions, which can be confusing. It would be helpful to provide a more detailed explanation or clarification of how the chain-of-action technique enhances the decision-making process and contributes to the overall effectiveness of the Auto-UI approach.\n\n2. The experiment section lacks an explanation for the rationale behind selecting specific baselines. It would be valuable to include a justification for choosing the particular baselines used in the evaluation. Additionally, providing information on the performance of a GPT4 model, if available, would offer a useful benchmark to compare the performance of the proposed Auto-UI approach."
                },
                "questions": {
                    "value": "GPT4 is reported to possess significantly improved agent capabilities compared to existing LLMs. However, it is important to note that the specific performance metrics and details of GPT4 have not been provided in the given context. Therefore, the performance of GPT4 remains unclear and unavailable for direct comparison in this discussion. What is the performance of GPT4?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2533/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2533/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2533/Reviewer_BNVX"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2533/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698248307687,
            "cdate": 1698248307687,
            "tmdate": 1699636190100,
            "mdate": 1699636190100,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5j1URc9oqh",
                "forum": "iSAgvYhZzg",
                "replyto": "pS9WH5HwaI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2533/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2533/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your insightful review and constructive feedback.\n\n> W1: \"While the authors highlight the chain-of-action technique as a contribution, it appears to primarily concatenate the output actions, which can be confusing. It would be helpful to provide a more detailed explanation or clarification of how the chain-of-action technique enhances the decision-making process and contributes to the overall effectiveness of the Auto-UI approach.\"\n\nChain-of-action (CoA) technique contributes two aspects instead of purely concatenating the output actions. It organizes a chain of previous action histories on the input side and a chain of future action plans on the output side, which encourages the agent to leverage action history and make future plans before predicting the current action. The working mechanism can be seen as bidirectional. \n\nCoA helps improve the decision-making process and contributes to the overall effectiveness by +5.74% accuracy. To investigate the effectiveness, we conduct additional analysis to ablate the CoA technique on category accuracy.\n\n| Model                     | Overall | Action Type  | Click        | Scroll       | Text |\n| ------------------------- | ------- | ------- | ------- | ------- | ------- |\n| w/o Chain of Actions | 58.99 | 81.69 | 48.38 | 75.83 | 93.57 |\n| w/ Chain of Actions | 68.24  (+9.25) | 87.03  (+5.34) | 58.34 (+9.96) | 82.74  (+6.91)  | 93.99 (+0.42) |\n\nThe results show that CoA helps improve the clicking and scrolling accuracy by a large margin (9.96 and 6.91), e.g., clicking accurate positions and scrolling in the correct direction, compared with the action type accuracy and text accuracy. The most possible reason is that CoA provides action history and future plans as the context for performing more accurate executions given that the model has already been effective in predicting the action type (over 81.69%).\n\n> W2 & Q: \"The experiment section lacks an explanation for the rationale behind selecting specific baselines. It would be valuable to include a justification for choosing the particular baselines used in the evaluation. Additionally, providing information on the performance of a GPT4 model, if available, would offer a useful benchmark to compare the performance of the proposed Auto-UI approach.\"\n\nYes. We have added a justification for the choice of the baselines in Section 4.2. The specialized UI agent is selected because it is the previous state-of-the-art approach in existing studies. The in-context learning LLMs are selected because they reflect the widely used paradigm of developing LLM agents. Fine-tuned LLMs are adopted to investigate the potential of leveraging open-source LLMs for our problem. The baselines encompass both the In-context Learning and fine-tuning paradigms, along with various backbone models of different sizes. These baselines were carefully chosen to ensure a thorough evaluation of our work's performance against existing approaches.\n\nFollowing the reviewer\u2019s comment, we have added the performance of the GPT-4V model. The main results are shown below.\n\n| Model                     | Overall | General  |  Install  | GoogleApps  | Single  |\n| ------------------------- | ------- | ------- | ------- | ------- | ------- |\n| PaLM 2-CoT|  39.6 | -  |  - |  - |   -  | \n| ChatGPT  | 7.72  | 5.93  | 4.38  | 10.47  |  9.39  |  8.42  |\n| GPT-4V  | 52.96   | 43.01   | 46.14   | 49.18      | 78.29  |\n| Auto-UI | 74.27   | 68.24   | 76.89   | 71.37      | 84.58  |\n\nThe results show that GPT-4V achieves much better performance than the PaLM and ChatGPT models. Auto-UI is still better than GPT-4V."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2533/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700377975306,
                "cdate": 1700377975306,
                "tmdate": 1700377975306,
                "mdate": 1700377975306,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ydnTigJlJ8",
            "forum": "iSAgvYhZzg",
            "replyto": "iSAgvYhZzg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2533/Reviewer_CMSv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2533/Reviewer_CMSv"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a \u201cchain-of-action\u201d approach to tackle the autonomous web-searching agent problem. Specifically, they propose a multimodal framework that firstly encodes both the language goals and the web-interaction histories, as well as the screen images, into a combined representation, where a decoder will generate a look-ahead future action plan and a formatted immediate next action to perform.\nThe authors conducted experiments on the AITW dataset where an AI agent is tasked to interact with a Web UI following certain goals, where they demonstrate the effectiveness of the proposed models against three major baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The proposed framework is claimed to be much lighter weight than methods that try to take the whole web information into textualized format for agents to comprehend.\n- The formatted action is sound and should be generalizable to other web-search domains.\n- The paper is pretty easy to follow, with illustrations onto the points.\n- The generalization ablation studies are helpful to gauge the capacity of the proposed framework."
                },
                "weaknesses": {
                    "value": "- The paper does not describe much about the actual training details, in that sense, to me, the proposed method is still a kind of BC, where the target decoding is optimized towards mimicking the golden action sequences. (Unless some RL or other mechanism is used here, which is not described.) In my opinion, the novelties here mainly lie in the multimodal representations (both modality taken into account) and the format of the action performed.\n- I\u2019m a bit skeptical about the ICL baseline, first of all more details (e.g., how actions are represented, how OCRed results are used) of that baseline need to be described, at least in the appendix. Secondly, it also needs to be evaluated at the action plan level, my guess is that this method should be quite accurate on those but might fail more on the lower-level executions. Thirdly, it is indeed unfair simply because the model is not taking the images into account, which could be the key towards the success of the proposed method in this work. So, at least a multimodal version of it needs to be taken into consideration, or, a better spatial representation of the html syntax is required. (HTML can be many times too coarse to represent a spatial layout.)\n- Similar to above, the third baseline, fine-tuning LLMs, need to have a version with multimodal inputs.\n- An error analysis is required both on the quantitative and qualitative sides, what are the major errors that these models exhibit?"
                },
                "questions": {
                    "value": "- I\u2019m a bit surprised that the language decoder is able to predict tokens as precisely as four decimal places, or is the actual precision here not important? I.e., could you not simply split image screens into patches and just use their centers as the coordinate representations? (And the more patches you grid the screen, the more precise it would be.)\n- What are the main types of errors observed by the proposed framework? And, does the framework provide good insights on how to assign these errors to specific modules? I.e., where should the improvements be?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2533/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698362800247,
            "cdate": 1698362800247,
            "tmdate": 1699636190011,
            "mdate": 1699636190011,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ymOxw9X9Hv",
                "forum": "iSAgvYhZzg",
                "replyto": "ydnTigJlJ8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2533/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2533/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your insightful review and constructive feedback.\n\n> W1: Training details & Novelty.\n\nThe training details are provided in Section 4.4. We agree that our approach is still a kind of BC. However, the novelty of Auto-UI lies within (i) a multimodal agent for autonomous UI control that can directly interact with the screens, thus circumventing the constraints of environment parsing and application-specific API access, and (ii) a chain-of-action technique that leverages the previously executed actions and future action plans to help the agent decide what action to execute at each step. Therefore, our work represents a significant departure from prior studies in the field. In addition, Auto-UI achieves state-of-the-art performance with an action type prediction accuracy of 90% and an action success rate of 74%. Notably, Auto-UI can infer an action as fast as within less than one second.\n\n> W2: About the ICL baseline.\n\nFirstly, the ICL baseline is implemented following the prior study [1]. To help readers understand the details, we have exactly provided examples of the ICL baseline in Appendix A.3 with represented actions and OCRed results. \n\nSecondly, we have conducted additional analysis on the action plan level. The result is shown below.\n\n| Model                     | Overall | Action Type  | Click        | Scroll       |\n| ------------------------- | ------- | ------- | ------- | ------- |\n| ChatGPT | 5.93    | 41.72       | 8.50  | 4.00   |\n| Auto-UI | 68.24   | 87.03       | 58.34 | 82.74  |\n\nIndeed, we see that the ICL method (ChatGPT) is quite accurate at predicting the action type (41.72%) but fails at lower-level executions, e.g., clicking positions (8.5%) and scrolling directions (4.0%).\n\nThirdly, this is exactly the motivation of our work. Previous work commonly uses HTML syntax while we argue that parsing the visual environment into textual elements may be prone to error propagation or information loss and the parsed elements generate lengthy inputs, thus leading to inference inefficiency. Following the reviewer\u2019s comment, we add the results of a strong multimodal ICL baseline (GPT-4V). The results show that our proposed approach is still much better than the baseline.\n\n| Model                     | Overall | General  |  Install  | GoogleApps  | Single  |\n| ------------------------- | ------- | ------- | ------- | ------- | ------- |\n| GPT-4V  | 52.96   | 43.01   | 46.14   | 49.18      | 78.29  |\n| Auto-UI | 74.27   | 68.24   | 76.89   | 71.37      | 84.58  |\n\n> W3: \u201cSimilar to above, the third baseline, fine-tuning LLMs, need to have a version with multimodal inputs.\u201d\n\nYes. We have added the results of fine-tuned LLMs with multimodal inputs (LLaVA) [1]. This can be seen as a variant of our framework by changing the backbone modules. We see that LLaVA is able to achieve slightly better performance gains and our proposed chain-of-action technique is also effective at enhancing the performance further.\n\n| Model                     | General  |\n| ------------------------- | ------- |\n| T5 backbone            | 68.24 |\n| LLaVA  backbone      | 68.70   |\n| w/o chain of actions | 58.40   |\n\n> W4 & Q2: \u201cAn error analysis is required both on the quantitative and qualitative sides, what are the major errors that these models exhibit?\u201d\n\nWe have conducted error analysis in Section 5.1 and find that the major errors lie within the click region and scroll direction predictions. Although the model is able to predict the right action most of the time, it tends to click the wrong place or scroll in the wrong direction. The result reveals a future direction of improving the model\u2019s ability to understand the screen layouts, e.g., using more advanced vision features.\n\n> Q1: \u201cI\u2019m a bit surprised that the language decoder is able to predict tokens as precisely as four decimal places, or is the actual precision here not important? I.e., could you not simply split image screens into patches and just use their centers as the coordinate representations? (And the more patches you grid the screen, the more precise it would be.)\u201d\n\nYes. Our results show that the actual precision here is not important. For clicking actions, as defined in [2], a click action is considered correct if its touch point and lift point fall within a 14% screen distance from the gold gestures or occur within the same detected bounding box with the gold gestures. For scrolling actions, a scroll action is considered correct if it has the same scroll axis as the gold gesture. \n\nActually, we did not use patches but the global (pooled) representation of the vision features as described in Section 3.2 (Encoding).\n\n[1] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. NeurIPS 2023.\n[2] Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. Android in the wild: A large-scale dataset for Android device control. arXiv preprint arXiv:2307.10088, 2023."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2533/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700377825234,
                "cdate": 1700377825234,
                "tmdate": 1700378196801,
                "mdate": 1700378196801,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eHuELxftIP",
                "forum": "iSAgvYhZzg",
                "replyto": "ymOxw9X9Hv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2533/Reviewer_CMSv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2533/Reviewer_CMSv"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the efforts to provide the above additional results, I do appreciate them.\n\nWhile the numbers seem decent, my main concerns still stand, which are exactly what the two novelties mentioned in your response.\nFor the first novelty mentioned, that is merely a design choice of adapting modules to the correct corresponding targeted domains.\n\nThe second novelty claimed, if at all, is the main issue -- it is not really novel, at least not as much as it is claimed.\nThe chain-of-action, is essentially just concatenating action history, which is very straightforward to come up with when approaching problems like this where a series of actions are to be taken.\nFurthermore, it is not taking the image history into consideration, which can potentially bring benefits and enrich the technical parts of the work (which also draws an analogy to state-action pairs of a multimodal RL agent).\nTherefore, this being claimed throughout the paper as the main novelty, does not really warrant the level of technical novelty contribution in learning conferences such as ICLR.\n\nI think this work has a chance to make a nice case in conferences like CSCW and/or NAACL, I encourage the authors to submit the manuscript to those types of venues."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2533/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700447211648,
                "cdate": 1700447211648,
                "tmdate": 1700447211648,
                "mdate": 1700447211648,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qKNfGlvfDH",
                "forum": "iSAgvYhZzg",
                "replyto": "ydnTigJlJ8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2533/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2533/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the further comment. We understand your concerns on the novelty. Though we respect the thoughtful comments, we humbly think those concerns are caused by misunderstanding, which we will explain in detail below.\n\n> \u201cFor the first novelty mentioned, that is merely a design choice of adapting modules to the correct corresponding targeted domains.\u201d\n\nTo the best of our knowledge, our work is the first to present a multimodal language agent framework for challenging autonomous user interface control.  The novelty of our approach can be distilled into the following two key facets:\n\n**(i) Addressing a non-trivial problem**: Our focus on developing multimodal autonomous language agents represents a cutting-edge research direction that has garnered significant interest. However, the inherent challenge lies in devising a comprehensive solution for multimodal perception and reasoning, encompassing aspects such as planning, memory, and decision-making. Existing studies have, to date, grappled with a sandbox setting, relying on external tools and application-specific APIs for environment perception and action interpretation. Regrettably, these approaches have left unresolved challenges in their wake.\n\n**(ii) Pioneering a new paradigm**: To address the complexities outlined above, our work introduces Auto-UI, a simple, effective, and efficient solution grounded in first principles thinking. This framework bypasses the need for intricate environment parsing or dependence on application-specific APIs. Auto-UI, propelled by our proposed mechanisms, achieves a state-of-the-art performance. As echoed by Reviewer BNVX and Reviewer tvGC, our work is an effective solution for real-world applications of autonomous agents.\n\nConsequently, characterizing our work as \"merely a design choice of adapting modules to the correct corresponding targeted domains\" oversimplifies its innovative contributions. Rather, it signifies a more efficient and reliable approach to task automation (echoed by Reviewer BNVX). Even so, proposing a design choice of adapting modules to the correct corresponding targeted domains has also been widely accepted in existing publications [1-3]. In light of these considerations, we believe our work holds significant value for the ICLR community.\n\n> About the novelty of the chain-of-action technique.\n\nFirstly, we wish to address a potential misunderstanding regarding the chain-of-action approach. It is NOT merely \"concatenating action history\"; rather, it consists of two crucial parts: a chain of previous action histories on the input side and a chain of future action plans on the output side. This innovative approach encourages the agent to utilize action history and formulate future plans prior to predicting the current action. In essence, it mirrors the memory and planning mechanisms observed in language agents.\n\nSecondly, while we did contemplate incorporating image history, our empirical findings indicated that its inclusion did not result in performance improvements. Consequently, we opted for the current implementation to maintain simplicity rather than pursuing unnecessary complexity.\n\nThirdly, we\u2019d like to humbly share [a recent article by Michael Black](https://perceiving-systems.blog/en/post/novelty-in-science) on novelty in research: simplicity is not in conflict with novelty. We humbly believe that a simple method, like Auto-UI, that solves an important problem but no one thought about before, is novel. We appreciate your thinking alike.\n\nIn conclusion, we hope our clarifications can alleviate your concerns  and you can consider our work more favorably. \n\n*References:*\n\n[1] Yehudai, Asaf, Matan Vetzler, Yosi Mass, Koren Lazar, Doron Cohen, and Boaz Carmeli. QAID: Question Answering Inspired Few-shot Intent Detection. ICLR 2023.\n\n[2] Chirkova, Nadezhda, and Sergey Troshin. CodeBPE: Investigating Subtokenization Options for Large Language Model Pretraining on Source Code. ICLR 2023.\n\n[3] Robinson, Joshua, and David Wingate. Leveraging Large Language Models for Multiple Choice Question Answering. ICLR 2023.\n\nThanks,\nAuthors"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2533/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700563551761,
                "cdate": 1700563551761,
                "tmdate": 1700564747980,
                "mdate": 1700564747980,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "kltAhQRXQy",
            "forum": "iSAgvYhZzg",
            "replyto": "iSAgvYhZzg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2533/Reviewer_tvGC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2533/Reviewer_tvGC"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an autonomous UI agent called Auto-UI that can interact in a multimodal UI environment without environment parsing or application-dependent API access. Specifically, it proposes a chain-of-action technique to help the agent make decisions."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. It is novel that the paper pays attention to the limitations in the real-world applications of autonomous agents and seeks to provide an agent that does not need extra intermediate environment parsing or interval application-dependent APIs.\n\n2. The paper proposes a chain-of-action technique which helps the agent to decide step-by-step."
                },
                "weaknesses": {
                    "value": "1. The Figure 1 in this paper is somewhat not clear enough, making it difficult to understand the two paradigms in (a) and (b).\n\n2. The author does not provide a specific explanation of the Sandbox Paradigm and the First Principles Thinking Paradigm, which is confused. \n\n3. We find some grammar mistakes in the paper, for example, on page 2, paragraph 2, line 5, do you want to express inefficiency instead of efficiency?\n\n4. The authors don't explain exactly what touch_point, lift_point, etc. mean in the first place, causing some confusion.\n\n5. The authors do not provide a specific example between Auto UI and other baselines in Section 5, which is not clear to understand the effectiveness of the provided Auto UI."
                },
                "questions": {
                    "value": "In Section 4.3, why do you use 14% instead of other number to evaluate the correction of a click action, could you provide some references?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2533/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698674099009,
            "cdate": 1698674099009,
            "tmdate": 1699636189925,
            "mdate": 1699636189925,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "k3dyrM1rJI",
                "forum": "iSAgvYhZzg",
                "replyto": "kltAhQRXQy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2533/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2533/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your insightful review and constructive feedback.\n\n> W1 & W2 \u201cThe Figure 1 in this paper is somewhat not clear enough, making it difficult to understand the two paradigms in (a) and (b).\u201d & \u201ca specific explanation of the Sandbox Paradigm and the First Principles Thinking Paradigm\u201d\n\nWe provide a detailed explanation of the two paradigms from the third and fifth paragraphs in the introduction. We elaborate on it as follows:\n\n*(a) sandbox paradigm*: depends on the intermediate transformation between environments and agents\n\n- input side: relies on external tools such as optical character recognition (OCR) and icon detectors to parse the environment into textual elements \n\n- output side: requires accessing internal APIs to interact with the environment, e.g., using a JavaScript element selection on a webpage or a Python interpreter to execute actions. \n\n*(b) first principles thinking paradigm*: allows direct interactions on the screen without needing access to intermediate environment parsing or interval application-dependent APIs.\n\n> W3: \u201cWe find some grammar mistakes in the paper, for example, on page 2, paragraph 2, line 5, do you want to express inefficiency instead of efficiency?\u201d\n\nYes. It is inefficiency. We have fixed the typo in the revised version.\n\n> W4: \u201cThe authors don't explain exactly what touch_point, lift_point, etc. mean in the first place, causing some confusion.\u201d\n\nTouch_point and lift_point simply mean the axis of touching and lifting. As the notation is widely accepted in related studies and we believe it is also commonsense, we provide examples when discussing coordinate normalization in Section 3.3. Even so, following the reviewer\u2019s comment, we have added an explanation in the first place (Figure 1).\n\n> W5: A specific example between Auto UI and other baselines in Section 5.\n\nSection 5 is about the analysis of our approach, which follows the same model architecture. We believe the reviewer means Section 4 (please correct us if it is wrong). We have provided a detailed description of the baselines in Section 4.2. Actually, the example of the specialized UI agent and fine-tuned LLMs is illustrated in Figure 1 while the example of the in-context learning LLMs is provided in Appendix A3.\n\n> Q: \u201cIn Section 4.3, why do you use 14% instead of other number to evaluate the correction of a click action, could you provide some references?\u201d\n\nThe choice basically follows the previous study [1] which releases the dataset and defines the evaluation criteria. We follow the setting for a fair comparison.\n\n[1] Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. Android in the wild: A large-scale dataset for Android device control. arXiv preprint arXiv:2307.10088, 2023."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2533/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700377179290,
                "cdate": 1700377179290,
                "tmdate": 1700377179290,
                "mdate": 1700377179290,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8rGkFYabUM",
            "forum": "iSAgvYhZzg",
            "replyto": "iSAgvYhZzg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2533/Reviewer_y4jm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2533/Reviewer_y4jm"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a multimodal work for Auto-UI, it proposes to leverage the chain-of-action (including previous history actions and future actions) for model prediction. Their model builds on the top of Llama 2 with an image encoder (for screen image). Empirical experiments on the AITW dataset shows very promising results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This work proposes a chain of action operation, leveraging the action history and future actions for current action prediction.\n2. Based on Llama 2, it incorporates a pretrained image encoder into the pretrained LLM for action decision, and shows promising results on AITW dataset."
                },
                "weaknesses": {
                    "value": "1. A potential weakness is where is the gain from? It looks PaLM and ChatGPT are pretty low on this dataset, while they only take text input, and BC models and Auto-UI models take image screen as input, and get very high results, it is unclear where is the gain from? image encoder? or a chain of action input?"
                },
                "questions": {
                    "value": "I try to understand the setting of the experiments, and why the strong PaLM and ChatGPT baselines are so low. Based on the main Table 2, it looks the most gain is from the image encoder, right? Since PaLM-CoT and ChatGPT-CoT only take text input, and their performance is pretty low, and also similarly for Llama 2. Is this right? Probably needs a baseline/ablation to see the performance of model without image encoder."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2533/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698967002917,
            "cdate": 1698967002917,
            "tmdate": 1699636189782,
            "mdate": 1699636189782,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "g0erJrz2KV",
                "forum": "iSAgvYhZzg",
                "replyto": "8rGkFYabUM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2533/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2533/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your insightful review and constructive feedback.\n\n> W & Q: Where is the gain from?\n\nThe performance gain mainly comes from two aspects, multimodal perception and chain-of-action technique. Following the reviewer\u2019s suggestion, we have conducted additional experiments to ablate the performance of the model without the image encoder (w/o multimodal perception) but use the parsed HTML layout instead.\n\n| Model                     | Overall |\n| ------------------------- | ------- |\n| Auto-UI                   | 74.27   |\n| w/o multimodal perception |   62.70      |\n| w/o chain of actions      | 68.53   |\n\nThe key takeaways are: \n\n(i) Multimodal perception is critical compared with existing baseline language agents built purely on large language models. The result shows that our model out of first principles thinking can serve as a strong autonomous agent.\n\n(ii) The chain-of-action technique helps improve the ability of our multimodal agent further, by leveraging the previously executed actions and future action plans to help the agent decide what action to execute at each step. As a result, the memory and planning ability can be enhanced.\n\nIt is reasonable that the baseline results are low due to the lack of vision encoders. As a result, they may suffer from error propagation or information loss (as discussed in the introduction section). To have a deeper understanding of the performance, we have conducted additional analysis on the performance regarding the action type and action execution (i.e., clicking accurate positions and scrolling in the correct direction). \n\n| Model                     | Overall | Action Type  | Click        | Scroll       |\n| ------------------------- | ------- | ------- | ------- | ------- |\n| ChatGPT | 5.93    | 41.72       | 8.50  | 4.00   |\n| Auto-UI | 68.24   | 87.03       | 58.34 | 82.74  |\n\nWe find that the ICL method (ChatGPT) is quite accurate at predicting the action type (41.72%) but fails at lower-level executions, e.g., clicking positions (8.5%) and scrolling directions (4.0%)."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2533/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700376938169,
                "cdate": 1700376938169,
                "tmdate": 1700376938169,
                "mdate": 1700376938169,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]