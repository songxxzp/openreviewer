[
    {
        "title": "A multiobjective continuation method to compute the regularization path of deep neural networks"
    },
    {
        "review": {
            "id": "4yFQ1SeQAS",
            "forum": "nrDRBhNHiB",
            "replyto": "nrDRBhNHiB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7218/Reviewer_dHK2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7218/Reviewer_dHK2"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies multi-objective optimization problem with $\\ell_1$ regularization. The paper claims that they extend the target problem from linear to high-dimensional non-linear problems. Numerical experiments on MNIST demonstrate the efficacy of the proposed optimization schema."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- The paper is written well and easy to follow. \n- The target problem is important to the community."
                },
                "weaknesses": {
                    "value": "- Missing references.\n The authors claim in introduction that `Very recently, there was a first attempt to extend the concept of regularization paths\n to DNNs by means of treating the empirical loss and sparsity.`\nHowever, dating back to 2020, the problem has been studied in OBProxSG where the multi-objective could be considered as a weighted sum of each individual target objective. \n\n   [1] Orthant Based Proximal Stochastic Gradient Method for $\\ell_1$-Regularized Optimization, 2020. \n\n- Experiments are not sufficient. I am concerned about the experiments. In particular, the paper studies multi-objective problems. However, in the experiment, only cross-entropy is considered as loss function. Where are other losses to form a real multi-objective problem? Meanwhile, the network and dataset are too simple to validate the efficacy."
                },
                "questions": {
                    "value": "See the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7218/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7218/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7218/Reviewer_dHK2"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7218/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698637386227,
            "cdate": 1698637386227,
            "tmdate": 1700033497386,
            "mdate": 1700033497386,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7KNZm33GFc",
                "forum": "nrDRBhNHiB",
                "replyto": "4yFQ1SeQAS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7218/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7218/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your detailed and thoughtful review of our paper.  Regarding your comments, suggestions and questions, we have carefully considered them, and below are answers to your questions. In summary, we have;\n\n* Added a second, much more complex example using the CIFAR10 dataset, and demonstrated that the advantages of our approach also hold in much higher dimensions.\n\n* Argued why our method is applicable to more complex examples than many alternative algorithms.\n\n* Argued why our method tends to be more robust than standard training approaches.\n\n* Increased the discussion on numerical details of our approaches and the numerical experiments.\n\n* Added the references suggested by the reviewer to the related work section of our paper.\n\nWe hope that this convinces the reviewer that our work is indeed novel, as it is the first method that allows for computing the entire front for non-convex problems of such large dimensions.\n\n**All changes in the updated PDF version of our paper have been marked in blue.**\n\n**Questions and Answers:**\n\n1. Missing references: The authors claim in introduction that Very recently, there was a first attempt to extend the concept of regularization paths to DNNs by means of treating the empirical loss and sparsity. However, dating back to 2020, the problem has been studied in OBProxSG where the multi-objective could be considered as a weighted sum of each individual target objective. [1] Orthant Based Proximal Stochastic Gradient Method for -Regularized Optimization, 2020. \n\nAnswer: **You are correct that the mentioned paper goes in a similar direction, and we have added it to our discussion on related work. However, similar to other works, this paper does not address the computation of the entire Pareto front. In this regard, our paper in fact is the very first that truly solves a multiobjective optimization problem in such high dimensions, both in the machine learning and the optimization literature.**\n\n2. Experiments are not sufficient:  I am concerned about the experiments. In particular, the paper studies multi-objective problems. However, in the experiment, only cross-entropy is considered as loss function. Where are other losses to form a real multi-objective problem? Meanwhile, the network and dataset are too simple to validate the efficacy.\n\nAnswer: **A second, much more complex experiment using CIFAR10 and a more complex model has been included in the updated version of our paper. In terms of the problem not being multi-objective, we disagree in the sense that the problem truly does have a Pareto front for the two tasks we consider. On the other hand, we agree that multiple main tasks are just as interesting, in particular when this results in more than two objectives. We have added a discussion on this aspect in the outlook of the paper. As the goal of this work is to provide the Pareto front for such a large-dimensional problem for the first time, we would rather leave this additional challenge for future work, and then also address the question of steering on higher-dimensional sets.**"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7218/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700691764255,
                "cdate": 1700691764255,
                "tmdate": 1700691764255,
                "mdate": 1700691764255,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rMZpOOgevb",
            "forum": "nrDRBhNHiB",
            "replyto": "nrDRBhNHiB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7218/Reviewer_4ykt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7218/Reviewer_4ykt"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a method to travel on the Pareto front for a two-objective optimization problem and shows how to apply it to compute the regularization path for deep NNs using the L1 penalty."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "An interesting approach that seems to work well in traveling on the Pareto front."
                },
                "weaknesses": {
                    "value": "- The title and the paper over-claims to work on multi-objective problems, but actually the method is only for two objective problems. For more than two objectives, it seems to me that the method will not explore the entire Pareto front.\n- A comparison is missing with the standard approach to obtain the regularization path that starts with no penalty and gradually increases the L1 penalty lambda and optimize the penalized loss for each value of lambda.\n- The L1 penalty is known to bias the weights and prevent the model from fitting the data well. In that respect non-convex penalties such as SCAD/MSC or the L0 penalty are preferred."
                },
                "questions": {
                    "value": "- Can the proposed method work on more than two objective functions?\n- How does the method compare din computation time and accuracy with the standard approach for obtaining the regularization path described above?\n- Can the proposed method work with the SCAD/MCP penalties?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7218/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698697852882,
            "cdate": 1698697852882,
            "tmdate": 1699636858667,
            "mdate": 1699636858667,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hB2X46F9UM",
                "forum": "nrDRBhNHiB",
                "replyto": "rMZpOOgevb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7218/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7218/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your detailed and thoughtful review of our paper.  Regarding your comments, suggestions, and questions, we have carefully considered them, and below are answers to your questions. In summary, we\n\n* added a second, much more complex example using the CIFAR10 dataset, and demonstrated that the advantages of our approach also hold in much higher dimensions\n* argue why our method is applicable to more complex examples than many alternative algorithms.\n* argue why our method tends to be more robust than standard training approaches.\n* increased the discussion on numerical details of our approaches and the numerical experiments.\n* added some references to related work.\n\nWe are convinced that this is the first work that allows for computing the entire front for non-convex problems of such large dimensions.\n\n**All changes in the updated PDF version of our paper have been marked in blue.**\n\n**Questions and Answers:**\n\n1. Can the proposed method work on more than two objective functions? \n\nAnswer: **We are convinced that our proposed method works on more than two objective functions. From a computational perspective, both the predictor and corrector steps should not be any more expensive. However, given that in more than two objective functions, the Pareto front is no longer a line, the task of selecting a predictor direction becomes more challenging. We have added some references and a short discussion to address this concern. However, the task of our paper was to compute the entire front of very high-dimensional, non-convex fronts for the first time, such that we would like to leave the explicit treatment of more objectives for future work. We now say so in the conclusion.**\n\n2. How does the method compare din computation time and accuracy with the standard approach for obtaining the regularization path described above? \n\nAnswer: **The comparison of our method with standard approach as you have described can be seen in the comparison of our method with weighted sum where the weighting parameter lambda takes the values 1 for sparsity only $\\ell^1$ norm, and lambda = 0 loss only. The results of the comparisons have been included as a table in the uploaded PDF version of our paper.**\n \n3.\tCan the proposed method work with the SCAD/MCP penalties?\n\nAnswer: **Thank you for bringing this up. We believe that including other penalties such as the SCAD (Smoothly Clipped Absolute Deviation) and MCP (Minimax Concave Penalty) which have non-convex shapes similar to the $\\ell^1$ norm as conflicting objectives to the loss function would be possible for our method. This could be the case when exploring more than two objective functions.**"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7218/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700691308792,
                "cdate": 1700691308792,
                "tmdate": 1700691308792,
                "mdate": 1700691308792,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GDPTOAsVBN",
            "forum": "nrDRBhNHiB",
            "replyto": "nrDRBhNHiB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7218/Reviewer_PWoo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7218/Reviewer_PWoo"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to extend the concept of regularization path from linear model to deep neural networks. In detail, the authors formulate the problem as a multi-objective composed of empirical loss as well as sparsity regularization. The authors propose an efficient approximation algorithm to recover the entire Pareto front as the regularization path. The proposed method is composed of two parts, the first part is a stochastic gradient descent combined with a proximal mapping. The other step is a multi-objective update step, which maps the solution after the gradient step back to the required Pareto set. To validate the proposed method, several experiments are conducted to validate the efficacy. In specific, the authors conduct experiments on MNIST a widely used dataset and validate the empirical Pareto front."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper proposes a novel view to solve the sparsity constrained problem in deep learning. The idea of using multi-objective to formulate the problem is novel. Additionally, an efficient predictor and corrector algorithms is proposed to find the Pareto front of the training process. The authors also put emphasis on the regularization path which can give a deeper understanding of the training process. The method is also validated with a widely used dataset."
                },
                "weaknesses": {
                    "value": "For the topic of extending regularization path from linear model to deep learning, several references[1][2][3] are missed.For [1], it utilizes Bregman Iteration to explore sparsity when training deep neural network where regularization can be generated during the iteration.  For [2], it proposed an efficient way to generate the reguarlization path from simple to complex via exploring inverse scale space.  For [3], it extends lasso from linear model to deep neural networks, regularization path is also discussed in this paper.\n\nIn addition, could the authors further explain the benefit of finding such Pareto front during training neural networks. \n\nFor the Multiobjective Proximal Gradient algorithm, for the setting training with only cross entropy loss, it seems that the formulation is similar to the Bregman Iteration as discussed in [4], could the authors give a further discussion?\n\nFor the experiment parts, the current experiments are not sufficient. Only showing the performance on MNIST is not persuasive. It would be better for the authors to add experiments on more datasets.\n\nFurthermore, could the authors illustrate whether the proposed method could be used to find the Pareto front of a series of Lottery Ticket Subnetworks[5].\n\n[1] Bungert, Leon, et al. \"A Bregman learning framework for sparse neural networks.\" The Journal of Machine Learning Research 23.1 (2022): 8673-8715.\n\n[2]Fu, Yanwei, et al. \"Exploring structural sparsity of deep networks via inverse scale spaces.\" IEEE Transactions on Pattern Analysis and Machine Intelligence 45.2 (2022): 1749-1765.\n\n[3] Lemhadri, Ismael, Feng Ruan, and Rob Tibshirani. \"Lassonet: Neural networks with feature sparsity.\" International conference on artificial intelligence and statistics. PMLR, 2021.\n\n[4] Osher, Stanley, et al. \"An iterative regularization method for total variation-based image restoration.\" Multiscale Modeling & Simulation 4.2 (2005): 460-489.\n\n[5]Frankle, Jonathan, and Michael Carbin. \"The lottery ticket hypothesis: Finding sparse, trainable neural networks.\" arXiv preprint arXiv:1803.03635 (2018)."
                },
                "questions": {
                    "value": "Please refer to weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7218/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699021862994,
            "cdate": 1699021862994,
            "tmdate": 1699636858547,
            "mdate": 1699636858547,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XTnHCUxVRp",
                "forum": "nrDRBhNHiB",
                "replyto": "GDPTOAsVBN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7218/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7218/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your detailed and thoughtful review of our paper.  Regarding your comments, suggestions and questions, we have carefully considered them. To address the mentioned weaknesses, we\n* Added a second, much more complex example using the CIFAR10 dataset, and a CNN architecture containing two convolutional layers and two linear layers with a total number of 4,742,546 parameters. We demonstrate that the advantages of our approach also hold in much higher dimensions.\n* Argue why our method is applicable to more complex examples than many alternative algorithms.\n\n* Argue why our method tends to be more robust than standard training approaches.\n\n* Included the discussion on numerical details of our approaches and the numerical experiments.\n\n* Added the references you mentioned to the related work section of our paper. \n\nWe believe that our method shows some advantages over the mentioned references. The mentioned references all explore the goal of inducing sparsity into DNNs. In contrast, the goal of our approach is not only to enforce sparsity but also to ensure a good trade-off between sparsity and loss, hence the concept of multiobjective optimization and the goal to identify the entire Pareto front. With our predictor-corrector (CM) method, this Pareto set of optimal compromises between sparsity and loss is obtained giving a decision maker series of points to choose from at the same time understanding the tradeoff between both objectives. We are convinced that this is the first work that allows for computing the entire front for non-convex problems of such large dimensions. \n\n**All changes in the updated PDF version of our paper have been marked in blue.**"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7218/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700690354452,
                "cdate": 1700690354452,
                "tmdate": 1700690574213,
                "mdate": 1700690574213,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AJtXtQZPFO",
                "forum": "nrDRBhNHiB",
                "replyto": "XTnHCUxVRp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7218/Reviewer_PWoo"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7218/Reviewer_PWoo"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the response, but I will keep my score."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7218/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700725939249,
                "cdate": 1700725939249,
                "tmdate": 1700725939249,
                "mdate": 1700725939249,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "SDcOU9WWb8",
            "forum": "nrDRBhNHiB",
            "replyto": "nrDRBhNHiB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7218/Reviewer_oW7r"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7218/Reviewer_oW7r"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a multiobjective optimization method for non-smooth problem with efficient predictor and corrector step by approximation of Pareto front. It can extend the regularization paths from linear models to nonlinear high-dimensional deep learning models. The method is used to train neural network starting sparse, which can help avoid overfitting by early stop.\n\nThis is the comments from the fast reviewer."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Strength:\n1. The idea using approximation of Pareto front to improve multiobjective optimization on non-smooth problem to extend regularization path to high-dimensional nonlinear deep learning model is interesting and effective.\n\n2. The results shows the method can help avoid the overfitting problem and obtain models with low loss and sparsity.\n\n3: The paper provides thorough related background information in Section 3, making it easy to follow.\n\n4: The predictor-corrector scheme is innovative, avoiding clustering around the unregularized and very sparse solutions, in comparison with weighted sum approach."
                },
                "weaknesses": {
                    "value": "1: You highlight the efficiency of your method in several aspects, such as reducing the computational expense by avoiding the computation of the full gradient in Section 4, and also provide your experimental settings. While your experimental settings are detailed, the inclusion of metrics on computational time would significantly enhance the evaluation of your method's efficiency.\n\n2: You claim that the algorithm shows a performance that is suitable for high-dimensional learning problems, may should be supported by more results extended to deeper DNNs including non-linear layers, and more complex datasets just as you mention in Section 5.\n\n3: Have you compared the performance of your method with existing regularization path generation methods?\n\n4: One of your key merits is avoiding overfitting, and it would strengthen the claim to conduct additional experiments."
                },
                "questions": {
                    "value": "1: You highlight the efficiency of your method in several aspects, such as reducing the computational expense by avoiding the computation of the full gradient in Section 4, and also provide your experimental settings. While your experimental settings are detailed, the inclusion of metrics on computational time would significantly enhance the evaluation of your method's efficiency.\n\n2: You claim that the algorithm shows a performance that is suitable for high-dimensional learning problems, may should be supported by more results extended to deeper DNNs including non-linear layers, and more complex datasets just as you mention in Section 5.\n\n3: Have you compared the performance of your method with existing regularization path generation methods?\n\n4: One of your key merits is avoiding overfitting, and it would strengthen the claim to conduct additional experiments.\n\n5. It seems that finding the initial point and using randomly multi-start to find components of Pareto front cost a lot. With more complex network structure, it is possible that finding the initial point be a bottleneck which is hard to accelerate.\n\n6. With stochastic gradient, it is possible that the initial points are different between experiments. Will different initial points affect the performance or the iteration times of the algorithm?\n\n7. What is the method to decide when should the training stop before when the slope of the Pareto front becomes too steep. In practical settings, it seems to complex and tricky to achieve the early stopping.\n8. One missing related reference: DessiLBI: Exploring Structural Sparsity on Deep Network via Differential Inclusion Paths. ICML2020"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7218/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699165149617,
            "cdate": 1699165149617,
            "tmdate": 1699636858382,
            "mdate": 1699636858382,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lp59hnWmBx",
                "forum": "nrDRBhNHiB",
                "replyto": "SDcOU9WWb8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7218/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7218/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your detailed and thoughtful review of our paper.  Regarding your comments, suggestions, and questions, we have carefully considered them, and below are answers to your questions. In summary, we have; \n* Added a second, much more complex example using the CIFAR10 dataset, and demonstrated that the advantages of our approach also hold in much higher dimensions.\n* Argue why our method is applicable to more complex examples than many alternative algorithms.\n* Argue why our method tends to be more robust than standard training approaches.\n* Increased the discussion on the numerical details of our approaches and the numerical experiments.\n* Added some references to related work.\nWe are convinced that this is the first work that allows for computing the entire front for non-convex problems of such large dimensions.\n\n**All changes in the updated PDF version of our paper have been marked in blue.**\n\n**All the answers below are numbered according to the question number asked.**\n\n## ANSWERS\n1. We have included a table detailing the metrics and experimental settings for our continuation method (CM) and weighted sum (WS) in the updated PDF version of our paper.\n\n2.    We have implemented both our continuation method (CM) and the weighted sum (WS) method on CIFAR10 using a CNN model architecture containing two convolutional layers and two fully connected linear layers with a total number of 4,742,546 parameters. We demonstrate that our CM still generalizes better than the WS and provides a set of well distributed points on the front. Updated results of our new experiments can be seen in the updated PDF version of our paper and we will be adding an appendix section in the final paper submission.\n\n3. Yes, we implemented methods such as the early stopping and grid search (tuning of different hyperparameter values learning rate, weight decay). We observed that for both CM and WS, these methods tend to have similar effects. E.g., early stopping reduces the accuracy of both methods, and different values of the learning rate and weight decay tend to affect the rate of convergence to the Pareto front for both methods. We will be including a detailed discussion on these in the appendix section which will be added in the final submission.\n\n4. By having added the CIFAR10 example, we believe that we can now make a much stronger case for our claim that we can avoid overfitting.\n\n5. Finding the initial point is approximately as expensive as training a neural network (NN) using a standard single-objective method, probably a little cheaper as we do not need to thrive for the lowest possible loss. However, the consecutive predictor and corrector steps remain very cheap even for very large architectures such as our CIFAR10 example, such that the benefit of computing the front comes at little extra cost.\n\n6. We have made extensive tests regarding multiple runs. Since our method does not aim for the smallest loss when finding the initial point on the front (cf. our response to question 5), we observe only very small variations in terms of repeated experiments. We will add more details to this in the appendix section of the final paper which will be added in the final submission.\n\n7. The reviewer raises a good point here. The focus of our paper was first to demonstrate that we can obtain the entire Pareto front even for extremely high-dimensional problems. The question of early stopping is of vital importance, though, and we will add a brief discussion in the appendix section of the final paper submission, where we argue that a small validation set should suffice to detect the turn-around point in terms of the out-of-sample loss.\n\n8. Thank you for the reference. We have added it to our paper and included it in the discussion on related work."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7218/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700689706917,
                "cdate": 1700689706917,
                "tmdate": 1700689706917,
                "mdate": 1700689706917,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]