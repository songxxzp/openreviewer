[
    {
        "title": "Theoretical Analysis of Robust Overfitting for Wide DNNs: An NTK Approach"
    },
    {
        "review": {
            "id": "lL4aVeQUuI",
            "forum": "1op5YGZu8X",
            "replyto": "1op5YGZu8X",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2578/Reviewer_edb8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2578/Reviewer_edb8"
            ],
            "content": {
                "summary": {
                    "value": "This paper applied neural tagent kernel techniques into adversarial training setting and proves that adversarial trained DNN can be approximated by a linearized DNN. For square loss it reveals a AT degreneration phenomena so that explains robust overfitting. The paper designed an algorithm AdvNTK based on the theoretical results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "I like the idea of analyzing robust overfitting from the time-dependent regularizer matrix that derived from adversarial NTK."
                },
                "weaknesses": {
                    "value": "Adversarial trained DNN can be approximated by linearized DNN only when the perturbation size considered is small, however, the paper seems not give a quantitive result regarding how small it should be. I understand that the author using attack learning rate for a total of time S. Yet it\u2019s unclear how large the attack rate is, as if the attack learning rate is infinitesimal, then the perturbation size is so small, and it\u2019s unclear for me whether such adversarial training algorithm has any generalization guarantee in terms of robustness.\n\nMoreover, I\u2019m not sure why it\u2019s interesting to study adversarial training within the NTK regimes, as in [1] show when the network is close to initialization, there\u2019s no robust network at all.\n\nIn the proof of theorem 2 the dependency of polyt seems odd to me, as polyt depends on the norm of W_t, which can go to infinity. I understand that for neural tagent kernel the network width does not move far from initialization so you can argue norm of W_t is bounded,  as theorem Lemma C.5, yet this theorem\u2019s proof also depends on polyt. Therefore the current presentation of omiting dependency on the norm of W_t, nor of x_t seems to hide something in the proof.\n\nWhat\u2019s the relationship between the experiment vs. theorems? How does the experiment validate theorems? The robust accuracy of both SVHN and CIFAR10 seems extremely lower than normal adversarial training. Is it because of the network architecture? Why cannot use standard architecture such as ResNet that people commonly use in practice? Can the author provide any simple experiment to confirm the AT degeneration phenomenon? In fact, it\u2019s unclear to me the difference between this wording vs robust overfitting, and if they are the same thing then there\u2019s no need to use a different paraphrase.\n\nIn terms of the writing, I think the author should put more discussion on the theorems, explain, and provide intuition.\n\n[1] Wang, Y., Ullah, E., Mianjy, P., & Arora, R. (2022). Adversarial robustness is at odds with lazy training.\u00a0Advances in Neural Information Processing Systems,\u00a035, 6505-6516."
                },
                "questions": {
                    "value": "This paper focuses on regression setting with squared loss. I\u2019m wondering if the idea can be generalized to classification setting."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2578/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2578/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2578/Reviewer_edb8"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2578/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698508067446,
            "cdate": 1698508067446,
            "tmdate": 1700607341939,
            "mdate": 1700607341939,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lB4Leuv1jY",
                "forum": "1op5YGZu8X",
                "replyto": "lL4aVeQUuI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2578/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2578/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer edb8 (1/3)"
                    },
                    "comment": {
                        "value": "Thank you for your thorough review and constructive comments. All your concerns have been carefully responded below. The manuscript is carefully revised accordingly. We sincerely hope our responses fully address your questions.\n\n**Q1.1:** *Adversarial trained DNN can be approximated by linearized DNN only when the perturbation size considered is small, however, the paper does not give a quantitative result regarding how small it should be.*\n\n**A1.1:** We respectfully argue that the approximation based on linearized DNNs in AT does not require the perturbation size to be small. Instead, such an approximation mainly depends on if the network widths are large enough. The only requirement for the perturbation size is that the learning rate should be continuously differentiable (see Assumption 3 on Page 6). We have added Remark 2 in Section 4.3 on Page 6 in the revised version to clarify this result, please kindly refer for more details.\n\n**Q1.2:** *I understand that the author using attack learning rate for a total of time S. Yet it's unclear how large the attack rate is, as if the attack learning rate is infinitesimal, then the perturbation size is so small, and it's unclear for me whether such adversarial training algorithm has any generalization guarantee in terms of robustness.*\n\n**A1.2:** We currently have no generalization guarantee in terms of robustness as well as the scale of the learning rate matrix $\\boldsymbol{\\upeta}(t)$. Nevertheless, we respectfully note that all the theoretical results in Section 4 do not depend on the scale of the learning rate matrix $\\boldsymbol{\\upeta}(t)$. The only requirement for the learning rate matrix $\\boldsymbol{\\upeta}(t)$ is to satisfy Assumption 3 (in Section 4.3 on Page 6). Therefore, one can choose any large enough $\\boldsymbol{\\upeta}(t)$ to perform an effective AT as long as Assumption 3 is fulfilled.\n\n**Q2:** *Moreover, I'm not sure why it's interesting to study adversarial training within the NTK regimes, as in [r1] show when the network is close to initialization, there\u2019s no robust network at all.*\n\n**A2:** Thanks. This is a good question. In short, the setting we studied is different from that in [r1] and thus it is of merit to study DNN in AT with NTK. Detailed reasons will then be explained. We have also added a discussion about [r1] in the revised version (see Section 2 on Page 2).\n\nFirstly, we would like to clarify that the setting we studied in our paper is more challenging and realistic than that in [r1]. Specifically, the two settings mainly differ in the following two aspects:\n- The results in [r1] only work for a two-layer neural network where the second-layer parameter is frozen, while our results work for neural networks with any (finite) number of layers and fully trainable parameters.\n- The results in [r1] only work for input data $x$ that is from the unit sphere $\\mathbb{S}^{d-1}$, while our results work for any $x \\in \\mathbb{R}^d$.\n\nAs a result, although it has been well studied that in the setting of [r1], there is no robust network in the NTK regime, it remains unknown what is the behavior of an adversarially trained deep neural network in the NTK regime. Therefore, studying DNN in AT with NTK theory is still of great importance.\n\n**Q3:** *In the proof of theorem 2 the dependency of $poly_t$ seems odd to me, as $poly_t$ depends on the norm of $W_t$, which can go to infinity. I understand that for a neural tangent kernel the network width does not move far from initialization so you can argue the norm of $W_t$ is bounded, as in Lemma C.5, yet this Lemma\u2019s proof also depends on $poly_t$. Therefore the current presentation of omiting dependency on the norm of $W_t$, nor of $x_t$ seems to hide something in the proof.*\n\n**A3**: Firstly, we respectfully note that $\\mathrm{Poly}\\_t$ depends on $\\frac{1}{\\sqrt{n\\_{l-1}}} || W^{(l)}\\_t ||\\_2$, rather than only $||W^{(l)}\\_t||\\_2$.\n\nBesides we did not argue that $|| W^{(l)}\\_t ||\\_2$ is bounded. Instead, we prove that as the network widths $n\\_0,\\cdots,n\\_L$ go to infinity, $\\frac{1}{\\sqrt{n\\_{l-1}}}||W^{(l)}\\_t||\\_2$ will be bounded while $||W^{(l)}\\_t||\\_2$ can go to infinity.\n\nMoreover, our proofs in Appendix C did not omit the dependency of $\\mathrm{Poly}\\_t$ on $\\frac{1}{\\sqrt{n\\_{l-1}}}||W^{(l)}\\_t||\\_2$, $\\frac{1}{\\sqrt{n\\_l}} ||x^{(l)}\\_t(\\boldsymbol{\\mathrm{x}}\\_{,t,S})||\\_2$, and $\\frac{1}{\\sqrt{n\\_l}} \\|h^{(l)}\\_t(\\boldsymbol{\\mathrm{x}}\\_{,t,S})\\|_2$. $\\mathrm{Poly}\\_t$ is the abbreviation of any deterministic polynomial with finite degree and finite positive constant coefficients, considering the aforementioned l2-norm terms. We use this abbreviation only for the sake of simplifying writing.\n\nFurthermore, the proof of Lemma C.5 relies on the dependency of $\\mathrm{Poly}_t$ on these l2-norm terms. Specifically, the construction of the new polynomial $P(A_t)$ relies on the aforementioned dependency. Please kindly refer to the bottom of Page 24 in the revised manuscript for more details."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2578/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700205114349,
                "cdate": 1700205114349,
                "tmdate": 1700205177265,
                "mdate": 1700205177265,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7IJrFSvSmk",
                "forum": "1op5YGZu8X",
                "replyto": "lL4aVeQUuI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2578/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2578/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer edb8 (3/3)"
                    },
                    "comment": {
                        "value": "**Q4.3:** *Why cannot use standard architecture such as ResNet that people commonly use in practice?*\n\n**A4.3:** Thanks for your question. The reason that we did not use ResNet and other standard architectures in our experiments is that most of these architectures leverage global average pooling (GAP). However, as explained in A4.2, we currently cannot efficiently calculate NTK models that adopt GAP layers under AT. We originally intended to leave these architectures that adopted GAP for future research.\n\nNevertheless, we have conducted additional experiments with ResNet-18 by replacing the GAP layer with the Flatten layer. The experiment results are shown as the following Table r1.\n\n\n| Dataset               | Radius | AT             | NTK            | Adv-NTK (Ours) |\n| --------------------- | ------ | -------------- | -------------- | -------------- |\n| CIFAR-10 (Subset 12K) | 4/255  | 28.01$\\pm$0.79 | 17.09$\\pm$0.25 | 28.43$\\pm$0.27 |\n| CIFAR-10 (Subset 12K) | 8/255  | 15.11$\\pm$0.65 | 4.07$\\pm$0.13  | 21.46$\\pm$0.47 |\n| SVHN (Subset 12K)     | 4/255  | 48.10$\\pm$1.08 | 24.18$\\pm$0.34 | 32.70$\\pm$0.26 |\n| SVHN (Subset 12K)     | 8/255  | 30.36$\\pm$1.92 | 10.05$\\pm$0.30 | 13.96$\\pm$0.16 |\n\n*Table r1: Robust test accuracy (%) of ResNet-18 (without GAP layer) trained with different methods.*\n\nComparing Table r1 with our original experiment results (see Table 1 on Page 9 and Table 4 on Page 44), we can find that the robustness performance of the ResNet-18 trained with Adv-NTK is even worse than that of MLP-x/CNN-x. We deduce that this is mainly due to the fact that current ResNet-18 NTK models did not adopt the GAP layer. We will add a more comprehensive empirical analysis of ResNet and a discussion on its practicality in the next version of our manuscript. \n\n**Q4.4:** *Can the author provide any simple experiment to confirm the AT degeneration phenomenon?*\n\n**A4.4:** Thanks for your suggestion. We have added additional experiments and plotted the results in Fig.1 (on Page 9) and Fig.2 (on Page 44). Specifically, we plotted the robust test accuracy of finite-width MLP-5/CNN-5 along vanilla AT. Please kindly refer to Fig.1 and Fig.2 and corresponding discussions along with them.\n\nBesides, we would also like to highlight that [r6] has already empirically confirmed the AT degeneration/robust overfitting (note that \"empirical\" AT degeneration can be seen as robust overfitting, see **A4.5**) in 2020. Please kindly refer to [r6] for a more comprehensive background knowledge of robust overfitting.\n\n**Q4.5:** *In fact, it\u2019s unclear to me the difference between this wording (AT degeneration) vs robust overfitting, and if they are the same thing then there\u2019s no need to use a different paraphrase.*\n\n**A4.5:** Please note that the two wordings are different. Specifically: (1) \"robust overfitting\" refers to an empirical phenomenon that currently lacks explanations. (2) \"AT degeneration\" refers to a theoretical phenomenon that characterizes the cause of \"robust overfitting\", i.e., \"a DNN in long-term AT will graduate degenerate to that obtained without AT\".\n\n**Q5:** *In terms of the writing, I think the author should put more discussion on the theorems, explain, and provide intuition.*\n\n**A5:** Thanks for your suggestion. We have revised the manuscript to improve its readability as follows:\n- In Section 1 (on Page 1), we explained our motivation for using NTK theory to study robust overfitting.\n- In Section 1 (on Page 2), we added a new paragraph to summarize the main contributions of this paper.\n- In Section 4.3 (on Page 6), we added Remark 2 to highlight that Theorem 2 depends mainly on network widths but not adversarial perturbation scales.\n\nThe manuscript will be continuously revised accordingly in the next version due to time limitation.\n\n**Q6:** *This paper focuses on regression setting with squared loss. I\u2019m wondering if the idea can be generalized to classification settings.*\n\n**A6:** Thanks for your question. The regression setting with squared loss can be generalized to the classification by simply replacing the categorical label-encoding with the one-hot label-encoding during model training. Moreover, we respectfully note that our empirical analysis in Section 6 was conducted on the image classification task.\n\n**References:**\n\n[r1] Wang et al. \u201cAdversarial robustness is at odds with lazy training.\u201d NeurIPS 2022.\n\n[r2] Arora et al. \"Harnessing the power of infinitely wide deep nets on small-data tasks.\" ICLR 2020.\n\n[r3] Han et al. \"Fast neural kernel embeddings for general activations.\" NeurIPS 2022.\n\n[r4] Arora et al. \"On exact computation with an infinitely wide neural net.\" NeurIPS 2019.\n\n[r5] Novak et al. \"Fast finite width neural tangent kernel.\" ICML 2022.\n\n[r6] Rice et al. \"Overfitting in adversarially robust deep learning.\" ICML 2020."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2578/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700205493536,
                "cdate": 1700205493536,
                "tmdate": 1700236916489,
                "mdate": 1700236916489,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PtB9ot6ZWy",
                "forum": "1op5YGZu8X",
                "replyto": "lL4aVeQUuI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2578/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2578/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up on rebuttal"
                    },
                    "comment": {
                        "value": "Dear Reviewer edb8,\n\nThank you again for your valuable comments! We have carefully considered your comments and have provided our responses.\n\nPlease let us know if our replies have satisfactorily addressed your concerns. Please do not hesitate to let us know if you have any further questions or if you require any additional clarification.\n\nThank you very much!"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2578/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700400906923,
                "cdate": 1700400906923,
                "tmdate": 1700400906923,
                "mdate": 1700400906923,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MvnukhyD28",
                "forum": "1op5YGZu8X",
                "replyto": "Ohso2tegfD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2578/Reviewer_edb8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2578/Reviewer_edb8"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Author,\n\nThank you for your response which addresses most of my questions. I'll increase the score accordingly. Please add the discussion as well as the new results to the final manuscript."
                    }
                },
                "number": 26,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2578/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700607318058,
                "cdate": 1700607318058,
                "tmdate": 1700607318058,
                "mdate": 1700607318058,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "c2yWCY59S8",
            "forum": "1op5YGZu8X",
            "replyto": "1op5YGZu8X",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2578/Reviewer_4ePw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2578/Reviewer_4ePw"
            ],
            "content": {
                "summary": {
                    "value": "\u2022\tThis paper theoretically explores the robust overfitting of Adversarial Training (AT). Specifically, it demonstrates that Deep Neural Networks (DNNs) trained with AT can be represented from the perspective of Neural Tangent Kernel (NTK). The paper further formulates the dynamics of Adversarial Training. Based on this formulation, it explains the reasons behind the occurrence of robust overfitting and proposes an algorithm, termed ADV-NTK, that can prevent robust overfitting in a manner akin to early stopping."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "\u2022\tThe paper tackles a critical and pressing issue: Overfitting in Adversarial Training.\n\n\u2022\tThe contribution of paper is based on a theoretically solid proof. They also offer a detailed and meticulous explanation enhancing the understandings."
                },
                "weaknesses": {
                    "value": "\u2022\tThe paper is written based on the logical flow of formulation and comparably not focusing on the motivation of the paper. For better readability, it seems to be a need for more appeal on how important task the paper tries to solve and what is the contributions of the paper.\n\n\u2022\tIn context of DNN-NTK, they replace the constrained-spaces condition with an additional learning rate term to control the strength of adversarial examples. (in Equation 7) However, unlike many attack mechanisms, it doesn't efficiently find a significant direction of attack. Despite this, does believe that the derived formula's proof is still not too loose?\n\n\u2022\tThe additional experiments are needed. In specific, it would be better to empirically verify 1) whether theoretically proven properties actually happen similarly in real-world dataset and 2) how much the robust overfitting problem has been addressed with the proposed Adv-NTK. For instance, tracking the performance (or loss) trend across iterations between vanilla AT and Adv-NTK."
                },
                "questions": {
                    "value": "\u2022\tIn the above part.\n\n[Overall]\n\u2022\tIf the authors provide further experiments on Adv-NTK and supplementary explanations for the justification for learning rates, I agree that this paper is accepted."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2578/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2578/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2578/Reviewer_4ePw"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2578/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698634558860,
            "cdate": 1698634558860,
            "tmdate": 1700576076686,
            "mdate": 1700576076686,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UggaKtddH5",
                "forum": "1op5YGZu8X",
                "replyto": "c2yWCY59S8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2578/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2578/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer 4ePw"
                    },
                    "comment": {
                        "value": "Thank you for your constructive comments and kind support! All your concerns have been carefully responded below. The manuscript is carefully revised accordingly. We sincerely hope our responses fully address your questions.\n\n**Q1:** *The paper is written based on the logical flow of formulation and comparably not focusing on the motivation of the paper. For better readability, there seems to be a need for more appeal on how important the task the paper tries to solve and what are the contributions of the paper.*\n\n**A1:** Thanks for your suggestion. We have revised Section 1 according to your suggestion as follows:\n- In the first paragraph, we further explained why studying robust overfitting is important.\n- In the third paragraph, we explained the motivation for using NTK theory to study robust overfitting.\n- We added a new paragraph that summarizes the main contribution of this paper.\n\nPlease also kindly refer to Section 1 for more details.\n\n**Q2:** *In the context of DNN-NTK, they replace the constrained-spaces condition with an additional learning rate term to control the strength of adversarial examples (in Eq.7). However, unlike many attack mechanisms, it doesn't efficiently find a significant direction of attack. Despite this, do you believe that the derived formula's proof is still not too loose?*\n\n**A2:** We respectfully argue that our gradient flow-based attack mechanism in Eq.(7) indeed finds significant direction to perform adversarial attacks, and thus our theoretical results are not loose compared with real-world settings.\n\nSpecifically, the mechanism in Eq.(7) is very similar to projected gradient descent (PGD), a very common adversarial attack method in the real-world setting (see Appendix F.1). The only difference is that our mechanism based on Eq.(7) leverages a learning rate $\\eta_i(t)$ and a total search time $S$ to control the adversarial strength (i.e., the strength of the ability to make models misbehave), while PGD uses a perturbation radius $\\rho$ to control the adversarial strength. As long as $\\eta_i(t) S$ is set to be large enough, our mechanism can find adversarial examples to effectively attack targeted DNNs, just like the PGD attack. Please also kindly refer to the first two paragraphs in Section 4.1 on Page 4 for a detailed discussion.\n\n**Q3:** *Additional experiments are needed. In specific, it would be better to empirically verify: (1) Whether theoretically proven properties actually happen similarly in real-world dataset. (2) How much the robust overfitting problem has been addressed with the proposed Adv-NTK. For instance, tracking the performance (or loss) trend across iterations between vanilla AT and Adv-NTK.*\n\n**A3:** Thanks for your suggestion. We have added additional experiments and plotted the results in Fig.1 (on Page 9) and Fig.2 (on Page 44). Specifically, we plotted the robust test accuracy of finite-width MLP-5/CNN-5 along vanilla AT. We also plotted the accuracy of infinite-width DNNs learned by NTK and Adv-NTK. The general finding is that: although Adv-NTK can achieve comparable or higher performance than the final model obtained by AT in some cases, it could not beat the best model along vanilla AT. This suggests there are still theoretical gaps between finite-width and infinite-width DNNs that need to be fulfilled.\n\nBesides, we would also like to highlight that our original experiment results in Table 1 (on Page 9) and Table 4 (on Page 44) have already empirically verified the raised questions. Specifically, on one hand, the NTK method in our experiments will learn infinite-width DNNs defined as Eq.(4) (in Section 3 on Page 3). On the other hand, as discussed in Section 5.1 (on Page 6), a long-term AT will result in an infinite-width DNN degenerate to the limit of Eq.(3) (in Section on Page 3), which however is exactly Eq.(4). Therefore, if Adv-NTK can learn infinite-width DNNs that are of better performance than DNNs learned by NTK (i.e., constructed based on Eq.(4)), then it is enough to illustrate that Adv-NTK can mitigate robust overfitting. And our results in Tables 1 and 4 indeed show that Adv-NTK performs better than NTK."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2578/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700204469090,
                "cdate": 1700204469090,
                "tmdate": 1700204469090,
                "mdate": 1700204469090,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zShTikBZ7j",
                "forum": "1op5YGZu8X",
                "replyto": "c2yWCY59S8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2578/Reviewer_4ePw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2578/Reviewer_4ePw"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Authors,\n\nThank you for your kind response. \nMost of my concerns are addressed by your response. I will raise my score to Accept.\n\nSincerely,\n\nReviewer 4ePw"
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2578/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700576033231,
                "cdate": 1700576033231,
                "tmdate": 1700659539309,
                "mdate": 1700659539309,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1rvf6xFXfQ",
            "forum": "1op5YGZu8X",
            "replyto": "1op5YGZu8X",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2578/Reviewer_s7R7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2578/Reviewer_s7R7"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the problem of robust overfitting in adversarial training using the Neural Tangent Kernel (NTK). First, the authors extend the theoretical framework of NTK to adversarial training, introducing an adversarial regularization kernel and demonstrating that adversarially trained DNNs can be well approximated by their linearized DNNs. They then derive the closed-form dynamics of adversarial training for the linearized DNN and uncover a phenomenon called adversarial training degeneration: prolonged adversarial training leads to the degradation of the wide DNN to a state similar to that of a DNN with normal training, which results in robust overfitting. Based on these theoretical findings, the authors propose an adversarial training algorithm called Adv-NTK for infinite-width DNNs, and experimental results show that it can enhance the robustness of infinite-width DNNs to a level comparable to that of finite-width DNNs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper provides a linear model-level explanation of the adversarial training degeneration phenomenon. The designed Adv-NTK algorithm enables infinite-width NTK models to achieve robustness similar to MLPs through adversarial training."
                },
                "weaknesses": {
                    "value": "1: This paper focuses on linearized models but lacks a detailed explanation of why linearization approximation is applicable. Theorem 1 demonstrates the convergence of two kernels for initialization. However, the properties that remain constant over time appear to be submerged in Appendix C.2. It is recommended that the authors include an informal presentation of the results from Appendix C.2 between Theorems 1 and 2 and provide corresponding discussions.\n\n2: Section 5.1 lacks a discussion regarding the impact of $\\eta S$ on the adversarial training degeneration phenomenon."
                },
                "questions": {
                    "value": "Are the results in Section 4 of this paper valid for adversarial training of any intensity? Intuitively, if the intensity of adversarial training is too high, some of the results in Chapter 4 may not be valid."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2578/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2578/Reviewer_s7R7",
                        "ICLR.cc/2024/Conference/Submission2578/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2578/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698779880075,
            "cdate": 1698779880075,
            "tmdate": 1700511518349,
            "mdate": 1700511518349,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "P22pbwe33n",
                "forum": "1op5YGZu8X",
                "replyto": "1rvf6xFXfQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2578/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2578/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Request for clarifying \"intensity of adversarial training\""
                    },
                    "comment": {
                        "value": "Dear Reviewer s7R7,\n\nThanks for your detailed review. We are currently working on the rebuttal to carefully answer your questions.\n\nTo fully address all your concerns, we would like to ask what do you mean by\n**\"intensity of adversarial training\"**? Does it mean **the scale of the learning rate matrix $\\boldsymbol{\\upeta}(t)$**, or **the total perturbation time $S$**, or **both of them**?\n\nA clarification of the term \"intensity of adversarial training\" would be very helpful for us to prepare our answers!\n\nThanks & sincerely,\n\nThe Authors"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2578/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699908817076,
                "cdate": 1699908817076,
                "tmdate": 1699908817076,
                "mdate": 1699908817076,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "N1g5IVn7k5",
                "forum": "1op5YGZu8X",
                "replyto": "P22pbwe33n",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2578/Reviewer_s7R7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2578/Reviewer_s7R7"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Authors,\n\nThanks for your response.\n\nI am concerned about whether the results still hold when the product $\\eta(t) S$ (or the integral $\\int\\eta(t)dt$) is large. I don\u2019t seem to see any limitations or assumptions in Theorem 2 about this (if I overlooked something, please tell me).\n\nThanks,\n\nReviewer s7R7"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2578/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699914069637,
                "cdate": 1699914069637,
                "tmdate": 1699914069637,
                "mdate": 1699914069637,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iWJhXS90ss",
                "forum": "1op5YGZu8X",
                "replyto": "1rvf6xFXfQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2578/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2578/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer s7R7"
                    },
                    "comment": {
                        "value": "Thank you for your thorough review and constructive comments. All your concerns have been carefully responded below. The manuscript is carefully revised accordingly. We sincerely hope our responses fully address your questions.\n\n**Q1.1:** *This paper focuses on linearized models but lacks a detailed explanation of why linearization approximation is applicable.*\n\n**A1.1:** Firstly, the idea that \"large network widths lead to model linearization\" comes from (vanilla) NTK theory and has already been theoretically proved in standard training (for example, see [r1, r2, r3, r4]). We respectively note that we have explained this linearization idea in both Section 2 (on Page 2) and Section 3 (on Page 3).\n\nBesides, in this paper, we generalized the linearization idea in vanilla NTK theory to the setting of AT and justified why \"wide DNNs can still be linearized in AT\" with rigorous proofs. We have added Remark 2 in Section 4.3 on Page 6 in the revised version to clarify this contribution, please also kindly refer for more details.\n\n**Q1.2:** *Theorem 1 demonstrates the convergence of two kernels for initialization. However, the properties that remain constant over time appear to be submerged in Appendix C.2. It is recommended that the authors include an informal presentation of the results from Appendix C.2 between Theorems 1 and 2 and provide corresponding discussions.*\n\n**A1.2:** Thanks for your suggestion. We have carefully revised our manuscript accordingly. Specifically, we made the following revisions:\n\n- We have added a new Appendix C.1 to explain the proof skeleton of Theorem 2.\n- We have informally presented the main results of Appendix C.3 (i.e., the old Appendix C.2) in Appendix C.1. They are Theorems C.1 and C.2, which illustrate that the two kernels remain constant during AT.\n\nPlease also kindly refer to Appendix C.1 on Page 17 for more details.\n\n**Q2:** *Section 5.1 lacks a discussion regarding the impact of $\\eta S$ on the adversarial training degeneration phenomenon.*\n\n**A2:** Thanks for your suggestion. We have added an additional discussion on the behavior of DNNs in long-term AT when large adversarial perturbations are present. Please kindly refer to Appendix E.2 on Page 42 for details.\n\n**Q3:** *Are the results in Section 4 of this paper valid for adversarial training of any intensity? Intuitively, if the intensity of adversarial training is too high, some of the results in Section 4 may not be valid.*\n\n**Clarification:** *I am concerned about whether the results still hold when the product $\\boldsymbol{\\upeta}(t)S$ (or the integral $\\int \\boldsymbol{\\upeta}(t) \\mathrm{d}t$) is large. I don\u2019t seem to see any limitations or assumptions in Theorem 2 about this (if I overlooked something, please tell me).*\n\n**A3:** Thanks for your question and clarification. Yes, the results in Section 4 are valid for adversarial training of any intensity. Specifically, our theoretical result that \"a wide DNN under AT behaves like a linearized DNN\" does not depend on the scale of $\\boldsymbol{\\upeta}(t)S$. Instead, it mainly depends on if the network widths are large enough **(This is one of the main reasons why our theoretical result is exciting)**. The only requirement for $\\boldsymbol{\\upeta}(t)S$ is that the learning rate matrix $\\boldsymbol{\\upeta}(t)$ should be continuously differentiable (see Assumption 3 on Page 6).\n\nWe have added Remark 2 in Section 4.3 on Page 6 in the revised version to highlight this contribution, please kindly refer for more details.\n\n**References:**\n\n[r1] Jacot et al. \"Neural tangent kernel: Convergence and generalization in neural networks.\" NeurIPS 2018.\n\n[r2] Lee et al. \"Wide neural networks of any depth evolve as linear models under gradient descent\". NeurIPS 2019.\n\n[r3] Arora et al. \"On exact computation with an infinitely wide neural net.\" NeurIPS 2019.\n\n[r4] Hron et al. \"Infinite attention: NNGP and NTK for deep attention networks.\" ICML 2020."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2578/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700204149254,
                "cdate": 1700204149254,
                "tmdate": 1700236823875,
                "mdate": 1700236823875,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vXcngerjSA",
                "forum": "1op5YGZu8X",
                "replyto": "1rvf6xFXfQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2578/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2578/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up on rebuttal"
                    },
                    "comment": {
                        "value": "Dear Reviewer s7R7,\n\nThank you again for your valuable comments! We have carefully considered your comments and have provided our responses.\n\nPlease let us know if our replies have satisfactorily addressed your concerns. Please do not hesitate to let us know if you have any further questions or if you require any additional clarification.\n\nThank you very much!"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2578/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700400878514,
                "cdate": 1700400878514,
                "tmdate": 1700400878514,
                "mdate": 1700400878514,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8Og8GIEDAn",
                "forum": "1op5YGZu8X",
                "replyto": "vXcngerjSA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2578/Reviewer_s7R7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2578/Reviewer_s7R7"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Authors,\n\nThank you for your response, which addressed most of my concerns. I will raise my score accordingly. It would be better if you could refer to the additional discussions provided in the appendix within the relevant sections of the main text. \n\nThank you,\n\nReviewer s7R7"
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2578/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700511502315,
                "cdate": 1700511502315,
                "tmdate": 1700511502315,
                "mdate": 1700511502315,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HKwygi9PSy",
            "forum": "1op5YGZu8X",
            "replyto": "1op5YGZu8X",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2578/Reviewer_gmGK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2578/Reviewer_gmGK"
            ],
            "content": {
                "summary": {
                    "value": "The paper explores the issue of robust overfitting in deep neural networks (DNNs) during adversarial training (AT). It extends neural tangent kernel (NTK) theory to explain this phenomenon for infinite width deep networks, showing that a wide DNN under AT can behave like a linearized DNN, leading to AT degeneration over time (assuming squared loss). To address this, the paper introduces Adv-NTK, an novel AT algorithm for infinite-width DNNs, designed to enhance network robustness. The effectiveness of Adv-NTK is demonstrated through experiments on real-world datasets, establishing its real potential in improving DNN robustness against adversarial attacks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper attempts to tackle an important problem of overfitting in adversarial training from a theoretical perspective, which should be relevant to the broader community.\n\nThe empirical validation of the Adv-NTK algorithm using real-world datasets like SVHN and CIFAR-10 enhances the quality of the paper. This empirical approach ensures that the theoretical findings are not only sound in theory but also applicable and effective in real-world scenarios."
                },
                "weaknesses": {
                    "value": "Its not clearly under what conditions the small step size can lead to linearization of the adversarial training of DDN.\n\nIn the proof, it does not try to find efficient direction, does that make any difference on the proof ?"
                },
                "questions": {
                    "value": "It would improve the paper if bound on the learning rate can be provided with respect to its training (or maybe some empirical results towards that)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2578/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2578/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2578/Reviewer_gmGK"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2578/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699639975158,
            "cdate": 1699639975158,
            "tmdate": 1699639975158,
            "mdate": 1699639975158,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "or1JzKjw6B",
                "forum": "1op5YGZu8X",
                "replyto": "HKwygi9PSy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2578/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2578/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Request for clarifying \"efficient direction\""
                    },
                    "comment": {
                        "value": "Dear Reviewer gmGK,\n\nThanks for your detailed review and kind support. We are currently working on the rebuttal to carefully answer your questions.\n\nTo fully address all your concerns, we would like to ask that in your question 2, what does **\"efficient direction\"** mean? Does it mean **the direction for searching adversarial examples**?\n\nIf not, a clarification of the term \"efficient direction\" would be very helpful for us to prepare our answers!\n\nThanks & sincerely,\n\nThe Authors"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2578/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699908425359,
                "cdate": 1699908425359,
                "tmdate": 1699908425359,
                "mdate": 1699908425359,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WyeuiJRSFX",
                "forum": "1op5YGZu8X",
                "replyto": "HKwygi9PSy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2578/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2578/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer gmGK"
                    },
                    "comment": {
                        "value": "Thank you for your constructive comments and kind support! All your concerns have been carefully responded below. The manuscript is carefully revised accordingly. We sincerely hope our responses fully address your questions.\n\n**Q1:** *It is not clear under what conditions the small step size can lead to linearization of the adversarial training of DDN.*\n\n**A1:** We respectfully note that the linearization of DNNs in AT does not require the step size in adversarial perturbation to be small. Instead, the linearization of DNNs mainly depends on if the network widths are large enough.\n\nThe idea that \"large network widths lead to model linearization\" has already been proved in vanilla NTK theory for standard training (for example, see [r1, r2, r3]). We further proved this idea in the setting of AT, and this is one of the most exciting contributions of our paper. We have added Remark 2 in Section 4.3 on Page 6 in the revised version to clarify this contribution, please kindly refer for more details.\n\n**Q2:** *In the proof, it does not try to find an efficient direction, does that make any difference on the proof?*\n\n**A2:** Firstly, we respectfully argue that the gradient flow-based attack mechanism (based on Eq.(7)) leveraged in our proof indeed finds efficient directions to perform adversarial attacks. Specifically, compared with the common projected gradient descent (PGD) attack (see Appendix F.1 on Page 42), the difference of our mechanism is that it leverages a learning rate $\\eta_i(t)$ and a total search time $S$ to control the adversarial strength (i.e., the strength of the ability to make models misbehave), while PGD uses a perturbation radius $\\rho$ to control the perturbation scale. As long as $\\eta_i(t) S$ is set to be large enough, our mechanism can find adversarial examples to effectively attack targeted DNNs, just like the PGD attack.\n\nBesides, from the perspective of the proof, the main difference in using the gradient flow-based attack instead of the PGD attack is that the gradient flow-based attack enables our proof to analyze AT dynamics without explicitly modeling the boundaries of adversarial perturbation spaces. Please also kindly refer to the first two paragraphs in Section 4.1 on Page 4 for a detailed discussion.\n\n**Q3:** *It would improve the paper if bound on the learning rate can be provided with respect to its training (or maybe some empirical results towards that).*\n\n**A3:** Thanks for your suggestion. However, we could not agree that providing bounds on the learning rate is a good idea, since as explained in **A1**, our theoretical result that \"a wide DNN under AT behaves like a linearized DNN\" does not depend on the scale of the learning rate. Instead, it mainly depends on if the network widths are large enough. The only requirement for the learning rate is that it should be continuously differentiable (see Assumption 3 on Page 6).\n\nNevertheless, the only thing the scale of learning rate could affect is the converging behavior of DNNs during long-term AT. We have added an intuitive discussion about this impact in the revised version, please kindly refer to Appendix E.2 on Page 42 for details.\n\n**References:**\n\n[r1] Jacot et al. \"Neural tangent kernel: Convergence and generalization in neural networks.\" NeurIPS 2018.\n\n[r2] Lee et al. \"Wide neural networks of any depth evolve as linear models under gradient descent\". NeurIPS 2019.\n\n[r3] Arora et al. \"On exact computation with an infinitely wide neural net.\" NeurIPS 2019."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2578/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700203953416,
                "cdate": 1700203953416,
                "tmdate": 1700203953416,
                "mdate": 1700203953416,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "atKksa1zAI",
                "forum": "1op5YGZu8X",
                "replyto": "HKwygi9PSy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2578/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2578/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up on rebuttal"
                    },
                    "comment": {
                        "value": "Dear Reviewer gmGK,\n\nThanks for your time in reviewing our paper and valuable comments!\n\nSince the discussion stage is about to end, we are writing to kindly ask if our replies have satisfactorily addressed your concerns. Please kindly let us know if you have any additional concerns and we are happy to discuss more.\n\nThank you very much!"
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2578/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700598527103,
                "cdate": 1700598527103,
                "tmdate": 1700598527103,
                "mdate": 1700598527103,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ACEACARbJP",
            "forum": "1op5YGZu8X",
            "replyto": "1op5YGZu8X",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2578/Reviewer_S6eo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2578/Reviewer_S6eo"
            ],
            "content": {
                "summary": {
                    "value": "This paper studied the robust overfitting issue in adversarial training. Specifically, the author studied the training dynamics of adversarial training under the NTK regime. Theoretical results show that after long training the term that captures the robustness will fade away and the trained network will degenerate to the network with standard training. The author further proposed an algorithm for adversarial training under the NTK regime and conducted experiment. Results show that the proposed algorithm outperforms standard adversarial training and vanilla NTK."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper provides a novel theoretical view in robust overfitting in adversarial training, and the theoretical analysis gives an intuition on why adversarial training fails on a simplified regime (NTK).\n2. The paper further proposed an algorithm applicable to NTK models. Experiment results aligns with the theoretical conclusion in the paper."
                },
                "weaknesses": {
                    "value": "My concerns are listed as the follows:\n\n1. The proposed algorithm can only work under NTK regime. Though the corresponding results can serve as an empirical evidence of the theoretical conclusion, it seems that the proposed algorithm has limited practical application.\n2. The assumption in the paper seems too strong comparing with practical setting. The author are encouraged to consider more practical setting like GD and cross entropy loss. \n3. In the studied setting, the scale of pertubation is also related to the norm of $\\partial_x \\mathcal{L}$, rather than depending on $S$, which introduces a discrepancy between the setting studied and real-world setting.\n4. Some missing references in convergence of DNN: [1], [2], [3]\n\n[1]Li, Yuanzhi, and Yingyu Liang. \"Learning overparameterized neural networks via stochastic gradient descent on structured data.\" Advances in neural information processing systems 31 (2018).\n\n[2]Zou, Difan, et al. \"Gradient descent optimizes over-parameterized deep ReLU networks.\" Machine learning 109 (2020): 467-492.\n\n[3]Allen-Zhu, Zeyuan, Yuanzhi Li, and Zhao Song. \"A convergence theory for deep learning via over-parameterization.\" International conference on machine learning. PMLR, 2019."
                },
                "questions": {
                    "value": "Please refer to the \"weakness\" section"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2578/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2578/Reviewer_S6eo",
                        "ICLR.cc/2024/Conference/Submission2578/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2578/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699655755473,
            "cdate": 1699655755473,
            "tmdate": 1700502343141,
            "mdate": 1700502343141,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JP0lU4fNaA",
                "forum": "1op5YGZu8X",
                "replyto": "ACEACARbJP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2578/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2578/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Request for clarifying the term \"perturbation\""
                    },
                    "comment": {
                        "value": "Dear Reviewer S6eo,\n\nThanks for your detailed review and kind support. We are currently working on the rebuttal to carefully answer your questions.\n\nTo fully address all your concerns, we would like to make sure that in your question 3, does the term **\"perturbation\"** refer to the **\"adversarial perturbation occurring in the search for adversarial examples\"**?\n\nWe are a bit confused by this question 3 because you also mentioned that **the dependence between \"the perturbation\" and \"the norm of $\\partial_x \\mathcal L$\" in our setting** introduces **a discrepancy between our setting and the real-world setting**. But in the real-world setting, projected gradient descent (PGD) is usually adopted to search adversarial examples, **which also requires calculating gradients based on $\\partial_x \\mathcal L$** (see Appendix F.1 in page 41).\n\nA clarification would be very helpful for us to prepare our answers!\n\nThanks & sincerely,\n\nThe Author"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2578/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699909297343,
                "cdate": 1699909297343,
                "tmdate": 1699909297343,
                "mdate": 1699909297343,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yeTEKe77sP",
                "forum": "1op5YGZu8X",
                "replyto": "JP0lU4fNaA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2578/Reviewer_S6eo"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2578/Reviewer_S6eo"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Authors,\n\nSorry for the confusion. Yes, my question is that in equation (1), the constraint of the adversarial example is $||x'-x|| \\leq \\rho$. Then how is $||x_{i,t,s}-x_i|| \\leq \\rho$ be guaranteed?\n\nThanks"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2578/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699915465361,
                "cdate": 1699915465361,
                "tmdate": 1699915465361,
                "mdate": 1699915465361,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UnmKbKlrll",
                "forum": "1op5YGZu8X",
                "replyto": "ACEACARbJP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2578/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2578/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer S6eo (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your thorough review and constructive comments. All your concerns have been carefully responded below. The manuscript is carefully revised accordingly. We sincerely hope our responses fully address your questions.\n\n**Q1:** *The proposed algorithm can only work under the NTK regime. Though the corresponding results can serve as empirical evidence of the theoretical conclusion, it seems that the proposed algorithm has limited practical application.*\n\n**A1:** First of all, we would like to highlight that the ultimate goal of this paper is to give a theoretical explanation for the robust overfitting phenomenon when adversarially training DNNs, but not designing a practical AT algorithm. The role of our empirical evidence in Section 6 is to verify our theoretical finding that the regularization matrix $\\Xi(t)$ indeed captures the robustness brought by AT. It is also unexpected for us that the proposed Adv-NTK can achieve comparable robustness with vanilla AT. However, making Adv-NTK further practical could be a bit out of the scope of the research goal of this paper. Therefore, we will leave this interesting research direction in future studies.\n\nBesides, we respectfully note that improving the practicality of algorithms that work under the NTK regime is a very active research direction and a variety of works such as [r1, r2, r3, r4, r5] are trying to improve the performance and efficiency of NTK models in real-world applications. For example, [r1] have made a five-layer NTK model (with convolutional layer and global average pooling layer (GAP; [r6])) under standard training to achieve almost 80% test accuracy on CIFAR-10 dataset with only 10,000 training data. To make our Adv-NTK as practical as that in [r1], we need to adapt a variety of calculation techniques to our AT setting, which includes GAP layers [r6], Monte Carlo-based NTK/gradient estimation [r2], single-precision calculation, and kernel compression [r7]. However, such adaptations would raise a series of mathematical and engineering challenges, and this is also one of the reasons that we will leave the practicality problem of Adv-NTK in future research but not in this paper.\n\n**Q2:** *The assumption in the paper seems too strong compared with the practical setting. The authors are encouraged to consider more practical settings like GD and cross entropy loss.*\n\n**A2:** Thanks for your suggestions.\n\n**About gradient descent (GD).** The NTK theory with GD for standard training has been well studied in [r8, r9]. We believe that by generalizing the results in [r8, r9] to our AT setting, we can prove results such as ADK/NTK kernel convergence, prediction convergence, parameter convergence, and DNN linearization when considering AT with GD. The main technical challenge would arise from analyzing our proposed adversarial example search strategy under GD.\n\nNevertheless, we would also like to again highlight that the main goal and contribution of this paper is to propose the first theoretical explanation for robust overfitting in DNNs. The generalization of our results to the GD setting will be left in future research.\n\n**About cross-entropy loss.** We respectfully note that existing NTK literature has not yet proved a closed-form training dynamics solution for standard DNN training under cross-entropy loss. Since our paper is built upon previous NTK research, for now, we are unable to generalize our close-form AT dynamics for squared loss (i.e., Theorem 3 and Corollary 1 in Section 4.3 on Page 6) to that for cross-entropy loss.\n\nHowever, please note that the squared loss is also a suitable replacement for cross-entropy loss in many practical applications. The reason is in practice, cross-entropy loss is usually used to train classification models. Fortunately, by simply replacing the categorical label-encoding (in cross-entropy loss) with the one-hot label-encoding (in squared loss), classification models can also be trained with squared loss."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2578/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700203423629,
                "cdate": 1700203423629,
                "tmdate": 1700203423629,
                "mdate": 1700203423629,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0fQwB8XwCa",
                "forum": "1op5YGZu8X",
                "replyto": "ACEACARbJP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2578/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2578/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up on rebuttal"
                    },
                    "comment": {
                        "value": "Dear Reviewer S6eo,\n\nThank you again for your valuable comments! We have carefully considered your comments and have provided our responses.\n\nPlease let us know if our replies have satisfactorily addressed your concerns. Please do not hesitate to let us know if you have any further questions or if you require any additional clarification.\n\nThank you very much!"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2578/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700400798595,
                "cdate": 1700400798595,
                "tmdate": 1700400798595,
                "mdate": 1700400798595,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mt09llfQ7G",
                "forum": "1op5YGZu8X",
                "replyto": "0fQwB8XwCa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2578/Reviewer_S6eo"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2578/Reviewer_S6eo"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Authors:\n\nThank you very much for your response.\n\nThe authors have addressed most of my concerns and I will make necessary changes to my review and rating.\n\nThanks,\n\nReviewer S6eo"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2578/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700502298743,
                "cdate": 1700502298743,
                "tmdate": 1700502298743,
                "mdate": 1700502298743,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]